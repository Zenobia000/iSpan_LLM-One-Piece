{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Backend 深度配置與優化\n",
    "\n",
    "## 🎯 學習目標\n",
    "\n",
    "本實驗將深入探討 Triton PyTorch Backend 的高級配置和優化技術，學習如何最大化利用 PyTorch 在 Triton 中的靈活性和性能。\n",
    "\n",
    "### 核心知識點\n",
    "- ✅ PyTorch Backend 進階配置\n",
    "- ✅ 動態形狀 (Dynamic Shapes) 處理\n",
    "- ✅ 記憶體池管理與優化\n",
    "- ✅ 自定義運算子整合\n",
    "- ✅ 批次處理優化\n",
    "- ✅ 模型熱更新機制\n",
    "\n",
    "### 技術架構\n",
    "```\n",
    "Triton Server\n",
    "├── PyTorch Backend Engine\n",
    "│   ├── Dynamic Shape Manager\n",
    "│   ├── Memory Pool Allocator  \n",
    "│   ├── Custom Operator Registry\n",
    "│   └── Batch Scheduler\n",
    "├── Model Repository\n",
    "│   ├── config.pbtxt (Advanced)\n",
    "│   ├── model.py (Custom Logic)\n",
    "│   └── version_policy.json\n",
    "└── Performance Monitor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境準備與驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "import tritonclient.http as httpclient\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# 設定日誌\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"🔧 環境資訊檢查\")\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 數量: {torch.cuda.device_count()}\")\n",
    "    print(f\"當前 GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA 版本: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定工作目錄\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_REPO = BASE_DIR / \"model_repository_advanced\"\n",
    "CONFIGS_DIR = BASE_DIR / \"configs\"\n",
    "SCRIPTS_DIR = BASE_DIR / \"scripts\"\n",
    "\n",
    "# 創建必要目錄\n",
    "for dir_path in [MODEL_REPO, CONFIGS_DIR, SCRIPTS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "print(f\"📁 工作目錄: {BASE_DIR}\")\n",
    "print(f\"📁 模型倉庫: {MODEL_REPO}\")\n",
    "print(f\"📁 配置目錄: {CONFIGS_DIR}\")\n",
    "print(f\"📁 腳本目錄: {SCRIPTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 動態形狀處理架構設計\n",
    "\n",
    "### 核心概念\n",
    "動態形狀是現代深度學習應用的關鍵需求，特別是在處理變長序列、不同尺寸圖像等場景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DynamicShapeConfig:\n",
    "    \"\"\"動態形狀配置類\"\"\"\n",
    "    input_name: str\n",
    "    min_shape: List[int]\n",
    "    opt_shape: List[int]\n",
    "    max_shape: List[int]\n",
    "    data_type: str = \"TYPE_FP32\"\n",
    "    \n",
    "class AdvancedModelConfig:\n",
    "    \"\"\"高級模型配置生成器\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.backend = \"pytorch\"\n",
    "        self.max_batch_size = 0  # 動態批次\n",
    "        self.dynamic_configs = []\n",
    "        \n",
    "    def add_dynamic_input(self, config: DynamicShapeConfig):\n",
    "        \"\"\"添加動態輸入配置\"\"\"\n",
    "        self.dynamic_configs.append(config)\n",
    "        \n",
    "    def generate_config(self) -> str:\n",
    "        \"\"\"生成進階 config.pbtxt\"\"\"\n",
    "        config_lines = [\n",
    "            f'name: \"{self.model_name}\"',\n",
    "            f'backend: \"{self.backend}\"',\n",
    "            f'max_batch_size: {self.max_batch_size}',\n",
    "            '',\n",
    "            '# 動態形狀輸入配置'\n",
    "        ]\n",
    "        \n",
    "        # 添加動態輸入\n",
    "        for i, config in enumerate(self.dynamic_configs):\n",
    "            config_lines.extend([\n",
    "                'input {',\n",
    "                f'  name: \"{config.input_name}\"',\n",
    "                f'  data_type: {config.data_type}',\n",
    "                f'  dims: [ -1 ]  # 動態維度',\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "            \n",
    "        # 添加輸出配置\n",
    "        config_lines.extend([\n",
    "            'output {',\n",
    "            '  name: \"output\"',\n",
    "            '  data_type: TYPE_FP32',\n",
    "            '  dims: [ -1 ]',\n",
    "            '}',\n",
    "            '',\n",
    "            '# 實例組配置',\n",
    "            'instance_group {',\n",
    "            '  count: 2',\n",
    "            '  kind: KIND_GPU',\n",
    "            '}',\n",
    "            '',\n",
    "            '# 動態批次配置',\n",
    "            'dynamic_batching {',\n",
    "            '  preferred_batch_size: [ 4, 8, 16 ]',\n",
    "            '  max_queue_delay_microseconds: 100',\n",
    "            '  preserve_ordering: true',\n",
    "            '}',\n",
    "            '',\n",
    "            '# 優化配置',\n",
    "            'optimization {',\n",
    "            '  cuda {',\n",
    "            '    graphs: true',\n",
    "            '    graph_spec {',\n",
    "            '      batch_size: 1',\n",
    "            '      input {',\n",
    "            f'        key: \"{self.dynamic_configs[0].input_name if self.dynamic_configs else \"input\"}\"',\n",
    "            f'        value {{',\n",
    "            f'          dim: [ {\"16\" if self.dynamic_configs else \"224\"} ]',\n",
    "            '        }',\n",
    "            '      }',\n",
    "            '    }',\n",
    "            '  }',\n",
    "            '}'\n",
    "        ])\n",
    "        \n",
    "        return '\\n'.join(config_lines)\n",
    "\n",
    "# 示例：文本分類模型動態配置\n",
    "text_config = DynamicShapeConfig(\n",
    "    input_name=\"input_ids\",\n",
    "    min_shape=[1],      # 最小序列長度\n",
    "    opt_shape=[128],    # 最優序列長度\n",
    "    max_shape=[512],    # 最大序列長度\n",
    "    data_type=\"TYPE_INT64\"\n",
    ")\n",
    "\n",
    "model_config = AdvancedModelConfig(\"text_classifier_dynamic\")\n",
    "model_config.add_dynamic_input(text_config)\n",
    "\n",
    "print(\"🔧 動態形狀配置生成器已創建\")\n",
    "print(f\"📝 支持的配置類型: {len(model_config.dynamic_configs)} 個動態輸入\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 記憶體池管理與優化\n",
    "\n",
    "### 記憶體優化策略\n",
    "- **預分配池**: 避免動態分配開銷\n",
    "- **分段管理**: 不同大小張量分別管理\n",
    "- **回收策略**: 智能垃圾回收機制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryPoolManager:\n",
    "    \"\"\"記憶體池管理器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pools = {}\n",
    "        self.usage_stats = {}\n",
    "        self.peak_usage = 0\n",
    "        \n",
    "    def create_pool(self, name: str, size_mb: int):\n",
    "        \"\"\"創建記憶體池\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            # 預分配 GPU 記憶體\n",
    "            pool_size = size_mb * 1024 * 1024\n",
    "            dummy_tensor = torch.empty(pool_size // 4, dtype=torch.float32, device='cuda')\n",
    "            del dummy_tensor  # 觸發記憶體分配但不保留引用\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        self.pools[name] = {\n",
    "            'size_mb': size_mb,\n",
    "            'allocated': 0,\n",
    "            'peak': 0,\n",
    "            'tensors': []\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"創建記憶體池 '{name}': {size_mb}MB\")\n",
    "        \n",
    "    def allocate_tensor(self, pool_name: str, shape: tuple, dtype=torch.float32) -> torch.Tensor:\n",
    "        \"\"\"從記憶體池分配張量\"\"\"\n",
    "        if pool_name not in self.pools:\n",
    "            raise ValueError(f\"記憶體池 '{pool_name}' 不存在\")\n",
    "            \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        tensor = torch.empty(shape, dtype=dtype, device=device)\n",
    "        \n",
    "        # 更新統計\n",
    "        pool = self.pools[pool_name]\n",
    "        tensor_size = tensor.numel() * tensor.element_size()\n",
    "        pool['allocated'] += tensor_size\n",
    "        pool['peak'] = max(pool['peak'], pool['allocated'])\n",
    "        pool['tensors'].append(tensor)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def get_memory_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"獲取記憶體使用統計\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_stats = {\n",
    "                'allocated_mb': torch.cuda.memory_allocated() / 1024**2,\n",
    "                'reserved_mb': torch.cuda.memory_reserved() / 1024**2,\n",
    "                'max_allocated_mb': torch.cuda.max_memory_allocated() / 1024**2\n",
    "            }\n",
    "            stats['gpu'] = gpu_stats\n",
    "            \n",
    "        # 系統記憶體\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "        stats['system'] = {\n",
    "            'rss_mb': memory_info.rss / 1024**2,\n",
    "            'vms_mb': memory_info.vms / 1024**2\n",
    "        }\n",
    "        \n",
    "        # 記憶體池統計\n",
    "        pool_stats = {}\n",
    "        for name, pool in self.pools.items():\n",
    "            pool_stats[name] = {\n",
    "                'size_mb': pool['size_mb'],\n",
    "                'allocated_mb': pool['allocated'] / 1024**2,\n",
    "                'peak_mb': pool['peak'] / 1024**2,\n",
    "                'utilization': pool['allocated'] / (pool['size_mb'] * 1024**2) * 100\n",
    "            }\n",
    "        stats['pools'] = pool_stats\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def cleanup_pool(self, pool_name: str):\n",
    "        \"\"\"清理記憶體池\"\"\"\n",
    "        if pool_name in self.pools:\n",
    "            pool = self.pools[pool_name]\n",
    "            # 清理張量引用\n",
    "            pool['tensors'].clear()\n",
    "            pool['allocated'] = 0\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            logger.info(f\"已清理記憶體池 '{pool_name}'\")\n",
    "\n",
    "# 創建記憶體管理器\n",
    "memory_manager = MemoryPoolManager()\n",
    "\n",
    "# 創建不同用途的記憶體池\n",
    "memory_manager.create_pool(\"inference\", 512)      # 推理專用 512MB\n",
    "memory_manager.create_pool(\"preprocessing\", 128)  # 預處理專用 128MB\n",
    "memory_manager.create_pool(\"cache\", 256)          # 緩存專用 256MB\n",
    "\n",
    "print(\"🧠 記憶體池管理器已初始化\")\n",
    "print(f\"📊 創建了 {len(memory_manager.pools)} 個記憶體池\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 自定義 PyTorch 模型實現\n",
    "\n",
    "### 高級模型類設計\n",
    "實現支持動態形狀、記憶體優化的 Triton PyTorch 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建高級文本分類模型\n",
    "model_dir = MODEL_REPO / \"text_classifier_advanced\" / \"1\"\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 高級模型實現\n",
    "model_code = '''\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "class AdvancedTextClassifier(nn.Module):\n",
    "    \"\"\"支持動態形狀的高級文本分類器\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30000, embed_dim=256, hidden_dim=512, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, 8, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 性能優化：預編譯圖\n",
    "        self._compiled = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # 動態序列長度處理\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != 0).float()\n",
    "            \n",
    "        # 嵌入層\n",
    "        embeds = self.embedding(input_ids)\n",
    "        \n",
    "        # LSTM 編碼\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        # 注意力機制\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out, \n",
    "                                   key_padding_mask=(attention_mask == 0))\n",
    "        \n",
    "        # 池化：使用注意力權重平均\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand_as(attn_out)\n",
    "        sum_embeddings = torch.sum(attn_out * mask_expanded, dim=1)\n",
    "        sum_mask = torch.sum(mask_expanded, dim=1)\n",
    "        pooled = sum_embeddings / (sum_mask + 1e-9)\n",
    "        \n",
    "        # 分類\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"Triton PyTorch Backend 高級模型\"\"\"\n",
    "    \n",
    "    def initialize(self, args):\n",
    "        self.model_config = model_config = json.loads(args[\\'model_config\\'])\n",
    "        self.model_instance_name = args[\\'model_instance_name\\']\n",
    "        \n",
    "        # 設定日誌\n",
    "        self.logger = logging.getLogger(f\"AdvancedModel-{self.model_instance_name}\")\n",
    "        \n",
    "        # 設備配置\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(f\"cuda:{args.get(\\'model_instance_device_id\\', 0)}\")\n",
    "            self.logger.info(f\"使用 GPU: {self.device}\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.logger.info(\"使用 CPU\")\n",
    "            \n",
    "        # 初始化模型\n",
    "        self.model = AdvancedTextClassifier()\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 性能優化：JIT 編譯\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                # 創建示例輸入進行 JIT 編譯\n",
    "                example_input = torch.randint(1, 1000, (1, 128), device=self.device)\n",
    "                self.model = torch.jit.trace(self.model, example_input)\n",
    "                self.logger.info(\"JIT 編譯成功\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"JIT 編譯失敗: {e}\")\n",
    "        \n",
    "        # 記憶體池初始化\n",
    "        self.memory_stats = {\n",
    "            \\'total_requests\\': 0,\n",
    "            \\'total_time\\': 0,\n",
    "            \\'peak_memory\\': 0\n",
    "        }\n",
    "        \n",
    "        # 預熱模型\n",
    "        self._warmup_model()\n",
    "        \n",
    "        self.logger.info(\"高級 PyTorch 模型初始化完成\")\n",
    "    \n",
    "    def _warmup_model(self):\n",
    "        \"\"\"模型預熱\"\"\"\n",
    "        self.logger.info(\"開始模型預熱...\")\n",
    "        \n",
    "        # 不同長度的預熱輸入\n",
    "        warmup_lengths = [16, 64, 128, 256]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for length in warmup_lengths:\n",
    "                dummy_input = torch.randint(1, 1000, (2, length), device=self.device)\n",
    "                _ = self.model(dummy_input)\n",
    "                \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        self.logger.info(\"模型預熱完成\")\n",
    "    \n",
    "    def execute(self, requests):\n",
    "        \"\"\"執行推理請求\"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # 獲取輸入\n",
    "                input_ids = pb_utils.get_input_tensor_by_name(request, \"input_ids\")\n",
    "                input_data = input_ids.as_numpy()\n",
    "                \n",
    "                # 轉換為 PyTorch 張量\n",
    "                input_tensor = torch.from_numpy(input_data).to(self.device)\n",
    "                \n",
    "                # 推理\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_tensor)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # 轉換回 numpy\n",
    "                output_data = probs.cpu().numpy()\n",
    "                \n",
    "                # 創建輸出張量\n",
    "                output_tensor = pb_utils.Tensor(\"output\", output_data)\n",
    "                response = pb_utils.InferenceResponse(output_tensors=[output_tensor])\n",
    "                \n",
    "                # 更新統計\n",
    "                end_time = time.time()\n",
    "                self.memory_stats[\\'total_requests\\'] += 1\n",
    "                self.memory_stats[\\'total_time\\'] += (end_time - start_time)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    current_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "                    self.memory_stats[\\'peak_memory\\'] = max(\n",
    "                        self.memory_stats[\\'peak_memory\\'], current_memory\n",
    "                    )\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"推理錯誤: {e}\")\n",
    "                error_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[],\n",
    "                    error=pb_utils.TritonError(f\"推理失敗: {str(e)}\")\n",
    "                )\n",
    "                responses.append(error_response)\n",
    "                continue\n",
    "                \n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"清理資源\"\"\"\n",
    "        self.logger.info(\"正在清理模型資源...\")\n",
    "        \n",
    "        # 輸出統計信息\n",
    "        if self.memory_stats[\\'total_requests\\'] > 0:\n",
    "            avg_time = self.memory_stats[\\'total_time\\'] / self.memory_stats[\\'total_requests\\']\n",
    "            self.logger.info(f\"總請求數: {self.memory_stats[\\'total_requests\\']}\") \n",
    "            self.logger.info(f\"平均推理時間: {avg_time:.4f}s\")\n",
    "            self.logger.info(f\"峰值記憶體使用: {self.memory_stats[\\'peak_memory\\']:.2f}MB\")\n",
    "        \n",
    "        # 清理 GPU 記憶體\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        self.logger.info(\"模型資源清理完成\")\n",
    "'''\n",
    "\n",
    "# 寫入模型文件\n",
    "with open(model_dir / \"model.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(model_code)\n",
    "\n",
    "print(f\"✅ 高級 PyTorch 模型已創建: {model_dir / 'model.py'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 進階配置文件生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成高級配置文件\n",
    "advanced_config = '''\n",
    "name: \"text_classifier_advanced\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 0\n",
    "\n",
    "# 動態輸入配置\n",
    "input {\n",
    "  name: \"input_ids\"\n",
    "  data_type: TYPE_INT64\n",
    "  dims: [ -1 ]  # 動態序列長度\n",
    "}\n",
    "\n",
    "output {\n",
    "  name: \"output\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: [ 10 ]  # 10 個分類\n",
    "}\n",
    "\n",
    "# 實例組配置 - 多實例提高吞吐量\n",
    "instance_group {\n",
    "  count: 2\n",
    "  kind: KIND_GPU\n",
    "  gpus: [ 0 ]\n",
    "}\n",
    "\n",
    "# 動態批次配置\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 2, 4, 8, 16 ]\n",
    "  max_queue_delay_microseconds: 500\n",
    "  preserve_ordering: false\n",
    "  priority_levels: 2\n",
    "  default_priority_level: 0\n",
    "  \n",
    "  # 批次大小控制\n",
    "  default_queue_policy {\n",
    "    timeout_action: REJECT\n",
    "    default_timeout_microseconds: 1000000\n",
    "    allow_timeout_override: true\n",
    "    max_batch_size: 32\n",
    "  }\n",
    "}\n",
    "\n",
    "# 模型優化配置\n",
    "optimization {\n",
    "  cuda {\n",
    "    graphs: true\n",
    "    busy_wait_events: true\n",
    "    graph_spec {\n",
    "      batch_size: 1\n",
    "      input {\n",
    "        key: \"input_ids\"\n",
    "        value {\n",
    "          dim: [ 128 ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    graph_spec {\n",
    "      batch_size: 4\n",
    "      input {\n",
    "        key: \"input_ids\"\n",
    "        value {\n",
    "          dim: [ 256 ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# 模型預熱配置\n",
    "model_warmup {\n",
    "  name: \"warmup_sample_1\"\n",
    "  batch_size: 1\n",
    "  inputs {\n",
    "    key: \"input_ids\"\n",
    "    value {\n",
    "      data_type: TYPE_INT64\n",
    "      dims: [ 128 ]\n",
    "      zero_data: true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "model_warmup {\n",
    "  name: \"warmup_sample_batch\"\n",
    "  batch_size: 8\n",
    "  inputs {\n",
    "    key: \"input_ids\"\n",
    "    value {\n",
    "      data_type: TYPE_INT64\n",
    "      dims: [ 256 ]\n",
    "      zero_data: true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# 性能參數\n",
    "parameters {\n",
    "  key: \"EXECUTION_ENV_PATH\"\n",
    "  value: { string_value: \"/opt/tritonserver/backends/python/triton_python_backend_stub\" }\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  key: \"shm_region_prefix_name\"\n",
    "  value: { string_value: \"triton_python_backend\" }\n",
    "}\n",
    "'''\n",
    "\n",
    "# 寫入配置文件\n",
    "config_path = MODEL_REPO / \"text_classifier_advanced\" / \"config.pbtxt\"\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(advanced_config.strip())\n",
    "\n",
    "print(f\"✅ 高級配置文件已創建: {config_path}\")\n",
    "print(\"\\n🔧 配置特性:\")\n",
    "print(\"- ✅ 動態序列長度支持\")\n",
    "print(\"- ✅ 智能批次調度\")\n",
    "print(\"- ✅ CUDA Graph 優化\")\n",
    "print(\"- ✅ 多實例並行\")\n",
    "print(\"- ✅ 模型預熱\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 性能監控與分析工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"性能監控器\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url=\"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.metrics_history = []\n",
    "        \n",
    "    def get_server_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"獲取服務器指標\"\"\"\n",
    "        try:\n",
    "            # 獲取服務器統計\n",
    "            stats = self.client.get_inference_statistics()\n",
    "            \n",
    "            # 獲取模型狀態\n",
    "            models = self.client.get_model_repository_index()\n",
    "            \n",
    "            metrics = {\n",
    "                'timestamp': time.time(),\n",
    "                'server_stats': stats,\n",
    "                'model_count': len(models),\n",
    "                'system_resources': self._get_system_resources()\n",
    "            }\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"獲取指標失敗: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _get_system_resources(self) -> Dict[str, float]:\n",
    "        \"\"\"獲取系統資源使用情況\"\"\"\n",
    "        resources = {\n",
    "            'cpu_percent': psutil.cpu_percent(),\n",
    "            'memory_percent': psutil.virtual_memory().percent,\n",
    "            'memory_used_gb': psutil.virtual_memory().used / 1024**3\n",
    "        }\n",
    "        \n",
    "        # GPU 資源\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu = gpus[0]\n",
    "                resources.update({\n",
    "                    'gpu_utilization': gpu.load * 100,\n",
    "                    'gpu_memory_used': gpu.memoryUsed,\n",
    "                    'gpu_memory_total': gpu.memoryTotal,\n",
    "                    'gpu_memory_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100,\n",
    "                    'gpu_temperature': gpu.temperature\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return resources\n",
    "    \n",
    "    def benchmark_model(self, model_name: str, test_data: List[np.ndarray], \n",
    "                       concurrent_requests: int = 1, iterations: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"模型基準測試\"\"\"\n",
    "        \n",
    "        latencies = []\n",
    "        errors = 0\n",
    "        \n",
    "        print(f\"🚀 開始基準測試: {model_name}\")\n",
    "        print(f\"📊 參數: {concurrent_requests} 並發, {iterations} 次迭代\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # 隨機選擇測試數據\n",
    "            test_input = test_data[i % len(test_data)]\n",
    "            \n",
    "            try:\n",
    "                # 創建輸入\n",
    "                inputs = [httpclient.InferInput(\"input_ids\", test_input.shape, \"INT64\")]\n",
    "                inputs[0].set_data_from_numpy(test_input)\n",
    "                \n",
    "                # 發送請求並計時\n",
    "                request_start = time.time()\n",
    "                response = self.client.infer(model_name, inputs)\n",
    "                request_end = time.time()\n",
    "                \n",
    "                latency = (request_end - request_start) * 1000  # 轉換為毫秒\n",
    "                latencies.append(latency)\n",
    "                \n",
    "                if i % 20 == 0:\n",
    "                    print(f\"  進度: {i}/{iterations}, 當前延遲: {latency:.2f}ms\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                logger.error(f\"請求失敗: {e}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # 計算統計數據\n",
    "        if latencies:\n",
    "            stats = {\n",
    "                'total_requests': iterations,\n",
    "                'successful_requests': len(latencies),\n",
    "                'error_rate': errors / iterations * 100,\n",
    "                'total_time_seconds': total_time,\n",
    "                'throughput_rps': len(latencies) / total_time,\n",
    "                'latency_stats': {\n",
    "                    'mean_ms': np.mean(latencies),\n",
    "                    'median_ms': np.median(latencies),\n",
    "                    'p95_ms': np.percentile(latencies, 95),\n",
    "                    'p99_ms': np.percentile(latencies, 99),\n",
    "                    'min_ms': np.min(latencies),\n",
    "                    'max_ms': np.max(latencies),\n",
    "                    'std_ms': np.std(latencies)\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            stats = {\n",
    "                'total_requests': iterations,\n",
    "                'successful_requests': 0,\n",
    "                'error_rate': 100.0,\n",
    "                'total_time_seconds': total_time,\n",
    "                'throughput_rps': 0\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def plot_performance_trends(self, save_path: Optional[str] = None):\n",
    "        \"\"\"繪製性能趨勢圖\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"⚠️  沒有歷史數據可以繪製\")\n",
    "            return\n",
    "            \n",
    "        # 提取時間序列數據\n",
    "        timestamps = [m['timestamp'] for m in self.metrics_history]\n",
    "        cpu_usage = [m['system_resources']['cpu_percent'] for m in self.metrics_history]\n",
    "        memory_usage = [m['system_resources']['memory_percent'] for m in self.metrics_history]\n",
    "        \n",
    "        # 創建圖表\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Triton Server 性能監控儀表板', fontsize=16)\n",
    "        \n",
    "        # CPU 使用率\n",
    "        axes[0, 0].plot(timestamps, cpu_usage, 'b-', linewidth=2)\n",
    "        axes[0, 0].set_title('CPU 使用率')\n",
    "        axes[0, 0].set_ylabel('使用率 (%)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 記憶體使用率\n",
    "        axes[0, 1].plot(timestamps, memory_usage, 'r-', linewidth=2)\n",
    "        axes[0, 1].set_title('記憶體使用率')\n",
    "        axes[0, 1].set_ylabel('使用率 (%)')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # GPU 使用率 (如果可用)\n",
    "        gpu_usage = []\n",
    "        gpu_memory = []\n",
    "        for m in self.metrics_history:\n",
    "            resources = m['system_resources']\n",
    "            gpu_usage.append(resources.get('gpu_utilization', 0))\n",
    "            gpu_memory.append(resources.get('gpu_memory_percent', 0))\n",
    "            \n",
    "        axes[1, 0].plot(timestamps, gpu_usage, 'g-', linewidth=2, label='GPU 利用率')\n",
    "        axes[1, 0].plot(timestamps, gpu_memory, 'orange', linewidth=2, label='GPU 記憶體')\n",
    "        axes[1, 0].set_title('GPU 資源')\n",
    "        axes[1, 0].set_ylabel('使用率 (%)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 模型數量\n",
    "        model_counts = [m['model_count'] for m in self.metrics_history]\n",
    "        axes[1, 1].plot(timestamps, model_counts, 'm-', linewidth=2)\n",
    "        axes[1, 1].set_title('載入模型數量')\n",
    "        axes[1, 1].set_ylabel('模型數量')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"📊 性能圖表已儲存: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# 創建性能監控器\n",
    "monitor = PerformanceMonitor()\n",
    "print(\"📊 性能監控器已創建\")\n",
    "\n",
    "# 準備測試數據\n",
    "def generate_test_data(num_samples: int = 50) -> List[np.ndarray]:\n",
    "    \"\"\"生成測試數據\"\"\"\n",
    "    test_data = []\n",
    "    \n",
    "    # 不同長度的序列\n",
    "    lengths = [16, 32, 64, 128, 256]\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        length = np.random.choice(lengths)\n",
    "        # 生成隨機序列，避免全零\n",
    "        sequence = np.random.randint(1, 5000, size=(1, length), dtype=np.int64)\n",
    "        test_data.append(sequence)\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "test_data = generate_test_data()\n",
    "print(f\"📋 生成了 {len(test_data)} 個測試樣本\")\n",
    "print(f\"📏 序列長度範圍: {[data.shape[1] for data in test_data[:5]]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Triton Server 啟動與模型載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 Docker 啟動腳本\n",
    "docker_script = f'''\n",
    "#!/bin/bash\n",
    "\n",
    "# Triton Server 高級配置啟動腳本\n",
    "\n",
    "MODEL_REPO=\"{MODEL_REPO.absolute()}\"\n",
    "CONTAINER_NAME=\"triton-advanced-pytorch\"\n",
    "\n",
    "echo \"🚀 啟動 Triton Server (PyTorch Backend 進階)\"\n",
    "echo \"📁 模型倉庫: $MODEL_REPO\"\n",
    "\n",
    "# 停止現有容器\n",
    "docker stop $CONTAINER_NAME 2>/dev/null || true\n",
    "docker rm $CONTAINER_NAME 2>/dev/null || true\n",
    "\n",
    "# 啟動 Triton Server\n",
    "docker run -d \\\n",
    "  --name $CONTAINER_NAME \\\n",
    "  --gpus all \\\n",
    "  -p 8000:8000 \\\n",
    "  -p 8001:8001 \\\n",
    "  -p 8002:8002 \\\n",
    "  -v \"$MODEL_REPO:/models\" \\\n",
    "  -e CUDA_VISIBLE_DEVICES=0 \\\n",
    "  nvcr.io/nvidia/tritonserver:24.01-py3 \\\n",
    "  tritonserver \\\n",
    "    --model-repository=/models \\\n",
    "    --backend-directory=/opt/tritonserver/backends \\\n",
    "    --model-control-mode=explicit \\\n",
    "    --strict-model-config=false \\\n",
    "    --log-verbose=1 \\\n",
    "    --log-info=true \\\n",
    "    --log-warning=true \\\n",
    "    --log-error=true \\\n",
    "    --exit-on-error=false \\\n",
    "    --exit-timeout-secs=120 \\\n",
    "    --buffer-manager-thread-count=2 \\\n",
    "    --model-load-thread-count=4\n",
    "\n",
    "echo \"⏳ 等待服務器啟動...\"\n",
    "sleep 10\n",
    "\n",
    "# 檢查服務器狀態\n",
    "if curl -s http://localhost:8000/v2/health/ready > /dev/null; then\n",
    "    echo \"✅ Triton Server 已就緒\"\n",
    "    echo \"🌐 HTTP: http://localhost:8000\"\n",
    "    echo \"📊 指標: http://localhost:8002/metrics\"\n",
    "    \n",
    "    # 載入高級模型\n",
    "    echo \"📥 載入高級 PyTorch 模型...\"\n",
    "    curl -X POST http://localhost:8000/v2/repository/models/text_classifier_advanced/load\n",
    "    \n",
    "    echo \"📋 檢查模型狀態:\"\n",
    "    curl -s http://localhost:8000/v2/models/text_classifier_advanced\n",
    "else\n",
    "    echo \"❌ Triton Server 啟動失敗\"\n",
    "    docker logs $CONTAINER_NAME\n",
    "    exit 1\n",
    "fi\n",
    "'''\n",
    "\n",
    "script_path = SCRIPTS_DIR / \"start_triton_advanced.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(docker_script.strip())\n",
    "\n",
    "# 設定執行權限\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"📜 Docker 啟動腳本已創建: {script_path}\")\n",
    "print(\"\\n🚀 要啟動 Triton Server，請執行:\")\n",
    "print(f\"   bash {script_path}\")\n",
    "print(\"\\n⚠️  注意: 確保 Docker 和 NVIDIA Container Toolkit 已安裝\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能測試與基準評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_server(max_wait_time: int = 60) -> bool:\n",
    "    \"\"\"等待服務器就緒\"\"\"\n",
    "    print(\"⏳ 等待 Triton Server 就緒...\")\n",
    "    \n",
    "    for i in range(max_wait_time):\n",
    "        try:\n",
    "            client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "            if client.is_server_ready():\n",
    "                print(\"✅ Triton Server 已就緒\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  等待中... ({i}/{max_wait_time}s)\")\n",
    "    \n",
    "    print(\"❌ 服務器啟動超時\")\n",
    "    return False\n",
    "\n",
    "def run_comprehensive_benchmark():\n",
    "    \"\"\"執行綜合基準測試\"\"\"\n",
    "    \n",
    "    if not wait_for_server():\n",
    "        print(\"無法連接到 Triton Server，請確保服務器已啟動\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n🎯 開始綜合性能基準測試\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 測試配置\n",
    "    test_configs = [\n",
    "        {\"name\": \"小批次低延遲\", \"concurrent\": 1, \"iterations\": 50},\n",
    "        {\"name\": \"中批次平衡\", \"concurrent\": 4, \"iterations\": 100},\n",
    "        {\"name\": \"大批次高吞吐\", \"concurrent\": 8, \"iterations\": 200}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in test_configs:\n",
    "        print(f\"\\n📊 測試: {config['name']}\")\n",
    "        print(f\"   並發: {config['concurrent']}, 迭代: {config['iterations']}\")\n",
    "        \n",
    "        try:\n",
    "            result = monitor.benchmark_model(\n",
    "                \"text_classifier_advanced\",\n",
    "                test_data,\n",
    "                concurrent_requests=config['concurrent'],\n",
    "                iterations=config['iterations']\n",
    "            )\n",
    "            results[config['name']] = result\n",
    "            \n",
    "            # 顯示結果\n",
    "            if 'latency_stats' in result:\n",
    "                stats = result['latency_stats']\n",
    "                print(f\"   ✅ 吞吐量: {result['throughput_rps']:.2f} RPS\")\n",
    "                print(f\"   ⚡ 平均延遲: {stats['mean_ms']:.2f}ms\")\n",
    "                print(f\"   📈 P95 延遲: {stats['p95_ms']:.2f}ms\")\n",
    "                print(f\"   ❌ 錯誤率: {result['error_rate']:.2f}%\")\n",
    "            else:\n",
    "                print(f\"   ❌ 測試失敗，錯誤率: {result['error_rate']:.2f}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 測試失敗: {e}\")\n",
    "            results[config['name']] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 執行性能測試（如果服務器可用）\n",
    "try:\n",
    "    client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "    if client.is_server_ready():\n",
    "        print(\"🎯 檢測到 Triton Server，開始性能測試\")\n",
    "        benchmark_results = run_comprehensive_benchmark()\n",
    "    else:\n",
    "        print(\"⚠️  Triton Server 未就緒，跳過性能測試\")\n",
    "        print(\"   請先啟動服務器後再運行此部分\")\n",
    "        benchmark_results = None\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  無法連接到 Triton Server: {e}\")\n",
    "    print(\"   請確保服務器已啟動並可訪問\")\n",
    "    benchmark_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 記憶體使用分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage():\n",
    "    \"\"\"分析記憶體使用情況\"\"\"\n",
    "    \n",
    "    print(\"🧠 記憶體使用分析\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 獲取記憶體統計\n",
    "    stats = memory_manager.get_memory_stats()\n",
    "    \n",
    "    # 系統記憶體\n",
    "    if 'system' in stats:\n",
    "        sys_stats = stats['system']\n",
    "        print(f\"📊 系統記憶體:\")\n",
    "        print(f\"   RSS: {sys_stats['rss_mb']:.2f} MB\")\n",
    "        print(f\"   VMS: {sys_stats['vms_mb']:.2f} MB\")\n",
    "    \n",
    "    # GPU 記憶體\n",
    "    if 'gpu' in stats:\n",
    "        gpu_stats = stats['gpu']\n",
    "        print(f\"\\n🎮 GPU 記憶體:\")\n",
    "        print(f\"   已分配: {gpu_stats['allocated_mb']:.2f} MB\")\n",
    "        print(f\"   已保留: {gpu_stats['reserved_mb']:.2f} MB\")\n",
    "        print(f\"   峰值: {gpu_stats['max_allocated_mb']:.2f} MB\")\n",
    "        \n",
    "        # 計算利用率\n",
    "        if torch.cuda.is_available():\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "            utilization = gpu_stats['allocated_mb'] / total_memory * 100\n",
    "            print(f\"   利用率: {utilization:.1f}%\")\n",
    "    \n",
    "    # 記憶體池統計\n",
    "    if 'pools' in stats and stats['pools']:\n",
    "        print(f\"\\n🏊 記憶體池:\")\n",
    "        for pool_name, pool_stats in stats['pools'].items():\n",
    "            print(f\"   {pool_name}:\")\n",
    "            print(f\"     大小: {pool_stats['size_mb']:.2f} MB\")\n",
    "            print(f\"     已用: {pool_stats['allocated_mb']:.2f} MB\")\n",
    "            print(f\"     峰值: {pool_stats['peak_mb']:.2f} MB\")\n",
    "            print(f\"     利用率: {pool_stats['utilization']:.1f}%\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def create_memory_visualization(stats: Dict[str, Any]):\n",
    "    \"\"\"創建記憶體使用視覺化\"\"\"\n",
    "    \n",
    "    if 'pools' not in stats or not stats['pools']:\n",
    "        print(\"⚠️  沒有記憶體池數據可視覺化\")\n",
    "        return\n",
    "    \n",
    "    # 準備數據\n",
    "    pool_names = list(stats['pools'].keys())\n",
    "    pool_sizes = [stats['pools'][name]['size_mb'] for name in pool_names]\n",
    "    pool_used = [stats['pools'][name]['allocated_mb'] for name in pool_names]\n",
    "    pool_utilization = [stats['pools'][name]['utilization'] for name in pool_names]\n",
    "    \n",
    "    # 創建圖表\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # 記憶體池大小對比\n",
    "    axes[0].bar(pool_names, pool_sizes, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_title('記憶體池大小', fontsize=14)\n",
    "    axes[0].set_ylabel('大小 (MB)')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 記憶體池使用情況\n",
    "    x = np.arange(len(pool_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1].bar(x - width/2, pool_sizes, width, label='總容量', color='lightgray', alpha=0.7)\n",
    "    axes[1].bar(x + width/2, pool_used, width, label='已使用', color='orange', alpha=0.7)\n",
    "    axes[1].set_title('記憶體使用對比', fontsize=14)\n",
    "    axes[1].set_ylabel('記憶體 (MB)')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(pool_names, rotation=45)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 利用率圓餅圖\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "    axes[2].pie(pool_utilization, labels=pool_names, colors=colors[:len(pool_names)], \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "    axes[2].set_title('記憶體池利用率', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存圖表\n",
    "    chart_path = BASE_DIR / \"memory_analysis.png\"\n",
    "    plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"📊 記憶體分析圖表已儲存: {chart_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 執行記憶體分析\n",
    "print(\"🔍 執行記憶體使用分析...\")\n",
    "memory_stats = analyze_memory_usage()\n",
    "\n",
    "# 創建視覺化（如果有數據）\n",
    "if memory_stats and 'pools' in memory_stats:\n",
    "    create_memory_visualization(memory_stats)\n",
    "else:\n",
    "    print(\"📊 記憶體池數據不足，跳過視覺化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 自定義運算子整合示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建自定義運算子模型\n",
    "custom_model_dir = MODEL_REPO / \"custom_ops_model\" / \"1\"\n",
    "custom_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "custom_ops_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "class CustomAttentionLayer(nn.Module):\n",
    "    \"\"\"自定義注意力層，展示自定義運算子整合\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 256, n_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model 必須能被 n_heads 整除\"\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len = query.shape[1]\n",
    "        \n",
    "        # 線性變換\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key) \n",
    "        V = self.v_linear(value)\n",
    "        \n",
    "        # 重塑為多頭\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 自定義注意力計算（優化版本）\n",
    "        attention = self._custom_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 合併多頭\n",
    "        attention = attention.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # 輸出投影\n",
    "        output = self.out_linear(attention)\n",
    "        return output\n",
    "    \n",
    "    def _custom_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"自定義注意力計算，展示自定義運算子\"\"\"\n",
    "        \n",
    "        # 注意力分數計算\n",
    "        if self.scale.device != Q.device:\n",
    "            self.scale = self.scale.to(Q.device)\n",
    "            \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # 應用掩碼\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, seq_len]\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax + Dropout\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 應用權重\n",
    "        attention = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attention\n",
    "\n",
    "class CustomOpsModel(nn.Module):\n",
    "    \"\"\"包含自定義運算子的模型\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000, d_model=256, n_heads=8, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))\n",
    "        \n",
    "        # 多層自定義注意力\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            CustomAttentionLayer(d_model, n_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        \n",
    "        self.output_norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 5)  # 5 類分類\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        seq_len = input_ids.shape[1]\n",
    "        \n",
    "        # 嵌入 + 位置編碼\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # 創建注意力掩碼\n",
    "        attention_mask = (input_ids != 0).float()\n",
    "        \n",
    "        # 多層 Transformer\n",
    "        for attention_layer, layer_norm in zip(self.attention_layers, self.layer_norms):\n",
    "            # 自注意力 + 殘差連接\n",
    "            attn_output = attention_layer(x, x, x, attention_mask)\n",
    "            x = layer_norm(x + attn_output)\n",
    "            \n",
    "            # FFN + 殘差連接\n",
    "            ffn_output = self.ffn(x)\n",
    "            x = layer_norm(x + ffn_output)\n",
    "        \n",
    "        # 最終標準化\n",
    "        x = self.output_norm(x)\n",
    "        \n",
    "        # 全局平均池化\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand_as(x)\n",
    "        sum_embeddings = torch.sum(x * mask_expanded, dim=1)\n",
    "        sum_mask = torch.sum(mask_expanded, dim=1)\n",
    "        pooled = sum_embeddings / (sum_mask + 1e-9)\n",
    "        \n",
    "        # 分類\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"Triton 自定義運算子模型\"\"\"\n",
    "    \n",
    "    def initialize(self, args):\n",
    "        self.model_config = json.loads(args[\\'model_config\\'])\n",
    "        \n",
    "        # 設備設定\n",
    "        device_id = args.get(\\'model_instance_device_id\\', 0)\n",
    "        self.device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 初始化自定義模型\n",
    "        self.model = CustomOpsModel()\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 效能優化\n",
    "        if torch.cuda.is_available():\n",
    "            # 啟用優化\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            \n",
    "        print(f\"自定義運算子模型已載入到 {self.device}\")\n",
    "        \n",
    "    def execute(self, requests):\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            try:\n",
    "                # 獲取輸入\n",
    "                input_tensor = pb_utils.get_input_tensor_by_name(request, \"input_ids\")\n",
    "                input_data = torch.from_numpy(input_tensor.as_numpy()).to(self.device)\n",
    "                \n",
    "                # 推理\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_data)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # 輸出\n",
    "                output_data = probs.cpu().numpy()\n",
    "                output_tensor = pb_utils.Tensor(\"output\", output_data)\n",
    "                response = pb_utils.InferenceResponse(output_tensors=[output_tensor])\n",
    "                \n",
    "            except Exception as e:\n",
    "                response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[],\n",
    "                    error=pb_utils.TritonError(f\"推理錯誤: {str(e)}\")\n",
    "                )\n",
    "            \n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def finalize(self):\n",
    "        print(\"自定義運算子模型資源已清理\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "'''\n",
    "\n",
    "# 寫入自定義運算子模型\n",
    "with open(custom_model_dir / \"model.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_ops_code)\n",
    "\n",
    "# 創建配置文件\n",
    "custom_config = '''\n",
    "name: \"custom_ops_model\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "\n",
    "input {\n",
    "  name: \"input_ids\"\n",
    "  data_type: TYPE_INT64\n",
    "  dims: [ -1 ]\n",
    "}\n",
    "\n",
    "output {\n",
    "  name: \"output\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: [ 5 ]\n",
    "}\n",
    "\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8, 16 ]\n",
    "  max_queue_delay_microseconds: 200\n",
    "}\n",
    "\n",
    "# 自定義運算子優化參數\n",
    "parameters {\n",
    "  key: \"FORCE_CPU_ONLY_INPUT_TENSORS\"\n",
    "  value: { string_value: \"no\" }\n",
    "}\n",
    "'''\n",
    "\n",
    "config_path = MODEL_REPO / \"custom_ops_model\" / \"config.pbtxt\"\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_config.strip())\n",
    "\n",
    "print(\"✅ 自定義運算子模型已創建\")\n",
    "print(f\"📁 模型路徑: {custom_model_dir}\")\n",
    "print(f\"🔧 配置文件: {config_path}\")\n",
    "print(\"\\n🎯 特性:\")\n",
    "print(\"- ✅ 自定義多頭注意力層\")\n",
    "print(\"- ✅ 優化的注意力計算\")\n",
    "print(\"- ✅ 多層 Transformer 架構\")\n",
    "print(\"- ✅ CUDA 優化設定\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 總結與最佳實踐\n",
    "\n",
    "### 🎯 本實驗重點回顧\n",
    "\n",
    "通過本實驗，我們深入探討了 Triton PyTorch Backend 的高級配置和優化技術：\n",
    "\n",
    "#### 核心技術成果\n",
    "1. **動態形狀處理**: 實現了靈活的序列長度支援\n",
    "2. **記憶體池管理**: 建立了高效的記憶體分配策略\n",
    "3. **自定義運算子**: 整合了優化的注意力機制\n",
    "4. **性能監控**: 構建了完整的監控和分析工具\n",
    "5. **進階配置**: 掌握了企業級配置最佳實踐\n",
    "\n",
    "#### 企業級部署要點\n",
    "- ✅ **CUDA Graph 優化**: 降低 GPU 內核啟動開銷\n",
    "- ✅ **動態批次調度**: 平衡延遲與吞吐量\n",
    "- ✅ **記憶體預分配**: 避免動態分配導致的性能抖動\n",
    "- ✅ **模型預熱**: 確保首次推理的穩定性能\n",
    "- ✅ **多實例部署**: 最大化 GPU 資源利用率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成最佳實踐總結報告\n",
    "def generate_best_practices_report():\n",
    "    \"\"\"生成最佳實踐報告\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        \"title\": \"PyTorch Backend 高級優化最佳實踐\",\n",
    "        \"sections\": {\n",
    "            \"配置優化\": {\n",
    "                \"動態批次\": [\n",
    "                    \"根據硬體資源設定合適的 preferred_batch_size\",\n",
    "                    \"調整 max_queue_delay_microseconds 平衡延遲\",\n",
    "                    \"使用 preserve_ordering=false 提高吞吐量\"\n",
    "                ],\n",
    "                \"CUDA 優化\": [\n",
    "                    \"啟用 CUDA Graph (graphs: true)\",\n",
    "                    \"預定義常用輸入形狀的 graph_spec\",\n",
    "                    \"設定 busy_wait_events 減少延遲\"\n",
    "                ],\n",
    "                \"實例組設定\": [\n",
    "                    \"GPU 模型使用 KIND_GPU\",\n",
    "                    \"根據 GPU 記憶體設定實例數量\",\n",
    "                    \"大模型考慮 model parallel\"\n",
    "                ]\n",
    "            },\n",
    "            \"記憶體管理\": {\n",
    "                \"預分配策略\": [\n",
    "                    \"推理前預分配記憶體池\",\n",
    "                    \"分不同大小級別管理張量\",\n",
    "                    \"定期清理未使用的張量\"\n",
    "                ],\n",
    "                \"GPU 記憶體\": [\n",
    "                    \"監控 GPU 記憶體使用率\",\n",
    "                    \"避免記憶體碎片化\",\n",
    "                    \"使用 torch.cuda.empty_cache() 適時清理\"\n",
    "                ]\n",
    "            },\n",
    "            \"性能調優\": {\n",
    "                \"PyTorch 優化\": [\n",
    "                    \"啟用 torch.backends.cudnn.benchmark\",\n",
    "                    \"使用 JIT 編譯 (torch.jit.trace)\",\n",
    "                    \"考慮 TensorFloat-32 (TF32) 優化\"\n",
    "                ],\n",
    "                \"模型優化\": [\n",
    "                    \"實施模型預熱降低冷啟動延遲\",\n",
    "                    \"使用 torch.no_grad() 降低記憶體使用\",\n",
    "                    \"考慮混合精度推理\"\n",
    "                ]\n",
    "            },\n",
    "            \"監控告警\": {\n",
    "                \"關鍵指標\": [\n",
    "                    \"推理延遲 (P50, P95, P99)\",\n",
    "                    \"吞吐量 (RPS)\",\n",
    "                    \"GPU 記憶體使用率\",\n",
    "                    \"錯誤率\"\n",
    "                ],\n",
    "                \"告警閾值\": [\n",
    "                    \"P99 延遲 > 500ms\",\n",
    "                    \"錯誤率 > 1%\",\n",
    "                    \"GPU 記憶體使用率 > 90%\",\n",
    "                    \"實例健康狀態異常\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"common_pitfalls\": {\n",
    "            \"配置陷阱\": [\n",
    "                \"忘記設定 max_batch_size=0 用於動態批次\",\n",
    "                \"實例數量過多導致記憶體不足\",\n",
    "                \"未正確配置動態形狀導致形狀不匹配\"\n",
    "            ],\n",
    "            \"性能陷阱\": [\n",
    "                \"頻繁的 CPU-GPU 數據傳輸\",\n",
    "                \"未使用批次推理\",\n",
    "                \"記憶體洩漏導致 OOM\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 輸出報告\n",
    "    print(f\"📋 {report['title']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for section_name, content in report['sections'].items():\n",
    "        print(f\"\\n🔹 {section_name}\")\n",
    "        for subsection, items in content.items():\n",
    "            print(f\"  📌 {subsection}:\")\n",
    "            for item in items:\n",
    "                print(f\"     • {item}\")\n",
    "    \n",
    "    print(f\"\\n⚠️  常見陷阱\")\n",
    "    for category, pitfalls in report['common_pitfalls'].items():\n",
    "        print(f\"  🚫 {category}:\")\n",
    "        for pitfall in pitfalls:\n",
    "            print(f\"     • {pitfall}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# 生成報告\n",
    "best_practices = generate_best_practices_report()\n",
    "\n",
    "print(\"\\n🎉 PyTorch Backend 高級配置實驗完成！\")\n",
    "print(\"\\n📚 下一步學習建議:\")\n",
    "print(\"1. 🔧 實踐 TensorRT Backend 整合 (Notebook 02)\")\n",
    "print(\"2. 🚀 探索 vLLM Backend 整合 (Notebook 03)\")\n",
    "print(\"3. 🛠️  開發自定義 Python Backend (Notebook 04)\")\n",
    "print(\"4. 📊 深入性能調優與監控\")\n",
    "print(\"5. 🏢 企業級部署實踐\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 相關資源與延伸閱讀\n",
    "\n",
    "### 官方文檔\n",
    "- [Triton PyTorch Backend](https://github.com/triton-inference-server/pytorch_backend)\n",
    "- [CUDA Graph 優化指南](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)\n",
    "- [PyTorch JIT 編譯](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)\n",
    "\n",
    "### 性能優化\n",
    "- [PyTorch 性能調優指南](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "- [NVIDIA GPU 優化最佳實踐](https://docs.nvidia.com/deeplearning/performance/index.html)\n",
    "- [Triton 性能調優](https://github.com/triton-inference-server/server/blob/main/docs/optimization.md)\n",
    "\n",
    "### 實踐案例\n",
    "- [企業級 AI 推理平台設計](https://developer.nvidia.com/blog/deploying-ai-at-scale-with-triton-inference-server/)\n",
    "- [大規模 NLP 模型部署](https://developer.nvidia.com/blog/how-to-deploy-almost-any-hugging-face-model-on-nvidia-triton-inference-server/)\n",
    "\n",
    "---\n",
    "\n",
    "**🎓 實驗完成標誌**: PyTorch Backend 高級配置與優化技術掌握 ✅"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}