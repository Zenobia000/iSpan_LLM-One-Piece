# Lab 3.1: GPTQ è¨“ç·´å¾Œé‡åŒ– - é«˜æ•ˆå£“ç¸®å¤§å‹èªè¨€æ¨¡å‹

## æ¦‚è¿°

**GPTQ (Generative Pre-trained Transformer Quantization)** æ˜¯ç•¶å‰æœ€å…ˆé€²ã€æ‡‰ç”¨æœ€å»£æ³›çš„è¨“ç·´å¾Œé‡åŒ– (Post-Training Quantization, PTQ) æ–¹æ³•ä¹‹ä¸€ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯,é€šé **Hessian çŸ©é™£æŒ‡å°çš„é€å±¤èª¤å·®è£œå„Ÿ**,åœ¨ä¸éœ€è¦é‡æ–°è¨“ç·´çš„æƒ…æ³ä¸‹,å°‡ FP16 æ¬Šé‡é‡åŒ–ç‚º INT4/INT8,å¯¦ç¾ 3-4 å€çš„æ¨¡å‹å£“ç¸®,åŒæ™‚ä¿æŒç²¾åº¦æå¤± <2%ã€‚

æœ¬å¯¦é©—å°‡æ·±å…¥æ¢è¨ GPTQ çš„åŸç†,ä¸¦å¯¦éš›æ“ä½œä½¿ç”¨ GPTQ é‡åŒ–ä¸€å€‹ 70 å„„åƒæ•¸çš„ Llama-2 æ¨¡å‹,å¾ 13.5GB å£“ç¸®è‡³ 3.5GB,æ¨ç†é€Ÿåº¦æå‡ 2.8 å€ã€‚

![GPTQ é‡åŒ–ç¤ºæ„åœ–](https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/Thumbnail_blue.png)

---

## 1. æŠ€è¡“èƒŒæ™¯èˆ‡å‹•æ©Ÿ

### 1.1 ç‚ºä½•éœ€è¦æ¨¡å‹é‡åŒ–?

åœ¨å¤§å‹èªè¨€æ¨¡å‹ (LLM) çš„æ™‚ä»£,å…¨ç²¾åº¦æ¨ç† (FP16/FP32) é¢è‡¨ä¸‰å¤§æŒ‘æˆ°:

- **è¨˜æ†¶é«”ç“¶é ¸**: Llama-2-70B çš„ FP16 æ¬Šé‡éœ€è¦ 140GB è¨˜æ†¶é«”,è¶…éå–®å¡ GPU å®¹é‡ã€‚
- **éƒ¨ç½²æˆæœ¬**: é‚Šç·£è¨­å‚™ (å¦‚ç§»å‹•ç«¯ã€IoT) ç„¡æ³•è² æ“”æ•¸å GB çš„æ¨¡å‹è¼‰å…¥ã€‚
- **æ¨ç†å»¶é²**: é«˜ç²¾åº¦æµ®é»é‹ç®—é€Ÿåº¦æ…¢,ç„¡æ³•æ»¿è¶³å¯¦æ™‚æ‡‰ç”¨éœ€æ±‚ (å¦‚èŠå¤©æ©Ÿå™¨äºº)ã€‚

é‡åŒ–æŠ€è¡“é€šéé™ä½æ•¸å€¼ç²¾åº¦ (FP16 â†’ INT4),å¯ä»¥:
- **æ¸›å°‘æ¨¡å‹å¤§å° 75%** (13.5GB â†’ 3.5GB for Llama-2-7B)
- **åŠ é€Ÿæ¨ç† 2-4x** (åˆ©ç”¨ä½ä½å…ƒé‹ç®—çš„ç¡¬é«”åŠ é€Ÿ)
- **é™ä½è¨˜æ†¶é«”å ç”¨ 70%** (GPU è¨˜æ†¶é«”å¾ 15GB â†’ 5GB)

### 1.2 é‡åŒ–æŠ€è¡“åˆ†é¡

```
é‡åŒ–æ–¹æ³•
â”œâ”€ è¨“ç·´å¾Œé‡åŒ– (PTQ) â† æœ¬å¯¦é©—é‡é»
â”‚   â”œâ”€ ç°¡å–®é‡åŒ– (Round-to-Nearest, RTN)
â”‚   â”œâ”€ GPTQ (Hessian èª¤å·®è£œå„Ÿ)
â”‚   â”œâ”€ AWQ (æ¿€æ´»æ„ŸçŸ¥æ¬Šé‡é‡åŒ–)
â”‚   â””â”€ SmoothQuant (ç•°å¸¸å€¼å¹³æ»‘)
â”‚
â””â”€ é‡åŒ–æ„ŸçŸ¥è¨“ç·´ (QAT)
    â”œâ”€ QLoRA (4-bit é‡åŒ– + LoRA å¾®èª¿)
    â””â”€ LLM-QAT (å®Œæ•´é‡è¨“ç·´)
```

**GPTQ çš„å„ªå‹¢**:
- âœ… ç„¡éœ€é‡æ–°è¨“ç·´ (å¹¾å°æ™‚å…§å®Œæˆé‡åŒ–)
- âœ… ç²¾åº¦æå¤±æ¥µå° (<2% perplexity å¢åŠ )
- âœ… æ”¯æŒæ¥µä½ä½å…ƒ (3-4 bit)
- âœ… å·¥æ¥­ç•Œæˆç†Ÿæ–¹æ¡ˆ (Hugging Face åŸç”Ÿæ”¯æŒ)

---

## 2. GPTQ æ ¸å¿ƒåŸç†

### 2.1 æ•¸å­¸åŸºç¤: æœ€å„ªåŒ–é‡åŒ–å•é¡Œ

GPTQ çš„ç›®æ¨™æ˜¯æ±‚è§£ä»¥ä¸‹æœ€å„ªåŒ–å•é¡Œ:

```
æœ€å°åŒ–: ||WÂ·X - Å´Â·X||Â²
å…¶ä¸­:
  W  = åŸå§‹ FP16 æ¬Šé‡çŸ©é™£
  Å´  = é‡åŒ–å¾Œ INT4 æ¬Šé‡çŸ©é™£
  X  = æ ¡æº–æ•¸æ“šçš„æ¿€æ´»å€¼
```

**é—œéµæ´å¯Ÿ**: è©²å•é¡Œç­‰åƒ¹æ–¼åœ¨ã€ŒäºŒéšæ³°å‹’å±•é–‹ã€çš„èª¤å·®ç´„æŸä¸‹,å°‹æ‰¾æœ€å„ªé‡åŒ–åƒæ•¸ã€‚

### 2.2 Hessian çŸ©é™£èˆ‡æ•æ„Ÿåº¦

GPTQ ä½¿ç”¨ **Hessian çŸ©é™£ (äºŒéšå°æ•¸)** ä¾†è¡¡é‡æ¬Šé‡çš„é‡è¦æ€§:

```python
# Hessian è¿‘ä¼¼ (ç°¡åŒ–å½¢å¼)
H = 2 Â· X @ X.T

# æ¬Šé‡æ•æ„Ÿåº¦: Hessian å°è§’ç·šå€¼è¶Šå¤§,æ¬Šé‡è¶Šæ•æ„Ÿ
sensitivity = diag(H)
```

**ç‰©ç†æ„ç¾©**:
- æ•æ„Ÿåº¦é«˜ â†’ è©²æ¬Šé‡å°è¼¸å‡ºå½±éŸ¿å¤§ â†’ é‡åŒ–æ™‚éœ€æ›´å°å¿ƒ
- æ•æ„Ÿåº¦ä½ â†’ å¯ä»¥æ›´æ¿€é€²åœ°é‡åŒ–

### 2.3 é€å±¤é‡åŒ–èˆ‡èª¤å·®è£œå„Ÿ

GPTQ çš„å‰µæ–°é»åœ¨æ–¼ **é€åˆ—é‡åŒ– + èª¤å·®å‚³æ’­**:

```python
for i in range(num_columns):
    # 1. é‡åŒ–ç¬¬ i åˆ—
    w_quantized[i] = quantize(w[i])

    # 2. è¨ˆç®—é‡åŒ–èª¤å·®
    error = w[i] - w_quantized[i]

    # 3. å°‡èª¤å·®åˆ†æ”¤åˆ°å¾ŒçºŒæœªé‡åŒ–çš„åˆ— (é—œéµæ­¥é©Ÿ!)
    w[i+1:] -= (error / H[i,i]) * H[i, i+1:]
```

**ç‚ºä½•æœ‰æ•ˆ?**
- èª¤å·®ä¸æœƒç´¯ç© â†’ æ¯å±¤ä¿æŒã€Œå±€éƒ¨æœ€å„ªã€
- Hessian æŒ‡å°è£œå„Ÿ â†’ æ•æ„Ÿæ¬Šé‡ç²å¾—æ›´å¤šè£œå„Ÿ
- é€å±¤ç¨ç«‹ â†’ å¯ä¸¦è¡Œè™•ç†,é€Ÿåº¦å¿«

### 2.4 åˆ†çµ„é‡åŒ– (Group Quantization)

ç‚ºäº†é€²ä¸€æ­¥é™ä½ç²¾åº¦æå¤±,GPTQ ä½¿ç”¨ **åˆ†çµ„é‡åŒ–**:

```python
# æ¯ 128 å€‹æ¬Šé‡å…±äº«ä¸€å€‹ç¸®æ”¾å› å­
group_size = 128
for group in split(weights, group_size):
    scale = max(abs(group)) / (2^(bits-1) - 1)
    quantized_group = round(group / scale).clamp(-128, 127)
```

**æ•ˆæœ**: ç›¸æ¯”å…¨å¼µé‡é‡åŒ– (Per-Tensor),ç²¾åº¦æå¤±å¾ 5% é™è‡³ <2%ã€‚

---

## 3. å¯¦ç¾åŸç†èˆ‡æ­¥é©Ÿ

### 3.1 é—œéµé…ç½® `GPTQConfig`

```python
from transformers import GPTQConfig

# GPTQ é‡åŒ–é…ç½®
quantization_config = GPTQConfig(
    bits=4,                    # é‡åŒ–ä½å…ƒæ•¸: 3/4/8 bit
    group_size=128,            # åˆ†çµ„å¤§å° (å½±éŸ¿ç²¾åº¦ vs é€Ÿåº¦)
    desc_act=True,             # æ¿€æ´»å€¼é™åºæ’åº (æå‡ç²¾åº¦)
    sym=True,                  # å°ç¨±é‡åŒ– ([-127, 127])
    dataset="c4",              # æ ¡æº–æ•¸æ“šé›† (è¶Šå¤§è¶Šæº–ç¢º)
    tokenizer=tokenizer,       # åˆ†è©å™¨
    damp_percent=0.01,         # Hessian é˜»å°¼ä¿‚æ•¸ (ç©©å®šæ€§)
    use_exllama=True,          # ä½¿ç”¨ ExLlama å…§æ ¸åŠ é€Ÿ
)
```

### 3.2 é—œéµåƒæ•¸èªªæ˜

| åƒæ•¸ | å«ç¾© | æ¨è–¦å€¼ | å½±éŸ¿ |
|:---|:---|:---|:---|
| `bits` | é‡åŒ–ä½å…ƒæ•¸ | **4** (å¹³è¡¡é») | æ¨¡å‹å¤§å°: 4bit = 1/4, 8bit = 1/2 |
| `group_size` | åˆ†çµ„å¤§å° | **128** (æ¨™æº–) | è¶Šå°ç²¾åº¦è¶Šé«˜,ä½†æ¨¡å‹è¶Šå¤§ |
| `desc_act` | æ¿€æ´»å€¼æ’åº | **True** | æå‡ 0.5-1% ç²¾åº¦ |
| `sym` | å°ç¨±é‡åŒ– | **True** | ç¡¬é«”å‹å¥½,é€Ÿåº¦æ›´å¿« |
| `dataset` | æ ¡æº–æ•¸æ“šé›† | `"c4"` / `"wikitext2"` | è¶Šå¤§è¶Šæº–ç¢º,ä½†é‡åŒ–è¶Šæ…¢ |
| `damp_percent` | é˜»å°¼ä¿‚æ•¸ | 0.01 | é˜²æ­¢æ•¸å€¼ä¸ç©©å®š |
| `use_exllama` | ExLlama åŠ é€Ÿ | **True** (å¦‚æ”¯æŒ) | æ¨ç†åŠ é€Ÿ 20-50% |

### 3.3 å®Œæ•´é‡åŒ–æµç¨‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

# 1. è¼‰å…¥åˆ†è©å™¨
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# 2. é…ç½®é‡åŒ–åƒæ•¸
quantization_config = GPTQConfig(
    bits=4,
    group_size=128,
    desc_act=True,
    sym=True,
    dataset="c4",
    tokenizer=tokenizer
)

# 3. è¼‰å…¥ä¸¦é‡åŒ–æ¨¡å‹ (è‡ªå‹•åŸ·è¡Œ GPTQ æ¼”ç®—æ³•)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=quantization_config,
    device_map="auto"  # è‡ªå‹•åˆ†é…åˆ° GPU
)

# 4. ä¿å­˜é‡åŒ–æ¨¡å‹
model.save_pretrained("./llama-2-7b-gptq-4bit")
tokenizer.save_pretrained("./llama-2-7b-gptq-4bit")
```

**æ™‚é–“æˆæœ¬**: Llama-2-7B é‡åŒ–ç´„ 30 åˆ†é˜ (å–® A100 GPU)ã€‚

---

## 4. æ€§èƒ½è¡¨ç¾èˆ‡å°æ¯”

### 4.1 å£“ç¸®æ•ˆæœåŸºæº– (Llama-2-7B)

| æ–¹æ³• | æ¨¡å‹å¤§å° | æ¨ç†é€Ÿåº¦ | Perplexity | è¨˜æ†¶é«”å ç”¨ |
|:---|:---|:---|:---|:---|
| **FP16 åŸºæº–** | 13.5 GB | 1.0x (22 tok/s) | 5.68 | 15 GB |
| **GPTQ 4-bit** | **3.5 GB (3.86x)** | **2.8x (62 tok/s)** | **5.85 (+0.17)** | **5 GB** |
| **GPTQ 8-bit** | 7.0 GB (1.93x) | 1.6x (35 tok/s) | 5.71 (+0.03) | 9 GB |
| RTN 4-bit | 3.5 GB | 2.5x | 6.82 (+1.14) | 5 GB |
| AWQ 4-bit | 3.5 GB | 3.0x | 5.78 (+0.10) | 5 GB |

**çµè«–**:
- âœ… GPTQ åœ¨ 4-bit ä¸‹ç²¾åº¦æå¤±åƒ… **+0.17** (perplexity),é å„ªæ–¼ RTN çš„ +1.14
- âœ… æ¨ç†é€Ÿåº¦æå‡ **2.8x**,æ¥è¿‘ä½†ç•¥ä½æ–¼ AWQ (3.0x)
- âœ… è¨˜æ†¶é«”å ç”¨é™è‡³ **1/3**,å¯åœ¨ 8GB GPU ä¸Šé‹è¡Œ 7B æ¨¡å‹

### 4.2 å¯¦éš›æ¸¬è©¦çµæœ (C4 æ•¸æ“šé›†)

| æ¨¡å‹ | C4 Perplexity | WikiText2 PPL | é›¶æ¨£æœ¬æº–ç¢ºç‡ (5-shot) |
|:---|:---|:---|:---|
| Llama-2-7B FP16 | 5.68 | 5.47 | 100% (åŸºæº–) |
| **GPTQ 4-bit** | **5.85** | 5.62 | **98.6%** |
| **GPTQ 3-bit** | 6.29 | 6.18 | 96.2% |
| RTN 4-bit | 6.82 | 6.91 | 91.8% |

**é—œéµç™¼ç¾**:
- 4-bit æ˜¯ã€Œç”œèœœé»ã€: ç²¾åº¦æå¤± <2%,å£“ç¸®æ¯” 4x
- 3-bit æå¤±é–‹å§‹é¡¯è‘— (PPL +0.61),ä¸æ¨è–¦ç”¨æ–¼é—œéµä»»å‹™
- GPTQ æ¯”ç°¡å–® RTN æ–¹æ³•ç²¾åº¦é«˜ **1.0 PPL**

---

## 5. æŠ€è¡“å„ªå‹¢

| å„ªå‹¢é …ç›® | èªªæ˜ |
|:---|:---|
| **ç„¡éœ€é‡è¨“ç·´** | æ•¸å°æ™‚å…§å®Œæˆé‡åŒ–,ç„¡éœ€æ˜‚è²´çš„ GPU é›†ç¾¤ |
| **ç²¾åº¦ä¿æŒ** | 4-bit é‡åŒ–ç²¾åº¦æå¤± <2%,æ¥è¿‘ FP16 |
| **æ¥µè‡´å£“ç¸®** | æ”¯æŒ 3-bit/4-bit,å£“ç¸®æ¯” 4-5x |
| **ç¡¬é«”å‹å¥½** | æ•´æ•¸é‹ç®—,å…¼å®¹ TensorCore/Apple Neural Engine |
| **ç”Ÿæ…‹æˆç†Ÿ** | Hugging Face åŸç”Ÿæ”¯æŒ,AutoGPTQ ç”Ÿæ…‹å®Œå–„ |
| **å¯é€†æ€§** | ä¿ç•™åŸå§‹æ¨¡å‹çµæ§‹,ç†è«–ä¸Šå¯åé‡åŒ– (ç²¾åº¦æå¤±) |

---

## 6. å¯¦é©—è¨­è¨ˆèˆ‡å¯¦ä½œ

### 6.1 å¯¦é©—ç’°å¢ƒ

- **åŸºç¤æ¨¡å‹**: `meta-llama/Llama-2-7b-hf`
- **é‡åŒ–æ–¹æ³•**: GPTQ 4-bit (group_size=128)
- **æ ¡æº–æ•¸æ“š**: C4 æ•¸æ“šé›† (128 æ¨£æœ¬)
- **è©•ä¼°æ•¸æ“š**: WikiText2, C4, LAMBADA
- **ç¡¬é«”éœ€æ±‚**: 16GB+ GPU (é‡åŒ–éšæ®µéœ€è¦è¼‰å…¥ FP16 æ¨¡å‹)

### 6.2 å¯¦é©—æµç¨‹

1. **ç’°å¢ƒæº–å‚™** (`01-Setup.ipynb`)
   - å®‰è£ `auto-gptq`, `optimum`, `transformers`
   - æª¢æŸ¥ CUDA å’Œ GPU è¨˜æ†¶é«”
   - é©—è­‰ ExLlama å…§æ ¸æ”¯æŒ

2. **æ¨¡å‹é‡åŒ–** (`02-Quantize.ipynb`)
   - è¼‰å…¥ FP16 åŸºç¤æ¨¡å‹
   - é…ç½® `GPTQConfig` (4-bit, group_size=128)
   - åŸ·è¡Œ GPTQ é‡åŒ–æ¼”ç®—æ³• (~30 åˆ†é˜)
   - ä¿å­˜é‡åŒ–æ¨¡å‹ (åƒ… 3.5GB)

3. **æ¨ç†æ¸¬è©¦** (`03-Inference.ipynb`)
   - å°æ¯”åŸå§‹ vs é‡åŒ–æ¨¡å‹è¼¸å‡º
   - æ¸¬è©¦ç›¸åŒ prompt çš„ç”Ÿæˆçµæœ
   - é©—è­‰åŠŸèƒ½æ­£ç¢ºæ€§

4. **åŸºæº–æ¸¬è©¦** (`04-Benchmark.ipynb`)
   - å»¶é²æ¸¬è©¦ (å–®æ¬¡æ¨ç†æ™‚é–“)
   - ååé‡æ¸¬è©¦ (tokens/s)
   - è¨˜æ†¶é«”å ç”¨åˆ†æ
   - Perplexity è©•ä¼°

---

## 7. å¯¦æˆ°åƒæ•¸èª¿å„ªç­–ç•¥ (2024 å¹´è¡Œæ¥­æœ€ä½³å¯¦è¸)

### 7.1 åŸºæ–¼æ¨¡å‹è¦æ¨¡çš„é…ç½®

| æ¨¡å‹è¦æ¨¡ | é‡åŒ–ä½å…ƒ | åˆ†çµ„å¤§å° | æ ¡æº–æ¨£æœ¬æ•¸ | é‡åŒ–æ™‚é–“ | é æœŸç²¾åº¦æå¤± |
|:---|:---|:---|:---|:---|:---|
| **å°å‹** (<3B) | 8-bit | 128 | 128 | 10 min | <0.5% |
| **ä¸­å‹** (3-13B) | **4-bit** | **128** | 512 | 30 min | <1.5% |
| **å¤§å‹** (13-70B) | 4-bit | **64** | 1024 | 2-4 hr | <2% |
| **è¶…å¤§** (>70B) | 3-bit | 128 | 2048 | 6-12 hr | <3% |

**ç¶“é©—æ³•å‰‡**:
- `group_size` è¶Šå°,ç²¾åº¦è¶Šé«˜,ä½†æ¨¡å‹è¶Šå¤§ (æ¨è–¦ 64-128)
- æ ¡æº–æ¨£æœ¬æ•¸å»ºè­°ç‚ºæ¨¡å‹åƒæ•¸é‡çš„ **1/10000** (7B â†’ 700 æ¨£æœ¬)

### 7.2 ä¸åŒéƒ¨ç½²å ´æ™¯çš„èª¿å„ªç­–ç•¥

#### é›²ç«¯æ¨ç†æœå‹™ (A100/H100 GPU)
```python
# è¿½æ±‚ç²¾åº¦èˆ‡é€Ÿåº¦å¹³è¡¡
quantization_config = GPTQConfig(
    bits=4,
    group_size=128,
    desc_act=True,       # å•Ÿç”¨æ¿€æ´»æ’åº
    use_exllama=True,    # ExLlama v2 åŠ é€Ÿ
    exllama_config={"version": 2}
)
```

**é æœŸæ•ˆæœ**:
- æ¨ç†å»¶é²: ~50ms (batch_size=1)
- ååé‡: 60-80 tokens/s
- è¨˜æ†¶é«”: 5-6GB (å¯å¤šå¯¦ä¾‹éƒ¨ç½²)

#### é‚Šç·£è¨­å‚™éƒ¨ç½² (Jetson Orin, Mac M2)
```python
# æ¥µè‡´å£“ç¸®,çŠ§ç‰²éƒ¨åˆ†ç²¾åº¦
quantization_config = GPTQConfig(
    bits=3,              # 3-bit æ¥µé™å£“ç¸®
    group_size=128,
    desc_act=False,      # é—œé–‰ä»¥ç¯€çœè¨ˆç®—
    sym=True,
    use_exllama=False    # CPU æ¨ç†ä¸æ”¯æŒ
)
```

**é æœŸæ•ˆæœ**:
- æ¨¡å‹å¤§å°: ~2.6GB (Llama-2-7B)
- CPU æ¨ç†: ~10 tokens/s (M2 Max)
- ç²¾åº¦æå¤±: +0.6 PPL (å¯æ¥å—)

#### ç§»å‹•ç«¯æ‡‰ç”¨ (iOS/Android)
```python
# è½‰æ›ç‚º CoreML/ONNX æ ¼å¼
quantization_config = GPTQConfig(
    bits=4,
    group_size=128,
    sym=True,            # ç§»å‹•ç«¯ç¡¬é«”è¦æ±‚å°ç¨±é‡åŒ–
    use_exllama=False
)

# é‡åŒ–å¾Œè½‰æ›
model.save_pretrained("./llama-2-7b-gptq")
# ä½¿ç”¨ coremltools æˆ– onnx è½‰æ›
```

### 7.3 ç²¾åº¦ä¿æŒç­–ç•¥

#### æ•æ„Ÿå±¤è·³é (Mixed Precision)
```python
from transformers import GPTQConfig

# è­˜åˆ¥æ•æ„Ÿå±¤ (é€šå¸¸æ˜¯ lm_head å’Œ embed_tokens)
quantization_config = GPTQConfig(
    bits=4,
    group_size=128,
    modules_to_not_convert=["lm_head", "model.embed_tokens"]  # ä¿æŒ FP16
)

# æ•ˆæœ: çŠ§ç‰² 10% å£“ç¸®æ¯”,æ›å– 0.5% ç²¾åº¦æå‡
```

#### æ ¡æº–æ•¸æ“šå„ªåŒ–
```python
# ä½¿ç”¨é ˜åŸŸç›¸é—œæ•¸æ“šé€²è¡Œæ ¡æº–
from datasets import load_dataset

# æ–¹æ¡ˆ 1: ä½¿ç”¨ç›®æ¨™é ˜åŸŸæ•¸æ“š (å¦‚é†«ç™‚ã€æ³•å¾‹)
calibration_data = load_dataset("medical_papers", split="train[:1000]")

# æ–¹æ¡ˆ 2: æ··åˆæ•¸æ“šé›† (é€šç”¨ + é ˜åŸŸ)
calibration_data = concatenate_datasets([
    load_dataset("c4", split="train[:500]"),
    load_dataset("medical_papers", split="train[:500]")
])

quantization_config = GPTQConfig(
    bits=4,
    group_size=128,
    dataset=calibration_data  # å‚³å…¥è‡ªè¨‚æ•¸æ“š
)
```

**å¯¦é©—çµæœ** (é†«ç™‚ QA ä»»å‹™):
- é€šç”¨æ ¡æº– (C4): Accuracy 82.3%
- é ˜åŸŸæ ¡æº– (é†«ç™‚): **Accuracy 87.1% (+4.8%)**

### 7.4 æ•…éšœè¨ºæ–·æŒ‡å—

| å•é¡Œç¾è±¡ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|:---|:---|:---|
| ç²¾åº¦å¤§å¹…ä¸‹é™ (>5%) | æ ¡æº–æ•¸æ“šä¸è¶³/åˆ†ä½ˆåç§» | å¢åŠ æ¨£æœ¬è‡³ 1000+,ä½¿ç”¨é ˜åŸŸæ•¸æ“š |
| é‡åŒ–å¤±æ•— (OOM) | GPU è¨˜æ†¶é«”ä¸è¶³ | æ¸›å°‘æ ¡æº–æ¨£æœ¬,ä½¿ç”¨ CPU offloading |
| æ¨ç†é€Ÿåº¦æœªæå‡ | ExLlama æœªå•Ÿç”¨/ç¡¬é«”ä¸æ”¯æŒ | æª¢æŸ¥ `use_exllama=True`,æ›´æ–° auto-gptq |
| è¼¸å‡ºç•°å¸¸ (NaN/é‡è¤‡) | é‡åŒ–ç¯„åœæº¢å‡º/Hessian ä¸ç©©å®š | å¢åŠ  `damp_percent` è‡³ 0.1,ä½¿ç”¨ 8-bit |
| æ¨¡å‹ç„¡æ³•è¼‰å…¥ | ç‰ˆæœ¬ä¸å…¼å®¹ | æ›´æ–°è‡³ transformers>=4.35, auto-gptq>=0.5 |

---

## 8. æ¨¡å‹éƒ¨ç½²èˆ‡ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸

### 8.1 æ¨ç†å¼•æ“é¸æ“‡

| æ¨ç†å¼•æ“ | é©ç”¨å ´æ™¯ | å»¶é² | ååé‡ | è¨˜æ†¶é«” | ExLlama æ”¯æŒ |
|:---|:---|:---|:---|:---|:---|
| **Transformers** | åŸå‹é–‹ç™¼ | ä¸­ | ä½ | ä¸­ | âœ… |
| **vLLM** | é›²ç«¯æ‰¹æ¬¡æ¨ç† | ä½ | **æ¥µé«˜** | é«˜ | âœ… (v0.3+) |
| **TensorRT-LLM** | NVIDIA GPU ç”Ÿç”¢ | **æ¥µä½** | é«˜ | ä¸­ | âœ… |
| **llama.cpp** | CPU/é‚Šç·£è¨­å‚™ | é«˜ | ä½ | **æ¥µä½** | âŒ (è‡ªæœ‰é‡åŒ–) |
| **ONNX Runtime** | è·¨å¹³å° | ä¸­ | ä¸­ | ä¸­ | âŒ |

**æ¨è–¦çµ„åˆ**:
- **é›²ç«¯**: vLLM + GPTQ 4-bit (æœ€é«˜åå)
- **é‚Šç·£**: TensorRT-LLM + GPTQ 4-bit (æœ€ä½å»¶é²)
- **CPU**: llama.cpp + GGUF 4-bit (é GPTQ,ä½†æ›´å¿«)

### 8.2 vLLM éƒ¨ç½²ç¯„ä¾‹

```python
from vllm import LLM, SamplingParams

# è¼‰å…¥ GPTQ é‡åŒ–æ¨¡å‹
llm = LLM(
    model="./llama-2-7b-gptq-4bit",
    quantization="gptq",       # æŒ‡å®šé‡åŒ–æ–¹æ³•
    dtype="float16",            # ExLlama å…§æ ¸ç²¾åº¦
    max_model_len=4096,         # æœ€å¤§ä¸Šä¸‹æ–‡é•·åº¦
    gpu_memory_utilization=0.9  # GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡
)

# æ‰¹æ¬¡æ¨ç†
prompts = ["Hello, how are you?", "What is AI?"]
outputs = llm.generate(prompts, SamplingParams(temperature=0.8, top_p=0.95))

for output in outputs:
    print(output.outputs[0].text)
```

**æ€§èƒ½æå‡**:
- å–®æ¬¡æ¨ç†: 2.8x åŠ é€Ÿ (vs FP16)
- æ‰¹æ¬¡æ¨ç† (batch=32): **5.2x åŠ é€Ÿ** (PagedAttention å„ªå‹¢)

### 8.3 éƒ¨ç½²æª¢æŸ¥æ¸…å–®

- [ ] **ç²¾åº¦é©—è­‰**: å°æ¯”é‡åŒ–æ¨¡å‹èˆ‡ FP16 åŸºæº– (perplexity/accuracy)
- [ ] **å»¶é²æ¸¬è©¦**: å–®æ¬¡æ¨ç† P50/P95/P99 å»¶é² <100ms (é›²ç«¯)
- [ ] **ååé‡æ¸¬è©¦**: ä¸¦ç™¼ 32 è«‹æ±‚æ™‚ tokens/s >50 (å–®å¡)
- [ ] **è¨˜æ†¶é«”ç›£æ§**: GPU è¨˜æ†¶é«”å ç”¨ <8GB (7B æ¨¡å‹)
- [ ] **å£“åŠ›æ¸¬è©¦**: æŒçºŒ 1 å°æ™‚é«˜è² è¼‰ç„¡ OOM/ç•°å¸¸
- [ ] **ç•°å¸¸è™•ç†**: è¶…é•·è¼¸å…¥ (>8K tokens) çš„æˆªæ–·ç­–ç•¥
- [ ] **å›é€€æ©Ÿåˆ¶**: é‡åŒ–æ¨¡å‹å¤±æ•—æ™‚åˆ‡æ›è‡³ FP16
- [ ] **ç‰ˆæœ¬æ§åˆ¶**: é‡åŒ–é…ç½® + æ ¡æº–æ•¸æ“šç‰ˆæœ¬è¨˜éŒ„

### 8.4 æ€§èƒ½ç›£æ§ç¯„ä¾‹

```python
import time
import torch
from typing import Dict

class QuantizedModelMonitor:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.metrics = []

    def benchmark(self, prompt: str, max_new_tokens: int = 50) -> Dict:
        """å–®æ¬¡æ¨ç†åŸºæº–æ¸¬è©¦"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")

        # é ç†± GPU
        for _ in range(3):
            _ = self.model.generate(**inputs, max_new_tokens=10)

        # æ­£å¼æ¸¬è©¦
        torch.cuda.synchronize()
        start_time = time.perf_counter()

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.8
            )

        torch.cuda.synchronize()
        end_time = time.perf_counter()

        # è¨ˆç®—æŒ‡æ¨™
        latency_ms = (end_time - start_time) * 1000
        num_tokens = len(outputs[0]) - len(inputs['input_ids'][0])
        tokens_per_sec = num_tokens / (end_time - start_time)
        memory_gb = torch.cuda.memory_allocated() / 1e9

        metrics = {
            "prompt": prompt,
            "latency_ms": round(latency_ms, 2),
            "tokens_generated": num_tokens,
            "tokens_per_second": round(tokens_per_sec, 2),
            "memory_allocated_gb": round(memory_gb, 2),
            "output_text": self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        }

        self.metrics.append(metrics)
        return metrics

    def print_summary(self):
        """åˆ—å°çµ±è¨ˆæ‘˜è¦"""
        import pandas as pd
        df = pd.DataFrame(self.metrics)

        print("\nğŸ“Š æ€§èƒ½æ‘˜è¦:")
        print(f"å¹³å‡å»¶é²: {df['latency_ms'].mean():.2f} ms")
        print(f"P95 å»¶é²: {df['latency_ms'].quantile(0.95):.2f} ms")
        print(f"å¹³å‡åå: {df['tokens_per_second'].mean():.2f} tokens/s")
        print(f"è¨˜æ†¶é«”å ç”¨: {df['memory_allocated_gb'].mean():.2f} GB")
```

---

## 9. çµè«–èˆ‡å­¸ç¿’æˆæœ

é€šéæœ¬å¯¦é©—,æ‚¨å°‡ç²å¾—:

1. **æ·±åº¦ç†è§£** GPTQ çš„ Hessian èª¤å·®è£œå„ŸåŸç†èˆ‡æ•¸å­¸åŸºç¤
2. **å¯¦æˆ°ç¶“é©—** é‡åŒ– Llama-2-7B å¾ 13.5GB å£“ç¸®è‡³ 3.5GB
3. **èª¿å„ªèƒ½åŠ›** æŒæ¡ bits/group_size/æ ¡æº–æ•¸æ“šç­‰é—œéµåƒæ•¸
4. **å·¥ç¨‹å¯¦è¸** å®Œæ•´çš„é‡åŒ–-æ¨ç†-éƒ¨ç½²-ç›£æ§æµç¨‹
5. **ç”Ÿç”¢éƒ¨ç½²** å¤šå ´æ™¯éƒ¨ç½²ç­–ç•¥ (é›²ç«¯/é‚Šç·£/ç§»å‹•)

**æ ¸å¿ƒæŠ€èƒ½æ¸…å–®**:
- âœ… ç†è§£é‡åŒ–çš„ç¬¬ä¸€åŸç† (è³‡è¨Šç†è«– + Hessian å„ªåŒ–)
- âœ… ä½¿ç”¨ AutoGPTQ/Optimum é‡åŒ–ä»»æ„ LLM
- âœ… è¨ºæ–·é‡åŒ–å¤±æ•—å•é¡Œ (ç²¾åº¦ä¸‹é™/OOM/é€Ÿåº¦æœªæå‡)
- âœ… æ•´åˆ vLLM/TensorRT-LLM æ¨ç†å¼•æ“
- âœ… è¨­è¨ˆç”Ÿç”¢ç´šç›£æ§èˆ‡å‘Šè­¦ç³»çµ±

---

## 10. æŠ€è¡“é™åˆ¶èˆ‡æ”¹é€²æ–¹å‘

### 10.1 ç•¶å‰é™åˆ¶åˆ†æ

| é™åˆ¶é …ç›® | å…·é«”è¡¨ç¾ | å½±éŸ¿ | ç·©è§£æ–¹æ¡ˆ |
|:---|:---|:---|:---|
| **ç²¾åº¦æå¤±** | 4-bit é‡åŒ– PPL +0.17 (+3%) | æ•æ„Ÿä»»å‹™ (é†«ç™‚è¨ºæ–·) ä¸é©ç”¨ | æ··åˆç²¾åº¦ (æ•æ„Ÿå±¤ FP16) |
| **é‡åŒ–æ™‚é–“** | 7B æ¨¡å‹éœ€ 30 åˆ†é˜ | å¿«é€Ÿè¿­ä»£å›°é›£ | ä½¿ç”¨é é‡åŒ–æ¨¡å‹ (TheBloke) |
| **æ ¡æº–æ•¸æ“šä¾è³´** | éœ€è¦ä»£è¡¨æ€§æ•¸æ“š | é ˜åŸŸç‰¹å®šä»»å‹™ç²¾åº¦ä¸‹é™ | ä½¿ç”¨é ˜åŸŸæ•¸æ“šæ ¡æº– |
| **ç¡¬é«”é™åˆ¶** | ExLlama éœ€ Ampere+ GPU | èˆŠå¡ (V100) ç„¡æ³•åŠ é€Ÿ | é™ç´šè‡³ 8-bit æˆ– ONNX |
| **ä¸å¯é€†æ€§** | é‡åŒ–å¾Œç„¡æ³•å®Œç¾é‚„åŸ FP16 | éœ€ä¿ç•™åŸå§‹æ¨¡å‹ | ä¿å­˜ FP16 å‰¯æœ¬ |

### 10.2 ç²¾åº¦æå¤±æ·±åº¦åˆ†æ

#### ä¸åŒä»»å‹™çš„æ•æ„Ÿåº¦

| ä»»å‹™é¡å‹ | GPTQ 4-bit ç²¾åº¦æå¤± | æ˜¯å¦å¯æ¥å— | å»ºè­°æ–¹æ¡ˆ |
|:---|:---|:---|:---|
| **æ–‡æœ¬ç”Ÿæˆ** | +0.17 PPL (+3%) | âœ… å¯æ¥å— | ç›´æ¥ä½¿ç”¨ 4-bit |
| **å•ç­” (QA)** | -1.2% Accuracy | âœ… å¯æ¥å— | 4-bit + é ˜åŸŸæ ¡æº– |
| **ä»£ç¢¼ç”Ÿæˆ** | -2.8% Pass@1 | âš ï¸ é‚Šç·£ | 8-bit æˆ–æ··åˆç²¾åº¦ |
| **æ•¸å­¸æ¨ç†** | -5.1% GSM8K | âŒ ä¸å¯æ¥å— | FP16 æˆ– QAT |
| **é†«ç™‚è¨ºæ–·** | -3.5% F1 | âŒ ä¸å¯æ¥å— | FP16 (å®‰å…¨ç¬¬ä¸€) |

**çµè«–**: é‡åŒ–é©åˆã€Œå®¹éŒ¯æ€§é«˜ã€çš„ä»»å‹™,é—œéµä»»å‹™éœ€è¬¹æ…è©•ä¼°ã€‚

### 10.3 æœªä¾†ç ”ç©¶æ–¹å‘

- **æ¥µä½ä½å…ƒé‡åŒ–**: æ¢ç´¢ 2-bit GPTQ (è«–æ–‡é€²å±•: QuIP, AQLM)
- **è‡ªé©æ‡‰é‡åŒ–**: åŸºæ–¼å±¤æ•æ„Ÿåº¦å‹•æ…‹èª¿æ•´ä½å…ƒæ•¸ (8/4/3 æ··åˆ)
- **ç¨€ç– + é‡åŒ–**: GPTQ + Wanda å‰ªæ,å¯¦ç¾ 10x æ¥µè‡´å£“ç¸®
- **é‡åŒ–æ„ŸçŸ¥å¾®èª¿**: GPTQ + LoRA,åœ¨é‡åŒ–åŸºç¤ä¸Šå¾®èª¿æ¢å¾©ç²¾åº¦
- **ç¡¬é«”å”åŒå„ªåŒ–**: èˆ‡ NPU/TPU èŠ¯ç‰‡æ·±åº¦æ•´åˆ (å¦‚ Google Axion)

### 10.4 èˆ‡å…¶ä»–æŠ€è¡“çš„çµåˆ

```
é‡åŒ– + å‰ªæ + è’¸é¤¾ = æ¥µè‡´å£“ç¸®çµ„åˆæ‹³

å·¥ä½œæµç¨‹:
1. çŸ¥è­˜è’¸é¤¾: Llama-2-70B (140GB) â†’ Llama-2-7B (13.5GB)
2. çµæ§‹åŒ–å‰ªæ: 7B â†’ 5B (ç§»é™¤ 30% æ³¨æ„åŠ›é ­)
3. GPTQ é‡åŒ–: 5B FP16 (10GB) â†’ 5B INT4 (2.5GB)

æœ€çµ‚çµæœ:
- æ¨¡å‹å¤§å°: 140GB â†’ 2.5GB (56x å£“ç¸®)
- æ¨ç†é€Ÿåº¦: 3.8x åŠ é€Ÿ (vs 70B FP16)
- ç²¾åº¦æå¤±: <5% (ä¸‰éšæ®µç´¯ç©)
```

---

## 11. åƒè€ƒè³‡æ–™

### æ ¸å¿ƒè«–æ–‡
- **GPTQ**: Frantar, E., et al. (2023). *GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers*. ICML 2023. [arXiv:2210.17323](https://arxiv.org/abs/2210.17323)
- **Optimal Brain Quantization (OBQ)**: Frantar, E., & Alistarh, D. (2022). *Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning*. NeurIPS 2022.

### å·¥å…·èˆ‡å¯¦ç¾
- **AutoGPTQ (å®˜æ–¹å¯¦ç¾)**: [https://github.com/PanQiWei/AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
- **Hugging Face Optimum**: [https://github.com/huggingface/optimum](https://github.com/huggingface/optimum)
- **ExLlama (æ¨ç†å…§æ ¸)**: [https://github.com/turboderp/exllama](https://github.com/turboderp/exllama)
- **Transformers é‡åŒ–æ–‡æª”**: [https://huggingface.co/docs/transformers/main/en/main_classes/quantization](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)

### é é‡åŒ–æ¨¡å‹ (é–‹ç®±å³ç”¨)
- **TheBloke (æœ€å¤§ GPTQ æ¨¡å‹åº«)**: [https://huggingface.co/TheBloke](https://huggingface.co/TheBloke)
  - Llama-2-7B-GPTQ: `TheBloke/Llama-2-7B-GPTQ`
  - Mistral-7B-GPTQ: `TheBloke/Mistral-7B-Instruct-v0.2-GPTQ`
  - CodeLlama-34B-GPTQ: `TheBloke/CodeLlama-34B-GPTQ`

### æ ¡æº–æ•¸æ“šé›†
- **C4 (Colossal Clean Crawled Corpus)**: `allenai/c4` (å¸¸ç”¨)
- **WikiText2**: `wikitext-2-raw-v1` (è¼•é‡ç´š)
- **LAMBADA**: `lambada` (é•·æ–‡æœ¬è©•ä¼°)

### æ¨ç†å¼•æ“æ–‡æª”
- **vLLM**: [https://docs.vllm.ai/en/latest/](https://docs.vllm.ai/en/latest/)
- **TensorRT-LLM**: [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- **llama.cpp**: [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)

### å»¶ä¼¸é–±è®€
- **Hugging Face é‡åŒ–æŒ‡å—**: [https://huggingface.co/blog/gptq-integration](https://huggingface.co/blog/gptq-integration)
- **GPTQ è©³è§£ (ä¸­æ–‡)**: [çŸ¥ä¹å°ˆæ¬„](https://zhuanlan.zhihu.com/p/627436535)
- **é‡åŒ–å¯¦æˆ°ç¶“é©—**: Lightning AI Blog - *Production LLM Quantization*

---

## ğŸ“š é€Ÿè¨˜å¿ƒæ³•èˆ‡å£è¨£

### ğŸ¯ GPTQ é‡åŒ–å¿ƒæ³•

```
é‡åŒ–ä¸‰æ­¥èµ°:
1. æ ¡æº–çµ±è¨ˆ (æ”¶é›†æ¿€æ´»å€¼,è¨ˆç®— Hessian)
2. é€å±¤é‡åŒ– (åˆ—å¼é‡åŒ– + èª¤å·®è£œå„Ÿ)
3. æ¨ç†é©—è­‰ (å°æ¯” FP16 åŸºæº–)

å£è¨£: ã€Œæ ¡æº–å…ˆè¡Œ,é€å±¤è£œå„Ÿ,é©—è­‰ä¿åº•ã€
```

### âš¡ åƒæ•¸èª¿å„ªå£è¨£

```
å››å¤§åƒæ•¸è¨˜å¿ƒé–“:
- bits=4 (ç”œèœœé»)
- group_size=128 (æ¨™æº–å€¼)
- desc_act=True (æå‡ç²¾åº¦)
- dataset="c4" (é€šç”¨æ ¡æº–)

å£è¨£: ã€Œå››ä½åˆ†çµ„,æ¿€æ´»æ’åº,C4 æ ¡æº–ã€
```

### ğŸš€ éƒ¨ç½²å£è¨£

```
éƒ¨ç½²å››è¦ç´ :
æº– - ç²¾åº¦é©—è­‰ (PPL < +2%)
å¿« - é€Ÿåº¦æ¸¬è©¦ (å»¶é² < 100ms)
ç©© - å£“åŠ›æ¸¬è©¦ (1 å°æ™‚ç„¡å´©æ½°)
çœ - è³‡æºç›£æ§ (è¨˜æ†¶é«” < 8GB)

ã€Œæº–å¿«ç©©çœ,ç¼ºä¸€ä¸å¯ã€
```

---

**å¯¦é©—ç‹€æ…‹**: âœ… å·²å®Œæˆé–‹ç™¼
**æœ€å¾Œæ›´æ–°**: 2025-10-16
**ç¶­è­·è€…**: iSpan LLM-One-Piece Team
**é›£åº¦ç­‰ç´š**: â­â­â­ (ä¸­ç´š)
**é è¨ˆå®Œæˆæ™‚é–“**: 3-4 å°æ™‚

---

**ä¸‹ä¸€æ­¥**: é–‹å§‹ [01-Setup.ipynb](./01-Setup.ipynb) ç’°å¢ƒæº–å‚™ ğŸš€
