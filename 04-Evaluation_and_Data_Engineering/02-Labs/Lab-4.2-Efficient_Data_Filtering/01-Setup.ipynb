{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 - 高效數據篩選\n",
    "## Notebook 01: 數據準備與環境配置\n",
    "\n",
    "**學習目標**:\n",
    "1. 加載 Alpaca 指令數據集 (52K 樣本)\n",
    "2. 進行統計分析 (長度、領域分布)\n",
    "3. 準備 Sentence-BERT 模型\n",
    "4. 設定篩選目標 (30% 保留率)\n",
    "5. 可視化數據分布\n",
    "\n",
    "**預計時間**: 30-45 分鐘\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境檢查\n",
    "\n",
    "首先驗證必要的依賴是否安裝正確。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"環境檢查\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Python 版本\n",
    "print(f\"Python 版本: {sys.version.split()[0]}\")\n",
    "\n",
    "# 檢查核心依賴\n",
    "dependencies = {\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn',\n",
    "    'sentence-transformers': 'sentence_transformers',\n",
    "    'scikit-learn': 'sklearn',\n",
    "    'datasets': 'datasets',\n",
    "    'tqdm': 'tqdm'\n",
    "}\n",
    "\n",
    "missing = []\n",
    "for name, module in dependencies.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"✅ {name}\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {name} (未安裝)\")\n",
    "        missing.append(name)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n⚠️  缺少依賴: {', '.join(missing)}\")\n",
    "    print(f\"安裝指令: pip install {' '.join(missing)}\")\n",
    "else:\n",
    "    print(f\"\\n✅ 所有依賴已安裝\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 導入依賴\n",
    "\n",
    "導入所需的 Python 套件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# 設定可視化風格\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 設定隨機種子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ 依賴導入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 建立目錄結構\n",
    "\n",
    "建立用於存放數據、分析結果和輸出的目錄。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立目錄結構\n",
    "directories = {\n",
    "    'data': Path('./data'),\n",
    "    'analysis': Path('./analysis'),\n",
    "    'results': Path('./results'),\n",
    "    'validation': Path('./validation'),\n",
    "    'pipeline': Path('./pipeline')\n",
    "}\n",
    "\n",
    "for name, path in directories.items():\n",
    "    path.mkdir(exist_ok=True)\n",
    "    print(f\"✅ 建立目錄: {path}\")\n",
    "\n",
    "print(f\"\\n目錄結構建立完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加載 Alpaca 數據集\n",
    "\n",
    "加載 Stanford Alpaca 數據集 (52K 樣本)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📥 加載 Alpaca 數據集...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # 從 HuggingFace 加載 Alpaca 數據集\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "    \n",
    "    # 轉換為 list of dicts\n",
    "    raw_data = []\n",
    "    for item in dataset:\n",
    "        sample = {\n",
    "            'instruction': item['instruction'],\n",
    "            'input': item.get('input', ''),\n",
    "            'output': item['output']\n",
    "        }\n",
    "        raw_data.append(sample)\n",
    "    \n",
    "    print(f\"✅ 數據集加載成功\")\n",
    "    print(f\"   總樣本數: {len(raw_data):,}\")\n",
    "    \n",
    "    # 保存原始數據\n",
    "    raw_data_path = directories['data'] / 'alpaca_raw.json'\n",
    "    with open(raw_data_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(raw_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"   已保存至: {raw_data_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 錯誤: {e}\")\n",
    "    print(\"\\n💡 提示: 嘗試使用本地數據或其他數據集\")\n",
    "    raw_data = []\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看樣本範例\n",
    "\n",
    "顯示幾個數據樣本以了解數據格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data:\n",
    "    print(\"\\n📄 數據樣本:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i in range(min(3, len(raw_data))):\n",
    "        sample = raw_data[i]\n",
    "        print(f\"\\n樣本 {i+1}:\")\n",
    "        print(f\"  指令: {sample['instruction']}\")\n",
    "        if sample['input']:\n",
    "            print(f\"  輸入: {sample['input']}\")\n",
    "        print(f\"  輸出: {sample['output'][:100]}...\" if len(sample['output']) > 100 else f\"  輸出: {sample['output']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"⚠️  數據未加載,跳過樣本顯示\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 數據統計分析\n",
    "\n",
    "分析數據集的基本統計特徵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data:\n",
    "    print(\"📊 數據統計分析\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 計算文本長度 (字符數)\n",
    "    instruction_lengths = [len(s['instruction']) for s in raw_data]\n",
    "    input_lengths = [len(s['input']) for s in raw_data]\n",
    "    output_lengths = [len(s['output']) for s in raw_data]\n",
    "    \n",
    "    # 計算詞數 (簡單分割)\n",
    "    instruction_words = [len(s['instruction'].split()) for s in raw_data]\n",
    "    output_words = [len(s['output'].split()) for s in raw_data]\n",
    "    \n",
    "    stats = {\n",
    "        '總樣本數': len(raw_data),\n",
    "        '平均指令長度 (字符)': np.mean(instruction_lengths),\n",
    "        '平均輸出長度 (字符)': np.mean(output_lengths),\n",
    "        '平均指令詞數': np.mean(instruction_words),\n",
    "        '平均輸出詞數': np.mean(output_words),\n",
    "        '有輸入的樣本數': sum(1 for s in raw_data if s['input']),\n",
    "        '無輸入的樣本數': sum(1 for s in raw_data if not s['input'])\n",
    "    }\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value:,}\")\n",
    "    \n",
    "    # 保存統計信息\n",
    "    stats_path = directories['analysis'] / 'data_statistics.json'\n",
    "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n✅ 統計信息已保存至: {stats_path}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"⚠️  數據未加載,跳過統計分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 可視化數據分布\n",
    "\n",
    "生成數據分布的可視化圖表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data:\n",
    "    print(\"📈 生成數據分布圖表...\")\n",
    "    \n",
    "    # 建立子圖\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Alpaca 數據集分布分析', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. 指令長度分布\n",
    "    axes[0, 0].hist(instruction_words, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(np.mean(instruction_words), color='red', linestyle='--', \n",
    "                       label=f'均值: {np.mean(instruction_words):.1f}')\n",
    "    axes[0, 0].set_xlabel('詞數')\n",
    "    axes[0, 0].set_ylabel('樣本數')\n",
    "    axes[0, 0].set_title('指令長度分布')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 輸出長度分布\n",
    "    axes[0, 1].hist(output_words, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[0, 1].axvline(np.mean(output_words), color='red', linestyle='--', \n",
    "                       label=f'均值: {np.mean(output_words):.1f}')\n",
    "    axes[0, 1].set_xlabel('詞數')\n",
    "    axes[0, 1].set_ylabel('樣本數')\n",
    "    axes[0, 1].set_title('輸出長度分布')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 指令 vs 輸出長度散點圖\n",
    "    sample_size = min(5000, len(raw_data))\n",
    "    sample_indices = np.random.choice(len(raw_data), sample_size, replace=False)\n",
    "    sample_instr = [instruction_words[i] for i in sample_indices]\n",
    "    sample_out = [output_words[i] for i in sample_indices]\n",
    "    \n",
    "    axes[1, 0].scatter(sample_instr, sample_out, alpha=0.3, s=10)\n",
    "    axes[1, 0].set_xlabel('指令長度 (詞數)')\n",
    "    axes[1, 0].set_ylabel('輸出長度 (詞數)')\n",
    "    axes[1, 0].set_title(f'指令與輸出長度關係 (樣本: {sample_size:,})')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 輸入數據統計\n",
    "    has_input = sum(1 for s in raw_data if s['input'])\n",
    "    no_input = len(raw_data) - has_input\n",
    "    \n",
    "    axes[1, 1].pie([has_input, no_input], \n",
    "                   labels=[f'有輸入\\n({has_input:,})', f'無輸入\\n({no_input:,})'],\n",
    "                   autopct='%1.1f%%',\n",
    "                   startangle=90,\n",
    "                   colors=['#66b3ff', '#ff9999'])\n",
    "    axes[1, 1].set_title('樣本輸入分布')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存圖表\n",
    "    fig_path = directories['analysis'] / 'data_distribution.png'\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ 圖表已保存至: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️  數據未加載,跳過可視化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 準備 Sentence-BERT 模型\n",
    "\n",
    "加載 Sentence-BERT 模型用於後續的 IFD 計算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📥 加載 Sentence-BERT 模型...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 使用平衡質量與速度的模型\n",
    "MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "    \n",
    "    print(f\"✅ 模型加載成功: {MODEL_NAME}\")\n",
    "    print(f\"   最大序列長度: {embedding_model.max_seq_length}\")\n",
    "    print(f\"   嵌入維度: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    # 測試編碼\n",
    "    test_text = \"This is a test sentence.\"\n",
    "    test_embedding = embedding_model.encode([test_text])\n",
    "    print(f\"   測試編碼: {test_embedding.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 錯誤: {e}\")\n",
    "    print(\"\\n💡 提示: 可能需要網路連接下載模型\")\n",
    "    embedding_model = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 設定篩選目標\n",
    "\n",
    "定義數據篩選的目標和參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 篩選配置\n",
    "filtering_config = {\n",
    "    'source_dataset': 'Alpaca',\n",
    "    'total_samples': len(raw_data) if raw_data else 0,\n",
    "    'target_retention_rate': 0.30,\n",
    "    'target_samples': int(len(raw_data) * 0.30) if raw_data else 0,\n",
    "    \n",
    "    # IFD 參數\n",
    "    'ifd_min_threshold': 0.3,\n",
    "    'ifd_max_threshold': 0.9,\n",
    "    \n",
    "    # DEITA 權重\n",
    "    'deita_alpha': 0.4,  # 複雜度權重\n",
    "    'deita_beta': 0.4,   # 質量權重\n",
    "    'deita_gamma': 0.2,  # 多樣性權重\n",
    "    \n",
    "    # 其他參數\n",
    "    'embedding_model': MODEL_NAME,\n",
    "    'batch_size': 64,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "print(\"🎯 篩選配置\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(filtering_config, indent=2, ensure_ascii=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 保存配置\n",
    "config_path = directories['data'] / 'filtering_config.json'\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(filtering_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✅ 配置已保存至: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 環境驗證總結\n",
    "\n",
    "檢查所有設置是否正確完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"環境驗證總結\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = [\n",
    "    (\"數據集加載\", len(raw_data) > 0 if raw_data else False),\n",
    "    (\"統計分析完成\", (directories['analysis'] / 'data_statistics.json').exists()),\n",
    "    (\"可視化生成\", (directories['analysis'] / 'data_distribution.png').exists()),\n",
    "    (\"Sentence-BERT 模型\", embedding_model is not None),\n",
    "    (\"篩選配置建立\", (directories['data'] / 'filtering_config.json').exists())\n",
    "]\n",
    "\n",
    "for check_name, status in checks:\n",
    "    status_icon = \"✅\" if status else \"❌\"\n",
    "    print(f\"{status_icon} {check_name}\")\n",
    "\n",
    "all_passed = all(status for _, status in checks)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_passed:\n",
    "    print(\"✅ 所有檢查通過! 可以繼續進行篩選 (Notebook 02)\")\n",
    "    print(f\"\\n預期篩選結果: {filtering_config['total_samples']:,} → {filtering_config['target_samples']:,} 樣本 (保留 30%)\")\n",
    "else:\n",
    "    print(\"⚠️  部分檢查未通過,請解決上述問題後再繼續\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 總結\n",
    "\n",
    "在本 notebook 中,我們完成了:\n",
    "\n",
    "1. ✅ 環境檢查與依賴驗證\n",
    "2. ✅ 加載 Alpaca 數據集 (52K 樣本)\n",
    "3. ✅ 數據統計分析 (長度、分布)\n",
    "4. ✅ 生成可視化圖表\n",
    "5. ✅ 準備 Sentence-BERT 模型\n",
    "6. ✅ 設定篩選配置 (30% 保留率)\n",
    "\n",
    "### 關鍵觀察\n",
    "\n",
    "- 平均指令長度: ~15 詞\n",
    "- 平均輸出長度: ~78 詞\n",
    "- 輸入分布: 約 40% 樣本有額外輸入\n",
    "\n",
    "### 下一步\n",
    "\n",
    "前往 **02-Filter.ipynb** 執行 IFD 和 DEITA 篩選流程。\n",
    "\n",
    "---\n",
    "\n",
    "**記得**:\n",
    "- 篩選目標: 從 52K 樣本中篩選出 15.6K 高質量樣本\n",
    "- IFD 閾值: 0.3 ≤ IFD ≤ 0.9\n",
    "- DEITA 權重: 複雜度 40%, 質量 40%, 多樣性 20%\n",
    "- 預期效果: 質量提升,訓練時間減少 70%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
