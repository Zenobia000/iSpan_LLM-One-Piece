{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: LoRA & QLoRA - Fine-Tuning a Llama-2 Model\n",
    "---\n",
    "## Notebook 2: The Training Process\n",
    "\n",
    "**Goal:** In this notebook, you will use QLoRA to fine-tune a `Llama-2-7B` model on an instruction-following dataset.\n",
    "\n",
    "**You will learn to:**\n",
    "-   Load a dataset for supervised fine-tuning and preprocess it with a tokenizer.\n",
    "-   Configure `BitsAndBytesConfig` to load a model in 4-bit precision (QLoRA).\n",
    "-   Load a pre-trained Llama-2 model for causal language modeling.\n",
    "-   Configure `peft.LoraConfig` to apply LoRA adapters to the model.\n",
    "-   Use the `transformers.Trainer` to efficiently fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Dataset and Preprocess\n",
    "\n",
    "First, we'll load our instruction-tuning dataset. We will use the `guanaco-llama2-1k` dataset, which is a small, high-quality dataset of prompts and responses formatted for Llama-2.\n",
    "\n",
    "#### Key Hugging Face Components:\n",
    "\n",
    "-   `datasets.load_dataset`: Fetches a dataset from the Hugging Face Hub.\n",
    "-   `transformers.AutoTokenizer`: Loads the appropriate tokenizer for our model.\n",
    "-   `dataset.map()`: A powerful method to apply a processing function to every example in the dataset. We use `batched=True` for efficient processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`mlabonne/guanaco-llama2-1k` 資料集**\n",
    "\n",
    "#### **為什麼選擇它？**\n",
    "- **格式相容**：專為 Llama 2 提示格式設計，無需額外處理。  \n",
    "- **輕量高效**：1000 條樣本，適合快速微調與測試。  \n",
    "- **開發者友好**：附官方教學與 Colab Notebook，易於上手。\n",
    "\n",
    "#### **特點**\n",
    "- **高質量來源**：基於 OpenAssistant 數據，涵蓋多樣化指令與回答場景。  \n",
    "- **即插即用**：格式化完成，支持 Llama 2 微調需求。\n",
    "\n",
    "#### **缺點**\n",
    "- **數據量小**：僅 1000 條樣本，適合小型實驗但不適用於大規模訓練。  \n",
    "- **覆蓋範圍有限**：樣本多樣性不足，需額外擴展應用場景。\n",
    "\n",
    "#### **展望**\n",
    "- **數據擴展**：結合其他資料集，擴充樣本規模。  \n",
    "- **遷移學習**：作為初始微調基礎，進一步提升性能。  \n",
    "- **定制化優化**：針對特定領域進行專屬微調。\n",
    "\n",
    "#### **結論**\n",
    "`mlabonne/guanaco-llama2-1k` 是一款輕量高效的資料集，為 Llama 2 微調提供理想基礎，適合快速實驗與模型測試。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb8c68cc45a4c4199ded34be166362f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a468d7110c94efa87d48c549a766877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded and tokenized.\n",
      "Train samples: 900\n",
      "Test samples: 100\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"NousResearch/Llama-2-7b-hf\"\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Important for Causal LM\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# --- Preprocess and Tokenize ---\n",
    "def preprocess_function(examples):\n",
    "    # The 'text' field contains the full formatted prompt.\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
    "\n",
    "# Split the dataset\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Tokenize both splits\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"]) # Remove original text column\n",
    "\n",
    "print(\"✅ Dataset loaded and tokenized.\")\n",
    "print(f\"Train samples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Test samples: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Base Model\n",
    "\n",
    "Now, we load the base model. The key to QLoRA is loading the base model in a quantized format.\n",
    "\n",
    "#### Key Hugging Face `transformers` Components:\n",
    "\n",
    "-   `transformers.BitsAndBytesConfig`: This configuration class is used to specify all the parameters for quantization.\n",
    "    -   `load_in_4bit=True`: This is the master switch to enable 4-bit loading.\n",
    "    -   `bnb_4bit_quant_type=\"nf4\"`: We use the \"nf4\" (Normalized Float 4) quantization type, which is recommended for QLoRA.\n",
    "    -   `bnb_4bit_compute_dtype=torch.bfloat16`: This sets the compute data type during the forward and backward passes. `bfloat16` is a good choice for modern GPUs.\n",
    "-   `transformers.AutoModelForCausalLM`: We use this to load our Llama-2 model, passing the `quantization_config` to it.\n",
    "\n",
    "\n",
    "好的，這是在 `02-Train.ipynb` 檔案中對 `Normalized Float 4 (NF4)` 核心思想的精簡 Markdown 表述，您可以將其整合到您的筆記中。\n",
    "\n",
    "---\n",
    "\n",
    "### `Normalized Float 4 (NF4)` 核心思想解析\n",
    "\n",
    "`Normalized Float 4 (NF4)` 是 QLoRA 中使用的一種創新的 4-bit 資料型態，其核心理念是：**為常態分佈的模型權重，設計一種資訊理論上最優的 4-bit 量化方法**。\n",
    "\n",
    "#### 核心方法：分位數量化 (Quantile Quantization)\n",
    "\n",
    "1.  **問題前提**：\n",
    "    大型語言模型的權重（weights）並非均勻分佈，而是高度集中於 0 附近，呈現**常態分佈**（鐘形曲線）。傳統的 4-bit 格式（如 FP4）是均勻量化的，這會導致在權重密集區精度損失較大。\n",
    "\n",
    "2.  **NF4 的解決方案**：\n",
    "    *   **建立理想模板**：首先，演算法會建立一個標準常態分佈的「模板」。\n",
    "    *   **計算分位數**：接著，它將這個模板劃分為 `2^4 = 16` 個**等機率**的區間。這意味著每個區間包含的數據點數量是相同的。\n",
    "    *   **非均勻量化**：這 16 個區間的分割點就成為了 NF4 的量化值。結果是一種非均勻的刻度，在數據密集的中心區域（靠近 0）刻度更精細，而在數據稀疏的兩端區域刻度較粗。\n",
    "\n",
    "3.  **類比**：\n",
    "    *   傳統 FP4 像一把**普通尺**，刻度是均勻的。\n",
    "    *   NF4 則像一把**特製的對數尺**，專為測量常態分佈而設計，在最需要的地方提供最高精度。\n",
    "\n",
    "#### 主要優勢\n",
    "\n",
    "*   **更少資訊損失**：相比傳統 4-bit 格式，NF4 在量化過程中能保留更多原始權重的有效資訊。\n",
    "*   **更高模型性能**：由於量化誤差更小，使用 QLoRA 微調後的模型表現更接近於使用 bfloat16 等更高精度的結果。\n",
    "\n",
    "https://lh3.googleusercontent.com/d/1-Ms-lrWOQkFsPG_D9oMcA1UrWzVKSdC7\n",
    "https://i.ytimg.com/vi/aZPAqBov3tQ/maxresdefault.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2db96357b5449cab72c42c4d5104e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# --- Quantization Configuration ---\n",
    "# Load model in 4-bit\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# --- Load Base Model ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"✅ Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure LoRA\n",
    "\n",
    "Now we configure LoRA using the `peft` library.\n",
    "\n",
    "#### Key Hugging Face `peft` Components:\n",
    "\n",
    "-   `peft.LoraConfig`: The main configuration class for LoRA.\n",
    "    -   `r`: The rank of the update matrices. A lower rank means fewer trainable parameters. A common range is 8-64.\n",
    "    -   `lora_alpha`: The scaling factor for the LoRA matrices. It's often set to twice the rank (`2*r`).\n",
    "    -   `target_modules`: A list of the names of the modules (e.g., attention layers) to apply LoRA to. For Llama models, this is typically `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`.\n",
    "    -   `lora_dropout`: Dropout probability for the LoRA layers to reduce overfitting.\n",
    "    -   `bias=\"none\"`: Specifies which biases to train. \"none\" is common.\n",
    "    -   `task_type=\"CAUSAL_LM\"`: Specifies the task type.\n",
    "-   `peft.get_peft_model`: This function takes the base model and the LoRA config and returns a PEFT model ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --- LoRA Configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# --- Create PEFT Model ---\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- Print Trainable Parameters ---\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set Up Training\n",
    "\n",
    "The final step is to configure and run the training process using the `transformers.Trainer`.\n",
    "\n",
    "#### Key Hugging Face Components:\n",
    "\n",
    "-   `transformers.TrainingArguments`: This class holds all the hyperparameters for the training run, such as learning rate, number of epochs, batch size, and logging settings.\n",
    "    -   `metric_for_best_model=\"perplexity\"`: We specify perplexity as the metric to determine the best model.\n",
    "    -   `greater_is_better=False`: Lower perplexity indicates better performance.\n",
    "-   `transformers.Trainer`: The standard trainer class from the `transformers` library. It requires a model, training arguments, datasets, a tokenizer, and a data collator.\n",
    "-   `transformers.DataCollatorForLanguageModeling`: This data collator will be used to form batches of tokenized data. It also handles the creation of the `labels` for causal language modeling, where the model predicts the next token.\n",
    "-   `compute_metrics`: A custom function to calculate **perplexity**, which is the standard evaluation metric for language modeling tasks. Perplexity measures how well the model predicts the next token - lower values indicate better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gradient configuration complete! Trainable: 4,194,304 / Total: 3,504,607,232\n",
      "🧹 GPU memory cleared before training\n",
      "[2025-10-15 12:27:24,422] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting PRODUCTION QLoRA training with Early Stopping...\n",
      "📊 Training config: 1 batch × 32 accum = 32 effective batch\n",
      "🎯 Max epochs: 3, Max steps: 1000\n",
      "📈 Evaluation every 50 steps\n",
      "⏹️  Early stopping: patience=3, threshold=0.01\n",
      "💾 Memory-optimized: fp16=True, gradient_checkpointing=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  19/1000 13:38 < 13:07:36, 0.02 it/s, Epoch 0.64/35]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# --- Custom Evaluation Metrics ---\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute perplexity from the model's predictions.\n",
    "    Perplexity is the standard metric for language modeling tasks.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Convert to tensors\n",
    "    predictions = torch.from_numpy(predictions).float()\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "    \n",
    "    # Shift for causal language modeling\n",
    "    shift_logits = predictions[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    # Calculate cross entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), \n",
    "        shift_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    # Calculate perplexity\n",
    "    try:\n",
    "        perplexity = math.exp(loss.item())\n",
    "    except OverflowError:\n",
    "        perplexity = float('inf')\n",
    "    \n",
    "    return {\n",
    "        \"perplexity\": perplexity,\n",
    "        \"eval_loss\": loss.item()\n",
    "    }\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- Gradient & Model Config ---\n",
    "peft_model.train()\n",
    "peft_model.enable_input_require_grads()      # ✅ 確保梯度可回傳\n",
    "peft_model.gradient_checkpointing_enable()   # ✅ 節省顯存\n",
    "peft_model.config.use_cache = False          # ⚠️ 需關閉 cache 以啟用反傳\n",
    "\n",
    "# Freeze all except LoRA layers\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'lora_' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "print(f\"✅ Gradient configuration complete! Trainable: {trainable_params:,} / Total: {total_params:,}\")\n",
    "\n",
    "# 🧹 清理GPU記憶體\n",
    "torch.cuda.empty_cache()\n",
    "print(\"🧹 GPU memory cleared before training\")\n",
    "\n",
    "# --- Production Training Arguments for RTX 2000 Ada (15.6 GB) ---\n",
    "# ✅ STABLE CONFIG: 已確認記憶體穩定，升級至正式訓練配置 + Early Stopping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-llama2-7b-guanaco\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # 📈 學習策略\n",
    "    learning_rate=3e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,                       # 3% warmup for stable learning\n",
    "    weight_decay=0.01,                       # L2 regularization\n",
    "    \n",
    "    # 🎯 訓練設定 - 平衡效能與穩定性\n",
    "    per_device_train_batch_size=1,           # 保持最小以確保記憶體穩定\n",
    "    per_device_eval_batch_size=1,            # 評估批次也保持最小\n",
    "    gradient_accumulation_steps=32,          # 恢復原值模擬 batch_size=16\n",
    "    max_grad_norm=1.0,                       # 梯度裁剪防止爆炸\n",
    "    \n",
    "    # 🕐 訓練週期\n",
    "    num_train_epochs=3,                      # 增加 epochs 讓 early stopping 有機會生效\n",
    "    max_steps=1000,                          # 增加最大步數上限\n",
    "    \n",
    "    # 💾 記憶體最佳化 (保持穩定配置)\n",
    "    fp16=True,                              # 半精度訓練\n",
    "    gradient_checkpointing=True,            # 梯度檢查點節省記憶體\n",
    "    optim=\"paged_adamw_8bit\",              # 8-bit 優化器\n",
    "    dataloader_pin_memory=False,           # 關閉以避免RAM壓力\n",
    "    dataloader_num_workers=0,              # 單程序避免記憶體競爭\n",
    "    remove_unused_columns=True,            # 移除未使用資料欄位\n",
    "    \n",
    "    # 📊 評估與儲存策略\n",
    "    eval_strategy=\"steps\",                  # 定期評估\n",
    "    eval_steps=50,                         # 縮短評估間隔以便 early stopping 更敏感\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,                         # 與評估同步保存\n",
    "    save_total_limit=3,                    # 保留最近3個檢查點\n",
    "    load_best_model_at_end=True,           # 載入最佳模型\n",
    "    \n",
    "    # 📏 評估指標 & Early Stopping\n",
    "    metric_for_best_model=\"eval_perplexity\", # 以困惑度為最佳模型指標\n",
    "    greater_is_better=False,               # 越低越好 (perplexity)\n",
    "    \n",
    "    # 📝 日誌設定\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,                      # 每10步記錄一次\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # 🔕 報告設定\n",
    "    report_to=None,                        # 不上傳至外部平台\n",
    "    disable_tqdm=False,                    # 保留進度條\n",
    "    \n",
    "    # 🚀 進階設定\n",
    "    push_to_hub=False,                     # 不推送至 Hub\n",
    "    hub_token=None,\n",
    "    ignore_data_skip=True,                 # 忽略資料跳過警告\n",
    ")\n",
    "\n",
    "# --- Early Stopping Callback Configuration ---\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,              # 連續5次評估無改善就停止\n",
    "    early_stopping_threshold=0.01,          # 改善閾值 (perplexity 下降 < 0.01 視為無改善)\n",
    ")\n",
    "\n",
    "# --- Create Trainer with Early Stopping ---\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],     # 恢復評估資料集\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,             # 恢復指標計算\n",
    "    callbacks=[early_stopping_callback],        # ✅ 加入 Early Stopping\n",
    ")\n",
    "\n",
    "# --- Start Production Training with Early Stopping ---\n",
    "print(\"🚀 Starting PRODUCTION QLoRA training with Early Stopping...\")\n",
    "print(f\"📊 Training config: {training_args.per_device_train_batch_size} batch × {training_args.gradient_accumulation_steps} accum = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps} effective batch\")\n",
    "print(f\"🎯 Max epochs: {training_args.num_train_epochs}, Max steps: {training_args.max_steps}\")\n",
    "print(f\"📈 Evaluation every {training_args.eval_steps} steps\")\n",
    "print(f\"⏹️  Early stopping: patience={early_stopping_callback.early_stopping_patience}, threshold={early_stopping_callback.early_stopping_threshold}\")\n",
    "print(f\"💾 Memory-optimized: fp16={training_args.fp16}, gradient_checkpointing={training_args.gradient_checkpointing}\")\n",
    "\n",
    "# Train with automatic early stopping\n",
    "train_result = trainer.train()\n",
    "print(\"✅ Production training complete!\")\n",
    "\n",
    "# Show training summary\n",
    "if hasattr(train_result, 'metrics'):\n",
    "    print(f\"\\n📈 Training Summary:\")\n",
    "    print(f\"Total steps completed: {train_result.metrics.get('train_steps', 'N/A')}\")\n",
    "    print(f\"Training runtime: {train_result.metrics.get('train_runtime', 'N/A'):.2f}s\" if 'train_runtime' in train_result.metrics else \"\")\n",
    "    \n",
    "    # Check if training was stopped early\n",
    "    if train_result.metrics.get('train_steps', 0) < training_args.max_steps:\n",
    "        print(\"⏹️  Training stopped early due to no improvement!\")\n",
    "    else:\n",
    "        print(\"🏁 Training completed full course!\")\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "print(\"\\n📊 Final Evaluation Results:\")\n",
    "final_metrics = trainer.evaluate()\n",
    "print(f\"Final Perplexity: {final_metrics['eval_perplexity']:.4f}\")\n",
    "print(f\"Final Loss: {final_metrics['eval_loss']:.4f}\")\n",
    "print(f\"Training Samples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Evaluation Samples: {len(tokenized_datasets['test'])}\")\n",
    "\n",
    "# Display early stopping information\n",
    "print(f\"\\n⏹️  Early Stopping Configuration:\")\n",
    "print(f\"   - Patience: {early_stopping_callback.early_stopping_patience} evaluations\")\n",
    "print(f\"   - Threshold: {early_stopping_callback.early_stopping_threshold} perplexity improvement\")\n",
    "print(f\"   - Evaluation frequency: Every {training_args.eval_steps} steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
