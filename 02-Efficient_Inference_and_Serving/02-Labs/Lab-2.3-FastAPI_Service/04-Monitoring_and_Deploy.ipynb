{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.3 Part 4: Monitoring and Deployment\n",
    "\n",
    "## Objectives\n",
    "- Add Prometheus metrics\n",
    "- Implement structured logging\n",
    "- Create Docker containers\n",
    "- Set up health checks\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Prometheus Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prometheus_client\n",
    "try:\n",
    "    import prometheus_client\n",
    "    print(f\"âœ… prometheus_client: {prometheus_client.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Install: pip install prometheus-client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_metrics.py\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import Response\n",
    "from pydantic import BaseModel\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Prometheus metrics\n",
    "request_counter = Counter(\n",
    "    'llm_requests_total',\n",
    "    'Total number of requests',\n",
    "    ['endpoint', 'status']\n",
    ")\n",
    "\n",
    "request_duration = Histogram(\n",
    "    'llm_request_duration_seconds',\n",
    "    'Request duration in seconds',\n",
    "    ['endpoint'],\n",
    "    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    ")\n",
    "\n",
    "tokens_generated = Counter(\n",
    "    'llm_tokens_generated_total',\n",
    "    'Total tokens generated'\n",
    ")\n",
    "\n",
    "active_requests = Gauge(\n",
    "    'llm_active_requests',\n",
    "    'Number of active requests'\n",
    ")\n",
    "\n",
    "# Global engine\n",
    "llm_engine = None\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 100\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    global llm_engine\n",
    "    llm_engine = LLM(model=\"gpt2\", gpu_memory_utilization=0.3)\n",
    "    print(\"âœ… Engine loaded\")\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: GenerateRequest):\n",
    "    \"\"\"Generate with metrics tracking.\"\"\"\n",
    "    active_requests.inc()\n",
    "    \n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=request.max_tokens,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        outputs = llm_engine.generate([request.prompt], sampling_params)\n",
    "        \n",
    "        duration = time.time() - start\n",
    "        generated_tokens = len(outputs[0].outputs[0].token_ids)\n",
    "        \n",
    "        # Update metrics\n",
    "        request_counter.labels(endpoint='generate', status='success').inc()\n",
    "        request_duration.labels(endpoint='generate').observe(duration)\n",
    "        tokens_generated.inc(generated_tokens)\n",
    "        \n",
    "        return {\n",
    "            \"text\": outputs[0].outputs[0].text,\n",
    "            \"tokens\": generated_tokens,\n",
    "            \"duration\": duration\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        request_counter.labels(endpoint='generate', status='error').inc()\n",
    "        raise\n",
    "    finally:\n",
    "        active_requests.dec()\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint.\"\"\"\n",
    "    return Response(\n",
    "        content=generate_latest(),\n",
    "        media_type=CONTENT_TYPE_LATEST\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Structured Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_logging.py\n",
    "from fastapi import FastAPI, Request\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Configure structured logging\n",
    "class JSONFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        log_data = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"level\": record.levelname,\n",
    "            \"message\": record.getMessage(),\n",
    "            \"module\": record.module,\n",
    "        }\n",
    "        return json.dumps(log_data)\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(JSONFormatter())\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    \"\"\"Log all requests.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Log request\n",
    "    logger.info(f\"Request: {request.method} {request.url.path}\")\n",
    "    \n",
    "    # Process request\n",
    "    response = await call_next(request)\n",
    "    \n",
    "    # Log response\n",
    "    duration = time.time() - start_time\n",
    "    logger.info(f\"Response: {response.status_code} ({duration:.3f}s)\")\n",
    "    \n",
    "    # Add custom headers\n",
    "    response.headers[\"X-Process-Time\"] = str(duration)\n",
    "    \n",
    "    return response\n",
    "\n",
    "@app.get(\"/test\")\n",
    "async def test():\n",
    "    logger.info(\"Test endpoint called\")\n",
    "    return {\"message\": \"test\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Docker Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04\n",
    "\n",
    "# Install Python\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip3 install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application\n",
    "COPY app_vllm.py .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app_vllm:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "fastapi==0.104.0\n",
    "uvicorn[standard]==0.24.0\n",
    "vllm>=0.6.0\n",
    "prometheus-client==0.19.0\n",
    "pydantic==2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Commands\n",
    "\n",
    "```bash\n",
    "# Build image\n",
    "docker build -t vllm-api:latest .\n",
    "\n",
    "# Run container\n",
    "docker run --gpus all \\\n",
    "  -p 8000:8000 \\\n",
    "  -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "  vllm-api:latest\n",
    "\n",
    "# Check logs\n",
    "docker logs -f <container_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  vllm-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    volumes:\n",
    "      - ~/.cache/huggingface:/root/.cache/huggingface\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 60s\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    depends_on:\n",
    "      - prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prometheus.yml\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'vllm-api'\n",
    "    static_configs:\n",
    "      - targets: ['vllm-api:8000']\n",
    "    metrics_path: '/metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Production Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Checklist\n",
    "\n",
    "#### Infrastructure âœ…\n",
    "- [ ] GPU drivers and CUDA installed\n",
    "- [ ] Docker and nvidia-docker runtime\n",
    "- [ ] Sufficient disk space for models\n",
    "- [ ] Network firewall configured\n",
    "\n",
    "#### Application âœ…\n",
    "- [ ] Environment variables configured\n",
    "- [ ] Model downloaded and cached\n",
    "- [ ] API keys and authentication\n",
    "- [ ] Rate limiting enabled\n",
    "\n",
    "#### Monitoring âœ…\n",
    "- [ ] Prometheus metrics exposed\n",
    "- [ ] Grafana dashboards created\n",
    "- [ ] Alert rules configured\n",
    "- [ ] Log aggregation setup\n",
    "\n",
    "#### Reliability âœ…\n",
    "- [ ] Health checks implemented\n",
    "- [ ] Graceful shutdown handling\n",
    "- [ ] Auto-restart on failure\n",
    "- [ ] Backup and disaster recovery\n",
    "\n",
    "#### Performance âœ…\n",
    "- [ ] Load testing completed\n",
    "- [ ] Latency benchmarks acceptable\n",
    "- [ ] Memory usage optimized\n",
    "- [ ] Auto-scaling configured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "âœ… **Completed Lab-2.3**:\n",
    "1. Built complete FastAPI service\n",
    "2. Implemented async processing\n",
    "3. Integrated vLLM backend\n",
    "4. Added Prometheus monitoring\n",
    "5. Created Docker deployment\n",
    "\n",
    "ðŸŽ“ **Skills Mastered**:\n",
    "- FastAPI development\n",
    "- Async/await patterns\n",
    "- Streaming responses (SSE, WebSocket)\n",
    "- vLLM integration\n",
    "- Prometheus metrics\n",
    "- Docker containerization\n",
    "\n",
    "ðŸ“Š **Production-Ready Features**:\n",
    "- OpenAI-compatible API\n",
    "- High-performance vLLM backend\n",
    "- Concurrent request handling\n",
    "- Monitoring and observability\n",
    "- Container deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **Lab-2.4**: Production Environment Deployment\n",
    "- **Lab-2.5**: Performance Monitoring and Tuning\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [FastAPI Docs](https://fastapi.tiangolo.com/)\n",
    "- [vLLM Server Guide](https://docs.vllm.ai/en/latest/serving/)\n",
    "- [Prometheus Python Client](https://github.com/prometheus/client_python)\n",
    "- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ‰ Congratulations! Lab-2.3 Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nYou've built a production-ready LLM API service! ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
