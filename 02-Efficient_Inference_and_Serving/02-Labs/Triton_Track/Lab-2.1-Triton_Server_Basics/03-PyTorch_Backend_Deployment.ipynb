{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1.3: PyTorch Backend 深度部署\n",
    "\n",
    "## 🎯 學習目標\n",
    "\n",
    "1. **掌握 PyTorch Backend 高級特性**\n",
    "   - 自定義推理邏輯實現\n",
    "   - 動態圖和靜態圖部署\n",
    "   - TorchScript 優化和部署\n",
    "\n",
    "2. **實現企業級推理優化**\n",
    "   - 批處理優化策略\n",
    "   - 記憶體管理和資源控制\n",
    "   - 多 GPU 推理配置\n",
    "\n",
    "3. **建構完整的推理流水線**\n",
    "   - 預處理和後處理整合\n",
    "   - 錯誤處理和容錯機制\n",
    "   - 性能監控和日誌記錄\n",
    "\n",
    "## 📋 企業案例背景\n",
    "\n",
    "**場景**: VISA 即時反詐騙系統需要：\n",
    "- 每秒處理 10,000+ 筆交易\n",
    "- 推理延遲 < 10ms (P99)\n",
    "- 99.99% 可用性要求\n",
    "- 支援 A/B 測試和模型熱更新\n",
    "\n",
    "**技術挑戰**: 如何在極高性能要求下實現穩定可靠的 ML 推理服務？\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Backend 深度解析\n",
    "\n",
    "### 1.1 Backend 架構理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.jit as jit\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from pathlib import Path\n",
    "import threading\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PyTorchBackendAnalyzer:\n",
    "    \"\"\"\n",
    "    PyTorch Backend 深度分析器\n",
    "    \n",
    "    分析和理解 Triton PyTorch Backend 的工作原理\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_backend_lifecycle():\n",
    "        \"\"\"\n",
    "        解釋 PyTorch Backend 的生命週期\n",
    "        \"\"\"\n",
    "        print(\"🔄 PyTorch Backend 生命週期分析\")\n",
    "        print(\"═\" * 50)\n",
    "        print()\n",
    "        \n",
    "        lifecycle_stages = {\n",
    "            \"1. 🚀 初始化階段 (initialize)\": [\n",
    "                \"解析模型配置 (config.pbtxt)\",\n",
    "                \"載入模型權重和結構\",\n",
    "                \"設置 GPU 設備和記憶體\",\n",
    "                \"初始化推理引擎\",\n",
    "                \"預熱模型 (可選)\"\n",
    "            ],\n",
    "            \"2. ⚡ 執行階段 (execute)\": [\n",
    "                \"接收批量推理請求\",\n",
    "                \"輸入數據預處理\",\n",
    "                \"模型前向推理\",\n",
    "                \"輸出後處理\",\n",
    "                \"返回推理結果\"\n",
    "            ],\n",
    "            \"3. 🧹 清理階段 (finalize)\": [\n",
    "                \"釋放 GPU 記憶體\",\n",
    "                \"清理緩存數據\",\n",
    "                \"關閉資源連接\",\n",
    "                \"保存統計信息\",\n",
    "                \"執行最終清理\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for stage, steps in lifecycle_stages.items():\n",
    "            print(f\"{stage}:\")\n",
    "            for step in steps:\n",
    "                print(f\"   ✅ {step}\")\n",
    "            print()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_deployment_modes():\n",
    "        \"\"\"\n",
    "        比較不同的 PyTorch 部署模式\n",
    "        \"\"\"\n",
    "        print(\"📊 PyTorch 部署模式對比\")\n",
    "        print(\"═\" * 50)\n",
    "        print()\n",
    "        \n",
    "        modes = {\n",
    "            \"🐍 Python 模式\": {\n",
    "                \"優點\": [\"開發靈活\", \"調試方便\", \"支援動態圖\"],\n",
    "                \"缺點\": [\"性能較低\", \"GIL 限制\", \"記憶體使用高\"],\n",
    "                \"適用場景\": \"原型開發、複雜邏輯\"\n",
    "            },\n",
    "            \"📜 TorchScript 模式\": {\n",
    "                \"優點\": [\"性能優化\", \"無 GIL 限制\", \"可序列化\"],\n",
    "                \"缺點\": [\"功能限制\", \"調試困難\", \"轉換複雜\"],\n",
    "                \"適用場景\": \"生產環境、高性能推理\"\n",
    "            },\n",
    "            \"⚡ TensorRT 模式\": {\n",
    "                \"優點\": [\"極高性能\", \"低延遲\", \"記憶體優化\"],\n",
    "                \"缺點\": [\"NVIDIA GPU 限制\", \"模型限制\", \"設置複雜\"],\n",
    "                \"適用場景\": \"極端性能要求、NVIDIA 環境\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for mode, details in modes.items():\n",
    "            print(f\"{mode}:\")\n",
    "            print(f\"   ✅ 優點: {', '.join(details['優點'])}\")\n",
    "            print(f\"   ❌ 缺點: {', '.join(details['缺點'])}\")\n",
    "            print(f\"   🎯 適用: {details['適用場景']}\")\n",
    "            print()\n",
    "\n",
    "# 執行分析\n",
    "analyzer = PyTorchBackendAnalyzer()\n",
    "analyzer.explain_backend_lifecycle()\n",
    "analyzer.compare_deployment_modes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 企業級模型構建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VISAFraudDetectionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    VISA 反詐騙檢測模型\n",
    "    \n",
    "    企業級特性:\n",
    "    - 多層特徵融合\n",
    "    - 實時推理優化\n",
    "    - 可解釋性支援\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        transaction_dim: int = 50,\n",
    "        user_dim: int = 30,\n",
    "        merchant_dim: int = 20,\n",
    "        hidden_dim: int = 128,\n",
    "        num_classes: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 特徵編碼器\n",
    "        self.transaction_encoder = nn.Sequential(\n",
    "            nn.Linear(transaction_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.user_encoder = nn.Sequential(\n",
    "            nn.Linear(user_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.merchant_encoder = nn.Sequential(\n",
    "            nn.Linear(merchant_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # 注意力機制\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 分類器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 風險評分器 (可解釋性)\n",
    "        self.risk_scorer = nn.Linear(hidden_dim * 2, 1)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        transaction_features: torch.Tensor,\n",
    "        user_features: torch.Tensor,\n",
    "        merchant_features: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        前向推理\n",
    "        \n",
    "        Returns:\n",
    "            logits: 分類預測\n",
    "            risk_score: 風險評分\n",
    "            attention_weights: 注意力權重 (可解釋性)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 特徵編碼\n",
    "        trans_encoded = self.transaction_encoder(transaction_features)\n",
    "        user_encoded = self.user_encoder(user_features)\n",
    "        merchant_encoded = self.merchant_encoder(merchant_features)\n",
    "        \n",
    "        # 特徵融合\n",
    "        context_features = torch.cat([user_encoded, merchant_encoded], dim=-1)\n",
    "        \n",
    "        # 注意力機制 (transaction as query, context as key/value)\n",
    "        trans_query = trans_encoded.unsqueeze(1)  # [batch, 1, hidden]\n",
    "        context_kv = context_features.unsqueeze(1)  # [batch, 1, hidden]\n",
    "        \n",
    "        attended_features, attention_weights = self.attention(\n",
    "            trans_query, context_kv, context_kv\n",
    "        )\n",
    "        attended_features = attended_features.squeeze(1)\n",
    "        \n",
    "        # 最終特徵\n",
    "        final_features = torch.cat([trans_encoded, attended_features], dim=-1)\n",
    "        \n",
    "        # 預測\n",
    "        logits = self.classifier(final_features)\n",
    "        risk_score = torch.sigmoid(self.risk_scorer(final_features))\n",
    "        \n",
    "        return logits, risk_score, attention_weights.squeeze(1)\n",
    "\n",
    "# 創建和測試模型\n",
    "print(\"🏦 創建 VISA 反詐騙檢測模型...\")\n",
    "model = VISAFraudDetectionModel()\n",
    "\n",
    "# 模型信息\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"📊 模型統計:\")\n",
    "print(f\"   📈 總參數: {total_params:,}\")\n",
    "print(f\"   🎯 可訓練參數: {trainable_params:,}\")\n",
    "print(f\"   💾 模型大小: {total_params * 4 / (1024**2):.2f} MB (FP32)\")\n",
    "print()\n",
    "\n",
    "# 測試推理\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 模擬輸入\n",
    "    batch_size = 8\n",
    "    transaction_features = torch.randn(batch_size, 50)\n",
    "    user_features = torch.randn(batch_size, 30)\n",
    "    merchant_features = torch.randn(batch_size, 20)\n",
    "    \n",
    "    # 推理\n",
    "    start_time = time.time()\n",
    "    logits, risk_scores, attention_weights = model(\n",
    "        transaction_features, user_features, merchant_features\n",
    "    )\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"⚡ 推理性能測試:\")\n",
    "    print(f\"   🔢 批量大小: {batch_size}\")\n",
    "    print(f\"   ⏱️  推理時間: {inference_time*1000:.2f} ms\")\n",
    "    print(f\"   📊 平均延遲: {inference_time*1000/batch_size:.2f} ms/sample\")\n",
    "    print(f\"   🚀 吞吐量: {batch_size/inference_time:.1f} samples/sec\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"📋 輸出格式:\")\n",
    "    print(f\"   🎯 Logits shape: {logits.shape}\")\n",
    "    print(f\"   📊 Risk scores shape: {risk_scores.shape}\")\n",
    "    print(f\"   👀 Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TorchScript 優化與部署\n",
    "\n",
    "### 2.1 模型轉換和優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchScriptOptimizer:\n",
    "    \"\"\"\n",
    "    TorchScript 優化器\n",
    "    \n",
    "    企業級功能:\n",
    "    - 自動化模型轉換\n",
    "    - 性能基準測試\n",
    "    - 優化驗證\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.scripted_model = None\n",
    "    \n",
    "    def convert_to_torchscript(\n",
    "        self, \n",
    "        method: str = \"trace\",\n",
    "        example_inputs: Tuple = None\n",
    "    ) -> jit.ScriptModule:\n",
    "        \"\"\"\n",
    "        轉換模型到 TorchScript\n",
    "        \n",
    "        Args:\n",
    "            method: 轉換方法 ('trace' 或 'script')\n",
    "            example_inputs: 示例輸入 (trace 模式需要)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"🔄 開始 TorchScript 轉換 (方法: {method})...\")\n",
    "        \n",
    "        try:\n",
    "            if method == \"trace\":\n",
    "                if example_inputs is None:\n",
    "                    # 創建示例輸入\n",
    "                    example_inputs = (\n",
    "                        torch.randn(1, 50).to(self.device),  # transaction_features\n",
    "                        torch.randn(1, 30).to(self.device),  # user_features\n",
    "                        torch.randn(1, 20).to(self.device)   # merchant_features\n",
    "                    )\n",
    "                \n",
    "                print(f\"   📊 使用 Trace 模式轉換...\")\n",
    "                self.scripted_model = torch.jit.trace(\n",
    "                    self.model, example_inputs\n",
    "                )\n",
    "                \n",
    "            elif method == \"script\":\n",
    "                print(f\"   📜 使用 Script 模式轉換...\")\n",
    "                self.scripted_model = torch.jit.script(self.model)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"不支持的轉換方法: {method}\")\n",
    "            \n",
    "            print(f\"✅ TorchScript 轉換完成\")\n",
    "            return self.scripted_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ TorchScript 轉換失敗: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def optimize_for_inference(self) -> jit.ScriptModule:\n",
    "        \"\"\"\n",
    "        為推理優化 TorchScript 模型\n",
    "        \"\"\"\n",
    "        if self.scripted_model is None:\n",
    "            raise ValueError(\"請先轉換模型到 TorchScript\")\n",
    "        \n",
    "        print(f\"⚡ 開始推理優化...\")\n",
    "        \n",
    "        # 1. 凍結模型\n",
    "        print(f\"   🧊 凍結模型參數...\")\n",
    "        self.scripted_model = torch.jit.freeze(\n",
    "            self.scripted_model.eval()\n",
    "        )\n",
    "        \n",
    "        # 2. 優化計算圖\n",
    "        print(f\"   📊 優化計算圖...\")\n",
    "        self.scripted_model = torch.jit.optimize_for_inference(\n",
    "            self.scripted_model\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 推理優化完成\")\n",
    "        return self.scripted_model\n",
    "    \n",
    "    def benchmark_performance(\n",
    "        self, \n",
    "        batch_sizes: List[int] = [1, 4, 8, 16, 32],\n",
    "        num_runs: int = 100\n",
    "    ) -> Dict[str, Dict[int, float]]:\n",
    "        \"\"\"\n",
    "        性能基準測試\n",
    "        \n",
    "        Returns:\n",
    "            Dict: {model_type: {batch_size: latency_ms}}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"📊 開始性能基準測試...\")\n",
    "        print(f\"   🔢 批量大小: {batch_sizes}\")\n",
    "        print(f\"   🔄 運行次數: {num_runs} 次/批量\")\n",
    "        print()\n",
    "        \n",
    "        results = {\n",
    "            \"original\": {},\n",
    "            \"torchscript\": {}\n",
    "        }\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # 準備測試數據\n",
    "            test_inputs = (\n",
    "                torch.randn(batch_size, 50).to(self.device),\n",
    "                torch.randn(batch_size, 30).to(self.device),\n",
    "                torch.randn(batch_size, 20).to(self.device)\n",
    "            )\n",
    "            \n",
    "            # 測試原始模型\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                # 預熱\n",
    "                for _ in range(10):\n",
    "                    _ = self.model(*test_inputs)\n",
    "                \n",
    "                # 計時\n",
    "                start_time = time.time()\n",
    "                for _ in range(num_runs):\n",
    "                    _ = self.model(*test_inputs)\n",
    "                torch.cuda.synchronize() if self.device == \"cuda\" else None\n",
    "                \n",
    "                original_time = (time.time() - start_time) / num_runs * 1000\n",
    "                results[\"original\"][batch_size] = original_time\n",
    "            \n",
    "            # 測試 TorchScript 模型\n",
    "            if self.scripted_model is not None:\n",
    "                with torch.no_grad():\n",
    "                    # 預熱\n",
    "                    for _ in range(10):\n",
    "                        _ = self.scripted_model(*test_inputs)\n",
    "                    \n",
    "                    # 計時\n",
    "                    start_time = time.time()\n",
    "                    for _ in range(num_runs):\n",
    "                        _ = self.scripted_model(*test_inputs)\n",
    "                    torch.cuda.synchronize() if self.device == \"cuda\" else None\n",
    "                    \n",
    "                    scripted_time = (time.time() - start_time) / num_runs * 1000\n",
    "                    results[\"torchscript\"][batch_size] = scripted_time\n",
    "            \n",
    "            print(f\"   📊 批量 {batch_size:2d}: 原始 {original_time:6.2f}ms\")\n",
    "            if batch_size in results[\"torchscript\"]:\n",
    "                speedup = original_time / results[\"torchscript\"][batch_size]\n",
    "                print(f\"   📊 批量 {batch_size:2d}: 優化 {results['torchscript'][batch_size]:6.2f}ms (加速 {speedup:.2f}x)\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_optimized_model(self, save_path: str):\n",
    "        \"\"\"\n",
    "        保存優化後的模型\n",
    "        \"\"\"\n",
    "        if self.scripted_model is None:\n",
    "            raise ValueError(\"沒有可保存的 TorchScript 模型\")\n",
    "        \n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"💾 保存優化模型到: {save_path}\")\n",
    "        torch.jit.save(self.scripted_model, str(save_path))\n",
    "        \n",
    "        # 驗證保存的模型\n",
    "        loaded_model = torch.jit.load(str(save_path))\n",
    "        print(f\"✅ 模型保存並驗證成功\")\n",
    "        \n",
    "        return str(save_path)\n",
    "\n",
    "# 執行 TorchScript 優化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"🖥️  使用設備: {device}\")\n",
    "print()\n",
    "\n",
    "optimizer = TorchScriptOptimizer(model, device)\n",
    "\n",
    "# 轉換到 TorchScript\n",
    "scripted_model = optimizer.convert_to_torchscript(method=\"trace\")\n",
    "print()\n",
    "\n",
    "# 推理優化\n",
    "optimized_model = optimizer.optimize_for_inference()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 性能基準測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行詳細的性能測試\n",
    "print(\"🏃‍♂️ 執行詳細性能基準測試...\")\n",
    "benchmark_results = optimizer.benchmark_performance(\n",
    "    batch_sizes=[1, 2, 4, 8, 16, 32] if device == \"cuda\" else [1, 2, 4],\n",
    "    num_runs=50 if device == \"cuda\" else 20\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"📈 性能提升分析:\")\n",
    "print(\"═\" * 60)\n",
    "\n",
    "total_speedup = 0\n",
    "count = 0\n",
    "\n",
    "for batch_size in benchmark_results[\"original\"].keys():\n",
    "    if batch_size in benchmark_results[\"torchscript\"]:\n",
    "        original_time = benchmark_results[\"original\"][batch_size]\n",
    "        optimized_time = benchmark_results[\"torchscript\"][batch_size]\n",
    "        speedup = original_time / optimized_time\n",
    "        \n",
    "        total_speedup += speedup\n",
    "        count += 1\n",
    "        \n",
    "        # 計算吞吐量\n",
    "        original_qps = 1000 * batch_size / original_time\n",
    "        optimized_qps = 1000 * batch_size / optimized_time\n",
    "        \n",
    "        print(f\"🔢 批量 {batch_size:2d}:\")\n",
    "        print(f\"   ⏱️  延遲: {original_time:6.2f}ms → {optimized_time:6.2f}ms\")\n",
    "        print(f\"   🚀 吞吐: {original_qps:6.1f} QPS → {optimized_qps:6.1f} QPS\")\n",
    "        print(f\"   📊 加速: {speedup:.2f}x\")\n",
    "        print()\n",
    "\n",
    "if count > 0:\n",
    "    avg_speedup = total_speedup / count\n",
    "    print(f\"🏆 平均加速比: {avg_speedup:.2f}x\")\n",
    "    print(f\"📈 性能提升: {(avg_speedup - 1) * 100:.1f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 保存優化模型\n",
    "model_save_path = \"/tmp/visa_fraud_detection_optimized.pt\"\n",
    "saved_path = optimizer.save_optimized_model(model_save_path)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 企業級 Triton Python Backend\n",
    "\n",
    "### 3.1 完整推理流水線實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬 Triton Python Backend 工具包\n",
    "class MockTritonPythonBackendUtils:\n",
    "    \"\"\"\n",
    "    模擬 Triton Python Backend 工具\n",
    "    (在實際部署中會使用 triton_python_backend_utils)\n",
    "    \"\"\"\n",
    "    \n",
    "    class Tensor:\n",
    "        def __init__(self, name: str, data: np.ndarray):\n",
    "            self.name = name\n",
    "            self.data = data\n",
    "        \n",
    "        def as_numpy(self):\n",
    "            return self.data\n",
    "    \n",
    "    class InferenceResponse:\n",
    "        def __init__(self, output_tensors: List, error=None):\n",
    "            self.output_tensors = output_tensors\n",
    "            self.error = error\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_input_tensor_by_name(request, name: str):\n",
    "        return request.get(name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def Tensor(name: str, data: np.ndarray):\n",
    "        return MockTritonPythonBackendUtils.Tensor(name, data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def InferenceResponse(output_tensors: List, error=None):\n",
    "        return MockTritonPythonBackendUtils.InferenceResponse(output_tensors, error)\n",
    "\n",
    "# 使用模擬的工具包\n",
    "pb_utils = MockTritonPythonBackendUtils\n",
    "\n",
    "class VISAFraudDetectionTritonModel:\n",
    "    \"\"\"\n",
    "    VISA 反詐騙檢測 Triton 模型\n",
    "    \n",
    "    企業級特性:\n",
    "    - 完整的預處理和後處理\n",
    "    - 錯誤處理和容錯機制\n",
    "    - 性能監控和日誌記錄\n",
    "    - 可解釋性輸出\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        初始化模型 - 在模型載入時執行一次\n",
    "        \"\"\"\n",
    "        print(\"🚀 初始化 VISA 反詐騙檢測模型...\")\n",
    "        \n",
    "        # 解析模型配置\n",
    "        self.model_config = json.loads(args.get('model_config', '{}'))\n",
    "        self.model_instance_name = args.get('model_instance_name', 'visa_fraud_0')\n",
    "        self.model_instance_device_id = args.get('model_instance_device_id', '0')\n",
    "        \n",
    "        # 設置設備\n",
    "        self.device = torch.device(\n",
    "            f\"cuda:{self.model_instance_device_id}\" \n",
    "            if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        # 載入優化後的模型\n",
    "        try:\n",
    "            print(f\"   📥 載入 TorchScript 模型...\")\n",
    "            # 在實際部署中，模型文件會在版本目錄中\n",
    "            # self.model = torch.jit.load(\"/models/visa_fraud/1/model.pt\")\n",
    "            \n",
    "            # 這裡使用之前優化的模型\n",
    "            self.model = optimized_model\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"   ✅ 模型載入成功\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 模型載入失敗: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # 初始化統計信息\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"total_inference_time\": 0.0,\n",
    "            \"high_risk_detections\": 0\n",
    "        }\n",
    "        \n",
    "        # 初始化風險閾值\n",
    "        self.risk_threshold = 0.7\n",
    "        \n",
    "        # 性能監控\n",
    "        self.performance_monitor = {\n",
    "            \"latency_p50\": [],\n",
    "            \"latency_p95\": [],\n",
    "            \"latency_p99\": [],\n",
    "            \"batch_sizes\": [],\n",
    "            \"timestamp\": []\n",
    "        }\n",
    "        \n",
    "        print(f\"   🎯 風險閾值: {self.risk_threshold}\")\n",
    "        print(f\"   🖥️  設備: {self.device}\")\n",
    "        print(f\"   📊 統計監控: 已啟用\")\n",
    "        print(f\"✅ 模型初始化完成\")\n",
    "    \n",
    "    def preprocess_inputs(self, raw_inputs: Dict) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        輸入預處理\n",
    "        \n",
    "        Args:\n",
    "            raw_inputs: 原始輸入數據\n",
    "        \n",
    "        Returns:\n",
    "            處理後的張量\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 提取輸入特徵\n",
    "            transaction_features = torch.from_numpy(\n",
    "                raw_inputs[\"transaction_features\"].as_numpy()\n",
    "            ).float().to(self.device)\n",
    "            \n",
    "            user_features = torch.from_numpy(\n",
    "                raw_inputs[\"user_features\"].as_numpy()\n",
    "            ).float().to(self.device)\n",
    "            \n",
    "            merchant_features = torch.from_numpy(\n",
    "                raw_inputs[\"merchant_features\"].as_numpy()\n",
    "            ).float().to(self.device)\n",
    "            \n",
    "            # 數據驗證\n",
    "            self._validate_input_shapes(transaction_features, user_features, merchant_features)\n",
    "            \n",
    "            # 特徵標準化 (在實際部署中會使用預計算的統計信息)\n",
    "            transaction_features = self._normalize_features(transaction_features, \"transaction\")\n",
    "            user_features = self._normalize_features(user_features, \"user\")\n",
    "            merchant_features = self._normalize_features(merchant_features, \"merchant\")\n",
    "            \n",
    "            return transaction_features, user_features, merchant_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"預處理失敗: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _validate_input_shapes(self, trans_feat, user_feat, merchant_feat):\n",
    "        \"\"\"\n",
    "        驗證輸入形狀\n",
    "        \"\"\"\n",
    "        if trans_feat.shape[-1] != 50:\n",
    "            raise ValueError(f\"交易特徵維度錯誤: 期望 50，得到 {trans_feat.shape[-1]}\")\n",
    "        \n",
    "        if user_feat.shape[-1] != 30:\n",
    "            raise ValueError(f\"用戶特徵維度錯誤: 期望 30，得到 {user_feat.shape[-1]}\")\n",
    "        \n",
    "        if merchant_feat.shape[-1] != 20:\n",
    "            raise ValueError(f\"商戶特徵維度錯誤: 期望 20，得到 {merchant_feat.shape[-1]}\")\n",
    "    \n",
    "    def _normalize_features(self, features: torch.Tensor, feature_type: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        特徵標準化\n",
    "        \"\"\"\n",
    "        # 在實際部署中，這些統計信息會從訓練數據中預計算\n",
    "        if feature_type == \"transaction\":\n",
    "            mean = torch.zeros(50).to(self.device)\n",
    "            std = torch.ones(50).to(self.device)\n",
    "        elif feature_type == \"user\":\n",
    "            mean = torch.zeros(30).to(self.device)\n",
    "            std = torch.ones(30).to(self.device)\n",
    "        else:  # merchant\n",
    "            mean = torch.zeros(20).to(self.device)\n",
    "            std = torch.ones(20).to(self.device)\n",
    "        \n",
    "        return (features - mean) / (std + 1e-8)\n",
    "    \n",
    "    def postprocess_outputs(\n",
    "        self, \n",
    "        logits: torch.Tensor, \n",
    "        risk_scores: torch.Tensor, \n",
    "        attention_weights: torch.Tensor\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        輸出後處理\n",
    "        \n",
    "        Returns:\n",
    "            處理後的輸出字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 計算預測概率\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            fraud_probabilities = probabilities[:, 1]  # 詐騙概率\n",
    "            \n",
    "            # 風險等級分類\n",
    "            risk_levels = self._classify_risk_levels(risk_scores.squeeze())\n",
    "            \n",
    "            # 決策邏輯\n",
    "            decisions = self._make_decisions(fraud_probabilities, risk_scores.squeeze())\n",
    "            \n",
    "            # 可解釋性分析\n",
    "            explanations = self._generate_explanations(attention_weights)\n",
    "            \n",
    "            # 統計更新\n",
    "            high_risk_count = (risk_scores.squeeze() > self.risk_threshold).sum().item()\n",
    "            self.stats[\"high_risk_detections\"] += high_risk_count\n",
    "            \n",
    "            return {\n",
    "                \"fraud_probabilities\": fraud_probabilities.cpu().numpy(),\n",
    "                \"risk_scores\": risk_scores.squeeze().cpu().numpy(),\n",
    "                \"risk_levels\": risk_levels,\n",
    "                \"decisions\": decisions,\n",
    "                \"explanations\": explanations,\n",
    "                \"attention_weights\": attention_weights.cpu().numpy()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"後處理失敗: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _classify_risk_levels(self, risk_scores: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        風險等級分類\n",
    "        \"\"\"\n",
    "        risk_levels = np.empty(risk_scores.shape[0], dtype='<U6')\n",
    "        \n",
    "        risk_scores_np = risk_scores.cpu().numpy()\n",
    "        \n",
    "        risk_levels[risk_scores_np <= 0.3] = 'LOW'\n",
    "        risk_levels[(risk_scores_np > 0.3) & (risk_scores_np <= 0.7)] = 'MEDIUM'\n",
    "        risk_levels[risk_scores_np > 0.7] = 'HIGH'\n",
    "        \n",
    "        return risk_levels\n",
    "    \n",
    "    def _make_decisions(self, fraud_probs: torch.Tensor, risk_scores: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        業務決策邏輯\n",
    "        \"\"\"\n",
    "        decisions = np.empty(fraud_probs.shape[0], dtype='<U8')\n",
    "        \n",
    "        fraud_probs_np = fraud_probs.cpu().numpy()\n",
    "        risk_scores_np = risk_scores.cpu().numpy()\n",
    "        \n",
    "        # 複合決策邏輯\n",
    "        high_risk = (fraud_probs_np > 0.8) | (risk_scores_np > 0.9)\n",
    "        medium_risk = ((fraud_probs_np > 0.5) & (fraud_probs_np <= 0.8)) | \\\n",
    "                     ((risk_scores_np > 0.5) & (risk_scores_np <= 0.9))\n",
    "        \n",
    "        decisions[high_risk] = 'BLOCK'\n",
    "        decisions[medium_risk] = 'REVIEW'\n",
    "        decisions[~(high_risk | medium_risk)] = 'APPROVE'\n",
    "        \n",
    "        return decisions\n",
    "    \n",
    "    def _generate_explanations(self, attention_weights: torch.Tensor) -> List[str]:\n",
    "        \"\"\"\n",
    "        生成可解釋性說明\n",
    "        \"\"\"\n",
    "        explanations = []\n",
    "        attention_np = attention_weights.cpu().numpy()\n",
    "        \n",
    "        for i in range(attention_np.shape[0]):\n",
    "            # 簡化的解釋邏輯\n",
    "            max_attention = np.max(attention_np[i])\n",
    "            \n",
    "            if max_attention > 0.8:\n",
    "                explanations.append(\"高度關注用戶和商戶特徵組合\")\n",
    "            elif max_attention > 0.5:\n",
    "                explanations.append(\"中等關注交易模式\")\n",
    "            else:\n",
    "                explanations.append(\"基於交易金額和頻率\")\n",
    "        \n",
    "        return explanations\n",
    "    \n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        執行推理 - 處理批量請求\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # 更新統計\n",
    "                self.stats[\"total_requests\"] += 1\n",
    "                \n",
    "                # 預處理\n",
    "                transaction_features, user_features, merchant_features = \\\n",
    "                    self.preprocess_inputs(request)\n",
    "                \n",
    "                batch_size = transaction_features.shape[0]\n",
    "                \n",
    "                # 推理\n",
    "                with torch.no_grad():\n",
    "                    logits, risk_scores, attention_weights = self.model(\n",
    "                        transaction_features, user_features, merchant_features\n",
    "                    )\n",
    "                \n",
    "                # 後處理\n",
    "                outputs = self.postprocess_outputs(logits, risk_scores, attention_weights)\n",
    "                \n",
    "                # 創建輸出張量\n",
    "                output_tensors = [\n",
    "                    pb_utils.Tensor(\"fraud_probabilities\", outputs[\"fraud_probabilities\"]),\n",
    "                    pb_utils.Tensor(\"risk_scores\", outputs[\"risk_scores\"]),\n",
    "                    pb_utils.Tensor(\"risk_levels\", outputs[\"risk_levels\"]),\n",
    "                    pb_utils.Tensor(\"decisions\", outputs[\"decisions\"]),\n",
    "                    pb_utils.Tensor(\"attention_weights\", outputs[\"attention_weights\"])\n",
    "                ]\n",
    "                \n",
    "                # 創建響應\n",
    "                inference_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=output_tensors\n",
    "                )\n",
    "                \n",
    "                # 記錄性能\n",
    "                inference_time = time.time() - start_time\n",
    "                self._record_performance(inference_time, batch_size)\n",
    "                \n",
    "                self.stats[\"successful_requests\"] += 1\n",
    "                self.stats[\"total_inference_time\"] += inference_time\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"推理執行失敗: {str(e)}\")\n",
    "                \n",
    "                # 創建錯誤響應\n",
    "                inference_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[],\n",
    "                    error=str(e)\n",
    "                )\n",
    "                \n",
    "                self.stats[\"failed_requests\"] += 1\n",
    "            \n",
    "            responses.append(inference_response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def _record_performance(self, inference_time: float, batch_size: int):\n",
    "        \"\"\"\n",
    "        記錄性能指標\n",
    "        \"\"\"\n",
    "        self.performance_monitor[\"latency_p50\"].append(inference_time)\n",
    "        self.performance_monitor[\"batch_sizes\"].append(batch_size)\n",
    "        self.performance_monitor[\"timestamp\"].append(time.time())\n",
    "        \n",
    "        # 保持最近 1000 次記錄\n",
    "        if len(self.performance_monitor[\"latency_p50\"]) > 1000:\n",
    "            for key in self.performance_monitor:\n",
    "                self.performance_monitor[key] = self.performance_monitor[key][-1000:]\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        清理資源 - 在模型卸載時執行\n",
    "        \"\"\"\n",
    "        print(\"🧹 清理 VISA 反詐騙檢測模型資源...\")\n",
    "        \n",
    "        # 打印統計信息\n",
    "        print(f\"📊 最終統計:\")\n",
    "        print(f\"   📈 總請求數: {self.stats['total_requests']}\")\n",
    "        print(f\"   ✅ 成功請求: {self.stats['successful_requests']}\")\n",
    "        print(f\"   ❌ 失敗請求: {self.stats['failed_requests']}\")\n",
    "        print(f\"   🚨 高風險檢測: {self.stats['high_risk_detections']}\")\n",
    "        \n",
    "        if self.stats['successful_requests'] > 0:\n",
    "            avg_latency = self.stats['total_inference_time'] / self.stats['successful_requests']\n",
    "            print(f\"   ⏱️  平均延遲: {avg_latency*1000:.2f} ms\")\n",
    "        \n",
    "        print(\"✅ 資源清理完成\")\n",
    "\n",
    "print(\"🏦 企業級 VISA 反詐騙檢測模型實現完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 模型測試和驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試企業級 Triton 模型\n",
    "print(\"🧪 測試企業級 Triton 模型...\")\n",
    "print()\n",
    "\n",
    "# 初始化模型\n",
    "visa_model = VISAFraudDetectionTritonModel()\n",
    "visa_model.initialize({\n",
    "    'model_config': json.dumps({\n",
    "        \"name\": \"visa_fraud_detection\",\n",
    "        \"backend\": \"pytorch\",\n",
    "        \"max_batch_size\": 32\n",
    "    }),\n",
    "    'model_instance_name': 'visa_fraud_0',\n",
    "    'model_instance_device_id': '0'\n",
    "})\n",
    "\n",
    "print()\n",
    "\n",
    "# 創建測試數據\n",
    "def create_test_requests(num_requests: int = 3, batch_size: int = 4):\n",
    "    \"\"\"\n",
    "    創建測試請求\n",
    "    \"\"\"\n",
    "    requests = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        # 模擬不同風險級別的交易\n",
    "        if i == 0:  # 正常交易\n",
    "            transaction_data = np.random.normal(0, 0.5, (batch_size, 50)).astype(np.float32)\n",
    "            user_data = np.random.normal(0, 0.3, (batch_size, 30)).astype(np.float32)\n",
    "            merchant_data = np.random.normal(0, 0.2, (batch_size, 20)).astype(np.float32)\n",
    "        elif i == 1:  # 中等風險交易\n",
    "            transaction_data = np.random.normal(1, 0.8, (batch_size, 50)).astype(np.float32)\n",
    "            user_data = np.random.normal(0.5, 0.5, (batch_size, 30)).astype(np.float32)\n",
    "            merchant_data = np.random.normal(0.3, 0.4, (batch_size, 20)).astype(np.float32)\n",
    "        else:  # 高風險交易\n",
    "            transaction_data = np.random.normal(2, 1.2, (batch_size, 50)).astype(np.float32)\n",
    "            user_data = np.random.normal(1, 0.8, (batch_size, 30)).astype(np.float32)\n",
    "            merchant_data = np.random.normal(0.8, 0.6, (batch_size, 20)).astype(np.float32)\n",
    "        \n",
    "        request = {\n",
    "            \"transaction_features\": pb_utils.Tensor(\"transaction_features\", transaction_data),\n",
    "            \"user_features\": pb_utils.Tensor(\"user_features\", user_data),\n",
    "            \"merchant_features\": pb_utils.Tensor(\"merchant_features\", merchant_data)\n",
    "        }\n",
    "        \n",
    "        requests.append(request)\n",
    "    \n",
    "    return requests\n",
    "\n",
    "# 生成測試請求\n",
    "test_requests = create_test_requests(num_requests=3, batch_size=4)\n",
    "\n",
    "print(\"📊 執行推理測試...\")\n",
    "print()\n",
    "\n",
    "# 執行推理\n",
    "responses = visa_model.execute(test_requests)\n",
    "\n",
    "# 分析結果\n",
    "for i, response in enumerate(responses):\n",
    "    if response.error:\n",
    "        print(f\"❌ 請求 {i+1} 失敗: {response.error}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"✅ 請求 {i+1} 結果:\")\n",
    "    \n",
    "    # 提取輸出\n",
    "    fraud_probs = response.output_tensors[0].as_numpy()\n",
    "    risk_scores = response.output_tensors[1].as_numpy()\n",
    "    risk_levels = response.output_tensors[2].as_numpy()\n",
    "    decisions = response.output_tensors[3].as_numpy()\n",
    "    \n",
    "    print(f\"   📊 批量大小: {len(fraud_probs)}\")\n",
    "    \n",
    "    for j in range(len(fraud_probs)):\n",
    "        print(f\"   🔍 交易 {j+1}:\")\n",
    "        print(f\"      🎯 詐騙概率: {fraud_probs[j]:.3f}\")\n",
    "        print(f\"      📊 風險評分: {risk_scores[j]:.3f}\")\n",
    "        print(f\"      🏷️  風險等級: {risk_levels[j]}\")\n",
    "        print(f\"      ⚖️  決策: {decisions[j]}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# 顯示最終統計\n",
    "print(\"📈 模型性能統計:\")\n",
    "success_rate = visa_model.stats['successful_requests'] / visa_model.stats['total_requests'] * 100\n",
    "avg_latency = visa_model.stats['total_inference_time'] / visa_model.stats['successful_requests'] * 1000\n",
    "\n",
    "print(f\"   ✅ 成功率: {success_rate:.1f}%\")\n",
    "print(f\"   ⏱️  平均延遲: {avg_latency:.2f} ms\")\n",
    "print(f\"   🚨 高風險檢測: {visa_model.stats['high_risk_detections']} 次\")\n",
    "print()\n",
    "\n",
    "# 清理資源\n",
    "visa_model.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 高級優化技術\n",
    "\n",
    "### 4.1 批處理和記憶體優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedBatchingOptimizer:\n",
    "    \"\"\"\n",
    "    高級批處理優化器\n",
    "    \n",
    "    企業級功能:\n",
    "    - 動態批處理調整\n",
    "    - 記憶體使用監控\n",
    "    - 負載平衡優化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "        self.batch_stats = {\n",
    "            \"optimal_batch_size\": 8,\n",
    "            \"max_batch_size\": 32,\n",
    "            \"min_batch_size\": 1,\n",
    "            \"memory_threshold\": 0.8  # 80% GPU 記憶體使用率閾值\n",
    "        }\n",
    "    \n",
    "    def analyze_batch_performance(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        batch_sizes: List[int] = None\n",
    "    ) -> Dict[int, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        分析不同批量大小的性能\n",
    "        \n",
    "        Returns:\n",
    "            {batch_size: {\"latency\": ms, \"throughput\": qps, \"memory\": gb}}\n",
    "        \"\"\"\n",
    "        \n",
    "        if batch_sizes is None:\n",
    "            batch_sizes = [1, 2, 4, 8, 16, 32, 64] if self.device == \"cuda\" else [1, 2, 4, 8]\n",
    "        \n",
    "        print(\"📊 分析批處理性能...\")\n",
    "        print(f\"   🔢 測試批量大小: {batch_sizes}\")\n",
    "        print()\n",
    "        \n",
    "        results = {}\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            try:\n",
    "                # 測試數據\n",
    "                test_inputs = (\n",
    "                    torch.randn(batch_size, 50).to(self.device),\n",
    "                    torch.randn(batch_size, 30).to(self.device),\n",
    "                    torch.randn(batch_size, 20).to(self.device)\n",
    "                )\n",
    "                \n",
    "                # 記憶體使用測量\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                # 性能測試\n",
    "                latencies = []\n",
    "                num_runs = 50\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # 預熱\n",
    "                    for _ in range(5):\n",
    "                        _ = model(*test_inputs)\n",
    "                    \n",
    "                    # 計時\n",
    "                    for _ in range(num_runs):\n",
    "                        start_time = time.time()\n",
    "                        _ = model(*test_inputs)\n",
    "                        if self.device == \"cuda\":\n",
    "                            torch.cuda.synchronize()\n",
    "                        latencies.append(time.time() - start_time)\n",
    "                \n",
    "                # 計算統計\n",
    "                avg_latency = np.mean(latencies) * 1000  # 轉換為毫秒\n",
    "                throughput = batch_size / (avg_latency / 1000)  # QPS\n",
    "                \n",
    "                # 記憶體使用\n",
    "                if self.device == \"cuda\":\n",
    "                    memory_used = torch.cuda.max_memory_allocated() / (1024**3)  # GB\n",
    "                else:\n",
    "                    memory_used = 0.0\n",
    "                \n",
    "                results[batch_size] = {\n",
    "                    \"latency\": avg_latency,\n",
    "                    \"throughput\": throughput,\n",
    "                    \"memory\": memory_used\n",
    "                }\n",
    "                \n",
    "                print(f\"   📊 批量 {batch_size:2d}: {avg_latency:6.2f}ms, {throughput:6.1f} QPS, {memory_used:.2f}GB\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"   ❌ 批量 {batch_size:2d}: 記憶體不足\")\n",
    "                    break\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def recommend_optimal_batch_size(\n",
    "        self, \n",
    "        performance_results: Dict[int, Dict[str, float]],\n",
    "        optimization_target: str = \"throughput\"  # \"throughput\" 或 \"latency\"\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        推薦最佳批量大小\n",
    "        \n",
    "        Args:\n",
    "            performance_results: 性能測試結果\n",
    "            optimization_target: 優化目標\n",
    "        \n",
    "        Returns:\n",
    "            推薦的批量大小\n",
    "        \"\"\"\n",
    "        \n",
    "        if not performance_results:\n",
    "            return self.batch_stats[\"optimal_batch_size\"]\n",
    "        \n",
    "        print(f\"🎯 推薦最佳批量大小 (優化目標: {optimization_target})...\")\n",
    "        \n",
    "        if optimization_target == \"throughput\":\n",
    "            # 尋找最高吞吐量\n",
    "            best_batch_size = max(\n",
    "                performance_results.keys(), \n",
    "                key=lambda x: performance_results[x][\"throughput\"]\n",
    "            )\n",
    "            best_value = performance_results[best_batch_size][\"throughput\"]\n",
    "            print(f\"   🚀 最高吞吐量: 批量 {best_batch_size} ({best_value:.1f} QPS)\")\n",
    "            \n",
    "        else:  # latency\n",
    "            # 尋找最低延遲\n",
    "            best_batch_size = min(\n",
    "                performance_results.keys(), \n",
    "                key=lambda x: performance_results[x][\"latency\"]\n",
    "            )\n",
    "            best_value = performance_results[best_batch_size][\"latency\"]\n",
    "            print(f\"   ⚡ 最低延遲: 批量 {best_batch_size} ({best_value:.2f} ms)\")\n",
    "        \n",
    "        # 檢查記憶體約束\n",
    "        if self.device == \"cuda\":\n",
    "            memory_usage = performance_results[best_batch_size][\"memory\"]\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            memory_ratio = memory_usage / total_memory\n",
    "            \n",
    "            if memory_ratio > self.batch_stats[\"memory_threshold\"]:\n",
    "                print(f\"   ⚠️  記憶體使用過高 ({memory_ratio:.1%})，建議降低批量大小\")\n",
    "                \n",
    "                # 尋找記憶體使用合理的最大批量\n",
    "                for batch_size in sorted(performance_results.keys(), reverse=True):\n",
    "                    mem_ratio = performance_results[batch_size][\"memory\"] / total_memory\n",
    "                    if mem_ratio <= self.batch_stats[\"memory_threshold\"]:\n",
    "                        best_batch_size = batch_size\n",
    "                        print(f\"   ✅ 調整後推薦: 批量 {best_batch_size} (記憶體使用: {mem_ratio:.1%})\")\n",
    "                        break\n",
    "        \n",
    "        return best_batch_size\n",
    "    \n",
    "    def generate_dynamic_batching_config(\n",
    "        self, \n",
    "        optimal_batch_size: int,\n",
    "        target_latency_ms: float = 50\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        生成動態批處理配置\n",
    "        \n",
    "        Args:\n",
    "            optimal_batch_size: 最佳批量大小\n",
    "            target_latency_ms: 目標延遲（毫秒）\n",
    "        \n",
    "        Returns:\n",
    "            Triton 動態批處理配置\n",
    "        \"\"\"\n",
    "        \n",
    "        # 計算等待時間（微秒）\n",
    "        max_queue_delay_microseconds = min(int(target_latency_ms * 100), 500)\n",
    "        \n",
    "        # 生成偏好批量大小列表\n",
    "        preferred_sizes = []\n",
    "        for size in [1, 2, 4, 8, 16, 32]:\n",
    "            if size <= optimal_batch_size:\n",
    "                preferred_sizes.append(size)\n",
    "        \n",
    "        if optimal_batch_size not in preferred_sizes:\n",
    "            preferred_sizes.append(optimal_batch_size)\n",
    "        \n",
    "        config = {\n",
    "            \"dynamic_batching\": {\n",
    "                \"enabled\": True,\n",
    "                \"max_queue_delay_microseconds\": max_queue_delay_microseconds,\n",
    "                \"preferred_batch_size\": sorted(preferred_sizes),\n",
    "                \"max_batch_size\": optimal_batch_size\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"⚙️  動態批處理配置:\")\n",
    "        print(f\"   🕐 最大等待時間: {max_queue_delay_microseconds} μs\")\n",
    "        print(f\"   🔢 偏好批量大小: {preferred_sizes}\")\n",
    "        print(f\"   📊 最大批量大小: {optimal_batch_size}\")\n",
    "        \n",
    "        return config\n",
    "\n",
    "# 執行批處理優化分析\n",
    "print(\"⚡ 執行高級批處理優化分析...\")\n",
    "print()\n",
    "\n",
    "batch_optimizer = AdvancedBatchingOptimizer(device)\n",
    "\n",
    "# 分析批處理性能\n",
    "perf_results = batch_optimizer.analyze_batch_performance(\n",
    "    model, \n",
    "    batch_sizes=[1, 2, 4, 8, 16] if device == \"cuda\" else [1, 2, 4]\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# 推薦最佳配置\n",
    "optimal_throughput = batch_optimizer.recommend_optimal_batch_size(\n",
    "    perf_results, \"throughput\"\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "optimal_latency = batch_optimizer.recommend_optimal_batch_size(\n",
    "    perf_results, \"latency\"\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# 生成配置\n",
    "throughput_config = batch_optimizer.generate_dynamic_batching_config(\n",
    "    optimal_throughput, target_latency_ms=30\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"📋 企業級批處理配置建議:\")\n",
    "print(json.dumps(throughput_config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 企業級監控和日誌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "class EnterpriseMonitoringSystem:\n",
    "    \"\"\"\n",
    "    企業級監控系統\n",
    "    \n",
    "    功能:\n",
    "    - 實時性能監控\n",
    "    - 異常檢測和告警\n",
    "    - 詳細日誌記錄\n",
    "    - 商業指標追蹤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, buffer_size: int = 1000):\n",
    "        self.model_name = model_name\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        # 性能指標緩衝區\n",
    "        self.metrics_buffer = {\n",
    "            \"latency\": deque(maxlen=buffer_size),\n",
    "            \"throughput\": deque(maxlen=buffer_size),\n",
    "            \"memory_usage\": deque(maxlen=buffer_size),\n",
    "            \"error_rate\": deque(maxlen=buffer_size),\n",
    "            \"timestamp\": deque(maxlen=buffer_size)\n",
    "        }\n",
    "        \n",
    "        # 商業指標\n",
    "        self.business_metrics = {\n",
    "            \"total_transactions\": 0,\n",
    "            \"blocked_transactions\": 0,\n",
    "            \"reviewed_transactions\": 0,\n",
    "            \"approved_transactions\": 0,\n",
    "            \"false_positive_rate\": 0.0,\n",
    "            \"detection_accuracy\": 0.0\n",
    "        }\n",
    "        \n",
    "        # 告警閾值\n",
    "        self.alert_thresholds = {\n",
    "            \"max_latency_ms\": 100,\n",
    "            \"min_throughput_qps\": 100,\n",
    "            \"max_error_rate\": 0.01,  # 1%\n",
    "            \"max_memory_usage_gb\": 8.0,\n",
    "            \"max_false_positive_rate\": 0.05  # 5%\n",
    "        }\n",
    "        \n",
    "        # 線程安全鎖\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        print(f\"📊 企業級監控系統已啟動 - 模型: {model_name}\")\n",
    "    \n",
    "    def record_inference_metrics(\n",
    "        self, \n",
    "        latency_ms: float,\n",
    "        batch_size: int,\n",
    "        memory_usage_gb: float = 0.0,\n",
    "        error_occurred: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        記錄推理指標\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            timestamp = time.time()\n",
    "            throughput = batch_size / (latency_ms / 1000)  # QPS\n",
    "            \n",
    "            self.metrics_buffer[\"latency\"].append(latency_ms)\n",
    "            self.metrics_buffer[\"throughput\"].append(throughput)\n",
    "            self.metrics_buffer[\"memory_usage\"].append(memory_usage_gb)\n",
    "            self.metrics_buffer[\"error_rate\"].append(1.0 if error_occurred else 0.0)\n",
    "            self.metrics_buffer[\"timestamp\"].append(timestamp)\n",
    "            \n",
    "            # 檢查告警\n",
    "            self._check_alerts(latency_ms, throughput, memory_usage_gb, error_occurred)\n",
    "    \n",
    "    def record_business_metrics(\n",
    "        self,\n",
    "        decisions: List[str],\n",
    "        ground_truth: List[bool] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        記錄商業指標\n",
    "        \n",
    "        Args:\n",
    "            decisions: 模型決策列表 ['APPROVE', 'BLOCK', 'REVIEW']\n",
    "            ground_truth: 真實標籤 (可選，用於計算準確性)\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            self.business_metrics[\"total_transactions\"] += len(decisions)\n",
    "            \n",
    "            for decision in decisions:\n",
    "                if decision == \"BLOCK\":\n",
    "                    self.business_metrics[\"blocked_transactions\"] += 1\n",
    "                elif decision == \"REVIEW\":\n",
    "                    self.business_metrics[\"reviewed_transactions\"] += 1\n",
    "                elif decision == \"APPROVE\":\n",
    "                    self.business_metrics[\"approved_transactions\"] += 1\n",
    "            \n",
    "            # 計算準確性指標 (如果有真實標籤)\n",
    "            if ground_truth is not None:\n",
    "                self._calculate_accuracy_metrics(decisions, ground_truth)\n",
    "    \n",
    "    def _calculate_accuracy_metrics(self, decisions: List[str], ground_truth: List[bool]):\n",
    "        \"\"\"\n",
    "        計算準確性指標\n",
    "        \"\"\"\n",
    "        if len(decisions) != len(ground_truth):\n",
    "            return\n",
    "        \n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_negatives = 0\n",
    "        \n",
    "        for decision, is_fraud in zip(decisions, ground_truth):\n",
    "            predicted_fraud = decision in [\"BLOCK\", \"REVIEW\"]\n",
    "            \n",
    "            if predicted_fraud and is_fraud:\n",
    "                true_positives += 1\n",
    "            elif predicted_fraud and not is_fraud:\n",
    "                false_positives += 1\n",
    "            elif not predicted_fraud and not is_fraud:\n",
    "                true_negatives += 1\n",
    "            elif not predicted_fraud and is_fraud:\n",
    "                false_negatives += 1\n",
    "        \n",
    "        # 更新指標\n",
    "        if (true_positives + false_negatives) > 0:\n",
    "            detection_rate = true_positives / (true_positives + false_negatives)\n",
    "            self.business_metrics[\"detection_accuracy\"] = detection_rate\n",
    "        \n",
    "        if (false_positives + true_negatives) > 0:\n",
    "            false_positive_rate = false_positives / (false_positives + true_negatives)\n",
    "            self.business_metrics[\"false_positive_rate\"] = false_positive_rate\n",
    "    \n",
    "    def _check_alerts(self, latency_ms: float, throughput: float, memory_gb: float, error: bool):\n",
    "        \"\"\"\n",
    "        檢查告警條件\n",
    "        \"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        if latency_ms > self.alert_thresholds[\"max_latency_ms\"]:\n",
    "            alerts.append(f\"🚨 高延遲告警: {latency_ms:.1f}ms > {self.alert_thresholds['max_latency_ms']}ms\")\n",
    "        \n",
    "        if throughput < self.alert_thresholds[\"min_throughput_qps\"]:\n",
    "            alerts.append(f\"🚨 低吞吐量告警: {throughput:.1f} QPS < {self.alert_thresholds['min_throughput_qps']} QPS\")\n",
    "        \n",
    "        if memory_gb > self.alert_thresholds[\"max_memory_usage_gb\"]:\n",
    "            alerts.append(f\"🚨 高記憶體使用告警: {memory_gb:.2f}GB > {self.alert_thresholds['max_memory_usage_gb']}GB\")\n",
    "        \n",
    "        if error:\n",
    "            alerts.append(f\"🚨 推理錯誤告警: 模型執行失敗\")\n",
    "        \n",
    "        # 檢查錯誤率\n",
    "        if len(self.metrics_buffer[\"error_rate\"]) >= 10:\n",
    "            recent_error_rate = np.mean(list(self.metrics_buffer[\"error_rate\"])[-10:])\n",
    "            if recent_error_rate > self.alert_thresholds[\"max_error_rate\"]:\n",
    "                alerts.append(f\"🚨 高錯誤率告警: {recent_error_rate:.1%} > {self.alert_thresholds['max_error_rate']:.1%}\")\n",
    "        \n",
    "        # 打印告警\n",
    "        for alert in alerts:\n",
    "            print(alert)\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        獲取性能摘要\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            if not self.metrics_buffer[\"latency\"]:\n",
    "                return {\"status\": \"no_data\"}\n",
    "            \n",
    "            latencies = list(self.metrics_buffer[\"latency\"])\n",
    "            throughputs = list(self.metrics_buffer[\"throughput\"])\n",
    "            memory_usage = list(self.metrics_buffer[\"memory_usage\"])\n",
    "            error_rates = list(self.metrics_buffer[\"error_rate\"])\n",
    "            \n",
    "            summary = {\n",
    "                \"model_name\": self.model_name,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"performance_metrics\": {\n",
    "                    \"latency_ms\": {\n",
    "                        \"mean\": np.mean(latencies),\n",
    "                        \"p50\": np.percentile(latencies, 50),\n",
    "                        \"p95\": np.percentile(latencies, 95),\n",
    "                        \"p99\": np.percentile(latencies, 99),\n",
    "                        \"max\": np.max(latencies)\n",
    "                    },\n",
    "                    \"throughput_qps\": {\n",
    "                        \"mean\": np.mean(throughputs),\n",
    "                        \"min\": np.min(throughputs),\n",
    "                        \"max\": np.max(throughputs)\n",
    "                    },\n",
    "                    \"memory_usage_gb\": {\n",
    "                        \"mean\": np.mean(memory_usage),\n",
    "                        \"max\": np.max(memory_usage)\n",
    "                    },\n",
    "                    \"error_rate\": np.mean(error_rates)\n",
    "                },\n",
    "                \"business_metrics\": self.business_metrics.copy(),\n",
    "                \"sample_count\": len(latencies)\n",
    "            }\n",
    "            \n",
    "            return summary\n",
    "    \n",
    "    def print_dashboard(self):\n",
    "        \"\"\"\n",
    "        打印監控儀表板\n",
    "        \"\"\"\n",
    "        summary = self.get_performance_summary()\n",
    "        \n",
    "        if summary.get(\"status\") == \"no_data\":\n",
    "            print(\"📊 監控儀表板: 暫無數據\")\n",
    "            return\n",
    "        \n",
    "        print(\"📊 企業級監控儀表板\")\n",
    "        print(\"═\" * 60)\n",
    "        print(f\"🏷️  模型: {summary['model_name']}\")\n",
    "        print(f\"🕐 時間: {summary['timestamp']}\")\n",
    "        print(f\"📈 樣本數: {summary['sample_count']}\")\n",
    "        print()\n",
    "        \n",
    "        # 性能指標\n",
    "        perf = summary['performance_metrics']\n",
    "        print(\"⚡ 性能指標:\")\n",
    "        print(f\"   📊 延遲: 平均 {perf['latency_ms']['mean']:.2f}ms\")\n",
    "        print(f\"   📊 延遲: P50 {perf['latency_ms']['p50']:.2f}ms, P95 {perf['latency_ms']['p95']:.2f}ms, P99 {perf['latency_ms']['p99']:.2f}ms\")\n",
    "        print(f\"   🚀 吞吐量: 平均 {perf['throughput_qps']['mean']:.1f} QPS (範圍: {perf['throughput_qps']['min']:.1f}-{perf['throughput_qps']['max']:.1f})\")\n",
    "        print(f\"   💾 記憶體: 平均 {perf['memory_usage_gb']['mean']:.2f}GB, 峰值 {perf['memory_usage_gb']['max']:.2f}GB\")\n",
    "        print(f\"   ❌ 錯誤率: {perf['error_rate']:.1%}\")\n",
    "        print()\n",
    "        \n",
    "        # 商業指標\n",
    "        biz = summary['business_metrics']\n",
    "        print(\"💼 商業指標:\")\n",
    "        print(f\"   📊 總交易數: {biz['total_transactions']:,}\")\n",
    "        print(f\"   🚫 攔截交易: {biz['blocked_transactions']:,}\")\n",
    "        print(f\"   👀 審核交易: {biz['reviewed_transactions']:,}\")\n",
    "        print(f\"   ✅ 通過交易: {biz['approved_transactions']:,}\")\n",
    "        \n",
    "        if biz['detection_accuracy'] > 0:\n",
    "            print(f\"   🎯 檢測準確率: {biz['detection_accuracy']:.1%}\")\n",
    "        \n",
    "        if biz['false_positive_rate'] > 0:\n",
    "            print(f\"   ⚠️  誤報率: {biz['false_positive_rate']:.1%}\")\n",
    "\n",
    "# 示例：企業級監控系統\n",
    "print(\"📊 企業級監控系統演示...\")\n",
    "print()\n",
    "\n",
    "monitor = EnterpriseMonitoringSystem(\"visa_fraud_detection_v2_prod\")\n",
    "\n",
    "# 模擬監控數據\n",
    "print(\"🔄 模擬性能數據收集...\")\n",
    "for i in range(20):\n",
    "    # 模擬不同性能場景\n",
    "    if i < 10:  # 正常性能\n",
    "        latency = np.random.normal(25, 5)\n",
    "        batch_size = np.random.choice([4, 8, 16])\n",
    "        memory = np.random.normal(3.2, 0.5)\n",
    "        error = False\n",
    "    else:  # 性能退化場景\n",
    "        latency = np.random.normal(45, 10)\n",
    "        batch_size = np.random.choice([2, 4, 8])\n",
    "        memory = np.random.normal(4.8, 0.8)\n",
    "        error = np.random.random() < 0.02  # 2% 錯誤率\n",
    "    \n",
    "    monitor.record_inference_metrics(\n",
    "        latency_ms=max(latency, 1),\n",
    "        batch_size=batch_size,\n",
    "        memory_usage_gb=max(memory, 0),\n",
    "        error_occurred=error\n",
    "    )\n",
    "    \n",
    "    # 模擬商業決策\n",
    "    decisions = np.random.choice(\n",
    "        ['APPROVE', 'BLOCK', 'REVIEW'], \n",
    "        size=batch_size, \n",
    "        p=[0.7, 0.2, 0.1]\n",
    "    )\n",
    "    \n",
    "    # 模擬真實標籤 (用於準確性計算)\n",
    "    ground_truth = [decision == 'BLOCK' for decision in decisions]\n",
    "    \n",
    "    monitor.record_business_metrics(decisions.tolist(), ground_truth)\n",
    "    \n",
    "    time.sleep(0.01)  # 模擬時間間隔\n",
    "\n",
    "print()\n",
    "\n",
    "# 顯示監控儀表板\n",
    "monitor.print_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 本章總結\n",
    "\n",
    "### 核心學習成果\n",
    "\n",
    "通過本實驗室，您已經掌握了：\n",
    "\n",
    "1. **🔧 PyTorch Backend 深度技能**\n",
    "   - 自定義推理邏輯實現\n",
    "   - TorchScript 優化和部署\n",
    "   - 企業級錯誤處理機制\n",
    "\n",
    "2. **⚡ 性能優化專業技能**\n",
    "   - 批處理策略優化\n",
    "   - 記憶體管理和資源控制\n",
    "   - 動態配置調整\n",
    "\n",
    "3. **🏢 企業級部署能力**\n",
    "   - 完整推理流水線設計\n",
    "   - 可解釋性和監控整合\n",
    "   - 商業指標追蹤\n",
    "\n",
    "4. **📊 監控和運維技能**\n",
    "   - 實時性能監控\n",
    "   - 異常檢測和告警\n",
    "   - 企業級儀表板設計\n",
    "\n",
    "### 性能提升成果\n",
    "\n",
    "通過 TorchScript 優化，實現了：\n",
    "- **推理加速**: 平均 1.5-2.5x 性能提升\n",
    "- **記憶體優化**: 減少 20-30% 記憶體使用\n",
    "- **吞吐量提升**: 支援更大批量處理\n",
    "\n",
    "### 企業級技能認證\n",
    "\n",
    "您現在具備了：\n",
    "- **VISA 級別**的高性能推理部署能力\n",
    "- **金融級別**的可靠性和監控技能\n",
    "- **生產環境**的運維和優化經驗\n",
    "\n",
    "### 下一步學習路徑\n",
    "\n",
    "在下一個實驗室 **Lab-2.1.4: Monitoring and Performance** 中，我們將：\n",
    "- 深入企業級監控系統設計\n",
    "- 實現自動化性能調優\n",
    "- 整合 Prometheus 和 Grafana\n",
    "- 建立完整的 SLA 監控體系\n",
    "\n",
    "---\n",
    "\n",
    "**🏆 恭喜！您已經完成了 PyTorch Backend 的企業級深度部署！**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}