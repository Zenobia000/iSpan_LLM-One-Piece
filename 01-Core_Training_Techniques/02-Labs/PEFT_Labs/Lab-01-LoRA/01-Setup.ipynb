{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1: LoRA & QLoRA - Fine-Tuning a Llama-2 Model\n",
        "\n",
        "**Goal:** This lab provides a deep dive into the most popular PEFT method, **Low-Rank Adaptation (LoRA)**, and its highly efficient variant, **QLoRA**.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Load a large language model (`Llama-2-7B`) in 4-bit precision.\n",
        "-   Understand and configure `LoraConfig` from the `peft` library in detail.\n",
        "-   Apply the LoRA adapter to the base model.\n",
        "-   Use the `transformers.Trainer` to fine-tune the model with LoRA.\n",
        "-   Perform inference with the fine-tuned LoRA model.\n",
        "-   Merge the LoRA adapter back into the base model for deployment.\n",
        "\n",
        "---\n",
        "## Notebook 1: Environment Setup\n",
        "\n",
        "This first notebook ensures your environment is ready for the lab. It will:\n",
        "1.  Check for GPU availability.\n",
        "2.  Install all required Python libraries from Hugging Face and the broader AI ecosystem.\n",
        "3.  Verify that the installations are successful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Check for GPU\n",
        "\n",
        "For fine-tuning large models (even with PEFT), a GPU is essential. The command below uses `nvidia-smi` to check for and display the status of any available NVIDIA GPUs. If this command fails or shows no devices, you may need to enable a GPU runtime (e.g., in Google Colab) or ensure your local environment has a correctly configured GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Aug 22 11:24:27 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA RTX A2000 12GB          Off |   00000000:01:00.0  On |                  Off |\n",
            "| 30%   36C    P8              6W /   70W |       4MiB /  12282MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Install Core Libraries\n",
        "\n",
        "Now, we will install the key Python libraries. We use `pip install -q` for a \"quiet\" installation, which reduces the verbosity of the output.\n",
        "\n",
        "-   **`transformers`**: The core Hugging Face library. It provides the `AutoModelForCausalLM` and `AutoTokenizer` classes for loading our base model, as well as the powerful `Trainer` API.\n",
        "-   **`peft`**: The Parameter-Efficient Fine-Tuning library. This provides the `LoraConfig` and other tools to create and manage our adapters.\n",
        "-   **`datasets`**: Used to easily load datasets from the Hugging Face Hub.\n",
        "-   **`accelerate`**: A library from Hugging Face that simplifies running PyTorch code across different hardware configurations (CPU, single GPU, multiple GPUs, TPUs). The `Trainer` uses it under the hood.\n",
        "-   **`bitsandbytes`**: This is a crucial library for quantization. It allows us to load our model in 4-bit precision (`load_in_4bit=True`), which is the key to making QLoRA work on consumer-grade hardware.\n",
        "-   **`sentencepiece` & `protobuf`**: These are common dependencies for the tokenizers used by many modern LLMs, including Llama-2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q transformers peft datasets accelerate bitsandbytes sentencepiece protobuf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Verify Installation\n",
        "\n",
        "Let's import the main libraries and print their versions to confirm that everything was installed correctly. If this cell runs without errors, your environment is ready to go!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0+cu128\n",
            "Transformers version: 4.55.2\n",
            "PEFT version: 0.17.0\n",
            "Datasets version: 4.0.0\n",
            "\n",
            "✅ Environment is set up correctly!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import peft\n",
        "import datasets\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"PEFT version: {peft.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "\n",
        "print(\"\\n✅ Environment is set up correctly!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM Engineering Course (Python 3.10)",
      "language": "python",
      "name": "llm-engineering-course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
