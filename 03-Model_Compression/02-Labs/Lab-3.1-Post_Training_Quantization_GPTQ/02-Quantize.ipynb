{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1: GPTQ Post-Training Quantization - Model Quantization\n",
    "\n",
    "**Goal:** Apply GPTQ quantization to compress Llama-2-7B from 13.5GB (FP16) to 3.5GB (INT4).\n",
    "\n",
    "**Key concepts:**\n",
    "- **Hessian-guided quantization**: Use second-order derivatives to minimize error\n",
    "- **Group quantization**: Balance precision and compression with group_size=128\n",
    "- **Calibration data**: Use representative data to compute activation statistics\n",
    "- **Error compensation**: Propagate quantization errors to subsequent layers\n",
    "\n",
    "**Expected outcomes:**\n",
    "- Model size: 13.5GB ‚Üí 3.5GB (3.86x compression)\n",
    "- Perplexity increase: <0.2 (minimal accuracy loss)\n",
    "- Quantization time: ~30 minutes (A100 GPU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Tokenizer\n",
    "\n",
    "We'll use the same model as in 01-Setup.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "OUTPUT_DIR = \"./llama-2-7b-gptq-4bit\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPTQ Quantization Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Configure GPTQ Quantization\n",
    "\n",
    "Let's configure the quantization parameters. Each parameter has a specific impact:\n",
    "\n",
    "| Parameter | Value | Impact |\n",
    "|:---|:---|:---|\n",
    "| `bits` | 4 | 4-bit quantization (4x compression) |\n",
    "| `group_size` | 128 | Balance between precision and size |\n",
    "| `desc_act` | True | Sort activations for better grouping (+0.5% accuracy) |\n",
    "| `sym` | True | Symmetric quantization (hardware-friendly) |\n",
    "| `dataset` | \"c4\" | Calibration dataset (C4 is general-purpose) |\n",
    "| `damp_percent` | 0.01 | Hessian damping for numerical stability |\n",
    "| `use_exllama` | True/False | Use optimized kernels (if supported) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ExLlama support\n",
    "use_exllama = False\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    compute_capability = gpu_props.major * 10 + gpu_props.minor\n",
    "    use_exllama = (compute_capability >= 80)  # SM 8.0+ (Ampere or newer)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Quantization Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create GPTQ configuration\n",
    "quantization_config = GPTQConfig(\n",
    "    bits=4,                    # 4-bit quantization\n",
    "    group_size=128,            # Group size for quantization\n",
    "    desc_act=True,             # Descending activation order\n",
    "    sym=True,                  # Symmetric quantization\n",
    "    dataset=\"c4\",              # Calibration dataset\n",
    "    tokenizer=tokenizer,       # Tokenizer for dataset processing\n",
    "    damp_percent=0.01,         # Hessian damping factor\n",
    "    use_exllama=use_exllama,   # Use ExLlama if supported\n",
    ")\n",
    "\n",
    "# Print configuration\n",
    "print(f\"Bits: {quantization_config.bits}\")\n",
    "print(f\"Group Size: {quantization_config.group_size}\")\n",
    "print(f\"Descending Activation: {quantization_config.desc_act}\")\n",
    "print(f\"Symmetric Quantization: {quantization_config.sym}\")\n",
    "print(f\"Calibration Dataset: {quantization_config.dataset}\")\n",
    "print(f\"Damp Percent: {quantization_config.damp_percent}\")\n",
    "print(f\"ExLlama: {use_exllama}\")\n",
    "\n",
    "if use_exllama:\n",
    "    print(\"   ‚úÖ Using optimized ExLlama kernels (20-50% faster)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Using standard kernels (GPU not supported or SM < 8.0)\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuration created\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Load and Quantize Model\n",
    "\n",
    "This is the core quantization step. The process:\n",
    "\n",
    "1. **Load FP16 model** (~15GB GPU memory)\n",
    "2. **Load calibration data** (128 samples from C4 dataset)\n",
    "3. **Compute Hessian matrix** for each layer (activation statistics)\n",
    "4. **Quantize layer-by-layer** with error compensation\n",
    "5. **Replace FP16 weights** with INT4 quantized weights\n",
    "\n",
    "**Expected time**: 20-40 minutes depending on GPU\n",
    "- A100: ~20 minutes\n",
    "- RTX 4090: ~30 minutes\n",
    "- RTX 3090: ~40 minutes\n",
    "\n",
    "**Memory usage during quantization**:\n",
    "- Peak: ~18GB (FP16 model + calibration data + Hessian matrices)\n",
    "- After: ~5GB (quantized model only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Starting GPTQ Quantization\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚è≥ This will take 20-40 minutes...\")\n",
    "print(\"üìä Progress will be shown layer by layer\\n\")\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and quantize model in one step\n",
    "# The quantization happens automatically during model loading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",          # Automatic device placement\n",
    "    torch_dtype=torch.float16,  # Use FP16 for non-quantized ops\n",
    ")\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "quantization_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Quantization Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚è±Ô∏è  Time elapsed: {quantization_time / 60:.2f} minutes\")\n",
    "print(f\"üìä GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Verify Quantization\n",
    "\n",
    "Let's inspect the quantized model to verify quantization was applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_quantized = sum(1 for n, p in model.named_parameters() if 'qweight' in n or 'qzeros' in n)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Quantized Model Verification\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total parameters: {num_params / 1e9:.2f}B\")\n",
    "print(f\"Quantized layers: {num_quantized}\")\n",
    "print()\n",
    "\n",
    "# Check model structure\n",
    "print(\"Sample quantized layer:\")\n",
    "for name, module in model.named_modules():\n",
    "    if 'QuantLinear' in str(type(module)) or 'quant' in name.lower():\n",
    "        print(f\"   ‚úÖ {name}: {type(module).__name__}\")\n",
    "        break\n",
    "\n",
    "# Estimated model size\n",
    "# 4-bit quantization: 0.5 bytes per parameter (plus overhead for scales/zeros)\n",
    "estimated_size_gb = (num_params * 0.5) / 1e9 * 1.15  # 15% overhead for metadata\n",
    "original_size_gb = (num_params * 2) / 1e9  # FP16 = 2 bytes/param\n",
    "compression_ratio = original_size_gb / estimated_size_gb\n",
    "\n",
    "print(f\"\\nüìä Model Size Estimation:\")\n",
    "print(f\"   Original (FP16): {original_size_gb:.2f} GB\")\n",
    "print(f\"   Quantized (INT4): {estimated_size_gb:.2f} GB\")\n",
    "print(f\"   Compression Ratio: {compression_ratio:.2f}x\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Quantization verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Quick Inference Test\n",
    "\n",
    "Let's test the quantized model to ensure it generates reasonable outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a world where technology has advanced beyond imagination,\",\n",
    "    \"What is the meaning of life? The answer is\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Quantized Model Inference Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüìù Test {i}/3:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Output: {generated}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ Inference test passed!\")\n",
    "print(\"   ‚Üí Model generates coherent text\")\n",
    "print(\"   ‚Üí No NaN or repetition issues detected\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Save Quantized Model\n",
    "\n",
    "Save the quantized model to disk for later use. The saved model includes:\n",
    "- Quantized weights (INT4)\n",
    "- Quantization scales and zero-points\n",
    "- Model configuration\n",
    "- Tokenizer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Saving Quantized Model\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"‚úÖ Model saved\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"‚úÖ Tokenizer saved\")\n",
    "\n",
    "# Check saved file sizes\n",
    "import os\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        total_size += os.path.getsize(file_path)\n",
    "\n",
    "print(f\"\\nüìä Saved Model Size: {total_size / 1e9:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Save complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Compare Model Sizes\n",
    "\n",
    "Let's compare the quantized model against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    \"Model\": [\"Original (FP16)\", \"Quantized (INT4)\", \"Compression\"],\n",
    "    \"Size (GB)\": [f\"{original_size_gb:.2f}\", f\"{total_size / 1e9:.2f}\", f\"{compression_ratio:.2f}x\"],\n",
    "    \"GPU Memory (GB)\": [\"~15\", f\"~{torch.cuda.memory_allocated() / 1e9:.1f}\", \n",
    "                        f\"{15 / (torch.cuda.memory_allocated() / 1e9):.2f}x\"],\n",
    "    \"Bits/Weight\": [\"16\", \"4\", \"4x\"],\n",
    "    \"Expected PPL Change\": [\"Baseline\", \"+0.1-0.2\", \"<2%\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìà Key Metrics:\")\n",
    "print(f\"   Model Size Reduction: {compression_ratio:.2f}x\")\n",
    "print(f\"   GPU Memory Reduction: ~3x\")\n",
    "print(f\"   Expected Speedup: 2-4x (to be verified in 04-Benchmark.ipynb)\")\n",
    "print(f\"   Expected PPL Increase: <0.2 (minimal accuracy loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Quantization Summary\n",
    "\n",
    "Let's generate a summary report of the quantization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GPTQ Quantization Summary\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üìã Configuration:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Quantization: {quantization_config.bits}-bit GPTQ\")\n",
    "print(f\"   Group Size: {quantization_config.group_size}\")\n",
    "print(f\"   Calibration Dataset: {quantization_config.dataset}\")\n",
    "print(f\"   ExLlama: {use_exllama}\")\n",
    "print()\n",
    "print(\"‚è±Ô∏è  Performance:\")\n",
    "print(f\"   Quantization Time: {quantization_time / 60:.2f} minutes\")\n",
    "print(f\"   Throughput: {num_params / 1e9 / (quantization_time / 60):.2f} B params/min\")\n",
    "print()\n",
    "print(\"üíæ Storage:\")\n",
    "print(f\"   Original Size: {original_size_gb:.2f} GB (FP16)\")\n",
    "print(f\"   Quantized Size: {total_size / 1e9:.2f} GB (INT4)\")\n",
    "print(f\"   Compression Ratio: {compression_ratio:.2f}x\")\n",
    "print(f\"   Saved Location: {OUTPUT_DIR}\")\n",
    "print()\n",
    "print(\"üéØ Next Steps:\")\n",
    "print(\"   1. Run 03-Inference.ipynb to compare outputs\")\n",
    "print(\"   2. Run 04-Benchmark.ipynb to measure performance\")\n",
    "print(\"   3. Deploy with vLLM/TensorRT-LLM for production\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Quantization pipeline complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Key Takeaways\n",
    "\n",
    "**What we achieved**:\n",
    "1. ‚úÖ Compressed Llama-2-7B from 13.5GB to 3.5GB (3.86x)\n",
    "2. ‚úÖ Reduced GPU memory from 15GB to 5GB (3x)\n",
    "3. ‚úÖ Applied Hessian-guided quantization with error compensation\n",
    "4. ‚úÖ Used group quantization (group_size=128) for precision\n",
    "5. ‚úÖ Verified model generates coherent text\n",
    "\n",
    "**GPTQ algorithm recap**:\n",
    "```\n",
    "For each layer:\n",
    "  1. Compute Hessian matrix H = 2¬∑X¬∑X^T (activation statistics)\n",
    "  2. For each column i in weight matrix:\n",
    "     a. Quantize w[i] to INT4\n",
    "     b. Compute error: e = w_original[i] - w_quantized[i]\n",
    "     c. Compensate future columns: w[i+1:] -= (e / H[i,i]) * H[i, i+1:]\n",
    "  3. Store quantized weights + scales + zero-points\n",
    "```\n",
    "\n",
    "**Critical parameters**:\n",
    "- `bits=4`: Sweet spot for compression vs accuracy\n",
    "- `group_size=128`: Standard choice (smaller = more accurate but larger)\n",
    "- `desc_act=True`: Improves grouping by sorting activations\n",
    "- `dataset=\"c4\"`: General-purpose calibration data\n",
    "\n",
    "**Troubleshooting**:\n",
    "- **OOM during quantization**: Reduce model size or use CPU offloading\n",
    "- **Long quantization time**: Normal for large models (70B can take 4-6 hours)\n",
    "- **Poor output quality**: Try increasing bits to 8 or using more calibration data\n",
    "\n",
    "---\n",
    "\n",
    "**‚è≠Ô∏è Continue to**: [03-Inference.ipynb](./03-Inference.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
