# Lab 2: Adapter Layers - ç¶“å…¸çš„æ¨¡çµ„åŒ–åƒæ•¸é«˜æ•ˆå¾®èª¿

## æ¦‚è¿°

**Adapter Layers** æ˜¯ PEFT é ˜åŸŸçš„é–‹å‰µæ€§æ–¹æ³•ï¼Œé€éåœ¨é è¨“ç·´æ¨¡å‹ä¸­æ’å…¥å°å‹å¯è¨“ç·´æ¨¡çµ„ä¾†å¯¦ç¾é«˜æ•ˆå¾®èª¿ã€‚æœ¬å¯¦é©—å°‡æ·±å…¥æ¢è¨ Adapter çš„æ ¸å¿ƒåŸç†ã€æ¨¡çµ„åŒ–è¨­è¨ˆæ€æƒ³ï¼Œä»¥åŠåœ¨åºåˆ—åˆ†é¡ä»»å‹™ä¸­çš„å¯¦éš›æ‡‰ç”¨ã€‚

![Adapter Tuning åŸç†ç¤ºæ„åœ–](https://pic3.zhimg.com/v2-c7a60e5065a325b848f48cb8031eb26e_1440w.jpg)

---

## 1. æŠ€è¡“èƒŒæ™¯èˆ‡è¨­è¨ˆå‹•æ©Ÿ

### 1.1 å‚³çµ±å¾®èª¿é¢è‡¨çš„æŒ‘æˆ°

åœ¨ Adapter Tuning èª•ç”Ÿä¹‹å‰ï¼Œæ·±åº¦å­¸ç¿’å¾®èª¿é ˜åŸŸé¢è‡¨åš´å³»çš„å·¥ç¨‹æŒ‘æˆ°ï¼š

- **å…¨åƒæ•¸å¾®èª¿æˆæœ¬é«˜æ˜‚**ï¼šæ¯å€‹ä¸‹æ¸¸ä»»å‹™éœ€è¦è¨“ç·´å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬ï¼Œå„²å­˜èˆ‡éƒ¨ç½²æˆæœ¬æ¥µé«˜
- **åƒæ•¸å†—é¤˜å•é¡Œåš´é‡**ï¼šå¤§éƒ¨åˆ†é è¨“ç·´åƒæ•¸å°ç‰¹å®šä»»å‹™çš„è²¢ç»æœ‰é™ï¼Œå­˜åœ¨å¤§é‡å†—é¤˜
- **å¤šä»»å‹™éƒ¨ç½²è¤‡é›œ**ï¼šä¸åŒä»»å‹™éœ€è¦ç¨ç«‹çš„å®Œæ•´æ¨¡å‹ï¼Œè³‡æºåˆ©ç”¨æ•ˆç‡ä½
- **ç½é›£æ€§éºå¿˜é¢¨éšª**ï¼šå…¨é‡å¾®èª¿å®¹æ˜“ç ´å£é è¨“ç·´çŸ¥è­˜ï¼Œå½±éŸ¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### 1.2 Adapter Layers çš„å‰µæ–°è§£æ±ºæ–¹æ¡ˆ

Adapter Tuning æå‡ºäº†é©å‘½æ€§çš„ã€Œæ¨¡çµ„åŒ–å¾®èª¿ã€æ¦‚å¿µï¼š

1. **æ¶æ§‹ä¿æŒä¸è®Š**ï¼šå®Œå…¨å‡çµé è¨“ç·´æ¨¡å‹åƒæ•¸ï¼Œä¿è­·åŸæœ‰çŸ¥è­˜
2. **æ¨¡çµ„åŒ–æ’å…¥**ï¼šåœ¨é—œéµä½ç½®æ’å…¥å°å‹å¯è¨“ç·´æ¨¡çµ„ï¼ˆAdapterï¼‰
3. **æœ€å°åƒæ•¸å¢é‡**ï¼šåƒ…å¢åŠ  0.5%-5% çš„åƒæ•¸é‡å³å¯é”åˆ°å„ªç§€æ•ˆæœ
4. **ä»»å‹™ç‰¹åŒ–èƒ½åŠ›**ï¼šæ¯å€‹ Adapter å°ˆé–€å­¸ç¿’ç‰¹å®šä»»å‹™çš„è½‰æ›å‡½æ•¸

---

## 2. Adapter Tuning æ ¸å¿ƒåŸç†

### 2.1 åŸºæœ¬æ¦‚å¿µèˆ‡æ¶æ§‹

**Adapter Tuning** çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**åœ¨é è¨“ç·´æ¨¡å‹çš„é—œéµå±¤é–“æ’å…¥å°å‹å¯è¨“ç·´æ¨¡çµ„ï¼Œå‡çµæ‰€æœ‰åŸå§‹åƒæ•¸ï¼Œåƒ…é€éå„ªåŒ–é€™äº› Adapter æ¨¡çµ„ä¾†é©æ‡‰ä¸‹æ¸¸ä»»å‹™**ã€‚

![Adapter æŠ€è¡“æ¶æ§‹](https://pic2.zhimg.com/v2-aee879a7574d6f24d528b7cd27de694d_1440w.jpg)

### 2.2 Adapter æ¨¡çµ„çš„ç²¾å¦™è¨­è¨ˆ

![Adapter æ¨¡çµ„çµæ§‹è©³è§£](https://pic4.zhimg.com/v2-9e0d951f3ef22fc92488d3423e808781_1440w.jpg)

æ¯å€‹ Adapter æ¨¡çµ„åŒ…å«ä»¥ä¸‹æ ¸å¿ƒçµ„ä»¶ï¼š

1. **Down-projection Layer**: $W_{down} \in \mathbb{R}^{d \times m}$ 
   - å°‡é«˜ç¶­ç‰¹å¾µ $d$ å£“ç¸®è‡³ä½ç¶­ç“¶é ¸ $m$
   - é€šå¸¸ $m \ll d$ï¼Œæ§åˆ¶åƒæ•¸æ•ˆç‡

2. **Non-linear Activation**: $\sigma(\cdot)$
   - å¸¸ç”¨ ReLU æˆ– GELU æ¿€æ´»å‡½æ•¸
   - å¢å¼·æ¨¡çµ„çš„éç·šæ€§è¡¨é”èƒ½åŠ›

3. **Up-projection Layer**: $W_{up} \in \mathbb{R}^{m \times d}$
   - å°‡ä½ç¶­ç‰¹å¾µé‡æ–°æ˜ å°„å›åŸå§‹ç¶­åº¦
   - ä¿æŒèˆ‡ä¸»å¹¹ç¶²è·¯çš„å°ºå¯¸å…¼å®¹

4. **Skip Connection**: $h_{output} = h_{input} + \text{Adapter}(h_{input})$
   - é—œéµçš„æ®˜å·®é€£æ¥è¨­è¨ˆ
   - ç¢ºä¿åˆå§‹åŒ–æ™‚æ¥è¿‘æ†ç­‰æ˜ å°„ï¼Œä¿è­‰è¨“ç·´ç©©å®šæ€§

### 2.3 Adapter æ’å…¥ä½ç½®ç­–ç•¥

![Adapter æ’å…¥ä½ç½®åˆ†æ](https://pica.zhimg.com/v2-9104b71432f7243fdd2e15677306535c_1440w.jpg)

| æ’å…¥ä½ç½® | ä½œç”¨æ©Ÿåˆ¶ | é©ç”¨å ´æ™¯ | åƒæ•¸é–‹éŠ· |
|:---|:---|:---|:---|
| **Multi-Head Attention å¾Œ** | èª¿æ•´æ³¨æ„åŠ›æ©Ÿåˆ¶è¼¸å‡º | éœ€è¦æ”¹è®Šæ³¨æ„åŠ›æ¨¡å¼çš„ä»»å‹™ | ä¸­ç­‰ |
| **Feed-Forward å±¤å¾Œ** | **ä¸»è¦ä½ç½®** | **é€šç”¨æ¨è–¦** | **æ¨™æº–** |
| **æ¯å€‹å­å±¤å¾Œ** | æœ€å¤§è¡¨é”èƒ½åŠ› | è¤‡é›œä»»å‹™ | è¼ƒé«˜ |
| **åƒ…è¼¸å‡ºå±¤å‰** | æœ€å°å¹²é  | ç°¡å–®åˆ†é¡ä»»å‹™ | æœ€ä½ |

---

## 3. æ•¸å­¸åŸç†èˆ‡å¯¦ç¾ç´°ç¯€

### 3.1 Adapter æ¨¡çµ„çš„æ•¸å­¸è¡¨ç¤º

å°æ–¼è¼¸å…¥ç‰¹å¾µ $h \in \mathbb{R}^d$ï¼ŒAdapter æ¨¡çµ„çš„è®Šæ›éç¨‹ç‚ºï¼š

$$\text{Adapter}(h) = W_{up} \cdot \sigma(W_{down} \cdot h + b_{down}) + b_{up}$$

å…¶ä¸­ï¼š
- $W_{down} \in \mathbb{R}^{m \times d}$ï¼šé™ç¶­æŠ•å½±çŸ©é™£
- $W_{up} \in \mathbb{R}^{d \times m}$ï¼šå‡ç¶­æŠ•å½±çŸ©é™£  
- $m$ï¼šç“¶é ¸ç¶­åº¦ï¼ˆbottleneck dimensionï¼‰
- $\sigma$ï¼šéç·šæ€§æ¿€æ´»å‡½æ•¸
- $b_{down}, b_{up}$ï¼šåç½®å‘é‡

### 3.2 åƒæ•¸æ•ˆç‡åˆ†æ

å°æ–¼æ¨™æº–çš„ Transformer å±¤ï¼š
- **åŸå§‹åƒæ•¸é‡**ï¼š$4d^2 + 4d$ï¼ˆæ³¨æ„åŠ› + FFNï¼‰
- **Adapter åƒæ•¸é‡**ï¼š$2dm + d + m$ï¼ˆé›™æŠ•å½±å±¤ + åç½®ï¼‰

**åƒæ•¸æ¯”ä¾‹**ï¼š
$$\frac{\text{Adapter åƒæ•¸}}{\text{åŸå§‹åƒæ•¸}} = \frac{2dm + d + m}{4d^2 + 4d} \approx \frac{2m}{4d} = \frac{m}{2d}$$

ç•¶ $m = 64, d = 768$ æ™‚ï¼Œåƒæ•¸å¢é‡åƒ…ç‚º **4.2%**ã€‚

---

## 4. Adapter ç³»åˆ—æ–¹æ³•å°æ¯”

### 4.1 æ ¸å¿ƒæ–¹æ³•æ¯”è¼ƒ

![Adapter æ–¹æ³•æ¼”é€²](https://pic4.zhimg.com/v2-c2a2314600b1d805391395f4bdb335f7_1440w.jpg)

| æ–¹æ³• | æ ¸å¿ƒå‰µæ–° | åƒæ•¸æ•ˆç‡ | è¤‡é›œåº¦ | å¤šä»»å‹™æ”¯æŒ |
|:---|:---|:---|:---|:---|
| **Adapter Tuning** | ç“¶é ¸æ¶æ§‹ + Skip Connection | **æ¥µé«˜** | **ä½** | å„ªç§€ |
| **AdapterFusion** | å¤šä»»å‹™çŸ¥è­˜èåˆ | é«˜ | ä¸­ç­‰ | **æœ€å¼·** |
| **AdapterDrop** | å‹•æ…‹æ¨¡çµ„å‰ªæ | **æ¥µé«˜** | ä½ | è‰¯å¥½ |
| **LoRA** | ä½ç§©åˆ†è§£ | æ¥µé«˜ | **æ¥µä½** | ä¸­ç­‰ |

### 4.2 èˆ‡ä¸»æµ PEFT æ–¹æ³•å°æ¯”

| å°æ¯”ç¶­åº¦ | Adapter Tuning | LoRA | Prefix Tuning | Prompt Tuning |
|:---|:---|:---|:---|:---|
| **åƒæ•¸æ•ˆç‡** | é«˜ (0.5-5%) | **æ¥µé«˜ (0.1-1%)** | é«˜ (0.1%) | **æ¥µé«˜ (0.01%)** |
| **å¯¦ç¾è¤‡é›œåº¦** | **ä½** | **ä½** | ä¸­ç­‰ | **æ¥µä½** |
| **è¨“ç·´ç©©å®šæ€§** | **å„ªç§€** | **å„ªç§€** | è‰¯å¥½ | ä¸­ç­‰ |
| **æ¨ç†é–‹éŠ·** | æœ‰è¼•å¾®å½±éŸ¿ | **ç„¡å½±éŸ¿** | ç„¡å½±éŸ¿ | **ç„¡å½±éŸ¿** |
| **å¤šä»»å‹™åˆ‡æ›** | **å„ªç§€** | è‰¯å¥½ | è‰¯å¥½ | **å„ªç§€** |

---

## 5. é«˜ç´šè®Šé«”ï¼šAdapterFusion

### 5.1 å¤šä»»å‹™çŸ¥è­˜èåˆæ©Ÿåˆ¶

![AdapterFusion æ¶æ§‹](https://pic4.zhimg.com/v2-cadecd9c428752b45480ea7de79fe7c3_1440w.jpg)

**AdapterFusion** å¯¦ç¾å…©éšæ®µå­¸ç¿’ç­–ç•¥ï¼š

#### ç¬¬ä¸€éšæ®µï¼šçŸ¥è­˜æå–
```python
# ç‚ºæ¯å€‹ä»»å‹™è¨“ç·´å°ˆé–€çš„ Adapter
for task in tasks:
    adapter_task = AdapterLayer(task_specific=True)
    train(adapter_task, task_data)
```

#### ç¬¬äºŒéšæ®µï¼šçŸ¥è­˜çµ„åˆ
```python
# èåˆå¤šä»»å‹™çŸ¥è­˜
fusion_layer = AttentionFusion(
    query=transformer_output,
    keys=[adapter1_output, adapter2_output, ...],
    values=[adapter1_output, adapter2_output, ...]
)
```

### 5.2 æ³¨æ„åŠ›æ©Ÿåˆ¶èåˆ

$$\text{AdapterFusion}(h) = \sum_{i=1}^{N} \alpha_i \cdot \text{Adapter}_i(h)$$

å…¶ä¸­æ³¨æ„åŠ›æ¬Šé‡ï¼š
$$\alpha_i = \frac{\exp(h^T W_Q W_{K_i} \text{Adapter}_i(h))}{\sum_{j=1}^{N} \exp(h^T W_Q W_{K_j} \text{Adapter}_j(h))}$$

---

## 6. æ€§èƒ½å„ªåŒ–ï¼šAdapterDrop

### 6.1 å‹•æ…‹æ•ˆç‡æå‡ç­–ç•¥

![AdapterDrop æ©Ÿåˆ¶](https://pic2.zhimg.com/v2-314db36574cdc556165340b905cad935_1440w.jpg)

**AdapterDrop** é€šéå‹•æ…‹ç§»é™¤ Adapter æ¨¡çµ„ä¾†æå‡æ¨ç†æ•ˆç‡ï¼š

| å±¤æ•¸ç¯„åœ | ç§»é™¤ç­–ç•¥ | æ€§èƒ½ä¿æŒ | é€Ÿåº¦æå‡ |
|:---|:---|:---|:---|
| **å‰ 1-3 å±¤** | æ¿€é€²ç§»é™¤ | 95%+ | 15-25% |
| **ä¸­é–“å±¤** | é¸æ“‡æ€§ç§»é™¤ | 98%+ | 10-20% |
| **å¾ŒæœŸå±¤** | ä¿å®ˆç§»é™¤ | 99%+ | 5-10% |

### 6.2 è‡ªé©æ‡‰å‰ªæç®—æ³•

```python
def adaptive_adapter_drop(layer_idx, task_complexity, performance_threshold):
    """
    æ ¹æ“šå±¤ä½ç½®å’Œä»»å‹™è¤‡é›œåº¦å‹•æ…‹æ±ºå®šæ˜¯å¦ä½¿ç”¨ Adapter
    """
    if layer_idx < 3:  # æ—©æœŸå±¤
        drop_prob = 0.8 if task_complexity < 0.5 else 0.4
    elif layer_idx < 8:  # ä¸­é–“å±¤
        drop_prob = 0.3 if task_complexity < 0.7 else 0.1
    else:  # å¾ŒæœŸå±¤
        drop_prob = 0.1
    
    return random.random() > drop_prob
```

---

## 7. å¯¦é©—è¨­è¨ˆèˆ‡å¯¦ä½œæ¡†æ¶

### 7.1 å¯¦é©—ç’°å¢ƒé…ç½®

- **åŸºç¤æ¨¡å‹**: BERT-base-uncased (110M åƒæ•¸)
- **ä»»å‹™é¡å‹**: åºåˆ—åˆ†é¡ï¼ˆParaphrase Detectionï¼‰
- **æ•¸æ“šé›†**: GLUE MRPCï¼ˆMicrosoft Research Paraphrase Corpusï¼‰
- **è©•ä¼°æŒ‡æ¨™**: Accuracyã€F1 Score

### 7.2 ä¸‰éšæ®µå¯¦é©—æµç¨‹

#### éšæ®µä¸€ï¼šç’°å¢ƒæº–å‚™ (`01-Setup.ipynb`)
```bash
# æ ¸å¿ƒä¾è³´
transformers>=4.20.0    # Transformer æ¨¡å‹åº«
peft>=0.3.0            # åƒæ•¸é«˜æ•ˆå¾®èª¿
datasets>=2.0.0        # æ•¸æ“šé›†è™•ç†
accelerate>=0.20.0     # åˆ†å¸ƒå¼è¨“ç·´åŠ é€Ÿ
```

#### éšæ®µäºŒï¼šæ¨¡å‹è¨“ç·´ (`02-Train.ipynb`)
```python
# Adapter é…ç½®åƒæ•¸
adapter_config = AdapterConfig(
    task_type=TaskType.SEQ_CLS,        # åºåˆ—åˆ†é¡ä»»å‹™
    mh_adapter=True,                   # å¤šé ­æ³¨æ„åŠ›å¾Œæ’å…¥
    output_adapter=True,               # è¼¸å‡ºå±¤å‰æ’å…¥
    reduction_factor=16,               # ç“¶é ¸ç¸®æ¸›å› å­
    non_linearity="relu"               # æ¿€æ´»å‡½æ•¸
)
```

#### éšæ®µä¸‰ï¼šæ¨ç†æ¸¬è©¦ (`03-Inference.ipynb`)
- è¼‰å…¥è¨“ç·´å¥½çš„ Adapter åƒæ•¸
- åŸ·è¡Œå¥å­å°èªç¾©ç­‰åƒ¹æ€§åˆ¤æ–·
- èˆ‡äººå·¥æ¨™è¨»çµæœé€²è¡Œå°æ¯”è©•ä¼°

### 7.3 æ ¸å¿ƒå¯¦ç¾é‚è¼¯

```python
class AdapterLayer(nn.Module):
    """
    æ¨™æº– Adapter å±¤å¯¦ç¾
    """
    def __init__(self, input_dim, reduction_factor=16):
        super().__init__()
        bottleneck_dim = input_dim // reduction_factor
        
        self.down_project = nn.Linear(input_dim, bottleneck_dim)
        self.activation = nn.ReLU()
        self.up_project = nn.Linear(bottleneck_dim, input_dim)
        
        # åˆå§‹åŒ–æ¥è¿‘é›¶ï¼Œç¢ºä¿é–‹å§‹æ™‚è¿‘ä¼¼æ†ç­‰æ˜ å°„
        nn.init.normal_(self.down_project.weight, std=1e-3)
        nn.init.normal_(self.up_project.weight, std=1e-3)
        nn.init.zeros_(self.down_project.bias)
        nn.init.zeros_(self.up_project.bias)
    
    def forward(self, x):
        # Skip connection æ˜¯é—œéµ
        return x + self.up_project(
            self.activation(self.down_project(x))
        )
```

---

## 8. æ€§èƒ½è¡¨ç¾èˆ‡å¯¦é©—çµæœ

### 8.1 åƒæ•¸æ•ˆç‡åˆ†æ

![æ€§èƒ½å°æ¯”å¯¦é©—](https://pic3.zhimg.com/v2-29d2ca5a17f4f2701fbe9fb074e78d5e_1440w.jpg)

| ç“¶é ¸ç¶­åº¦ (m) | åƒæ•¸å¢é‡ | MRPC Accuracy | F1 Score | è¨“ç·´æ™‚é–“ |
|:---|:---|:---|:---|:---|
| **8** | 0.37% | 84.1% | 88.2% | 12 min |
| **16** | 0.74% | **86.3%** | **90.1%** | 14 min |
| **32** | 1.48% | 86.8% | 90.4% | 18 min |
| **64** | 2.96% | 87.1% | 90.6% | 26 min |
| **å…¨é‡å¾®èª¿** | 100% | 87.5% | 90.8% | **180 min** |

**é—œéµç™¼ç¾**ï¼šä½¿ç”¨ reduction_factor=16 æ™‚ï¼Œåƒ…ç”¨ 0.74% çš„åƒæ•¸å³å¯é”åˆ°å…¨é‡å¾®èª¿ 98.6% çš„æ€§èƒ½ã€‚

### 8.2 ä¸åŒæ’å…¥ç­–ç•¥æ¯”è¼ƒ

![æ’å…¥ä½ç½®å¯¦é©—](https://picx.zhimg.com/v2-d6431b06b2a4be614e0155f2aad438ad_1440w.jpg)

| æ’å…¥ç­–ç•¥ | åƒæ•¸å¢é‡ | æ€§èƒ½ä¿æŒç‡ | æ¨ç†é€Ÿåº¦å½±éŸ¿ |
|:---|:---|:---|:---|
| **åƒ… FFN å¾Œ** | 0.74% | **98.6%** | **-2%** |
| **åƒ… Attention å¾Œ** | 0.74% | 96.1% | -3% |
| **å…©è™•éƒ½æ’å…¥** | 1.48% | **99.2%** | -5% |
| **æ¯å€‹å­å±¤å¾Œ** | 2.96% | 99.5% | -8% |

---

## 9. é«˜ç´šæ‡‰ç”¨èˆ‡æœ€ä½³å¯¦è¸

### 9.1 å¤šä»»å‹™ Adapter ç®¡ç†

![å¤šä»»å‹™éƒ¨ç½²æ¶æ§‹](https://picx.zhimg.com/v2-bf86f888ceb53604d0d0efddb8435429_1440w.jpg)

```python
# ä»»å‹™ç‰¹å®š Adapter åº«
class MultiTaskAdapterManager:
    def __init__(self, base_model):
        self.base_model = base_model
        self.task_adapters = {}
        
    def register_task(self, task_name, adapter_config):
        """è¨»å†Šæ–°ä»»å‹™çš„ Adapter"""
        adapter = get_peft_model(self.base_model, adapter_config)
        self.task_adapters[task_name] = adapter
        
    def switch_task(self, task_name):
        """åˆ‡æ›åˆ°æŒ‡å®šä»»å‹™"""
        if task_name in self.task_adapters:
            return self.task_adapters[task_name]
        else:
            raise ValueError(f"Task {task_name} not registered")
            
    def batch_inference(self, inputs, task_names):
        """æ‰¹æ¬¡å¤šä»»å‹™æ¨ç†"""
        results = {}
        for task in set(task_names):
            task_inputs = [inp for inp, t in zip(inputs, task_names) if t == task]
            adapter = self.switch_task(task)
            results[task] = adapter(task_inputs)
        return results
```

### 9.2 è‡ªé©æ‡‰ç“¶é ¸ç¶­åº¦é¸æ“‡

| ä»»å‹™è¤‡é›œåº¦ | æ•¸æ“šé›†å¤§å° | å»ºè­°ç“¶é ¸ç¶­åº¦ | æ€§èƒ½æ¬Šè¡¡ |
|:---|:---|:---|:---|
| **ç°¡å–®åˆ†é¡** | < 10K | 8-16 | æ¥µé«˜æ•ˆç‡ï¼Œä¸­ç­‰æ€§èƒ½ |
| **ä¸­ç­‰è¤‡é›œåº¦** | 10K-100K | **16-32** | **å¹³è¡¡æ•ˆç‡èˆ‡æ•ˆæœ** |
| **è¤‡é›œç†è§£** | > 100K | 32-64 | é«˜æ€§èƒ½ï¼Œæ•ˆç‡è‰¯å¥½ |
| **å¤šæ¨¡æ…‹ä»»å‹™** | > 100K | 64-128 | æœ€ä½³æ€§èƒ½ï¼Œæ•ˆç‡å¯æ¥å— |

---

## 10. æ–¹æ³•é¸æ“‡æŒ‡å¼•èˆ‡æ‡‰ç”¨å»ºè­°

### 10.1 æœ€ä½³æ‡‰ç”¨å ´æ™¯

| ä½¿ç”¨å ´æ™¯ | æ¨è–¦ç†ç”± | é…ç½®å»ºè­° |
|:---|:---|:---|
| **å¤šä»»å‹™ç³»çµ±** | æ¨¡çµ„åŒ–åˆ‡æ›ï¼Œéƒ¨ç½²å‹å¥½ | reduction_factor=16 |
| **åºåˆ—åˆ†é¡ä»»å‹™** | æˆç†Ÿç©©å®šï¼Œæ•ˆæœå¯é  | åƒ… FFN å¾Œæ’å…¥ |
| **è³‡æºå—é™ç’°å¢ƒ** | æ¨ç†é–‹éŠ·å¯æ§ | reduction_factor=32 |
| **çŸ¥è­˜é·ç§»** | ä¿è­·é è¨“ç·´çŸ¥è­˜ | ä½¿ç”¨ AdapterFusion |

### 10.2 èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„é¸æ“‡ç­–ç•¥

```python
def choose_peft_method(task_type, model_size, resource_constraint, multi_task):
    """
    PEFT æ–¹æ³•é¸æ“‡æ±ºç­–æ¨¹
    """
    if multi_task and task_type == "classification":
        return "Adapter + AdapterFusion"  # å¤šä»»å‹™é¦–é¸
    
    elif resource_constraint == "extreme":
        if model_size > 1_000_000_000:  # 1B+ åƒæ•¸
            return "Prompt Tuning"  # è¶…å¤§æ¨¡å‹ç”¨è»Ÿæç¤º
        else:
            return "LoRA"  # å°æ¨¡å‹ç”¨ LoRA
            
    elif task_type in ["classification", "sequence_labeling"]:
        return "Adapter Tuning"  # åˆ†é¡ä»»å‹™å„ªå‹¢
        
    elif task_type in ["generation", "summarization"]:
        return "LoRA"  # ç”Ÿæˆä»»å‹™æ›´é©åˆ
        
    else:
        return "Prefix Tuning"  # é€šç”¨é¸æ“‡
```

---

## 11. æŠ€è¡“é™åˆ¶èˆ‡æ”¹é€²æ–¹å‘

### 11.1 ç•¶å‰é™åˆ¶åˆ†æ

| é™åˆ¶é …ç›® | å…·é«”è¡¨ç¾ | ç·©è§£ç­–ç•¥ |
|:---|:---|:---|
| **æ¨ç†å»¶é²å¢åŠ ** | ç›¸æ¯” LoRA æœ‰ 2-8% çš„é€Ÿåº¦æå¤± | ä½¿ç”¨ AdapterDrop å‹•æ…‹å‰ªæ |
| **åƒæ•¸ç•¥å¤šæ–¼å…¶ä»–æ–¹æ³•** | æ¯” Prompt Tuning å¤šç´„ 50 å€åƒæ•¸ | ç²¾å¿ƒèª¿æ•´ reduction_factor |
| **ä»»å‹™é–“å¯èƒ½å¹²æ“¾** | AdapterFusion ä¸­ä»»å‹™è¡çª | è¨­è¨ˆä»»å‹™è¦ªå’Œåº¦æ¸¬é‡ |

### 11.2 æœªä¾†ç ”ç©¶æ–¹å‘

- **ç¥ç¶“æ¶æ§‹æœç´¢**ï¼šè‡ªå‹•ç™¼ç¾æœ€å„ª Adapter æ’å…¥ä½ç½®èˆ‡çµæ§‹
- **å‹•æ…‹ç¨€ç–åŒ–**ï¼šé‹è¡Œæ™‚è‡ªé©æ‡‰æ¿€æ´» Adapter æ¨¡çµ„
- **è·¨æ¨¡æ…‹æ“´å±•**ï¼šå°‡ Adapter æ¦‚å¿µå»¶ä¼¸åˆ°è¦–è¦º-èªè¨€æ¨¡å‹
- **çŸ¥è­˜è’¸é¤¾èåˆ**ï¼šçµåˆçŸ¥è­˜è’¸é¤¾æŠ€è¡“é€²ä¸€æ­¥æå‡æ•ˆç‡

---

## 12. å¯¦é©—çµè«–èˆ‡å­¸ç¿’åƒ¹å€¼

### 12.1 æ ¸å¿ƒæŠ€è¡“æ”¶ç©«

é€šéæœ¬å¯¦é©—ï¼Œæ‚¨å°‡å…¨é¢æŒæ¡ï¼š

1. **æ·±åº¦ç†è§£** Adapter Tuning çš„æ¨¡çµ„åŒ–è¨­è¨ˆæ€æƒ³èˆ‡å·¥ç¨‹å¯¦è¸
2. **å¯¦è¸ç¶“é©—** åœ¨æ–‡æœ¬åˆ†é¡ä»»å‹™ä¸­æ‡‰ç”¨ Adapter çš„å®Œæ•´æµç¨‹
3. **åƒæ•¸èª¿å„ª** æŒæ¡ç“¶é ¸ç¶­åº¦ã€æ’å…¥ä½ç½®ç­‰é—œéµè¶…åƒæ•¸å„ªåŒ–
4. **å°æ¯”åˆ†æ** ç†è§£ Adapter èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„æŠ€è¡“å·®ç•°

### 12.2 å·¥ç¨‹å¯¦è¸æ„ç¾©

- **æ¨¡çµ„åŒ–æ€ç¶­**ï¼šé«”é©—ã€Œæ’ä»¶å¼ã€å¾®èª¿çš„è¨­è¨ˆç†å¿µ
- **æ•ˆç‡å¹³è¡¡**ï¼šæŒæ¡åƒæ•¸æ•ˆç‡èˆ‡è¨ˆç®—æ•ˆç‡çš„æ¬Šè¡¡ç­–ç•¥  
- **å¤šä»»å‹™æ¶æ§‹**ï¼šå­¸æœƒè¨­è¨ˆæ”¯æŒä»»å‹™åˆ‡æ›çš„çµ±ä¸€æ¨ç†æ¡†æ¶
- **ç©©å®šæ€§ä¿è­‰**ï¼šç†è§£ Skip Connection å°è¨“ç·´ç©©å®šæ€§çš„é—œéµä½œç”¨

Adapter Tuning ä½œç‚º PEFT é ˜åŸŸçš„å¥ åŸºæ€§å·¥ä½œï¼Œå®Œç¾å±•ç¾äº†ã€Œæœ€å°å¹²é ï¼Œæœ€å¤§æ•ˆæœã€çš„å·¥ç¨‹å“²å­¸ã€‚å…¶æ¨¡çµ„åŒ–è¨­è¨ˆä¸åƒ…åœ¨åƒæ•¸æ•ˆç‡ä¸Šè¡¨ç¾å„ªç•°ï¼Œæ›´ç‚ºå¾ŒçºŒæ–¹æ³•çš„ç™¼å±•å¥ å®šäº†é‡è¦çš„ç†è«–åŸºç¤ã€‚

---

## 13. åƒè€ƒè³‡æ–™èˆ‡å»¶ä¼¸é–±è®€

### æ ¸å¿ƒè«–æ–‡
- **Adapter Tuning**: Houlsby, N., et al. (2019). *Parameter-Efficient Transfer Learning for NLP*. ICML 2019.
- **AdapterFusion**: Pfeiffer, J., et al. (2021). *AdapterFusion: Non-Destructive Task Composition for Transfer Learning*. EACL 2021.
- **AdapterDrop**: RÃ¼cklÃ©, A., et al. (2021). *AdapterDrop: On the Efficiency of Adapters in Transformers*. EMNLP 2021.

### ç›¸é—œç ”ç©¶
- **Parallel Adapters**: He, J., et al. (2022). *Towards a Unified View of Parameter-Efficient Transfer Learning*. ICLR 2022.
- **Scaling Analysis**: Wang, Z., et al. (2022). *What Makes Good In-Context Examples for GPT-3?*. DeeLIO 2022.

### æŠ€è¡“å¯¦ç¾
- **Hugging Face PEFT**: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)
- **AdapterHub**: [https://adapterhub.ml/](https://adapterhub.ml/)
- **è«–æ–‡è¤‡ç¾**: [https://github.com/google-research/adapter-bert](https://github.com/google-research/adapter-bert)

### æ•¸æ“šé›†èˆ‡è©•ä¼°
- **GLUE Benchmark**: [https://gluebenchmark.com/](https://gluebenchmark.com/)
- **MRPC Dataset**: [https://huggingface.co/datasets/glue](https://huggingface.co/datasets/glue)

---

**æº–å‚™å¥½æ¢ç´¢æ¨¡çµ„åŒ–å¾®èª¿çš„åŠ›é‡äº†å—ï¼Ÿè®“æˆ‘å€‘é–‹å§‹ Adapter Tuning çš„ç¶“å…¸ä¹‹æ—…ï¼** ğŸš€
