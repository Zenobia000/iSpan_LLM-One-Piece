{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.4 Part 1: Production Architecture Design\n",
    "\n",
    "## Objectives\n",
    "- Design production-grade LLM service architecture\n",
    "- Plan resource requirements and capacity\n",
    "- Select appropriate technology stack\n",
    "- Estimate costs and performance\n",
    "\n",
    "## Estimated Time: 60-120 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Production Requirements Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production requirements specification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "print(\"Production Architecture Design Tool\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProductionRequirements:\n",
    "    \"\"\"\n",
    "    Production requirements specification\n",
    "    \"\"\"\n",
    "    # Traffic requirements\n",
    "    peak_requests_per_second: int\n",
    "    average_requests_per_second: int\n",
    "    daily_request_volume: int\n",
    "    \n",
    "    # Performance requirements\n",
    "    max_latency_p95: float  # milliseconds\n",
    "    max_latency_p99: float  # milliseconds\n",
    "    target_availability: float  # 99.9% = 0.999\n",
    "    \n",
    "    # Business requirements\n",
    "    budget_monthly_usd: float\n",
    "    geographic_regions: List[str]\n",
    "    compliance_requirements: List[str]\n",
    "    \n",
    "    # Model requirements\n",
    "    model_size: str  # \"7B\", \"13B\", \"70B\"\n",
    "    max_context_length: int\n",
    "    average_response_tokens: int\n",
    "\n",
    "# Example production requirements\n",
    "requirements = ProductionRequirements(\n",
    "    peak_requests_per_second=1000,\n",
    "    average_requests_per_second=200,\n",
    "    daily_request_volume=17_280_000,  # 200 RPS * 86400 seconds\n",
    "    \n",
    "    max_latency_p95=500.0,  # 500ms\n",
    "    max_latency_p99=1000.0,  # 1s\n",
    "    target_availability=0.999,  # 99.9%\n",
    "    \n",
    "    budget_monthly_usd=50_000,\n",
    "    geographic_regions=[\"us-east-1\", \"eu-west-1\", \"ap-southeast-1\"],\n",
    "    compliance_requirements=[\"GDPR\", \"SOC2\", \"HIPAA\"],\n",
    "    \n",
    "    model_size=\"7B\",\n",
    "    max_context_length=4096,\n",
    "    average_response_tokens=150\n",
    ")\n",
    "\n",
    "print(\"Production Requirements:\")\n",
    "print(f\"Peak RPS: {requirements.peak_requests_per_second:,}\")\n",
    "print(f\"Daily volume: {requirements.daily_request_volume:,} requests\")\n",
    "print(f\"P95 latency: {requirements.max_latency_p95}ms\")\n",
    "print(f\"Availability: {requirements.target_availability*100}%\")\n",
    "print(f\"Budget: ${requirements.budget_monthly_usd:,}/month\")\n",
    "print(f\"Model: {requirements.model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Resource Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceCalculator:\n",
    "    \"\"\"\n",
    "    Calculate required resources for production deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model resource requirements (per instance)\n",
    "        self.model_specs = {\n",
    "            \"7B\": {\n",
    "                \"gpu_memory_gb\": 16,\n",
    "                \"cpu_cores\": 8,\n",
    "                \"ram_gb\": 32,\n",
    "                \"throughput_tokens_per_sec\": 2000,\n",
    "                \"concurrent_requests\": 32\n",
    "            },\n",
    "            \"13B\": {\n",
    "                \"gpu_memory_gb\": 32,\n",
    "                \"cpu_cores\": 16,\n",
    "                \"ram_gb\": 64,\n",
    "                \"throughput_tokens_per_sec\": 1500,\n",
    "                \"concurrent_requests\": 24\n",
    "            },\n",
    "            \"70B\": {\n",
    "                \"gpu_memory_gb\": 160,  # Multi-GPU setup\n",
    "                \"cpu_cores\": 32,\n",
    "                \"ram_gb\": 128,\n",
    "                \"throughput_tokens_per_sec\": 800,\n",
    "                \"concurrent_requests\": 16\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_required_instances(self, requirements: ProductionRequirements) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate number of instances needed\n",
    "        \"\"\"\n",
    "        model_spec = self.model_specs[requirements.model_size]\n",
    "        \n",
    "        # Calculate tokens per second needed\n",
    "        tokens_per_request = requirements.average_response_tokens\n",
    "        total_tokens_per_sec = requirements.peak_requests_per_second * tokens_per_request\n",
    "        \n",
    "        # Calculate instances needed based on throughput\n",
    "        instances_for_throughput = np.ceil(\n",
    "            total_tokens_per_sec / model_spec[\"throughput_tokens_per_sec\"]\n",
    "        )\n",
    "        \n",
    "        # Calculate instances needed based on concurrent requests\n",
    "        instances_for_concurrency = np.ceil(\n",
    "            requirements.peak_requests_per_second / model_spec[\"concurrent_requests\"]\n",
    "        )\n",
    "        \n",
    "        # Take the maximum\n",
    "        min_instances = max(instances_for_throughput, instances_for_concurrency)\n",
    "        \n",
    "        # Add buffer for availability (20% overhead)\n",
    "        recommended_instances = int(min_instances * 1.2)\n",
    "        \n",
    "        return {\n",
    "            \"min_instances\": int(min_instances),\n",
    "            \"recommended_instances\": recommended_instances,\n",
    "            \"tokens_per_sec_needed\": int(total_tokens_per_sec),\n",
    "            \"tokens_per_sec_capacity\": int(recommended_instances * model_spec[\"throughput_tokens_per_sec\"]),\n",
    "            \"utilization_percentage\": (min_instances / recommended_instances) * 100\n",
    "        }\n",
    "    \n",
    "    def calculate_total_resources(self, requirements: ProductionRequirements) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate total resource requirements\n",
    "        \"\"\"\n",
    "        model_spec = self.model_specs[requirements.model_size]\n",
    "        sizing = self.calculate_required_instances(requirements)\n",
    "        instances = sizing[\"recommended_instances\"]\n",
    "        \n",
    "        return {\n",
    "            \"instances\": instances,\n",
    "            \"total_gpu_memory_gb\": instances * model_spec[\"gpu_memory_gb\"],\n",
    "            \"total_cpu_cores\": instances * model_spec[\"cpu_cores\"],\n",
    "            \"total_ram_gb\": instances * model_spec[\"ram_gb\"],\n",
    "            \"total_throughput_tokens_per_sec\": instances * model_spec[\"throughput_tokens_per_sec\"],\n",
    "            \"total_concurrent_requests\": instances * model_spec[\"concurrent_requests\"]\n",
    "        }\n",
    "\n",
    "calculator = ResourceCalculator()\n",
    "sizing = calculator.calculate_required_instances(requirements)\n",
    "resources = calculator.calculate_total_resources(requirements)\n",
    "\n",
    "print(\"Resource Estimation:\")\n",
    "print(f\"Minimum instances: {sizing['min_instances']}\")\n",
    "print(f\"Recommended instances: {sizing['recommended_instances']}\")\n",
    "print(f\"Expected utilization: {sizing['utilization_percentage']:.1f}%\")\n",
    "print()\n",
    "print(\"Total Resources:\")\n",
    "print(f\"GPU Memory: {resources['total_gpu_memory_gb']} GB\")\n",
    "print(f\"CPU Cores: {resources['total_cpu_cores']}\")\n",
    "print(f\"RAM: {resources['total_ram_gb']} GB\")\n",
    "print(f\"Throughput Capacity: {resources['total_throughput_tokens_per_sec']:,} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Architecture Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchitecturePattern:\n",
    "    \"\"\"\n",
    "    Different production architecture patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def single_node_pattern():\n",
    "        return {\n",
    "            \"name\": \"Single Node\",\n",
    "            \"description\": \"Single server with multiple GPU setup\",\n",
    "            \"pros\": [\n",
    "                \"Simple deployment\",\n",
    "                \"Low latency (no network hops)\",\n",
    "                \"Easy debugging\",\n",
    "                \"Cost effective for small scale\"\n",
    "            ],\n",
    "            \"cons\": [\n",
    "                \"Single point of failure\",\n",
    "                \"Limited scalability\",\n",
    "                \"No geographic distribution\",\n",
    "                \"Hardware limitations\"\n",
    "            ],\n",
    "            \"use_cases\": [\n",
    "                \"Development/testing\",\n",
    "                \"< 100 RPS\",\n",
    "                \"Internal tools\",\n",
    "                \"Proof of concept\"\n",
    "            ],\n",
    "            \"max_rps\": 100\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def horizontal_scaling_pattern():\n",
    "        return {\n",
    "            \"name\": \"Horizontal Scaling\",\n",
    "            \"description\": \"Multiple identical nodes behind load balancer\",\n",
    "            \"pros\": [\n",
    "                \"High availability\",\n",
    "                \"Linear scalability\",\n",
    "                \"Rolling updates\",\n",
    "                \"Fault tolerance\"\n",
    "            ],\n",
    "            \"cons\": [\n",
    "                \"More complex deployment\",\n",
    "                \"Load balancer overhead\",\n",
    "                \"State management issues\",\n",
    "                \"Higher costs\"\n",
    "            ],\n",
    "            \"use_cases\": [\n",
    "                \"Production services\",\n",
    "                \"100-1000 RPS\",\n",
    "                \"Customer-facing APIs\",\n",
    "                \"24/7 availability\"\n",
    "            ],\n",
    "            \"max_rps\": 1000\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def microservices_pattern():\n",
    "        return {\n",
    "            \"name\": \"Microservices\",\n",
    "            \"description\": \"Separate services for different functions\",\n",
    "            \"pros\": [\n",
    "                \"Independent scaling\",\n",
    "                \"Technology diversity\",\n",
    "                \"Team autonomy\",\n",
    "                \"Fault isolation\"\n",
    "            ],\n",
    "            \"cons\": [\n",
    "                \"Complex networking\",\n",
    "                \"Service discovery overhead\",\n",
    "                \"Distributed tracing needed\",\n",
    "                \"Higher operational complexity\"\n",
    "            ],\n",
    "            \"use_cases\": [\n",
    "                \"Large organizations\",\n",
    "                \"> 1000 RPS\",\n",
    "                \"Multiple model types\",\n",
    "                \"Different SLA requirements\"\n",
    "            ],\n",
    "            \"max_rps\": 10000\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def multi_region_pattern():\n",
    "        return {\n",
    "            \"name\": \"Multi-Region\",\n",
    "            \"description\": \"Deployed across multiple geographic regions\",\n",
    "            \"pros\": [\n",
    "                \"Global low latency\",\n",
    "                \"Disaster recovery\",\n",
    "                \"Compliance (data locality)\",\n",
    "                \"Load distribution\"\n",
    "            ],\n",
    "            \"cons\": [\n",
    "                \"Complex routing\",\n",
    "                \"Data synchronization\",\n",
    "                \"Higher costs\",\n",
    "                \"Regulatory complexity\"\n",
    "            ],\n",
    "            \"use_cases\": [\n",
    "                \"Global services\",\n",
    "                \"Compliance requirements\",\n",
    "                \"Mission critical systems\",\n",
    "                \"Enterprise customers\"\n",
    "            ],\n",
    "            \"max_rps\": 50000\n",
    "        }\n",
    "\n",
    "# Analyze which pattern fits our requirements\n",
    "patterns = [\n",
    "    ArchitecturePattern.single_node_pattern(),\n",
    "    ArchitecturePattern.horizontal_scaling_pattern(),\n",
    "    ArchitecturePattern.microservices_pattern(),\n",
    "    ArchitecturePattern.multi_region_pattern()\n",
    "]\n",
    "\n",
    "print(\"Architecture Pattern Analysis:\")\n",
    "print(f\"Required RPS: {requirements.peak_requests_per_second}\")\n",
    "print(f\"Regions: {len(requirements.geographic_regions)}\")\n",
    "print(f\"Compliance: {requirements.compliance_requirements}\")\n",
    "print()\n",
    "\n",
    "suitable_patterns = []\n",
    "for pattern in patterns:\n",
    "    if pattern[\"max_rps\"] >= requirements.peak_requests_per_second:\n",
    "        suitable_patterns.append(pattern)\n",
    "        print(f\"✅ {pattern['name']}: {pattern['description']}\")\n",
    "        print(f\"   Max RPS: {pattern['max_rps']}\")\n",
    "        print(f\"   Use cases: {', '.join(pattern['use_cases'])}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"❌ {pattern['name']}: Insufficient capacity ({pattern['max_rps']} < {requirements.peak_requests_per_second})\")\n",
    "\n",
    "# Recommend based on requirements\n",
    "if len(requirements.geographic_regions) > 1:\n",
    "    recommended = \"Multi-Region\"\n",
    "elif requirements.peak_requests_per_second > 500:\n",
    "    recommended = \"Microservices\"\n",
    "elif requirements.peak_requests_per_second > 100:\n",
    "    recommended = \"Horizontal Scaling\"\n",
    "else:\n",
    "    recommended = \"Single Node\"\n",
    "\n",
    "print(f\"🎯 Recommended Pattern: {recommended}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Technology Stack Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnologyStack:\n",
    "    \"\"\"\n",
    "    Production technology stack options\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_inference_engines():\n",
    "        return {\n",
    "            \"vLLM\": {\n",
    "                \"description\": \"High-performance inference with PagedAttention\",\n",
    "                \"pros\": [\"Highest throughput\", \"Memory efficient\", \"Easy to use\"],\n",
    "                \"cons\": [\"GPU only\", \"Limited model support\"],\n",
    "                \"best_for\": \"High-throughput production services\",\n",
    "                \"maturity\": \"Stable\"\n",
    "            },\n",
    "            \"TensorRT-LLM\": {\n",
    "                \"description\": \"NVIDIA optimized inference engine\",\n",
    "                \"pros\": [\"Extreme performance\", \"Advanced optimizations\", \"Enterprise support\"],\n",
    "                \"cons\": [\"Complex setup\", \"NVIDIA GPU only\", \"Steep learning curve\"],\n",
    "                \"best_for\": \"Maximum performance with NVIDIA hardware\",\n",
    "                \"maturity\": \"Production ready\"\n",
    "            },\n",
    "            \"TGI\": {\n",
    "                \"description\": \"HuggingFace Text Generation Inference\",\n",
    "                \"pros\": [\"HF integration\", \"Wide model support\", \"Good documentation\"],\n",
    "                \"cons\": [\"Lower performance\", \"Resource intensive\"],\n",
    "                \"best_for\": \"Quick deployment with HuggingFace models\",\n",
    "                \"maturity\": \"Stable\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_container_platforms():\n",
    "        return {\n",
    "            \"Kubernetes\": {\n",
    "                \"description\": \"Industry standard container orchestration\",\n",
    "                \"pros\": [\"Mature ecosystem\", \"Auto-scaling\", \"Service discovery\", \"Multi-cloud\"],\n",
    "                \"cons\": [\"Complex\", \"Resource overhead\", \"Learning curve\"],\n",
    "                \"best_for\": \"Production at scale\",\n",
    "                \"cost_factor\": 1.2\n",
    "            },\n",
    "            \"Docker Swarm\": {\n",
    "                \"description\": \"Simple Docker-native orchestration\",\n",
    "                \"pros\": [\"Simple\", \"Docker native\", \"Low overhead\"],\n",
    "                \"cons\": [\"Limited features\", \"Smaller ecosystem\"],\n",
    "                \"best_for\": \"Small to medium deployments\",\n",
    "                \"cost_factor\": 1.0\n",
    "            },\n",
    "            \"Nomad\": {\n",
    "                \"description\": \"HashiCorp's orchestrator\",\n",
    "                \"pros\": [\"Simple\", \"Flexible\", \"Multi-workload\"],\n",
    "                \"cons\": [\"Smaller ecosystem\", \"Less mature\"],\n",
    "                \"best_for\": \"HashiCorp stack environments\",\n",
    "                \"cost_factor\": 1.1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_monitoring_stacks():\n",
    "        return {\n",
    "            \"Prometheus + Grafana\": {\n",
    "                \"description\": \"Open source monitoring stack\",\n",
    "                \"pros\": [\"Free\", \"Powerful\", \"Large community\", \"Kubernetes native\"],\n",
    "                \"cons\": [\"Setup complexity\", \"Storage scaling\"],\n",
    "                \"cost_monthly\": 0\n",
    "            },\n",
    "            \"DataDog\": {\n",
    "                \"description\": \"SaaS monitoring platform\",\n",
    "                \"pros\": [\"Easy setup\", \"Rich features\", \"Great UI\", \"APM included\"],\n",
    "                \"cons\": [\"Expensive\", \"Vendor lock-in\"],\n",
    "                \"cost_monthly\": 2000\n",
    "            },\n",
    "            \"New Relic\": {\n",
    "                \"description\": \"Application performance monitoring\",\n",
    "                \"pros\": [\"APM focus\", \"AI insights\", \"Good alerting\"],\n",
    "                \"cons\": [\"Expensive\", \"Complex pricing\"],\n",
    "                \"cost_monthly\": 1500\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Analyze technology choices\n",
    "tech_stack = TechnologyStack()\n",
    "\n",
    "print(\"Technology Stack Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Inference Engine Selection\n",
    "engines = tech_stack.get_inference_engines()\n",
    "print(\"\\n🚀 Inference Engines:\")\n",
    "for name, details in engines.items():\n",
    "    print(f\"\\n{name}: {details['description']}\")\n",
    "    print(f\"  Best for: {details['best_for']}\")\n",
    "    print(f\"  Maturity: {details['maturity']}\")\n",
    "\n",
    "# Recommend inference engine\n",
    "if requirements.peak_requests_per_second > 500:\n",
    "    recommended_engine = \"vLLM\"  # High throughput\n",
    "elif \"NVIDIA\" in str(requirements.budget_monthly_usd):  # High budget = likely NVIDIA\n",
    "    recommended_engine = \"TensorRT-LLM\"  # Maximum performance\n",
    "else:\n",
    "    recommended_engine = \"vLLM\"  # Best balance\n",
    "\n",
    "print(f\"\\n🎯 Recommended Inference Engine: {recommended_engine}\")\n",
    "\n",
    "# Container Platform Selection\n",
    "platforms = tech_stack.get_container_platforms()\n",
    "print(\"\\n🐳 Container Platforms:\")\n",
    "for name, details in platforms.items():\n",
    "    monthly_cost_factor = details['cost_factor'] * 10000  # Example base cost\n",
    "    print(f\"\\n{name}: {details['description']}\")\n",
    "    print(f\"  Best for: {details['best_for']}\")\n",
    "    print(f\"  Cost factor: {details['cost_factor']}x\")\n",
    "\n",
    "# Recommend platform\n",
    "if requirements.peak_requests_per_second > 1000 or len(requirements.geographic_regions) > 1:\n",
    "    recommended_platform = \"Kubernetes\"\n",
    "elif requirements.budget_monthly_usd < 20000:\n",
    "    recommended_platform = \"Docker Swarm\"\n",
    "else:\n",
    "    recommended_platform = \"Kubernetes\"\n",
    "\n",
    "print(f\"\\n🎯 Recommended Platform: {recommended_platform}\")\n",
    "\n",
    "# Monitoring Stack Selection\n",
    "monitoring = tech_stack.get_monitoring_stacks()\n",
    "print(\"\\n📊 Monitoring Stacks:\")\n",
    "for name, details in monitoring.items():\n",
    "    print(f\"\\n{name}: {details['description']}\")\n",
    "    print(f\"  Monthly cost: ${details['cost_monthly']}\")\n",
    "\n",
    "# Recommend monitoring\n",
    "if requirements.budget_monthly_usd > 30000:\n",
    "    recommended_monitoring = \"DataDog\"\n",
    "else:\n",
    "    recommended_monitoring = \"Prometheus + Grafana\"\n",
    "\n",
    "print(f\"\\n🎯 Recommended Monitoring: {recommended_monitoring}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostCalculator:\n",
    "    \"\"\"\n",
    "    Calculate production deployment costs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # AWS pricing (approximate, varies by region)\n",
    "        self.instance_pricing = {\n",
    "            \"g5.xlarge\": {  # 1 x A10G (24GB)\n",
    "                \"gpu_memory_gb\": 24,\n",
    "                \"hourly_on_demand\": 1.006,\n",
    "                \"hourly_spot\": 0.30,  # ~70% savings\n",
    "                \"vcpu\": 4,\n",
    "                \"ram_gb\": 16\n",
    "            },\n",
    "            \"g5.2xlarge\": {  # 1 x A10G (24GB)\n",
    "                \"gpu_memory_gb\": 24,\n",
    "                \"hourly_on_demand\": 1.212,\n",
    "                \"hourly_spot\": 0.36,\n",
    "                \"vcpu\": 8,\n",
    "                \"ram_gb\": 32\n",
    "            },\n",
    "            \"g5.4xlarge\": {  # 1 x A10G (24GB)\n",
    "                \"gpu_memory_gb\": 24,\n",
    "                \"hourly_on_demand\": 1.624,\n",
    "                \"hourly_spot\": 0.49,\n",
    "                \"vcpu\": 16,\n",
    "                \"ram_gb\": 64\n",
    "            },\n",
    "            \"p3.2xlarge\": {  # 1 x V100 (16GB)\n",
    "                \"gpu_memory_gb\": 16,\n",
    "                \"hourly_on_demand\": 3.06,\n",
    "                \"hourly_spot\": 0.92,\n",
    "                \"vcpu\": 8,\n",
    "                \"ram_gb\": 61\n",
    "            },\n",
    "            \"p4d.2xlarge\": {  # 1 x A100 (40GB)\n",
    "                \"gpu_memory_gb\": 40,\n",
    "                \"hourly_on_demand\": 3.83,\n",
    "                \"hourly_spot\": 1.15,\n",
    "                \"vcpu\": 8,\n",
    "                \"ram_gb\": 96\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def select_optimal_instance(self, gpu_memory_needed: int) -> str:\n",
    "        \"\"\"\n",
    "        Select the most cost-effective instance type\n",
    "        \"\"\"\n",
    "        suitable_instances = [\n",
    "            (name, spec) for name, spec in self.instance_pricing.items()\n",
    "            if spec['gpu_memory_gb'] >= gpu_memory_needed\n",
    "        ]\n",
    "        \n",
    "        if not suitable_instances:\n",
    "            return \"p4d.2xlarge\"  # Fallback to largest\n",
    "        \n",
    "        # Sort by cost efficiency (cost per GB GPU memory)\n",
    "        sorted_instances = sorted(\n",
    "            suitable_instances,\n",
    "            key=lambda x: x[1]['hourly_spot'] / x[1]['gpu_memory_gb']\n",
    "        )\n",
    "        \n",
    "        return sorted_instances[0][0]\n",
    "    \n",
    "    def calculate_monthly_cost(self, requirements: ProductionRequirements, \n",
    "                             resources: Dict, use_spot: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate total monthly costs\n",
    "        \"\"\"\n",
    "        # Select instance type\n",
    "        gpu_per_instance = calculator.model_specs[requirements.model_size][\"gpu_memory_gb\"]\n",
    "        instance_type = self.select_optimal_instance(gpu_per_instance)\n",
    "        instance_spec = self.instance_pricing[instance_type]\n",
    "        \n",
    "        # Calculate compute costs\n",
    "        hourly_rate = instance_spec['hourly_spot'] if use_spot else instance_spec['hourly_on_demand']\n",
    "        hours_per_month = 24 * 30  # 720 hours\n",
    "        \n",
    "        compute_cost_per_instance = hourly_rate * hours_per_month\n",
    "        total_compute_cost = compute_cost_per_instance * resources['instances']\n",
    "        \n",
    "        # Additional costs\n",
    "        load_balancer_cost = 25 * len(requirements.geographic_regions)  # ALB cost\n",
    "        storage_cost = 100  # EBS for logs, configs\n",
    "        data_transfer_cost = 200  # Estimated\n",
    "        monitoring_cost = 200 if \"Prometheus\" in recommended_monitoring else 2000\n",
    "        \n",
    "        total_cost = (\n",
    "            total_compute_cost + \n",
    "            load_balancer_cost + \n",
    "            storage_cost + \n",
    "            data_transfer_cost + \n",
    "            monitoring_cost\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"instance_type\": instance_type,\n",
    "            \"instances\": resources['instances'],\n",
    "            \"hourly_rate_per_instance\": hourly_rate,\n",
    "            \"compute_cost_monthly\": total_compute_cost,\n",
    "            \"load_balancer_cost\": load_balancer_cost,\n",
    "            \"storage_cost\": storage_cost,\n",
    "            \"data_transfer_cost\": data_transfer_cost,\n",
    "            \"monitoring_cost\": monitoring_cost,\n",
    "            \"total_monthly_cost\": total_cost,\n",
    "            \"cost_per_request\": total_cost / requirements.daily_request_volume / 30,\n",
    "            \"budget_utilization\": (total_cost / requirements.budget_monthly_usd) * 100\n",
    "        }\n",
    "\n",
    "# Calculate costs\n",
    "cost_calc = CostCalculator()\n",
    "\n",
    "# Compare spot vs on-demand\n",
    "cost_spot = cost_calc.calculate_monthly_cost(requirements, resources, use_spot=True)\n",
    "cost_on_demand = cost_calc.calculate_monthly_cost(requirements, resources, use_spot=False)\n",
    "\n",
    "print(\"Cost Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\n📊 Resource Requirements:\")\n",
    "print(f\"Instances needed: {resources['instances']}\")\n",
    "print(f\"Instance type: {cost_spot['instance_type']}\")\n",
    "print(f\"Total GPU memory: {resources['total_gpu_memory_gb']} GB\")\n",
    "\n",
    "print(f\"\\n💰 Cost Comparison:\")\n",
    "print(f\"\\nOn-Demand Pricing:\")\n",
    "print(f\"  Hourly per instance: ${cost_on_demand['hourly_rate_per_instance']:.3f}\")\n",
    "print(f\"  Monthly total: ${cost_on_demand['total_monthly_cost']:,.0f}\")\n",
    "print(f\"  Budget utilization: {cost_on_demand['budget_utilization']:.1f}%\")\n",
    "\n",
    "print(f\"\\nSpot Instance Pricing:\")\n",
    "print(f\"  Hourly per instance: ${cost_spot['hourly_rate_per_instance']:.3f}\")\n",
    "print(f\"  Monthly total: ${cost_spot['total_monthly_cost']:,.0f}\")\n",
    "print(f\"  Budget utilization: {cost_spot['budget_utilization']:.1f}%\")\n",
    "print(f\"  Savings: ${cost_on_demand['total_monthly_cost'] - cost_spot['total_monthly_cost']:,.0f} ({(1 - cost_spot['total_monthly_cost']/cost_on_demand['total_monthly_cost'])*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\n📈 Cost Breakdown (Spot):\")\n",
    "print(f\"  Compute: ${cost_spot['compute_cost_monthly']:,.0f} ({cost_spot['compute_cost_monthly']/cost_spot['total_monthly_cost']*100:.0f}%)\")\n",
    "print(f\"  Load balancer: ${cost_spot['load_balancer_cost']:,.0f}\")\n",
    "print(f\"  Storage: ${cost_spot['storage_cost']:,.0f}\")\n",
    "print(f\"  Data transfer: ${cost_spot['data_transfer_cost']:,.0f}\")\n",
    "print(f\"  Monitoring: ${cost_spot['monitoring_cost']:,.0f}\")\n",
    "\n",
    "print(f\"\\n🎯 Cost per request: ${cost_spot['cost_per_request']:.6f}\")\n",
    "\n",
    "# Check if within budget\n",
    "if cost_spot['budget_utilization'] <= 100:\n",
    "    print(f\"\\n✅ Within budget (${requirements.budget_monthly_usd:,})\")\n",
    "else:\n",
    "    print(f\"\\n❌ Over budget by ${cost_spot['total_monthly_cost'] - requirements.budget_monthly_usd:,.0f}\")\n",
    "    print(\"   Consider: smaller model, fewer instances, or spot instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Capacity Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic pattern analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulate daily traffic pattern\n",
    "hours = np.arange(24)\n",
    "base_traffic = requirements.average_requests_per_second\n",
    "\n",
    "# Typical business hours pattern\n",
    "traffic_multiplier = np.array([\n",
    "    0.1, 0.05, 0.05, 0.05, 0.1, 0.2,   # 00-05: Very low\n",
    "    0.4, 0.7, 1.2, 1.5, 1.8, 2.0,     # 06-11: Morning ramp\n",
    "    1.9, 1.7, 2.2, 2.5, 2.3, 2.0,     # 12-17: Peak hours\n",
    "    1.5, 1.2, 0.8, 0.5, 0.3, 0.2      # 18-23: Evening decline\n",
    "])\n",
    "\n",
    "daily_traffic = base_traffic * traffic_multiplier\n",
    "peak_hour_traffic = np.max(daily_traffic)\n",
    "\n",
    "# Plot traffic pattern\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Daily traffic pattern\n",
    "ax1.plot(hours, daily_traffic, marker='o', linewidth=2)\n",
    "ax1.axhline(y=requirements.peak_requests_per_second, color='r', linestyle='--', \n",
    "           label=f'Capacity: {requirements.peak_requests_per_second} RPS')\n",
    "ax1.axhline(y=peak_hour_traffic, color='orange', linestyle='--', \n",
    "           label=f'Peak hour: {peak_hour_traffic:.0f} RPS')\n",
    "ax1.set_xlabel('Hour of Day')\n",
    "ax1.set_ylabel('Requests per Second')\n",
    "ax1.set_title('Daily Traffic Pattern')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Resource utilization\n",
    "capacity = resources['total_throughput_tokens_per_sec'] / requirements.average_response_tokens\n",
    "utilization = daily_traffic / capacity * 100\n",
    "\n",
    "ax2.plot(hours, utilization, marker='s', linewidth=2, color='green')\n",
    "ax2.axhline(y=80, color='orange', linestyle='--', label='Target utilization: 80%')\n",
    "ax2.axhline(y=100, color='red', linestyle='--', label='Max capacity: 100%')\n",
    "ax2.set_xlabel('Hour of Day')\n",
    "ax2.set_ylabel('Resource Utilization (%)')\n",
    "ax2.set_title('Resource Utilization Pattern')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Capacity Planning Analysis:\")\n",
    "print(f\"Peak hour traffic: {peak_hour_traffic:.0f} RPS\")\n",
    "print(f\"Provisioned capacity: {capacity:.0f} RPS\")\n",
    "print(f\"Peak utilization: {np.max(utilization):.1f}%\")\n",
    "print(f\"Average utilization: {np.mean(utilization):.1f}%\")\n",
    "\n",
    "if np.max(utilization) > 90:\n",
    "    print(\"\\n⚠️  Warning: Peak utilization > 90%, consider auto-scaling\")\n",
    "elif np.max(utilization) < 60:\n",
    "    print(\"\\n💡 Suggestion: Low utilization, consider cost optimization\")\n",
    "else:\n",
    "    print(\"\\n✅ Good capacity planning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Architecture Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate architecture summary\n",
    "architecture_summary = f\"\"\"\n",
    "Production Architecture Summary\n",
    "===============================\n",
    "\n",
    "🎯 Requirements:\n",
    "  • Peak traffic: {requirements.peak_requests_per_second:,} RPS\n",
    "  • Latency SLA: P95 < {requirements.max_latency_p95}ms\n",
    "  • Availability: {requirements.target_availability*100}%\n",
    "  • Regions: {len(requirements.geographic_regions)}\n",
    "  • Model: {requirements.model_size}\n",
    "\n",
    "🏗️  Recommended Architecture:\n",
    "  • Pattern: {recommended}\n",
    "  • Inference Engine: {recommended_engine}\n",
    "  • Container Platform: {recommended_platform}\n",
    "  • Monitoring: {recommended_monitoring}\n",
    "\n",
    "💻 Resource Requirements:\n",
    "  • Instances: {resources['instances']}\n",
    "  • Instance type: {cost_spot['instance_type']}\n",
    "  • Total GPU memory: {resources['total_gpu_memory_gb']} GB\n",
    "  • Total CPU cores: {resources['total_cpu_cores']}\n",
    "  • Total RAM: {resources['total_ram_gb']} GB\n",
    "\n",
    "💰 Cost Estimate (Spot pricing):\n",
    "  • Monthly cost: ${cost_spot['total_monthly_cost']:,.0f}\n",
    "  • Budget utilization: {cost_spot['budget_utilization']:.1f}%\n",
    "  • Cost per request: ${cost_spot['cost_per_request']:.6f}\n",
    "  • Savings vs on-demand: {(1 - cost_spot['total_monthly_cost']/cost_on_demand['total_monthly_cost'])*100:.0f}%\n",
    "\n",
    "📈 Performance Capacity:\n",
    "  • Throughput: {resources['total_throughput_tokens_per_sec']:,} tokens/sec\n",
    "  • Concurrent requests: {resources['total_concurrent_requests']}\n",
    "  • Peak utilization: {np.max(utilization):.1f}%\n",
    "\"\"\"\n",
    "\n",
    "print(architecture_summary)\n",
    "\n",
    "# Save architecture plan to file\n",
    "with open('architecture_plan.txt', 'w') as f:\n",
    "    f.write(architecture_summary)\n",
    "\n",
    "print(\"\\n✅ Architecture plan saved to 'architecture_plan.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ **Completed**:\n",
    "1. Analyzed production requirements\n",
    "2. Calculated resource needs\n",
    "3. Evaluated architecture patterns\n",
    "4. Selected optimal technology stack\n",
    "5. Estimated costs and capacity\n",
    "6. Generated architecture plan\n",
    "\n",
    "📊 **Key Decisions**:\n",
    "- Architecture pattern based on scale and requirements\n",
    "- Cost-optimized instance selection\n",
    "- Technology stack for production readiness\n",
    "- Capacity planning for traffic patterns\n",
    "\n",
    "➡️ **Next**: In `02-Deployment_Implementation.ipynb`, we'll implement:\n",
    "- Docker optimization and builds\n",
    "- Kubernetes deployment configs\n",
    "- Auto-scaling setup\n",
    "- CI/CD pipeline design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Modify requirements**: Change peak RPS to 5000 and see how architecture recommendations change\n",
    "2. **Cost optimization**: Compare different instance types and spot vs on-demand pricing\n",
    "3. **Multi-region**: Add more regions and analyze the cost impact\n",
    "4. **Model scaling**: Compare resource needs for 7B vs 13B vs 70B models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}