{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.4: è¨˜æ†¶é«”åˆ†æèˆ‡å„ªåŒ– (Memory Profiling)\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- æŒæ¡ PyTorch è¨˜æ†¶é«”åˆ†æå·¥å…·\n",
    "- è­˜åˆ¥è¨“ç·´éç¨‹ä¸­çš„è¨˜æ†¶é«”ç“¶é ¸\n",
    "- ä½¿ç”¨ Profiler é€²è¡Œæ€§èƒ½åˆ†æ\n",
    "- åˆ¶å®šè¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 30-45åˆ†é˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. ç†è«–èƒŒæ™¯\n",
    "\n",
    "### 1.1 GPU è¨˜æ†¶é«”çµ„æˆ\n",
    "\n",
    "è¨“ç·´æ·±åº¦å­¸ç¿’æ¨¡å‹æ™‚, GPU è¨˜æ†¶é«”ä¸»è¦ç”¨æ–¼:\n",
    "\n",
    "```\n",
    "ç¸½è¨˜æ†¶é«” = æ¨¡å‹åƒæ•¸ + æ¢¯åº¦ + å„ªåŒ–å™¨ç‹€æ…‹ + æ¿€æ´»å€¼ + è‡¨æ™‚ç·©å­˜\n",
    "```\n",
    "\n",
    "**å„éƒ¨åˆ†å æ¯” (ä»¥ 7B æ¨¡å‹ FP32 è¨“ç·´ç‚ºä¾‹)**:\n",
    "- æ¨¡å‹åƒæ•¸: 28GB (7B Ã— 4 bytes)\n",
    "- æ¢¯åº¦: 28GB (èˆ‡åƒæ•¸åŒå¤§å°)\n",
    "- å„ªåŒ–å™¨ç‹€æ…‹ (Adam): 56GB (momentum + variance)\n",
    "- æ¿€æ´»å€¼: è¦–æ‰¹æ¬¡å¤§å°è€Œå®š (é€šå¸¸ 10-30GB)\n",
    "- **ç¸½è¨ˆ**: ~120-140GB\n",
    "\n",
    "### 1.2 è¨˜æ†¶é«”åˆ†æç›®æ¨™\n",
    "\n",
    "1. **å®šä½è¨˜æ†¶é«”å³°å€¼**: æ‰¾å‡ºè¨“ç·´éç¨‹ä¸­è¨˜æ†¶é«”å ç”¨æœ€é«˜çš„æ™‚åˆ»\n",
    "2. **è­˜åˆ¥è¨˜æ†¶é«”æ´©æ¼**: æª¢æŸ¥æ˜¯å¦æœ‰æœªé‡‹æ”¾çš„å¼µé‡\n",
    "3. **å„ªåŒ–è¨˜æ†¶é«”åˆ†é…**: æ¸›å°‘ä¸å¿…è¦çš„è¨˜æ†¶é«”é–‹éŠ·\n",
    "4. **é æ¸¬è¨˜æ†¶é«”éœ€æ±‚**: ç‚ºç”Ÿç”¢ç’°å¢ƒè¦åŠƒç¡¬é«”è³‡æº"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2. ç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "ä½¿ç”¨è¨­å‚™: cuda\n",
      "GPU: NVIDIA RTX 2000 Ada Generation\n",
      "ç¸½è¨˜æ†¶é«”: 16.71 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ç¸½è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3. åŸºç¤è¨˜æ†¶é«”ç›£æ§å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU è¨˜æ†¶é«”åˆå§‹ç‹€æ…‹\n",
      "============================================================\n",
      "è¨˜æ†¶é«”ä½¿ç”¨:\n",
      "  å·²åˆ†é… (Allocated): 0.000 GB\n",
      "  å·²ä¿ç•™ (Reserved):  0.000 GB\n",
      "  å³°å€¼åˆ†é… (Peak):     0.000 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'allocated': 0.0, 'reserved': 0.0, 'peak': 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_gpu_memory(prefix=\"\"):\n",
    "    \"\"\"æ‰“å°ç•¶å‰ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(f\"{prefix}GPU ä¸å¯ç”¨\")\n",
    "        return\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    print(f\"{prefix}è¨˜æ†¶é«”ä½¿ç”¨:\")\n",
    "    print(f\"  å·²åˆ†é… (Allocated): {allocated:.3f} GB\")\n",
    "    print(f\"  å·²ä¿ç•™ (Reserved):  {reserved:.3f} GB\")\n",
    "    print(f\"  å³°å€¼åˆ†é… (Peak):     {max_allocated:.3f} GB\")\n",
    "    \n",
    "    return {\n",
    "        \"allocated\": allocated,\n",
    "        \"reserved\": reserved,\n",
    "        \"peak\": max_allocated\n",
    "    }\n",
    "\n",
    "\n",
    "def reset_gpu_memory():\n",
    "    \"\"\"é‡ç½® GPU è¨˜æ†¶é«”çµ±è¨ˆ\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# æ¸¬è©¦\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU è¨˜æ†¶é«”åˆå§‹ç‹€æ…‹\")\n",
    "print(\"=\" * 60)\n",
    "reset_gpu_memory()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4. è¨˜æ†¶é«”è¿½è¹¤å™¨é¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è¨˜æ†¶é«”åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class MemoryProfiler:\n",
    "    \"\"\"è¨˜æ†¶é«”åˆ†æå™¨\"\"\"\n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "        self.timeline = []\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"é‡ç½®åˆ†æå™¨\"\"\"\n",
    "        self.snapshots = []\n",
    "        self.timeline = []\n",
    "        reset_gpu_memory()\n",
    "    \n",
    "    def snapshot(self, label=\"\", timestamp=None):\n",
    "        \"\"\"è¨˜éŒ„ç•¶å‰è¨˜æ†¶é«”å¿«ç…§\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        \n",
    "        if timestamp is None:\n",
    "            timestamp = time.time()\n",
    "        \n",
    "        snapshot = {\n",
    "            \"label\": label,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9,\n",
    "            \"reserved\": torch.cuda.memory_reserved() / 1e9,\n",
    "            \"peak\": torch.cuda.max_memory_allocated() / 1e9\n",
    "        }\n",
    "        \n",
    "        self.snapshots.append(snapshot)\n",
    "        return snapshot\n",
    "    \n",
    "    def start_timeline(self):\n",
    "        \"\"\"é–‹å§‹è¨˜éŒ„è¨˜æ†¶é«”æ™‚é–“ç·š\"\"\"\n",
    "        self.timeline = []\n",
    "        self.timeline_start = time.time()\n",
    "    \n",
    "    def record_timeline(self, label=\"\"):\n",
    "        \"\"\"è¨˜éŒ„æ™‚é–“ç·šé»\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        \n",
    "        elapsed = time.time() - self.timeline_start\n",
    "        self.timeline.append({\n",
    "            \"time\": elapsed,\n",
    "            \"label\": label,\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9\n",
    "        })\n",
    "    \n",
    "    def plot_snapshots(self):\n",
    "        \"\"\"ç¹ªè£½è¨˜æ†¶é«”å¿«ç…§\"\"\"\n",
    "        if not self.snapshots:\n",
    "            print(\"æ²’æœ‰å¿«ç…§å¯ç¹ªè£½\")\n",
    "            return\n",
    "        \n",
    "        labels = [s[\"label\"] for s in self.snapshots]\n",
    "        allocated = [s[\"allocated\"] for s in self.snapshots]\n",
    "        reserved = [s[\"reserved\"] for s in self.snapshots]\n",
    "        peak = [s[\"peak\"] for s in self.snapshots]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax.bar(x - width, allocated, width, label=\"å·²åˆ†é…\", color=\"#3498db\")\n",
    "        ax.bar(x, reserved, width, label=\"å·²ä¿ç•™\", color=\"#95a5a6\")\n",
    "        ax.bar(x + width, peak, width, label=\"å³°å€¼\", color=\"#e74c3c\")\n",
    "        \n",
    "        ax.set_xlabel(\"éšæ®µ\")\n",
    "        ax.set_ylabel(\"è¨˜æ†¶é«” (GB)\")\n",
    "        ax.set_title(\"è¨˜æ†¶é«”ä½¿ç”¨å¿«ç…§\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_timeline(self):\n",
    "        \"\"\"ç¹ªè£½è¨˜æ†¶é«”æ™‚é–“ç·š\"\"\"\n",
    "        if not self.timeline:\n",
    "            print(\"æ²’æœ‰æ™‚é–“ç·šå¯ç¹ªè£½\")\n",
    "            return\n",
    "        \n",
    "        times = [t[\"time\"] for t in self.timeline]\n",
    "        memory = [t[\"allocated\"] for t in self.timeline]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(times, memory, linewidth=2, color=\"#3498db\", marker=\"o\")\n",
    "        plt.xlabel(\"æ™‚é–“ (ç§’)\")\n",
    "        plt.ylabel(\"å·²åˆ†é…è¨˜æ†¶é«” (GB)\")\n",
    "        plt.title(\"è¨˜æ†¶é«”ä½¿ç”¨æ™‚é–“ç·š\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"æ‰“å°åˆ†ææ‘˜è¦\"\"\"\n",
    "        if not self.snapshots:\n",
    "            print(\"æ²’æœ‰å¿«ç…§æ•¸æ“š\")\n",
    "            return\n",
    "        \n",
    "        peak_snapshot = max(self.snapshots, key=lambda s: s[\"allocated\"])\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"è¨˜æ†¶é«”åˆ†ææ‘˜è¦\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ç¸½å¿«ç…§æ•¸: {len(self.snapshots)}\")\n",
    "        print(f\"\\nå³°å€¼è¨˜æ†¶é«”ä½¿ç”¨:\")\n",
    "        print(f\"  éšæ®µ: {peak_snapshot['label']}\")\n",
    "        print(f\"  å·²åˆ†é…: {peak_snapshot['allocated']:.3f} GB\")\n",
    "        print(f\"  å·²ä¿ç•™: {peak_snapshot['reserved']:.3f} GB\")\n",
    "        print(f\"  å³°å€¼: {peak_snapshot['peak']:.3f} GB\")\n",
    "\n",
    "\n",
    "# å‰µå»ºå…¨å±€åˆ†æå™¨\n",
    "profiler = MemoryProfiler()\n",
    "print(\"âœ… è¨˜æ†¶é«”åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 5. å¯¦é©— 1: æ¨¡å‹è¼‰å…¥çš„è¨˜æ†¶é«”åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "å¯¦é©— 1: æ¨¡å‹è¼‰å…¥è¨˜æ†¶é«”åˆ†æ\n",
      "======================================================================\n",
      "\n",
      "è¼‰å…¥ GPT-2 Medium...\n",
      "ç§»å‹•åˆ° GPU...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 23.06 MiB is free. Process 3859131 has 12.57 GiB memory in use. Process 3880458 has 2.10 GiB memory in use. Including non-PyTorch memory, this process has 510.00 MiB memory in use. Of the allocated memory 413.55 MiB is allocated by PyTorch, and 12.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ç§»åˆ° GPU\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mç§»å‹•åˆ° GPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m profiler\u001b[38;5;241m.\u001b[39msnapshot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ¨¡å‹ç§»åˆ° GPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# è¨ˆç®—æ¨¡å‹åƒæ•¸é‡\u001b[39;00m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4343\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   4339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4340\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4342\u001b[0m         )\n\u001b[0;32m-> 4343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 23.06 MiB is free. Process 3859131 has 12.57 GiB memory in use. Process 3880458 has 2.10 GiB memory in use. Including non-PyTorch memory, this process has 510.00 MiB memory in use. Of the allocated memory 413.55 MiB is allocated by PyTorch, and 12.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 1: æ¨¡å‹è¼‰å…¥è¨˜æ†¶é«”åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "profiler.reset()\n",
    "profiler.snapshot(\"åˆå§‹ç‹€æ…‹\")\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "print(\"\\nè¼‰å…¥ GPT-2 Medium...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "profiler.snapshot(\"æ¨¡å‹è¼‰å…¥ (CPU)\")\n",
    "\n",
    "# ç§»åˆ° GPU\n",
    "print(\"ç§»å‹•åˆ° GPU...\")\n",
    "model = model.to(device)\n",
    "profiler.snapshot(\"æ¨¡å‹ç§»åˆ° GPU\")\n",
    "\n",
    "# è¨ˆç®—æ¨¡å‹åƒæ•¸é‡\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "param_size_gb = total_params * 4 / 1e9  # FP32: 4 bytes per param\n",
    "\n",
    "print(f\"\\næ¨¡å‹çµ±è¨ˆ:\")\n",
    "print(f\"  ç¸½åƒæ•¸é‡: {total_params / 1e6:.1f}M\")\n",
    "print(f\"  å¯è¨“ç·´åƒæ•¸: {trainable_params / 1e6:.1f}M\")\n",
    "print(f\"  ç†è«–å¤§å° (FP32): {param_size_gb:.3f} GB\")\n",
    "\n",
    "# é¡¯ç¤ºè¨˜æ†¶é«”å¿«ç…§\n",
    "profiler.plot_snapshots()\n",
    "profiler.print_summary()\n",
    "\n",
    "# åˆ†æ\n",
    "stats = profiler.snapshots[-1]\n",
    "print(f\"\\nåˆ†æ:\")\n",
    "print(f\"  å¯¦éš›è¨˜æ†¶é«”å ç”¨: {stats['allocated']:.3f} GB\")\n",
    "print(f\"  ç†è«–åƒæ•¸å¤§å°: {param_size_gb:.3f} GB\")\n",
    "print(f\"  é–‹éŠ·: {(stats['allocated'] - param_size_gb) / param_size_gb * 100:.1f}%\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del model\n",
    "reset_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 6. å¯¦é©— 2: è¨“ç·´éç¨‹è¨˜æ†¶é«”åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æº–å‚™æ•¸æ“š\n",
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, num_samples=100, seq_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.texts = [f\"The quick brown fox jumps over the lazy dog. \" * 10 for _ in range(num_samples)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encodings = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = SimpleTextDataset(tokenizer, num_samples=100, seq_length=128)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 2: è¨“ç·´éç¨‹è¨˜æ†¶é«”åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# é‡æ–°è¼‰å…¥æ¨¡å‹\n",
    "profiler.reset()\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "profiler.snapshot(\"è¨“ç·´å‰\")\n",
    "\n",
    "# è¨“ç·´ä¸€å€‹ epoch\n",
    "model.train()\n",
    "profiler.start_timeline()\n",
    "\n",
    "print(\"\\né–‹å§‹è¨“ç·´...\")\n",
    "for step, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    # è¨˜éŒ„å‰å‘å‚³æ’­å‰\n",
    "    if step == 0:\n",
    "        profiler.record_timeline(\"å‰å‘å‰\")\n",
    "    \n",
    "    # å‰å‘å‚³æ’­\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    if step == 0:\n",
    "        profiler.record_timeline(\"å‰å‘å¾Œ\")\n",
    "    \n",
    "    # åå‘å‚³æ’­\n",
    "    loss.backward()\n",
    "    \n",
    "    if step == 0:\n",
    "        profiler.record_timeline(\"åå‘å¾Œ\")\n",
    "    \n",
    "    # æ›´æ–°åƒæ•¸\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step == 0:\n",
    "        profiler.record_timeline(\"æ›´æ–°å¾Œ\")\n",
    "        profiler.snapshot(\"ç¬¬1æ¬¡è¿­ä»£å®Œæˆ\")\n",
    "    \n",
    "    # æ¯10æ­¥è¨˜éŒ„ä¸€æ¬¡\n",
    "    if step % 10 == 0:\n",
    "        profiler.record_timeline(f\"Step {step}\")\n",
    "\n",
    "profiler.snapshot(\"è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# ç¹ªè£½çµæœ\n",
    "print(\"\\nè¨˜æ†¶é«”å¿«ç…§:\")\n",
    "profiler.plot_snapshots()\n",
    "\n",
    "print(\"\\nè¨˜æ†¶é«”æ™‚é–“ç·š:\")\n",
    "profiler.plot_timeline()\n",
    "\n",
    "profiler.print_summary()\n",
    "\n",
    "# æ¸…ç†\n",
    "del model, optimizer\n",
    "reset_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 7. å¯¦é©— 3: ä¸åŒæ‰¹æ¬¡å¤§å°çš„è¨˜æ†¶é«”å½±éŸ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 3: æ‰¹æ¬¡å¤§å°å°è¨˜æ†¶é«”çš„å½±éŸ¿\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def measure_batch_size_memory(batch_size, num_steps=10):\n",
    "    \"\"\"æ¸¬é‡ç‰¹å®šæ‰¹æ¬¡å¤§å°çš„è¨˜æ†¶é«”ä½¿ç”¨\"\"\"\n",
    "    reset_gpu_memory()\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        try:\n",
    "            batch = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "            batch = next(dataloader_iter)\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    del model, optimizer\n",
    "    reset_gpu_memory()\n",
    "    \n",
    "    return peak_memory\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°\n",
    "batch_sizes = [1, 2, 4, 8, 16]\n",
    "memory_usage = []\n",
    "\n",
    "print(\"\\næ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°...\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"  æ¸¬è©¦ batch_size={bs}...\", end=\" \")\n",
    "    try:\n",
    "        mem = measure_batch_size_memory(bs, num_steps=10)\n",
    "        memory_usage.append(mem)\n",
    "        print(f\"å³°å€¼è¨˜æ†¶é«”: {mem:.2f} GB\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"OOM (è¨˜æ†¶é«”ä¸è¶³)\")\n",
    "            memory_usage.append(None)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# ç¹ªè£½çµæœ\n",
    "valid_bs = [bs for bs, mem in zip(batch_sizes, memory_usage) if mem is not None]\n",
    "valid_mem = [mem for mem in memory_usage if mem is not None]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(valid_bs, valid_mem, marker=\"o\", linewidth=2, markersize=10, color=\"#3498db\")\n",
    "plt.xlabel(\"æ‰¹æ¬¡å¤§å° (Batch Size)\")\n",
    "plt.ylabel(\"å³°å€¼è¨˜æ†¶é«” (GB)\")\n",
    "plt.title(\"æ‰¹æ¬¡å¤§å° vs è¨˜æ†¶é«”ä½¿ç”¨\", fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "for bs, mem in zip(valid_bs, valid_mem):\n",
    "    plt.text(bs, mem, f\"{mem:.2f}GB\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# åˆ†æ\n",
    "print(\"\\n=\" * 70)\n",
    "print(\"æ‰¹æ¬¡å¤§å°è¨˜æ†¶é«”åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'æ‰¹æ¬¡å¤§å°':<15} {'å³°å€¼è¨˜æ†¶é«” (GB)':<20} {'ç›¸å°æ–¼ BS=1':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for bs, mem in zip(batch_sizes, memory_usage):\n",
    "    if mem is not None:\n",
    "        relative = f\"{mem / memory_usage[0]:.2f}x\" if memory_usage[0] else \"N/A\"\n",
    "        print(f\"{bs:<15} {mem:<20.2f} {relative:<20}\")\n",
    "    else:\n",
    "        print(f\"{bs:<15} {'OOM':<20} {'N/A':<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 8. å¯¦é©— 4: PyTorch Profiler è©³ç´°åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 4: PyTorch Profiler è©³ç´°æ€§èƒ½åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "reset_gpu_memory()\n",
    "\n",
    "# é‡æ–°è¼‰å…¥æ¨¡å‹\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(\"\\nä½¿ç”¨ PyTorch Profiler åˆ†æè¨“ç·´...\")\n",
    "print(\"(é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)\\n\")\n",
    "\n",
    "model.train()\n",
    "dataloader_iter = iter(dataloader)\n",
    "\n",
    "# ä½¿ç”¨ Profiler\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=False\n",
    ") as prof:\n",
    "    for step in range(5):  # åªåˆ†æ 5 æ­¥\n",
    "        with record_function(f\"training_step_{step}\"):\n",
    "            batch = next(dataloader_iter)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with record_function(\"forward\"):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            with record_function(\"backward\"):\n",
    "                loss.backward()\n",
    "            \n",
    "            with record_function(\"optimizer_step\"):\n",
    "                optimizer.step()\n",
    "\n",
    "# æ‰“å° Profiler çµæœ\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Profiler å ±å‘Š (æŒ‰ CUDA æ™‚é–“æ’åº, Top 10)\")\n",
    "print(\"=\" * 70)\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"cuda_time_total\",\n",
    "    row_limit=10\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Profiler å ±å‘Š (æŒ‰è¨˜æ†¶é«”ä½¿ç”¨æ’åº, Top 10)\")\n",
    "print(\"=\" * 70)\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"self_cuda_memory_usage\",\n",
    "    row_limit=10\n",
    "))\n",
    "\n",
    "# åˆ†æå‡½æ•¸ç´šåˆ¥çš„æ€§èƒ½\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"å‡½æ•¸ç´šåˆ¥åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "events = prof.key_averages()\n",
    "for evt in events:\n",
    "    if evt.key in [\"forward\", \"backward\", \"optimizer_step\"]:\n",
    "        print(f\"\\n{evt.key}:\")\n",
    "        print(f\"  CPU æ™‚é–“: {evt.cpu_time_total / 1e3:.2f} ms\")\n",
    "        print(f\"  CUDA æ™‚é–“: {evt.cuda_time_total / 1e3:.2f} ms\")\n",
    "        print(f\"  è¨˜æ†¶é«”: {evt.cpu_memory_usage / 1e6:.2f} MB\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del model, optimizer\n",
    "reset_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 9. å¯¦é©— 5: å„ªåŒ–æŠ€è¡“å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 5: å„ªåŒ–æŠ€è¡“è¨˜æ†¶é«”å°æ¯”\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def measure_optimization(config_name, use_amp=False, use_checkpoint=False, accumulation_steps=1):\n",
    "    \"\"\"æ¸¬é‡ä¸åŒå„ªåŒ–é…ç½®çš„è¨˜æ†¶é«”ä½¿ç”¨\"\"\"\n",
    "    reset_gpu_memory()\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(device)\n",
    "    if use_checkpoint:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # è¨“ç·´å¹¾æ­¥\n",
    "    for step in range(10):\n",
    "        if step % accumulation_steps == 0:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        batch = next(dataloader_iter)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        if use_amp:\n",
    "            with autocast(dtype=torch.float16):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "        else:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    del model, optimizer\n",
    "    reset_gpu_memory()\n",
    "    \n",
    "    return peak_memory\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ä¸åŒé…ç½®\n",
    "configs = [\n",
    "    (\"åŸºæº– (ç„¡å„ªåŒ–)\", False, False, 1),\n",
    "    (\"æ··åˆç²¾åº¦ (FP16)\", True, False, 1),\n",
    "    (\"æ¢¯åº¦æª¢æŸ¥é»\", False, True, 1),\n",
    "    (\"æ¢¯åº¦ç´¯ç© (x4)\", False, False, 4),\n",
    "    (\"FP16 + æª¢æŸ¥é»\", True, True, 1),\n",
    "    (\"å…¨éƒ¨å„ªåŒ–\", True, True, 4)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\næ¸¬è©¦ä¸åŒå„ªåŒ–é…ç½®...\")\n",
    "for config_name, use_amp, use_checkpoint, accum in configs:\n",
    "    print(f\"  {config_name}...\", end=\" \")\n",
    "    try:\n",
    "        mem = measure_optimization(config_name, use_amp, use_checkpoint, accum)\n",
    "        results.append((config_name, mem))\n",
    "        print(f\"å³°å€¼: {mem:.2f} GB\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"OOM\")\n",
    "            results.append((config_name, None))\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# ç¹ªè£½çµæœ\n",
    "valid_results = [(name, mem) for name, mem in results if mem is not None]\n",
    "names = [name for name, _ in valid_results]\n",
    "memories = [mem for _, mem in valid_results]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#f39c12\", \"#9b59b6\", \"#1abc9c\"]\n",
    "bars = plt.bar(range(len(names)), memories, color=colors[:len(names)])\n",
    "plt.xticks(range(len(names)), names, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"å³°å€¼è¨˜æ†¶é«” (GB)\")\n",
    "plt.title(\"ä¸åŒå„ªåŒ–æŠ€è¡“çš„è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”\", fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "for bar, mem in zip(bars, memories):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{mem:.2f}GB',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# åˆ†æ\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"å„ªåŒ–æŠ€è¡“è¨˜æ†¶é«”ç¯€çœåˆ†æ\")\n",
    "print(\"=\" * 80)\n",
    "baseline_mem = results[0][1]\n",
    "print(f\"\\n{'é…ç½®':<25} {'å³°å€¼è¨˜æ†¶é«” (GB)':<20} {'ç¯€çœæ¯”ä¾‹':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for name, mem in results:\n",
    "    if mem is not None:\n",
    "        saving = (baseline_mem - mem) / baseline_mem * 100 if baseline_mem else 0\n",
    "        print(f\"{name:<25} {mem:<20.2f} {saving:>12.1f}%\")\n",
    "    else:\n",
    "        print(f\"{name:<25} {'OOM':<20} {'N/A':<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 10. è¨˜æ†¶é«”å„ªåŒ–å»ºè­°ç”Ÿæˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimization_suggestions(model_params_gb, available_gpu_gb, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šç¡¬é«”é…ç½®ç”Ÿæˆè¨˜æ†¶é«”å„ªåŒ–å»ºè­°\n",
    "    \n",
    "    Args:\n",
    "        model_params_gb: æ¨¡å‹åƒæ•¸å¤§å° (GB)\n",
    "        available_gpu_gb: å¯ç”¨ GPU è¨˜æ†¶é«” (GB)\n",
    "        batch_size: æœŸæœ›çš„æ‰¹æ¬¡å¤§å°\n",
    "        seq_length: åºåˆ—é•·åº¦\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"è¨˜æ†¶é«”å„ªåŒ–å»ºè­°ç”Ÿæˆå™¨\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\né…ç½®ä¿¡æ¯:\")\n",
    "    print(f\"  æ¨¡å‹å¤§å°: {model_params_gb:.2f} GB\")\n",
    "    print(f\"  å¯ç”¨è¨˜æ†¶é«”: {available_gpu_gb:.2f} GB\")\n",
    "    print(f\"  ç›®æ¨™æ‰¹æ¬¡: {batch_size}\")\n",
    "    print(f\"  åºåˆ—é•·åº¦: {seq_length}\")\n",
    "    \n",
    "    # ä¼°ç®—è¨˜æ†¶é«”éœ€æ±‚\n",
    "    # æ¨¡å‹åƒæ•¸ + æ¢¯åº¦ + å„ªåŒ–å™¨ç‹€æ…‹ (Adam: 2x params) + æ¿€æ´»å€¼\n",
    "    estimated_base = model_params_gb * 4  # params + grads + optimizer\n",
    "    activation_estimate = batch_size * seq_length * 0.001  # ç²—ç•¥ä¼°è¨ˆ\n",
    "    total_estimate = estimated_base + activation_estimate\n",
    "    \n",
    "    print(f\"\\nä¼°ç®—è¨˜æ†¶é«”éœ€æ±‚:\")\n",
    "    print(f\"  æ¨¡å‹ + æ¢¯åº¦ + å„ªåŒ–å™¨: {estimated_base:.2f} GB\")\n",
    "    print(f\"  æ¿€æ´»å€¼ (ä¼°è¨ˆ): {activation_estimate:.2f} GB\")\n",
    "    print(f\"  ç¸½è¨ˆ: {total_estimate:.2f} GB\")\n",
    "    \n",
    "    # ç”Ÿæˆå»ºè­°\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"å„ªåŒ–å»ºè­°\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    if total_estimate > available_gpu_gb:\n",
    "        shortage = total_estimate - available_gpu_gb\n",
    "        print(f\"\\nâš ï¸  è¨˜æ†¶é«”ä¸è¶³: ç¼ºå°‘ {shortage:.2f} GB\")\n",
    "        print(\"\\nå¿…è¦å„ªåŒ– (æŒ‰å„ªå…ˆç´š):\")\n",
    "        \n",
    "        # 1. æ··åˆç²¾åº¦\n",
    "        print(\"\\n1. âœ… å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´ (FP16/BF16)\")\n",
    "        print(\"   - è¨˜æ†¶é«”ç¯€çœ: ~50%\")\n",
    "        print(\"   - é€Ÿåº¦æå‡: 2-3x\")\n",
    "        print(\"   - ä»£ç¢¼: model.to(device); use autocast()\")\n",
    "        suggestions.append(\"mixed_precision\")\n",
    "        \n",
    "        estimated_base *= 0.5\n",
    "        total_estimate = estimated_base + activation_estimate\n",
    "        \n",
    "        if total_estimate > available_gpu_gb:\n",
    "            # 2. æ¢¯åº¦æª¢æŸ¥é»\n",
    "            print(\"\\n2. âœ… å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»\")\n",
    "            print(\"   - è¨˜æ†¶é«”ç¯€çœ: ~40%\")\n",
    "            print(\"   - æ™‚é–“é–‹éŠ·: +20-30%\")\n",
    "            print(\"   - ä»£ç¢¼: model.gradient_checkpointing_enable()\")\n",
    "            suggestions.append(\"gradient_checkpointing\")\n",
    "            \n",
    "            activation_estimate *= 0.6\n",
    "            total_estimate = estimated_base + activation_estimate\n",
    "        \n",
    "        if total_estimate > available_gpu_gb:\n",
    "            # 3. æ¸›å°æ‰¹æ¬¡ + æ¢¯åº¦ç´¯ç©\n",
    "            suggested_micro_batch = max(1, batch_size // 4)\n",
    "            accumulation = batch_size // suggested_micro_batch\n",
    "            print(f\"\\n3. âœ… æ¸›å°æ‰¹æ¬¡å¤§å° + æ¢¯åº¦ç´¯ç©\")\n",
    "            print(f\"   - å»ºè­°: micro_batch={suggested_micro_batch}, accumulation={accumulation}\")\n",
    "            print(f\"   - æœ‰æ•ˆæ‰¹æ¬¡: {suggested_micro_batch * accumulation}\")\n",
    "            print(f\"   - è¨˜æ†¶é«”ç¯€çœ: ~{(1 - suggested_micro_batch/batch_size)*100:.0f}%\")\n",
    "            suggestions.append(f\"gradient_accumulation_{accumulation}\")\n",
    "        \n",
    "        if total_estimate > available_gpu_gb:\n",
    "            # 4. è€ƒæ…®å…¶ä»–æ–¹æ¡ˆ\n",
    "            print(\"\\n4. ğŸ’¡ è€ƒæ…®å…¶ä»–æ–¹æ¡ˆ:\")\n",
    "            print(\"   - DeepSpeed ZeRO (å¤šGPU/CPU offload)\")\n",
    "            print(\"   - æ¨¡å‹é‡åŒ– (INT8/INT4)\")\n",
    "            print(\"   - PEFT æŠ€è¡“ (LoRA/QLoRA)\")\n",
    "    else:\n",
    "        margin = available_gpu_gb - total_estimate\n",
    "        print(f\"\\nâœ… è¨˜æ†¶é«”å……è¶³: å‰©é¤˜ {margin:.2f} GB\")\n",
    "        print(\"\\nå¯é¸å„ªåŒ– (æå‡é€Ÿåº¦):\")\n",
    "        print(\"\\n1. âš¡ å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´\")\n",
    "        print(\"   - é€Ÿåº¦æå‡: 2-3x\")\n",
    "        print(\"   - è¨˜æ†¶é«”é¡å¤–ç¯€çœ: ~50%\")\n",
    "        \n",
    "        if margin > model_params_gb:\n",
    "            print(\"\\n2. ğŸ“ˆ å¯ä»¥å¢å¤§æ‰¹æ¬¡å¤§å°\")\n",
    "            suggested_bs = int(batch_size * (1 + margin / total_estimate))\n",
    "            print(f\"   - å»ºè­°æ‰¹æ¬¡: {suggested_bs}\")\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨\n",
    "print(\"\\nç¤ºä¾‹ 1: 8GB GPU è¨“ç·´ GPT-2 Medium\")\n",
    "generate_optimization_suggestions(\n",
    "    model_params_gb=1.4,  # GPT-2 Medium FP32\n",
    "    available_gpu_gb=8.0,\n",
    "    batch_size=8,\n",
    "    seq_length=512\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"ç¤ºä¾‹ 2: 24GB GPU è¨“ç·´ GPT-2 Large\")\n",
    "generate_optimization_suggestions(\n",
    "    model_params_gb=3.0,  # GPT-2 Large FP32\n",
    "    available_gpu_gb=24.0,\n",
    "    batch_size=16,\n",
    "    seq_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 11. å¯¦é©—ç¸½çµèˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### å¯¦é©—çµè«–\n",
    "\n",
    "1. **è¨˜æ†¶é«”çµ„æˆç†è§£**:\n",
    "   - æ¨¡å‹åƒæ•¸: ~å›ºå®šå¤§å°\n",
    "   - æ¢¯åº¦: èˆ‡åƒæ•¸åŒå¤§å°\n",
    "   - å„ªåŒ–å™¨ç‹€æ…‹: Adam éœ€è¦ 2x åƒæ•¸å¤§å°\n",
    "   - æ¿€æ´»å€¼: èˆ‡æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•·åº¦æˆæ­£æ¯”\n",
    "\n",
    "2. **æ‰¹æ¬¡å¤§å°å½±éŸ¿**:\n",
    "   - æ‰¹æ¬¡è¶Šå¤§, æ¿€æ´»å€¼è¨˜æ†¶é«”è¶Šé«˜\n",
    "   - è¨˜æ†¶é«”å¢é•·æ¥è¿‘ç·šæ€§\n",
    "   - éœ€è¦åœ¨æ‰¹æ¬¡å¤§å°å’Œè¨˜æ†¶é«”ä¹‹é–“æ¬Šè¡¡\n",
    "\n",
    "3. **å„ªåŒ–æŠ€è¡“æ•ˆæœ**:\n",
    "   - æ··åˆç²¾åº¦: æœ€é«˜å„ªå…ˆç´š, ç¯€çœ ~50%, æé€Ÿ 2-3x\n",
    "   - æ¢¯åº¦æª¢æŸ¥é»: ç¯€çœ 30-50%, æ™‚é–“ä»£åƒ¹ 20-30%\n",
    "   - æ¢¯åº¦ç´¯ç©: ä¸å¢åŠ è¨˜æ†¶é«”, å¯¦ç¾å¤§æ‰¹æ¬¡æ•ˆæœ\n",
    "   - çµ„åˆå„ªåŒ–: å¯ç¯€çœ 70-80% è¨˜æ†¶é«”\n",
    "\n",
    "### è¨˜æ†¶é«”å„ªåŒ–æ±ºç­–æ¨¹\n",
    "\n",
    "```\n",
    "é–‹å§‹è¨“ç·´\n",
    "    |\n",
    "    â”œâ”€ OOM? â”€ å¦ â”€â†’ è€ƒæ…®æ··åˆç²¾åº¦åŠ é€Ÿ\n",
    "    |         |        |\n",
    "    |         â””â”€ OOM? â”€ å¦ â”€â†’ å¯ä»¥å¢å¤§æ‰¹æ¬¡\n",
    "    |\n",
    "    â””â”€ æ˜¯\n",
    "       |\n",
    "       â”œâ”€ æ­¥é©Ÿ 1: å•Ÿç”¨æ··åˆç²¾åº¦ (FP16/BF16)\n",
    "       |          â†“\n",
    "       |       é‚„ OOM?\n",
    "       |\n",
    "       â”œâ”€ æ­¥é©Ÿ 2: å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»\n",
    "       |          â†“\n",
    "       |       é‚„ OOM?\n",
    "       |\n",
    "       â”œâ”€ æ­¥é©Ÿ 3: æ¸›å°æ‰¹æ¬¡ + æ¢¯åº¦ç´¯ç©\n",
    "       |          â†“\n",
    "       |       é‚„ OOM?\n",
    "       |\n",
    "       â””â”€ æ­¥é©Ÿ 4: è€ƒæ…® DeepSpeed/PEFT/é‡åŒ–\n",
    "```\n",
    "\n",
    "### PyTorch è¨˜æ†¶é«”ç®¡ç†å·¥å…·ç¸½çµ\n",
    "\n",
    "#### 1. åŸºç¤ç›£æ§\n",
    "\n",
    "```python\n",
    "# æŸ¥çœ‹ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨\n",
    "torch.cuda.memory_allocated()  # å·²åˆ†é…\n",
    "torch.cuda.memory_reserved()   # å·²ä¿ç•™\n",
    "torch.cuda.max_memory_allocated()  # å³°å€¼\n",
    "\n",
    "# é‡ç½®çµ±è¨ˆ\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.empty_cache()  # æ¸…ç©ºç·©å­˜\n",
    "```\n",
    "\n",
    "#### 2. Profiler ä½¿ç”¨\n",
    "\n",
    "```python\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    profile_memory=True,\n",
    "    record_shapes=True\n",
    ") as prof:\n",
    "    # è¨“ç·´ä»£ç¢¼\n",
    "    pass\n",
    "\n",
    "# æŸ¥çœ‹çµæœ\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))\n",
    "```\n",
    "\n",
    "#### 3. è¨˜æ†¶é«”å¿«ç…§\n",
    "\n",
    "```python\n",
    "# è¨˜éŒ„è¨˜æ†¶é«”å¿«ç…§ (éœ€è¦ PyTorch 2.1+)\n",
    "torch.cuda.memory._record_memory_history()\n",
    "# ... è¨“ç·´ä»£ç¢¼\n",
    "torch.cuda.memory._dump_snapshot(\"memory_snapshot.pickle\")\n",
    "```\n",
    "\n",
    "### å¸¸è¦‹è¨˜æ†¶é«”å•é¡Œè¨ºæ–·\n",
    "\n",
    "#### å•é¡Œ 1: OOM (Out of Memory)\n",
    "\n",
    "**ç—‡ç‹€**: `RuntimeError: CUDA out of memory`\n",
    "\n",
    "**è¨ºæ–·æ­¥é©Ÿ**:\n",
    "1. æ‰“å°å³°å€¼è¨˜æ†¶é«”: `torch.cuda.max_memory_allocated()`\n",
    "2. æª¢æŸ¥æ‰¹æ¬¡å¤§å°æ˜¯å¦éå¤§\n",
    "3. æª¢æŸ¥æ˜¯å¦æœ‰è¨˜æ†¶é«”æ´©æ¼\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ**:\n",
    "```python\n",
    "# 1. æ¸›å°æ‰¹æ¬¡\n",
    "batch_size = 1\n",
    "\n",
    "# 2. å•Ÿç”¨æ··åˆç²¾åº¦\n",
    "with autocast(dtype=torch.float16):\n",
    "    outputs = model(**batch)\n",
    "\n",
    "# 3. æ¢¯åº¦æª¢æŸ¥é»\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 4. æ¸…ç©ºç·©å­˜\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "#### å•é¡Œ 2: è¨˜æ†¶é«”æ´©æ¼\n",
    "\n",
    "**ç—‡ç‹€**: è¨˜æ†¶é«”æŒçºŒå¢é•·, ä¸é‡‹æ”¾\n",
    "\n",
    "**è¨ºæ–·**:\n",
    "```python\n",
    "# ç›£æ§æ¯æ­¥è¨˜æ†¶é«”\n",
    "for step in range(100):\n",
    "    # è¨“ç·´\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "```\n",
    "\n",
    "**å¸¸è¦‹åŸå› **:\n",
    "- æœª detach çš„å¼µé‡\n",
    "- ä¿å­˜äº†æ•´å€‹è¨ˆç®—åœ–\n",
    "- å…¨å±€è®Šæ•¸ç´¯ç©\n",
    "\n",
    "**è§£æ±º**:\n",
    "```python\n",
    "# ä½¿ç”¨ .detach() æ–·é–‹è¨ˆç®—åœ–\n",
    "loss_value = loss.detach().item()\n",
    "\n",
    "# é¿å…ä¿å­˜å¼µé‡åˆ°åˆ—è¡¨\n",
    "losses.append(loss.item())  # âœ… æ­£ç¢º\n",
    "losses.append(loss)         # âŒ æœƒæ´©æ¼\n",
    "```\n",
    "\n",
    "#### å•é¡Œ 3: è¨˜æ†¶é«”ç¢ç‰‡åŒ–\n",
    "\n",
    "**ç—‡ç‹€**: ç¸½è¨˜æ†¶é«”å……è¶³, ä½†åˆ†é…å¤±æ•—\n",
    "\n",
    "**è§£æ±º**:\n",
    "```python\n",
    "# å®šæœŸæ¸…ç†\n",
    "if step % 100 == 0:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "```\n",
    "\n",
    "### ç”Ÿç”¢ç’°å¢ƒå»ºè­°\n",
    "\n",
    "#### 1. è¨˜æ†¶é«”ç›£æ§\n",
    "\n",
    "```python\n",
    "class MemoryMonitor:\n",
    "    def __init__(self, log_interval=100):\n",
    "        self.log_interval = log_interval\n",
    "        self.peak_memory = 0\n",
    "    \n",
    "    def log(self, step):\n",
    "        if step % self.log_interval == 0:\n",
    "            current = torch.cuda.memory_allocated() / 1e9\n",
    "            peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "            self.peak_memory = max(self.peak_memory, peak)\n",
    "            \n",
    "            print(f\"Step {step}: {current:.2f}GB (peak: {peak:.2f}GB)\")\n",
    "            \n",
    "            # è¨˜éŒ„åˆ° tensorboard/wandb\n",
    "            # writer.add_scalar('memory/allocated', current, step)\n",
    "\n",
    "monitor = MemoryMonitor()\n",
    "for step in range(num_steps):\n",
    "    # è¨“ç·´\n",
    "    monitor.log(step)\n",
    "```\n",
    "\n",
    "#### 2. è‡ªå‹•å„ªåŒ–é¸æ“‡\n",
    "\n",
    "```python\n",
    "def auto_configure(model, gpu_memory_gb):\n",
    "    \"\"\"æ ¹æ“š GPU è¨˜æ†¶é«”è‡ªå‹•é…ç½®å„ªåŒ–\"\"\"\n",
    "    model_size_gb = sum(p.numel() for p in model.parameters()) * 4 / 1e9\n",
    "    \n",
    "    config = {}\n",
    "    \n",
    "    # ç¸½æ˜¯ä½¿ç”¨æ··åˆç²¾åº¦\n",
    "    config['use_amp'] = True\n",
    "    \n",
    "    # è¨˜æ†¶é«”ç·Šå¼µæ™‚ä½¿ç”¨æª¢æŸ¥é»\n",
    "    if model_size_gb * 4 > gpu_memory_gb * 0.7:\n",
    "        config['use_checkpoint'] = True\n",
    "    \n",
    "    # è‡ªå‹•è¨ˆç®—æ‰¹æ¬¡å¤§å°\n",
    "    available = gpu_memory_gb - model_size_gb * 2  # ç•™çµ¦æ¿€æ´»å€¼\n",
    "    config['batch_size'] = max(1, int(available / 0.5))  # ç²—ç•¥ä¼°è¨ˆ\n",
    "    \n",
    "    return config\n",
    "```\n",
    "\n",
    "### è¨˜æ†¶é«”å„ªåŒ– Checklist\n",
    "\n",
    "è¨“ç·´å‰æª¢æŸ¥:\n",
    "- [ ] å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´\n",
    "- [ ] æ ¹æ“š GPU é¸æ“‡åˆé©çš„æ‰¹æ¬¡å¤§å°\n",
    "- [ ] å¤§æ¨¡å‹è€ƒæ…®æ¢¯åº¦æª¢æŸ¥é»\n",
    "- [ ] è¨­ç½®è¨˜æ†¶é«”ç›£æ§\n",
    "\n",
    "è¨“ç·´ä¸­ç›£æ§:\n",
    "- [ ] å®šæœŸè¨˜éŒ„å³°å€¼è¨˜æ†¶é«”\n",
    "- [ ] æª¢æŸ¥è¨˜æ†¶é«”æ˜¯å¦æŒçºŒå¢é•· (æ´©æ¼)\n",
    "- [ ] è§€å¯Ÿ GPU åˆ©ç”¨ç‡\n",
    "\n",
    "é‡åˆ° OOM:\n",
    "- [ ] æ¸›å°æ‰¹æ¬¡å¤§å°\n",
    "- [ ] å•Ÿç”¨æ¢¯åº¦ç´¯ç©\n",
    "- [ ] å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»\n",
    "- [ ] æ¸…ç©º CUDA ç·©å­˜\n",
    "- [ ] è€ƒæ…® DeepSpeed/FSDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 12. ä¸‹ä¸€æ­¥å­¸ç¿’\n",
    "\n",
    "æ­å–œå®Œæˆ Lab-1.4 æ‰€æœ‰å¯¦é©—! ğŸ‰\n",
    "\n",
    "æ‚¨å·²ç¶“æŒæ¡äº†:\n",
    "- âœ… æ··åˆç²¾åº¦è¨“ç·´ (é€Ÿåº¦ 2-3x, è¨˜æ†¶é«”çœ 50%)\n",
    "- âœ… æ¢¯åº¦ç´¯ç© (çªç ´è¨˜æ†¶é«”é™åˆ¶)\n",
    "- âœ… æ¢¯åº¦æª¢æŸ¥é» (è¨˜æ†¶é«”çœ 30-50%)\n",
    "- âœ… è¨˜æ†¶é«”åˆ†æèˆ‡å„ªåŒ– (å®šä½ç“¶é ¸, åˆ¶å®šç­–ç•¥)\n",
    "\n",
    "### æ¨è–¦ä¸‹ä¸€æ­¥:\n",
    "\n",
    "1. **æ‡‰ç”¨åˆ° PEFT Labs**\n",
    "   - åœ¨ LoRA/QLoRA è¨“ç·´ä¸­æ‡‰ç”¨é€™äº›å„ªåŒ–\n",
    "   - è¨“ç·´æ›´å¤§çš„æ¨¡å‹\n",
    "\n",
    "2. **å­¸ç¿’åˆ†æ•£å¼è¨“ç·´**\n",
    "   - Lab-1.2: PyTorch DDP Basics\n",
    "   - Lab-1.3: DeepSpeed (å¤šGPU)\n",
    "\n",
    "3. **é«˜ç´šå„ªåŒ–æŠ€è¡“**\n",
    "   - FlashAttention (é•·åºåˆ—å„ªåŒ–)\n",
    "   - Efficient Attention (MQA/GQA)\n",
    "\n",
    "4. **ç”Ÿç”¢éƒ¨ç½²**\n",
    "   - æ¨¡å‹é‡åŒ–èˆ‡å£“ç¸®\n",
    "   - æ¨ç†å„ªåŒ–\n",
    "\n",
    "è¨“ç·´å„ªåŒ–æ˜¯ LLM å·¥ç¨‹çš„æ ¸å¿ƒæŠ€èƒ½, ç¹¼çºŒåŠ æ²¹! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
