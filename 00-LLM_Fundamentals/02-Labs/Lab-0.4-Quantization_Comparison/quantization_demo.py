#!/usr/bin/env python3
"""
é‡åŒ–æŠ€è¡“æ¼”ç¤ºç¨‹å¼ç¢¼
å±•ç¤ºä¸åŒé‡åŒ–æ–¹æ³•çš„å¯¦ç¾å’Œæ•ˆæœå°æ¯”
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import time
from typing import Dict, List, Tuple
import json
import pandas as pd
from datetime import datetime

class QuantizationDemo:
    """é‡åŒ–æŠ€è¡“æ¼”ç¤ºé¡"""

    def __init__(self):
        self.quantization_results = {}
        self.precision_bytes = {
            'fp32': 4, 'fp16': 2, 'bf16': 2,
            'int8': 1, 'int4': 0.5, 'nf4': 0.5
        }

    def demonstrate_precision_formats(self):
        """æ¼”ç¤ºä¸åŒç²¾åº¦æ ¼å¼"""

        print("=== æ•¸å€¼ç²¾åº¦æ ¼å¼æ¼”ç¤º ===")

        # å‰µå»ºæ¸¬è©¦æ•¸å€¼
        test_values = [
            0.123456789,
            1000.123456,
            0.000001234,
            -456.789123,
            1e-8,
            1e8
        ]

        precision_demo = {}

        for precision in ['fp32', 'fp16', 'bf16']:
            precision_demo[precision] = []

            for value in test_values:
                if precision == 'fp32':
                    quantized = np.float32(value)
                elif precision == 'fp16':
                    # æ¨¡æ“¬fp16ç²¾åº¦é™åˆ¶
                    quantized = np.float16(value)
                elif precision == 'bf16':
                    # æ¨¡æ“¬bf16ï¼ˆèˆ‡fp32ç¯„åœç›¸åŒä½†ç²¾åº¦è¼ƒä½ï¼‰
                    quantized = np.float32(np.float16(value))  # ç°¡åŒ–æ¨¡æ“¬

                error = abs(float(quantized) - value) / (abs(value) + 1e-10)

                precision_demo[precision].append({
                    'original': value,
                    'quantized': float(quantized),
                    'error': error
                })

        # é¡¯ç¤ºçµæœ
        for precision, results in precision_demo.items():
            avg_error = np.mean([r['error'] for r in results])
            print(f"{precision.upper()}: å¹³å‡ç›¸å°èª¤å·® = {avg_error:.6f}")

        return precision_demo

    def demonstrate_linear_quantization(self):
        """æ¼”ç¤ºç·šæ€§é‡åŒ–éç¨‹"""

        print("\n=== ç·šæ€§é‡åŒ–æ¼”ç¤º ===")

        # å‰µå»ºæ¨¡æ“¬æ¬Šé‡åˆ†ä½ˆ
        np.random.seed(42)
        weights = np.random.normal(0, 0.5, 1000)  # æ­£æ…‹åˆ†ä½ˆæ¬Šé‡

        quantization_demo = {}

        # INT8 å°ç¨±é‡åŒ–
        print("1. INT8 å°ç¨±é‡åŒ–:")
        int8_result = self._apply_symmetric_quantization(weights, 8)
        quantization_demo['int8_symmetric'] = int8_result

        # INT8 éå°ç¨±é‡åŒ–
        print("2. INT8 éå°ç¨±é‡åŒ–:")
        int8_asymm_result = self._apply_asymmetric_quantization(weights, 8)
        quantization_demo['int8_asymmetric'] = int8_asymm_result

        # INT4 é‡åŒ–
        print("3. INT4 é‡åŒ–:")
        int4_result = self._apply_symmetric_quantization(weights, 4)
        quantization_demo['int4_symmetric'] = int4_result

        return quantization_demo

    def _apply_symmetric_quantization(self, weights: np.ndarray, bits: int) -> Dict:
        """æ‡‰ç”¨å°ç¨±é‡åŒ–"""

        # å°ç¨±é‡åŒ–åƒæ•¸
        max_val = np.max(np.abs(weights))
        qmax = 2**(bits-1) - 1
        scale = max_val / qmax

        # é‡åŒ–
        quantized_weights = np.round(weights / scale)
        quantized_weights = np.clip(quantized_weights, -qmax, qmax)

        # åé‡åŒ–
        dequantized_weights = quantized_weights * scale

        # è¨ˆç®—é‡åŒ–èª¤å·®
        mse = np.mean((weights - dequantized_weights) ** 2)
        snr = 10 * np.log10(np.var(weights) / mse) if mse > 0 else float('inf')

        print(f"   Scale: {scale:.6f}")
        print(f"   MSE: {mse:.6f}")
        print(f"   SNR: {snr:.2f} dB")

        return {
            'type': 'symmetric',
            'bits': bits,
            'scale': scale,
            'zero_point': 0,
            'mse': mse,
            'snr_db': snr,
            'quantized_weights': quantized_weights,
            'dequantized_weights': dequantized_weights
        }

    def _apply_asymmetric_quantization(self, weights: np.ndarray, bits: int) -> Dict:
        """æ‡‰ç”¨éå°ç¨±é‡åŒ–"""

        # éå°ç¨±é‡åŒ–åƒæ•¸
        min_val = np.min(weights)
        max_val = np.max(weights)
        qmin = 0
        qmax = 2**bits - 1

        scale = (max_val - min_val) / (qmax - qmin)
        zero_point = qmin - min_val / scale

        # é‡åŒ–
        quantized_weights = np.round(weights / scale + zero_point)
        quantized_weights = np.clip(quantized_weights, qmin, qmax)

        # åé‡åŒ–
        dequantized_weights = (quantized_weights - zero_point) * scale

        # è¨ˆç®—é‡åŒ–èª¤å·®
        mse = np.mean((weights - dequantized_weights) ** 2)
        snr = 10 * np.log10(np.var(weights) / mse) if mse > 0 else float('inf')

        print(f"   Scale: {scale:.6f}")
        print(f"   Zero point: {zero_point:.2f}")
        print(f"   MSE: {mse:.6f}")
        print(f"   SNR: {snr:.2f} dB")

        return {
            'type': 'asymmetric',
            'bits': bits,
            'scale': scale,
            'zero_point': zero_point,
            'mse': mse,
            'snr_db': snr,
            'quantized_weights': quantized_weights,
            'dequantized_weights': dequantized_weights
        }

    def demonstrate_ptq_quantization(self, model_name: str = "microsoft/DialoGPT-small"):
        """æ¼”ç¤ºPTQé‡åŒ–"""

        print(f"\n=== PTQé‡åŒ–æ¼”ç¤º: {model_name} ===")

        try:
            # è¼‰å…¥åŸå§‹æ¨¡å‹
            print("1. è¼‰å…¥åŸå§‹FP16æ¨¡å‹...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            original_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )

            # æ¸¬è©¦åŸå§‹æ¨¡å‹
            original_performance = self._benchmark_model(original_model, tokenizer, "åŸå§‹FP16")

            # INT8 PTQé‡åŒ–
            print("2. åŸ·è¡ŒINT8 PTQé‡åŒ–...")
            int8_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=BitsAndBytesConfig(load_in_8bit=True),
                device_map="auto"
            )

            int8_performance = self._benchmark_model(int8_model, tokenizer, "INT8é‡åŒ–")

            # INT4 PTQé‡åŒ–
            print("3. åŸ·è¡ŒINT4 NF4é‡åŒ–...")
            int4_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16
                ),
                device_map="auto"
            )

            int4_performance = self._benchmark_model(int4_model, tokenizer, "INT4é‡åŒ–")

            # æ•´ç†å°æ¯”çµæœ
            ptq_comparison = {
                'original_fp16': original_performance,
                'int8_ptq': int8_performance,
                'int4_nf4': int4_performance
            }

            self.quantization_results['ptq_demo'] = ptq_comparison

            return ptq_comparison

        except Exception as e:
            print(f"PTQé‡åŒ–æ¼”ç¤ºå¤±æ•—: {e}")
            return None

    def _benchmark_model(self, model, tokenizer, model_type: str) -> Dict:
        """åŸºæº–æ¸¬è©¦æ¨¡å‹æ€§èƒ½"""

        print(f"  æ¸¬è©¦ {model_type} æ¨¡å‹æ€§èƒ½...")

        # æ¸¬è©¦prompt
        test_prompts = [
            "äººå·¥æ™ºèƒ½çš„ç™¼å±•",
            "æ©Ÿå™¨å­¸ç¿’æ‡‰ç”¨",
            "æ·±åº¦å­¸ç¿’æŠ€è¡“"
        ]

        # è¨ˆç®—æ¨¡å‹å¤§å°
        model_size_mb = sum(
            param.numel() * param.element_size() for param in model.parameters()
        ) / (1024**2)

        # æ€§èƒ½æ¸¬è©¦
        inference_times = []
        generated_responses = []

        for prompt in test_prompts:
            try:
                inputs = tokenizer(prompt, return_tensors="pt")
                if torch.cuda.is_available():
                    inputs = {k: v.cuda() for k, v in inputs.items()}

                start_time = time.time()

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 20,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )

                inference_time = time.time() - start_time
                inference_times.append(inference_time)

                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                response = generated_text[len(prompt):].strip()
                generated_responses.append(response)

            except Exception as e:
                print(f"    ç”Ÿæˆå‡ºéŒ¯: {e}")
                inference_times.append(float('inf'))
                generated_responses.append("ERROR")

        # è¨ˆç®—å¹³å‡æ€§èƒ½
        avg_inference_time = np.mean([t for t in inference_times if t != float('inf')])
        tokens_per_second = 20 / avg_inference_time if avg_inference_time > 0 else 0

        return {
            'model_type': model_type,
            'model_size_mb': model_size_mb,
            'avg_inference_time': avg_inference_time,
            'tokens_per_second': tokens_per_second,
            'generated_responses': generated_responses,
            'functional_test_passed': all('ERROR' not in resp for resp in generated_responses)
        }

    def analyze_quantization_effects(self):
        """åˆ†æé‡åŒ–æ•ˆæœ"""

        if 'ptq_demo' not in self.quantization_results:
            print("è«‹å…ˆé‹è¡ŒPTQé‡åŒ–æ¼”ç¤º")
            return None

        print("\n=== é‡åŒ–æ•ˆæœåˆ†æ ===")

        ptq_results = self.quantization_results['ptq_demo']

        # å‰µå»ºå°æ¯”è¡¨æ ¼
        comparison_data = []

        for model_type, performance in ptq_results.items():
            if performance:
                row = {
                    'æ¨¡å‹é¡å‹': performance['model_type'],
                    'æ¨¡å‹å¤§å°(MB)': f"{performance['model_size_mb']:.1f}",
                    'æ¨ç†æ™‚é–“(s)': f"{performance['avg_inference_time']:.3f}",
                    'Tokens/s': f"{performance['tokens_per_second']:.1f}",
                    'åŠŸèƒ½æ­£å¸¸': "âœ…" if performance['functional_test_passed'] else "âŒ"
                }

                # è¨ˆç®—ç›¸å°æ–¼åŸå§‹æ¨¡å‹çš„è®ŠåŒ–
                if model_type != 'original_fp16':
                    original = ptq_results['original_fp16']
                    if original:
                        size_ratio = performance['model_size_mb'] / original['model_size_mb']
                        speed_ratio = original['avg_inference_time'] / performance['avg_inference_time']

                        row['å¤§å°æ¯”ä¾‹'] = f"{size_ratio:.2f}x"
                        row['é€Ÿåº¦æå‡'] = f"{speed_ratio:.2f}x"

                comparison_data.append(row)

        comparison_df = pd.DataFrame(comparison_data)

        print("é‡åŒ–æ•ˆæœå°æ¯”:")
        print(comparison_df.to_string(index=False))

        # åˆ†æé—œéµæ´å¯Ÿ
        insights = self._generate_quantization_insights(ptq_results)

        return {
            'comparison_table': comparison_df,
            'insights': insights,
            'recommended_strategy': self._recommend_quantization_strategy(ptq_results)
        }

    def _generate_quantization_insights(self, results: Dict) -> List[str]:
        """ç”Ÿæˆé‡åŒ–æ´å¯Ÿ"""

        insights = []

        if 'original_fp16' in results and 'int8_ptq' in results:
            original = results['original_fp16']
            int8 = results['int8_ptq']

            if original and int8:
                compression_ratio = original['model_size_mb'] / int8['model_size_mb']
                speedup = original['avg_inference_time'] / int8['avg_inference_time']

                insights.append(f"INT8é‡åŒ–å¯¦ç¾ {compression_ratio:.1f}x æ¨¡å‹å£“ç¸®")
                insights.append(f"æ¨ç†é€Ÿåº¦æå‡ {speedup:.1f}x")

        if 'int4_nf4' in results:
            int4 = results['int4_nf4']
            if int4 and int4['functional_test_passed']:
                insights.append("INT4 NF4é‡åŒ–åœ¨ä¿æŒåŠŸèƒ½æ€§çš„åŒæ™‚å¯¦ç¾æ¥µè‡´å£“ç¸®")
            elif int4:
                insights.append("INT4é‡åŒ–å¯èƒ½å°è‡´åŠŸèƒ½æ€§å•é¡Œï¼Œéœ€è¬¹æ…ä½¿ç”¨")

        insights.append("é‡åŒ–æŠ€è¡“éœ€è¦åœ¨å£“ç¸®æ¯”ã€æ€§èƒ½å’Œæº–ç¢ºæ€§é–“æ‰¾å¹³è¡¡")

        return insights

    def _recommend_quantization_strategy(self, results: Dict) -> str:
        """æ¨è–¦é‡åŒ–ç­–ç•¥"""

        # åŸºæ–¼çµæœæ¨è–¦æœ€ä½³ç­–ç•¥
        strategies = []

        for model_type, performance in results.items():
            if performance and performance['functional_test_passed']:
                # è¨ˆç®—ç¶œåˆè©•åˆ†ï¼šå£“ç¸®æ•ˆæœ + æ€§èƒ½æå‡
                if model_type != 'original_fp16':
                    original = results['original_fp16']
                    if original:
                        compression_score = original['model_size_mb'] / performance['model_size_mb']
                        speed_score = original['avg_inference_time'] / performance['avg_inference_time']
                        overall_score = compression_score * 0.6 + speed_score * 0.4

                        strategies.append({
                            'strategy': model_type,
                            'score': overall_score
                        })

        if strategies:
            best_strategy = max(strategies, key=lambda x: x['score'])
            return f"æ¨è–¦ç­–ç•¥: {best_strategy['strategy']} (ç¶œåˆè©•åˆ†: {best_strategy['score']:.2f})"
        else:
            return "ç„¡æ³•ç¢ºå®šæœ€ä½³ç­–ç•¥ï¼Œå»ºè­°é€²ä¸€æ­¥æ¸¬è©¦"

    def demonstrate_quantization_aware_training(self):
        """æ¼”ç¤ºé‡åŒ–æ„ŸçŸ¥è¨“ç·´æ¦‚å¿µ"""

        print("\n=== é‡åŒ–æ„ŸçŸ¥è¨“ç·´(QAT)æ¦‚å¿µæ¼”ç¤º ===")

        # æ¨¡æ“¬QATè¨“ç·´éç¨‹
        qat_simulation = {
            'training_phases': [
                {
                    'phase': 'normal_training',
                    'description': 'æ­£å¸¸FP32è¨“ç·´',
                    'precision': 'fp32',
                    'accuracy': 0.95
                },
                {
                    'phase': 'qat_insertion',
                    'description': 'æ’å…¥å½é‡åŒ–å±¤',
                    'precision': 'fp32_with_fake_quantization',
                    'accuracy': 0.93
                },
                {
                    'phase': 'qat_fine_tuning',
                    'description': 'é‡åŒ–æ„ŸçŸ¥å¾®èª¿',
                    'precision': 'fp32_with_fake_quantization',
                    'accuracy': 0.94
                },
                {
                    'phase': 'real_quantization',
                    'description': 'è½‰æ›ç‚ºçœŸå¯¦é‡åŒ–',
                    'precision': 'int8',
                    'accuracy': 0.94
                }
            ],
            'key_concepts': {
                'fake_quantization': 'è¨“ç·´æ™‚æ¨¡æ“¬é‡åŒ–éç¨‹ä½†ä¿æŒFP32ç²¾åº¦è¨ˆç®—',
                'straight_through_estimator': 'é‡åŒ–å‡½æ•¸çš„æ¢¯åº¦è¿‘ä¼¼æ–¹æ³•',
                'learnable_quantization_params': 'é‡åŒ–åƒæ•¸ä½œç‚ºå¯å­¸ç¿’è®Šé‡'
            }
        }

        # é¡¯ç¤ºQATéç¨‹
        for phase in qat_simulation['training_phases']:
            print(f"éšæ®µ: {phase['phase']}")
            print(f"  æè¿°: {phase['description']}")
            print(f"  ç²¾åº¦: {phase['precision']}")
            print(f"  æº–ç¢ºç‡: {phase['accuracy']:.3f}")

        print("\nQATé—œéµæ¦‚å¿µ:")
        for concept, description in qat_simulation['key_concepts'].items():
            print(f"  {concept}: {description}")

        return qat_simulation

    def compare_quantization_methods(self):
        """å°æ¯”ä¸åŒé‡åŒ–æ–¹æ³•"""

        print("\n=== é‡åŒ–æ–¹æ³•å°æ¯” ===")

        # é‡åŒ–æ–¹æ³•ç‰¹é»å°æ¯”
        quantization_methods = {
            'PTQ': {
                'å¯¦æ–½è¤‡é›œåº¦': 'ä½',
                'æº–ç¢ºæ€§ä¿æŒ': 'ä¸­ç­‰',
                'å£“ç¸®æ•ˆæœ': 'è‰¯å¥½',
                'é©ç”¨å ´æ™¯': 'å¿«é€Ÿéƒ¨ç½²',
                'æ ¡æº–æ•¸æ“šéœ€æ±‚': 'å°‘é‡(100-1000æ¨£æœ¬)',
                'è¨“ç·´æ™‚é–“': 'ç„¡éœ€è¨“ç·´'
            },
            'QAT': {
                'å¯¦æ–½è¤‡é›œåº¦': 'é«˜',
                'æº–ç¢ºæ€§ä¿æŒ': 'å„ªç§€',
                'å£“ç¸®æ•ˆæœ': 'è‰¯å¥½',
                'é©ç”¨å ´æ™¯': 'é«˜è³ªé‡è¦æ±‚',
                'æ ¡æº–æ•¸æ“šéœ€æ±‚': 'å®Œæ•´è¨“ç·´é›†',
                'è¨“ç·´æ™‚é–“': 'é¡å¤–è¨“ç·´æ™‚é–“'
            },
            'GPTQ': {
                'å¯¦æ–½è¤‡é›œåº¦': 'ä¸­ç­‰',
                'æº–ç¢ºæ€§ä¿æŒ': 'è‰¯å¥½',
                'å£“ç¸®æ•ˆæœ': 'å„ªç§€',
                'é©ç”¨å ´æ™¯': 'å¤§æ¨¡å‹å£“ç¸®',
                'æ ¡æº–æ•¸æ“šéœ€æ±‚': 'å°‘é‡é«˜è³ªé‡æ¨£æœ¬',
                'è¨“ç·´æ™‚é–“': 'ä¸€æ¬¡æ€§æ ¡æº–'
            },
            'AWQ': {
                'å¯¦æ–½è¤‡é›œåº¦': 'ä¸­ç­‰',
                'æº–ç¢ºæ€§ä¿æŒ': 'å„ªç§€',
                'å£“ç¸®æ•ˆæœ': 'è‰¯å¥½',
                'é©ç”¨å ´æ™¯': 'å¹³è¡¡æ€§è¦æ±‚',
                'æ ¡æº–æ•¸æ“šéœ€æ±‚': 'å°‘é‡æ¨£æœ¬',
                'è¨“ç·´æ™‚é–“': 'å¿«é€Ÿæ ¡æº–'
            }
        }

        # è½‰æ›ç‚ºDataFrame
        comparison_df = pd.DataFrame(quantization_methods).T

        print("é‡åŒ–æ–¹æ³•å°æ¯”:")
        print(comparison_df.to_string())

        # ç”Ÿæˆé¸æ“‡å»ºè­°
        selection_guide = {
            'å¿«é€Ÿéƒ¨ç½²': 'PTQ - å¯¦æ–½ç°¡å–®ï¼Œæ•ˆæœå¯æ¥å—',
            'é«˜è³ªé‡è¦æ±‚': 'QAT - æº–ç¢ºæ€§æœ€ä½³ï¼Œéœ€è¦é¡å¤–è¨“ç·´',
            'å¤§æ¨¡å‹å£“ç¸®': 'GPTQ - å°ˆé–€é‡å°å¤§æ¨¡å‹å„ªåŒ–',
            'å¹³è¡¡æ€§è¦æ±‚': 'AWQ - åœ¨å„å€‹æ–¹é¢éƒ½æœ‰è‰¯å¥½è¡¨ç¾'
        }

        print("\né‡åŒ–æ–¹æ³•é¸æ“‡æŒ‡å—:")
        for scenario, recommendation in selection_guide.items():
            print(f"  {scenario}: {recommendation}")

        return {
            'comparison_table': comparison_df,
            'selection_guide': selection_guide
        }

    def visualize_quantization_effects(self):
        """å¯è¦–åŒ–é‡åŒ–æ•ˆæœ"""

        print("\n=== ç”Ÿæˆé‡åŒ–æ•ˆæœå¯è¦–åŒ– ===")

        # å‰µå»ºé‡åŒ–ç²¾åº¦å°æ¯”åœ–
        precisions = ['FP32', 'FP16', 'INT8', 'INT4']
        model_sizes = [100, 50, 25, 12.5]  # ç›¸å°å¤§å°
        inference_speeds = [1.0, 1.8, 2.5, 3.2]  # ç›¸å°é€Ÿåº¦
        accuracy_scores = [1.0, 0.998, 0.985, 0.97]  # ç›¸å°æº–ç¢ºç‡

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        # 1. æ¨¡å‹å¤§å°å°æ¯”
        bars1 = ax1.bar(precisions, model_sizes, color='skyblue', alpha=0.7)
        ax1.set_title('æ¨¡å‹å¤§å°å°æ¯”')
        ax1.set_ylabel('ç›¸å°å¤§å°')
        for bar, size in zip(bars1, model_sizes):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{size}%', ha='center', va='bottom')

        # 2. æ¨ç†é€Ÿåº¦å°æ¯”
        bars2 = ax2.bar(precisions, inference_speeds, color='lightgreen', alpha=0.7)
        ax2.set_title('æ¨ç†é€Ÿåº¦å°æ¯”')
        ax2.set_ylabel('ç›¸å°é€Ÿåº¦')
        for bar, speed in zip(bars2, inference_speeds):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                    f'{speed:.1f}x', ha='center', va='bottom')

        # 3. æº–ç¢ºç‡å°æ¯”
        bars3 = ax3.bar(precisions, accuracy_scores, color='salmon', alpha=0.7)
        ax3.set_title('æº–ç¢ºç‡å°æ¯”')
        ax3.set_ylabel('ç›¸å°æº–ç¢ºç‡')
        ax3.set_ylim(0.95, 1.005)
        for bar, acc in zip(bars3, accuracy_scores):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{acc:.3f}', ha='center', va='bottom')

        # 4. ç¶œåˆæ€§èƒ½é›·é”åœ–
        categories = ['æ¨¡å‹å¤§å°\n(è¶Šå°è¶Šå¥½)', 'æ¨ç†é€Ÿåº¦\n(è¶Šå¿«è¶Šå¥½)', 'æº–ç¢ºç‡\n(è¶Šé«˜è¶Šå¥½)', 'å¯¦æ–½é›£åº¦\n(è¶Šç°¡å–®è¶Šå¥½)']

        # æ­¸ä¸€åŒ–æ•¸æ“š
        fp32_scores = [0.25, 1.0, 1.0, 1.0]  # FP32ä½œç‚ºåŸºç·š
        int8_scores = [1.0, 0.8, 0.985, 0.9]  # INT8
        int4_scores = [1.0, 0.9, 0.97, 0.7]   # INT4

        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]  # é–‰åˆé›·é”åœ–

        for scores, label, color in [
            (fp32_scores + fp32_scores[:1], 'FP32', 'blue'),
            (int8_scores + int8_scores[:1], 'INT8', 'green'),
            (int4_scores + int4_scores[:1], 'INT4', 'red')
        ]:
            ax4.plot(angles, scores, 'o-', linewidth=2, label=label, color=color)
            ax4.fill(angles, scores, alpha=0.1, color=color)

        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(categories)
        ax4.set_ylim(0, 1.1)
        ax4.set_title('ç¶œåˆæ€§èƒ½å°æ¯”')
        ax4.legend()
        ax4.grid(True)

        plt.tight_layout()
        plt.savefig('quantization_effects_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜: quantization_effects_comparison.png")

    def generate_quantization_report(self) -> str:
        """ç”Ÿæˆé‡åŒ–æŠ€è¡“å ±å‘Š"""

        report = f"""# é‡åŒ–æŠ€è¡“æ¼”ç¤ºåˆ†æå ±å‘Š

ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## é‡åŒ–æŠ€è¡“æ¦‚è¿°

æœ¬å ±å‘Šå±•ç¤ºäº†ä¸åŒé‡åŒ–æŠ€è¡“çš„å¯¦ç¾åŸç†å’Œæ•ˆæœå°æ¯”ï¼Œç‚ºå¯¦éš›æ‡‰ç”¨ä¸­çš„é‡åŒ–ç­–ç•¥é¸æ“‡æä¾›åƒè€ƒã€‚

## æ ¸å¿ƒç™¼ç¾

### 1. ç²¾åº¦æ ¼å¼ç‰¹æ€§
- **FP32**: æœ€é«˜ç²¾åº¦ï¼Œæœ€å¤§å­˜å„²é–‹éŠ·
- **FP16**: ç²¾åº¦ç•¥å¾®ä¸‹é™ï¼Œå­˜å„²æ¸›åŠ
- **INT8**: è¼ƒå¤§ç²¾åº¦æå¤±ï¼Œå­˜å„²é€²ä¸€æ­¥æ¸›å°‘
- **INT4**: æ¥µè‡´å£“ç¸®ï¼Œéœ€è¦ç‰¹æ®ŠæŠ€è¡“ä¿è­‰å¯ç”¨æ€§

### 2. é‡åŒ–æ–¹æ³•æ¯”è¼ƒ
"""

        # æ·»åŠ å¯¦éš›æ¸¬è©¦çµæœ
        if 'ptq_demo' in self.quantization_results:
            results = self.quantization_results['ptq_demo']

            report += "### PTQé‡åŒ–å¯¦æ¸¬æ•ˆæœ\n"

            for model_type, performance in results.items():
                if performance:
                    report += f"""
**{performance['model_type']}**:
- æ¨¡å‹å¤§å°: {performance['model_size_mb']:.1f} MB
- æ¨ç†æ™‚é–“: {performance['avg_inference_time']:.3f} ç§’
- æ¨ç†é€Ÿåº¦: {performance['tokens_per_second']:.1f} tokens/s
- åŠŸèƒ½å®Œæ•´æ€§: {'æ­£å¸¸' if performance['functional_test_passed'] else 'æœ‰å•é¡Œ'}
"""

        report += """
## å¯¦éš›æ‡‰ç”¨å»ºè­°

### é‡åŒ–ç­–ç•¥é¸æ“‡
1. **åŸå‹é–‹ç™¼éšæ®µ**: ä½¿ç”¨FP16ï¼Œå¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡
2. **ç”Ÿç”¢éƒ¨ç½²éšæ®µ**: æ ¹æ“šç¡¬é«”æ¢ä»¶é¸æ“‡INT8æˆ–INT4
3. **é‚Šç·£è¨­å‚™éƒ¨ç½²**: å„ªå…ˆè€ƒæ…®INT4æˆ–æ›´æ¿€é€²çš„é‡åŒ–
4. **é«˜ç²¾åº¦è¦æ±‚å ´æ™¯**: ä½¿ç”¨QATä¿è­‰é‡åŒ–å¾Œæ€§èƒ½

### å¯¦æ–½å»ºè­°
1. **å……åˆ†æ¸¬è©¦**: é‡åŒ–å¾Œå¿…é ˆé€²è¡Œå…¨é¢åŠŸèƒ½å’Œæ€§èƒ½æ¸¬è©¦
2. **æ¼¸é€²å¯¦æ–½**: å¾ä¿å®ˆçš„é‡åŒ–ç­–ç•¥é–‹å§‹ï¼Œé€æ­¥å„ªåŒ–
3. **ç›£æ§éƒ¨ç½²**: éƒ¨ç½²å¾ŒæŒçºŒç›£æ§é‡åŒ–æ¨¡å‹çš„è¡¨ç¾
4. **å‚™é¸æ–¹æ¡ˆ**: æº–å‚™å›æ»¾åˆ°åŸå§‹æ¨¡å‹çš„æ–¹æ¡ˆ

### æŠ€è¡“é¸å‹
- **å¿«é€Ÿéƒ¨ç½²**: PTQ + BitsAndBytesConfig
- **é«˜è³ªé‡è¦æ±‚**: QAT + å®Œæ•´è¨“ç·´æµç¨‹
- **å¤§æ¨¡å‹è™•ç†**: GPTQ + å°ˆæ¥­æ ¡æº–æ•¸æ“š
- **å¹³è¡¡æ–¹æ¡ˆ**: AWQ + è‡ªå‹•ç²¾åº¦æœç´¢

## æœªä¾†ç™¼å±•è¶¨å‹¢

1. **æ›´ä½ç²¾åº¦**: FP8ã€FP6ç­‰æ›´ä½ç²¾åº¦æ ¼å¼çš„ç™¼å±•
2. **ç¡¬é«”å”åŒ**: èˆ‡å°ˆç”¨ç¡¬é«”æ·±åº¦å”åŒçš„é‡åŒ–æŠ€è¡“
3. **è‡ªå‹•åŒ–**: è‡ªå‹•é‡åŒ–åƒæ•¸æœç´¢å’Œå„ªåŒ–
4. **å¤šæ¨¡æ…‹**: å¤šæ¨¡æ…‹æ¨¡å‹çš„é‡åŒ–æŠ€è¡“

---
*æœ¬å ±å‘ŠåŸºæ–¼æ¼”ç¤ºæ•¸æ“šç”Ÿæˆï¼Œå¯¦éš›æ‡‰ç”¨æ™‚è«‹æ ¹æ“šå…·é«”æ¨¡å‹å’Œéœ€æ±‚é€²è¡Œè©³ç´°æ¸¬è©¦ã€‚*
"""

        return report

    def run_complete_quantization_demo(self):
        """é‹è¡Œå®Œæ•´é‡åŒ–æ¼”ç¤º"""

        print("ğŸ”¢ é‡åŒ–æŠ€è¡“å®Œæ•´æ¼”ç¤º")
        print("=" * 60)

        try:
            # 1. ç²¾åº¦æ ¼å¼æ¼”ç¤º
            precision_demo = self.demonstrate_precision_formats()

            # 2. ç·šæ€§é‡åŒ–æ¼”ç¤º
            linear_quant_demo = self.demonstrate_linear_quantization()

            # 3. PTQé‡åŒ–æ¼”ç¤º
            ptq_demo = self.demonstrate_ptq_quantization()

            # 4. QATæ¦‚å¿µæ¼”ç¤º
            qat_demo = self.demonstrate_quantization_aware_training()

            # 5. é‡åŒ–æ–¹æ³•å°æ¯”
            methods_comparison = self.compare_quantization_methods()

            # 6. æ•ˆæœåˆ†æ
            if ptq_demo:
                effects_analysis = self.analyze_quantization_effects()

                # 7. å¯è¦–åŒ–
                self.visualize_quantization_effects()

            # 8. ç”Ÿæˆå ±å‘Š
            report = self.generate_quantization_report()

            # ä¿å­˜å ±å‘Š
            with open('quantization_demo_report.md', 'w', encoding='utf-8') as f:
                f.write(report)

            print("\nâœ… é‡åŒ–æ¼”ç¤ºå®Œæˆï¼")
            print("ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
            print("   - quantization_demo_report.md")
            print("   - quantization_effects_comparison.png")

            print("\nğŸ“ é—œéµå­¸ç¿’è¦é»:")
            print("1. é‡åŒ–æ˜¯ç²¾åº¦å’Œæ•ˆç‡ä¹‹é–“çš„æ¬Šè¡¡")
            print("2. ä¸åŒé‡åŒ–æ–¹æ³•é©ç”¨æ–¼ä¸åŒå ´æ™¯")
            print("3. PTQé©åˆå¿«é€Ÿéƒ¨ç½²ï¼ŒQATé©åˆé«˜è³ªé‡è¦æ±‚")
            print("4. é‡åŒ–å¾Œå¿…é ˆé€²è¡Œå……åˆ†çš„æ¸¬è©¦é©—è­‰")

            return {
                'precision_demo': precision_demo,
                'linear_quant_demo': linear_quant_demo,
                'ptq_demo': ptq_demo,
                'qat_demo': qat_demo,
                'methods_comparison': methods_comparison
            }

        except Exception as e:
            print(f"âŒ é‡åŒ–æ¼”ç¤ºå¤±æ•—: {e}")
            return None

def main():
    """ä¸»å‡½æ•¸"""

    print("é‡åŒ–æŠ€è¡“æ¼”ç¤ºç¨‹å¼")
    print("æœ¬ç¨‹å¼å°‡å±•ç¤ºé‡åŒ–æŠ€è¡“çš„åŸç†å’Œå¯¦éš›æ‡‰ç”¨\n")

    # å‰µå»ºæ¼”ç¤ºå¯¦ä¾‹
    demo = QuantizationDemo()

    # é‹è¡Œå®Œæ•´æ¼”ç¤º
    results = demo.run_complete_quantization_demo()

    if results:
        print("\nğŸ”¬ æ¼”ç¤ºç¸½çµ:")
        print("- ç†è§£äº†ä¸åŒç²¾åº¦æ ¼å¼çš„ç‰¹é»")
        print("- æŒæ¡äº†é‡åŒ–çš„æ•¸å­¸åŸç†")
        print("- é«”é©—äº†PTQå’ŒQATçš„å€åˆ¥")
        print("- å­¸æœƒäº†é‡åŒ–æ–¹æ³•çš„é¸æ“‡ç­–ç•¥")

        print("\nğŸ’¡ å¯¦éš›æ‡‰ç”¨æç¤º:")
        print("- åœ¨ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨å‰å‹™å¿…å……åˆ†æ¸¬è©¦")
        print("- æ ¹æ“šå…·é«”ç¡¬é«”é¸æ“‡åˆé©çš„é‡åŒ–æ–¹æ¡ˆ")
        print("- å»ºç«‹é‡åŒ–æ¨¡å‹çš„ç›£æ§å’Œå›æ»¾æ©Ÿåˆ¶")

if __name__ == "__main__":
    main()