{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4: Prefix Tuning - Fine-Tuning a GPT-2 Model for Generation\n",
        "---\n",
        "## Notebook 2: The Training Process\n",
        "\n",
        "**Goal:** In this notebook, you will fine-tune a `gpt2` model on a text generation task using **Prefix Tuning**. We'll train it to generate positive reviews of movies.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Load a dataset for text generation (`imdb`) and preprocess it.\n",
        "-   Load a pre-trained GPT-2 model.\n",
        "-   Deeply understand and configure `peft.PrefixTuningConfig`.\n",
        "-   Apply prefixes to the GPT-2 model.\n",
        "-   Fine-tune the model by training *only* the prefix vectors using the `transformers.Trainer`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Dataset and Preprocess\n",
        "\n",
        "We will use the `imdb` dataset, which contains movie reviews. We'll filter it to only use the positive reviews (`label=1`) to teach the model how to generate text in a specific, positive style.\n",
        "\n",
        "#### Key Hugging Face Components:\n",
        "\n",
        "-   `transformers.AutoTokenizer`: We'll load the tokenizer for `gpt2`. Since GPT-2 is an autoregressive model, we need to set the `pad_token` to be the same as the `eos_token`.\n",
        "-   `dataset.filter()`: Used to select only the positive reviews from the dataset.\n",
        "-   `dataset.map()`: We'll tokenize the reviews. For text generation, the `labels` are typically the same as the `input_ids`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Set pad token\n",
        "\n",
        "# --- Load and Filter Dataset ---\n",
        "dataset = load_dataset(\"imdb\", split=\"train[:500]\")\n",
        "# Filter for positive reviews only\n",
        "positive_dataset = dataset.filter(lambda example: example[\"label\"] == 1)\n",
        "positive_dataset = positive_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the text\n",
        "    outputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "    # For language modeling, the labels are the same as the input_ids\n",
        "    outputs[\"labels\"] = outputs[\"input_ids\"]\n",
        "    return outputs\n",
        "\n",
        "# --- Apply Preprocessing ---\n",
        "tokenized_datasets = positive_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"âœ… Dataset loaded and preprocessed.\")\n",
        "print(tokenized_datasets[\"train\"][0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load the Base Model\n",
        "\n",
        "Next, we load the `gpt2` model. Since this is a text generation task, we use `AutoModelForCausalLM`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "print(\"âœ… Base GPT-2 model loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Configure Prefix Tuning\n",
        "\n",
        "Here we configure Prefix Tuning. This method is more powerful than Prompt Tuning because the trainable parameters (the prefix) are injected into the attention mechanism of *every* transformer layer, giving it more influence over the generation process.\n",
        "\n",
        "#### Key Hugging Face `peft` Components:\n",
        "\n",
        "-   `peft.PrefixTuningConfig`: The configuration class for this method.\n",
        "    -   `task_type=\"CAUSAL_LM\"`: We specify the task type for causal language modeling.\n",
        "    -   `num_virtual_tokens`: The length of the prefix. This is the main hyperparameter. It defines the length of the trainable prefix tensor that is fed into each attention layer.\n",
        "-   `peft.get_peft_model`: Applies the configuration to our base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_model, PrefixTuningConfig, TaskType\n",
        "\n",
        "# --- Prefix Tuning Configuration ---\n",
        "prefix_config = PrefixTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    num_virtual_tokens=20 # This is the length of the prefix\n",
        ")\n",
        "\n",
        "# --- Create PeftModel ---\n",
        "peft_model = get_peft_model(model, prefix_config)\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Set Up Training\n",
        "\n",
        "The final step is to configure and run the training process using the `transformers.Trainer`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-prefix-tuning-imdb\",\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# --- Create Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"ðŸš€ Starting training with Prefix Tuning...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
