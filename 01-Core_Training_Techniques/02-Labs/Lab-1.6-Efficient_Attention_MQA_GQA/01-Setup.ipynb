{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.6: æ¨™æº– MHA åŸºæº–æ¸¬è©¦\n",
    "## Multi-Head Attention Baseline\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- å¯¦ç¾æ¨™æº– Multi-Head Attention (MHA)\n",
    "- å¯¦ç¾ KV Cache æ©Ÿåˆ¶\n",
    "- æ¸¬é‡æ¨ç†æ€§èƒ½åŸºæº–\n",
    "- åˆ†æ KV Cache è¨˜æ†¶é«”å ç”¨\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 30-45åˆ†é˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"æ¨™æº– Multi-Head Attention\"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads çµ„ K\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads çµ„ V\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        B, N, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # è™•ç† KV Cache\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            K = torch.cat([past_k, K], dim=1)\n",
    "            V = torch.cat([past_v, V], dim=1)\n",
    "        \n",
    "        # Transpose for attention\n",
    "        Q = Q.transpose(1, 2)  # [B, H, N, D]\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(B, N, self.hidden_dim)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            return output, (K.transpose(1, 2), V.transpose(1, 2))\n",
    "        return output\n",
    "\n",
    "# æ¸¬è©¦\n",
    "mha = MultiHeadAttention(768, 12).to(device)\n",
    "x = torch.randn(2, 128, 768, device=device)\n",
    "out = mha(x)\n",
    "print(f\"âœ… MHA æ¸¬è©¦é€šé: {x.shape} â†’ {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. KV Cache æ€§èƒ½æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_kv_cache_size(num_layers, seq_len, num_kv_heads, head_dim, dtype=torch.float16):\n",
    "    \"\"\"è¨ˆç®— KV Cache å¤§å°\"\"\"\n",
    "    bytes_per_element = 2 if dtype == torch.float16 else 4\n",
    "    \n",
    "    # æ¯å±¤: 2 (K+V) Ã— seq_len Ã— num_kv_heads Ã— head_dim Ã— bytes\n",
    "    per_layer_mb = 2 * seq_len * num_kv_heads * head_dim * bytes_per_element / 1e6\n",
    "    total_mb = per_layer_mb * num_layers\n",
    "    \n",
    "    return {\n",
    "        'per_layer_mb': per_layer_mb,\n",
    "        'total_mb': total_mb,\n",
    "        'total_gb': total_mb / 1024\n",
    "    }\n",
    "\n",
    "# Llama-2-7B é…ç½®\n",
    "config = {\n",
    "    'num_layers': 32,\n",
    "    'num_query_heads': 32,\n",
    "    'head_dim': 128,\n",
    "    'seq_len': 2048\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KV Cache å¤§å°åˆ†æ (Llama-2-7B é…ç½®)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\né…ç½®:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# å°æ¯”ä¸åŒæ¶æ§‹\n",
    "architectures = [\n",
    "    ('MHA (æ¨™æº–)', config['num_query_heads']),\n",
    "    ('GQA-8', 8),\n",
    "    ('GQA-4', 4),\n",
    "    ('MQA', 1)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ¶æ§‹':<20} {'KV Heads':<12} {'æ¯å±¤(MB)':<15} {'ç¸½è¨ˆ(GB)':<15} {'ç›¸å°MHA':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "mha_total = None\n",
    "for name, num_kv_heads in architectures:\n",
    "    cache_size = measure_kv_cache_size(\n",
    "        config['num_layers'],\n",
    "        config['seq_len'],\n",
    "        num_kv_heads,\n",
    "        config['head_dim']\n",
    "    )\n",
    "    \n",
    "    if mha_total is None:\n",
    "        mha_total = cache_size['total_gb']\n",
    "        relative = \"100%\"\n",
    "    else:\n",
    "        relative = f\"{cache_size['total_gb']/mha_total*100:.1f}%\"\n",
    "    \n",
    "    print(f\"{name:<20} {num_kv_heads:<12} {cache_size['per_layer_mb']:<15.2f} {cache_size['total_gb']:<15.2f} {relative:<12}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ è§€å¯Ÿ: MQA å°‡ KV Cache æ¸›å°‘è‡³ MHA çš„ 3%, GQA-8 æ¸›å°‘è‡³ 25%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(attn_module, seq_len, num_tokens=100):\n",
    "    \"\"\"æ¸¬è©¦ç”Ÿæˆæ€§èƒ½\"\"\"\n",
    "    attn_module.eval()\n",
    "    \n",
    "    # æ¨¡æ“¬æ¨ç†\n",
    "    x = torch.randn(1, seq_len, 768, device=device, dtype=torch.float16)\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        # Prefill\n",
    "        start = time.time()\n",
    "        out, past_kv = attn_module(x, use_cache=True)\n",
    "        torch.cuda.synchronize()\n",
    "        prefill_time = time.time() - start\n",
    "        \n",
    "        # Decode\n",
    "        for _ in range(num_tokens):\n",
    "            new_token = torch.randn(1, 1, 768, device=device, dtype=torch.float16)\n",
    "            start = time.time()\n",
    "            out, past_kv = attn_module(new_token, past_kv=past_kv, use_cache=True)\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    return {\n",
    "        'prefill_time': prefill_time,\n",
    "        'avg_decode_time': np.mean(times),\n",
    "        'total_time': prefill_time + sum(times),\n",
    "        'peak_memory_gb': peak_mem\n",
    "    }\n",
    "\n",
    "# æ¸¬è©¦\n",
    "print(\"MHA æ¨ç†åŸºæº–æ¸¬è©¦...\")\n",
    "result = benchmark_generation(mha, seq_len=128, num_tokens=50)\n",
    "print(f\"Prefill: {result['prefill_time']*1000:.2f}ms\")\n",
    "print(f\"Decode: {result['avg_decode_time']*1000:.2f}ms/token\")\n",
    "print(f\"è¨˜æ†¶é«”: {result['peak_memory_gb']:.3f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
