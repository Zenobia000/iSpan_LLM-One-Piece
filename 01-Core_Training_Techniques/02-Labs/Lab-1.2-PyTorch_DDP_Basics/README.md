# Lab-1.2: PyTorch DDP åˆ†æ•£å¼è¨“ç·´åŸºç¤
## PyTorch Distributed Data Parallel (DDP) Basics

---

## âš ï¸ ç’°å¢ƒé™åˆ¶è²æ˜

æœ¬å¯¦é©—å®¤éœ€è¦**å¤šGPUç’°å¢ƒ**æ‰èƒ½å®Œæ•´åŸ·è¡Œï¼Œç›®å‰å°ˆæ¡ˆé–‹ç™¼ç’°å¢ƒç‚ºå–®GPUï¼Œå› æ­¤**å¯¦é©—å®¤å…§å®¹æš«æœªé–‹ç™¼**ã€‚

### ç‚ºä½•éœ€è¦å¤šGPU?
- **DDP (Distributed Data Parallel)** æ˜¯ PyTorch çš„å¤šGPUåˆ†æ•£å¼è¨“ç·´æ¡†æ¶
- éœ€è¦è‡³å°‘ **2å€‹GPU** æ‰èƒ½é©—è­‰åˆ†æ•£å¼è¨“ç·´é‚è¼¯
- å–®GPUç’°å¢ƒç„¡æ³•å±•ç¤ºçœŸå¯¦çš„æ¢¯åº¦åŒæ­¥ã€é€šè¨Šå„ªåŒ–ç­‰æ ¸å¿ƒåŠŸèƒ½

---

## ğŸ“š å­¸ç¿’æ›¿ä»£æ–¹æ¡ˆ

é›–ç„¶ç„¡æ³•åœ¨å–®GPUç’°å¢ƒåŸ·è¡Œå¯¦é©—ï¼Œæ‚¨ä»å¯é€šéä»¥ä¸‹æ–¹å¼å­¸ç¿’ DDP æŠ€è¡“:

### 1. ç†è«–å­¸ç¿’ (å®Œæ•´å¯ç”¨) âœ…
è«‹åƒé–±å®Œæ•´çš„ç†è«–æ–‡ä»¶:
- **`01-Theory/1.2-Distributed_Training.md`** (526è¡Œ, 13.6KB)
  - ç¬¬2ç« : PyTorch DDP å®Œæ•´è¬›è§£
  - æ¶µè“‹: åŸºæœ¬åŸç†ã€é€šè¨Šæ©Ÿåˆ¶ã€æœ€ä½³å¯¦è¸
  - åŒ…å«: ä»£ç¢¼ç¯„ä¾‹ã€é…ç½®èªªæ˜ã€æ€§èƒ½å„ªåŒ–

### 2. ä»£ç¢¼ç¯„ä¾‹ (ç†è«–æ–‡ä»¶ä¸­) âœ…
ç†è«–æ–‡ä»¶åŒ…å«å®Œæ•´çš„ DDP ä»£ç¢¼ç¯„ä¾‹:

```python
# å®Œæ•´çš„ DDP è¨“ç·´ç¯„ä¾‹ (ä¾†è‡ª 1.2-Distributed_Training.md)
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def train_ddp(rank, world_size):
    setup(rank, world_size)

    # å‰µå»ºæ¨¡å‹ä¸¦åŒ…è£ç‚º DDP
    model = MyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    # è¨“ç·´å¾ªç’°
    for data, labels in dataloader:
        data, labels = data.to(rank), labels.to(rank)

        optimizer.zero_grad()
        outputs = ddp_model(data)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    dist.destroy_process_group()
```

### 3. é…ç½®æ¨¡æ¿åƒè€ƒ

#### å¤šGPUå•Ÿå‹•è…³æœ¬
```bash
# å–®ç¯€é»å¤šGPUè¨“ç·´
torchrun --nproc_per_node=4 train.py

# å¤šç¯€é»å¤šGPUè¨“ç·´
torchrun --nnodes=2 --nproc_per_node=4 \
         --node_rank=0 --master_addr="192.168.1.1" \
         --master_port=29500 train.py
```

#### DDP é…ç½®æœ€ä½³å¯¦è¸
```python
# DDP é…ç½®åƒæ•¸
ddp_config = {
    'backend': 'nccl',           # GPU ä½¿ç”¨ nccl
    'init_method': 'env://',     # ç’°å¢ƒè®Šæ•¸åˆå§‹åŒ–
    'world_size': 4,             # ç¸½GPUæ•¸é‡
    'rank': 0,                   # ç•¶å‰é€²ç¨‹ç·¨è™Ÿ
    'find_unused_parameters': False,  # æ€§èƒ½å„ªåŒ–
    'broadcast_buffers': True,   # åŒæ­¥buffer
    'gradient_as_bucket_view': True,  # æ¢¯åº¦æ¡¶è¦–åœ–å„ªåŒ–
}
```

---

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å³ä½¿ç„¡æ³•åŸ·è¡Œå¯¦éš›çš„å¤šGPUè¨“ç·´ï¼Œæ‚¨ä»æ‡‰æŒæ¡ä»¥ä¸‹æ ¸å¿ƒæ¦‚å¿µ:

### åŸºç¤æ¦‚å¿µ
- [x] ç†è§£ DDP çš„åŸºæœ¬åŸç†èˆ‡å·¥ä½œæµç¨‹
- [x] æŒæ¡åˆ†æ•£å¼è¨“ç·´çš„åˆå§‹åŒ–èˆ‡é€²ç¨‹ç®¡ç†
- [x] äº†è§£æ¢¯åº¦åŒæ­¥æ©Ÿåˆ¶ (All-Reduce)

### é…ç½®æŠ€èƒ½
- [x] ç†è§£ DDP çš„é…ç½®åƒæ•¸èˆ‡å«ç¾©
- [x] æŒæ¡å¤šGPUè¨“ç·´çš„å•Ÿå‹•æ–¹å¼
- [x] äº†è§£é€šè¨Šå¾Œç«¯çš„é¸æ“‡ (NCCL vs Gloo)

### å„ªåŒ–ç­–ç•¥
- [x] ç†è§£æ¢¯åº¦ç´¯ç©åœ¨åˆ†æ•£å¼è¨“ç·´ä¸­çš„æ‡‰ç”¨
- [x] æŒæ¡æ··åˆç²¾åº¦è¨“ç·´çš„åˆ†æ•£å¼é…ç½®
- [x] äº†è§£é€šè¨Šå„ªåŒ–çš„æœ€ä½³å¯¦è¸

---

## ğŸ”§ å¦‚ä½•åœ¨å¤šGPUç’°å¢ƒä¸­åŸ·è¡Œ

ç•¶æ‚¨æœ‰å¤šGPUç’°å¢ƒæ™‚ï¼Œå¯ä»¥æŒ‰ä»¥ä¸‹æ­¥é©ŸåŸ·è¡Œ DDP è¨“ç·´:

### å‰ç½®æº–å‚™
```bash
# æª¢æŸ¥GPUæ•¸é‡
nvidia-smi

# é©—è­‰ PyTorch åˆ†æ•£å¼æ”¯æ´
python -c "import torch; print(torch.distributed.is_available())"
python -c "import torch; print(torch.distributed.is_nccl_available())"
```

### åŸ·è¡Œæ­¥é©Ÿ

#### 1. æº–å‚™è¨“ç·´è…³æœ¬ (åƒè€ƒç†è«–æ–‡ä»¶)
å‰µå»º `train_ddp.py`, å¯¦ç¾:
- é€²ç¨‹åˆå§‹åŒ– (`dist.init_process_group`)
- æ¨¡å‹åŒ…è£ (`DDP(model)`)
- æ•¸æ“šé›†åˆ†ç‰‡ (`DistributedSampler`)
- è¨“ç·´å¾ªç’°

#### 2. å•Ÿå‹•åˆ†æ•£å¼è¨“ç·´
```bash
# å–®æ©Ÿ4GPUè¨“ç·´
torchrun --nproc_per_node=4 train_ddp.py

# æˆ–ä½¿ç”¨ torch.distributed.launch (èˆŠç‰ˆ)
python -m torch.distributed.launch --nproc_per_node=4 train_ddp.py
```

#### 3. ç›£æ§è¨“ç·´
```bash
# ç›£æ§GPUä½¿ç”¨
watch -n 0.5 nvidia-smi

# æŸ¥çœ‹è¨“ç·´æ—¥èªŒ
tail -f train.log
```

---

## ğŸ“– å»¶ä¼¸å­¸ç¿’è³‡æº

### å®˜æ–¹æ–‡æª”
- [PyTorch DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [Getting Started with DDP](https://pytorch.org/tutorials/beginner/dist_overview.html)
- [DDP API Reference](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)

### ç†è«–æ·±å…¥
- **ç†è«–æ–‡ä»¶**: `01-Theory/1.2-Distributed_Training.md`
  - ç¬¬2.1ç¯€: PyTorch DDP å®Œæ•´è¬›è§£
  - ç¬¬2.2ç¯€: é€šè¨Šå„ªåŒ–èˆ‡æ¢¯åº¦åŒæ­¥
  - ç¬¬2.3ç¯€: åˆ†æ•£å¼è¨“ç·´æœ€ä½³å¯¦è¸

### å¯¦æˆ°ç¯„ä¾‹
- [PyTorch Examples - ImageNet](https://github.com/pytorch/examples/tree/main/imagenet)
- [HuggingFace Accelerate](https://huggingface.co/docs/accelerate)

---

## ğŸš€ æœªä¾†è¨ˆåŠƒ

ç•¶å°ˆæ¡ˆç²å¾—å¤šGPUç’°å¢ƒå¾Œï¼Œå°‡è£œå……å®Œæ•´çš„å¯¦é©—å…§å®¹:

### è¨ˆåŠƒä¸­çš„å¯¦é©—å…§å®¹
- [ ] **01-Setup.ipynb**: ç’°å¢ƒé©—è­‰èˆ‡é€²ç¨‹åˆå§‹åŒ–
- [ ] **02-Train.ipynb**: DDP è¨“ç·´å®Œæ•´æµç¨‹
- [ ] **03-Optimization.ipynb**: é€šè¨Šå„ªåŒ–èˆ‡æ€§èƒ½èª¿å„ª
- [ ] **04-Advanced.ipynb**: æ¢¯åº¦ç´¯ç©ã€æ··åˆç²¾åº¦ã€checkpointing

### å¯¦é©—ç›®æ¨™
- å±•ç¤ºçœŸå¯¦çš„å¤šGPUè¨“ç·´åŠ é€Ÿ
- å°æ¯”å–®GPU vs å¤šGPUæ€§èƒ½
- æ¼”ç¤ºé€šè¨Šç“¶é ¸èˆ‡å„ªåŒ–ç­–ç•¥
- æä¾›å®Œæ•´çš„ç”Ÿç”¢ç´šé…ç½®ç¯„ä¾‹

---

## ğŸ’¡ çµ¦å­¸ç¿’è€…çš„å»ºè­°

1. **ç†è«–å…ˆè¡Œ**: å…ˆå¾¹åº•ç†è§£ DDP çš„å·¥ä½œåŸç†
2. **ä»£ç¢¼é–±è®€**: ä»”ç´°ç ”è®€ç†è«–æ–‡ä»¶ä¸­çš„ä»£ç¢¼ç¯„ä¾‹
3. **é…ç½®ç†Ÿæ‚‰**: æŒæ¡å„ç¨®é…ç½®åƒæ•¸çš„å«ç¾©
4. **é›²ç«¯å¯¦è¸**: è€ƒæ…®ä½¿ç”¨é›²ç«¯å¤šGPUè³‡æºé€²è¡Œå¯¦é©— (AWS, GCP, Azure)
5. **è€å¿ƒç­‰å¾…**: å°ˆæ¡ˆæœªä¾†å°‡è£œå……å®Œæ•´å¯¦é©—å…§å®¹

---

**ç‹€æ…‹**: â¸ï¸ å¾…é–‹ç™¼ (ç­‰å¾…å¤šGPUç’°å¢ƒ)
**æœ€å¾Œæ›´æ–°**: 2025-10-08
**ç›¸é—œæ–‡ä»¶**:
- ç†è«–: `01-Theory/1.2-Distributed_Training.md`
- å¯¦é©—å®¤: `Lab-1.3-Finetune_Alpaca_with_DeepSpeed` (åŒæ¨£å¾…é–‹ç™¼)
