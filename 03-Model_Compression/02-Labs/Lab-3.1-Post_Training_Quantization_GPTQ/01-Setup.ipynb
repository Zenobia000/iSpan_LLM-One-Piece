{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1: GPTQ Post-Training Quantization - Environment Setup\n",
    "\n",
    "**Goal:** Prepare the environment for GPTQ quantization experiments.\n",
    "\n",
    "**You will learn to:**\n",
    "- Verify GPU and CUDA compatibility for quantization\n",
    "- Install AutoGPTQ, Optimum, and related libraries\n",
    "- Check ExLlama kernel support for accelerated inference\n",
    "- Load a baseline FP16 model to establish performance benchmarks\n",
    "\n",
    "---\n",
    "\n",
    "## Why Environment Verification Matters\n",
    "\n",
    "**GPTQ quantization has specific hardware requirements**:\n",
    "- **GPU Memory**: Quantization process requires loading FP16 model (~15GB for Llama-2-7B)\n",
    "- **CUDA Compute Capability**: ExLlama acceleration requires Ampere+ architecture (SM 8.0+)\n",
    "- **Library Versions**: AutoGPTQ requires transformers>=4.35.0, torch>=2.0.0\n",
    "\n",
    "**Time investment**: 5-10 minutes (one-time setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Hardware Verification\n",
    "\n",
    "First, let's verify GPU availability and specifications. GPTQ quantization **requires a CUDA-capable GPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA GPU status\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Configuration Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch and CUDA versions\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # GPU details\n",
    "    gpu_id = 0\n",
    "    gpu_props = torch.cuda.get_device_properties(gpu_id)\n",
    "    \n",
    "    print(f\"\\n✅ GPU Detected:\")\n",
    "    print(f\"   Name: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "    print(f\"   Total Memory: {gpu_props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Compute Capability: SM {gpu_props.major}.{gpu_props.minor}\")\n",
    "    \n",
    "    # Check if ExLlama is supported (Ampere+ = SM 8.0+)\n",
    "    if gpu_props.major >= 8:\n",
    "        print(f\"   ✅ ExLlama Support: YES (SM {gpu_props.major}.{gpu_props.minor} >= 8.0)\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  ExLlama Support: NO (SM {gpu_props.major}.{gpu_props.minor} < 8.0)\")\n",
    "        print(f\"      → Will use slower fallback kernels\")\n",
    "    \n",
    "    # Memory recommendation\n",
    "    if gpu_props.total_memory / 1e9 >= 16:\n",
    "        print(f\"   ✅ Memory: Sufficient for quantization (>= 16GB)\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  Memory: Limited (<16GB). May need CPU offloading.\")\n",
    "else:\n",
    "    print(\"\\n❌ No GPU detected!\")\n",
    "    print(\"   GPTQ quantization requires a CUDA GPU.\")\n",
    "    print(\"   Please run on a machine with NVIDIA GPU.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Install Quantization Libraries\n",
    "\n",
    "We'll install the core libraries for GPTQ quantization:\n",
    "\n",
    "- **auto-gptq**: Official GPTQ implementation\n",
    "- **optimum**: Hugging Face optimization toolkit (includes GPTQ integration)\n",
    "- **transformers**: Latest version with quantization support\n",
    "- **accelerate**: For distributed/mixed-precision training\n",
    "- **datasets**: For loading calibration data\n",
    "\n",
    "**Note**: This may take 3-5 minutes. We use `-q` for quiet mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core quantization libraries\n",
    "!pip install -q auto-gptq optimum  # GPTQ quantization engine\n",
    "!pip install -q transformers>=4.35.0  # Quantization model support\n",
    "!pip install -q accelerate datasets  # Training and data utilities\n",
    "\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Verify Library Versions\n",
    "\n",
    "Let's verify all libraries are installed correctly with compatible versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import datasets\n",
    "import optimum\n",
    "\n",
    "# Try importing auto_gptq (may be installed as different package name)\n",
    "try:\n",
    "    import auto_gptq\n",
    "    gptq_version = auto_gptq.__version__\n",
    "except ImportError:\n",
    "    gptq_version = \"Not installed or import failed\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Library Version Check\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch:      {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"Accelerate:   {accelerate.__version__}\")\n",
    "print(f\"Datasets:     {datasets.__version__}\")\n",
    "print(f\"Optimum:      {optimum.__version__}\")\n",
    "print(f\"AutoGPTQ:     {gptq_version}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Version checks\n",
    "def check_version(name, current, required):\n",
    "    from packaging import version\n",
    "    if version.parse(current) >= version.parse(required):\n",
    "        print(f\"✅ {name}: {current} >= {required}\")\n",
    "    else:\n",
    "        print(f\"⚠️  {name}: {current} < {required} (may cause issues)\")\n",
    "\n",
    "check_version(\"Transformers\", transformers.__version__, \"4.35.0\")\n",
    "check_version(\"PyTorch\", torch.__version__.split(\"+\")[0], \"2.0.0\")\n",
    "\n",
    "print(\"\\n✅ All libraries verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Check ExLlama Kernel Support\n",
    "\n",
    "ExLlama is a highly optimized CUDA kernel for GPTQ inference. It provides:\n",
    "- **20-50% faster inference** compared to default kernels\n",
    "- **Lower memory usage** through optimized matrix multiplication\n",
    "\n",
    "**Requirements**:\n",
    "- NVIDIA GPU with Compute Capability >= 8.0 (Ampere+)\n",
    "- Examples: A100, A10G, RTX 3090/4090, RTX A6000\n",
    "- **Not supported**: V100 (SM 7.0), T4 (SM 7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ExLlama can be used\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    compute_capability = gpu_props.major * 10 + gpu_props.minor\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ExLlama Kernel Support Check\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Compute Capability: SM {gpu_props.major}.{gpu_props.minor} ({compute_capability})\")\n",
    "    print()\n",
    "    \n",
    "    if compute_capability >= 80:\n",
    "        print(\"✅ ExLlama is SUPPORTED!\")\n",
    "        print(\"   → Expected inference speedup: 20-50%\")\n",
    "        print(\"   → Use: GPTQConfig(use_exllama=True)\")\n",
    "    else:\n",
    "        print(\"⚠️  ExLlama is NOT supported on this GPU\")\n",
    "        print(f\"   → Your GPU: SM {gpu_props.major}.{gpu_props.minor} (need >= 8.0)\")\n",
    "        print(\"   → Will use standard CUDA kernels (slower)\")\n",
    "        print(\"   → Use: GPTQConfig(use_exllama=False)\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"❌ No GPU available for ExLlama check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Load Baseline Model (FP16)\n",
    "\n",
    "Let's load the baseline **Llama-2-7B** model in FP16 precision. This will:\n",
    "- Establish a performance baseline for comparison\n",
    "- Verify model loading works correctly\n",
    "- Measure FP16 memory usage\n",
    "\n",
    "**Note**: This requires **~15GB GPU memory**. If you encounter OOM:\n",
    "- Use a smaller model (e.g., `meta-llama/Llama-2-7b-hf` → `TinyLlama/TinyLlama-1.1B`)\n",
    "- Enable CPU offloading: `device_map=\"auto\"` with `max_memory={0: \"10GB\", \"cpu\": \"30GB\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Change to TinyLlama if OOM\n",
    "# Alternative for limited GPU: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loading Baseline Model: {MODEL_NAME}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"⏳ This may take 1-3 minutes...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"✅ Tokenizer loaded\")\n",
    "\n",
    "# Load model in FP16\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # FP16 precision\n",
    "    device_map=\"auto\",          # Automatic device placement\n",
    "    trust_remote_code=True      # Allow custom model code\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded in FP16\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\n📊 GPU Memory Usage:\")\n",
    "    print(f\"   Allocated: {memory_allocated:.2f} GB\")\n",
    "    print(f\"   Reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# Model info\n",
    "num_params = sum(p.numel() for p in model_fp16.parameters())\n",
    "print(f\"\\n📝 Model Info:\")\n",
    "print(f\"   Parameters: {num_params / 1e9:.2f}B\")\n",
    "print(f\"   Precision: FP16 (2 bytes/param)\")\n",
    "print(f\"   Estimated size: {num_params * 2 / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ Baseline model ready for quantization!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Test Baseline Inference\n",
    "\n",
    "Let's perform a quick inference test to verify the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Baseline Inference Test (FP16)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_fp16.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "latency = end_time - start_time\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Output: {generated_text}\\n\")\n",
    "print(f\"⏱️  Latency: {latency:.2f} seconds\")\n",
    "print(f\"📊 Tokens/sec: {len(outputs[0]) / latency:.2f}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Inference test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Clean Up Memory (Optional)\n",
    "\n",
    "Free GPU memory before proceeding to quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clear memory if needed\n",
    "# Uncomment if you need to free GPU memory\n",
    "\n",
    "# del model_fp16\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"✅ Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✅ Setup Complete!\n",
    "\n",
    "**Summary**:\n",
    "- ✅ GPU verified (CUDA available)\n",
    "- ✅ Quantization libraries installed\n",
    "- ✅ ExLlama support checked\n",
    "- ✅ Baseline FP16 model loaded\n",
    "- ✅ Inference test passed\n",
    "\n",
    "**Next Steps**:\n",
    "1. Proceed to **02-Quantize.ipynb** to apply GPTQ quantization\n",
    "2. Compare quantized model (3.5GB) vs baseline (13.5GB)\n",
    "3. Benchmark inference performance (2-4x speedup expected)\n",
    "\n",
    "**Troubleshooting**:\n",
    "- **OOM during model loading**: Use TinyLlama-1.1B or enable CPU offloading\n",
    "- **ExLlama not supported**: Use `use_exllama=False` in quantization config\n",
    "- **Import errors**: Ensure transformers>=4.35.0 and torch>=2.0.0\n",
    "\n",
    "---\n",
    "\n",
    "**⏭️ Continue to**: [02-Quantize.ipynb](./02-Quantize.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
