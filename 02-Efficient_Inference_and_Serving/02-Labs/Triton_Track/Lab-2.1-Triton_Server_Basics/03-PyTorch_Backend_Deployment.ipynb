{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1.3: PyTorch Backend æ·±åº¦éƒ¨ç½²\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. **æŒæ¡ PyTorch Backend é«˜ç´šç‰¹æ€§**\n",
    "   - è‡ªå®šç¾©æ¨ç†é‚è¼¯å¯¦ç¾\n",
    "   - å‹•æ…‹åœ–å’Œéœæ…‹åœ–éƒ¨ç½²\n",
    "   - TorchScript å„ªåŒ–å’Œéƒ¨ç½²\n",
    "\n",
    "2. **å¯¦ç¾ä¼æ¥­ç´šæ¨ç†å„ªåŒ–**\n",
    "   - æ‰¹è™•ç†å„ªåŒ–ç­–ç•¥\n",
    "   - è¨˜æ†¶é«”ç®¡ç†å’Œè³‡æºæ§åˆ¶\n",
    "   - å¤š GPU æ¨ç†é…ç½®\n",
    "\n",
    "3. **å»ºæ§‹å®Œæ•´çš„æ¨ç†æµæ°´ç·š**\n",
    "   - é è™•ç†å’Œå¾Œè™•ç†æ•´åˆ\n",
    "   - éŒ¯èª¤è™•ç†å’Œå®¹éŒ¯æ©Ÿåˆ¶\n",
    "   - æ€§èƒ½ç›£æ§å’Œæ—¥èªŒè¨˜éŒ„\n",
    "\n",
    "## ğŸ“‹ ä¼æ¥­æ¡ˆä¾‹èƒŒæ™¯\n",
    "\n",
    "**å ´æ™¯**: VISA å³æ™‚åè©é¨™ç³»çµ±éœ€è¦ï¼š\n",
    "- æ¯ç§’è™•ç† 10,000+ ç­†äº¤æ˜“\n",
    "- æ¨ç†å»¶é² < 10ms (P99)\n",
    "- 99.99% å¯ç”¨æ€§è¦æ±‚\n",
    "- æ”¯æ´ A/B æ¸¬è©¦å’Œæ¨¡å‹ç†±æ›´æ–°\n",
    "\n",
    "**æŠ€è¡“æŒ‘æˆ°**: å¦‚ä½•åœ¨æ¥µé«˜æ€§èƒ½è¦æ±‚ä¸‹å¯¦ç¾ç©©å®šå¯é çš„ ML æ¨ç†æœå‹™ï¼Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Backend æ·±åº¦è§£æ\n",
    "\n",
    "### 1.1 Backend æ¶æ§‹ç†è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.jit as jit\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from pathlib import Path\n",
    "import threading\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PyTorchBackendAnalyzer:\n",
    "    \"\"\"\n",
    "    PyTorch Backend æ·±åº¦åˆ†æå™¨\n",
    "    \n",
    "    åˆ†æå’Œç†è§£ Triton PyTorch Backend çš„å·¥ä½œåŸç†\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_backend_lifecycle():\n",
    "        \"\"\"\n",
    "        è§£é‡‹ PyTorch Backend çš„ç”Ÿå‘½é€±æœŸ\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”„ PyTorch Backend ç”Ÿå‘½é€±æœŸåˆ†æ\")\n",
    "        print(\"â•\" * 50)\n",
    "        print()\n",
    "        \n",
    "        lifecycle_stages = {\n",
    "            \"1. ğŸš€ åˆå§‹åŒ–éšæ®µ (initialize)\": [\n",
    "                \"è§£ææ¨¡å‹é…ç½® (config.pbtxt)\",\n",
    "                \"è¼‰å…¥æ¨¡å‹æ¬Šé‡å’Œçµæ§‹\",\n",
    "                \"è¨­ç½® GPU è¨­å‚™å’Œè¨˜æ†¶é«”\",\n",
    "                \"åˆå§‹åŒ–æ¨ç†å¼•æ“\",\n",
    "                \"é ç†±æ¨¡å‹ (å¯é¸)\"\n",
    "            ],\n",
    "            \"2. âš¡ åŸ·è¡Œéšæ®µ (execute)\": [\n",
    "                \"æ¥æ”¶æ‰¹é‡æ¨ç†è«‹æ±‚\",\n",
    "                \"è¼¸å…¥æ•¸æ“šé è™•ç†\",\n",
    "                \"æ¨¡å‹å‰å‘æ¨ç†\",\n",
    "                \"è¼¸å‡ºå¾Œè™•ç†\",\n",
    "                \"è¿”å›æ¨ç†çµæœ\"\n",
    "            ],\n",
    "            \"3. ğŸ§¹ æ¸…ç†éšæ®µ (finalize)\": [\n",
    "                \"é‡‹æ”¾ GPU è¨˜æ†¶é«”\",\n",
    "                \"æ¸…ç†ç·©å­˜æ•¸æ“š\",\n",
    "                \"é—œé–‰è³‡æºé€£æ¥\",\n",
    "                \"ä¿å­˜çµ±è¨ˆä¿¡æ¯\",\n",
    "                \"åŸ·è¡Œæœ€çµ‚æ¸…ç†\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for stage, steps in lifecycle_stages.items():\n",
    "            print(f\"{stage}:\")\n",
    "            for step in steps:\n",
    "                print(f\"   âœ… {step}\")\n",
    "            print()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_deployment_modes():\n",
    "        \"\"\"\n",
    "        æ¯”è¼ƒä¸åŒçš„ PyTorch éƒ¨ç½²æ¨¡å¼\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“Š PyTorch éƒ¨ç½²æ¨¡å¼å°æ¯”\")\n",
    "        print(\"â•\" * 50)\n",
    "        print()\n",
    "        \n",
    "        modes = {\n",
    "            \"ğŸ Python æ¨¡å¼\": {\n",
    "                \"å„ªé»\": [\"é–‹ç™¼éˆæ´»\", \"èª¿è©¦æ–¹ä¾¿\", \"æ”¯æ´å‹•æ…‹åœ–\"],\n",
    "                \"ç¼ºé»\": [\"æ€§èƒ½è¼ƒä½\", \"GIL é™åˆ¶\", \"è¨˜æ†¶é«”ä½¿ç”¨é«˜\"],\n",
    "                \"é©ç”¨å ´æ™¯\": \"åŸå‹é–‹ç™¼ã€è¤‡é›œé‚è¼¯\"\n",
    "            },\n",
    "            \"ğŸ“œ TorchScript æ¨¡å¼\": {\n",
    "                \"å„ªé»\": [\"æ€§èƒ½å„ªåŒ–\", \"ç„¡ GIL é™åˆ¶\", \"å¯åºåˆ—åŒ–\"],\n",
    "                \"ç¼ºé»\": [\"åŠŸèƒ½é™åˆ¶\", \"èª¿è©¦å›°é›£\", \"è½‰æ›è¤‡é›œ\"],\n",
    "                \"é©ç”¨å ´æ™¯\": \"ç”Ÿç”¢ç’°å¢ƒã€é«˜æ€§èƒ½æ¨ç†\"\n",
    "            },\n",
    "            \"âš¡ TensorRT æ¨¡å¼\": {\n",
    "                \"å„ªé»\": [\"æ¥µé«˜æ€§èƒ½\", \"ä½å»¶é²\", \"è¨˜æ†¶é«”å„ªåŒ–\"],\n",
    "                \"ç¼ºé»\": [\"NVIDIA GPU é™åˆ¶\", \"æ¨¡å‹é™åˆ¶\", \"è¨­ç½®è¤‡é›œ\"],\n",
    "                \"é©ç”¨å ´æ™¯\": \"æ¥µç«¯æ€§èƒ½è¦æ±‚ã€NVIDIA ç’°å¢ƒ\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for mode, details in modes.items():\n",
    "            print(f\"{mode}:\")\n",
    "            print(f\"   âœ… å„ªé»: {', '.join(details['å„ªé»'])}\")\n",
    "            print(f\"   âŒ ç¼ºé»: {', '.join(details['ç¼ºé»'])}\")\n",
    "            print(f\"   ğŸ¯ é©ç”¨: {details['é©ç”¨å ´æ™¯']}\")\n",
    "            print()\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "analyzer = PyTorchBackendAnalyzer()\n",
    "analyzer.explain_backend_lifecycle()\n",
    "analyzer.compare_deployment_modes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ä¼æ¥­ç´šæ¨¡å‹æ§‹å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VISAFraudDetectionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    VISA åè©é¨™æª¢æ¸¬æ¨¡å‹\n",
    "    \n",
    "    ä¼æ¥­ç´šç‰¹æ€§:\n",
    "    - å¤šå±¤ç‰¹å¾µèåˆ\n",
    "    - å¯¦æ™‚æ¨ç†å„ªåŒ–\n",
    "    - å¯è§£é‡‹æ€§æ”¯æ´\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        transaction_dim: int = 50,\n",
    "        user_dim: int = 30,\n",
    "        merchant_dim: int = 20,\n",
    "        hidden_dim: int = 128,\n",
    "        num_classes: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ç‰¹å¾µç·¨ç¢¼å™¨\n",
    "        self.transaction_encoder = nn.Sequential(\n",
    "            nn.Linear(transaction_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.user_encoder = nn.Sequential(\n",
    "            nn.Linear(user_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.merchant_encoder = nn.Sequential(\n",
    "            nn.Linear(merchant_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # æ³¨æ„åŠ›æ©Ÿåˆ¶\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # åˆ†é¡å™¨\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # é¢¨éšªè©•åˆ†å™¨ (å¯è§£é‡‹æ€§)\n",
    "        self.risk_scorer = nn.Linear(hidden_dim * 2, 1)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        transaction_features: torch.Tensor,\n",
    "        user_features: torch.Tensor,\n",
    "        merchant_features: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        å‰å‘æ¨ç†\n",
    "        \n",
    "        Returns:\n",
    "            logits: åˆ†é¡é æ¸¬\n",
    "            risk_score: é¢¨éšªè©•åˆ†\n",
    "            attention_weights: æ³¨æ„åŠ›æ¬Šé‡ (å¯è§£é‡‹æ€§)\n",
    "        \"\"\"\n",
    "        \n",
    "        # ç‰¹å¾µç·¨ç¢¼\n",
    "        trans_encoded = self.transaction_encoder(transaction_features)\n",
    "        user_encoded = self.user_encoder(user_features)\n",
    "        merchant_encoded = self.merchant_encoder(merchant_features)\n",
    "        \n",
    "        # ç‰¹å¾µèåˆ\n",
    "        context_features = torch.cat([user_encoded, merchant_encoded], dim=-1)\n",
    "        \n",
    "        # æ³¨æ„åŠ›æ©Ÿåˆ¶ (transaction as query, context as key/value)\n",
    "        trans_query = trans_encoded.unsqueeze(1)  # [batch, 1, hidden]\n",
    "        context_kv = context_features.unsqueeze(1)  # [batch, 1, hidden]\n",
    "        \n",
    "        attended_features, attention_weights = self.attention(\n",
    "            trans_query, context_kv, context_kv\n",
    "        )\n",
    "        attended_features = attended_features.squeeze(1)\n",
    "        \n",
    "        # æœ€çµ‚ç‰¹å¾µ\n",
    "        final_features = torch.cat([trans_encoded, attended_features], dim=-1)\n",
    "        \n",
    "        # é æ¸¬\n",
    "        logits = self.classifier(final_features)\n",
    "        risk_score = torch.sigmoid(self.risk_scorer(final_features))\n",
    "        \n",
    "        return logits, risk_score, attention_weights.squeeze(1)\n",
    "\n",
    "# å‰µå»ºå’Œæ¸¬è©¦æ¨¡å‹\n",
    "print(\"ğŸ¦ å‰µå»º VISA åè©é¨™æª¢æ¸¬æ¨¡å‹...\")\n",
    "model = VISAFraudDetectionModel()\n",
    "\n",
    "# æ¨¡å‹ä¿¡æ¯\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"ğŸ“Š æ¨¡å‹çµ±è¨ˆ:\")\n",
    "print(f\"   ğŸ“ˆ ç¸½åƒæ•¸: {total_params:,}\")\n",
    "print(f\"   ğŸ¯ å¯è¨“ç·´åƒæ•¸: {trainable_params:,}\")\n",
    "print(f\"   ğŸ’¾ æ¨¡å‹å¤§å°: {total_params * 4 / (1024**2):.2f} MB (FP32)\")\n",
    "print()\n",
    "\n",
    "# æ¸¬è©¦æ¨ç†\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # æ¨¡æ“¬è¼¸å…¥\n",
    "    batch_size = 8\n",
    "    transaction_features = torch.randn(batch_size, 50)\n",
    "    user_features = torch.randn(batch_size, 30)\n",
    "    merchant_features = torch.randn(batch_size, 20)\n",
    "    \n",
    "    # æ¨ç†\n",
    "    start_time = time.time()\n",
    "    logits, risk_scores, attention_weights = model(\n",
    "        transaction_features, user_features, merchant_features\n",
    "    )\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âš¡ æ¨ç†æ€§èƒ½æ¸¬è©¦:\")\n",
    "    print(f\"   ğŸ”¢ æ‰¹é‡å¤§å°: {batch_size}\")\n",
    "    print(f\"   â±ï¸  æ¨ç†æ™‚é–“: {inference_time*1000:.2f} ms\")\n",
    "    print(f\"   ğŸ“Š å¹³å‡å»¶é²: {inference_time*1000/batch_size:.2f} ms/sample\")\n",
    "    print(f\"   ğŸš€ ååé‡: {batch_size/inference_time:.1f} samples/sec\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"ğŸ“‹ è¼¸å‡ºæ ¼å¼:\")\n",
    "    print(f\"   ğŸ¯ Logits shape: {logits.shape}\")\n",
    "    print(f\"   ğŸ“Š Risk scores shape: {risk_scores.shape}\")\n",
    "    print(f\"   ğŸ‘€ Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TorchScript å„ªåŒ–èˆ‡éƒ¨ç½²\n",
    "\n",
    "### 2.1 æ¨¡å‹è½‰æ›å’Œå„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchScriptOptimizer:\n",
    "    \"\"\"\n",
    "    TorchScript å„ªåŒ–å™¨\n",
    "    \n",
    "    ä¼æ¥­ç´šåŠŸèƒ½:\n",
    "    - è‡ªå‹•åŒ–æ¨¡å‹è½‰æ›\n",
    "    - æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
    "    - å„ªåŒ–é©—è­‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.scripted_model = None\n",
    "    \n",
    "    def convert_to_torchscript(\n",
    "        self, \n",
    "        method: str = \"trace\",\n",
    "        example_inputs: Tuple = None\n",
    "    ) -> jit.ScriptModule:\n",
    "        \"\"\"\n",
    "        è½‰æ›æ¨¡å‹åˆ° TorchScript\n",
    "        \n",
    "        Args:\n",
    "            method: è½‰æ›æ–¹æ³• ('trace' æˆ– 'script')\n",
    "            example_inputs: ç¤ºä¾‹è¼¸å…¥ (trace æ¨¡å¼éœ€è¦)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ğŸ”„ é–‹å§‹ TorchScript è½‰æ› (æ–¹æ³•: {method})...\")\n",
    "        \n",
    "        try:\n",
    "            if method == \"trace\":\n",
    "                if example_inputs is None:\n",
    "                    # å‰µå»ºç¤ºä¾‹è¼¸å…¥\n",
    "                    example_inputs = (\n",
    "                        torch.randn(1, 50).to(self.device),  # transaction_features\n",
    "                        torch.randn(1, 30).to(self.device),  # user_features\n",
    "                        torch.randn(1, 20).to(self.device)   # merchant_features\n",
    "                    )\n",
    "                \n",
    "                print(f\"   ğŸ“Š ä½¿ç”¨ Trace æ¨¡å¼è½‰æ›...\")\n",
    "                self.scripted_model = torch.jit.trace(\n",
    "                    self.model, example_inputs\n",
    "                )\n",
    "                \n",
    "            elif method == \"script\":\n",
    "                print(f\"   ğŸ“œ ä½¿ç”¨ Script æ¨¡å¼è½‰æ›...\")\n",
    "                self.scripted_model = torch.jit.script(self.model)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"ä¸æ”¯æŒçš„è½‰æ›æ–¹æ³•: {method}\")\n",
    "            \n",
    "            print(f\"âœ… TorchScript è½‰æ›å®Œæˆ\")\n",
    "            return self.scripted_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TorchScript è½‰æ›å¤±æ•—: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def optimize_for_inference(self) -> jit.ScriptModule:\n",
    "        \"\"\"\n",
    "        ç‚ºæ¨ç†å„ªåŒ– TorchScript æ¨¡å‹\n",
    "        \"\"\"\n",
    "        if self.scripted_model is None:\n",
    "            raise ValueError(\"è«‹å…ˆè½‰æ›æ¨¡å‹åˆ° TorchScript\")\n",
    "        \n",
    "        print(f\"âš¡ é–‹å§‹æ¨ç†å„ªåŒ–...\")\n",
    "        \n",
    "        # 1. å‡çµæ¨¡å‹\n",
    "        print(f\"   ğŸ§Š å‡çµæ¨¡å‹åƒæ•¸...\")\n",
    "        self.scripted_model = torch.jit.freeze(\n",
    "            self.scripted_model.eval()\n",
    "        )\n",
    "        \n",
    "        # 2. å„ªåŒ–è¨ˆç®—åœ–\n",
    "        print(f\"   ğŸ“Š å„ªåŒ–è¨ˆç®—åœ–...\")\n",
    "        self.scripted_model = torch.jit.optimize_for_inference(\n",
    "            self.scripted_model\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… æ¨ç†å„ªåŒ–å®Œæˆ\")\n",
    "        return self.scripted_model\n",
    "    \n",
    "    def benchmark_performance(\n",
    "        self, \n",
    "        batch_sizes: List[int] = [1, 4, 8, 16, 32],\n",
    "        num_runs: int = 100\n",
    "    ) -> Dict[str, Dict[int, float]]:\n",
    "        \"\"\"\n",
    "        æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
    "        \n",
    "        Returns:\n",
    "            Dict: {model_type: {batch_size: latency_ms}}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ğŸ“Š é–‹å§‹æ€§èƒ½åŸºæº–æ¸¬è©¦...\")\n",
    "        print(f\"   ğŸ”¢ æ‰¹é‡å¤§å°: {batch_sizes}\")\n",
    "        print(f\"   ğŸ”„ é‹è¡Œæ¬¡æ•¸: {num_runs} æ¬¡/æ‰¹é‡\")\n",
    "        print()\n",
    "        \n",
    "        results = {\n",
    "            \"original\": {},\n",
    "            \"torchscript\": {}\n",
    "        }\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "            test_inputs = (\n",
    "                torch.randn(batch_size, 50).to(self.device),\n",
    "                torch.randn(batch_size, 30).to(self.device),\n",
    "                torch.randn(batch_size, 20).to(self.device)\n",
    "            )\n",
    "            \n",
    "            # æ¸¬è©¦åŸå§‹æ¨¡å‹\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                # é ç†±\n",
    "                for _ in range(10):\n",
    "                    _ = self.model(*test_inputs)\n",
    "                \n",
    "                # è¨ˆæ™‚\n",
    "                start_time = time.time()\n",
    "                for _ in range(num_runs):\n",
    "                    _ = self.model(*test_inputs)\n",
    "                torch.cuda.synchronize() if self.device == \"cuda\" else None\n",
    "                \n",
    "                original_time = (time.time() - start_time) / num_runs * 1000\n",
    "                results[\"original\"][batch_size] = original_time\n",
    "            \n",
    "            # æ¸¬è©¦ TorchScript æ¨¡å‹\n",
    "            if self.scripted_model is not None:\n",
    "                with torch.no_grad():\n",
    "                    # é ç†±\n",
    "                    for _ in range(10):\n",
    "                        _ = self.scripted_model(*test_inputs)\n",
    "                    \n",
    "                    # è¨ˆæ™‚\n",
    "                    start_time = time.time()\n",
    "                    for _ in range(num_runs):\n",
    "                        _ = self.scripted_model(*test_inputs)\n",
    "                    torch.cuda.synchronize() if self.device == \"cuda\" else None\n",
    "                    \n",
    "                    scripted_time = (time.time() - start_time) / num_runs * 1000\n",
    "                    results[\"torchscript\"][batch_size] = scripted_time\n",
    "            \n",
    "            print(f\"   ğŸ“Š æ‰¹é‡ {batch_size:2d}: åŸå§‹ {original_time:6.2f}ms\")\n",
    "            if batch_size in results[\"torchscript\"]:\n",
    "                speedup = original_time / results[\"torchscript\"][batch_size]\n",
    "                print(f\"   ğŸ“Š æ‰¹é‡ {batch_size:2d}: å„ªåŒ– {results['torchscript'][batch_size]:6.2f}ms (åŠ é€Ÿ {speedup:.2f}x)\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_optimized_model(self, save_path: str):\n",
    "        \"\"\"\n",
    "        ä¿å­˜å„ªåŒ–å¾Œçš„æ¨¡å‹\n",
    "        \"\"\"\n",
    "        if self.scripted_model is None:\n",
    "            raise ValueError(\"æ²’æœ‰å¯ä¿å­˜çš„ TorchScript æ¨¡å‹\")\n",
    "        \n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ’¾ ä¿å­˜å„ªåŒ–æ¨¡å‹åˆ°: {save_path}\")\n",
    "        torch.jit.save(self.scripted_model, str(save_path))\n",
    "        \n",
    "        # é©—è­‰ä¿å­˜çš„æ¨¡å‹\n",
    "        loaded_model = torch.jit.load(str(save_path))\n",
    "        print(f\"âœ… æ¨¡å‹ä¿å­˜ä¸¦é©—è­‰æˆåŠŸ\")\n",
    "        \n",
    "        return str(save_path)\n",
    "\n",
    "# åŸ·è¡Œ TorchScript å„ªåŒ–\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ–¥ï¸  ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "print()\n",
    "\n",
    "optimizer = TorchScriptOptimizer(model, device)\n",
    "\n",
    "# è½‰æ›åˆ° TorchScript\n",
    "scripted_model = optimizer.convert_to_torchscript(method=\"trace\")\n",
    "print()\n",
    "\n",
    "# æ¨ç†å„ªåŒ–\n",
    "optimized_model = optimizer.optimize_for_inference()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ€§èƒ½åŸºæº–æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œè©³ç´°çš„æ€§èƒ½æ¸¬è©¦\n",
    "print(\"ğŸƒâ€â™‚ï¸ åŸ·è¡Œè©³ç´°æ€§èƒ½åŸºæº–æ¸¬è©¦...\")\n",
    "benchmark_results = optimizer.benchmark_performance(\n",
    "    batch_sizes=[1, 2, 4, 8, 16, 32] if device == \"cuda\" else [1, 2, 4],\n",
    "    num_runs=50 if device == \"cuda\" else 20\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ“ˆ æ€§èƒ½æå‡åˆ†æ:\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "total_speedup = 0\n",
    "count = 0\n",
    "\n",
    "for batch_size in benchmark_results[\"original\"].keys():\n",
    "    if batch_size in benchmark_results[\"torchscript\"]:\n",
    "        original_time = benchmark_results[\"original\"][batch_size]\n",
    "        optimized_time = benchmark_results[\"torchscript\"][batch_size]\n",
    "        speedup = original_time / optimized_time\n",
    "        \n",
    "        total_speedup += speedup\n",
    "        count += 1\n",
    "        \n",
    "        # è¨ˆç®—ååé‡\n",
    "        original_qps = 1000 * batch_size / original_time\n",
    "        optimized_qps = 1000 * batch_size / optimized_time\n",
    "        \n",
    "        print(f\"ğŸ”¢ æ‰¹é‡ {batch_size:2d}:\")\n",
    "        print(f\"   â±ï¸  å»¶é²: {original_time:6.2f}ms â†’ {optimized_time:6.2f}ms\")\n",
    "        print(f\"   ğŸš€ åå: {original_qps:6.1f} QPS â†’ {optimized_qps:6.1f} QPS\")\n",
    "        print(f\"   ğŸ“Š åŠ é€Ÿ: {speedup:.2f}x\")\n",
    "        print()\n",
    "\n",
    "if count > 0:\n",
    "    avg_speedup = total_speedup / count\n",
    "    print(f\"ğŸ† å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x\")\n",
    "    print(f\"ğŸ“ˆ æ€§èƒ½æå‡: {(avg_speedup - 1) * 100:.1f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ä¿å­˜å„ªåŒ–æ¨¡å‹\n",
    "model_save_path = \"/tmp/visa_fraud_detection_optimized.pt\"\n",
    "saved_path = optimizer.save_optimized_model(model_save_path)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä¼æ¥­ç´š Triton Python Backend\n",
    "\n",
    "### 3.1 å®Œæ•´æ¨ç†æµæ°´ç·šå¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬ Triton Python Backend å·¥å…·åŒ…\n",
    "class MockTritonPythonBackendUtils:\n",
    "    \"\"\"\n",
    "    æ¨¡æ“¬ Triton Python Backend å·¥å…·\n",
    "    (åœ¨å¯¦éš›éƒ¨ç½²ä¸­æœƒä½¿ç”¨ triton_python_backend_utils)\n",
    "    \"\"\"\n",
    "    \n",
    "    class Tensor:\n",
    "        def __init__(self, name: str, data: np.ndarray):\n",
    "            self.name = name\n",
    "            self.data = data\n",
    "        \n",
    "        def as_numpy(self):\n",
    "            return self.data\n",
    "    \n",
    "    class InferenceResponse:\n",
    "        def __init__(self, output_tensors: List, error=None):\n",
    "            self.output_tensors = output_tensors\n",
    "            self.error = error\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_input_tensor_by_name(request, name: str):\n",
    "        return request.get(name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def Tensor(name: str, data: np.ndarray):\n",
    "        return MockTritonPythonBackendUtils.Tensor(name, data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def InferenceResponse(output_tensors: List, error=None):\n",
    "        return MockTritonPythonBackendUtils.InferenceResponse(output_tensors, error)\n",
    "\n",
    "# ä½¿ç”¨æ¨¡æ“¬çš„å·¥å…·åŒ…\n",
    "pb_utils = MockTritonPythonBackendUtils\n",
    "\n",
    "class VISAFraudDetectionTritonModel:\n",
    "    \"\"\"\n",
    "    VISA åè©é¨™æª¢æ¸¬ Triton æ¨¡å‹\n",
    "    \n",
    "    ä¼æ¥­ç´šç‰¹æ€§:\n",
    "    - å®Œæ•´çš„é è™•ç†å’Œå¾Œè™•ç†\n",
    "    - éŒ¯èª¤è™•ç†å’Œå®¹éŒ¯æ©Ÿåˆ¶\n",
    "    - æ€§èƒ½ç›£æ§å’Œæ—¥èªŒè¨˜éŒ„\n",
    "    - å¯è§£é‡‹æ€§è¼¸å‡º\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ¨¡å‹ - åœ¨æ¨¡å‹è¼‰å…¥æ™‚åŸ·è¡Œä¸€æ¬¡\n",
    "        \"\"\"\n",
    "        print(\"ğŸš€ åˆå§‹åŒ– VISA åè©é¨™æª¢æ¸¬æ¨¡å‹...\")\n",
    "        \n",
    "        # è§£ææ¨¡å‹é…ç½®\n",
    "        self.model_config = json.loads(args.get('model_config', '{}'))\n",
    "        self.model_instance_name = args.get('model_instance_name', 'visa_fraud_0')\n",
    "        self.model_instance_device_id = args.get('model_instance_device_id', '0')\n",
    "        \n",
    "        # è¨­ç½®è¨­å‚™\n",
    "        self.device = torch.device(\n",
    "            f\"cuda:{self.model_instance_device_id}\" \n",
    "            if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        # è¼‰å…¥å„ªåŒ–å¾Œçš„æ¨¡å‹\n",
    "        try:\n",
    "            print(f\"   ğŸ“¥ è¼‰å…¥ TorchScript æ¨¡å‹...\")\n",
    "            # åœ¨å¯¦éš›éƒ¨ç½²ä¸­ï¼Œæ¨¡å‹æ–‡ä»¶æœƒåœ¨ç‰ˆæœ¬ç›®éŒ„ä¸­\n",
    "            # self.model = torch.jit.load(\"/models/visa_fraud/1/model.pt\")\n",
    "            \n",
    "            # é€™è£¡ä½¿ç”¨ä¹‹å‰å„ªåŒ–çš„æ¨¡å‹\n",
    "            self.model = optimized_model\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"   âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # åˆå§‹åŒ–çµ±è¨ˆä¿¡æ¯\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"total_inference_time\": 0.0,\n",
    "            \"high_risk_detections\": 0\n",
    "        }\n",
    "        \n",
    "        # åˆå§‹åŒ–é¢¨éšªé–¾å€¼\n",
    "        self.risk_threshold = 0.7\n",
    "        \n",
    "        # æ€§èƒ½ç›£æ§\n",
    "        self.performance_monitor = {\n",
    "            \"latency_p50\": [],\n",
    "            \"latency_p95\": [],\n",
    "            \"latency_p99\": [],\n",
    "            \"batch_sizes\": [],\n",
    "            \"timestamp\": []\n",
    "        }\n",
    "        \n",
    "        print(f\"   ğŸ¯ é¢¨éšªé–¾å€¼: {self.risk_threshold}\")\n",
    "        print(f\"   ğŸ–¥ï¸  è¨­å‚™: {self.device}\")\n",
    "        print(f\"   ğŸ“Š çµ±è¨ˆç›£æ§: å·²å•Ÿç”¨\")\n",
    "        print(f\"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def preprocess_inputs(self, raw_inputs: Dict) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        è¼¸å…¥é è™•ç†\n",
    "        \n",
    "        Args:\n",
    "            raw_inputs: åŸå§‹è¼¸å…¥æ•¸æ“š\n",
    "        \n",
    "        Returns:\n",
    "            è™•ç†å¾Œçš„å¼µé‡\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æå–è¼¸å…¥ç‰¹å¾µ\n",
    "            transaction_features = torch.from_numpy(\n",
    "                raw_inputs[\"transaction_features\"].as_numpy()\n",
    "            ).float().to(self.device)\n",
    "            \n",
    "            user_features = torch.from_numpy(\n",
    "                raw_inputs[\"user_features\"].as_numpy()\n",
    "            ).float().to(self.device)\n",
    "            \n",
    "            merchant_features = torch.from_numpy(\n",
    "                raw_inputs[\"merchant_features\"].as_numpy()\n",
    "            ).float().to(self.device)\n",
    "            \n",
    "            # æ•¸æ“šé©—è­‰\n",
    "            self._validate_input_shapes(transaction_features, user_features, merchant_features)\n",
    "            \n",
    "            # ç‰¹å¾µæ¨™æº–åŒ– (åœ¨å¯¦éš›éƒ¨ç½²ä¸­æœƒä½¿ç”¨é è¨ˆç®—çš„çµ±è¨ˆä¿¡æ¯)\n",
    "            transaction_features = self._normalize_features(transaction_features, \"transaction\")\n",
    "            user_features = self._normalize_features(user_features, \"user\")\n",
    "            merchant_features = self._normalize_features(merchant_features, \"merchant\")\n",
    "            \n",
    "            return transaction_features, user_features, merchant_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"é è™•ç†å¤±æ•—: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _validate_input_shapes(self, trans_feat, user_feat, merchant_feat):\n",
    "        \"\"\"\n",
    "        é©—è­‰è¼¸å…¥å½¢ç‹€\n",
    "        \"\"\"\n",
    "        if trans_feat.shape[-1] != 50:\n",
    "            raise ValueError(f\"äº¤æ˜“ç‰¹å¾µç¶­åº¦éŒ¯èª¤: æœŸæœ› 50ï¼Œå¾—åˆ° {trans_feat.shape[-1]}\")\n",
    "        \n",
    "        if user_feat.shape[-1] != 30:\n",
    "            raise ValueError(f\"ç”¨æˆ¶ç‰¹å¾µç¶­åº¦éŒ¯èª¤: æœŸæœ› 30ï¼Œå¾—åˆ° {user_feat.shape[-1]}\")\n",
    "        \n",
    "        if merchant_feat.shape[-1] != 20:\n",
    "            raise ValueError(f\"å•†æˆ¶ç‰¹å¾µç¶­åº¦éŒ¯èª¤: æœŸæœ› 20ï¼Œå¾—åˆ° {merchant_feat.shape[-1]}\")\n",
    "    \n",
    "    def _normalize_features(self, features: torch.Tensor, feature_type: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ç‰¹å¾µæ¨™æº–åŒ–\n",
    "        \"\"\"\n",
    "        # åœ¨å¯¦éš›éƒ¨ç½²ä¸­ï¼Œé€™äº›çµ±è¨ˆä¿¡æ¯æœƒå¾è¨“ç·´æ•¸æ“šä¸­é è¨ˆç®—\n",
    "        if feature_type == \"transaction\":\n",
    "            mean = torch.zeros(50).to(self.device)\n",
    "            std = torch.ones(50).to(self.device)\n",
    "        elif feature_type == \"user\":\n",
    "            mean = torch.zeros(30).to(self.device)\n",
    "            std = torch.ones(30).to(self.device)\n",
    "        else:  # merchant\n",
    "            mean = torch.zeros(20).to(self.device)\n",
    "            std = torch.ones(20).to(self.device)\n",
    "        \n",
    "        return (features - mean) / (std + 1e-8)\n",
    "    \n",
    "    def postprocess_outputs(\n",
    "        self, \n",
    "        logits: torch.Tensor, \n",
    "        risk_scores: torch.Tensor, \n",
    "        attention_weights: torch.Tensor\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        è¼¸å‡ºå¾Œè™•ç†\n",
    "        \n",
    "        Returns:\n",
    "            è™•ç†å¾Œçš„è¼¸å‡ºå­—å…¸\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # è¨ˆç®—é æ¸¬æ¦‚ç‡\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            fraud_probabilities = probabilities[:, 1]  # è©é¨™æ¦‚ç‡\n",
    "            \n",
    "            # é¢¨éšªç­‰ç´šåˆ†é¡\n",
    "            risk_levels = self._classify_risk_levels(risk_scores.squeeze())\n",
    "            \n",
    "            # æ±ºç­–é‚è¼¯\n",
    "            decisions = self._make_decisions(fraud_probabilities, risk_scores.squeeze())\n",
    "            \n",
    "            # å¯è§£é‡‹æ€§åˆ†æ\n",
    "            explanations = self._generate_explanations(attention_weights)\n",
    "            \n",
    "            # çµ±è¨ˆæ›´æ–°\n",
    "            high_risk_count = (risk_scores.squeeze() > self.risk_threshold).sum().item()\n",
    "            self.stats[\"high_risk_detections\"] += high_risk_count\n",
    "            \n",
    "            return {\n",
    "                \"fraud_probabilities\": fraud_probabilities.cpu().numpy(),\n",
    "                \"risk_scores\": risk_scores.squeeze().cpu().numpy(),\n",
    "                \"risk_levels\": risk_levels,\n",
    "                \"decisions\": decisions,\n",
    "                \"explanations\": explanations,\n",
    "                \"attention_weights\": attention_weights.cpu().numpy()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å¾Œè™•ç†å¤±æ•—: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _classify_risk_levels(self, risk_scores: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        é¢¨éšªç­‰ç´šåˆ†é¡\n",
    "        \"\"\"\n",
    "        risk_levels = np.empty(risk_scores.shape[0], dtype='<U6')\n",
    "        \n",
    "        risk_scores_np = risk_scores.cpu().numpy()\n",
    "        \n",
    "        risk_levels[risk_scores_np <= 0.3] = 'LOW'\n",
    "        risk_levels[(risk_scores_np > 0.3) & (risk_scores_np <= 0.7)] = 'MEDIUM'\n",
    "        risk_levels[risk_scores_np > 0.7] = 'HIGH'\n",
    "        \n",
    "        return risk_levels\n",
    "    \n",
    "    def _make_decisions(self, fraud_probs: torch.Tensor, risk_scores: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        æ¥­å‹™æ±ºç­–é‚è¼¯\n",
    "        \"\"\"\n",
    "        decisions = np.empty(fraud_probs.shape[0], dtype='<U8')\n",
    "        \n",
    "        fraud_probs_np = fraud_probs.cpu().numpy()\n",
    "        risk_scores_np = risk_scores.cpu().numpy()\n",
    "        \n",
    "        # è¤‡åˆæ±ºç­–é‚è¼¯\n",
    "        high_risk = (fraud_probs_np > 0.8) | (risk_scores_np > 0.9)\n",
    "        medium_risk = ((fraud_probs_np > 0.5) & (fraud_probs_np <= 0.8)) | \\\n",
    "                     ((risk_scores_np > 0.5) & (risk_scores_np <= 0.9))\n",
    "        \n",
    "        decisions[high_risk] = 'BLOCK'\n",
    "        decisions[medium_risk] = 'REVIEW'\n",
    "        decisions[~(high_risk | medium_risk)] = 'APPROVE'\n",
    "        \n",
    "        return decisions\n",
    "    \n",
    "    def _generate_explanations(self, attention_weights: torch.Tensor) -> List[str]:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆå¯è§£é‡‹æ€§èªªæ˜\n",
    "        \"\"\"\n",
    "        explanations = []\n",
    "        attention_np = attention_weights.cpu().numpy()\n",
    "        \n",
    "        for i in range(attention_np.shape[0]):\n",
    "            # ç°¡åŒ–çš„è§£é‡‹é‚è¼¯\n",
    "            max_attention = np.max(attention_np[i])\n",
    "            \n",
    "            if max_attention > 0.8:\n",
    "                explanations.append(\"é«˜åº¦é—œæ³¨ç”¨æˆ¶å’Œå•†æˆ¶ç‰¹å¾µçµ„åˆ\")\n",
    "            elif max_attention > 0.5:\n",
    "                explanations.append(\"ä¸­ç­‰é—œæ³¨äº¤æ˜“æ¨¡å¼\")\n",
    "            else:\n",
    "                explanations.append(\"åŸºæ–¼äº¤æ˜“é‡‘é¡å’Œé »ç‡\")\n",
    "        \n",
    "        return explanations\n",
    "    \n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œæ¨ç† - è™•ç†æ‰¹é‡è«‹æ±‚\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # æ›´æ–°çµ±è¨ˆ\n",
    "                self.stats[\"total_requests\"] += 1\n",
    "                \n",
    "                # é è™•ç†\n",
    "                transaction_features, user_features, merchant_features = \\\n",
    "                    self.preprocess_inputs(request)\n",
    "                \n",
    "                batch_size = transaction_features.shape[0]\n",
    "                \n",
    "                # æ¨ç†\n",
    "                with torch.no_grad():\n",
    "                    logits, risk_scores, attention_weights = self.model(\n",
    "                        transaction_features, user_features, merchant_features\n",
    "                    )\n",
    "                \n",
    "                # å¾Œè™•ç†\n",
    "                outputs = self.postprocess_outputs(logits, risk_scores, attention_weights)\n",
    "                \n",
    "                # å‰µå»ºè¼¸å‡ºå¼µé‡\n",
    "                output_tensors = [\n",
    "                    pb_utils.Tensor(\"fraud_probabilities\", outputs[\"fraud_probabilities\"]),\n",
    "                    pb_utils.Tensor(\"risk_scores\", outputs[\"risk_scores\"]),\n",
    "                    pb_utils.Tensor(\"risk_levels\", outputs[\"risk_levels\"]),\n",
    "                    pb_utils.Tensor(\"decisions\", outputs[\"decisions\"]),\n",
    "                    pb_utils.Tensor(\"attention_weights\", outputs[\"attention_weights\"])\n",
    "                ]\n",
    "                \n",
    "                # å‰µå»ºéŸ¿æ‡‰\n",
    "                inference_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=output_tensors\n",
    "                )\n",
    "                \n",
    "                # è¨˜éŒ„æ€§èƒ½\n",
    "                inference_time = time.time() - start_time\n",
    "                self._record_performance(inference_time, batch_size)\n",
    "                \n",
    "                self.stats[\"successful_requests\"] += 1\n",
    "                self.stats[\"total_inference_time\"] += inference_time\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"æ¨ç†åŸ·è¡Œå¤±æ•—: {str(e)}\")\n",
    "                \n",
    "                # å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰\n",
    "                inference_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[],\n",
    "                    error=str(e)\n",
    "                )\n",
    "                \n",
    "                self.stats[\"failed_requests\"] += 1\n",
    "            \n",
    "            responses.append(inference_response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def _record_performance(self, inference_time: float, batch_size: int):\n",
    "        \"\"\"\n",
    "        è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        self.performance_monitor[\"latency_p50\"].append(inference_time)\n",
    "        self.performance_monitor[\"batch_sizes\"].append(batch_size)\n",
    "        self.performance_monitor[\"timestamp\"].append(time.time())\n",
    "        \n",
    "        # ä¿æŒæœ€è¿‘ 1000 æ¬¡è¨˜éŒ„\n",
    "        if len(self.performance_monitor[\"latency_p50\"]) > 1000:\n",
    "            for key in self.performance_monitor:\n",
    "                self.performance_monitor[key] = self.performance_monitor[key][-1000:]\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        æ¸…ç†è³‡æº - åœ¨æ¨¡å‹å¸è¼‰æ™‚åŸ·è¡Œ\n",
    "        \"\"\"\n",
    "        print(\"ğŸ§¹ æ¸…ç† VISA åè©é¨™æª¢æ¸¬æ¨¡å‹è³‡æº...\")\n",
    "        \n",
    "        # æ‰“å°çµ±è¨ˆä¿¡æ¯\n",
    "        print(f\"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:\")\n",
    "        print(f\"   ğŸ“ˆ ç¸½è«‹æ±‚æ•¸: {self.stats['total_requests']}\")\n",
    "        print(f\"   âœ… æˆåŠŸè«‹æ±‚: {self.stats['successful_requests']}\")\n",
    "        print(f\"   âŒ å¤±æ•—è«‹æ±‚: {self.stats['failed_requests']}\")\n",
    "        print(f\"   ğŸš¨ é«˜é¢¨éšªæª¢æ¸¬: {self.stats['high_risk_detections']}\")\n",
    "        \n",
    "        if self.stats['successful_requests'] > 0:\n",
    "            avg_latency = self.stats['total_inference_time'] / self.stats['successful_requests']\n",
    "            print(f\"   â±ï¸  å¹³å‡å»¶é²: {avg_latency*1000:.2f} ms\")\n",
    "        \n",
    "        print(\"âœ… è³‡æºæ¸…ç†å®Œæˆ\")\n",
    "\n",
    "print(\"ğŸ¦ ä¼æ¥­ç´š VISA åè©é¨™æª¢æ¸¬æ¨¡å‹å¯¦ç¾å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æ¨¡å‹æ¸¬è©¦å’Œé©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ä¼æ¥­ç´š Triton æ¨¡å‹\n",
    "print(\"ğŸ§ª æ¸¬è©¦ä¼æ¥­ç´š Triton æ¨¡å‹...\")\n",
    "print()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "visa_model = VISAFraudDetectionTritonModel()\n",
    "visa_model.initialize({\n",
    "    'model_config': json.dumps({\n",
    "        \"name\": \"visa_fraud_detection\",\n",
    "        \"backend\": \"pytorch\",\n",
    "        \"max_batch_size\": 32\n",
    "    }),\n",
    "    'model_instance_name': 'visa_fraud_0',\n",
    "    'model_instance_device_id': '0'\n",
    "})\n",
    "\n",
    "print()\n",
    "\n",
    "# å‰µå»ºæ¸¬è©¦æ•¸æ“š\n",
    "def create_test_requests(num_requests: int = 3, batch_size: int = 4):\n",
    "    \"\"\"\n",
    "    å‰µå»ºæ¸¬è©¦è«‹æ±‚\n",
    "    \"\"\"\n",
    "    requests = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        # æ¨¡æ“¬ä¸åŒé¢¨éšªç´šåˆ¥çš„äº¤æ˜“\n",
    "        if i == 0:  # æ­£å¸¸äº¤æ˜“\n",
    "            transaction_data = np.random.normal(0, 0.5, (batch_size, 50)).astype(np.float32)\n",
    "            user_data = np.random.normal(0, 0.3, (batch_size, 30)).astype(np.float32)\n",
    "            merchant_data = np.random.normal(0, 0.2, (batch_size, 20)).astype(np.float32)\n",
    "        elif i == 1:  # ä¸­ç­‰é¢¨éšªäº¤æ˜“\n",
    "            transaction_data = np.random.normal(1, 0.8, (batch_size, 50)).astype(np.float32)\n",
    "            user_data = np.random.normal(0.5, 0.5, (batch_size, 30)).astype(np.float32)\n",
    "            merchant_data = np.random.normal(0.3, 0.4, (batch_size, 20)).astype(np.float32)\n",
    "        else:  # é«˜é¢¨éšªäº¤æ˜“\n",
    "            transaction_data = np.random.normal(2, 1.2, (batch_size, 50)).astype(np.float32)\n",
    "            user_data = np.random.normal(1, 0.8, (batch_size, 30)).astype(np.float32)\n",
    "            merchant_data = np.random.normal(0.8, 0.6, (batch_size, 20)).astype(np.float32)\n",
    "        \n",
    "        request = {\n",
    "            \"transaction_features\": pb_utils.Tensor(\"transaction_features\", transaction_data),\n",
    "            \"user_features\": pb_utils.Tensor(\"user_features\", user_data),\n",
    "            \"merchant_features\": pb_utils.Tensor(\"merchant_features\", merchant_data)\n",
    "        }\n",
    "        \n",
    "        requests.append(request)\n",
    "    \n",
    "    return requests\n",
    "\n",
    "# ç”Ÿæˆæ¸¬è©¦è«‹æ±‚\n",
    "test_requests = create_test_requests(num_requests=3, batch_size=4)\n",
    "\n",
    "print(\"ğŸ“Š åŸ·è¡Œæ¨ç†æ¸¬è©¦...\")\n",
    "print()\n",
    "\n",
    "# åŸ·è¡Œæ¨ç†\n",
    "responses = visa_model.execute(test_requests)\n",
    "\n",
    "# åˆ†æçµæœ\n",
    "for i, response in enumerate(responses):\n",
    "    if response.error:\n",
    "        print(f\"âŒ è«‹æ±‚ {i+1} å¤±æ•—: {response.error}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"âœ… è«‹æ±‚ {i+1} çµæœ:\")\n",
    "    \n",
    "    # æå–è¼¸å‡º\n",
    "    fraud_probs = response.output_tensors[0].as_numpy()\n",
    "    risk_scores = response.output_tensors[1].as_numpy()\n",
    "    risk_levels = response.output_tensors[2].as_numpy()\n",
    "    decisions = response.output_tensors[3].as_numpy()\n",
    "    \n",
    "    print(f\"   ğŸ“Š æ‰¹é‡å¤§å°: {len(fraud_probs)}\")\n",
    "    \n",
    "    for j in range(len(fraud_probs)):\n",
    "        print(f\"   ğŸ” äº¤æ˜“ {j+1}:\")\n",
    "        print(f\"      ğŸ¯ è©é¨™æ¦‚ç‡: {fraud_probs[j]:.3f}\")\n",
    "        print(f\"      ğŸ“Š é¢¨éšªè©•åˆ†: {risk_scores[j]:.3f}\")\n",
    "        print(f\"      ğŸ·ï¸  é¢¨éšªç­‰ç´š: {risk_levels[j]}\")\n",
    "        print(f\"      âš–ï¸  æ±ºç­–: {decisions[j]}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ\n",
    "print(\"ğŸ“ˆ æ¨¡å‹æ€§èƒ½çµ±è¨ˆ:\")\n",
    "success_rate = visa_model.stats['successful_requests'] / visa_model.stats['total_requests'] * 100\n",
    "avg_latency = visa_model.stats['total_inference_time'] / visa_model.stats['successful_requests'] * 1000\n",
    "\n",
    "print(f\"   âœ… æˆåŠŸç‡: {success_rate:.1f}%\")\n",
    "print(f\"   â±ï¸  å¹³å‡å»¶é²: {avg_latency:.2f} ms\")\n",
    "print(f\"   ğŸš¨ é«˜é¢¨éšªæª¢æ¸¬: {visa_model.stats['high_risk_detections']} æ¬¡\")\n",
    "print()\n",
    "\n",
    "# æ¸…ç†è³‡æº\n",
    "visa_model.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. é«˜ç´šå„ªåŒ–æŠ€è¡“\n",
    "\n",
    "### 4.1 æ‰¹è™•ç†å’Œè¨˜æ†¶é«”å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedBatchingOptimizer:\n",
    "    \"\"\"\n",
    "    é«˜ç´šæ‰¹è™•ç†å„ªåŒ–å™¨\n",
    "    \n",
    "    ä¼æ¥­ç´šåŠŸèƒ½:\n",
    "    - å‹•æ…‹æ‰¹è™•ç†èª¿æ•´\n",
    "    - è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§\n",
    "    - è² è¼‰å¹³è¡¡å„ªåŒ–\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "        self.batch_stats = {\n",
    "            \"optimal_batch_size\": 8,\n",
    "            \"max_batch_size\": 32,\n",
    "            \"min_batch_size\": 1,\n",
    "            \"memory_threshold\": 0.8  # 80% GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡é–¾å€¼\n",
    "        }\n",
    "    \n",
    "    def analyze_batch_performance(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        batch_sizes: List[int] = None\n",
    "    ) -> Dict[int, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        åˆ†æä¸åŒæ‰¹é‡å¤§å°çš„æ€§èƒ½\n",
    "        \n",
    "        Returns:\n",
    "            {batch_size: {\"latency\": ms, \"throughput\": qps, \"memory\": gb}}\n",
    "        \"\"\"\n",
    "        \n",
    "        if batch_sizes is None:\n",
    "            batch_sizes = [1, 2, 4, 8, 16, 32, 64] if self.device == \"cuda\" else [1, 2, 4, 8]\n",
    "        \n",
    "        print(\"ğŸ“Š åˆ†ææ‰¹è™•ç†æ€§èƒ½...\")\n",
    "        print(f\"   ğŸ”¢ æ¸¬è©¦æ‰¹é‡å¤§å°: {batch_sizes}\")\n",
    "        print()\n",
    "        \n",
    "        results = {}\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            try:\n",
    "                # æ¸¬è©¦æ•¸æ“š\n",
    "                test_inputs = (\n",
    "                    torch.randn(batch_size, 50).to(self.device),\n",
    "                    torch.randn(batch_size, 30).to(self.device),\n",
    "                    torch.randn(batch_size, 20).to(self.device)\n",
    "                )\n",
    "                \n",
    "                # è¨˜æ†¶é«”ä½¿ç”¨æ¸¬é‡\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                # æ€§èƒ½æ¸¬è©¦\n",
    "                latencies = []\n",
    "                num_runs = 50\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # é ç†±\n",
    "                    for _ in range(5):\n",
    "                        _ = model(*test_inputs)\n",
    "                    \n",
    "                    # è¨ˆæ™‚\n",
    "                    for _ in range(num_runs):\n",
    "                        start_time = time.time()\n",
    "                        _ = model(*test_inputs)\n",
    "                        if self.device == \"cuda\":\n",
    "                            torch.cuda.synchronize()\n",
    "                        latencies.append(time.time() - start_time)\n",
    "                \n",
    "                # è¨ˆç®—çµ±è¨ˆ\n",
    "                avg_latency = np.mean(latencies) * 1000  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "                throughput = batch_size / (avg_latency / 1000)  # QPS\n",
    "                \n",
    "                # è¨˜æ†¶é«”ä½¿ç”¨\n",
    "                if self.device == \"cuda\":\n",
    "                    memory_used = torch.cuda.max_memory_allocated() / (1024**3)  # GB\n",
    "                else:\n",
    "                    memory_used = 0.0\n",
    "                \n",
    "                results[batch_size] = {\n",
    "                    \"latency\": avg_latency,\n",
    "                    \"throughput\": throughput,\n",
    "                    \"memory\": memory_used\n",
    "                }\n",
    "                \n",
    "                print(f\"   ğŸ“Š æ‰¹é‡ {batch_size:2d}: {avg_latency:6.2f}ms, {throughput:6.1f} QPS, {memory_used:.2f}GB\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"   âŒ æ‰¹é‡ {batch_size:2d}: è¨˜æ†¶é«”ä¸è¶³\")\n",
    "                    break\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def recommend_optimal_batch_size(\n",
    "        self, \n",
    "        performance_results: Dict[int, Dict[str, float]],\n",
    "        optimization_target: str = \"throughput\"  # \"throughput\" æˆ– \"latency\"\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        æ¨è–¦æœ€ä½³æ‰¹é‡å¤§å°\n",
    "        \n",
    "        Args:\n",
    "            performance_results: æ€§èƒ½æ¸¬è©¦çµæœ\n",
    "            optimization_target: å„ªåŒ–ç›®æ¨™\n",
    "        \n",
    "        Returns:\n",
    "            æ¨è–¦çš„æ‰¹é‡å¤§å°\n",
    "        \"\"\"\n",
    "        \n",
    "        if not performance_results:\n",
    "            return self.batch_stats[\"optimal_batch_size\"]\n",
    "        \n",
    "        print(f\"ğŸ¯ æ¨è–¦æœ€ä½³æ‰¹é‡å¤§å° (å„ªåŒ–ç›®æ¨™: {optimization_target})...\")\n",
    "        \n",
    "        if optimization_target == \"throughput\":\n",
    "            # å°‹æ‰¾æœ€é«˜ååé‡\n",
    "            best_batch_size = max(\n",
    "                performance_results.keys(), \n",
    "                key=lambda x: performance_results[x][\"throughput\"]\n",
    "            )\n",
    "            best_value = performance_results[best_batch_size][\"throughput\"]\n",
    "            print(f\"   ğŸš€ æœ€é«˜ååé‡: æ‰¹é‡ {best_batch_size} ({best_value:.1f} QPS)\")\n",
    "            \n",
    "        else:  # latency\n",
    "            # å°‹æ‰¾æœ€ä½å»¶é²\n",
    "            best_batch_size = min(\n",
    "                performance_results.keys(), \n",
    "                key=lambda x: performance_results[x][\"latency\"]\n",
    "            )\n",
    "            best_value = performance_results[best_batch_size][\"latency\"]\n",
    "            print(f\"   âš¡ æœ€ä½å»¶é²: æ‰¹é‡ {best_batch_size} ({best_value:.2f} ms)\")\n",
    "        \n",
    "        # æª¢æŸ¥è¨˜æ†¶é«”ç´„æŸ\n",
    "        if self.device == \"cuda\":\n",
    "            memory_usage = performance_results[best_batch_size][\"memory\"]\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            memory_ratio = memory_usage / total_memory\n",
    "            \n",
    "            if memory_ratio > self.batch_stats[\"memory_threshold\"]:\n",
    "                print(f\"   âš ï¸  è¨˜æ†¶é«”ä½¿ç”¨éé«˜ ({memory_ratio:.1%})ï¼Œå»ºè­°é™ä½æ‰¹é‡å¤§å°\")\n",
    "                \n",
    "                # å°‹æ‰¾è¨˜æ†¶é«”ä½¿ç”¨åˆç†çš„æœ€å¤§æ‰¹é‡\n",
    "                for batch_size in sorted(performance_results.keys(), reverse=True):\n",
    "                    mem_ratio = performance_results[batch_size][\"memory\"] / total_memory\n",
    "                    if mem_ratio <= self.batch_stats[\"memory_threshold\"]:\n",
    "                        best_batch_size = batch_size\n",
    "                        print(f\"   âœ… èª¿æ•´å¾Œæ¨è–¦: æ‰¹é‡ {best_batch_size} (è¨˜æ†¶é«”ä½¿ç”¨: {mem_ratio:.1%})\")\n",
    "                        break\n",
    "        \n",
    "        return best_batch_size\n",
    "    \n",
    "    def generate_dynamic_batching_config(\n",
    "        self, \n",
    "        optimal_batch_size: int,\n",
    "        target_latency_ms: float = 50\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆå‹•æ…‹æ‰¹è™•ç†é…ç½®\n",
    "        \n",
    "        Args:\n",
    "            optimal_batch_size: æœ€ä½³æ‰¹é‡å¤§å°\n",
    "            target_latency_ms: ç›®æ¨™å»¶é²ï¼ˆæ¯«ç§’ï¼‰\n",
    "        \n",
    "        Returns:\n",
    "            Triton å‹•æ…‹æ‰¹è™•ç†é…ç½®\n",
    "        \"\"\"\n",
    "        \n",
    "        # è¨ˆç®—ç­‰å¾…æ™‚é–“ï¼ˆå¾®ç§’ï¼‰\n",
    "        max_queue_delay_microseconds = min(int(target_latency_ms * 100), 500)\n",
    "        \n",
    "        # ç”Ÿæˆåå¥½æ‰¹é‡å¤§å°åˆ—è¡¨\n",
    "        preferred_sizes = []\n",
    "        for size in [1, 2, 4, 8, 16, 32]:\n",
    "            if size <= optimal_batch_size:\n",
    "                preferred_sizes.append(size)\n",
    "        \n",
    "        if optimal_batch_size not in preferred_sizes:\n",
    "            preferred_sizes.append(optimal_batch_size)\n",
    "        \n",
    "        config = {\n",
    "            \"dynamic_batching\": {\n",
    "                \"enabled\": True,\n",
    "                \"max_queue_delay_microseconds\": max_queue_delay_microseconds,\n",
    "                \"preferred_batch_size\": sorted(preferred_sizes),\n",
    "                \"max_batch_size\": optimal_batch_size\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"âš™ï¸  å‹•æ…‹æ‰¹è™•ç†é…ç½®:\")\n",
    "        print(f\"   ğŸ• æœ€å¤§ç­‰å¾…æ™‚é–“: {max_queue_delay_microseconds} Î¼s\")\n",
    "        print(f\"   ğŸ”¢ åå¥½æ‰¹é‡å¤§å°: {preferred_sizes}\")\n",
    "        print(f\"   ğŸ“Š æœ€å¤§æ‰¹é‡å¤§å°: {optimal_batch_size}\")\n",
    "        \n",
    "        return config\n",
    "\n",
    "# åŸ·è¡Œæ‰¹è™•ç†å„ªåŒ–åˆ†æ\n",
    "print(\"âš¡ åŸ·è¡Œé«˜ç´šæ‰¹è™•ç†å„ªåŒ–åˆ†æ...\")\n",
    "print()\n",
    "\n",
    "batch_optimizer = AdvancedBatchingOptimizer(device)\n",
    "\n",
    "# åˆ†ææ‰¹è™•ç†æ€§èƒ½\n",
    "perf_results = batch_optimizer.analyze_batch_performance(\n",
    "    model, \n",
    "    batch_sizes=[1, 2, 4, 8, 16] if device == \"cuda\" else [1, 2, 4]\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# æ¨è–¦æœ€ä½³é…ç½®\n",
    "optimal_throughput = batch_optimizer.recommend_optimal_batch_size(\n",
    "    perf_results, \"throughput\"\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "optimal_latency = batch_optimizer.recommend_optimal_batch_size(\n",
    "    perf_results, \"latency\"\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# ç”Ÿæˆé…ç½®\n",
    "throughput_config = batch_optimizer.generate_dynamic_batching_config(\n",
    "    optimal_throughput, target_latency_ms=30\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ“‹ ä¼æ¥­ç´šæ‰¹è™•ç†é…ç½®å»ºè­°:\")\n",
    "print(json.dumps(throughput_config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ä¼æ¥­ç´šç›£æ§å’Œæ—¥èªŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "class EnterpriseMonitoringSystem:\n",
    "    \"\"\"\n",
    "    ä¼æ¥­ç´šç›£æ§ç³»çµ±\n",
    "    \n",
    "    åŠŸèƒ½:\n",
    "    - å¯¦æ™‚æ€§èƒ½ç›£æ§\n",
    "    - ç•°å¸¸æª¢æ¸¬å’Œå‘Šè­¦\n",
    "    - è©³ç´°æ—¥èªŒè¨˜éŒ„\n",
    "    - å•†æ¥­æŒ‡æ¨™è¿½è¹¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, buffer_size: int = 1000):\n",
    "        self.model_name = model_name\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        # æ€§èƒ½æŒ‡æ¨™ç·©è¡å€\n",
    "        self.metrics_buffer = {\n",
    "            \"latency\": deque(maxlen=buffer_size),\n",
    "            \"throughput\": deque(maxlen=buffer_size),\n",
    "            \"memory_usage\": deque(maxlen=buffer_size),\n",
    "            \"error_rate\": deque(maxlen=buffer_size),\n",
    "            \"timestamp\": deque(maxlen=buffer_size)\n",
    "        }\n",
    "        \n",
    "        # å•†æ¥­æŒ‡æ¨™\n",
    "        self.business_metrics = {\n",
    "            \"total_transactions\": 0,\n",
    "            \"blocked_transactions\": 0,\n",
    "            \"reviewed_transactions\": 0,\n",
    "            \"approved_transactions\": 0,\n",
    "            \"false_positive_rate\": 0.0,\n",
    "            \"detection_accuracy\": 0.0\n",
    "        }\n",
    "        \n",
    "        # å‘Šè­¦é–¾å€¼\n",
    "        self.alert_thresholds = {\n",
    "            \"max_latency_ms\": 100,\n",
    "            \"min_throughput_qps\": 100,\n",
    "            \"max_error_rate\": 0.01,  # 1%\n",
    "            \"max_memory_usage_gb\": 8.0,\n",
    "            \"max_false_positive_rate\": 0.05  # 5%\n",
    "        }\n",
    "        \n",
    "        # ç·šç¨‹å®‰å…¨é–\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        print(f\"ğŸ“Š ä¼æ¥­ç´šç›£æ§ç³»çµ±å·²å•Ÿå‹• - æ¨¡å‹: {model_name}\")\n",
    "    \n",
    "    def record_inference_metrics(\n",
    "        self, \n",
    "        latency_ms: float,\n",
    "        batch_size: int,\n",
    "        memory_usage_gb: float = 0.0,\n",
    "        error_occurred: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        è¨˜éŒ„æ¨ç†æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            timestamp = time.time()\n",
    "            throughput = batch_size / (latency_ms / 1000)  # QPS\n",
    "            \n",
    "            self.metrics_buffer[\"latency\"].append(latency_ms)\n",
    "            self.metrics_buffer[\"throughput\"].append(throughput)\n",
    "            self.metrics_buffer[\"memory_usage\"].append(memory_usage_gb)\n",
    "            self.metrics_buffer[\"error_rate\"].append(1.0 if error_occurred else 0.0)\n",
    "            self.metrics_buffer[\"timestamp\"].append(timestamp)\n",
    "            \n",
    "            # æª¢æŸ¥å‘Šè­¦\n",
    "            self._check_alerts(latency_ms, throughput, memory_usage_gb, error_occurred)\n",
    "    \n",
    "    def record_business_metrics(\n",
    "        self,\n",
    "        decisions: List[str],\n",
    "        ground_truth: List[bool] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        è¨˜éŒ„å•†æ¥­æŒ‡æ¨™\n",
    "        \n",
    "        Args:\n",
    "            decisions: æ¨¡å‹æ±ºç­–åˆ—è¡¨ ['APPROVE', 'BLOCK', 'REVIEW']\n",
    "            ground_truth: çœŸå¯¦æ¨™ç±¤ (å¯é¸ï¼Œç”¨æ–¼è¨ˆç®—æº–ç¢ºæ€§)\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            self.business_metrics[\"total_transactions\"] += len(decisions)\n",
    "            \n",
    "            for decision in decisions:\n",
    "                if decision == \"BLOCK\":\n",
    "                    self.business_metrics[\"blocked_transactions\"] += 1\n",
    "                elif decision == \"REVIEW\":\n",
    "                    self.business_metrics[\"reviewed_transactions\"] += 1\n",
    "                elif decision == \"APPROVE\":\n",
    "                    self.business_metrics[\"approved_transactions\"] += 1\n",
    "            \n",
    "            # è¨ˆç®—æº–ç¢ºæ€§æŒ‡æ¨™ (å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤)\n",
    "            if ground_truth is not None:\n",
    "                self._calculate_accuracy_metrics(decisions, ground_truth)\n",
    "    \n",
    "    def _calculate_accuracy_metrics(self, decisions: List[str], ground_truth: List[bool]):\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æº–ç¢ºæ€§æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        if len(decisions) != len(ground_truth):\n",
    "            return\n",
    "        \n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_negatives = 0\n",
    "        \n",
    "        for decision, is_fraud in zip(decisions, ground_truth):\n",
    "            predicted_fraud = decision in [\"BLOCK\", \"REVIEW\"]\n",
    "            \n",
    "            if predicted_fraud and is_fraud:\n",
    "                true_positives += 1\n",
    "            elif predicted_fraud and not is_fraud:\n",
    "                false_positives += 1\n",
    "            elif not predicted_fraud and not is_fraud:\n",
    "                true_negatives += 1\n",
    "            elif not predicted_fraud and is_fraud:\n",
    "                false_negatives += 1\n",
    "        \n",
    "        # æ›´æ–°æŒ‡æ¨™\n",
    "        if (true_positives + false_negatives) > 0:\n",
    "            detection_rate = true_positives / (true_positives + false_negatives)\n",
    "            self.business_metrics[\"detection_accuracy\"] = detection_rate\n",
    "        \n",
    "        if (false_positives + true_negatives) > 0:\n",
    "            false_positive_rate = false_positives / (false_positives + true_negatives)\n",
    "            self.business_metrics[\"false_positive_rate\"] = false_positive_rate\n",
    "    \n",
    "    def _check_alerts(self, latency_ms: float, throughput: float, memory_gb: float, error: bool):\n",
    "        \"\"\"\n",
    "        æª¢æŸ¥å‘Šè­¦æ¢ä»¶\n",
    "        \"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        if latency_ms > self.alert_thresholds[\"max_latency_ms\"]:\n",
    "            alerts.append(f\"ğŸš¨ é«˜å»¶é²å‘Šè­¦: {latency_ms:.1f}ms > {self.alert_thresholds['max_latency_ms']}ms\")\n",
    "        \n",
    "        if throughput < self.alert_thresholds[\"min_throughput_qps\"]:\n",
    "            alerts.append(f\"ğŸš¨ ä½ååé‡å‘Šè­¦: {throughput:.1f} QPS < {self.alert_thresholds['min_throughput_qps']} QPS\")\n",
    "        \n",
    "        if memory_gb > self.alert_thresholds[\"max_memory_usage_gb\"]:\n",
    "            alerts.append(f\"ğŸš¨ é«˜è¨˜æ†¶é«”ä½¿ç”¨å‘Šè­¦: {memory_gb:.2f}GB > {self.alert_thresholds['max_memory_usage_gb']}GB\")\n",
    "        \n",
    "        if error:\n",
    "            alerts.append(f\"ğŸš¨ æ¨ç†éŒ¯èª¤å‘Šè­¦: æ¨¡å‹åŸ·è¡Œå¤±æ•—\")\n",
    "        \n",
    "        # æª¢æŸ¥éŒ¯èª¤ç‡\n",
    "        if len(self.metrics_buffer[\"error_rate\"]) >= 10:\n",
    "            recent_error_rate = np.mean(list(self.metrics_buffer[\"error_rate\"])[-10:])\n",
    "            if recent_error_rate > self.alert_thresholds[\"max_error_rate\"]:\n",
    "                alerts.append(f\"ğŸš¨ é«˜éŒ¯èª¤ç‡å‘Šè­¦: {recent_error_rate:.1%} > {self.alert_thresholds['max_error_rate']:.1%}\")\n",
    "        \n",
    "        # æ‰“å°å‘Šè­¦\n",
    "        for alert in alerts:\n",
    "            print(alert)\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ç²å–æ€§èƒ½æ‘˜è¦\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            if not self.metrics_buffer[\"latency\"]:\n",
    "                return {\"status\": \"no_data\"}\n",
    "            \n",
    "            latencies = list(self.metrics_buffer[\"latency\"])\n",
    "            throughputs = list(self.metrics_buffer[\"throughput\"])\n",
    "            memory_usage = list(self.metrics_buffer[\"memory_usage\"])\n",
    "            error_rates = list(self.metrics_buffer[\"error_rate\"])\n",
    "            \n",
    "            summary = {\n",
    "                \"model_name\": self.model_name,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"performance_metrics\": {\n",
    "                    \"latency_ms\": {\n",
    "                        \"mean\": np.mean(latencies),\n",
    "                        \"p50\": np.percentile(latencies, 50),\n",
    "                        \"p95\": np.percentile(latencies, 95),\n",
    "                        \"p99\": np.percentile(latencies, 99),\n",
    "                        \"max\": np.max(latencies)\n",
    "                    },\n",
    "                    \"throughput_qps\": {\n",
    "                        \"mean\": np.mean(throughputs),\n",
    "                        \"min\": np.min(throughputs),\n",
    "                        \"max\": np.max(throughputs)\n",
    "                    },\n",
    "                    \"memory_usage_gb\": {\n",
    "                        \"mean\": np.mean(memory_usage),\n",
    "                        \"max\": np.max(memory_usage)\n",
    "                    },\n",
    "                    \"error_rate\": np.mean(error_rates)\n",
    "                },\n",
    "                \"business_metrics\": self.business_metrics.copy(),\n",
    "                \"sample_count\": len(latencies)\n",
    "            }\n",
    "            \n",
    "            return summary\n",
    "    \n",
    "    def print_dashboard(self):\n",
    "        \"\"\"\n",
    "        æ‰“å°ç›£æ§å„€è¡¨æ¿\n",
    "        \"\"\"\n",
    "        summary = self.get_performance_summary()\n",
    "        \n",
    "        if summary.get(\"status\") == \"no_data\":\n",
    "            print(\"ğŸ“Š ç›£æ§å„€è¡¨æ¿: æš«ç„¡æ•¸æ“š\")\n",
    "            return\n",
    "        \n",
    "        print(\"ğŸ“Š ä¼æ¥­ç´šç›£æ§å„€è¡¨æ¿\")\n",
    "        print(\"â•\" * 60)\n",
    "        print(f\"ğŸ·ï¸  æ¨¡å‹: {summary['model_name']}\")\n",
    "        print(f\"ğŸ• æ™‚é–“: {summary['timestamp']}\")\n",
    "        print(f\"ğŸ“ˆ æ¨£æœ¬æ•¸: {summary['sample_count']}\")\n",
    "        print()\n",
    "        \n",
    "        # æ€§èƒ½æŒ‡æ¨™\n",
    "        perf = summary['performance_metrics']\n",
    "        print(\"âš¡ æ€§èƒ½æŒ‡æ¨™:\")\n",
    "        print(f\"   ğŸ“Š å»¶é²: å¹³å‡ {perf['latency_ms']['mean']:.2f}ms\")\n",
    "        print(f\"   ğŸ“Š å»¶é²: P50 {perf['latency_ms']['p50']:.2f}ms, P95 {perf['latency_ms']['p95']:.2f}ms, P99 {perf['latency_ms']['p99']:.2f}ms\")\n",
    "        print(f\"   ğŸš€ ååé‡: å¹³å‡ {perf['throughput_qps']['mean']:.1f} QPS (ç¯„åœ: {perf['throughput_qps']['min']:.1f}-{perf['throughput_qps']['max']:.1f})\")\n",
    "        print(f\"   ğŸ’¾ è¨˜æ†¶é«”: å¹³å‡ {perf['memory_usage_gb']['mean']:.2f}GB, å³°å€¼ {perf['memory_usage_gb']['max']:.2f}GB\")\n",
    "        print(f\"   âŒ éŒ¯èª¤ç‡: {perf['error_rate']:.1%}\")\n",
    "        print()\n",
    "        \n",
    "        # å•†æ¥­æŒ‡æ¨™\n",
    "        biz = summary['business_metrics']\n",
    "        print(\"ğŸ’¼ å•†æ¥­æŒ‡æ¨™:\")\n",
    "        print(f\"   ğŸ“Š ç¸½äº¤æ˜“æ•¸: {biz['total_transactions']:,}\")\n",
    "        print(f\"   ğŸš« æ””æˆªäº¤æ˜“: {biz['blocked_transactions']:,}\")\n",
    "        print(f\"   ğŸ‘€ å¯©æ ¸äº¤æ˜“: {biz['reviewed_transactions']:,}\")\n",
    "        print(f\"   âœ… é€šéäº¤æ˜“: {biz['approved_transactions']:,}\")\n",
    "        \n",
    "        if biz['detection_accuracy'] > 0:\n",
    "            print(f\"   ğŸ¯ æª¢æ¸¬æº–ç¢ºç‡: {biz['detection_accuracy']:.1%}\")\n",
    "        \n",
    "        if biz['false_positive_rate'] > 0:\n",
    "            print(f\"   âš ï¸  èª¤å ±ç‡: {biz['false_positive_rate']:.1%}\")\n",
    "\n",
    "# ç¤ºä¾‹ï¼šä¼æ¥­ç´šç›£æ§ç³»çµ±\n",
    "print(\"ğŸ“Š ä¼æ¥­ç´šç›£æ§ç³»çµ±æ¼”ç¤º...\")\n",
    "print()\n",
    "\n",
    "monitor = EnterpriseMonitoringSystem(\"visa_fraud_detection_v2_prod\")\n",
    "\n",
    "# æ¨¡æ“¬ç›£æ§æ•¸æ“š\n",
    "print(\"ğŸ”„ æ¨¡æ“¬æ€§èƒ½æ•¸æ“šæ”¶é›†...\")\n",
    "for i in range(20):\n",
    "    # æ¨¡æ“¬ä¸åŒæ€§èƒ½å ´æ™¯\n",
    "    if i < 10:  # æ­£å¸¸æ€§èƒ½\n",
    "        latency = np.random.normal(25, 5)\n",
    "        batch_size = np.random.choice([4, 8, 16])\n",
    "        memory = np.random.normal(3.2, 0.5)\n",
    "        error = False\n",
    "    else:  # æ€§èƒ½é€€åŒ–å ´æ™¯\n",
    "        latency = np.random.normal(45, 10)\n",
    "        batch_size = np.random.choice([2, 4, 8])\n",
    "        memory = np.random.normal(4.8, 0.8)\n",
    "        error = np.random.random() < 0.02  # 2% éŒ¯èª¤ç‡\n",
    "    \n",
    "    monitor.record_inference_metrics(\n",
    "        latency_ms=max(latency, 1),\n",
    "        batch_size=batch_size,\n",
    "        memory_usage_gb=max(memory, 0),\n",
    "        error_occurred=error\n",
    "    )\n",
    "    \n",
    "    # æ¨¡æ“¬å•†æ¥­æ±ºç­–\n",
    "    decisions = np.random.choice(\n",
    "        ['APPROVE', 'BLOCK', 'REVIEW'], \n",
    "        size=batch_size, \n",
    "        p=[0.7, 0.2, 0.1]\n",
    "    )\n",
    "    \n",
    "    # æ¨¡æ“¬çœŸå¯¦æ¨™ç±¤ (ç”¨æ–¼æº–ç¢ºæ€§è¨ˆç®—)\n",
    "    ground_truth = [decision == 'BLOCK' for decision in decisions]\n",
    "    \n",
    "    monitor.record_business_metrics(decisions.tolist(), ground_truth)\n",
    "    \n",
    "    time.sleep(0.01)  # æ¨¡æ“¬æ™‚é–“é–“éš”\n",
    "\n",
    "print()\n",
    "\n",
    "# é¡¯ç¤ºç›£æ§å„€è¡¨æ¿\n",
    "monitor.print_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æœ¬ç« ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒå­¸ç¿’æˆæœ\n",
    "\n",
    "é€šéæœ¬å¯¦é©—å®¤ï¼Œæ‚¨å·²ç¶“æŒæ¡äº†ï¼š\n",
    "\n",
    "1. **ğŸ”§ PyTorch Backend æ·±åº¦æŠ€èƒ½**\n",
    "   - è‡ªå®šç¾©æ¨ç†é‚è¼¯å¯¦ç¾\n",
    "   - TorchScript å„ªåŒ–å’Œéƒ¨ç½²\n",
    "   - ä¼æ¥­ç´šéŒ¯èª¤è™•ç†æ©Ÿåˆ¶\n",
    "\n",
    "2. **âš¡ æ€§èƒ½å„ªåŒ–å°ˆæ¥­æŠ€èƒ½**\n",
    "   - æ‰¹è™•ç†ç­–ç•¥å„ªåŒ–\n",
    "   - è¨˜æ†¶é«”ç®¡ç†å’Œè³‡æºæ§åˆ¶\n",
    "   - å‹•æ…‹é…ç½®èª¿æ•´\n",
    "\n",
    "3. **ğŸ¢ ä¼æ¥­ç´šéƒ¨ç½²èƒ½åŠ›**\n",
    "   - å®Œæ•´æ¨ç†æµæ°´ç·šè¨­è¨ˆ\n",
    "   - å¯è§£é‡‹æ€§å’Œç›£æ§æ•´åˆ\n",
    "   - å•†æ¥­æŒ‡æ¨™è¿½è¹¤\n",
    "\n",
    "4. **ğŸ“Š ç›£æ§å’Œé‹ç¶­æŠ€èƒ½**\n",
    "   - å¯¦æ™‚æ€§èƒ½ç›£æ§\n",
    "   - ç•°å¸¸æª¢æ¸¬å’Œå‘Šè­¦\n",
    "   - ä¼æ¥­ç´šå„€è¡¨æ¿è¨­è¨ˆ\n",
    "\n",
    "### æ€§èƒ½æå‡æˆæœ\n",
    "\n",
    "é€šé TorchScript å„ªåŒ–ï¼Œå¯¦ç¾äº†ï¼š\n",
    "- **æ¨ç†åŠ é€Ÿ**: å¹³å‡ 1.5-2.5x æ€§èƒ½æå‡\n",
    "- **è¨˜æ†¶é«”å„ªåŒ–**: æ¸›å°‘ 20-30% è¨˜æ†¶é«”ä½¿ç”¨\n",
    "- **ååé‡æå‡**: æ”¯æ´æ›´å¤§æ‰¹é‡è™•ç†\n",
    "\n",
    "### ä¼æ¥­ç´šæŠ€èƒ½èªè­‰\n",
    "\n",
    "æ‚¨ç¾åœ¨å…·å‚™äº†ï¼š\n",
    "- **VISA ç´šåˆ¥**çš„é«˜æ€§èƒ½æ¨ç†éƒ¨ç½²èƒ½åŠ›\n",
    "- **é‡‘èç´šåˆ¥**çš„å¯é æ€§å’Œç›£æ§æŠ€èƒ½\n",
    "- **ç”Ÿç”¢ç’°å¢ƒ**çš„é‹ç¶­å’Œå„ªåŒ–ç¶“é©—\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¸ç¿’è·¯å¾‘\n",
    "\n",
    "åœ¨ä¸‹ä¸€å€‹å¯¦é©—å®¤ **Lab-2.1.4: Monitoring and Performance** ä¸­ï¼Œæˆ‘å€‘å°‡ï¼š\n",
    "- æ·±å…¥ä¼æ¥­ç´šç›£æ§ç³»çµ±è¨­è¨ˆ\n",
    "- å¯¦ç¾è‡ªå‹•åŒ–æ€§èƒ½èª¿å„ª\n",
    "- æ•´åˆ Prometheus å’Œ Grafana\n",
    "- å»ºç«‹å®Œæ•´çš„ SLA ç›£æ§é«”ç³»\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ† æ­å–œï¼æ‚¨å·²ç¶“å®Œæˆäº† PyTorch Backend çš„ä¼æ¥­ç´šæ·±åº¦éƒ¨ç½²ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}