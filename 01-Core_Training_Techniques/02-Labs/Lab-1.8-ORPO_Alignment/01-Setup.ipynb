{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.8: ORPO ç’°å¢ƒè¨­ç½®èˆ‡æ•¸æ“šæº–å‚™\n",
    "\n",
    "**å¯¦é©—ç›®æ¨™**: æº–å‚™ ORPO (Odds Ratio Preference Optimization) è¨“ç·´ç’°å¢ƒ\n",
    "\n",
    "**ORPO** æ˜¯æ¯” DPO æ›´å…ˆé€²çš„å°é½ŠæŠ€è¡“ï¼Œä¸»è¦å‰µæ–°é»ï¼š\n",
    "- **å–®éšæ®µè¨“ç·´**: ç„¡éœ€é å…ˆ SFTï¼Œç›´æ¥å¾åå¥½æ•¸æ“šé€²è¡Œå°é½Š\n",
    "- **çµ±ä¸€ç›®æ¨™**: åŒæ™‚å„ªåŒ– instruction following å’Œ preference alignment\n",
    "- **æ•ˆç‡æå‡**: è¨“ç·´æ™‚é–“å’Œè¨˜æ†¶é«”ä½¿ç”¨éƒ½å¤§å¹…æ¸›å°‘\n",
    "\n",
    "## ORPO vs DPO\n",
    "\n",
    "| ç‰¹æ€§ | DPO | ORPO |\n",
    "|------|-----|------|\n",
    "| è¨“ç·´éšæ®µ | 2 (SFT + DPO) | 1 (ORPO) |\n",
    "| åƒè€ƒæ¨¡å‹ | éœ€è¦ | ä¸éœ€è¦ |\n",
    "| è¨˜æ†¶é«”å ç”¨ | 2x | 1x |\n",
    "| è¨“ç·´æ™‚é–“ | 100% | 50-60% |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1: ç’°å¢ƒæª¢æŸ¥èˆ‡å®‰è£\n",
    "\n",
    "æª¢æŸ¥ ORPO æ‰€éœ€çš„ç’°å¢ƒå’Œå¥—ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "print(f'Python ç‰ˆæœ¬: {sys.version}')\n",
    "print(f'PyTorch ç‰ˆæœ¬: {torch.__version__}')\n",
    "print(f'CUDA å¯ç”¨: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU è¨­å‚™: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "    print(f'ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "# è¨­ç½®éš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print('\\nğŸš€ ORPO ç’°å¢ƒæª¢æŸ¥å®Œæˆ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ä¸¦å®‰è£å¿…è¦å¥—ä»¶\n",
    "def install_and_import(package, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = package\n",
    "    \n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f'âœ… {package} å·²å®‰è£')\n",
    "    except ImportError:\n",
    "        print(f'âš ï¸  å®‰è£ {package}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f'âœ… {package} å®‰è£å®Œæˆ')\n",
    "\n",
    "# æª¢æŸ¥å¿…è¦å¥—ä»¶\n",
    "required_packages = [\n",
    "    'transformers',\n",
    "    'peft', \n",
    "    'trl',\n",
    "    'datasets',\n",
    "    'bitsandbytes',\n",
    "    'accelerate',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn'\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    install_and_import(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥æ‰€éœ€åº«ä¸¦æª¢æŸ¥ç‰ˆæœ¬\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import datasets\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('=== å¥—ä»¶ç‰ˆæœ¬è³‡è¨Š ===')\n",
    "print(f'ğŸ¤— Transformers: {transformers.__version__}')\n",
    "print(f'ğŸ¯ PEFT: {peft.__version__}')\n",
    "print(f'ğŸš€ TRL: {trl.__version__}')\n",
    "print(f'ğŸ“Š Datasets: {datasets.__version__}')\n",
    "print(f'âš¡ Accelerate: {accelerate.__version__}')\n",
    "\n",
    "# æª¢æŸ¥ TRL æ˜¯å¦æ”¯æ´ ORPO\n",
    "try:\n",
    "    from trl import ORPOTrainer, ORPOConfig\n",
    "    print('âœ… TRL æ”¯æ´ ORPO')\n",
    "except ImportError:\n",
    "    print('âš ï¸  TRL ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æ´ ORPOï¼Œå°‡ä½¿ç”¨è‡ªå®šç¾©å¯¦ç¾')\n",
    "\n",
    "print('\\nâœ… ORPO ç’°å¢ƒæº–å‚™å®Œæˆï¼')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2: åå¥½æ•¸æ“šé›†æº–å‚™\n",
    "\n",
    "ORPO ç›´æ¥ä½¿ç”¨åå¥½æ•¸æ“šé€²è¡Œè¨“ç·´ï¼Œç„¡éœ€é¡å¤–çš„ instruction datasetã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# è¼‰å…¥åå¥½æ•¸æ“šé›†\n",
    "print('ğŸ“¥ è¼‰å…¥åå¥½æ•¸æ“šé›†...')\n",
    "\n",
    "try:\n",
    "    # è¼‰å…¥ Anthropic HH-RLHF æ•¸æ“šé›†\n",
    "    dataset = load_dataset('Anthropic/hh-rlhf', split='train[:500]')  # è¼‰å…¥è¼ƒå¤§æ¨£æœ¬\n",
    "    print(f'âœ… æˆåŠŸè¼‰å…¥ HH-RLHF æ•¸æ“šé›†ï¼Œæ¨£æœ¬æ•¸: {len(dataset)}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'âŒ HH-RLHF è¼‰å…¥å¤±æ•—: {e}')\n",
    "    print('ä½¿ç”¨å¢å¼·ç‰ˆæ¨¡æ“¬æ•¸æ“š...')\n",
    "    \n",
    "    # å‰µå»ºæ›´è±å¯Œçš„æ¨¡æ“¬åå¥½æ•¸æ“š\n",
    "    mock_data = {\n",
    "        'chosen': [\n",
    "            'Human: è«‹è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?\\n\\nAssistant: æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹é‡è¦åˆ†æ”¯ï¼Œå®ƒè®“è¨ˆç®—æ©Ÿèƒ½å¤ å¾æ•¸æ“šä¸­è‡ªå‹•å­¸ç¿’ä¸¦æ”¹é€²æ€§èƒ½ï¼Œè€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼š\\n\\n1. **å­¸ç¿’éç¨‹**: é€šéåˆ†æå¤§é‡æ•¸æ“šä¾†è­˜åˆ¥æ¨¡å¼å’Œè¦å¾‹\\n2. **ç®—æ³•é¡å‹**: åŒ…æ‹¬ç›£ç£å­¸ç¿’ã€ç„¡ç›£ç£å­¸ç¿’å’Œå¼·åŒ–å­¸ç¿’\\n3. **æ‡‰ç”¨é ˜åŸŸ**: å»£æ³›æ‡‰ç”¨æ–¼åœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†ã€æ¨è–¦ç³»çµ±ç­‰\\n\\næ©Ÿå™¨å­¸ç¿’çš„åƒ¹å€¼åœ¨æ–¼èƒ½å¤ è™•ç†è¤‡é›œçš„æ•¸æ“šæ¨¡å¼ï¼Œç‚ºæ±ºç­–æä¾›æ™ºèƒ½æ”¯æŒã€‚',\n",
    "            \n",
    "            'Human: å¦‚ä½•é–‹å§‹å­¸ç¿’ç¨‹å¼è¨­è¨ˆ?\\n\\nAssistant: å­¸ç¿’ç¨‹å¼è¨­è¨ˆæ˜¯ä¸€å€‹å¾ªåºæ¼¸é€²çš„éç¨‹ï¼Œå»ºè­°æŒ‰ä»¥ä¸‹æ­¥é©Ÿé€²è¡Œï¼š\\n\\n**åŸºç¤éšæ®µ**ï¼š\\n1. é¸æ“‡é©åˆçš„å…¥é–€èªè¨€ï¼ˆå¦‚ Pythonï¼‰\\n2. å­¸ç¿’åŸºæœ¬èªæ³•å’Œæ¦‚å¿µ\\n3. ç·´ç¿’ç°¡å–®çš„é‚è¼¯å•é¡Œ\\n\\n**é€²éšéšæ®µ**ï¼š\\n1. å­¸ç¿’æ•¸æ“šçµæ§‹å’Œç®—æ³•\\n2. åšå¯¦éš›é …ç›®ç·´ç¿’\\n3. é–±è®€ä»–äººä»£ç¢¼\\n\\n**æŒçºŒæˆé•·**ï¼š\\n1. åƒèˆ‡é–‹æºå°ˆæ¡ˆ\\n2. å­¸ç¿’è»Ÿé«”å·¥ç¨‹å¯¦è¸\\n3. ä¿æŒå­¸ç¿’æ–°æŠ€è¡“çš„ç¿’æ…£\\n\\nè¨˜ä½ï¼Œç¨‹å¼è¨­è¨ˆé‡åœ¨å¯¦ä½œï¼Œå¤šå¯«å¤šç·´æ˜¯é—œéµã€‚',\n",
    "            \n",
    "            'Human: å¥åº·é£²é£Ÿçš„åŸºæœ¬åŸå‰‡æ˜¯ä»€éº¼?\\n\\nAssistant: å¥åº·é£²é£Ÿæ‡‰éµå¾ªä»¥ä¸‹ç§‘å­¸åŸå‰‡ï¼š\\n\\n**ç‡Ÿé¤Šå‡è¡¡**ï¼š\\n- ç¢³æ°´åŒ–åˆç‰©ï¼šæä¾›èƒ½é‡ï¼Œé¸æ“‡å…¨ç©€é¡\\n- è›‹ç™½è³ªï¼šç¶­æŒè‚Œè‚‰ï¼Œé¸æ“‡å„ªè³ªä¾†æº\\n- è„‚è‚ªï¼šé¸æ“‡ä¸é£½å’Œè„‚è‚ªï¼Œé¿å…åå¼è„‚è‚ª\\n- ç¶­ç”Ÿç´ ç¤¦ç‰©è³ªï¼šå¤šåƒè”¬æœè£œå……\\n\\n**é£²é£Ÿç¿’æ…£**ï¼š\\n- è¦å¾‹ç”¨é¤ï¼Œé¿å…æš´é£²æš´é£Ÿ\\n- æ§åˆ¶ä»½é‡ï¼Œé©é‡æ”å–\\n- å¤šå–æ°´ï¼Œå°‘å–å«ç³–é£²æ–™\\n- æ¸›å°‘åŠ å·¥é£Ÿå“\\n\\n**å€‹äººåŒ–èª¿æ•´**ï¼š\\næ ¹æ“šå¹´é½¡ã€æ€§åˆ¥ã€æ´»å‹•é‡èª¿æ•´é£²é£Ÿè¨ˆåŠƒã€‚å»ºè­°è«®è©¢ç‡Ÿé¤Šå¸«åˆ¶å®šå€‹äººåŒ–æ–¹æ¡ˆã€‚',\n",
    "            \n",
    "            'Human: å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡?\\n\\nAssistant: æé«˜å·¥ä½œæ•ˆç‡éœ€è¦ç³»çµ±æ€§æ–¹æ³•ï¼š\\n\\n**æ™‚é–“ç®¡ç†**ï¼š\\n1. ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•ï¼š25åˆ†é˜å°ˆæ³¨ + 5åˆ†é˜ä¼‘æ¯\\n2. å„ªå…ˆç´šæ’åºï¼šå…ˆåšé‡è¦ä¸”ç·Šæ€¥çš„ä»»å‹™\\n3. æ™‚é–“å¡Šè¦åŠƒï¼šç‚ºä¸åŒä»»å‹™åˆ†é…å›ºå®šæ™‚é–“\\n\\n**ç’°å¢ƒå„ªåŒ–**ï¼š\\n1. æ•´ç†å·¥ä½œç©ºé–“ï¼Œæ¸›å°‘å¹²æ“¾\\n2. ä½¿ç”¨é«˜æ•ˆå·¥å…·å’Œè»Ÿé«”\\n3. å»ºç«‹æ¨™æº–åŒ–æµç¨‹\\n\\n**å€‹äººç¿’æ…£**ï¼š\\n1. ä¿æŒå¥åº·ä½œæ¯\\n2. å®šæœŸä¼‘æ¯å’Œé‹å‹•\\n3. æŒçºŒå­¸ç¿’æ–°æŠ€èƒ½\\n\\n**åœ˜éšŠå”ä½œ**ï¼š\\n1. æ¸…æ™°æºé€šï¼Œæ¸›å°‘èª¤è§£\\n2. æœ‰æ•ˆæˆæ¬Šå’Œåˆ†å·¥\\n3. å®šæœŸå›é¡§å’Œæ”¹é€²\\n\\né—œéµæ˜¯æ‰¾åˆ°é©åˆè‡ªå·±çš„æ–¹æ³•ä¸¦æŒçºŒåŸ·è¡Œã€‚',\n",
    "            \n",
    "            'Human: åˆ†æäººå·¥æ™ºèƒ½å°æœªä¾†å°±æ¥­çš„å½±éŸ¿\\n\\nAssistant: äººå·¥æ™ºèƒ½å°å°±æ¥­å¸‚å ´çš„å½±éŸ¿æ˜¯è¤‡é›œä¸”å¤šé¢å‘çš„ï¼š\\n\\n**æ½›åœ¨æŒ‘æˆ°**ï¼š\\n1. **è‡ªå‹•åŒ–æ›¿ä»£**ï¼šé‡è¤‡æ€§å·¥ä½œé¢è‡¨è¢«å–ä»£é¢¨éšª\\n2. **æŠ€èƒ½é´»æº**ï¼šéœ€è¦æ–°æŠ€èƒ½ä»¥é©æ‡‰è®ŠåŒ–\\n3. **éæ¸¡æœŸç—›è‹¦**ï¼šçŸ­æœŸå…§å¯èƒ½é€ æˆå¤±æ¥­\\n\\n**æ–°æ©Ÿé‡**ï¼š\\n1. **æ–°è·æ¥­å‰µé€ **ï¼šAIå·¥ç¨‹å¸«ã€æ•¸æ“šç§‘å­¸å®¶ç­‰\\n2. **å·¥ä½œå¢å¼·**ï¼šAIä½œç‚ºå·¥å…·æå‡äººé¡èƒ½åŠ›\\n3. **å‰µæ„é ˜åŸŸæ“´å±•**ï¼šæ›´å¤šæ™‚é–“å°ˆæ³¨å‰µæ–°å·¥ä½œ\\n\\n**æ‡‰å°ç­–ç•¥**ï¼š\\n1. **çµ‚èº«å­¸ç¿’**ï¼šæŒçºŒæ›´æ–°æŠ€èƒ½\\n2. **è·¨é ˜åŸŸèƒ½åŠ›**ï¼šåŸ¹é¤ŠAIç„¡æ³•æ›¿ä»£çš„è»ŸæŠ€èƒ½\\n3. **æ”¿ç­–æ”¯æŒ**ï¼šæ”¿åºœéœ€åˆ¶å®šè½‰å‹æ”¿ç­–\\n\\nç¸½é«”è€Œè¨€ï¼ŒAIæ›´å¯èƒ½æ˜¯å·¥ä½œè½‰å‹è€Œéå®Œå…¨æ›¿ä»£ï¼Œé—œéµåœ¨æ–¼ä¸»å‹•é©æ‡‰ã€‚'\n",
    "        ],\n",
    "        'rejected': [\n",
    "            'Human: è«‹è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?\\n\\nAssistant: æ©Ÿå™¨å­¸ç¿’å°±æ˜¯è®“æ©Ÿå™¨è®Šè°æ˜ï¼Œèƒ½å¤ è‡ªå·±å­¸æ±è¥¿ã€‚',\n",
    "            \n",
    "            'Human: å¦‚ä½•é–‹å§‹å­¸ç¿’ç¨‹å¼è¨­è¨ˆ?\\n\\nAssistant: å­¸ç¨‹å¼å°±æ˜¯å¯«codeï¼Œè²·æœ¬æ›¸çœ‹çœ‹å°±æœƒäº†ã€‚',\n",
    "            \n",
    "            'Human: å¥åº·é£²é£Ÿçš„åŸºæœ¬åŸå‰‡æ˜¯ä»€éº¼?\\n\\nAssistant: å¥åº·é£²é£Ÿå°±æ˜¯å°‘åƒå¤šå‹•ï¼Œä¸è¦åƒåƒåœ¾é£Ÿå“ã€‚',\n",
    "            \n",
    "            'Human: å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡?\\n\\nAssistant: å·¥ä½œæ•ˆç‡å°±æ˜¯è¦å¿«ä¸€é»ï¼Œå¤šåšä¸€äº›äº‹æƒ…ã€‚',\n",
    "            \n",
    "            'Human: åˆ†æäººå·¥æ™ºèƒ½å°æœªä¾†å°±æ¥­çš„å½±éŸ¿\\n\\nAssistant: AIæœƒæ¶èµ°å¾ˆå¤šå·¥ä½œï¼Œå¤§å®¶è¦å°å¿ƒã€‚'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(mock_data)\n",
    "    print(f'âœ… å‰µå»ºæ¨¡æ“¬æ•¸æ“šé›†ï¼Œæ¨£æœ¬æ•¸: {len(dataset)}')\n",
    "\n",
    "print(f'\\næ•¸æ“šé›†æ¬„ä½: {list(dataset.features.keys())}')\n",
    "print(f'æ•¸æ“šé›†å¤§å°: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¢ç´¢æ•¸æ“šé›†å…§å®¹\n",
    "print('=== åå¥½æ•¸æ“šæ¨£æœ¬æ¢ç´¢ ===')\n",
    "\n",
    "for i in range(min(2, len(dataset))):\n",
    "    sample = dataset[i]\n",
    "    print(f'\\nğŸ“ æ¨£æœ¬ {i+1}:')\n",
    "    \n",
    "    if 'chosen' in sample:\n",
    "        print('âœ… åå¥½å›æ‡‰ (chosen):')\n",
    "        chosen_text = sample['chosen']\n",
    "        print(chosen_text[:300] + '...' if len(chosen_text) > 300 else chosen_text)\n",
    "        print()\n",
    "        \n",
    "    if 'rejected' in sample:\n",
    "        print('âŒ éåå¥½å›æ‡‰ (rejected):')\n",
    "        rejected_text = sample['rejected']\n",
    "        print(rejected_text[:300] + '...' if len(rejected_text) > 300 else rejected_text)\n",
    "        print()\n",
    "    \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3: æ•¸æ“šé è™•ç†èˆ‡æ ¼å¼åŒ–\n",
    "\n",
    "å°‡æ•¸æ“šè½‰æ›ç‚º ORPO è¨“ç·´æ‰€éœ€çš„æ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_and_response(text):\n",
    "    \"\"\"å¾å°è©±æ–‡æœ¬ä¸­æå–æç¤ºå’Œå›æ‡‰\"\"\"\n",
    "    if 'Human:' in text and 'Assistant:' in text:\n",
    "        parts = text.split('Assistant:')\n",
    "        if len(parts) >= 2:\n",
    "            prompt = parts[0].replace('Human:', '').strip()\n",
    "            response = parts[1].strip()\n",
    "            return prompt, response\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def preprocess_orpo_dataset(dataset):\n",
    "    \"\"\"é è™•ç†æ•¸æ“šé›†ç‚º ORPO æ ¼å¼\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # æå– chosen å’Œ rejected çš„ prompt å’Œ response\n",
    "        chosen_prompt, chosen_response = extract_prompt_and_response(sample['chosen'])\n",
    "        rejected_prompt, rejected_response = extract_prompt_and_response(sample['rejected'])\n",
    "        \n",
    "        # ç¢ºä¿å…©å€‹å›æ‡‰ä½¿ç”¨ç›¸åŒçš„ prompt\n",
    "        if chosen_prompt and chosen_response and rejected_response:\n",
    "            # ORPO æ ¼å¼: prompt, chosen, rejected\n",
    "            processed_data.append({\n",
    "                'prompt': chosen_prompt,\n",
    "                'chosen': chosen_response,\n",
    "                'rejected': rejected_response\n",
    "            })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# é è™•ç†æ•¸æ“š\n",
    "print('ğŸ”„ é è™•ç† ORPO æ•¸æ“š...')\n",
    "processed_data = preprocess_orpo_dataset(dataset)\n",
    "\n",
    "print(f'âœ… è™•ç†å®Œæˆï¼Œæœ‰æ•ˆæ¨£æœ¬æ•¸: {len(processed_data)}')\n",
    "\n",
    "# é¡¯ç¤ºè™•ç†å¾Œçš„æ¨£æœ¬\n",
    "if processed_data:\n",
    "    print('\\n=== è™•ç†å¾Œæ¨£æœ¬ç¤ºä¾‹ ===')\n",
    "    sample = processed_data[0]\n",
    "    print(f'ğŸ“ Prompt: {sample[\"prompt\"]}\\n')\n",
    "    print(f'âœ… Chosen: {sample[\"chosen\"][:200]}...')\n",
    "    print(f'âŒ Rejected: {sample[\"rejected\"][:200]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4: ORPO æå¤±å‡½æ•¸é è¦½\n",
    "\n",
    "å¯¦ç¾ä¸¦æ¸¬è©¦ ORPO æ ¸å¿ƒæå¤±å‡½æ•¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_log_probs(model, input_ids, attention_mask, labels, tokenizer):\n",
    "    \"\"\"è¨ˆç®—åºåˆ—çš„å°æ•¸æ¦‚ç‡\"\"\"\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹ token çš„å°æ•¸æ¦‚ç‡\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # æ”¶é›†ç›®æ¨™ token çš„å°æ•¸æ¦‚ç‡\n",
    "    target_log_probs = torch.gather(log_probs[:, :-1], dim=-1, \n",
    "                                   index=labels[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # åƒ…è¨ˆç®—é padding token çš„å¹³å‡å°æ•¸æ¦‚ç‡\n",
    "    mask = (labels[:, 1:] != tokenizer.pad_token_id).float()\n",
    "    sequence_log_prob = (target_log_probs * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "    \n",
    "    return sequence_log_prob\n",
    "\n",
    "\n",
    "def compute_odds_ratio_loss(chosen_log_probs, rejected_log_probs):\n",
    "    \"\"\"è¨ˆç®— ORPO çš„ Odds Ratio æå¤±\"\"\"\n",
    "    # è¨ˆç®— log odds\n",
    "    chosen_log_odds = chosen_log_probs - torch.log1p(-torch.exp(chosen_log_probs.clamp(max=0)))\n",
    "    rejected_log_odds = rejected_log_probs - torch.log1p(-torch.exp(rejected_log_probs.clamp(max=0)))\n",
    "    \n",
    "    # Odds Ratio æå¤±\n",
    "    log_odds_ratio = chosen_log_odds - rejected_log_odds\n",
    "    loss = -F.logsigmoid(log_odds_ratio).mean()\n",
    "    \n",
    "    return loss, log_odds_ratio.mean()\n",
    "\n",
    "\n",
    "def orpo_loss(model, batch, tokenizer, lambda_or=0.5):\n",
    "    \"\"\"\n",
    "    ORPO æå¤±å‡½æ•¸\n",
    "    \n",
    "    Args:\n",
    "        model: è¨“ç·´ä¸­çš„æ¨¡å‹\n",
    "        batch: åŒ…å« prompt, chosen, rejected çš„æ‰¹æ¬¡\n",
    "        tokenizer: tokenizer\n",
    "        lambda_or: Odds Ratio æå¤±æ¬Šé‡\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, metrics\n",
    "    \"\"\"\n",
    "    # 1. SFT æå¤± (æ¨™æº–èªè¨€æ¨¡å‹æå¤±ï¼Œåƒ…åœ¨ chosen ä¸Š)\n",
    "    chosen_input_ids = batch['chosen_input_ids']\n",
    "    chosen_attention_mask = batch['chosen_attention_mask']\n",
    "    \n",
    "    outputs = model(input_ids=chosen_input_ids, attention_mask=chosen_attention_mask, \n",
    "                   labels=chosen_input_ids)\n",
    "    sft_loss = outputs.loss\n",
    "    \n",
    "    # 2. Odds Ratio æå¤±\n",
    "    chosen_log_probs = compute_log_probs(model, batch['chosen_input_ids'], \n",
    "                                        batch['chosen_attention_mask'], \n",
    "                                        batch['chosen_input_ids'], tokenizer)\n",
    "    \n",
    "    rejected_log_probs = compute_log_probs(model, batch['rejected_input_ids'],\n",
    "                                          batch['rejected_attention_mask'],\n",
    "                                          batch['rejected_input_ids'], tokenizer)\n",
    "    \n",
    "    or_loss, log_odds_ratio = compute_odds_ratio_loss(chosen_log_probs, rejected_log_probs)\n",
    "    \n",
    "    # 3. ç¸½æå¤±\n",
    "    total_loss = sft_loss + lambda_or * or_loss\n",
    "    \n",
    "    metrics = {\n",
    "        'total_loss': total_loss.item(),\n",
    "        'sft_loss': sft_loss.item(),\n",
    "        'or_loss': or_loss.item(),\n",
    "        'log_odds_ratio': log_odds_ratio.item(),\n",
    "        'lambda_or': lambda_or\n",
    "    }\n",
    "    \n",
    "    return total_loss, metrics\n",
    "\n",
    "\n",
    "print('âœ… ORPO æå¤±å‡½æ•¸å¯¦ç¾å®Œæˆ')\n",
    "print('\\nğŸ”¬ ORPO æ ¸å¿ƒæ¦‚å¿µ:')\n",
    "print('â€¢ SFT Loss: ç¢ºä¿æ¨¡å‹åŸºæœ¬çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›')\n",
    "print('â€¢ Odds Ratio Loss: å„ªåŒ–åå¥½å°é½Šï¼Œæå‡ chosen ç›¸å° rejected çš„æ©Ÿç‡')\n",
    "print('â€¢ Lambda å¹³è¡¡: æ§åˆ¶å…©å€‹æå¤±çš„ç›¸å°é‡è¦æ€§')\n",
    "print('â€¢ å–®éšæ®µè¨“ç·´: åŒæ™‚å„ªåŒ–å…©å€‹ç›®æ¨™ï¼Œç„¡éœ€åˆ†éšæ®µ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5: å¯¦é©—è¨­ç½®ç¸½çµ\n",
    "\n",
    "ç¸½çµ ORPO ç’°å¢ƒè¨­ç½®å®Œæˆæƒ…æ³ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦é©—è¨­ç½®ç¸½çµ\n",
    "print('=== ORPO ç’°å¢ƒè¨­ç½®ç¸½çµ ===')\n",
    "print(f'âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}')\n",
    "print(f'âœ… GPU å¯ç”¨: {torch.cuda.is_available()}')\n",
    "print(f'âœ… æ•¸æ“šæ¨£æœ¬: {len(processed_data) if processed_data else 0}')\n",
    "print(f'âœ… ORPO æå¤±å‡½æ•¸: å·²å¯¦ç¾')\n",
    "\n",
    "if processed_data:\n",
    "    # ä¿å­˜æ•¸æ“šé›†\n",
    "    output_dir = './orpo_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # è½‰æ›ç‚º Dataset ä¸¦ä¿å­˜\n",
    "    orpo_dataset = Dataset.from_list(processed_data)\n",
    "    orpo_dataset.save_to_disk(output_dir)\n",
    "    \n",
    "    print(f'ğŸ’¾ æ•¸æ“šé›†å·²ä¿å­˜è‡³: {output_dir}')\n",
    "\n",
    "print('\\nğŸš€ ORPO ç’°å¢ƒè¨­ç½®å®Œæˆï¼')\n",
    "print('ä¸‹ä¸€æ­¥: åŸ·è¡Œ 02-ORPO_Training.ipynb é–‹å§‹å–®éšæ®µå°é½Šè¨“ç·´')\n",
    "\n",
    "print('\\nğŸ”¬ ORPO æŠ€è¡“å„ªå‹¢:')\n",
    "print('â€¢ å–®éšæ®µè¨“ç·´ï¼šç„¡éœ€ SFT + DPO å…©éšæ®µ')\n",
    "print('â€¢ è¨˜æ†¶é«”é«˜æ•ˆï¼šç„¡éœ€åƒè€ƒæ¨¡å‹ï¼Œè¨˜æ†¶é«”ä½¿ç”¨æ¸›åŠ')\n",
    "print('â€¢ è¨“ç·´ç°¡åŒ–ï¼šçµ±ä¸€æå¤±å‡½æ•¸ï¼Œè¶…åƒæ•¸èª¿å„ªæ›´å®¹æ˜“')\n",
    "print('â€¢ æ•ˆæœæ›´ä½³ï¼šç†è«–ä¸Šæ¯” DPO æœ‰æ›´å¥½çš„å°é½Šæ•ˆæœ')\n",
    "\n",
    "# æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\nğŸ’¾ GPU è¨˜æ†¶é«”å·²æ¸…ç†ï¼Œç•¶å‰ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f} GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}