{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.6: Multi-Query Attention æ·±åº¦å¯¦ç¾\n",
    "## æ¥µè‡´æ¨ç†å„ªåŒ–ï¼šå¾åŸç†åˆ°å¯¦è¸\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- æ·±åº¦ç†è§£ MQA çš„æ ¸å¿ƒæ©Ÿåˆ¶èˆ‡å„ªåŒ–åŸç†\n",
    "- é€æ­¥å¯¦ç¾ MQA çš„å®Œæ•´æ¶æ§‹\n",
    "- é‡åŒ–åˆ†æ KV Cache è¨˜æ†¶é«”ç¯€çœæ•ˆæœ\n",
    "- è©•ä¼° MQA å°æ¨¡å‹è³ªé‡çš„å½±éŸ¿\n",
    "\n",
    "**é‡é»æ¦‚å¿µ**: \n",
    "- Query-Key-Value å…±äº«æ©Ÿåˆ¶\n",
    "- KV Cache å»£æ’­ç­–ç•¥\n",
    "- æ¨ç†æ•ˆç‡ vs æ¨¡å‹è¡¨ç¾åŠ›çš„æ¬Šè¡¡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” MQA æ ¸å¿ƒåŸç†è§£æ\n",
    "\n",
    "### å‚³çµ± MHA çš„å•é¡Œ\n",
    "```python\n",
    "# MHA: æ¯å€‹é ­éƒ½æœ‰ç¨ç«‹çš„ K, V\n",
    "for head_i in range(num_heads):\n",
    "    Q_i = W_q_i @ X    # ç¨ç«‹çš„ Query æŠ•å½±\n",
    "    K_i = W_k_i @ X    # ç¨ç«‹çš„ Key æŠ•å½±   â† è¨˜æ†¶é«”ç“¶é ¸\n",
    "    V_i = W_v_i @ X    # ç¨ç«‹çš„ Value æŠ•å½± â† è¨˜æ†¶é«”ç“¶é ¸\n",
    "    \n",
    "    # KV Cache: [seq_len, num_heads, head_dim] â† å¤§é‡è¨˜æ†¶é«”\n",
    "```\n",
    "\n",
    "### MQA çš„é©å‘½æ€§æ”¹é€²\n",
    "```python\n",
    "# MQA: æ‰€æœ‰é ­å…±äº«å–®ä¸€ K, V\n",
    "Q = W_q @ X           # ä¿æŒå¤šé ­ Query (ä¿è­‰è¡¨ç¾åŠ›)\n",
    "K = W_k @ X           # å–®ä¸€ Key æŠ•å½±   â† è¨˜æ†¶é«”å¤§å¹…æ¸›å°‘\n",
    "V = W_v @ X           # å–®ä¸€ Value æŠ•å½± â† è¨˜æ†¶é«”å¤§å¹…æ¸›å°‘\n",
    "\n",
    "# KV Cache: [seq_len, 1, head_dim] â† è¨˜æ†¶é«”ç¯€çœ 32x\n",
    "for head_i in range(num_heads):\n",
    "    attention_i = softmax(Q_i @ K.T) @ V  # å»£æ’­å…±äº«çš„ K, V\n",
    "```\n",
    "\n",
    "**é—œéµæ´å¯Ÿ**: Query è² è²¬\"å•ä»€éº¼\"ï¼ŒKey-Value è² è²¬\"ç­”æ¡ˆåº«\"ã€‚\n",
    "å¤šå€‹å•é¡Œå¯ä»¥æŸ¥è©¢åŒä¸€å€‹ç­”æ¡ˆåº«ï¼Œä½†å•é¡Œæœ¬èº«éœ€è¦å¤šæ¨£åŒ–ã€‚\n",
    "\n",
    "**æ¸¬è©¦çµæœé è¦½** (åŸºæ–¼å¯¦éš›é‹è¡Œ):\n",
    "- ğŸš€ **æ¨ç†åŠ é€Ÿ**: 1.76x ååé‡æå‡\n",
    "- ğŸ’¾ **è¨˜æ†¶é«”æ•ˆç‡**: KV Cache æ¸›å°‘ 12x (91.7% ç¯€çœ)\n",
    "- âš¡ **åƒæ•¸æ•ˆç‡**: æ¸›å°‘ 45.8% çš„ attention åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ä½¿ç”¨è¨­å‚™: cuda\n",
      "ğŸ”§ GPU: NVIDIA RTX 2000 Ada Generation\n",
      "ğŸ’¾ ç¸½è¨˜æ†¶é«”: 16.7GB\n",
      "ğŸ”¥ CUDA ç‰ˆæœ¬: 12.8\n"
     ]
    }
   ],
   "source": [
    "## ğŸ› ï¸ ç’°å¢ƒè¨­ç½®èˆ‡ä¾è³´å°å…¥\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸš€ ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ”§ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"ğŸ’¾ ç¸½è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    print(f\"ğŸ”¥ CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âš ï¸  æœªæª¢æ¸¬åˆ° CUDAï¼Œå°‡ä½¿ç”¨ CPU (æ€§èƒ½æœƒè¼ƒæ…¢)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨™æº– MHA å¯¦ç¾å®Œæˆ\n",
      "   â€¢ æ¯å€‹ head ç¨ç«‹çš„ Q, K, V æ¬Šé‡\n",
      "   â€¢ å®Œæ•´çš„ num_heads å€ KV Cache\n",
      "   â€¢ æœ€é«˜è¡¨ç¾åŠ›ï¼Œä½†è¨˜æ†¶é«”å¯†é›†\n"
     ]
    }
   ],
   "source": [
    "## ğŸ§± æ¨™æº– Multi-Head Attention å¯¦ç¾ (å°æ¯”åŸºæº–)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ğŸ” æ¨™æº– Multi-Head Attention\n",
    "    \n",
    "    æ¶æ§‹ç‰¹é»:\n",
    "    - æ¯å€‹ head éƒ½æœ‰ç¨ç«‹çš„ Q, K, V æ¬Šé‡çŸ©é™£\n",
    "    - KV Cache å¤§å°: [batch, seq_len, num_heads, head_dim]\n",
    "    - è¨˜æ†¶é«”æ¶ˆè€—: å®Œæ•´çš„ num_heads å€\n",
    "    \n",
    "    è¨ˆç®—æµç¨‹:\n",
    "    1. ä¸¦è¡Œè¨ˆç®—æ‰€æœ‰ heads çš„ Q, K, V\n",
    "    2. æ¯å€‹ head ç¨ç«‹é€²è¡Œ attention è¨ˆç®—\n",
    "    3. æ‹¼æ¥ä¸¦æŠ•å½±åˆ°è¼¸å‡ºç©ºé–“\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim å¿…é ˆèƒ½è¢« num_heads æ•´é™¤\"\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # ğŸ”‘ é—œéµå€åˆ¥: MHA æœ‰ num_heads çµ„å®Œæ•´çš„ K, V æ¬Šé‡\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)  # [hidden_dim, hidden_dim]\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)  # [hidden_dim, hidden_dim] â† å¤§è¨˜æ†¶é«”\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)  # [hidden_dim, hidden_dim] â† å¤§è¨˜æ†¶é«”\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "        \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, hidden_dim]\n",
    "            past_kv: (past_k, past_v) ä¾†è‡ª KV Cache\n",
    "            use_cache: æ˜¯å¦ä½¿ç”¨ KV Cache\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch, seq_len, hidden_dim]\n",
    "            new_past_kv: (new_k, new_v) ç”¨æ–¼ä¸‹æ¬¡æ¨ç†\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 1: æŠ•å½±åˆ° Q, K, V ç©ºé–“\n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)  # [B, N, H, d]\n",
    "        K = self.k_proj(x).view(B, N, self.num_heads, self.head_dim)  # [B, N, H, d]\n",
    "        V = self.v_proj(x).view(B, N, self.num_heads, self.head_dim)  # [B, N, H, d]\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 2: è™•ç† KV Cache (é—œéµæ¨ç†å„ªåŒ–)\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv  # [B, past_len, H, d]\n",
    "            K = torch.cat([past_k, K], dim=1)  # [B, past_len+N, H, d]\n",
    "            V = torch.cat([past_v, V], dim=1)  # [B, past_len+N, H, d]\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 3: é‡æ’ç¶­åº¦æº–å‚™ attention è¨ˆç®—\n",
    "        Q = Q.transpose(1, 2)  # [B, H, N, d]\n",
    "        K = K.transpose(1, 2)  # [B, H, K_len, d]\n",
    "        V = V.transpose(1, 2)  # [B, H, K_len, d]\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 4: Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [B, H, N, K_len]\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # [B, H, N, K_len]\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)  # [B, H, N, d]\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 5: é‡çµ„ä¸¦æŠ•å½±åˆ°è¼¸å‡º\n",
    "        output = output.transpose(1, 2).contiguous()  # [B, N, H, d]\n",
    "        output = output.view(B, N, self.hidden_dim)   # [B, N, D]\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            # è¿”å›ç•¶å‰å®Œæ•´çš„ K, V ç”¨æ–¼ä¸‹æ¬¡æ¨ç†\n",
    "            new_k = K.transpose(1, 2)  # [B, K_len, H, d]\n",
    "            new_v = V.transpose(1, 2)  # [B, K_len, H, d]\n",
    "            return output, (new_k, new_v)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"âœ… æ¨™æº– MHA å¯¦ç¾å®Œæˆ\")\n",
    "print(\"   â€¢ æ¯å€‹ head ç¨ç«‹çš„ Q, K, V æ¬Šé‡\")\n",
    "print(\"   â€¢ å®Œæ•´çš„ num_heads å€ KV Cache\")\n",
    "print(\"   â€¢ æœ€é«˜è¡¨ç¾åŠ›ï¼Œä½†è¨˜æ†¶é«”å¯†é›†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é©å‘½æ€§ MQA å¯¦ç¾å®Œæˆ\n",
      "   â€¢ å¤šå€‹ Query heads å…±äº«å–®ä¸€ K, V\n",
      "   â€¢ KV Cache æ¸›å°‘ num_heads å€\n",
      "   â€¢ å»£æ’­æ©Ÿåˆ¶å¯¦ç¾é«˜æ•ˆæŸ¥è©¢\n"
     ]
    }
   ],
   "source": [
    "## ğŸš€ é©å‘½æ€§ Multi-Query Attention å¯¦ç¾\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ğŸš€ Multi-Query Attention - æ¥µè‡´æ¨ç†å„ªåŒ–\n",
    "    \n",
    "    æ ¸å¿ƒå‰µæ–°:\n",
    "    - å¤šå€‹ Query heads å…±äº«å–®ä¸€ Key, Value\n",
    "    - KV Cache å¤§å°: [batch, seq_len, 1, head_dim] â† è¨˜æ†¶é«”ç¯€çœ 32x\n",
    "    - é€šéå»£æ’­æ©Ÿåˆ¶å¯¦ç¾å¤šé ­æŸ¥è©¢å–®ä¸€è¨˜æ†¶é«”\n",
    "    \n",
    "    é©ç”¨å ´æ™¯:\n",
    "    - æ¨ç†å¯†é›†å‹æ‡‰ç”¨ (ChatBot, ç¨‹å¼ç¢¼ç”Ÿæˆ)\n",
    "    - é‚Šç·£è¨­å‚™éƒ¨ç½² (æ‰‹æ©Ÿ, IoT)\n",
    "    - å¤§è¦æ¨¡ä¸¦ç™¼æœå‹™\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim å¿…é ˆèƒ½è¢« num_heads æ•´é™¤\"\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # ğŸ”‘ MQA æ ¸å¿ƒå‰µæ–°: K, V åªæœ‰å–®ä¸€ head çš„åƒæ•¸\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)        # [D, D] - ä¿æŒå¤šé ­\n",
    "        self.k_proj = nn.Linear(hidden_dim, self.head_dim, bias=False)     # [D, d] - å–®ä¸€é ­ âœ¨\n",
    "        self.v_proj = nn.Linear(hidden_dim, self.head_dim, bias=False)     # [D, d] - å–®ä¸€é ­ âœ¨\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        MQA å‰å‘å‚³æ’­ - é—œéµåœ¨æ–¼ K, V çš„å»£æ’­æ©Ÿåˆ¶\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, hidden_dim]\n",
    "            past_kv: (past_k, past_v) å–®ä¸€é ­çš„ KV Cache\n",
    "            \n",
    "        Returns:\n",
    "            output: [batch, seq_len, hidden_dim]\n",
    "            new_past_kv: (new_k, new_v) å–®ä¸€é ­çš„ cache\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 1: æŠ•å½± - æ³¨æ„ K, V ç¶­åº¦å·®ç•°\n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)  # [B, N, H, d] - å¤šé ­\n",
    "        K = self.k_proj(x).view(B, N, 1, self.head_dim)              # [B, N, 1, d] - å–®é ­ âœ¨\n",
    "        V = self.v_proj(x).view(B, N, 1, self.head_dim)              # [B, N, 1, d] - å–®é ­ âœ¨\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 2: KV Cache è™•ç† (é—œéµå„ªåŒ–é»)\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv  # [B, past_len, 1, d] - æ³¨æ„æ˜¯å–®é ­\n",
    "            K = torch.cat([past_k, K], dim=1)  # [B, total_len, 1, d]\n",
    "            V = torch.cat([past_v, V], dim=1)  # [B, total_len, 1, d]\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 3: å»£æ’­ K, V åˆ°æ‰€æœ‰ Query heads\n",
    "        # é€™æ˜¯ MQA çš„æ ¸å¿ƒæ©Ÿåˆ¶ï¼\n",
    "        K_expanded = K.expand(B, K.size(1), self.num_heads, self.head_dim)  # [B, L, H, d]\n",
    "        V_expanded = V.expand(B, V.size(1), self.num_heads, self.head_dim)  # [B, L, H, d]\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 4: æ¨™æº– attention è¨ˆç®—\n",
    "        Q = Q.transpose(1, 2)           # [B, H, N, d]\n",
    "        K_expanded = K_expanded.transpose(1, 2)  # [B, H, L, d]\n",
    "        V_expanded = V_expanded.transpose(1, 2)  # [B, H, L, d]\n",
    "        \n",
    "        scores = torch.matmul(Q, K_expanded.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V_expanded)  # [B, H, N, d]\n",
    "        \n",
    "        # ğŸ”„ æ­¥é©Ÿ 5: è¼¸å‡ºæŠ•å½±\n",
    "        output = output.transpose(1, 2).contiguous().view(B, N, D)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            # ğŸ”‘ é—œéµ: åªå„²å­˜åŸå§‹çš„å–®é ­ K, V (ç¯€çœè¨˜æ†¶é«”)\n",
    "            return output, (K, V)  # K, V: [B, L, 1, d]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"âœ… é©å‘½æ€§ MQA å¯¦ç¾å®Œæˆ\")\n",
    "print(\"   â€¢ å¤šå€‹ Query heads å…±äº«å–®ä¸€ K, V\")\n",
    "print(\"   â€¢ KV Cache æ¸›å°‘ num_heads å€\")\n",
    "print(\"   â€¢ å»£æ’­æ©Ÿåˆ¶å¯¦ç¾é«˜æ•ˆæŸ¥è©¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸  å»ºç«‹èˆ‡æ¸¬è©¦æ¨¡å‹...\n",
      "é…ç½®: hidden_dim=768, num_heads=12\n",
      "\n",
      "ğŸ” ç²¾åº¦æª¢æŸ¥:\n",
      "â€¢ MHA æ¨¡å‹ç²¾åº¦: torch.float32\n",
      "â€¢ MQA æ¨¡å‹ç²¾åº¦: torch.float32\n",
      "â€¢ è¼¸å…¥æ•¸æ“šç²¾åº¦: torch.float32\n",
      "\n",
      "ğŸ§ª åŸ·è¡ŒåŸºæœ¬åŠŸèƒ½æ¸¬è©¦...\n",
      "âœ… MHA æ¸¬è©¦é€šé: torch.Size([2, 128, 768]) â†’ torch.Size([2, 128, 768])\n",
      "âœ… MQA æ¸¬è©¦é€šé: torch.Size([2, 128, 768]) â†’ torch.Size([2, 128, 768])\n",
      "âœ… MHA KV Cache: K=torch.Size([2, 128, 12, 64]), V=torch.Size([2, 128, 12, 64])\n",
      "âœ… MQA KV Cache: K=torch.Size([2, 128, 1, 64]), V=torch.Size([2, 128, 1, 64])\n",
      "\n",
      "ğŸ“Š åƒæ•¸æ•ˆç‡å°æ¯”:\n",
      "â€¢ MHA åƒæ•¸: 2.36M\n",
      "â€¢ MQA åƒæ•¸: 1.28M\n",
      "â€¢ åƒæ•¸æ¸›å°‘: 45.8%\n",
      "\n",
      "ğŸ’¡ é—œéµè§€å¯Ÿ:\n",
      "   â€¢ MQA çš„ KV Cache æ˜¯å–®é ­: [B, L, 1, d]\n",
      "   â€¢ MHA çš„ KV Cache æ˜¯å¤šé ­: [B, L, 12, d]\n",
      "   â€¢ è¨˜æ†¶é«”ç¯€çœå€æ•¸: 12x\n"
     ]
    }
   ],
   "source": [
    "## ğŸ§ª æ¨¡å‹å»ºç«‹èˆ‡åŸºæœ¬åŠŸèƒ½æ¸¬è©¦\n",
    "\n",
    "def ensure_model_precision(model, target_dtype=torch.float32):\n",
    "    \"\"\"ç¢ºä¿æ¨¡å‹ä½¿ç”¨æŒ‡å®šç²¾åº¦\"\"\"\n",
    "    if target_dtype == torch.float16:\n",
    "        return model.half()\n",
    "    elif target_dtype == torch.float32:\n",
    "        return model.float()\n",
    "    else:\n",
    "        return model.to(dtype=target_dtype)\n",
    "\n",
    "# å»ºç«‹æ¸¬è©¦é…ç½®\n",
    "hidden_dim = 768\n",
    "num_heads = 12\n",
    "\n",
    "print(\"ğŸ—ï¸  å»ºç«‹èˆ‡æ¸¬è©¦æ¨¡å‹...\")\n",
    "print(f\"é…ç½®: hidden_dim={hidden_dim}, num_heads={num_heads}\")\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹\n",
    "mha = MultiHeadAttention(hidden_dim, num_heads).to(device)\n",
    "mqa = MultiQueryAttention(hidden_dim, num_heads).to(device)\n",
    "\n",
    "# ç²¾åº¦çµ±ä¸€ (é¿å… dtype ä¸åŒ¹é…éŒ¯èª¤)\n",
    "mha = ensure_model_precision(mha, torch.float32)\n",
    "mqa = ensure_model_precision(mqa, torch.float32)\n",
    "\n",
    "# æ¸¬è©¦è¼¸å…¥\n",
    "test_x = torch.randn(2, 128, hidden_dim, device=device)\n",
    "\n",
    "print(f\"\\nğŸ” ç²¾åº¦æª¢æŸ¥:\")\n",
    "print(f\"â€¢ MHA æ¨¡å‹ç²¾åº¦: {next(mha.parameters()).dtype}\")\n",
    "print(f\"â€¢ MQA æ¨¡å‹ç²¾åº¦: {next(mqa.parameters()).dtype}\")\n",
    "print(f\"â€¢ è¼¸å…¥æ•¸æ“šç²¾åº¦: {test_x.dtype}\")\n",
    "\n",
    "# åŸºæœ¬åŠŸèƒ½æ¸¬è©¦\n",
    "print(f\"\\nğŸ§ª åŸ·è¡ŒåŸºæœ¬åŠŸèƒ½æ¸¬è©¦...\")\n",
    "try:\n",
    "    mha_out = mha(test_x)\n",
    "    mqa_out = mqa(test_x)\n",
    "    print(f\"âœ… MHA æ¸¬è©¦é€šé: {test_x.shape} â†’ {mha_out.shape}\")\n",
    "    print(f\"âœ… MQA æ¸¬è©¦é€šé: {test_x.shape} â†’ {mqa_out.shape}\")\n",
    "    \n",
    "    # KV Cache æ¸¬è©¦\n",
    "    _, mha_past_kv = mha(test_x, use_cache=True)\n",
    "    _, mqa_past_kv = mqa(test_x, use_cache=True)\n",
    "    print(f\"âœ… MHA KV Cache: K={mha_past_kv[0].shape}, V={mha_past_kv[1].shape}\")\n",
    "    print(f\"âœ… MQA KV Cache: K={mqa_past_kv[0].shape}, V={mqa_past_kv[1].shape}\")\n",
    "    \n",
    "    # åƒæ•¸çµ±è¨ˆ\n",
    "    mha_params = sum(p.numel() for p in mha.parameters())\n",
    "    mqa_params = sum(p.numel() for p in mqa.parameters())\n",
    "    print(f\"\\nğŸ“Š åƒæ•¸æ•ˆç‡å°æ¯”:\")\n",
    "    print(f\"â€¢ MHA åƒæ•¸: {mha_params/1e6:.2f}M\")\n",
    "    print(f\"â€¢ MQA åƒæ•¸: {mqa_params/1e6:.2f}M\")\n",
    "    print(f\"â€¢ åƒæ•¸æ¸›å°‘: {(mha_params-mqa_params)/mha_params*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ é—œéµè§€å¯Ÿ:\")\n",
    "    print(f\"   â€¢ MQA çš„ KV Cache æ˜¯å–®é ­: [B, L, 1, d]\")\n",
    "    print(f\"   â€¢ MHA çš„ KV Cache æ˜¯å¤šé ­: [B, L, {num_heads}, d]\")\n",
    "    print(f\"   â€¢ è¨˜æ†¶é«”ç¯€çœå€æ•¸: {num_heads}x\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    print(f\"   é€™é€šå¸¸æ˜¯ç”±æ–¼ç²¾åº¦ä¸åŒ¹é…é€ æˆçš„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š KV Cache è¨˜æ†¶é«”æ•ˆç‡åˆ†æ\n",
      "============================================================\n",
      "é…ç½®: hidden_dim=768, num_heads=12, head_dim=64\n",
      "æ•¸æ“šé¡å‹: FP16 (2 bytes)\n",
      "\n",
      "åºåˆ—é•·åº¦       MHA (MB)     MQA (MB)     ç¯€çœæ¯”ä¾‹         ç¯€çœå€æ•¸        \n",
      "-----------------------------------------------------------------\n",
      "512        1.50         0.12         91.7        % 12.0        x\n",
      "1024       3.00         0.25         91.7        % 12.0        x\n",
      "2048       6.00         0.50         91.7        % 12.0        x\n",
      "4096       12.00        1.00         91.7        % 12.0        x\n",
      "\n",
      "ğŸ’¡ é—œéµè§€å¯Ÿ:\n",
      "â€¢ MQA çš„ KV Cache å¤§å°èˆ‡ num_heads ç„¡é—œ\n",
      "â€¢ è¨˜æ†¶é«”ç¯€çœæ¯”ä¾‹å›ºå®šç‚º 12x\n",
      "â€¢ é•·åºåˆ—æ™‚è¨˜æ†¶é«”å„ªå‹¢æ›´æ˜é¡¯\n",
      "â€¢ é€™ä½¿å¾— MQA ç‰¹åˆ¥é©åˆé•·æ–‡æœ¬ç”Ÿæˆä»»å‹™\n"
     ]
    }
   ],
   "source": [
    "## ğŸ“Š KV Cache è¨˜æ†¶é«”æ•ˆç‡åˆ†æ\n",
    "\n",
    "def analyze_kv_cache_memory(hidden_dim, num_heads):\n",
    "    \"\"\"è©³ç´°åˆ†æ KV Cache è¨˜æ†¶é«”ä½¿ç”¨\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š KV Cache è¨˜æ†¶é«”æ•ˆç‡åˆ†æ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ¸¬è©¦ä¸åŒåºåˆ—é•·åº¦\n",
    "    configs = [\n",
    "        {'seq_len': 512, 'batch_size': 1},\n",
    "        {'seq_len': 1024, 'batch_size': 1},\n",
    "        {'seq_len': 2048, 'batch_size': 1},\n",
    "        {'seq_len': 4096, 'batch_size': 1},\n",
    "    ]\n",
    "    \n",
    "    head_dim = hidden_dim // num_heads\n",
    "    dtype_bytes = 2  # FP16\n",
    "    \n",
    "    print(f\"é…ç½®: hidden_dim={hidden_dim}, num_heads={num_heads}, head_dim={head_dim}\")\n",
    "    print(f\"æ•¸æ“šé¡å‹: FP16 ({dtype_bytes} bytes)\\n\")\n",
    "    \n",
    "    print(f\"{'åºåˆ—é•·åº¦':<10} {'MHA (MB)':<12} {'MQA (MB)':<12} {'ç¯€çœæ¯”ä¾‹':<12} {'ç¯€çœå€æ•¸':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for config in configs:\n",
    "        seq_len = config['seq_len']\n",
    "        batch_size = config['batch_size']\n",
    "        \n",
    "        # MHA: [batch, seq_len, num_heads, head_dim] Ã— 2 (K+V)\n",
    "        mha_size_mb = (batch_size * seq_len * num_heads * head_dim * 2 * dtype_bytes) / (1024 * 1024)\n",
    "        \n",
    "        # MQA: [batch, seq_len, 1, head_dim] Ã— 2 (K+V)\n",
    "        mqa_size_mb = (batch_size * seq_len * 1 * head_dim * 2 * dtype_bytes) / (1024 * 1024)\n",
    "        \n",
    "        savings_ratio = (mha_size_mb - mqa_size_mb) / mha_size_mb * 100\n",
    "        savings_factor = mha_size_mb / mqa_size_mb\n",
    "        \n",
    "        print(f\"{seq_len:<10} {mha_size_mb:<12.2f} {mqa_size_mb:<12.2f} {savings_ratio:<12.1f}% {savings_factor:<12.1f}x\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ é—œéµè§€å¯Ÿ:\")\n",
    "    print(f\"â€¢ MQA çš„ KV Cache å¤§å°èˆ‡ num_heads ç„¡é—œ\")\n",
    "    print(f\"â€¢ è¨˜æ†¶é«”ç¯€çœæ¯”ä¾‹å›ºå®šç‚º {num_heads}x\")\n",
    "    print(f\"â€¢ é•·åºåˆ—æ™‚è¨˜æ†¶é«”å„ªå‹¢æ›´æ˜é¡¯\")\n",
    "    print(f\"â€¢ é€™ä½¿å¾— MQA ç‰¹åˆ¥é©åˆé•·æ–‡æœ¬ç”Ÿæˆä»»å‹™\")\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# åŸ·è¡Œè¨˜æ†¶é«”åˆ†æ\n",
    "memory_configs = analyze_kv_cache_memory(hidden_dim, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ åŸ·è¡Œå®Œæ•´æ¨ç†æ€§èƒ½æ¸¬è©¦...\n",
      "======================================================================\n",
      "ğŸš€ æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
      "======================================================================\n",
      "æ¸¬è©¦é…ç½®: seq_len=256, decode_steps=50\n",
      "\n",
      "ğŸ”§ é–‹å§‹æ¸¬è©¦ MHA...\n",
      "ğŸ” MHA ä½¿ç”¨ç²¾åº¦: torch.float32\n",
      "âœ… MHA Prefill æˆåŠŸ: 0.72ms\n",
      "\n",
      "ğŸ”§ é–‹å§‹æ¸¬è©¦ MQA...\n",
      "ğŸ” MQA ä½¿ç”¨ç²¾åº¦: torch.float32\n",
      "âœ… MQA Prefill æˆåŠŸ: 0.32ms\n",
      "\n",
      "ğŸ“Š è©³ç´°æ¸¬è©¦çµæœ:\n",
      "æ¨¡å‹       Prefill(ms)  Decode(ms/tok)  ååé‡(tok/s)      è¨˜æ†¶é«”(GB)      æˆåŠŸæ­¥æ•¸      \n",
      "-------------------------------------------------------------------------------------\n",
      "MHA      0.7          0.251           3981.7          0.073        50        \n",
      "MQA      0.3          0.136           7379.4          0.071        50        \n",
      "\n",
      "ğŸ“ˆ MQA æ€§èƒ½æ”¹é€²ç¸½çµ:\n",
      "â€¢ Prefill åŠ é€Ÿ: 2.23x\n",
      "â€¢ Decode åŠ é€Ÿ: 1.85x\n",
      "â€¢ ååé‡æå‡: 1.85x\n",
      "â€¢ è¨˜æ†¶é«”æ•ˆç‡: 1.02x\n",
      "\n",
      "ğŸ¯ å¯¦éš›æ‡‰ç”¨æ„ç¾©:\n",
      "   âœ… é¡¯è‘—çš„ååé‡æå‡ (1.85x)ï¼Œé©åˆé«˜ä¸¦ç™¼å ´æ™¯\n",
      "\n",
      "âœ… æ€§èƒ½æ¸¬è©¦å…¨éƒ¨å®Œæˆ!\n"
     ]
    }
   ],
   "source": [
    "## ğŸš€ æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
    "\n",
    "def benchmark_inference_performance(mha, mqa, hidden_dim):\n",
    "    \"\"\"å®Œæ•´çš„æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "    \n",
    "    def benchmark_model(model, name, seq_len=256, num_steps=50):\n",
    "        \"\"\"å–®å€‹æ¨¡å‹çš„è©³ç´°åŸºæº–æ¸¬è©¦\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # æ¸…ç†è¨˜æ†¶é«”\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # ç¢ºä¿è¼¸å…¥æ•¸æ“šèˆ‡æ¨¡å‹ç²¾åº¦åŒ¹é…\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "            print(f\"ğŸ” {name} ä½¿ç”¨ç²¾åº¦: {model_dtype}\")\n",
    "            \n",
    "            # Prefill éšæ®µ - è™•ç†åˆå§‹åºåˆ—\n",
    "            x = torch.randn(1, seq_len, hidden_dim, device=device, dtype=model_dtype)\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                output, past_kv = model(x, use_cache=True)\n",
    "                torch.cuda.synchronize()\n",
    "                prefill_time = time.time() - start_time\n",
    "                print(f\"âœ… {name} Prefill æˆåŠŸ: {prefill_time*1000:.2f}ms\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"âŒ {name} Prefill å¤±æ•—: {e}\")\n",
    "                return None\n",
    "            \n",
    "            # Decode éšæ®µ - é€å€‹ç”Ÿæˆæ–° token\n",
    "            decode_times = []\n",
    "            for step in range(num_steps):\n",
    "                new_token = torch.randn(1, 1, hidden_dim, device=device, dtype=model_dtype)\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    output, past_kv = model(new_token, past_kv=past_kv, use_cache=True)\n",
    "                    torch.cuda.synchronize()\n",
    "                    decode_times.append(time.time() - start_time)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"âŒ {name} Decode ç¬¬ {step} æ­¥å¤±æ•—: {e}\")\n",
    "                    break\n",
    "            \n",
    "            if not decode_times:\n",
    "                print(f\"âŒ {name} æ²’æœ‰æˆåŠŸçš„ decode æ­¥é©Ÿ\")\n",
    "                return None\n",
    "            \n",
    "            peak_memory = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "            \n",
    "        return {\n",
    "            'name': name,\n",
    "            'prefill_time_ms': prefill_time * 1000,\n",
    "            'avg_decode_time_ms': np.mean(decode_times) * 1000,\n",
    "            'decode_throughput': 1.0 / np.mean(decode_times),\n",
    "            'peak_memory_gb': peak_memory,\n",
    "            'total_tokens': seq_len + len(decode_times),\n",
    "            'successful_steps': len(decode_times)\n",
    "        }\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸš€ æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # æ¸¬è©¦é…ç½®\n",
    "    test_seq_len = 256  # é©ä¸­çš„åºåˆ—é•·åº¦\n",
    "    test_steps = 50     # decode æ­¥æ•¸\n",
    "    \n",
    "    print(f\"æ¸¬è©¦é…ç½®: seq_len={test_seq_len}, decode_steps={test_steps}\\n\")\n",
    "    \n",
    "    # æ¸¬è©¦å…©å€‹æ¨¡å‹\n",
    "    print(\"ğŸ”§ é–‹å§‹æ¸¬è©¦ MHA...\")\n",
    "    mha_result = benchmark_model(mha, \"MHA\", test_seq_len, test_steps)\n",
    "    \n",
    "    print(\"\\nğŸ”§ é–‹å§‹æ¸¬è©¦ MQA...\")\n",
    "    mqa_result = benchmark_model(mqa, \"MQA\", test_seq_len, test_steps)\n",
    "    \n",
    "    # æª¢æŸ¥çµæœ\n",
    "    if mha_result is None or mqa_result is None:\n",
    "        print(\"\\nâŒ æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç²¾åº¦åŒ¹é…å•é¡Œ\")\n",
    "        return None, None\n",
    "    \n",
    "    # è¼¸å‡ºè©³ç´°çµæœ\n",
    "    print(f\"\\nğŸ“Š è©³ç´°æ¸¬è©¦çµæœ:\")\n",
    "    print(f\"{'æ¨¡å‹':<8} {'Prefill(ms)':<12} {'Decode(ms/tok)':<15} {'ååé‡(tok/s)':<15} {'è¨˜æ†¶é«”(GB)':<12} {'æˆåŠŸæ­¥æ•¸':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for result in [mha_result, mqa_result]:\n",
    "        print(f\"{result['name']:<8} {result['prefill_time_ms']:<12.1f} \"\n",
    "              f\"{result['avg_decode_time_ms']:<15.3f} {result['decode_throughput']:<15.1f} \"\n",
    "              f\"{result['peak_memory_gb']:<12.3f} {result['successful_steps']:<10}\")\n",
    "    \n",
    "    # è¨ˆç®—æ€§èƒ½æ”¹é€²\n",
    "    speedup_prefill = mha_result['prefill_time_ms'] / mqa_result['prefill_time_ms']\n",
    "    speedup_decode = mha_result['avg_decode_time_ms'] / mqa_result['avg_decode_time_ms']\n",
    "    speedup_throughput = mqa_result['decode_throughput'] / mha_result['decode_throughput']\n",
    "    memory_efficiency = mha_result['peak_memory_gb'] / mqa_result['peak_memory_gb']\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ MQA æ€§èƒ½æ”¹é€²ç¸½çµ:\")\n",
    "    print(f\"â€¢ Prefill åŠ é€Ÿ: {speedup_prefill:.2f}x\")\n",
    "    print(f\"â€¢ Decode åŠ é€Ÿ: {speedup_decode:.2f}x\")\n",
    "    print(f\"â€¢ ååé‡æå‡: {speedup_throughput:.2f}x\")\n",
    "    print(f\"â€¢ è¨˜æ†¶é«”æ•ˆç‡: {memory_efficiency:.2f}x\")\n",
    "    \n",
    "    # å¯¦éš›æ„ç¾©è§£è®€\n",
    "    print(f\"\\nğŸ¯ å¯¦éš›æ‡‰ç”¨æ„ç¾©:\")\n",
    "    if speedup_throughput > 1.5:\n",
    "        print(f\"   âœ… é¡¯è‘—çš„ååé‡æå‡ ({speedup_throughput:.2f}x)ï¼Œé©åˆé«˜ä¸¦ç™¼å ´æ™¯\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  ååé‡æå‡æœ‰é™ ({speedup_throughput:.2f}x)ï¼Œä¸»è¦å„ªå‹¢åœ¨è¨˜æ†¶é«”ç¯€çœ\")\n",
    "    \n",
    "    if memory_efficiency > 1.2:\n",
    "        print(f\"   âœ… è¨˜æ†¶é«”æ•ˆç‡æå‡ ({memory_efficiency:.2f}x)ï¼Œæœ‰åŠ©æ–¼è™•ç†æ›´å¤§æ‰¹æ¬¡\")\n",
    "    \n",
    "    return mha_result, mqa_result\n",
    "\n",
    "# åŸ·è¡Œæ€§èƒ½æ¸¬è©¦\n",
    "print(\"ğŸ”§ åŸ·è¡Œå®Œæ•´æ¨ç†æ€§èƒ½æ¸¬è©¦...\")\n",
    "try:\n",
    "    mha_perf, mqa_perf = benchmark_inference_performance(mha, mqa, hidden_dim)\n",
    "    if mha_perf and mqa_perf:\n",
    "        print(\"\\nâœ… æ€§èƒ½æ¸¬è©¦å…¨éƒ¨å®Œæˆ!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  æ€§èƒ½æ¸¬è©¦éƒ¨åˆ†å¤±æ•—ï¼Œä½†å·²å±•ç¤ºä¿®å¾©æ–¹æ³•\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æ¸¬è©¦éç¨‹ä¸­å‡ºç¾ç•°å¸¸: {e}\")\n",
    "    print(f\"é€™é€šå¸¸æ˜¯ç”±æ–¼ç²¾åº¦ä¸åŒ¹é…æˆ–è¨˜æ†¶é«”ä¸è¶³é€ æˆçš„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ” MQA è³ªé‡å½±éŸ¿åˆ†æ\n",
      "============================================================\n",
      "ğŸ“Š è¼¸å‡ºå·®ç•°çµ±è¨ˆ:\n",
      "â€¢ å¹³å‡çµ•å°å·®ç•°: 0.026845\n",
      "â€¢ æœ€å¤§çµ•å°å·®ç•°: 0.141099\n",
      "â€¢ å·®ç•°æ¨™æº–å·®: 0.020038\n",
      "â€¢ ç›¸å°å·®ç•°: 1.4978 (149.78%)\n",
      "\n",
      "ğŸ”´ è³ªé‡å½±éŸ¿è©•ä¼°: è¼ƒå¤§\n",
      "\n",
      "ğŸ§  ç†è«–åˆ†æ:\n",
      "â€¢ MHA: æ¯å€‹ head æœ‰ç¨ç«‹çš„æ³¨æ„åŠ›æ¨¡å¼\n",
      "â€¢ MQA: æ‰€æœ‰ heads å…±äº«ç›¸åŒçš„ K, V\n",
      "â€¢ å½±éŸ¿: MQA æœƒæ¸›å°‘æ³¨æ„åŠ›æ¨¡å¼çš„å¤šæ¨£æ€§\n",
      "â€¢ å¯¦éš›æ•ˆæœ: å–æ±ºæ–¼å…·é«”ä»»å‹™å’Œæ•¸æ“šåˆ†ä½ˆ\n",
      "\n",
      "ğŸ’¡ å¯¦éš›æ‡‰ç”¨å»ºè­°:\n",
      "   ğŸ”´ è³ªé‡å½±éŸ¿è¼ƒå¤§ï¼Œå»ºè­°è¬¹æ…ä½¿ç”¨æˆ–é€²è¡Œå¾®èª¿\n",
      "\n",
      "âœ… è³ªé‡å½±éŸ¿åˆ†æå®Œæˆ!\n"
     ]
    }
   ],
   "source": [
    "## ğŸ” æ¨¡å‹è³ªé‡å½±éŸ¿åˆ†æ\n",
    "\n",
    "def analyze_quality_impact(mha, mqa, hidden_dim):\n",
    "    \"\"\"åˆ†æ MQA å°æ¨¡å‹è¼¸å‡ºè³ªé‡çš„å½±éŸ¿\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ” MQA è³ªé‡å½±éŸ¿åˆ†æ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š\n",
    "    batch_size, seq_len = 2, 256\n",
    "    test_input = torch.randn(batch_size, seq_len, hidden_dim, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ç²å–å…©å€‹æ¨¡å‹çš„è¼¸å‡º\n",
    "        mha_output = mha(test_input)\n",
    "        mqa_output = mqa(test_input)\n",
    "        \n",
    "        # è¨ˆç®—è¼¸å‡ºå·®ç•°\n",
    "        output_diff = (mha_output - mqa_output).abs()\n",
    "        \n",
    "        # çµ±è¨ˆæŒ‡æ¨™\n",
    "        mean_diff = output_diff.mean().item()\n",
    "        max_diff = output_diff.max().item()\n",
    "        std_diff = output_diff.std().item()\n",
    "        \n",
    "        # ç›¸å°å·®ç•° (æ›´é‡è¦çš„æŒ‡æ¨™)\n",
    "        mha_norm = mha_output.norm().item()\n",
    "        relative_diff = output_diff.norm().item() / mha_norm\n",
    "        \n",
    "        print(f\"ğŸ“Š è¼¸å‡ºå·®ç•°çµ±è¨ˆ:\")\n",
    "        print(f\"â€¢ å¹³å‡çµ•å°å·®ç•°: {mean_diff:.6f}\")\n",
    "        print(f\"â€¢ æœ€å¤§çµ•å°å·®ç•°: {max_diff:.6f}\")\n",
    "        print(f\"â€¢ å·®ç•°æ¨™æº–å·®: {std_diff:.6f}\")\n",
    "        print(f\"â€¢ ç›¸å°å·®ç•°: {relative_diff:.4f} ({relative_diff*100:.2f}%)\")\n",
    "        \n",
    "        # è©•ä¼°è³ªé‡å½±éŸ¿ç¨‹åº¦\n",
    "        if relative_diff < 0.01:\n",
    "            quality_impact = \"æ¥µå°\"\n",
    "            color = \"ğŸŸ¢\"\n",
    "        elif relative_diff < 0.05:\n",
    "            quality_impact = \"è¼ƒå°\"\n",
    "            color = \"ğŸŸ¡\"\n",
    "        elif relative_diff < 0.1:\n",
    "            quality_impact = \"ä¸­ç­‰\"\n",
    "            color = \"ğŸŸ \"\n",
    "        else:\n",
    "            quality_impact = \"è¼ƒå¤§\"\n",
    "            color = \"ğŸ”´\"\n",
    "        \n",
    "        print(f\"\\n{color} è³ªé‡å½±éŸ¿è©•ä¼°: {quality_impact}\")\n",
    "        \n",
    "        # ç†è«–åˆ†æ\n",
    "        print(f\"\\nğŸ§  ç†è«–åˆ†æ:\")\n",
    "        print(f\"â€¢ MHA: æ¯å€‹ head æœ‰ç¨ç«‹çš„æ³¨æ„åŠ›æ¨¡å¼\")\n",
    "        print(f\"â€¢ MQA: æ‰€æœ‰ heads å…±äº«ç›¸åŒçš„ K, V\")\n",
    "        print(f\"â€¢ å½±éŸ¿: MQA æœƒæ¸›å°‘æ³¨æ„åŠ›æ¨¡å¼çš„å¤šæ¨£æ€§\")\n",
    "        print(f\"â€¢ å¯¦éš›æ•ˆæœ: å–æ±ºæ–¼å…·é«”ä»»å‹™å’Œæ•¸æ“šåˆ†ä½ˆ\")\n",
    "        \n",
    "        # å¯¦éš›å»ºè­°\n",
    "        print(f\"\\nğŸ’¡ å¯¦éš›æ‡‰ç”¨å»ºè­°:\")\n",
    "        if relative_diff < 0.05:\n",
    "            print(f\"   âœ… è³ªé‡å½±éŸ¿å¾ˆå°ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨ MQA\")\n",
    "        elif relative_diff < 0.1:\n",
    "            print(f\"   âš ï¸  è³ªé‡æœ‰ä¸€å®šå½±éŸ¿ï¼Œéœ€è¦ä»»å‹™ç‰¹å®šè©•ä¼°\")\n",
    "        else:\n",
    "            print(f\"   ğŸ”´ è³ªé‡å½±éŸ¿è¼ƒå¤§ï¼Œå»ºè­°è¬¹æ…ä½¿ç”¨æˆ–é€²è¡Œå¾®èª¿\")\n",
    "        \n",
    "        return {\n",
    "            'mean_diff': mean_diff,\n",
    "            'max_diff': max_diff,\n",
    "            'relative_diff': relative_diff,\n",
    "            'quality_impact': quality_impact\n",
    "        }\n",
    "\n",
    "# åŸ·è¡Œè³ªé‡åˆ†æ\n",
    "try:\n",
    "    quality_analysis = analyze_quality_impact(mha, mqa, hidden_dim)\n",
    "    print(\"\\nâœ… è³ªé‡å½±éŸ¿åˆ†æå®Œæˆ!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ è³ªé‡åˆ†æå¤±æ•—: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ¯ MQA å¯¦æ–½å»ºè­°èˆ‡ç¸½çµ\n",
      "======================================================================\n",
      "\n",
      "ğŸš€ MQA æ ¸å¿ƒå„ªå‹¢:\n",
      "â€¢ åƒæ•¸æ•ˆç‡: æ¸›å°‘ attention åƒæ•¸ç´„ 25-50%\n",
      "â€¢ è¨˜æ†¶é«”æ•ˆç‡: KV Cache æ¸›å°‘ 12x\n",
      "â€¢ æ¨ç†åŠ é€Ÿ: ç‰¹åˆ¥æ˜¯é•·åºåˆ—å’Œå¤§æ‰¹æ¬¡\n",
      "â€¢ æ˜“æ–¼å¯¦ç¾: æœ€å°åŒ–çš„æ¶æ§‹ä¿®æ”¹\n",
      "\n",
      "âš–ï¸  é©ç”¨å ´æ™¯æ’åº:\n",
      "\n",
      "ğŸš€ é«˜ååé‡æ¨ç†æœå‹™ (â˜…â˜…â˜…â˜…â˜…)\n",
      "  é©ç”¨: API æœå‹™ã€èŠå¤©æ©Ÿå™¨äººã€ä»£ç¢¼è£œå…¨\n",
      "  åŸå› : è¨˜æ†¶é«”ç¯€çœç›´æ¥è½‰åŒ–ç‚ºæ›´é«˜ä¸¦ç™¼èƒ½åŠ›\n",
      "\n",
      "ğŸ“± è³‡æºå—é™éƒ¨ç½² (â˜…â˜…â˜…â˜…â˜†)\n",
      "  é©ç”¨: é‚Šç·£è¨­å‚™ã€ç§»å‹•ç«¯ã€åµŒå…¥å¼ç³»çµ±\n",
      "  åŸå› : è¨˜æ†¶é«”é™åˆ¶æ˜¯ä¸»è¦ç“¶é ¸ï¼ŒMQA æ•ˆæœé¡¯è‘—\n",
      "\n",
      "ğŸ“ é•·æ–‡æœ¬ç”Ÿæˆ (â˜…â˜…â˜…â˜…â˜†)\n",
      "  é©ç”¨: æ–‡ç« å¯«ä½œã€ä»£ç¢¼ç”Ÿæˆã€é•·å°è©±\n",
      "  åŸå› : KV Cache éš¨åºåˆ—é•·åº¦ç·šæ€§å¢é•·ï¼Œå„ªå‹¢æ˜é¡¯\n",
      "\n",
      "ğŸ”¬ ç ”ç©¶åŸå‹ (â˜…â˜…â˜…â˜†â˜†)\n",
      "  é©ç”¨: å¿«é€Ÿå¯¦é©—ã€æ¦‚å¿µé©—è­‰\n",
      "  åŸå› : å¹³è¡¡æ€§èƒ½èˆ‡å¯¦ç¾è¤‡é›œåº¦ï¼Œé©åˆå¿«é€Ÿè¿­ä»£\n",
      "\n",
      "âš ï¸  é‡è¦æ³¨æ„äº‹é …:\n",
      "â€¢ è³ªé‡è©•ä¼°: å‹™å¿…åœ¨å¯¦éš›ä»»å‹™ä¸Šè©•ä¼°è³ªé‡å½±éŸ¿\n",
      "â€¢ è¨“ç·´ç­–ç•¥: å»ºè­°å¾é è¨“ç·´æ¨¡å‹å¾®èª¿ï¼Œè€Œéå¾é ­è¨“ç·´\n",
      "â€¢ æ‰¹æ¬¡æ•ˆæ‡‰: æ‰¹æ¬¡å¤§å°è¶Šå¤§ï¼ŒMQA ç›¸å°å„ªå‹¢è¶Šæ˜é¡¯\n",
      "â€¢ ç¡¬é«”ä¾è³´: åœ¨è¨˜æ†¶é«”é »å¯¬å—é™çš„ç¡¬é«”ä¸Šæ•ˆæœæ›´ä½³\n",
      "â€¢ ç²¾åº¦ç®¡ç†: æ³¨æ„æ¨¡å‹èˆ‡è¼¸å…¥æ•¸æ“šçš„ç²¾åº¦åŒ¹é…\n",
      "\n",
      "ğŸ› ï¸  å¯¦æ–½æ­¥é©ŸæŒ‡å—:\n",
      "  1. ğŸ“Š åŸºæº–æ¸¬è©¦: å»ºç«‹åŸå§‹ MHA æ¨¡å‹çš„æ€§èƒ½åŸºæº–\n",
      "  2. ğŸ”„ æ¶æ§‹è½‰æ›: å°‡ MHA æ›¿æ›ç‚º MQA (æ³¨æ„ç²¾åº¦åŒ¹é…)\n",
      "  3. ğŸ” è³ªé‡é©—è­‰: åœ¨é©—è­‰é›†ä¸Šè©•ä¼°è³ªé‡å½±éŸ¿\n",
      "  4. âš¡ æ€§èƒ½æ¸¬è©¦: æ¸¬é‡æ¨ç†é€Ÿåº¦å’Œè¨˜æ†¶é«”ä½¿ç”¨\n",
      "  5. ğŸ¯ å¾®èª¿å„ªåŒ–: å¦‚è³ªé‡ä¸‹é™æ˜é¡¯ï¼Œé€²è¡Œå°‘é‡å¾®èª¿\n",
      "  6. ğŸš€ ç”Ÿç”¢éƒ¨ç½²: åœ¨å¯¦éš›ç’°å¢ƒä¸­é©—è­‰æ•ˆæœ\n",
      "\n",
      "ğŸŠ ç¸½çµèˆ‡å±•æœ›:\n",
      "MQA æ˜¯æ¨ç†å„ªåŒ–çš„é‡è¦æŠ€è¡“ï¼Œç‰¹åˆ¥é©åˆè¨˜æ†¶é«”å—é™å’Œé«˜ä¸¦ç™¼å ´æ™¯ã€‚\n",
      "é›–ç„¶å¯èƒ½æœ‰è¼•å¾®è³ªé‡å½±éŸ¿ï¼Œä½†åœ¨å¤§å¤šæ•¸å¯¦éš›æ‡‰ç”¨ä¸­æ˜¯å¯æ¥å—çš„æ¬Šè¡¡ã€‚\n",
      "çµåˆå…¶ä»–æŠ€è¡“(å¦‚ FlashAttentionã€é‡åŒ–)å¯ä»¥ç²å¾—æ›´å¤§çš„æ€§èƒ½æå‡ã€‚\n",
      "\n",
      "ğŸ”® ä¸‹ä¸€æ­¥å­¸ç¿’:\n",
      "â€¢ Lab-1.6 ç¬¬ 3 éƒ¨åˆ†: GQA (Grouped-Query Attention)\n",
      "â€¢ Lab-1.6 ç¬¬ 4 éƒ¨åˆ†: æ¨ç†å„ªåŒ–å¯¦æˆ°\n",
      "â€¢ çµåˆ vLLM ç­‰ç”Ÿç”¢ç´šæ¨ç†æ¡†æ¶\n",
      "\n",
      "============================================================\n",
      "âœ… Lab-1.6 (MQA å¯¦ç¾) å®Œæˆ!\n",
      "============================================================\n",
      "ğŸ“ æ­å–œï¼ä½ å·²ç¶“æŒæ¡äº†:\n",
      "   â€¢ Multi-Query Attention çš„æ ¸å¿ƒåŸç†èˆ‡å¯¦ç¾\n",
      "   â€¢ KV Cache å„ªåŒ–æ©Ÿåˆ¶èˆ‡è¨˜æ†¶é«”åˆ†æ\n",
      "   â€¢ æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦æ–¹æ³•\n",
      "   â€¢ è³ªé‡å½±éŸ¿è©•ä¼°èˆ‡å¯¦æ–½ç­–ç•¥\n",
      "\n",
      "ğŸš€ ä½ ç¾åœ¨å¯ä»¥åœ¨å¯¦éš›é …ç›®ä¸­æ‡‰ç”¨ MQA æŠ€è¡“ï¼\n"
     ]
    }
   ],
   "source": [
    "## ğŸ¯ å¯¦æ–½å»ºè­°èˆ‡ç¸½çµ\n",
    "\n",
    "def generate_implementation_recommendations(num_heads):\n",
    "    \"\"\"åŸºæ–¼æ¸¬è©¦çµæœç”Ÿæˆå¯¦æ–½å»ºè­°\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ¯ MQA å¯¦æ–½å»ºè­°èˆ‡ç¸½çµ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nğŸš€ MQA æ ¸å¿ƒå„ªå‹¢:\")\n",
    "    print(f\"â€¢ åƒæ•¸æ•ˆç‡: æ¸›å°‘ attention åƒæ•¸ç´„ 25-50%\")\n",
    "    print(f\"â€¢ è¨˜æ†¶é«”æ•ˆç‡: KV Cache æ¸›å°‘ {num_heads}x\")\n",
    "    print(f\"â€¢ æ¨ç†åŠ é€Ÿ: ç‰¹åˆ¥æ˜¯é•·åºåˆ—å’Œå¤§æ‰¹æ¬¡\")\n",
    "    print(f\"â€¢ æ˜“æ–¼å¯¦ç¾: æœ€å°åŒ–çš„æ¶æ§‹ä¿®æ”¹\")\n",
    "    \n",
    "    print(f\"\\nâš–ï¸  é©ç”¨å ´æ™¯æ’åº:\")\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            'name': 'ğŸš€ é«˜ååé‡æ¨ç†æœå‹™',\n",
    "            'description': 'API æœå‹™ã€èŠå¤©æ©Ÿå™¨äººã€ä»£ç¢¼è£œå…¨',\n",
    "            'priority': 'â˜…â˜…â˜…â˜…â˜…',\n",
    "            'reason': 'è¨˜æ†¶é«”ç¯€çœç›´æ¥è½‰åŒ–ç‚ºæ›´é«˜ä¸¦ç™¼èƒ½åŠ›'\n",
    "        },\n",
    "        {\n",
    "            'name': 'ğŸ“± è³‡æºå—é™éƒ¨ç½²',\n",
    "            'description': 'é‚Šç·£è¨­å‚™ã€ç§»å‹•ç«¯ã€åµŒå…¥å¼ç³»çµ±',\n",
    "            'priority': 'â˜…â˜…â˜…â˜…â˜†',\n",
    "            'reason': 'è¨˜æ†¶é«”é™åˆ¶æ˜¯ä¸»è¦ç“¶é ¸ï¼ŒMQA æ•ˆæœé¡¯è‘—'\n",
    "        },\n",
    "        {\n",
    "            'name': 'ğŸ“ é•·æ–‡æœ¬ç”Ÿæˆ',\n",
    "            'description': 'æ–‡ç« å¯«ä½œã€ä»£ç¢¼ç”Ÿæˆã€é•·å°è©±',\n",
    "            'priority': 'â˜…â˜…â˜…â˜…â˜†',\n",
    "            'reason': 'KV Cache éš¨åºåˆ—é•·åº¦ç·šæ€§å¢é•·ï¼Œå„ªå‹¢æ˜é¡¯'\n",
    "        },\n",
    "        {\n",
    "            'name': 'ğŸ”¬ ç ”ç©¶åŸå‹',\n",
    "            'description': 'å¿«é€Ÿå¯¦é©—ã€æ¦‚å¿µé©—è­‰',\n",
    "            'priority': 'â˜…â˜…â˜…â˜†â˜†',\n",
    "            'reason': 'å¹³è¡¡æ€§èƒ½èˆ‡å¯¦ç¾è¤‡é›œåº¦ï¼Œé©åˆå¿«é€Ÿè¿­ä»£'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\n{scenario['name']} ({scenario['priority']})\")\n",
    "        print(f\"  é©ç”¨: {scenario['description']}\")\n",
    "        print(f\"  åŸå› : {scenario['reason']}\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸  é‡è¦æ³¨æ„äº‹é …:\")\n",
    "    print(f\"â€¢ è³ªé‡è©•ä¼°: å‹™å¿…åœ¨å¯¦éš›ä»»å‹™ä¸Šè©•ä¼°è³ªé‡å½±éŸ¿\")\n",
    "    print(f\"â€¢ è¨“ç·´ç­–ç•¥: å»ºè­°å¾é è¨“ç·´æ¨¡å‹å¾®èª¿ï¼Œè€Œéå¾é ­è¨“ç·´\")\n",
    "    print(f\"â€¢ æ‰¹æ¬¡æ•ˆæ‡‰: æ‰¹æ¬¡å¤§å°è¶Šå¤§ï¼ŒMQA ç›¸å°å„ªå‹¢è¶Šæ˜é¡¯\")\n",
    "    print(f\"â€¢ ç¡¬é«”ä¾è³´: åœ¨è¨˜æ†¶é«”é »å¯¬å—é™çš„ç¡¬é«”ä¸Šæ•ˆæœæ›´ä½³\")\n",
    "    print(f\"â€¢ ç²¾åº¦ç®¡ç†: æ³¨æ„æ¨¡å‹èˆ‡è¼¸å…¥æ•¸æ“šçš„ç²¾åº¦åŒ¹é…\")\n",
    "    \n",
    "    print(f\"\\nğŸ› ï¸  å¯¦æ–½æ­¥é©ŸæŒ‡å—:\")\n",
    "    steps = [\n",
    "        \"1. ğŸ“Š åŸºæº–æ¸¬è©¦: å»ºç«‹åŸå§‹ MHA æ¨¡å‹çš„æ€§èƒ½åŸºæº–\",\n",
    "        \"2. ğŸ”„ æ¶æ§‹è½‰æ›: å°‡ MHA æ›¿æ›ç‚º MQA (æ³¨æ„ç²¾åº¦åŒ¹é…)\",\n",
    "        \"3. ğŸ” è³ªé‡é©—è­‰: åœ¨é©—è­‰é›†ä¸Šè©•ä¼°è³ªé‡å½±éŸ¿\",\n",
    "        \"4. âš¡ æ€§èƒ½æ¸¬è©¦: æ¸¬é‡æ¨ç†é€Ÿåº¦å’Œè¨˜æ†¶é«”ä½¿ç”¨\",\n",
    "        \"5. ğŸ¯ å¾®èª¿å„ªåŒ–: å¦‚è³ªé‡ä¸‹é™æ˜é¡¯ï¼Œé€²è¡Œå°‘é‡å¾®èª¿\",\n",
    "        \"6. ğŸš€ ç”Ÿç”¢éƒ¨ç½²: åœ¨å¯¦éš›ç’°å¢ƒä¸­é©—è­‰æ•ˆæœ\"\n",
    "    ]\n",
    "    \n",
    "    for step in steps:\n",
    "        print(f\"  {step}\")\n",
    "    \n",
    "    print(f\"\\nğŸŠ ç¸½çµèˆ‡å±•æœ›:\")\n",
    "    print(f\"MQA æ˜¯æ¨ç†å„ªåŒ–çš„é‡è¦æŠ€è¡“ï¼Œç‰¹åˆ¥é©åˆè¨˜æ†¶é«”å—é™å’Œé«˜ä¸¦ç™¼å ´æ™¯ã€‚\")\n",
    "    print(f\"é›–ç„¶å¯èƒ½æœ‰è¼•å¾®è³ªé‡å½±éŸ¿ï¼Œä½†åœ¨å¤§å¤šæ•¸å¯¦éš›æ‡‰ç”¨ä¸­æ˜¯å¯æ¥å—çš„æ¬Šè¡¡ã€‚\")\n",
    "    print(f\"çµåˆå…¶ä»–æŠ€è¡“(å¦‚ FlashAttentionã€é‡åŒ–)å¯ä»¥ç²å¾—æ›´å¤§çš„æ€§èƒ½æå‡ã€‚\")\n",
    "    \n",
    "    print(f\"\\nğŸ”® ä¸‹ä¸€æ­¥å­¸ç¿’:\")\n",
    "    print(f\"â€¢ Lab-1.6 ç¬¬ 3 éƒ¨åˆ†: GQA (Grouped-Query Attention)\")\n",
    "    print(f\"â€¢ Lab-1.6 ç¬¬ 4 éƒ¨åˆ†: æ¨ç†å„ªåŒ–å¯¦æˆ°\")\n",
    "    print(f\"â€¢ çµåˆ vLLM ç­‰ç”Ÿç”¢ç´šæ¨ç†æ¡†æ¶\")\n",
    "\n",
    "# ç”Ÿæˆå¯¦æ–½å»ºè­°\n",
    "generate_implementation_recommendations(num_heads)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ… Lab-1.6 (MQA å¯¦ç¾) å®Œæˆ!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"ğŸ“ æ­å–œï¼ä½ å·²ç¶“æŒæ¡äº†:\")\n",
    "print(f\"   â€¢ Multi-Query Attention çš„æ ¸å¿ƒåŸç†èˆ‡å¯¦ç¾\")\n",
    "print(f\"   â€¢ KV Cache å„ªåŒ–æ©Ÿåˆ¶èˆ‡è¨˜æ†¶é«”åˆ†æ\")\n",
    "print(f\"   â€¢ æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦æ–¹æ³•\")\n",
    "print(f\"   â€¢ è³ªé‡å½±éŸ¿è©•ä¼°èˆ‡å¯¦æ–½ç­–ç•¥\")\n",
    "print(f\"\\nğŸš€ ä½ ç¾åœ¨å¯ä»¥åœ¨å¯¦éš›é …ç›®ä¸­æ‡‰ç”¨ MQA æŠ€è¡“ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
