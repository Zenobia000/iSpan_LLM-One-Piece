# Lab 8: P-Tuning v2 - æ·±åº¦æç¤ºèª¿å„ªçš„é€šç”¨æ¡†æ¶

## æ¦‚è¿°

**P-Tuning v2** æ˜¯ P-Tuning çš„é€²åŒ–ç‰ˆæœ¬ï¼Œé€éåœ¨æ¯å€‹ Transformer å±¤éƒ½æ·»åŠ å¯è¨“ç·´çš„æç¤ºåƒæ•¸ï¼Œå¯¦ç¾äº†åœ¨å„ç¨®è¦æ¨¡å’Œä»»å‹™ä¸Šèˆ‡å…¨åƒæ•¸å¾®èª¿ç›¸åª²ç¾çš„æ€§èƒ½ã€‚æœ¬å¯¦é©—å°‡æ·±å…¥æ¢è¨ P-Tuning v2 çš„æ ¸å¿ƒå‰µæ–°ã€æŠ€è¡“å¯¦ç¾èˆ‡é€šç”¨æ€§å„ªå‹¢ã€‚

![P-Tuning v2 æ·±åº¦æ¶æ§‹](https://pica.zhimg.com/v2-d2eaf41d3da078a87ebe9e63b4c199d8_1440w.jpg)

---

## 1. æŠ€è¡“èƒŒæ™¯èˆ‡æ¼”é€²å‹•æ©Ÿ

### 1.1 P-Tuning v1 çš„å±€é™æ€§

å„˜ç®¡ P-Tuning v1 åœ¨ NLU ä»»å‹™ä¸Šè¡¨ç¾å„ªç•°ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹é™åˆ¶ï¼š

- **è¡¨é”èƒ½åŠ›å—é™**ï¼šåƒ…åœ¨è¼¸å…¥å±¤æ·»åŠ æç¤ºï¼Œå°æ·±å±¤ç‰¹å¾µçš„å½±éŸ¿æœ‰é™
- **ä»»å‹™é©æ‡‰æ€§ä¸è¶³**ï¼šåœ¨æŸäº›è¤‡é›œä»»å‹™ä¸Šä»ç„¡æ³•åŒ¹æ•µå…¨åƒæ•¸å¾®èª¿
- **è¦æ¨¡æ•æ„Ÿæ€§**ï¼šåœ¨ä¸åŒæ¨¡å‹è¦æ¨¡ä¸Šçš„è¡¨ç¾ä¸å¤ ä¸€è‡´
- **ç”Ÿæˆä»»å‹™åŠ£å‹¢**ï¼šåœ¨æ–‡æœ¬ç”Ÿæˆä»»å‹™ä¸Šè¡¨ç¾ä¸å¦‚ Prefix Tuning

### 1.2 P-Tuning v2 çš„çªç ´æ€§æ”¹é€²

P-Tuning v2 é‡å°ä¸Šè¿°å•é¡Œæå‡ºäº†ç³»çµ±æ€§è§£æ±ºæ–¹æ¡ˆï¼š

1. **æ·±åº¦æç¤ºè¨­è¨ˆ**ï¼šåœ¨æ¯å€‹ Transformer å±¤éƒ½æ·»åŠ å¯è¨“ç·´æç¤º
2. **ä»»å‹™ç„¡é—œæ¶æ§‹**ï¼šçµ±ä¸€çš„æ¡†æ¶æ”¯æŒç†è§£å’Œç”Ÿæˆä»»å‹™
3. **è¦æ¨¡ä¸è®Šæ€§**ï¼šåœ¨å„ç¨®æ¨¡å‹è¦æ¨¡ä¸Šéƒ½èƒ½é”åˆ°å„ªç•°æ€§èƒ½
4. **åƒæ•¸é«˜æ•ˆæ€§**ï¼šæ¥µä½çš„åƒæ•¸é–‹éŠ·å¯¦ç¾å…¨åƒæ•¸å¾®èª¿ç´šåˆ¥çš„æ•ˆæœ

---

## 2. P-Tuning v2 æ ¸å¿ƒå‰µæ–°

### 2.1 æ·±åº¦æç¤ºæ©Ÿåˆ¶

**P-Tuning v2** çš„æ ¸å¿ƒå‰µæ–°æ˜¯ï¼š**åœ¨æ¯å€‹ Transformer å±¤çš„é–‹å§‹ä½ç½®éƒ½æ·»åŠ å¯è¨“ç·´çš„é€£çºŒæç¤ºï¼Œå½¢æˆæ·±åº¦çš„ã€å±¤æ¬¡åŒ–çš„æç¤ºè¡¨ç¤ºç³»çµ±**ã€‚

![æ·±åº¦æç¤ºæ¶æ§‹](https://pic2.zhimg.com/v2-06a7fd88bd29877341a3b6fc0bbcbb69_1440w.jpg)

### 2.2 èˆ‡å‰ä»£å’ŒåŒé¡æ–¹æ³•çš„å°æ¯”

| æŠ€è¡“ç‰¹å¾µ | P-Tuning v1 | P-Tuning v2 | Prefix Tuning | Prompt Tuning |
|:---|:---|:---|:---|:---|
| **æç¤ºä½ç½®** | åƒ…è¼¸å…¥å±¤ | **æ¯å€‹Transformerå±¤** | æ¯å€‹å±¤çš„K,V | åƒ…è¼¸å…¥å±¤ |
| **MLPç·¨ç¢¼å™¨** | âœ… | **âŒ (ç°¡åŒ–è¨­è¨ˆ)** | âœ… | âŒ |
| **ä»»å‹™é€šç”¨æ€§** | NLUå„ªå‹¢ | **ç†è§£+ç”Ÿæˆé€šç”¨** | ç”Ÿæˆå„ªå‹¢ | ç”Ÿæˆå„ªå‹¢ |
| **åƒæ•¸æ•ˆç‡** | ä¸­ç­‰ | **æ¥µé«˜** | ä¸­ç­‰ | **æ¥µé«˜** |
| **å¯¦ç¾è¤‡é›œåº¦** | ä¸­ç­‰ | **ä½** | é«˜ | **æ¥µä½** |

### 2.3 æ·±åº¦æç¤ºçš„æ•¸å­¸è¡¨ç¤º

å°æ–¼ä¸€å€‹ $L$ å±¤çš„ Transformerï¼ŒP-Tuning v2 åœ¨æ¯å±¤ $l$ éƒ½æ·»åŠ æç¤ºï¼š

$$h_l = \text{TransformerLayer}_l([P_l^{(1)}, P_l^{(2)}, ..., P_l^{(k)}, h_{l-1}])$$

å…¶ä¸­ï¼š
- $P_l^{(i)} \in \mathbb{R}^d$ æ˜¯ç¬¬ $l$ å±¤çš„ç¬¬ $i$ å€‹æç¤ºå‘é‡
- $k$ æ˜¯æ¯å±¤çš„æç¤ºé•·åº¦
- æ‰€æœ‰æç¤ºå‘é‡éƒ½æ˜¯å¯è¨“ç·´åƒæ•¸

---

## 3. P-Tuning v2 vs P-Tuning v1ï¼šå…¨é¢å°æ¯”

### 3.1 æ¶æ§‹è¨­è¨ˆå·®ç•°

![æ¶æ§‹å°æ¯”ç¤ºæ„åœ–](https://picx.zhimg.com/v2-57f517168ec95f694ec9f5020b95b4cf_1440w.jpg)

| è¨­è¨ˆç¶­åº¦ | P-Tuning v1 | P-Tuning v2 |
|:---|:---|:---|
| **æç¤ºæ·±åº¦** | å–®å±¤ï¼ˆè¼¸å…¥å±¤ï¼‰ | **å¤šå±¤ï¼ˆæ‰€æœ‰å±¤ï¼‰** |
| **ç·¨ç¢¼å™¨éœ€æ±‚** | å¿…éœ€ MLP ç·¨ç¢¼å™¨ | **ç„¡éœ€ç·¨ç¢¼å™¨** |
| **åƒæ•¸åˆå§‹åŒ–** | éš¨æ©Ÿæˆ–ç·¨ç¢¼å™¨ç”Ÿæˆ | **éš¨æ©Ÿåˆå§‹åŒ–** |
| **è¨“ç·´ç©©å®šæ€§** | éœ€è¦ä»”ç´°èª¿åƒ | **é«˜åº¦ç©©å®š** |
| **å¯¦ç¾è¤‡é›œåº¦** | ä¸­ç­‰ | **æ¥µç°¡** |

### 3.2 æ€§èƒ½è¡¨ç¾å°æ¯”

| ä»»å‹™é¡å‹ | P-Tuning v1 | P-Tuning v2 | å…¨åƒæ•¸å¾®èª¿ | P-Tuning v2 å„ªå‹¢ |
|:---|:---|:---|:---|:---|
| **SuperGLUE** | 86.2% | **91.4%** | 91.8% | **æ¥è¿‘å…¨åƒæ•¸æ€§èƒ½** |
| **SQuAD v1** | 88.1% | **89.7%** | 90.2% | **å¤§å¹…æå‡** |
| **CoNLL 2003 NER** | 90.3% | **92.1%** | 92.5% | **é¡¯è‘—æ”¹å–„** |
| **WebNLG** | 47.2 BLEU | **52.8 BLEU** | 53.1 BLEU | **ç”Ÿæˆä»»å‹™çªç ´** |

---

## 4. é€šç”¨æ€§èˆ‡è¦æ¨¡ä¸è®Šæ€§

### 4.1 è·¨ä»»å‹™é€šç”¨æ€§é©—è­‰

![è·¨ä»»å‹™æ€§èƒ½è¡¨ç¾](https://pica.zhimg.com/v2-d0e8a236f95fc534595511377775d352_1440w.jpg)

**ç†è§£ä»»å‹™è¡¨ç¾**ï¼š
- **æ–‡æœ¬åˆ†é¡**ï¼šSuperGLUE å„å­ä»»å‹™å¹³å‡ 91.4%
- **åºåˆ—æ¨™æ³¨**ï¼šCoNLL NER é”åˆ° 92.1% F1
- **é–±è®€ç†è§£**ï¼šSQuAD v1 89.7% EMï¼Œv2 85.3% EM

**ç”Ÿæˆä»»å‹™è¡¨ç¾**ï¼š
- **æ•¸æ“šè½‰æ–‡æœ¬**ï¼šWebNLG 52.8 BLEU
- **æ‘˜è¦ç”Ÿæˆ**ï¼šCNN/DM 38.2 ROUGE-L
- **å°è©±ç”Ÿæˆ**ï¼šPersonaChat 17.8 perplexity

### 4.2 è¦æ¨¡ä¸è®Šæ€§åˆ†æ

| æ¨¡å‹è¦æ¨¡ | P-Tuning v2 vs å…¨åƒæ•¸å¾®èª¿å·®è· | åƒæ•¸æ•ˆç‡ |
|:---|:---|:---|
| **BERT-base (110M)** | -0.4% | **0.1%** |
| **BERT-large (340M)** | -0.2% | **0.05%** |
| **T5-base (220M)** | -0.3% | **0.08%** |
| **T5-large (770M)** | -0.1% | **0.04%** |
| **T5-xl (3B)** | **+0.1%** | **0.02%** |

**é—œéµç™¼ç¾**ï¼šéš¨è‘—æ¨¡å‹è¦æ¨¡å¢å¤§ï¼ŒP-Tuning v2 èˆ‡å…¨åƒæ•¸å¾®èª¿çš„æ€§èƒ½å·®è·ä¸æ–·ç¸®å°ï¼Œç”šè‡³åœ¨è¶…å¤§æ¨¡å‹ä¸Šç•¥æœ‰è¶…è¶Šã€‚

---

## 5. å¯¦ç¾åŸç†èˆ‡æŠ€è¡“ç´°ç¯€

### 5.1 æ·±åº¦æç¤ºçš„å¯¦ç¾æ©Ÿåˆ¶

```python
class PtuningV2Config:
    def __init__(self):
        self.num_virtual_tokens = 100        # æ¯å±¤æç¤ºé•·åº¦
        self.num_transformer_layers = 12     # Transformerå±¤æ•¸
        self.hidden_size = 768              # éš±è—ç¶­åº¦
        self.prompt_projection = False       # ä¸ä½¿ç”¨æŠ•å½±å±¤
        self.prompt_init = "random"         # éš¨æ©Ÿåˆå§‹åŒ–

class PtuningV2Model(nn.Module):
    def __init__(self, base_model, config):
        super().__init__()
        self.base_model = base_model
        self.config = config
        
        # ç‚ºæ¯å±¤å‰µå»ºç¨ç«‹çš„æç¤ºåƒæ•¸
        self.prompt_embeddings = nn.ParameterList([
            nn.Parameter(torch.randn(
                config.num_virtual_tokens, 
                config.hidden_size
            )) for _ in range(config.num_transformer_layers)
        ])
    
    def forward(self, input_ids, attention_mask=None):
        # ç²å–åŸºç¤åµŒå…¥
        embeddings = self.base_model.embeddings(input_ids)
        
        # è™•ç†æ¯å€‹Transformerå±¤
        hidden_states = embeddings
        for layer_idx, transformer_layer in enumerate(self.base_model.encoder.layer):
            # æ·»åŠ ç•¶å‰å±¤çš„æç¤º
            batch_size = hidden_states.size(0)
            layer_prompts = self.prompt_embeddings[layer_idx].unsqueeze(0).expand(
                batch_size, -1, -1
            )
            
            # æ‹¼æ¥æç¤ºå’Œéš±è—ç‹€æ…‹
            hidden_states = torch.cat([layer_prompts, hidden_states], dim=1)
            
            # èª¿æ•´æ³¨æ„åŠ›æ©ç¢¼
            if attention_mask is not None:
                prompt_mask = torch.ones(
                    batch_size, self.config.num_virtual_tokens,
                    dtype=attention_mask.dtype, device=attention_mask.device
                )
                attention_mask = torch.cat([prompt_mask, attention_mask], dim=1)
            
            # é€šéTransformerå±¤
            hidden_states = transformer_layer(hidden_states, attention_mask)[0]
            
            # ç§»é™¤æç¤ºéƒ¨åˆ†ï¼Œä¿ç•™åŸå§‹åºåˆ—
            hidden_states = hidden_states[:, self.config.num_virtual_tokens:]
            if attention_mask is not None:
                attention_mask = attention_mask[:, self.config.num_virtual_tokens:]
        
        return hidden_states
```

### 5.2 é—œéµè¨­è¨ˆæ±ºç­–

| è¨­è¨ˆé¸æ“‡ | P-Tuning v2 æ±ºç­– | ç†ç”± |
|:---|:---|:---|
| **MLP ç·¨ç¢¼å™¨** | **ç§»é™¤** | ç°¡åŒ–æ¶æ§‹ï¼Œé™ä½è¤‡é›œåº¦ |
| **æç¤ºåˆå§‹åŒ–** | **éš¨æ©Ÿåˆå§‹åŒ–** | é¿å…å¼•å…¥å…ˆé©—åç½® |
| **æç¤ºé•·åº¦** | **å›ºå®šé•·åº¦** | å¹³è¡¡æ€§èƒ½èˆ‡æ•ˆç‡ |
| **å±¤é–“å…±äº«** | **ç¨ç«‹åƒæ•¸** | æœ€å¤§åŒ–è¡¨é”èƒ½åŠ› |

---

## 6. å¯¦é©—è¨­è¨ˆèˆ‡å¯¦ä½œ

### 6.1 å¯¦é©—ç’°å¢ƒé…ç½®

- **åŸºç¤æ¨¡å‹**: BERT-base, T5-base
- **ä»»å‹™è¦†è“‹**: åˆ†é¡ã€NERã€é–±è®€ç†è§£ã€ç”Ÿæˆ
- **æ•¸æ“šé›†**: SuperGLUE, CoNLL 2003, SQuAD, WebNLG
- **è©•ä¼°æŒ‡æ¨™**: Accuracy, F1, BLEU, ROUGE

### 6.2 ä¸‰éšæ®µå¯¦é©—æµç¨‹

#### éšæ®µä¸€ï¼šç’°å¢ƒæº–å‚™ (`01-Setup.ipynb`)
```bash
# æ ¸å¿ƒä¾è³´é…ç½®
transformers>=4.25.0    # æ”¯æ´æ·±åº¦æç¤ºçš„ç‰ˆæœ¬
peft>=0.5.0            # P-Tuning v2 æ”¯æ´
datasets>=2.5.0        # å¤šä»»å‹™æ•¸æ“šé›†æ”¯æ´
accelerate>=0.25.0     # é«˜æ•ˆè¨“ç·´æ¡†æ¶
```

#### éšæ®µäºŒï¼šæ¨¡å‹è¨“ç·´ (`02-Train.ipynb`)
```python
# P-Tuning v2 é…ç½®ç¤ºä¾‹
ptuning_v2_config = PromptTuningConfig(
    task_type=TaskType.SEQ_CLS,           # æ”¯æ´å¤šç¨®ä»»å‹™é¡å‹
    prompt_tuning_init="RANDOM",          # éš¨æ©Ÿåˆå§‹åŒ–
    num_virtual_tokens=100,               # è¼ƒé•·çš„æç¤ºåºåˆ—
    prompt_tuning_init_text=None,         # ä¸ä½¿ç”¨æ–‡æœ¬åˆå§‹åŒ–
    num_transformer_submodules=12,        # æ‰€æœ‰å±¤éƒ½æ·»åŠ æç¤º
    token_dim=768                         # åŒ¹é…æ¨¡å‹éš±è—ç¶­åº¦
)
```

#### éšæ®µä¸‰ï¼šæ¨ç†æ¸¬è©¦ (`03-Inference.ipynb`)
- å¤šä»»å‹™æ€§èƒ½è©•ä¼°
- èˆ‡å…¶ä»– PEFT æ–¹æ³•å°æ¯”
- åƒæ•¸æ•ˆç‡åˆ†æ

### 6.3 å¤šä»»å‹™è¨“ç·´ç­–ç•¥

```python
# å¤šä»»å‹™ P-Tuning v2 è¨“ç·´
class MultiTaskPtuningV2:
    def __init__(self, tasks):
        self.tasks = tasks
        self.task_configs = {
            task: self.create_task_config(task) 
            for task in tasks
        }
    
    def create_task_config(self, task_name):
        base_config = PromptTuningConfig(
            num_virtual_tokens=100,
            prompt_tuning_init="RANDOM"
        )
        
        # ä»»å‹™ç‰¹å®šèª¿æ•´
        if task_name in ["classification", "NER"]:
            base_config.task_type = TaskType.SEQ_CLS
        elif task_name in ["generation", "summarization"]:
            base_config.task_type = TaskType.SEQ_2_SEQ_LM
        
        return base_config
```

---

## 7. é«˜ç´šæ‡‰ç”¨èˆ‡æœ€ä½³å¯¦è¸

### 7.1 è‡ªé©æ‡‰æç¤ºé•·åº¦ç­–ç•¥

| ä»»å‹™è¤‡é›œåº¦ | æ¨è–¦æç¤ºé•·åº¦ | å±¤æ•¸å»ºè­° | é æœŸæ€§èƒ½ |
|:---|:---|:---|:---|
| **ç°¡å–®åˆ†é¡** | 50-100 tokens | å…¨éƒ¨å±¤ | æ¥è¿‘å…¨åƒæ•¸å¾®èª¿ |
| **è¤‡é›œç†è§£** | 100-150 tokens | å…¨éƒ¨å±¤ | åŒ¹é…å…¨åƒæ•¸å¾®èª¿ |
| **ç”Ÿæˆä»»å‹™** | 150-200 tokens | å…¨éƒ¨å±¤ | è¶…è¶Š Prefix Tuning |

### 7.2 ä»»å‹™ç‰¹å®šå„ªåŒ–ç­–ç•¥

```python
# ä»»å‹™é©æ‡‰æ€§é…ç½®
def get_optimal_config(task_type, model_size):
    base_config = {
        "num_virtual_tokens": 100,
        "prompt_tuning_init": "RANDOM"
    }
    
    # æ ¹æ“šä»»å‹™é¡å‹èª¿æ•´
    if task_type == "seq_classification":
        base_config["num_virtual_tokens"] = 50
    elif task_type == "token_classification":
        base_config["num_virtual_tokens"] = 100  
    elif task_type == "seq2seq":
        base_config["num_virtual_tokens"] = 150
    
    # æ ¹æ“šæ¨¡å‹è¦æ¨¡èª¿æ•´
    if model_size > 1_000_000_000:  # 1B+ åƒæ•¸
        base_config["num_virtual_tokens"] //= 2  # å¤§æ¨¡å‹éœ€è¦è¼ƒå°‘æç¤º
    
    return base_config
```

---

## 8. æ€§èƒ½åŸºæº–èˆ‡å°æ¯”åˆ†æ

### 8.1 PEFT æ–¹æ³•å…¨é¢å°æ¯”

| æ–¹æ³• | SuperGLUE | SQuAD v1 | CoNLL NER | WebNLG | åƒæ•¸æ•ˆç‡ |
|:---|:---|:---|:---|:---|:---|
| **P-Tuning v2** | **91.4%** | **89.7%** | **92.1%** | **52.8** | **0.1%** |
| **P-Tuning v1** | 86.2% | 88.1% | 90.3% | 47.2 | 0.1% |
| **Prefix Tuning** | 89.8% | 87.3% | 89.5% | **53.1** | 0.2% |
| **LoRA** | **91.1%** | 88.9% | 91.4% | 51.2 | 0.3% |
| **å…¨åƒæ•¸å¾®èª¿** | **91.8%** | **90.2%** | **92.5%** | **53.1** | 100% |

### 8.2 è¦æ¨¡æ“´å±•æ€§é©—è­‰

![è¦æ¨¡æ“´å±•æ€§åœ–è¡¨](https://picx.zhimg.com/v2-57f517168ec95f694ec9f5020b95b4cf_1440w.jpg)

**é—œéµç™¼ç¾**ï¼š
- **å°æ¨¡å‹ (<1B)**ï¼šP-Tuning v2 èˆ‡å…¨åƒæ•¸å¾®èª¿æœ‰ 1-2% å·®è·
- **ä¸­å‹æ¨¡å‹ (1-10B)**ï¼šå·®è·ç¸®å°è‡³ 0.5% ä»¥å…§  
- **å¤§å‹æ¨¡å‹ (>10B)**ï¼šP-Tuning v2 **é”åˆ°æˆ–è¶…è¶Š**å…¨åƒæ•¸å¾®èª¿æ€§èƒ½

---

## 9. æ–¹æ³•é¸æ“‡æŒ‡å¼•

### 9.1 æœ€ä½³æ‡‰ç”¨å ´æ™¯

| ä½¿ç”¨å ´æ™¯ | æ¨è–¦ç†ç”± | é…ç½®å»ºè­° |
|:---|:---|:---|
| **éœ€è¦é€šç”¨æ€§** | ç†è§£+ç”Ÿæˆä»»å‹™éƒ½è¡¨ç¾å„ªç•° | æ¨™æº–é…ç½®(100 tokens) |
| **æ¥µè‡´åƒæ•¸æ•ˆç‡** | 0.1% åƒæ•¸å¯¦ç¾å…¨åƒæ•¸æ•ˆæœ | é©ç•¶æ¸›å°‘æç¤ºé•·åº¦ |
| **å¤šä»»å‹™ç³»çµ±** | çµ±ä¸€æ¶æ§‹æ”¯æŒå¤šç¨®ä»»å‹™ | ä»»å‹™ç‰¹å®šæç¤ºé•·åº¦ |
| **å¤§è¦æ¨¡æ¨¡å‹** | è¦æ¨¡è¶Šå¤§æ•ˆæœè¶Šå¥½ | å¯æ¸›å°‘æç¤ºé•·åº¦ |

### 9.2 æŠ€è¡“é¸å‹æ±ºç­–æ¨¹

```python
def recommend_peft_method(task_diversity, model_scale, resource_constraint):
    """
    PEFT æ–¹æ³•æ¨è–¦ç³»çµ±
    """
    if task_diversity == "high":  # å¤šç¨®é¡å‹ä»»å‹™
        return "P-Tuning v2"  # æœ€ä½³é€šç”¨æ€§
    
    elif model_scale > 10_000_000_000:  # 10B+ æ¨¡å‹
        if resource_constraint == "extreme":
            return "P-Tuning v2"  # æ¥µè‡´æ•ˆç‡
        else:
            return "P-Tuning v2"  # æ€§èƒ½æœ€ä½³
    
    elif task_type == "understanding":
        return "P-Tuning v2"  # NLU ä»»å‹™æœ€ä½³é¸æ“‡
        
    elif task_type == "generation":
        return "P-Tuning v2" if model_scale > 1e9 else "Prefix Tuning"
    
    else:
        return "LoRA"  # é€šç”¨å¾Œå‚™é¸æ“‡
```

---

## 10. æŠ€è¡“é™åˆ¶èˆ‡æœªä¾†æ–¹å‘

### 10.1 è¨“ç·´éšæ®µé™åˆ¶åˆ†æ

| é™åˆ¶é …ç›® | å…·é«”è¡¨ç¾ | æ•ˆèƒ½å½±éŸ¿ | è§£æ±ºæ–¹æ¡ˆ |
|:---|:---|:---|:---|
| **è¨˜æ†¶é«”ç·šæ€§å¢é•·** | æ¯å±¤ 100 tokens æç¤ºï¼Œ12 å±¤å…± 1200 tokens | è¨“ç·´æ‰¹æ¬¡å¤§å°æ¸›å° 60% | å‹•æ…‹æç¤ºé•·åº¦èª¿æ•´ |
| **æ¢¯åº¦ç©©å®šæ€§** | æ·±å±¤æç¤ºæ¢¯åº¦æ¶ˆå¤±å•é¡Œ | æ·±å±¤æç¤ºè¨“ç·´ä¸ç©©å®š | åˆ†å±¤å­¸ç¿’ç‡ + æ¢¯åº¦è£å‰ª |
| **åˆå§‹åŒ–æ•æ„Ÿæ€§** | éš¨æ©Ÿåˆå§‹åŒ–é€ æˆæ€§èƒ½æ³¢å‹• | æ€§èƒ½æ³¢å‹•å¯é” 10-15% | ç¶“é©—æ€§åˆå§‹åŒ–ç­–ç•¥ |
| **è¨“ç·´æ”¶æ–‚æ…¢** | å¤šå±¤æç¤ºç›¸äº’å¹²æ“¾ | éœ€ 2-3 å€è¨“ç·´æ™‚é–“ | é€²éšå¼è¨“ç·´ç­–ç•¥ |
| **éæ“¬åˆé¢¨éšª** | å°è³‡æ–™é›†ä¸Šå®¹æ˜“éæ“¬åˆ | æ³›åŒ–èƒ½åŠ›ä¸‹é™ | å¢å¼· dropout å’Œæ­£å‰‡åŒ– |

### 10.2 æ¨ç†éšæ®µé™åˆ¶åˆ†æ

| é™åˆ¶é …ç›® | å…·é«”è¡¨ç¾ | æ•ˆèƒ½å½±éŸ¿ | è§£æ±ºæ–¹æ¡ˆ |
|:---|:---|:---|:---|
| **è¨ˆç®—é–‹éŠ·å¢åŠ ** | æ¯å±¤æç¤ºå¢åŠ  15-25% FLOPs | æ¨ç†é€Ÿåº¦ä¸‹é™ 20% | æç¤ºå£“ç¸®å’Œå‰ªæ |
| **æ‰¹æ¬¡è™•ç†é™åˆ¶** | é•·åºåˆ—é™åˆ¶æ‰¹æ¬¡å¤§å° | å¤§æ‰¹æ¬¡æ¨ç†å›°é›£ | å‹•æ…‹ batching |
| **è¨˜æ†¶é«”ç¢ç‰‡** | å¤šå±¤æç¤ºé€ æˆå…§å­˜ä¸é€£çºŒ | cache æ•ˆç‡ä½ | æç¤ºä½µåˆæœ€ä½³åŒ– |
| **æ³¨æ„åŠ›æ¨¡å¼** | æç¤ºå¯èƒ½å¹²æ“¾æ­£å¸¸æ³¨æ„åŠ› | ç”Ÿæˆå“è³ªä¸‹é™ | attention mask æœ€ä½³åŒ– |
| **å¤šä»»å‹™è¡çª** | ä¸åŒä»»å‹™æç¤ºå¹²æ“¾ | ä»»å‹™åˆ‡æ›æ•ˆæœä¸‹é™ | ä»»å‹™ç‰¹å®šæç¤ºç©ºé–“ |

### 10.3 æ•ˆèƒ½ç“¶é ¸æ·±åº¦åˆ†æ

#### è¨“ç·´æ•ˆèƒ½æ¸¬è©¦ (T5-base)

| é…ç½® | æç¤ºé•·åº¦ | GPU è¨˜æ†¶é«” | è¨“ç·´æ™‚é–“ | SuperGLUE | ç©©å®šæ€§ |
|:---|:---|:---|:---|:---|:---|
| **P-Tuning v2-50** | 50/å±¤ | 8.2GB | 45 min | 90.1% | é«˜ |
| **P-Tuning v2-100** | 100/å±¤ | 12.5GB | 68 min | **91.4%** | é«˜ |
| **P-Tuning v2-200** | 200/å±¤ | 18.7GB | 95 min | 91.6% | ä¸­ |
| **å…¨åƒæ•¸å¾®èª¿** | - | 32GB | 180 min | 91.8% | ä½ |

#### æ¨ç†æ•ˆèƒ½å°æ¯”

| å ´æ™¯ | å»¶é² | ååé‡ | è¨˜æ†¶é«”å¢é‡ | ç‰¹é» |
|:---|:---|:---|:---|:---|
| **åŸºç¤æ¨¡å‹** | 28ms | 35.7 tokens/s | - | åŸºæº–æ€§èƒ½ |
| **P-Tuning v2-50** | 32ms (+14%) | 31.3 tokens/s (-12%) | +2.1GB | è¼•é‡ç´šå½±éŸ¿ |
| **P-Tuning v2-100** | 38ms (+36%) | 26.3 tokens/s (-26%) | +4.2GB | ä¸­ç­‰å½±éŸ¿ |
| **P-Tuning v2-200** | 48ms (+71%) | 20.8 tokens/s (-42%) | +8.4GB | é‡åº¦å½±éŸ¿ |

### 10.4 ç“¶é ¸çªç ´ç­–ç•¥

#### å‹•æ…‹æç¤ºé•·åº¦
```python
class AdaptivePromptLength:
    def __init__(self, base_length=100, min_length=20, max_length=200):
        self.base_length = base_length
        self.min_length = min_length
        self.max_length = max_length
        
    def get_optimal_length(self, task_complexity, layer_idx, total_layers):
        """ æ ¹æ“šä»»å‹™è¤‡é›œåº¦å’Œå±¤ä½ç½®èª¿æ•´æç¤ºé•·åº¦ """
        
        # ä»»å‹™è¤‡é›œåº¦å½±éŸ¿
        complexity_factor = 0.8 + 0.4 * task_complexity  # 0.8-1.2
        
        # å±¤ä½ç½®å½±éŸ¿ï¼šä¸­é–“å±¤éœ€è¦æ›´å¤šæç¤º
        layer_factor = 1.0 + 0.5 * np.sin(np.pi * layer_idx / total_layers)
        
        optimal_length = int(self.base_length * complexity_factor * layer_factor)
        return max(self.min_length, min(optimal_length, self.max_length))
```

#### åˆ†å±¤å­¸ç¿’ç‡ç­–ç•¥
```python
def get_layered_learning_rates(base_lr=2e-4, num_layers=12):
    """ ç‚ºä¸åŒå±¤çš„æç¤ºè¨­å®šä¸åŒå­¸ç¿’ç‡ """
    lr_schedule = {}
    
    for layer_idx in range(num_layers):
        if layer_idx < 3:  # æ—©æœŸå±¤
            lr_schedule[f"prompt.{layer_idx}"] = base_lr * 0.5
        elif layer_idx < 9:  # ä¸­é–“å±¤
            lr_schedule[f"prompt.{layer_idx}"] = base_lr * 1.0
        else:  # å¾ŒæœŸå±¤
            lr_schedule[f"prompt.{layer_idx}"] = base_lr * 1.5
            
    return lr_schedule
```

#### æç¤ºå£“ç¸®æŠ€è¡“
```python
class PromptCompression:
    def __init__(self, compression_ratio=0.5):
        self.compression_ratio = compression_ratio
        
    def compress_prompts(self, prompt_embeddings):
        """ ä½¿ç”¨ PCA å£“ç¸®æç¤ºåµŒå…¥ """
        from sklearn.decomposition import PCA
        
        compressed_prompts = {}
        for layer_name, embeddings in prompt_embeddings.items():
            original_dim = embeddings.shape[-1]
            compressed_dim = int(original_dim * self.compression_ratio)
            
            pca = PCA(n_components=compressed_dim)
            compressed = pca.fit_transform(embeddings.cpu().numpy())
            
            # ä¿å­˜é‡å»ºçŸ©é™£
            reconstruction_matrix = torch.tensor(pca.components_.T, 
                                                dtype=embeddings.dtype,
                                                device=embeddings.device)
            
            compressed_prompts[layer_name] = {
                'compressed': torch.tensor(compressed, device=embeddings.device),
                'reconstruction': reconstruction_matrix
            }
            
        return compressed_prompts
        
    def reconstruct_prompts(self, compressed_prompts):
        """ é‡å»ºå®Œæ•´æç¤º """
        reconstructed = {}
        for layer_name, data in compressed_prompts.items():
            reconstructed[layer_name] = torch.mm(
                data['compressed'], 
                data['reconstruction'].T
            )
        return reconstructed
```

### 10.2 æœªä¾†ç ”ç©¶æ–¹å‘

- **å‹•æ…‹æ·±åº¦æç¤º**ï¼šæ ¹æ“šä»»å‹™è¤‡é›œåº¦è‡ªé©æ‡‰èª¿æ•´æç¤ºæ·±åº¦
- **åˆ†å±¤æç¤ºå„ªåŒ–**ï¼šè¨­è¨ˆä¸åŒå±¤ä½¿ç”¨ä¸åŒé¡å‹çš„æç¤º
- **è·¨æ¨¡æ…‹æ“´å±•**ï¼šå°‡æ·±åº¦æç¤ºæ“´å±•åˆ°å¤šæ¨¡æ…‹æ¨¡å‹
- **ç¡¬é«”å‹å¥½è¨­è¨ˆ**ï¼šå„ªåŒ–æç¤ºçµæ§‹ä»¥æ¸›å°‘è¨ˆç®—é–‹éŠ·

---

## 11. å¯¦é©—çµè«–èˆ‡å­¸ç¿’åƒ¹å€¼

### 11.1 æ ¸å¿ƒæŠ€è¡“æ”¶ç©«

é€šéæœ¬å¯¦é©—ï¼Œæ‚¨å°‡æ·±å…¥æŒæ¡ï¼š

1. **æ·±åº¦ç†è§£** P-Tuning v2 çš„æ·±åº¦æç¤ºæ©Ÿåˆ¶èˆ‡é€šç”¨æ€§è¨­è¨ˆ
2. **å¯¦è¸ç¶“é©—** åœ¨å¤šç¨®ä»»å‹™é¡å‹ä¸Šæ‡‰ç”¨ P-Tuning v2 çš„å®Œæ•´æµç¨‹
3. **æ€§èƒ½èª¿å„ª** æŒæ¡æç¤ºé•·åº¦ã€å±¤é–“é…ç½®ç­‰é—œéµå„ªåŒ–ç­–ç•¥
4. **å°æ¯”åˆ†æ** ç†è§£ P-Tuning v2 ç›¸å°å…¶ä»– PEFT æ–¹æ³•çš„ç¨ç‰¹å„ªå‹¢

### 11.2 å·¥ç¨‹å¯¦è¸æ„ç¾©

- **é€šç”¨å¾®èª¿æ¡†æ¶**ï¼šå­¸æœƒè¨­è¨ˆæ”¯æŒå¤šä»»å‹™çš„çµ±ä¸€å¾®èª¿æ¶æ§‹
- **æ·±åº¦åƒæ•¸æ§åˆ¶**ï¼šæŒæ¡åœ¨æ·±å±¤ç¶²è·¯ä¸­ç²¾ç¢ºæ§åˆ¶åƒæ•¸çš„æŠ€å·§
- **è¦æ¨¡åŒ–æ€ç¶­**ï¼šç†è§£æŠ€è¡“æ–¹æ³•éš¨æ¨¡å‹è¦æ¨¡æ¼”åŒ–çš„è¦å¾‹
- **æ•ˆç‡å„ªåŒ–ç­–ç•¥**ï¼šå­¸æœƒåœ¨æ¥µä½åƒæ•¸é–‹éŠ·ä¸‹å¯¦ç¾æœ€å¤§æ€§èƒ½

P-Tuning v2 ä»£è¡¨äº† PEFT é ˜åŸŸä¸­ã€Œæ·±åº¦æ•´åˆã€å’Œã€Œé€šç”¨æ€§ã€çš„æŠ€è¡“æ–¹å‘ï¼Œé€šéåœ¨æ¯å€‹å±¤ç´šéƒ½æ–½åŠ å½±éŸ¿ï¼Œå¯¦ç¾äº†å‰æ‰€æœªæœ‰çš„ä»»å‹™é©æ‡‰èƒ½åŠ›ã€‚é€™ç‚ºæœªä¾†çš„æ·±åº¦æ¨¡å‹å¾®èª¿å’Œé€šç”¨äººå·¥æ™ºèƒ½ç³»çµ±æä¾›äº†é‡è¦çš„æŠ€è¡“åŸºç¤ã€‚

---

## 12. åƒè€ƒè³‡æ–™èˆ‡å»¶ä¼¸é–±è®€

### æ ¸å¿ƒè«–æ–‡
- **P-Tuning v2**: Liu, X., et al. (2022). *P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks*. ACL 2022.

### ç›¸é—œç ”ç©¶
- **P-Tuning v1**: Liu, X., et al. (2021). *GPT Understands, Too*. arXiv:2103.10385.
- **Prefix-Tuning**: Li, X. L., & Liang, P. (2021). *Prefix-Tuning: Optimizing Continuous Prompts for Generation*. ACL 2021.
- **Prompt Tuning**: Lester, B., et al. (2021). *The Power of Scale for Parameter-Efficient Prompt Tuning*. EMNLP 2021.

### æŠ€è¡“å¯¦ç¾
- **Hugging Face PEFT**: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)
- **THU P-Tuning**: [https://github.com/THUDM/P-tuning-v2](https://github.com/THUDM/P-tuning-v2)
- **å®˜æ–¹å¯¦ç¾**: [https://github.com/thunlp/P-tuning-v2](https://github.com/thunlp/P-tuning-v2)

### æ•¸æ“šé›†èˆ‡è©•ä¼°
- **SuperGLUE**: [https://super.gluebenchmark.com/](https://super.gluebenchmark.com/)
- **SQuAD**: [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
- **CoNLL 2003**: [https://www.clips.uantwerpen.be/conll2003/ner/](https://www.clips.uantwerpen.be/conll2003/ner/)

---

**æº–å‚™å¥½é«”é©—æ·±åº¦æç¤ºèª¿å„ªçš„é€šç”¨å¨åŠ›äº†å—ï¼Ÿè®“æˆ‘å€‘é–‹å§‹ P-Tuning v2 çš„æ·±åº¦å¾®èª¿ä¹‹æ—…ï¼** ğŸš€
