{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.6: 標準 MHA 基準測試\n",
    "## Multi-Head Attention Baseline\n",
    "\n",
    "**學習目標**:\n",
    "- 實現標準 Multi-Head Attention (MHA)\n",
    "- 實現 KV Cache 機制\n",
    "- 測量推理性能基準\n",
    "- 分析 KV Cache 記憶體占用\n",
    "\n",
    "**預計時間**: 30-45分鐘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention 實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"標準 Multi-Head Attention\"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads 組 K\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads 組 V\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        B, N, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # 處理 KV Cache\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            K = torch.cat([past_k, K], dim=1)\n",
    "            V = torch.cat([past_v, V], dim=1)\n",
    "        \n",
    "        # Transpose for attention\n",
    "        Q = Q.transpose(1, 2)  # [B, H, N, D]\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(B, N, self.hidden_dim)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            return output, (K.transpose(1, 2), V.transpose(1, 2))\n",
    "        return output\n",
    "\n",
    "# 測試\n",
    "mha = MultiHeadAttention(768, 12).to(device)\n",
    "x = torch.randn(2, 128, 768, device=device)\n",
    "out = mha(x)\n",
    "print(f\"✅ MHA 測試通過: {x.shape} → {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. KV Cache 性能測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_kv_cache_size(num_layers, seq_len, num_kv_heads, head_dim, dtype=torch.float16):\n",
    "    \"\"\"計算 KV Cache 大小\"\"\"\n",
    "    bytes_per_element = 2 if dtype == torch.float16 else 4\n",
    "    \n",
    "    # 每層: 2 (K+V) × seq_len × num_kv_heads × head_dim × bytes\n",
    "    per_layer_mb = 2 * seq_len * num_kv_heads * head_dim * bytes_per_element / 1e6\n",
    "    total_mb = per_layer_mb * num_layers\n",
    "    \n",
    "    return {\n",
    "        'per_layer_mb': per_layer_mb,\n",
    "        'total_mb': total_mb,\n",
    "        'total_gb': total_mb / 1024\n",
    "    }\n",
    "\n",
    "# Llama-2-7B 配置\n",
    "config = {\n",
    "    'num_layers': 32,\n",
    "    'num_query_heads': 32,\n",
    "    'head_dim': 128,\n",
    "    'seq_len': 2048\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KV Cache 大小分析 (Llama-2-7B 配置)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n配置:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# 對比不同架構\n",
    "architectures = [\n",
    "    ('MHA (標準)', config['num_query_heads']),\n",
    "    ('GQA-8', 8),\n",
    "    ('GQA-4', 4),\n",
    "    ('MQA', 1)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'架構':<20} {'KV Heads':<12} {'每層(MB)':<15} {'總計(GB)':<15} {'相對MHA':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "mha_total = None\n",
    "for name, num_kv_heads in architectures:\n",
    "    cache_size = measure_kv_cache_size(\n",
    "        config['num_layers'],\n",
    "        config['seq_len'],\n",
    "        num_kv_heads,\n",
    "        config['head_dim']\n",
    "    )\n",
    "    \n",
    "    if mha_total is None:\n",
    "        mha_total = cache_size['total_gb']\n",
    "        relative = \"100%\"\n",
    "    else:\n",
    "        relative = f\"{cache_size['total_gb']/mha_total*100:.1f}%\"\n",
    "    \n",
    "    print(f\"{name:<20} {num_kv_heads:<12} {cache_size['per_layer_mb']:<15.2f} {cache_size['total_gb']:<15.2f} {relative:<12}\")\n",
    "\n",
    "print(f\"\\n💡 觀察: MQA 將 KV Cache 減少至 MHA 的 3%, GQA-8 減少至 25%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. 推理性能基準測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(attn_module, seq_len, num_tokens=100):\n",
    "    \"\"\"測試生成性能\"\"\"\n",
    "    attn_module.eval()\n",
    "    \n",
    "    # 模擬推理\n",
    "    x = torch.randn(1, seq_len, 768, device=device, dtype=torch.float16)\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        # Prefill\n",
    "        start = time.time()\n",
    "        out, past_kv = attn_module(x, use_cache=True)\n",
    "        torch.cuda.synchronize()\n",
    "        prefill_time = time.time() - start\n",
    "        \n",
    "        # Decode\n",
    "        for _ in range(num_tokens):\n",
    "            new_token = torch.randn(1, 1, 768, device=device, dtype=torch.float16)\n",
    "            start = time.time()\n",
    "            out, past_kv = attn_module(new_token, past_kv=past_kv, use_cache=True)\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    return {\n",
    "        'prefill_time': prefill_time,\n",
    "        'avg_decode_time': np.mean(times),\n",
    "        'total_time': prefill_time + sum(times),\n",
    "        'peak_memory_gb': peak_mem\n",
    "    }\n",
    "\n",
    "# 測試\n",
    "print(\"MHA 推理基準測試...\")\n",
    "result = benchmark_generation(mha, seq_len=128, num_tokens=50)\n",
    "print(f\"Prefill: {result['prefill_time']*1000:.2f}ms\")\n",
    "print(f\"Decode: {result['avg_decode_time']*1000:.2f}ms/token\")\n",
    "print(f\"記憶體: {result['peak_memory_gb']:.3f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
