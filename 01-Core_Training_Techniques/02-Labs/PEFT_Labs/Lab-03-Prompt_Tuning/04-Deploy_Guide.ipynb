{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Prompt Tuning - Deployment Guide\n",
    "---\n",
    "## ğŸ¯ å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬ Notebook å±•ç¤º Prompt Tuning çš„æ ¸å¿ƒç‰¹æ€§:**è»Ÿæç¤ºçš„ç¨ç«‹éƒ¨ç½²èˆ‡å¤šä»»å‹™åˆ‡æ›**ã€‚\n",
    "\n",
    "èˆ‡å…¶ä»– PEFT æ–¹æ³•ä¸åŒ,Prompt Tuning çš„è»Ÿæç¤ºåƒæ•¸**ç„¡éœ€åˆä½µ**åˆ°åŸºç¤æ¨¡å‹ä¸­,é€™å¸¶ä¾†ç¨ç‰¹çš„éƒ¨ç½²å„ªå‹¢:\n",
    "\n",
    "### é—œéµå­¸ç¿’è¦é»\n",
    "- ç†è§£ Prompt Tuning çš„éƒ¨ç½²ç‰¹æ€§èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„å·®ç•°\n",
    "- æŒæ¡è»Ÿæç¤ºçš„å„²å­˜ã€è¼‰å…¥èˆ‡ç‰ˆæœ¬ç®¡ç†\n",
    "- å¯¦ç¾å¤šä»»å‹™è»Ÿæç¤ºçš„å‹•æ…‹åˆ‡æ›æ©Ÿåˆ¶\n",
    "- å„ªåŒ–ç”Ÿç”¢ç’°å¢ƒçš„æ¨ç†æ•ˆèƒ½\n",
    "- å­¸ç¿’æœ€ä½³å¯¦è¸èˆ‡æ•…éšœæ’é™¤ç­–ç•¥\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­ç½®èˆ‡ä¾è³´æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„å‡½å¼åº«\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PromptTuningConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# è¨­å®šè¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Tuning éƒ¨ç½²ç‰¹æ€§åˆ†æ\n",
    "\n",
    "### 2.1 ç‚ºä»€éº¼ Prompt Tuning ä¸éœ€è¦åˆä½µ?\n",
    "\n",
    "**æ ¸å¿ƒåŸç†**:\n",
    "```\n",
    "Prompt Tuning: [è»Ÿæç¤ºå‘é‡] + [è¼¸å…¥æ–‡æœ¬] â†’ æ¨¡å‹è™•ç†\n",
    "LoRA/IAÂ³:      ä¿®æ”¹æ¬Šé‡çŸ©é™£ W' = W + Î”W â†’ å¯åˆä½µ\n",
    "```\n",
    "\n",
    "**é—œéµå·®ç•°å°æ¯”**:\n",
    "\n",
    "| å°æ¯”ç¶­åº¦ | Prompt Tuning | LoRA | IAÂ³ |\n",
    "|---------|--------------|------|-----|\n",
    "| **åƒæ•¸ä½ç½®** | è¼¸å…¥å±¤(è»Ÿæç¤ºåµŒå…¥) | æ¬Šé‡çŸ©é™£åˆ†è§£ | ç¸®æ”¾å‘é‡ |\n",
    "| **æ˜¯å¦å¯åˆä½µ** | âŒ ä¸éœ€è¦ | âœ… å¯åˆä½µ | âœ… å¯åˆä½µ |\n",
    "| **éƒ¨ç½²æ–¹å¼** | è»Ÿæç¤º + åŸºç¤æ¨¡å‹ | åˆä½µå¾Œå–®ä¸€æ¨¡å‹ | åˆä½µå¾Œå–®ä¸€æ¨¡å‹ |\n",
    "| **æ¨ç†é–‹éŠ·** | è»Ÿæç¤ºé•·åº¦å½±éŸ¿ | é›¶é–‹éŠ·(åˆä½µå¾Œ) | é›¶é–‹éŠ·(åˆä½µå¾Œ) |\n",
    "| **å¤šä»»å‹™åˆ‡æ›** | **æ¥µå…¶ä¾¿æ·** | éœ€é‡æ–°è¼‰å…¥ | éœ€é‡æ–°è¼‰å…¥ |\n",
    "| **å„²å­˜æˆæœ¬** | æ¥µä½(åƒ…è»Ÿæç¤º) | ä¸­ç­‰ | ä½ |\n",
    "\n",
    "### 2.2 Prompt Tuning çš„ç¨ç‰¹éƒ¨ç½²å„ªå‹¢\n",
    "\n",
    "1. **å¤šä»»å‹™å…±äº«åŸºç¤æ¨¡å‹**: ä¸€å€‹æ¨¡å‹ + N å€‹è»Ÿæç¤º = N å€‹ä»»å‹™èƒ½åŠ›\n",
    "2. **å‹•æ…‹ä»»å‹™åˆ‡æ›**: åƒ…éœ€æ›¿æ›è»Ÿæç¤º,ç„¡éœ€é‡æ–°è¼‰å…¥æ¨¡å‹\n",
    "3. **ç‰ˆæœ¬ç®¡ç†ç°¡å–®**: è»Ÿæç¤ºåƒæ•¸é‡æ¥µå°,æ˜“æ–¼ç‰ˆæœ¬æ§åˆ¶èˆ‡å¯¦é©—è¿½è¹¤\n",
    "4. **è¨˜æ†¶é«”æ•ˆç‡é«˜**: å¤šä»»å‹™éƒ¨ç½²åƒ…å¢åŠ å°‘é‡è¨˜æ†¶é«”é–‹éŠ·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¼‰å…¥è¨“ç·´å¥½çš„ Prompt Tuning æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹èˆ‡é©é…å™¨è·¯å¾‘\n",
    "base_model_name = \"t5-small\"\n",
    "adapter_path = \"./t5-prompt-tuning-billsum\"  # å‡è¨­é€™æ˜¯è¨“ç·´å¥½çš„é©é…å™¨è·¯å¾‘\n",
    "\n",
    "# è¼‰å…¥åŸºç¤æ¨¡å‹èˆ‡åˆ†è©å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float32,  # T5 é€šå¸¸ä½¿ç”¨ FP32\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"åŸºç¤æ¨¡å‹åƒæ•¸é‡: {base_model.num_parameters():,}\")\n",
    "print(f\"åŸºç¤æ¨¡å‹è¨˜æ†¶é«”å ç”¨: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ Prompt Tuning é©é…å™¨\n",
    "if os.path.exists(adapter_path):\n",
    "    # å¾æª¢æŸ¥é»è¼‰å…¥æœ€æ–°çš„é©é…å™¨\n",
    "    checkpoints = [d for d in os.listdir(adapter_path) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(\n",
    "            [os.path.join(adapter_path, d) for d in checkpoints],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        print(f\"è¼‰å…¥æœ€æ–°æª¢æŸ¥é»: {latest_checkpoint}\")\n",
    "        peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
    "    else:\n",
    "        # ç›´æ¥å¾ç›®éŒ„è¼‰å…¥\n",
    "        peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    print(f\"âœ… æˆåŠŸè¼‰å…¥ Prompt Tuning é©é…å™¨: {adapter_path}\")\n",
    "else:\n",
    "    # è‹¥ç„¡è¨“ç·´å¥½çš„é©é…å™¨,å‰µå»ºç¤ºç¯„é…ç½®\n",
    "    print(\"æœªæ‰¾åˆ°è¨“ç·´å¥½çš„é©é…å™¨,å‰µå»ºç¤ºç¯„ Prompt Tuning é…ç½®...\")\n",
    "    \n",
    "    prompt_config = PromptTuningConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        prompt_tuning_init=\"TEXT\",\n",
    "        prompt_tuning_init_text=\"Summarize the following text:\",\n",
    "        num_virtual_tokens=8,\n",
    "        tokenizer_name_or_path=base_model_name\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(base_model, prompt_config)\n",
    "    print(\"å·²å‰µå»ºç¤ºç¯„ Prompt Tuning æ¨¡å‹\")\n",
    "\n",
    "# é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸çµ±è¨ˆ\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è»Ÿæç¤ºçµæ§‹åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_soft_prompts(model):\n",
    "    \"\"\"åˆ†æè»Ÿæç¤ºçµæ§‹èˆ‡åƒæ•¸\"\"\"\n",
    "    prompt_params = {}\n",
    "    total_prompt_params = 0\n",
    "    \n",
    "    print(\"=== è»Ÿæç¤ºåƒæ•¸åˆ†æ ===\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prompt' in name.lower():\n",
    "            prompt_params[name] = {\n",
    "                'shape': param.shape,\n",
    "                'dtype': param.dtype,\n",
    "                'numel': param.numel()\n",
    "            }\n",
    "            total_prompt_params += param.numel()\n",
    "            \n",
    "            print(f\"\\nè»Ÿæç¤ºåƒæ•¸: {name}\")\n",
    "            print(f\"  å½¢ç‹€: {param.shape}\")\n",
    "            print(f\"  æ•¸æ“šé¡å‹: {param.dtype}\")\n",
    "            print(f\"  åƒæ•¸é‡: {param.numel():,}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    prompt_ratio = (total_prompt_params / total_params) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"è»Ÿæç¤ºç¸½åƒæ•¸é‡: {total_prompt_params:,}\")\n",
    "    print(f\"æ¨¡å‹ç¸½åƒæ•¸é‡: {total_params:,}\")\n",
    "    print(f\"è»Ÿæç¤ºå æ¯”: {prompt_ratio:.4f}%\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return prompt_params\n",
    "\n",
    "soft_prompt_structure = analyze_soft_prompts(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨ç†æ•ˆèƒ½åŸºæº–æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_summarization(model, tokenizer, test_samples, num_runs=3):\n",
    "    \"\"\"æ‘˜è¦ç”Ÿæˆæ•ˆèƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for sample in test_samples:\n",
    "                # æº–å‚™è¼¸å…¥\n",
    "                prompt = \"summarize: \" + sample['text'][:1024]  # é™åˆ¶è¼¸å…¥é•·åº¦\n",
    "                inputs = tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=1024,\n",
    "                    truncation=True\n",
    "                )\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                # ç”Ÿæˆæ‘˜è¦\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    max_length=150,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                \n",
    "                generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                if run == 0:  # åƒ…åœ¨ç¬¬ä¸€è¼ªä¿å­˜çµæœ\n",
    "                    results.append({\n",
    "                        'original': sample['text'][:200] + '...',\n",
    "                        'reference': sample['summary'],\n",
    "                        'generated': generated_summary\n",
    "                    })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    return avg_time, results\n",
    "\n",
    "# è¼‰å…¥æ¸¬è©¦æ¨£æœ¬\n",
    "print(\"è¼‰å…¥æ¸¬è©¦æ•¸æ“šé›†...\")\n",
    "test_dataset = load_dataset(\"billsum\", split=\"test[:5]\")\n",
    "\n",
    "print(\"\\n=== æ¨ç†æ•ˆèƒ½æ¸¬è©¦ ===\")\n",
    "inference_time, inference_results = benchmark_summarization(\n",
    "    peft_model,\n",
    "    tokenizer,\n",
    "    test_dataset\n",
    ")\n",
    "\n",
    "print(f\"å¹³å‡æ¨ç†æ™‚é–“: {inference_time:.4f} ç§’ (5 å€‹æ¨£æœ¬)\")\n",
    "print(f\"æ¯å€‹æ¨£æœ¬å¹³å‡æ™‚é–“: {inference_time / len(test_dataset):.4f} ç§’\")\n",
    "print(f\"è¨˜æ†¶é«”å ç”¨: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤ºç”Ÿæˆç¯„ä¾‹\n",
    "print(\"\\n=== æ‘˜è¦ç”Ÿæˆç¯„ä¾‹ ===\")\n",
    "for i, result in enumerate(inference_results[:2]):  # åƒ…é¡¯ç¤ºå‰å…©å€‹ç¯„ä¾‹\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ç¯„ä¾‹ {i+1}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"åŸå§‹æ–‡æœ¬: {result['original']}\")\n",
    "    print(f\"\\nåƒè€ƒæ‘˜è¦: {result['reference']}\")\n",
    "    print(f\"\\nç”Ÿæˆæ‘˜è¦: {result['generated']}\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è»Ÿæç¤ºå„²å­˜èˆ‡ç‰ˆæœ¬ç®¡ç†\n",
    "\n",
    "### 6.1 è»Ÿæç¤ºçš„ç¨ç«‹å„²å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šå„²å­˜è·¯å¾‘\n",
    "save_path = \"./prompt-tuning-deployed\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "print(f\"=== å„²å­˜ Prompt Tuning è»Ÿæç¤ºåˆ°: {save_path} ===\")\n",
    "\n",
    "# å„²å­˜è»Ÿæç¤ºé©é…å™¨(ä¸åŒ…å«åŸºç¤æ¨¡å‹)\n",
    "peft_model.save_pretrained(save_path)\n",
    "\n",
    "print(\"âœ… è»Ÿæç¤ºå„²å­˜å®Œæˆ!\")\n",
    "\n",
    "# æª¢æŸ¥å„²å­˜çš„æª”æ¡ˆ\n",
    "saved_files = os.listdir(save_path)\n",
    "print(f\"\\nå„²å­˜çš„æª”æ¡ˆ: {saved_files}\")\n",
    "\n",
    "# è¨ˆç®—è»Ÿæç¤ºæª”æ¡ˆå¤§å°\n",
    "total_size = 0\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(save_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        total_size += size\n",
    "        print(f\"  {file}: {size / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nè»Ÿæç¤ºç¸½å¤§å°: {total_size / 1024:.2f} KB\")\n",
    "print(f\"åŸºç¤æ¨¡å‹å¤§å°(åƒè€ƒ): ~242 MB (t5-small)\")\n",
    "print(f\"å„²å­˜æ•ˆç‡: {(total_size / (242 * 1024 * 1024)) * 100:.4f}% çš„åŸºç¤æ¨¡å‹å¤§å°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 è»Ÿæç¤ºé…ç½®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®€å–ä¸¦é¡¯ç¤ºè»Ÿæç¤ºé…ç½®\n",
    "config_path = os.path.join(save_path, \"adapter_config.json\")\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        prompt_config = json.load(f)\n",
    "    \n",
    "    print(\"=== Prompt Tuning é…ç½®è©³æƒ… ===\")\n",
    "    print(json.dumps(prompt_config, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    print(f\"\\né—œéµé…ç½®åƒæ•¸:\")\n",
    "    print(f\"  ä»»å‹™é¡å‹: {prompt_config.get('task_type', 'N/A')}\")\n",
    "    print(f\"  è»Ÿæç¤ºé•·åº¦: {prompt_config.get('num_virtual_tokens', 'N/A')} tokens\")\n",
    "    print(f\"  åˆå§‹åŒ–æ–¹æ³•: {prompt_config.get('prompt_tuning_init', 'N/A')}\")\n",
    "    print(f\"  åˆå§‹åŒ–æ–‡æœ¬: {prompt_config.get('prompt_tuning_init_text', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¤šä»»å‹™è»Ÿæç¤ºç®¡ç†èˆ‡å‹•æ…‹åˆ‡æ›\n",
    "\n",
    "### 7.1 æ¨¡æ“¬å¤šä»»å‹™å ´æ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskPromptManager:\n",
    "    \"\"\"\n",
    "    å¤šä»»å‹™è»Ÿæç¤ºç®¡ç†å™¨\n",
    "    \n",
    "    å¯¦ç¾åŠŸèƒ½:\n",
    "    - ç®¡ç†å¤šå€‹ä»»å‹™çš„è»Ÿæç¤º\n",
    "    - å‹•æ…‹åˆ‡æ›ä»»å‹™\n",
    "    - è¨˜æ†¶é«”æ•ˆç‡å„ªåŒ–\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, tokenizer):\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_prompts = {}  # å„²å­˜ä»»å‹™ç‰¹å®šçš„è»Ÿæç¤ºè·¯å¾‘\n",
    "        self.current_task = None\n",
    "        self.current_model = None\n",
    "    \n",
    "    def register_task(self, task_name, adapter_path):\n",
    "        \"\"\"è¨»å†Šæ–°ä»»å‹™çš„è»Ÿæç¤º\"\"\"\n",
    "        if os.path.exists(adapter_path):\n",
    "            self.task_prompts[task_name] = adapter_path\n",
    "            print(f\"âœ… å·²è¨»å†Šä»»å‹™ '{task_name}': {adapter_path}\")\n",
    "        else:\n",
    "            print(f\"âŒ ä»»å‹™è·¯å¾‘ä¸å­˜åœ¨: {adapter_path}\")\n",
    "    \n",
    "    def switch_task(self, task_name):\n",
    "        \"\"\"åˆ‡æ›åˆ°æŒ‡å®šä»»å‹™\"\"\"\n",
    "        if task_name not in self.task_prompts:\n",
    "            raise ValueError(f\"ä»»å‹™ '{task_name}' å°šæœªè¨»å†Š\")\n",
    "        \n",
    "        # æ¸…ç†ç•¶å‰æ¨¡å‹(è‹¥å­˜åœ¨)\n",
    "        if self.current_model is not None:\n",
    "            del self.current_model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # è¼‰å…¥æ–°ä»»å‹™çš„è»Ÿæç¤º\n",
    "        adapter_path = self.task_prompts[task_name]\n",
    "        self.current_model = PeftModel.from_pretrained(\n",
    "            self.base_model,\n",
    "            adapter_path\n",
    "        )\n",
    "        self.current_task = task_name\n",
    "        \n",
    "        print(f\"ğŸ”„ å·²åˆ‡æ›åˆ°ä»»å‹™: {task_name}\")\n",
    "    \n",
    "    def generate(self, input_text, **generation_kwargs):\n",
    "        \"\"\"ä½¿ç”¨ç•¶å‰ä»»å‹™çš„è»Ÿæç¤ºç”Ÿæˆè¼¸å‡º\"\"\"\n",
    "        if self.current_model is None:\n",
    "            raise RuntimeError(\"å°šæœªé¸æ“‡ä»»å‹™,è«‹å…ˆèª¿ç”¨ switch_task()\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=1024,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.current_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                **generation_kwargs\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def list_tasks(self):\n",
    "        \"\"\"åˆ—å‡ºæ‰€æœ‰å·²è¨»å†Šçš„ä»»å‹™\"\"\"\n",
    "        print(\"\\n=== å·²è¨»å†Šçš„ä»»å‹™ ===\")\n",
    "        for task_name, adapter_path in self.task_prompts.items():\n",
    "            status = \"âœ“\" if task_name == self.current_task else \" \"\n",
    "            print(f\"[{status}] {task_name}: {adapter_path}\")\n",
    "\n",
    "# å‰µå»ºå¤šä»»å‹™ç®¡ç†å™¨ç¤ºç¯„\n",
    "manager = MultiTaskPromptManager(base_model, tokenizer)\n",
    "\n",
    "# è¨»å†Šä»»å‹™(å¯¦éš›ä½¿ç”¨æ™‚,é€™äº›æ‡‰è©²æ˜¯ä¸åŒä»»å‹™çš„è»Ÿæç¤º)\n",
    "manager.register_task(\"summarization\", save_path)\n",
    "# manager.register_task(\"translation\", \"./prompt-translation\")\n",
    "# manager.register_task(\"qa\", \"./prompt-qa\")\n",
    "\n",
    "manager.list_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 æ¸¬è©¦ä»»å‹™åˆ‡æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ‡æ›åˆ°æ‘˜è¦ä»»å‹™\n",
    "manager.switch_task(\"summarization\")\n",
    "\n",
    "# æ¸¬è©¦ç”Ÿæˆ\n",
    "test_text = \"summarize: \" + test_dataset[0]['text'][:500]\n",
    "generated = manager.generate(\n",
    "    test_text,\n",
    "    max_length=150,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== ä»»å‹™åˆ‡æ›æ¸¬è©¦ ===\")\n",
    "print(f\"ç•¶å‰ä»»å‹™: {manager.current_task}\")\n",
    "print(f\"\\nè¼¸å…¥æ–‡æœ¬: {test_dataset[0]['text'][:200]}...\")\n",
    "print(f\"\\nç”Ÿæˆæ‘˜è¦: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç”Ÿç”¢éƒ¨ç½²é…ç½®\n",
    "\n",
    "### 8.1 éƒ¨ç½²é…ç½®æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºéƒ¨ç½²é…ç½®æª”æ¡ˆ\n",
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"peft_method\": \"Prompt Tuning\",\n",
    "        \"task_type\": \"Seq2Seq Summarization\",\n",
    "        \"mergeable\": False,\n",
    "        \"soft_prompt_tokens\": prompt_config.get('num_virtual_tokens', 8),\n",
    "        \"soft_prompt_size_kb\": total_size / 1024\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"avg_inference_time_seconds\": inference_time / len(test_dataset),\n",
    "        \"memory_usage_mb\": peft_model.get_memory_footprint() / 1024**2,\n",
    "        \"soft_prompt_overhead\": \"Minimal (input length reduction)\"\n",
    "    },\n",
    "    \"deployment_strategy\": {\n",
    "        \"type\": \"Soft Prompt + Base Model\",\n",
    "        \"base_model_loading\": \"Load once, reuse for multiple tasks\",\n",
    "        \"task_switching\": \"Dynamic soft prompt swapping\",\n",
    "        \"multi_task_support\": True\n",
    "    },\n",
    "    \"deployment_requirements\": {\n",
    "        \"python_packages\": [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.20.0\",\n",
    "            \"peft>=0.3.0\"\n",
    "        ],\n",
    "        \"minimum_gpu_memory_gb\": 4,\n",
    "        \"recommended_gpu_memory_gb\": 8\n",
    "    },\n",
    "    \"usage_example\": {\n",
    "        \"load_base_model\": f\"AutoModelForSeq2SeqLM.from_pretrained('{base_model_name}')\",\n",
    "        \"load_soft_prompt\": f\"PeftModel.from_pretrained(base_model, '{save_path}')\",\n",
    "        \"inference_note\": \"Add task-specific prefix (e.g., 'summarize:') to input\"\n",
    "    },\n",
    "    \"best_practices\": {\n",
    "        \"multi_task_deployment\": \"Load base model once, swap soft prompts for different tasks\",\n",
    "        \"version_control\": \"Store soft prompts separately for easy A/B testing\",\n",
    "        \"scaling\": \"Base model can be shared across multiple task endpoints\",\n",
    "        \"monitoring\": \"Track soft prompt switching frequency and cache hit rate\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# å„²å­˜é…ç½®æª”æ¡ˆ\n",
    "config_output_path = os.path.join(save_path, \"deployment_config.json\")\n",
    "with open(config_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=== ç”Ÿç”¢éƒ¨ç½²é…ç½® ===\")\n",
    "print(json.dumps(deployment_config, indent=2, ensure_ascii=False))\n",
    "print(f\"\\nâœ… é…ç½®æª”æ¡ˆå·²å„²å­˜åˆ°: {config_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 éƒ¨ç½²æ¶æ§‹åœ–\n",
    "\n",
    "```\n",
    "ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æ¶æ§‹ (å¤šä»»å‹™å ´æ™¯)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    API Gateway                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚        â”‚        â”‚\n",
    "    â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”\n",
    "    â”‚Task Aâ”‚ â”‚Task Bâ”‚ â”‚Task Câ”‚\n",
    "    â”‚ (æ‘˜è¦)â”‚ â”‚(ç¿»è­¯)â”‚ â”‚ (QA) â”‚\n",
    "    â””â”€â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”€â”€â”˜\n",
    "        â”‚        â”‚       â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Soft Prompt Selector     â”‚  â† å‹•æ…‹åˆ‡æ›è»Ÿæç¤º\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚  Prompt A  â”‚ Prompt B â”‚ C â”‚  (åƒ…å¹¾ KB å¤§å°)\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚    T5 Base Model          â”‚  â† å…±äº«åŸºç¤æ¨¡å‹\n",
    "    â”‚    (åƒ…è¼‰å…¥ä¸€æ¬¡)            â”‚  (242 MB)\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "å„ªå‹¢:\n",
    "âœ… å–®ä¸€åŸºç¤æ¨¡å‹æ”¯æ´å¤šä»»å‹™\n",
    "âœ… è»Ÿæç¤ºåˆ‡æ›æˆæœ¬æ¥µä½ (<1ms)\n",
    "âœ… ç¸½è¨˜æ†¶é«”å ç”¨ â‰ˆ 1å€‹åŸºç¤æ¨¡å‹ + Nå€‹è»Ÿæç¤º\n",
    "âœ… æ˜“æ–¼ A/B æ¸¬è©¦èˆ‡ç‰ˆæœ¬è¿­ä»£\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prompt Tuning éƒ¨ç½²æœ€ä½³å¯¦è¸\n",
    "\n",
    "### 9.1 ä½•æ™‚é¸æ“‡ Prompt Tuning éƒ¨ç½²ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prompt Tuning éƒ¨ç½²å ´æ™¯æ¨è–¦ ===\")\n",
    "print(\"\")\n",
    "print(\"ğŸ¯ æœ€é©åˆå ´æ™¯:\")\n",
    "print(\"  â€¢ éœ€è¦æ”¯æ´å¤šå€‹ç›¸ä¼¼ä»»å‹™(å…±äº«åŸºç¤æ¨¡å‹)\")\n",
    "print(\"  â€¢ é »ç¹çš„æ¨¡å‹ç‰ˆæœ¬è¿­ä»£èˆ‡ A/B æ¸¬è©¦\")\n",
    "print(\"  â€¢ ä½¿ç”¨è¶…å¤§è¦æ¨¡æ¨¡å‹(>10B åƒæ•¸,è¦æ¨¡æ•ˆæ‡‰é¡¯è‘—)\")\n",
    "print(\"  â€¢ éœ€è¦å¿«é€Ÿéƒ¨ç½²æ–°ä»»å‹™èƒ½åŠ›\")\n",
    "print(\"  â€¢ å„²å­˜èˆ‡å‚³è¼¸æˆæœ¬æ•æ„Ÿ(è»Ÿæç¤ºåƒ…å¹¾ KB)\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸ éœ€è¦è¬¹æ…è€ƒæ…®çš„å ´æ™¯:\")\n",
    "print(\"  â€¢ å–®ä¸€ä»»å‹™ä¸”å°æ¨ç†å»¶é²æ¥µå…¶æ•æ„Ÿ(è»Ÿæç¤ºæœƒç•¥å¾®å¢åŠ è¼¸å…¥é•·åº¦)\")\n",
    "print(\"  â€¢ å°å‹æ¨¡å‹(<1B åƒæ•¸,è¦æ¨¡æ•ˆæ‡‰ä¸æ˜é¡¯)\")\n",
    "print(\"  â€¢ ä»»å‹™éœ€è¦å¤§å¹…ä¿®æ”¹æ¨¡å‹å…§éƒ¨è¡¨ç¤º\")\n",
    "print(\"  â€¢ éœ€è¦æ¥µè‡´çš„æ¨ç†æ•ˆèƒ½å„ªåŒ–(è€ƒæ…® LoRA/IAÂ³ åˆä½µ)\")\n",
    "print(\"\")\n",
    "print(\"ğŸ”„ èˆ‡å…¶ä»–æ–¹æ³•çš„çµ„åˆç­–ç•¥:\")\n",
    "print(\"  â€¢ Prompt Tuning + LoRA: å¤§æ¨¡å‹ç”¨ Prompt Tuning,å°æ¨¡å‹ç”¨ LoRA\")\n",
    "print(\"  â€¢ å…ˆç”¨ Prompt Tuning å¿«é€Ÿé©—è­‰,å†ç”¨å…¶ä»–æ–¹æ³•ç²¾èª¿\")\n",
    "print(\"  â€¢ ä¸åŒå±¤ä½¿ç”¨ä¸åŒ PEFT æ–¹æ³•(æ··åˆç­–ç•¥)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 éƒ¨ç½²æª¢æ ¸æ¸…å–®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prompt Tuning éƒ¨ç½²æª¢æ ¸æ¸…å–® ===\")\n",
    "print(\"\")\n",
    "print(\"ğŸ“‹ éƒ¨ç½²å‰æª¢æŸ¥:\")\n",
    "print(\"  â–¡ ç¢ºèªè»Ÿæç¤ºè¨“ç·´å®Œæˆä¸¦å„²å­˜\")\n",
    "print(\"  â–¡ åœ¨é©—è­‰é›†ä¸Šé©—è­‰è»Ÿæç¤ºæ•ˆæœ\")\n",
    "print(\"  â–¡ è¨˜éŒ„è»Ÿæç¤ºé…ç½®(é•·åº¦ã€åˆå§‹åŒ–æ–¹æ³•ç­‰)\")\n",
    "print(\"  â–¡ æ¸¬è©¦åŸºç¤æ¨¡å‹ + è»Ÿæç¤ºçš„è¼‰å…¥æµç¨‹\")\n",
    "print(\"\")\n",
    "print(\"ğŸ”„ å¤šä»»å‹™éƒ¨ç½²:\")\n",
    "print(\"  â–¡ è¨­è¨ˆè»Ÿæç¤ºå‘½åèˆ‡ç‰ˆæœ¬ç®¡ç†è¦ç¯„\")\n",
    "print(\"  â–¡ å¯¦ç¾è»Ÿæç¤ºå‹•æ…‹è¼‰å…¥èˆ‡åˆ‡æ›æ©Ÿåˆ¶\")\n",
    "print(\"  â–¡ é…ç½®è»Ÿæç¤ºå¿«å–ç­–ç•¥(ç†±é–€ä»»å‹™å¸¸é§è¨˜æ†¶é«”)\")\n",
    "print(\"  â–¡ ç›£æ§ä»»å‹™åˆ‡æ›é »ç‡èˆ‡æ•ˆèƒ½å½±éŸ¿\")\n",
    "print(\"\")\n",
    "print(\"âœ… éƒ¨ç½²å¾Œé©—è­‰:\")\n",
    "print(\"  â–¡ æ¨ç†é€Ÿåº¦æ¸¬è©¦\")\n",
    "print(\"  â–¡ è¨˜æ†¶é«”å ç”¨ç›£æ§\")\n",
    "print(\"  â–¡ è¼¸å‡ºå“è³ªæŠ½æ¨£æª¢æŸ¥\")\n",
    "print(\"  â–¡ é•·æ™‚é–“ç©©å®šæ€§æ¸¬è©¦\")\n",
    "print(\"  â–¡ è»Ÿæç¤ºåˆ‡æ›æ­£ç¢ºæ€§é©—è­‰(å¤šä»»å‹™å ´æ™¯)\")\n",
    "print(\"\")\n",
    "print(\"ğŸ“š æ–‡ä»¶èˆ‡ç›£æ§:\")\n",
    "print(\"  â–¡ å„²å­˜éƒ¨ç½²é…ç½®æª”æ¡ˆ\")\n",
    "print(\"  â–¡ è¨˜éŒ„æ•ˆèƒ½åŸºæº–æ•¸æ“š\")\n",
    "print(\"  â–¡ æº–å‚™å›æ»¾æ–¹æ¡ˆ(ä¿ç•™å¤šç‰ˆæœ¬è»Ÿæç¤º)\")\n",
    "print(\"  â–¡ æ›´æ–° API æ–‡ä»¶èˆ‡ä½¿ç”¨ç¯„ä¾‹\")\n",
    "print(\"  â–¡ è¨­ç½®è»Ÿæç¤ºç‰ˆæœ¬è¿½è¹¤ç³»çµ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 æ•…éšœæ’é™¤æŒ‡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prompt Tuning å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ ===\")\n",
    "print(\"\")\n",
    "print(\"â“ å•é¡Œ 1: è¼‰å…¥è»Ÿæç¤ºæ™‚å‡ºç¾ç¶­åº¦ä¸åŒ¹é…\")\n",
    "print(\"   åŸå› : è»Ÿæç¤ºè¨“ç·´æ™‚çš„åŸºç¤æ¨¡å‹ç‰ˆæœ¬èˆ‡éƒ¨ç½²æ™‚ä¸ä¸€è‡´\")\n",
    "print(\"   è§£æ±º: ç¢ºä¿ä½¿ç”¨å®Œå…¨ç›¸åŒçš„åŸºç¤æ¨¡å‹(åç¨±èˆ‡ç‰ˆæœ¬)\")\n",
    "print(\"\")\n",
    "print(\"â“ å•é¡Œ 2: æ¨ç†çµæœèˆ‡è¨“ç·´æ™‚å·®ç•°å¾ˆå¤§\")\n",
    "print(\"   åŸå› : å¯èƒ½ç¼ºå°‘ä»»å‹™å‰ç¶´(å¦‚ 'summarize:')\")\n",
    "print(\"   è§£æ±º: æª¢æŸ¥è¼¸å…¥æ ¼å¼,ç¢ºä¿åŒ…å«æ­£ç¢ºçš„ä»»å‹™å‰ç¶´\")\n",
    "print(\"\")\n",
    "print(\"â“ å•é¡Œ 3: å¤šä»»å‹™åˆ‡æ›å¾Œè¼¸å‡ºç•°å¸¸\")\n",
    "print(\"   åŸå› : è»Ÿæç¤ºæœªæ­£ç¢ºåˆ‡æ›æˆ–åŸºç¤æ¨¡å‹ç‹€æ…‹æ±¡æŸ“\")\n",
    "print(\"   è§£æ±º: ç¢ºä¿æ¯æ¬¡åˆ‡æ›éƒ½å®Œæ•´è¼‰å…¥æ–°è»Ÿæç¤º,æ¸…ç©º CUDA cache\")\n",
    "print(\"\")\n",
    "print(\"â“ å•é¡Œ 4: è¨˜æ†¶é«”å ç”¨æŒçºŒå¢é•·\")\n",
    "print(\"   åŸå› : å¤šæ¬¡è»Ÿæç¤ºåˆ‡æ›æœªé‡‹æ”¾èˆŠæ¨¡å‹\")\n",
    "print(\"   è§£æ±º: åœ¨åˆ‡æ›å‰åŸ·è¡Œ del model + gc.collect() + torch.cuda.empty_cache()\")\n",
    "print(\"\")\n",
    "print(\"â“ å•é¡Œ 5: ç”Ÿæˆå“è³ªä½æ–¼é æœŸ\")\n",
    "print(\"   åŸå› : å¯èƒ½ä½¿ç”¨äº†å°å‹åŸºç¤æ¨¡å‹(è¦æ¨¡æ•ˆæ‡‰ä¸è¶³)\")\n",
    "print(\"   è§£æ±º: è€ƒæ…®å‡ç´šåˆ°æ›´å¤§çš„åŸºç¤æ¨¡å‹(>1B),æˆ–æ”¹ç”¨ LoRA æ–¹æ³•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. PEFT æ–¹æ³•éƒ¨ç½²ç­–ç•¥å°æ¯”ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT æ–¹æ³•éƒ¨ç½²ç­–ç•¥å°æ¯”\n",
    "deployment_comparison = {\n",
    "    \"Prompt Tuning\": {\n",
    "        \"can_merge\": False,\n",
    "        \"deployment_unit\": \"Soft Prompt (å¹¾ KB) + Base Model\",\n",
    "        \"inference_overhead\": \"Input length increase\",\n",
    "        \"multi_task_support\": \"Excellent (dynamic switching)\",\n",
    "        \"deployment_complexity\": \"Medium\",\n",
    "        \"best_for\": \"Multi-task, large models, frequent updates\"\n",
    "    },\n",
    "    \"LoRA\": {\n",
    "        \"can_merge\": True,\n",
    "        \"deployment_unit\": \"Merged single model OR Adapter + Base Model\",\n",
    "        \"inference_overhead\": \"Zero (if merged)\",\n",
    "        \"multi_task_support\": \"Good (need reload)\",\n",
    "        \"deployment_complexity\": \"Simple (if merged)\",\n",
    "        \"best_for\": \"Single task, balanced efficiency and quality\"\n",
    "    },\n",
    "    \"IA3\": {\n",
    "        \"can_merge\": True,\n",
    "        \"deployment_unit\": \"Merged single model\",\n",
    "        \"inference_overhead\": \"Zero (fully merged)\",\n",
    "        \"multi_task_support\": \"Medium (need reload)\",\n",
    "        \"deployment_complexity\": \"Simple\",\n",
    "        \"best_for\": \"Extreme efficiency, inference-critical\"\n",
    "    },\n",
    "    \"Prefix Tuning\": {\n",
    "        \"can_merge\": False,\n",
    "        \"deployment_unit\": \"Prefix + Base Model\",\n",
    "        \"inference_overhead\": \"Attention computation increase\",\n",
    "        \"multi_task_support\": \"Good (dynamic switching)\",\n",
    "        \"deployment_complexity\": \"Complex\",\n",
    "        \"best_for\": \"Generation tasks, need expressiveness\"\n",
    "    },\n",
    "    \"AdapterLayers\": {\n",
    "        \"can_merge\": False,\n",
    "        \"deployment_unit\": \"Adapter Layers + Base Model\",\n",
    "        \"inference_overhead\": \"Bottleneck layer computation\",\n",
    "        \"multi_task_support\": \"Excellent (modular design)\",\n",
    "        \"deployment_complexity\": \"Medium\",\n",
    "        \"best_for\": \"Multi-task modular deployment\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== PEFT æ–¹æ³•éƒ¨ç½²ç­–ç•¥å°æ¯” ===\")\n",
    "print(f\"{'æ–¹æ³•':<15} {'å¯åˆä½µ':<10} {'éƒ¨ç½²å–®å…ƒ':<30} {'æ¨ç†é–‹éŠ·':<20} {'å¤šä»»å‹™æ”¯æ´':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for method, info in deployment_comparison.items():\n",
    "    mergeable = \"âœ…\" if info[\"can_merge\"] else \"âŒ\"\n",
    "    print(f\"{method:<15} {mergeable:<10} {info['deployment_unit']:<30} {info['inference_overhead']:<20} {info['multi_task_support']:<20}\")\n",
    "\n",
    "print(\"\\n=== Prompt Tuning ç¨ç‰¹éƒ¨ç½²å„ªå‹¢ ===\")\n",
    "print(\"âœ… å¤šä»»å‹™å…±äº«: ä¸€å€‹åŸºç¤æ¨¡å‹ + N å€‹è»Ÿæç¤º = N å€‹ä»»å‹™èƒ½åŠ›\")\n",
    "print(\"âœ… å‹•æ…‹åˆ‡æ›: è»Ÿæç¤ºåˆ‡æ›æˆæœ¬æ¥µä½(<1ms)\")\n",
    "print(\"âœ… ç‰ˆæœ¬ç®¡ç†: è»Ÿæç¤ºæª”æ¡ˆæ¥µå°,æ˜“æ–¼ç‰ˆæœ¬æ§åˆ¶èˆ‡å¯¦é©—è¿½è¹¤\")\n",
    "print(\"âœ… è¦æ¨¡å‹å¥½: å¤§æ¨¡å‹ä¸Šæ•ˆæœé€¼è¿‘å…¨åƒæ•¸å¾®èª¿,ä½†å„²å­˜æˆæœ¬æ¥µä½\")\n",
    "print(\"âœ… ç„¡éœ€åˆä½µ: ä¿æŒåŸºç¤æ¨¡å‹åŸå§‹ç‹€æ…‹,æ˜“æ–¼å›æ»¾èˆ‡æ›´æ–°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. å¯¦é©—ç¸½çµ\n",
    "\n",
    "### 11.1 æ ¸å¿ƒå­¸ç¿’æˆæœ\n",
    "\n",
    "é€šéæœ¬å¯¦é©—,æˆ‘å€‘æ·±å…¥æ¢ç´¢äº† **Prompt Tuning çš„ç¨ç‰¹éƒ¨ç½²ç‰¹æ€§**:\n",
    "\n",
    "1. **ç†è«–ç†è§£**: æŒæ¡äº†ç‚ºä½• Prompt Tuning ä¸éœ€è¦ä¹Ÿä¸æ‡‰è©²åˆä½µçš„åŸç†\n",
    "2. **å¯¦è¸ç¶“é©—**: å®Œæˆäº†è»Ÿæç¤ºçš„å„²å­˜ã€è¼‰å…¥ã€ç‰ˆæœ¬ç®¡ç†å…¨æµç¨‹\n",
    "3. **å¤šä»»å‹™ç®¡ç†**: å¯¦ç¾äº†åŸºæ–¼è»Ÿæç¤ºå‹•æ…‹åˆ‡æ›çš„å¤šä»»å‹™éƒ¨ç½²æ¶æ§‹\n",
    "4. **æœ€ä½³å¯¦è¸**: å­¸ç¿’äº†ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²çš„å®Œæ•´æª¢æ ¸æ¸…å–®èˆ‡æ•…éšœæ’é™¤ç­–ç•¥\n",
    "\n",
    "### 11.2 Prompt Tuning çš„ç¨ç‰¹åƒ¹å€¼\n",
    "\n",
    "- **å¤šä»»å‹™å‹å¥½**: å–®ä¸€åŸºç¤æ¨¡å‹æ”¯æ´ç„¡é™ä»»å‹™,åƒ…éœ€åˆ‡æ›è»Ÿæç¤º\n",
    "- **æ¥µè‡´å„²å­˜æ•ˆç‡**: æ¯å€‹ä»»å‹™åƒ…éœ€å¹¾ KB çš„è»Ÿæç¤ºæª”æ¡ˆ\n",
    "- **éƒ¨ç½²éˆæ´»æ€§**: ç„¡éœ€ä¿®æ”¹åŸºç¤æ¨¡å‹,æ˜“æ–¼ç‰ˆæœ¬ç®¡ç†èˆ‡å›æ»¾\n",
    "- **è¦æ¨¡é©…å‹•**: éš¨è‘—æ¨¡å‹è¦æ¨¡å¢å¤§,æ•ˆæœé€æ­¥é€¼è¿‘å…¨åƒæ•¸å¾®èª¿\n",
    "\n",
    "### 11.3 éƒ¨ç½²ç­–ç•¥é¸æ“‡å»ºè­°\n",
    "\n",
    "| å ´æ™¯ | æ¨è–¦æ–¹æ³• | ç†ç”± |\n",
    "|------|---------|------|\n",
    "| **å¤šä»»å‹™ç³»çµ±** | **Prompt Tuning** | å…±äº«åŸºç¤æ¨¡å‹,å‹•æ…‹åˆ‡æ› |\n",
    "| **å–®ä»»å‹™é«˜æ•ˆèƒ½** | LoRA(åˆä½µ) æˆ– IAÂ³ | é›¶æ¨ç†é–‹éŠ· |\n",
    "| **é »ç¹æ›´æ–°è¿­ä»£** | Prompt Tuning | ç‰ˆæœ¬ç®¡ç†ç°¡å–® |\n",
    "| **æ¥µè‡´æ¨ç†é€Ÿåº¦** | IAÂ³(åˆä½µ) | å®Œå…¨ç„¡é–‹éŠ· |\n",
    "| **å¤§æ¨¡å‹(>10B)** | Prompt Tuning | è¦æ¨¡æ•ˆæ‡‰é¡¯è‘— |\n",
    "| **å°æ¨¡å‹(<1B)** | LoRA | è¡¨é”èƒ½åŠ›æ›´å¼· |\n",
    "\n",
    "Prompt Tuning å®Œç¾è©®é‡‹äº† **ã€Œç°¡æ½”è€Œä¸ç°¡å–®ã€** çš„è¨­è¨ˆå“²å­¸,åœ¨é©ç•¶å ´æ™¯ä¸‹,æ¥µç°¡çš„æ–¹æ³•å¾€å¾€å¸¶ä¾†æœ€å„ªé›…çš„è§£æ±ºæ–¹æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. è³‡æºæ¸…ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†å¯¦é©—è³‡æº\n",
    "print(\"=== æ¸…ç†å¯¦é©—è³‡æº ===\")\n",
    "\n",
    "# æ¸…ç†æ¨¡å‹\n",
    "if 'peft_model' in locals():\n",
    "    del peft_model\n",
    "if 'base_model' in locals():\n",
    "    del base_model\n",
    "if 'manager' in locals():\n",
    "    del manager\n",
    "\n",
    "# åƒåœ¾å›æ”¶\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"âœ… è³‡æºæ¸…ç†å®Œæˆ\")\n",
    "print(\"\\nğŸ‰ Lab 3 - Prompt Tuning éƒ¨ç½²æŒ‡å—å¯¦é©—å®Œæˆ!\")\n",
    "print(f\"è»Ÿæç¤ºå·²å„²å­˜åˆ°: {save_path}\")\n",
    "print(\"æ‚¨ç¾åœ¨å¯ä»¥å°‡è»Ÿæç¤ºèˆ‡åŸºç¤æ¨¡å‹åˆ†åˆ¥éƒ¨ç½²,äº«å—å¤šä»»å‹™å‹•æ…‹åˆ‡æ›çš„ä¾¿åˆ©!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
