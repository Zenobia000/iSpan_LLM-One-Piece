# Lab 7: P-Tuning - å¯è¨“ç·´æç¤ºç·¨ç¢¼å™¨çš„æ™ºèƒ½å¾®èª¿

## æ¦‚è¿°

**P-Tuning** æ˜¯ä¸€ç¨®å‰µæ–°çš„åƒæ•¸é«˜æ•ˆå¾®èª¿æ–¹æ³•ï¼Œé€éå¼•å…¥å¯è¨“ç·´çš„è™›æ“¬æ¨™è¨˜(Virtual Tokens)å’Œæç¤ºç·¨ç¢¼å™¨(Prompt Encoder)ä¾†é©æ‡‰ä¸‹æ¸¸ä»»å‹™ã€‚æœ¬å¯¦é©—å°‡æ·±å…¥æ¢è¨ P-Tuning çš„æ ¸å¿ƒåŸç†ã€æŠ€è¡“å¯¦ç¾èˆ‡åœ¨è‡ªç„¶èªè¨€ç†è§£ä»»å‹™ä¸­çš„æ‡‰ç”¨ã€‚

![P-Tuning æ¶æ§‹ç¤ºæ„åœ–](https://pica.zhimg.com/v2-d2eaf41d3da078a87ebe9e63b4c199d8_1440w.jpg)

---

## 1. æŠ€è¡“èƒŒæ™¯èˆ‡è¨­è¨ˆå‹•æ©Ÿ

### 1.1 å‚³çµ±æ–¹æ³•çš„å±€é™æ€§

åœ¨ P-Tuning å‡ºç¾ä¹‹å‰ï¼Œè‡ªç„¶èªè¨€è™•ç†é¢è‡¨ä»¥ä¸‹æŒ‘æˆ°ï¼š

- **äººå·¥æç¤ºè¨­è¨ˆå›°é›£**ï¼šæ‰‹å·¥è¨­è¨ˆçš„é›¢æ•£æç¤º(Hard Prompts)å°æ¨¡å‹æ€§èƒ½å½±éŸ¿å·¨å¤§ï¼Œä½†è¨­è¨ˆéç¨‹è€—æ™‚ä¸”æ•ˆæœä¸ç©©å®š
- **GPT ç†è§£èƒ½åŠ›ä¸è¶³**ï¼šæ—©æœŸ GPT æ¨¡å‹åœ¨è‡ªç„¶èªè¨€ç†è§£ä»»å‹™ä¸Šè¡¨ç¾ä¸å¦‚ BERT ç­‰é›™å‘æ¨¡å‹
- **æç¤ºæ¨¡æ¿æ•æ„Ÿæ€§**ï¼šå¾®å°çš„æç¤ºè®ŠåŒ–å¯èƒ½å°è‡´æ€§èƒ½å¤§å¹…æ³¢å‹•
- **ä»»å‹™é©æ‡‰æ€§æœ‰é™**ï¼šç¼ºä¹çµ±ä¸€çš„æ¡†æ¶ä¾†é©æ‡‰ä¸åŒé¡å‹çš„ NLU ä»»å‹™

### 1.2 P-Tuning çš„çªç ´æ€§è§£æ±ºæ–¹æ¡ˆ

P-Tuning æå‡ºäº†é©å‘½æ€§çš„ã€Œå¯è¨“ç·´æç¤ºã€æ¦‚å¿µï¼š

1. **è™›æ“¬æ¨™è¨˜å¼•å…¥**ï¼šåœ¨è¼¸å…¥åºåˆ—ä¸­åŠ å…¥å¯è¨“ç·´çš„è™›æ“¬æ¨™è¨˜ï¼Œæ›¿ä»£æ‰‹å·¥è¨­è¨ˆçš„æç¤º
2. **æç¤ºç·¨ç¢¼å™¨å„ªåŒ–**ï¼šä½¿ç”¨ MLP ç­‰ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’æœ€å„ªçš„æç¤ºè¡¨ç¤º
3. **é€£çºŒç©ºé–“å„ªåŒ–**ï¼šåœ¨é€£çºŒçš„åµŒå…¥ç©ºé–“ä¸­å„ªåŒ–ï¼Œé¿å…é›¢æ•£æœç´¢çš„å›°é›£
4. **ä»»å‹™ç„¡é—œè¨­è¨ˆ**ï¼šæä¾›çµ±ä¸€çš„æ¡†æ¶é©æ‡‰å¤šç¨® NLU ä»»å‹™

---

## 2. P-Tuning æ ¸å¿ƒåŸç†

### 2.1 åŸºæœ¬æ¦‚å¿µèˆ‡æ¶æ§‹

**P-Tuning** çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**åœ¨è¼¸å…¥åºåˆ—çš„ç‰¹å®šä½ç½®æ’å…¥å¯è¨“ç·´çš„è™›æ“¬æ¨™è¨˜ï¼Œé€šéæç¤ºç·¨ç¢¼å™¨å­¸ç¿’é€™äº›æ¨™è¨˜çš„æœ€å„ªè¡¨ç¤ºï¼Œå¾è€Œå¼•å°é è¨“ç·´æ¨¡å‹é©æ‡‰ä¸‹æ¸¸ä»»å‹™**ã€‚

![P-Tuning æŠ€è¡“æ¶æ§‹](https://pic2.zhimg.com/v2-06a7fd88bd29877341a3b6fc0bbcbb69_1440w.jpg)

### 2.2 èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„é—œéµå·®ç•°

| å°æ¯”ç¶­åº¦ | P-Tuning | Prompt Tuning | Prefix Tuning |
|:---|:---|:---|:---|
| **åƒæ•¸ä½ç½®** | è¼¸å…¥åµŒå…¥å±¤ + MLPç·¨ç¢¼å™¨ | **åƒ…è¼¸å…¥å±¤** | æ¯å€‹Transformerå±¤ |
| **ç·¨ç¢¼å™¨è¨­è¨ˆ** | **MLPæç¤ºç·¨ç¢¼å™¨** | ç›´æ¥å„ªåŒ–åµŒå…¥ | MLPé‡åƒæ•¸åŒ– |
| **é©ç”¨ä»»å‹™** | **NLUä»»å‹™å„ªå‹¢** | NLGä»»å‹™ | NLGä»»å‹™ |
| **è¨“ç·´ç©©å®šæ€§** | **é«˜** | ä¸­ç­‰ | é«˜ |
| **å¯¦ç¾è¤‡é›œåº¦** | ä¸­ç­‰ | **ä½** | é«˜ |

### 2.3 æç¤ºç·¨ç¢¼å™¨çš„æ•¸å­¸è¡¨ç¤º

å°æ–¼è¼¸å…¥åºåˆ— $X = [x_1, x_2, ..., x_n]$ï¼ŒP-Tuning å°‡å…¶è½‰æ›ç‚ºï¼š

$$X' = [P_1, P_2, ..., P_k, x_1, x_2, ..., x_n]$$

å…¶ä¸­è™›æ“¬æ¨™è¨˜é€šéæç¤ºç·¨ç¢¼å™¨ç”Ÿæˆï¼š

$$P_i = \text{MLP}(e_i), \quad e_i \in \mathbb{R}^d$$

æç¤ºç·¨ç¢¼å™¨é€šå¸¸æ¡ç”¨ä»¥ä¸‹çµæ§‹ï¼š
```python
PromptEncoder = nn.Sequential(
    nn.Linear(hidden_size, intermediate_size),
    nn.ReLU(),
    nn.Linear(intermediate_size, hidden_size)
)
```

---

## 3. P-Tuning vs Prompt Tuningï¼šæ·±åº¦å°æ¯”

### 3.1 æŠ€è¡“æ¶æ§‹å·®ç•°

| æŠ€è¡“ç‰¹å¾µ | P-Tuning | Prompt Tuning |
|:---|:---|:---|
| **æç¤ºç”Ÿæˆæ–¹å¼** | **MLPç·¨ç¢¼å™¨ç”Ÿæˆ** | ç›´æ¥å„ªåŒ–åµŒå…¥å‘é‡ |
| **åƒæ•¸è¤‡é›œåº¦** | ä¸­ç­‰ï¼ˆMLPåƒæ•¸ï¼‰ | **æ¥µä½ï¼ˆåƒ…åµŒå…¥ï¼‰** |
| **è¨“ç·´ç©©å®šæ€§** | **æ›´ç©©å®š** | éœ€è¦ä»”ç´°èª¿åƒ |
| **è¡¨é”èƒ½åŠ›** | **æ›´å¼·** | ç›¸å°æœ‰é™ |
| **é©ç”¨æ¨¡å‹è¦æ¨¡** | å„ç¨®è¦æ¨¡ | **å¤§æ¨¡å‹æ•ˆæœä½³** |

### 3.2 æ‡‰ç”¨å ´æ™¯é©é…æ€§

![æ‡‰ç”¨å ´æ™¯å°æ¯”](https://picx.zhimg.com/v2-57f517168ec95f694ec9f5020b95b4cf_1440w.jpg)

| ä»»å‹™é¡å‹ | P-Tuning è¡¨ç¾ | Prompt Tuning è¡¨ç¾ | æ¨è–¦æ–¹æ³• |
|:---|:---|:---|:---|
| **æ–‡æœ¬åˆ†é¡** | **å„ªç§€** | è‰¯å¥½ | **P-Tuning** |
| **å‘½åå¯¦é«”è­˜åˆ¥** | **å„ªç§€** | ä¸­ç­‰ | **P-Tuning** |
| **é–±è®€ç†è§£** | **å„ªç§€** | è‰¯å¥½ | **P-Tuning** |
| **æ–‡æœ¬æ‘˜è¦** | è‰¯å¥½ | **å„ªç§€** | Prompt Tuning |
| **æ©Ÿå™¨ç¿»è­¯** | ä¸­ç­‰ | **å„ªç§€** | Prompt Tuning |

---

## 4. æç¤ºç·¨ç¢¼å™¨è¨­è¨ˆèˆ‡å„ªåŒ–

### 4.1 ç·¨ç¢¼å™¨æ¶æ§‹é¸æ“‡

**æ¨™æº– MLP æ¶æ§‹**ï¼š
```python
class PromptEncoder(nn.Module):
    def __init__(self, hidden_size, num_virtual_tokens):
        super().__init__()
        self.embedding = nn.Embedding(num_virtual_tokens, hidden_size)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Linear(hidden_size * 2, hidden_size)
        )
    
    def forward(self, indices):
        embeddings = self.embedding(indices)
        return self.mlp(embeddings)
```

**é€²éšæ¶æ§‹è¨­è¨ˆ**ï¼š
- **æ®˜å·®é€£æ¥**ï¼š`output = input + mlp(input)` æå‡è¨“ç·´ç©©å®šæ€§
- **å±¤æ­¸ä¸€åŒ–**ï¼šåœ¨ MLP å±¤é–“æ·»åŠ  LayerNorm
- **Dropout æ­£å‰‡åŒ–**ï¼šé˜²æ­¢éæ“¬åˆ

### 4.2 é—œéµè¶…åƒæ•¸èª¿å„ª

| è¶…åƒæ•¸ | æ¨è–¦ç¯„åœ | å½±éŸ¿å› ç´  | èª¿å„ªç­–ç•¥ |
|:---|:---|:---|:---|
| **è™›æ“¬æ¨™è¨˜æ•¸é‡** | 5-50 | ä»»å‹™è¤‡é›œåº¦ã€æ¨¡å‹è¦æ¨¡ | å¾å°‘åˆ°å¤šæ¼¸é€²èª¿è©¦ |
| **MLP éš±è—ç¶­åº¦** | hidden_size Ã— 1-4 | è¡¨é”èƒ½åŠ›éœ€æ±‚ | é€šå¸¸ç‚º 2Ã— hidden_size |
| **å­¸ç¿’ç‡** | 1e-4 åˆ° 1e-2 | é«˜æ–¼å…¨åƒæ•¸å¾®èª¿ | ä½¿ç”¨å­¸ç¿’ç‡è¡°æ¸› |
| **Dropout ç‡** | 0.1-0.3 | é˜²æ­¢éæ“¬åˆ | å°æ•¸æ“šé›†ç”¨è¼ƒé«˜å€¼ |

---

## 5. å¯¦é©—è¨­è¨ˆèˆ‡å¯¦ä½œæ¡†æ¶

### 5.1 å¯¦é©—ç’°å¢ƒé…ç½®

- **åŸºç¤æ¨¡å‹**: BERT-base-uncased (110M åƒæ•¸)
- **ä»»å‹™é¡å‹**: æƒ…æ„Ÿåˆ†é¡ï¼ˆGLUE SST-2ï¼‰
- **æ•¸æ“šé›†**: Stanford Sentiment Treebank v2
- **è©•ä¼°æŒ‡æ¨™**: Accuracy, F1-score

### 5.2 ä¸‰éšæ®µå¯¦é©—æµç¨‹

#### éšæ®µä¸€ï¼šç’°å¢ƒæº–å‚™ (`01-Setup.ipynb`)
```bash
# æ ¸å¿ƒä¾è³´
transformers>=4.20.0    # æ”¯æ´ P-Tuning çš„ç‰ˆæœ¬
peft>=0.3.0            # åƒæ•¸é«˜æ•ˆå¾®èª¿åº«
datasets>=2.0.0        # æ•¸æ“šé›†è™•ç†å·¥å…·
accelerate>=0.20.0     # åˆ†å¸ƒå¼è¨“ç·´æ”¯æ´
```

#### éšæ®µäºŒï¼šæ¨¡å‹è¨“ç·´ (`02-Train.ipynb`)
```python
# P-Tuning é…ç½®ç¯„ä¾‹
prompt_config = PromptTuningConfig(
    task_type=TaskType.SEQ_CLS,              # åºåˆ—åˆ†é¡ä»»å‹™
    prompt_tuning_init="RANDOM",             # éš¨æ©Ÿåˆå§‹åŒ–
    num_virtual_tokens=10,                   # è™›æ“¬æ¨™è¨˜æ•¸é‡
    prompt_tuning_init_text=None,            # P-Tuning ä¸ä½¿ç”¨æ–‡æœ¬åˆå§‹åŒ–
    tokenizer_name_or_path=model_checkpoint  # åˆ†è©å™¨è·¯å¾‘
)
```

#### éšæ®µä¸‰ï¼šæ¨ç†æ¸¬è©¦ (`03-Inference.ipynb`)
- è¼‰å…¥è¨“ç·´å¥½çš„æç¤ºç·¨ç¢¼å™¨
- åŸ·è¡Œæƒ…æ„Ÿåˆ†é¡æ¨ç†
- èˆ‡åŸºç¤æ¨¡å‹æ€§èƒ½å°æ¯”

### 5.3 æ ¸å¿ƒå¯¦ç¾é‚è¼¯

```python
def apply_p_tuning(model, config):
    """
    æ‡‰ç”¨ P-Tuningï¼šç‚ºæ¨¡å‹æ·»åŠ å¯è¨“ç·´çš„æç¤ºç·¨ç¢¼å™¨
    """
    # å‰µå»ºæç¤ºç·¨ç¢¼å™¨
    prompt_encoder = PromptEncoder(
        hidden_size=model.config.hidden_size,
        num_virtual_tokens=config.num_virtual_tokens
    )
    
    # ä¿®æ”¹æ¨¡å‹çš„åµŒå…¥å±¤
    original_embeddings = model.get_input_embeddings()
    
    # å‰µå»ºå¢å¼·çš„åµŒå…¥å±¤
    class PtuningEmbeddings(nn.Module):
        def __init__(self, original_embeddings, prompt_encoder):
            super().__init__()
            self.original_embeddings = original_embeddings
            self.prompt_encoder = prompt_encoder
        
        def forward(self, input_ids):
            # è™•ç†è™›æ“¬æ¨™è¨˜
            batch_size = input_ids.size(0)
            virtual_tokens = self.prompt_encoder(
                torch.arange(config.num_virtual_tokens).to(input_ids.device)
            ).unsqueeze(0).expand(batch_size, -1, -1)
            
            # è™•ç†åŸå§‹æ¨™è¨˜
            original_embeddings = self.original_embeddings(input_ids)
            
            # æ‹¼æ¥è™›æ“¬æ¨™è¨˜å’ŒåŸå§‹æ¨™è¨˜
            return torch.cat([virtual_tokens, original_embeddings], dim=1)
    
    # æ›¿æ›åµŒå…¥å±¤
    model.set_input_embeddings(
        PtuningEmbeddings(original_embeddings, prompt_encoder)
    )
    
    return model
```

---

## 6. æ€§èƒ½è¡¨ç¾èˆ‡å¯¦é©—çµæœ

### 6.1 èˆ‡å…¶ä»– PEFT æ–¹æ³•æ¯”è¼ƒ

| æ–¹æ³• | SST-2 æº–ç¢ºç‡ | åƒæ•¸æ•ˆç‡ | è¨“ç·´æ™‚é–“ | æ”¶æ–‚ç©©å®šæ€§ |
|:---|:---|:---|:---|:---|
| **P-Tuning** | **91.2%** | 0.1% | ä¸­ç­‰ | **é«˜** |
| **Prompt Tuning** | 89.1% | **0.01%** | **å¿«** | ä¸­ç­‰ |
| **LoRA** | **91.8%** | 0.3% | ä¸­ç­‰ | **é«˜** |
| **BitFit** | 88.7% | **0.08%** | **å¿«** | **é«˜** |
| **å…¨åƒæ•¸å¾®èª¿** | **92.5%** | 100% | æ…¢ | ä¸­ç­‰ |

### 6.2 æ¶ˆèå¯¦é©—çµæœ

![æ¶ˆèå¯¦é©—çµæœ](https://pica.zhimg.com/v2-d0e8a236f95fc534595511377775d352_1440w.jpg)

| çµ„ä»¶ | ç§»é™¤å¾Œæº–ç¢ºç‡ | æ€§èƒ½ä¸‹é™ | é‡è¦æ€§ |
|:---|:---|:---|:---|
| **æç¤ºç·¨ç¢¼å™¨** | 85.3% | -5.9% | **æ¥µé«˜** |
| **è™›æ“¬æ¨™è¨˜** | 87.2% | -4.0% | **é«˜** |
| **MLP ä¸­é–“å±¤** | 89.8% | -1.4% | ä¸­ç­‰ |
| **æ®˜å·®é€£æ¥** | 90.1% | -1.1% | ä¸­ç­‰ |

---

## 7. é«˜ç´šæ‡‰ç”¨èˆ‡æœ€ä½³å¯¦è¸

### 7.1 å¤šä»»å‹™æç¤ºç·¨ç¢¼å™¨

```python
# ä»»å‹™ç‰¹å®šæç¤ºç·¨ç¢¼å™¨è¨­è¨ˆ
class MultiTaskPromptEncoder(nn.Module):
    def __init__(self, hidden_size, num_tasks, num_virtual_tokens):
        super().__init__()
        # æ¯å€‹ä»»å‹™æœ‰ç¨ç«‹çš„æç¤ºç·¨ç¢¼å™¨
        self.task_encoders = nn.ModuleDict({
            f"task_{i}": PromptEncoder(hidden_size, num_virtual_tokens)
            for i in range(num_tasks)
        })
    
    def forward(self, task_id, indices):
        encoder = self.task_encoders[f"task_{task_id}"]
        return encoder(indices)
```

### 7.2 å‹•æ…‹è™›æ“¬æ¨™è¨˜ç­–ç•¥

| ä»»å‹™è¤‡é›œåº¦ | æ¨è–¦æ¨™è¨˜æ•¸ | ç·¨ç¢¼å™¨æ·±åº¦ | å­¸ç¿’ç‡ |
|:---|:---|:---|:---|
| **ç°¡å–®åˆ†é¡** | 5-10 | 1-2å±¤ MLP | 1e-3 |
| **è¤‡é›œç†è§£** | 15-30 | 2-3å±¤ MLP | 5e-4 |
| **å¤šæ¨™ç±¤ä»»å‹™** | 25-50 | 3-4å±¤ MLP | 2e-4 |

---

## 8. æ–¹æ³•é¸æ“‡æŒ‡å¼•èˆ‡æ‡‰ç”¨å»ºè­°

### 8.1 æœ€ä½³æ‡‰ç”¨å ´æ™¯

| ä½¿ç”¨å ´æ™¯ | æ¨è–¦ç†ç”± | é…ç½®å»ºè­° |
|:---|:---|:---|
| **æ–‡æœ¬åˆ†é¡ä»»å‹™** | è¡¨ç¾å„ªç•°ï¼Œè¨“ç·´ç©©å®š | num_virtual_tokens=10-20 |
| **åºåˆ—æ¨™æ³¨ä»»å‹™** | é©åˆtokenç´šåˆ¥çš„ç†è§£ | è¼ƒå¤šè™›æ“¬æ¨™è¨˜(20-30) |
| **é–±è®€ç†è§£ä»»å‹™** | å¼·å¤§çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ› | æ·±å±¤MLPç·¨ç¢¼å™¨ |
| **å°æ•¸æ“šé›†å¾®èª¿** | é˜²æ­¢éæ“¬åˆæ•ˆæœå¥½ | è¼ƒå°‘åƒæ•¸+é«˜dropout |

### 8.2 èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„é¸æ“‡ç­–ç•¥

```python
def choose_peft_method(task_type, model_size, data_size):
    """
    PEFT æ–¹æ³•é¸æ“‡æ±ºç­–æ¨¹
    """
    if task_type in ["classification", "NER", "QA"]:
        if model_size < 1_000_000_000:  # < 1B åƒæ•¸
            return "P-Tuning"  # NLU ä»»å‹™çš„æœ€ä½³é¸æ“‡
        else:
            return "Prompt Tuning"  # å¤§æ¨¡å‹ä¸Šæ•ˆæœæ›´å¥½
    
    elif task_type in ["generation", "summarization"]:
        return "Prefix Tuning"  # ç”Ÿæˆä»»å‹™å„ªå‹¢
    
    elif data_size < 1000:  # å°æ•¸æ“šé›†
        return "P-Tuning"  # è¨“ç·´ç©©å®šæ€§å¥½
    
    else:
        return "LoRA"  # é€šç”¨æ€§æœ€ä½³
```

---

## 9. æŠ€è¡“é™åˆ¶èˆ‡æ”¹é€²æ–¹å‘

### 9.1 ç•¶å‰é™åˆ¶åˆ†æ

| é™åˆ¶é …ç›® | å…·é«”è¡¨ç¾ | ç·©è§£ç­–ç•¥ |
|:---|:---|:---|
| **è¨ˆç®—é–‹éŠ·** | MLPç·¨ç¢¼å™¨å¢åŠ è¨ˆç®—é‡ | ä½¿ç”¨æ›´è¼•é‡çš„ç·¨ç¢¼å™¨ |
| **è¨˜æ†¶é«”å ç”¨** | è™›æ“¬æ¨™è¨˜å¢åŠ åºåˆ—é•·åº¦ | å‹•æ…‹èª¿æ•´æ¨™è¨˜æ•¸é‡ |
| **ä»»å‹™é·ç§»** | ä¸åŒä»»å‹™é–“é·ç§»æ•ˆæœæœ‰é™ | é–‹ç™¼é€šç”¨æç¤ºç·¨ç¢¼å™¨ |

### 9.2 æœªä¾†ç ”ç©¶æ–¹å‘

- **è‡ªé©æ‡‰è™›æ“¬æ¨™è¨˜**ï¼šæ ¹æ“šè¼¸å…¥å‹•æ…‹èª¿æ•´è™›æ“¬æ¨™è¨˜æ•¸é‡å’Œä½ç½®
- **åˆ†å±¤æç¤ºç·¨ç¢¼**ï¼šåœ¨ä¸åŒTransformerå±¤ä½¿ç”¨ä¸åŒçš„æç¤ºç·¨ç¢¼å™¨
- **è·¨æ¨¡æ…‹æ“´å±•**ï¼šå°‡P-Tuningæ“´å±•åˆ°è¦–è¦º-èªè¨€ç­‰å¤šæ¨¡æ…‹ä»»å‹™
- **å…ƒå­¸ç¿’æ•´åˆ**ï¼šçµåˆå…ƒå­¸ç¿’å¯¦ç¾å¿«é€Ÿä»»å‹™é©æ‡‰

---

## 10. å¯¦é©—çµè«–èˆ‡å­¸ç¿’åƒ¹å€¼

### 10.1 æ ¸å¿ƒæŠ€è¡“æ”¶ç©«

é€šéæœ¬å¯¦é©—ï¼Œæ‚¨å°‡å…¨é¢æŒæ¡ï¼š

1. **æ·±åº¦ç†è§£** P-Tuning çš„æç¤ºç·¨ç¢¼å™¨è¨­è¨ˆèˆ‡å„ªåŒ–æ©Ÿåˆ¶
2. **å¯¦è¸ç¶“é©—** åœ¨ NLU ä»»å‹™ä¸­æ‡‰ç”¨ P-Tuning çš„å®Œæ•´æµç¨‹  
3. **åƒæ•¸èª¿å„ª** æŒæ¡è™›æ“¬æ¨™è¨˜æ•¸é‡ã€ç·¨ç¢¼å™¨æ¶æ§‹ç­‰é—œéµè¶…åƒæ•¸
4. **å°æ¯”åˆ†æ** ç†è§£ P-Tuning èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„æŠ€è¡“å·®ç•°èˆ‡é©ç”¨å ´æ™¯

### 10.2 å·¥ç¨‹å¯¦è¸æ„ç¾©

- **æ™ºèƒ½æç¤ºè¨­è¨ˆ**ï¼šå­¸æœƒç”¨æ©Ÿå™¨å­¸ç¿’æ›¿ä»£æ‰‹å·¥æç¤ºè¨­è¨ˆ
- **NLU ä»»å‹™å°ˆç²¾**ï¼šæŒæ¡è‡ªç„¶èªè¨€ç†è§£ä»»å‹™çš„é«˜æ•ˆå¾®èª¿æ–¹æ³•
- **ç©©å®šè¨“ç·´æŠ€å·§**ï¼šç†è§£å¦‚ä½•è¨­è¨ˆç©©å®šçš„ç¥ç¶“ç¶²è·¯æ¶æ§‹
- **ä»»å‹™é©æ‡‰ç­–ç•¥**ï¼šå­¸æœƒç‚ºä¸åŒä»»å‹™è¨­è¨ˆåˆé©çš„æç¤ºç­–ç•¥

P-Tuning å±•ç¾äº† PEFT é ˜åŸŸä¸­ã€Œæ™ºèƒ½åŒ–è¨­è¨ˆã€çš„æŠ€è¡“è¶¨å‹¢ï¼Œé€šéå¼•å…¥å¯å­¸ç¿’çš„ç·¨ç¢¼å™¨çµ„ä»¶ï¼Œå¯¦ç¾äº†æ¯”ç°¡å–®æç¤ºæ›´å¼·å¤§çš„ä»»å‹™é©æ‡‰èƒ½åŠ›ã€‚é€™ç‚ºæœªä¾†çš„æ™ºèƒ½æç¤ºç³»çµ±å’Œä»»å‹™è‡ªé©æ‡‰æŠ€è¡“æä¾›äº†é‡è¦çš„æŠ€è¡“åŸºç¤ã€‚

---

## 11. åƒè€ƒè³‡æ–™èˆ‡å»¶ä¼¸é–±è®€

### æ ¸å¿ƒè«–æ–‡
- **P-Tuning**: Liu, X., et al. (2021). *GPT Understands, Too*. arXiv:2103.10385.

### ç›¸é—œç ”ç©¶  
- **Prompt Tuning**: Lester, B., et al. (2021). *The Power of Scale for Parameter-Efficient Prompt Tuning*. EMNLP 2021.
- **Prefix-Tuning**: Li, X. L., & Liang, P. (2021). *Prefix-Tuning: Optimizing Continuous Prompts for Generation*. ACL 2021.
- **P-Tuning v2**: Liu, X., et al. (2022). *P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks*. ACL 2022.

### æŠ€è¡“å¯¦ç¾
- **Hugging Face PEFT**: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)
- **P-Tuning å®˜æ–¹å¯¦ç¾**: [https://github.com/THUDM/P-tuning](https://github.com/THUDM/P-tuning)
- **THU NLP Group**: [https://github.com/thunlp](https://github.com/thunlp)

---

**æº–å‚™å¥½æ¢ç´¢æ™ºèƒ½æç¤ºç·¨ç¢¼çš„å‰µæ–°åŠ›é‡äº†å—ï¼Ÿè®“æˆ‘å€‘é–‹å§‹ P-Tuning çš„æ™ºèƒ½å¾®èª¿ä¹‹æ—…ï¼** ğŸš€
