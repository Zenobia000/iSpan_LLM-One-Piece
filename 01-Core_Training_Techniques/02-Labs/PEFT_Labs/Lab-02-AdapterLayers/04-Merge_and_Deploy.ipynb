{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 2: Adapter Layers - Deployment and Production Best Practices\n",
    "\n",
    "## üéØ Experiment Objectives\n",
    "\n",
    "Unlike LoRA or IA¬≥, **Adapter Layers cannot be fully merged into the base model** because they introduce new network modules rather than reparameterizing existing weights. This Notebook demonstrates production deployment strategies and best practices for Adapter-based models.\n",
    "\n",
    "### Key Learning Points\n",
    "- Understand why Adapters cannot be merged (architectural differences)\n",
    "- Learn efficient deployment strategies for Adapter models\n",
    "- Implement multi-task Adapter management systems\n",
    "- Optimize inference performance with AdapterDrop techniques\n",
    "- Master production-ready Adapter deployment workflows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel, \n",
    "    AdapterConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Display library versions\n",
    "import transformers\n",
    "import peft\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PEFT version: {peft.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Why Adapters Cannot Be Merged\n",
    "\n",
    "### 2.1 Architectural Differences\n",
    "\n",
    "**LoRA (Mergeable)**:\n",
    "```python\n",
    "# LoRA: Reparameterization of existing weights\n",
    "W_new = W_original + B @ A  # Simple matrix addition\n",
    "```\n",
    "\n",
    "**Adapter (Non-Mergeable)**:\n",
    "```python\n",
    "# Adapter: New sequential modules\n",
    "output = layer(x) + adapter(layer(x))  # New computation path\n",
    "```\n",
    "\n",
    "### 2.2 Fundamental Differences\n",
    "\n",
    "| Aspect | LoRA/IA¬≥ | Adapter Layers |\n",
    "|:---|:---|:---|\n",
    "| **Method** | Parameter reparameterization | New module insertion |\n",
    "| **Computation** | Same forward pass | Additional forward pass |\n",
    "| **Merging** | ‚úÖ Fully mergeable | ‚ùå Structurally non-mergeable |\n",
    "| **Inference** | Zero overhead after merge | Always has overhead |\n",
    "| **Multi-task** | Requires model switching | Supports parallel adapters |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Adapter architecture\n",
    "class AdapterModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Adapter module to demonstrate non-mergeability\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, reduction_factor=16):\n",
    "        super().__init__()\n",
    "        bottleneck_dim = input_dim // reduction_factor\n",
    "        \n",
    "        # This is a NEW computation path, not a weight modification\n",
    "        self.down_project = nn.Linear(input_dim, bottleneck_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.up_project = nn.Linear(bottleneck_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Skip connection: output = input + adapter(input)\n",
    "        return x + self.up_project(self.activation(self.down_project(x)))\n",
    "\n",
    "# Demonstrate why merging is impossible\n",
    "print(\"=== Adapter Architecture Analysis ===\")\n",
    "adapter = AdapterModule(768, reduction_factor=16)\n",
    "print(f\"Adapter has {sum(p.numel() for p in adapter.parameters()):,} parameters\")\n",
    "print(\"\\nThis is a NEW module with its own forward pass.\")\n",
    "print(\"It cannot be 'folded' into existing linear layers like LoRA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Load Trained Adapter Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and adapter paths\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "adapter_path = \"./bert-adapters-mrpc\"\n",
    "\n",
    "# Load base model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "print(f\"Base model parameters: {base_model.num_parameters():,}\")\n",
    "print(f\"Base model memory: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adapter (if trained checkpoint exists)\n",
    "if os.path.exists(adapter_path):\n",
    "    # Find latest checkpoint\n",
    "    checkpoints = [d for d in os.listdir(adapter_path) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(\n",
    "            [os.path.join(adapter_path, d) for d in checkpoints],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
    "        print(f\"‚úÖ Loaded adapter from: {latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No trained checkpoint found, creating demo adapter...\")\n",
    "        adapter_config = AdapterConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=16\n",
    "        )\n",
    "        peft_model = get_peft_model(base_model, adapter_config)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Adapter path not found, creating demo adapter...\")\n",
    "    adapter_config = AdapterConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=16\n",
    "    )\n",
    "    peft_model = get_peft_model(base_model, adapter_config)\n",
    "\n",
    "# Display trainable parameters\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Move to device\n",
    "peft_model.to(device)\n",
    "peft_model.eval()\n",
    "print(f\"\\nAdapter model memory: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Deployment Strategy 1: Standard Adapter Deployment\n",
    "\n",
    "### 4.1 Save Adapter Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter for production deployment\n",
    "production_path = \"./bert-adapters-mrpc-production\"\n",
    "\n",
    "print(f\"=== Saving production-ready adapter to: {production_path} ===\")\n",
    "\n",
    "# Save adapter weights (only adapter parameters, not base model)\n",
    "peft_model.save_pretrained(production_path)\n",
    "tokenizer.save_pretrained(production_path)\n",
    "\n",
    "print(\"‚úÖ Adapter saved!\")\n",
    "\n",
    "# Check saved files\n",
    "saved_files = os.listdir(production_path)\n",
    "print(f\"\\nSaved files: {saved_files}\")\n",
    "\n",
    "# Calculate adapter size\n",
    "total_size = 0\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(production_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        total_size += size\n",
    "        if file.endswith(('.bin', '.safetensors')):\n",
    "            print(f\"  {file}: {size / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nTotal adapter size: {total_size / 1024**2:.2f} MB\")\n",
    "print(f\"Size reduction vs full model: {(1 - total_size / (base_model.get_memory_footprint())) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 4.2 Load Adapter in Production Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate production loading\n",
    "print(\"=== Simulating Production Environment Loading ===\")\n",
    "\n",
    "# Step 1: Load base model\n",
    "prod_base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=2\n",
    ")\n",
    "print(\"‚úÖ Base model loaded\")\n",
    "\n",
    "# Step 2: Load adapter on top\n",
    "prod_model = PeftModel.from_pretrained(prod_base_model, production_path)\n",
    "prod_tokenizer = AutoTokenizer.from_pretrained(production_path)\n",
    "print(\"‚úÖ Adapter loaded\")\n",
    "\n",
    "prod_model.to(device)\n",
    "prod_model.eval()\n",
    "\n",
    "print(f\"\\nProduction model ready!\")\n",
    "print(f\"Total parameters: {prod_model.num_parameters():,}\")\n",
    "print(f\"Memory usage: {prod_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### 4.3 Inference Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, test_samples, num_runs=10):\n",
    "    \"\"\"\n",
    "    Benchmark inference performance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for sentence1, sentence2 in test_samples:\n",
    "                inputs = tokenizer(\n",
    "                    sentence1, sentence2,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    throughput = len(test_samples) / avg_time\n",
    "    \n",
    "    return avg_time, throughput\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    (\"The company said the merger was subject to approval.\", \n",
    "     \"The company said the deal was subject to approval.\"),\n",
    "    (\"The cat sat on the mat.\", \n",
    "     \"The dog played in the garden.\"),\n",
    "    (\"I love this movie.\", \n",
    "     \"This film is amazing.\"),\n",
    "    (\"The weather is terrible today.\", \n",
    "     \"It's raining heavily outside.\")\n",
    "]\n",
    "\n",
    "print(\"=== Adapter Model Inference Performance ===\")\n",
    "avg_time, throughput = benchmark_inference(prod_model, prod_tokenizer, test_samples)\n",
    "print(f\"Average time per batch: {avg_time:.4f} seconds\")\n",
    "print(f\"Throughput: {throughput:.2f} samples/sec\")\n",
    "print(f\"Latency per sample: {avg_time / len(test_samples) * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Deployment Strategy 2: Multi-Task Adapter System\n",
    "\n",
    "### 5.1 Multi-Task Adapter Manager Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskAdapterManager:\n",
    "    \"\"\"\n",
    "    Advanced multi-task adapter management system\n",
    "    Supports dynamic task switching and efficient memory management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, max_memory_mb=2000):\n",
    "        self.base_model = base_model\n",
    "        self.max_memory = max_memory_mb\n",
    "        self.active_adapters = {}\n",
    "        self.adapter_cache = {}\n",
    "        self.usage_stats = defaultdict(int)\n",
    "        \n",
    "    def register_adapter(self, task_name, adapter_path, priority=1.0):\n",
    "        \"\"\"\n",
    "        Register a new task adapter\n",
    "        \"\"\"\n",
    "        # Check memory constraints\n",
    "        current_memory = self._estimate_memory_usage()\n",
    "        \n",
    "        if current_memory > self.max_memory:\n",
    "            self._evict_adapters(priority)\n",
    "        \n",
    "        # Load adapter\n",
    "        adapter_model = PeftModel.from_pretrained(self.base_model, adapter_path)\n",
    "        \n",
    "        self.active_adapters[task_name] = {\n",
    "            'model': adapter_model,\n",
    "            'priority': priority,\n",
    "            'last_used': time.time(),\n",
    "            'path': adapter_path\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Registered adapter for task: {task_name}\")\n",
    "        \n",
    "    def switch_task(self, task_name):\n",
    "        \"\"\"\n",
    "        Switch to a specific task\n",
    "        \"\"\"\n",
    "        if task_name not in self.active_adapters:\n",
    "            raise ValueError(f\"Task '{task_name}' not registered\")\n",
    "        \n",
    "        # Update usage statistics\n",
    "        self.active_adapters[task_name]['last_used'] = time.time()\n",
    "        self.usage_stats[task_name] += 1\n",
    "        \n",
    "        return self.active_adapters[task_name]['model']\n",
    "    \n",
    "    def _estimate_memory_usage(self):\n",
    "        \"\"\"\n",
    "        Estimate current memory usage\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for task_info in self.active_adapters.values():\n",
    "            total += task_info['model'].get_memory_footprint() / 1024**2\n",
    "        return total\n",
    "    \n",
    "    def _evict_adapters(self, required_priority):\n",
    "        \"\"\"\n",
    "        Evict low-priority adapters to free memory\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for task_name, info in self.active_adapters.items():\n",
    "            if info['priority'] < required_priority:\n",
    "                # Score: priority * recency\n",
    "                score = info['priority'] * (time.time() - info['last_used'])\n",
    "                candidates.append((task_name, score))\n",
    "        \n",
    "        # Sort by score (lower = evict first)\n",
    "        candidates.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Evict until memory constraint satisfied\n",
    "        for task_name, _ in candidates:\n",
    "            del self.active_adapters[task_name]\n",
    "            print(f\"üóëÔ∏è Evicted adapter: {task_name}\")\n",
    "            \n",
    "            if self._estimate_memory_usage() <= self.max_memory * 0.8:\n",
    "                break\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"\n",
    "        Get usage statistics\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'active_tasks': len(self.active_adapters),\n",
    "            'total_memory_mb': self._estimate_memory_usage(),\n",
    "            'usage_counts': dict(self.usage_stats)\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "print(\"‚úÖ MultiTaskAdapterManager class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multi-task adapter management\n",
    "print(\"=== Multi-Task Adapter System Demo ===\")\n",
    "\n",
    "# Initialize manager\n",
    "manager = MultiTaskAdapterManager(base_model, max_memory_mb=2000)\n",
    "\n",
    "# Register task (using the same adapter for demo)\n",
    "if os.path.exists(production_path):\n",
    "    manager.register_adapter(\"paraphrase_detection\", production_path, priority=1.0)\n",
    "    \n",
    "    # Get statistics\n",
    "    stats = manager.get_statistics()\n",
    "    print(f\"\\nActive tasks: {stats['active_tasks']}\")\n",
    "    print(f\"Total memory: {stats['total_memory_mb']:.2f} MB\")\n",
    "    \n",
    "    # Switch task\n",
    "    task_model = manager.switch_task(\"paraphrase_detection\")\n",
    "    print(\"\\n‚úÖ Task switched successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No production adapter found, skipping demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 6. Deployment Strategy 3: AdapterDrop Optimization\n",
    "\n",
    "### 6.1 Implement Dynamic Adapter Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterDropOptimizer:\n",
    "    \"\"\"\n",
    "    Implements AdapterDrop technique for inference speedup\n",
    "    Dynamically removes adapters from selected layers during inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, drop_ratio=0.5, preserve_last_n=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: PEFT model with adapters\n",
    "            drop_ratio: Fraction of adapters to drop (0.0-1.0)\n",
    "            preserve_last_n: Always keep last N layers' adapters\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.preserve_last_n = preserve_last_n\n",
    "        self.original_state = None\n",
    "        \n",
    "    def compute_layer_importance(self, validation_data=None):\n",
    "        \"\"\"\n",
    "        Compute importance scores for each adapter layer\n",
    "        (Simplified: using layer depth as proxy)\n",
    "        \"\"\"\n",
    "        # Get all adapter modules\n",
    "        adapter_modules = []\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'adapter' in name.lower():\n",
    "                adapter_modules.append((name, module))\n",
    "        \n",
    "        # Assign importance (deeper layers = more important)\n",
    "        num_layers = len(adapter_modules)\n",
    "        importance_scores = {}\n",
    "        \n",
    "        for idx, (name, module) in enumerate(adapter_modules):\n",
    "            # Importance increases with depth\n",
    "            importance_scores[name] = idx / max(num_layers - 1, 1)\n",
    "        \n",
    "        return importance_scores\n",
    "    \n",
    "    def apply_drop(self):\n",
    "        \"\"\"\n",
    "        Apply adapter dropping based on importance\n",
    "        \"\"\"\n",
    "        importance = self.compute_layer_importance()\n",
    "        \n",
    "        # Sort by importance (ascending)\n",
    "        sorted_adapters = sorted(importance.items(), key=lambda x: x[1])\n",
    "        \n",
    "        # Determine which to drop\n",
    "        num_to_drop = int(len(sorted_adapters) * self.drop_ratio)\n",
    "        \n",
    "        # Always preserve last N layers\n",
    "        droppable = sorted_adapters[:-self.preserve_last_n] if self.preserve_last_n > 0 else sorted_adapters\n",
    "        to_drop = droppable[:num_to_drop]\n",
    "        \n",
    "        dropped_count = 0\n",
    "        for name, _ in to_drop:\n",
    "            # Deactivate adapter (implementation depends on PEFT version)\n",
    "            # This is a simplified demonstration\n",
    "            dropped_count += 1\n",
    "        \n",
    "        print(f\"‚úÖ Dropped {dropped_count} adapters (ratio: {self.drop_ratio})\")\n",
    "        print(f\"   Preserved {len(sorted_adapters) - dropped_count} adapters\")\n",
    "        \n",
    "        return dropped_count\n",
    "    \n",
    "    def estimate_speedup(self, dropped_count, total_count):\n",
    "        \"\"\"\n",
    "        Estimate inference speedup from dropping adapters\n",
    "        \"\"\"\n",
    "        # Empirical: each adapter adds ~2-5% overhead\n",
    "        overhead_per_adapter = 0.03  # 3% average\n",
    "        \n",
    "        original_overhead = total_count * overhead_per_adapter\n",
    "        new_overhead = (total_count - dropped_count) * overhead_per_adapter\n",
    "        \n",
    "        speedup = (1 + original_overhead) / (1 + new_overhead)\n",
    "        return speedup\n",
    "\n",
    "print(\"‚úÖ AdapterDropOptimizer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate AdapterDrop optimization\n",
    "print(\"=== AdapterDrop Optimization Demo ===\")\n",
    "\n",
    "optimizer = AdapterDropOptimizer(\n",
    "    peft_model,\n",
    "    drop_ratio=0.5,  # Drop 50% of adapters\n",
    "    preserve_last_n=3  # Keep last 3 layers\n",
    ")\n",
    "\n",
    "# Compute importance\n",
    "importance = optimizer.compute_layer_importance()\n",
    "print(f\"\\nFound {len(importance)} adapter modules\")\n",
    "\n",
    "# Apply drop\n",
    "dropped = optimizer.apply_drop()\n",
    "\n",
    "# Estimate speedup\n",
    "speedup = optimizer.estimate_speedup(dropped, len(importance))\n",
    "print(f\"\\nüìä Estimated speedup: {speedup:.2%}\")\n",
    "print(f\"   Expected latency reduction: {(1 - 1/speedup) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Production Deployment Checklist\n",
    "\n",
    "### 7.1 Pre-Deployment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_deployment_checklist(model, tokenizer, test_samples):\n",
    "    \"\"\"\n",
    "    Comprehensive pre-deployment verification\n",
    "    \"\"\"\n",
    "    print(\"=== Production Deployment Checklist ===\")\n",
    "    checklist = {}\n",
    "    \n",
    "    # 1. Model loading\n",
    "    try:\n",
    "        assert model is not None\n",
    "        checklist['model_loaded'] = '‚úÖ'\n",
    "    except:\n",
    "        checklist['model_loaded'] = '‚ùå'\n",
    "    \n",
    "    # 2. Tokenizer compatibility\n",
    "    try:\n",
    "        test_input = tokenizer(\"test\", return_tensors=\"pt\")\n",
    "        checklist['tokenizer_compatible'] = '‚úÖ'\n",
    "    except:\n",
    "        checklist['tokenizer_compatible'] = '‚ùå'\n",
    "    \n",
    "    # 3. Inference functionality\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(\"test sentence\", \"another sentence\", \n",
    "                             return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "        checklist['inference_works'] = '‚úÖ'\n",
    "    except Exception as e:\n",
    "        checklist['inference_works'] = f'‚ùå {str(e)}'\n",
    "    \n",
    "    # 4. Performance benchmarking\n",
    "    try:\n",
    "        avg_time, throughput = benchmark_inference(model, tokenizer, test_samples, num_runs=3)\n",
    "        checklist['performance_tested'] = f'‚úÖ ({throughput:.1f} samples/sec)'\n",
    "    except:\n",
    "        checklist['performance_tested'] = '‚ùå'\n",
    "    \n",
    "    # 5. Memory usage\n",
    "    try:\n",
    "        memory_mb = model.get_memory_footprint() / 1024**2\n",
    "        checklist['memory_acceptable'] = f'‚úÖ ({memory_mb:.1f} MB)'\n",
    "    except:\n",
    "        checklist['memory_acceptable'] = '‚ùå'\n",
    "    \n",
    "    # Display checklist\n",
    "    print(\"\\nüìã Checklist Results:\")\n",
    "    for item, status in checklist.items():\n",
    "        print(f\"  {item.replace('_', ' ').title()}: {status}\")\n",
    "    \n",
    "    # Overall status\n",
    "    all_passed = all('‚úÖ' in str(v) for v in checklist.values())\n",
    "    print(f\"\\n{'üéâ Ready for production!' if all_passed else '‚ö†Ô∏è Issues detected, please review'}\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# Run checklist\n",
    "if os.path.exists(production_path):\n",
    "    checklist_results = production_deployment_checklist(prod_model, prod_tokenizer, test_samples)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No production model available for checklist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### 7.2 Create Deployment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive deployment configuration\n",
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": model_checkpoint,\n",
    "        \"peft_method\": \"Adapter Layers\",\n",
    "        \"task_type\": \"Sequence Classification\",\n",
    "        \"num_labels\": 2,\n",
    "        \"mergeable\": False,\n",
    "        \"total_parameters\": prod_model.num_parameters() if os.path.exists(production_path) else 0,\n",
    "    },\n",
    "    \"adapter_config\": {\n",
    "        \"reduction_factor\": 16,\n",
    "        \"adapter_locations\": \"FFN layers\",\n",
    "        \"trainable_params_percentage\": 0.74\n",
    "    },\n",
    "    \"deployment_requirements\": {\n",
    "        \"python_packages\": [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.20.0\",\n",
    "            \"peft>=0.3.0\"\n",
    "        ],\n",
    "        \"minimum_memory_gb\": 4,\n",
    "        \"recommended_memory_gb\": 8,\n",
    "        \"gpu_required\": False,\n",
    "        \"cpu_cores_recommended\": 4\n",
    "    },\n",
    "    \"deployment_notes\": {\n",
    "        \"merging\": \"Adapters CANNOT be merged - must deploy with PEFT library\",\n",
    "        \"inference_overhead\": \"~2-8% latency overhead per adapter module\",\n",
    "        \"optimization\": \"Consider AdapterDrop for production speedup\",\n",
    "        \"multi_task\": \"Supports efficient multi-task deployment with adapter switching\"\n",
    "    },\n",
    "    \"loading_instructions\": {\n",
    "        \"step1\": \"Load base model: AutoModelForSequenceClassification.from_pretrained()\",\n",
    "        \"step2\": \"Load adapter: PeftModel.from_pretrained(base_model, adapter_path)\",\n",
    "        \"step3\": \"Set to eval mode: model.eval()\",\n",
    "        \"step4\": \"Perform inference with standard transformers API\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "if os.path.exists(production_path):\n",
    "    config_file = os.path.join(production_path, \"deployment_config.json\")\n",
    "    with open(config_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"=== Deployment Configuration ===\")\n",
    "    print(json.dumps(deployment_config, indent=2, ensure_ascii=False))\n",
    "    print(f\"\\n‚úÖ Configuration saved to: {config_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping config save - no production path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 8. Adapter vs LoRA Deployment Comparison\n",
    "\n",
    "### 8.1 Key Differences Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison table\n",
    "comparison = {\n",
    "    \"Aspect\": [\n",
    "        \"Merging Capability\",\n",
    "        \"Inference Overhead\",\n",
    "        \"Deployment Complexity\",\n",
    "        \"Multi-Task Support\",\n",
    "        \"Parameter Efficiency\",\n",
    "        \"Memory Overhead\",\n",
    "        \"Production Dependencies\",\n",
    "        \"Hot-Swapping\"\n",
    "    ],\n",
    "    \"Adapter Layers\": [\n",
    "        \"‚ùå Cannot merge\",\n",
    "        \"2-8% latency increase\",\n",
    "        \"Medium (requires PEFT)\",\n",
    "        \"‚úÖ Excellent (parallel adapters)\",\n",
    "        \"0.5-5% parameters\",\n",
    "        \"50-200MB per adapter\",\n",
    "        \"Requires PEFT library\",\n",
    "        \"‚úÖ Supported\"\n",
    "    ],\n",
    "    \"LoRA\": [\n",
    "        \"‚úÖ Fully mergeable\",\n",
    "        \"Zero (after merge)\",\n",
    "        \"Simple (standard model)\",\n",
    "        \"‚ö†Ô∏è Requires model switching\",\n",
    "        \"0.1-1% parameters\",\n",
    "        \"Zero (after merge)\",\n",
    "        \"None (after merge)\",\n",
    "        \"‚ùå Not supported (after merge)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== Adapter vs LoRA Deployment Comparison ===\")\n",
    "print(f\"\\n{'Aspect':<25} {'Adapter Layers':<35} {'LoRA':<35}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for i, aspect in enumerate(comparison[\"Aspect\"]):\n",
    "    adapter_val = comparison[\"Adapter Layers\"][i]\n",
    "    lora_val = comparison[\"LoRA\"][i]\n",
    "    print(f\"{aspect:<25} {adapter_val:<35} {lora_val:<35}\")\n",
    "\n",
    "print(\"\\n=== Deployment Strategy Recommendations ===\")\n",
    "print(\"\\nüìä Choose Adapter Layers when:\")\n",
    "print(\"  ‚Ä¢ Need multi-task deployment with task switching\")\n",
    "print(\"  ‚Ä¢ Want to preserve modular architecture\")\n",
    "print(\"  ‚Ä¢ Overhead 2-8% is acceptable\")\n",
    "print(\"  ‚Ä¢ Need hot-swapping capabilities\")\n",
    "\n",
    "print(\"\\nüìä Choose LoRA when:\")\n",
    "print(\"  ‚Ä¢ Need zero-overhead inference\")\n",
    "print(\"  ‚Ä¢ Want simple deployment (no PEFT dependency)\")\n",
    "print(\"  ‚Ä¢ Single-task production deployment\")\n",
    "print(\"  ‚Ä¢ Maximum parameter efficiency required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices\n",
    "\n",
    "### 9.1 Key Takeaways\n",
    "\n",
    "Through this experiment, we learned:\n",
    "\n",
    "1. **Architectural Understanding**: Adapters are new modules, not reparameterizations\n",
    "2. **Deployment Strategies**: Multiple approaches for production (standard, multi-task, optimized)\n",
    "3. **Performance Trade-offs**: Small inference overhead vs deployment flexibility\n",
    "4. **Production Readiness**: Comprehensive verification and configuration\n",
    "\n",
    "### 9.2 Production Deployment Checklist\n",
    "\n",
    "‚úÖ **Pre-Deployment**:\n",
    "- [ ] Verify adapter training completed successfully\n",
    "- [ ] Test adapter loading and inference\n",
    "- [ ] Benchmark performance on production hardware\n",
    "- [ ] Document memory requirements\n",
    "\n",
    "‚úÖ **Deployment**:\n",
    "- [ ] Save adapter with proper versioning\n",
    "- [ ] Create deployment configuration file\n",
    "- [ ] Set up monitoring for latency/throughput\n",
    "- [ ] Implement error handling\n",
    "\n",
    "‚úÖ **Post-Deployment**:\n",
    "- [ ] Monitor inference performance\n",
    "- [ ] Collect user feedback\n",
    "- [ ] Consider AdapterDrop if latency issues\n",
    "- [ ] Plan for model updates\n",
    "\n",
    "### 9.3 Advanced Optimization Techniques\n",
    "\n",
    "- **AdapterDrop**: Remove up to 50% of adapters with minimal performance loss\n",
    "- **Multi-Task Batching**: Group similar tasks for efficient processing\n",
    "- **Adapter Compression**: Quantize adapter weights for smaller footprint\n",
    "- **Dynamic Loading**: Load/unload adapters based on demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"=== Cleaning Up Resources ===\")\n",
    "if 'peft_model' in locals():\n",
    "    del peft_model\n",
    "if 'prod_model' in locals():\n",
    "    del prod_model\n",
    "if 'base_model' in locals():\n",
    "    del base_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Resources cleaned up\")\n",
    "\n",
    "print(\"\\nüéâ Lab 2 - Adapter Deployment Completed!\")\n",
    "print(\"\\nKey Insight: While Adapters cannot be merged, they offer\")\n",
    "print(\"unique advantages for multi-task systems and modular deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
