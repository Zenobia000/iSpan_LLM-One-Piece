{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.3.3: vLLM Backend 整合與優化\n",
    "\n",
    "## 🎯 學習目標\n",
    "\n",
    "- 掌握 Triton + vLLM 架構整合\n",
    "- 實現 PagedAttention 在 Triton 中應用\n",
    "- 優化大語言模型推理性能\n",
    "- 設計混合部署策略\n",
    "\n",
    "## 📚 理論基礎\n",
    "\n",
    "### vLLM Backend 架構\n",
    "```\n",
    "Triton Server\n",
    "├── Model Repository\n",
    "│   └── llm_model/\n",
    "│       ├── config.pbtxt          # vLLM backend 配置\n",
    "│       └── 1/\n",
    "│           └── model.py          # vLLM 整合實現\n",
    "└── vLLM Engine\n",
    "    ├── PagedAttention           # 記憶體效率優化\n",
    "    ├── Continuous Batching      # 動態批次處理\n",
    "    └── KV Cache 管理           # 快取最佳化\n",
    "```\n",
    "\n",
    "### 核心優化技術\n",
    "1. **PagedAttention**: 將 attention 的 KV cache 分頁管理\n",
    "2. **Continuous Batching**: 動態添加/移除序列\n",
    "3. **Speculative Decoding**: 加速生成過程\n",
    "4. **Quantization**: INT8/FP16 推理優化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 環境設置與檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('vllm_backend.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 設置圖表樣式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📋 系統環境檢查\")\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 數量: {torch.cuda.device_count()}\")\n",
    "    print(f\"當前 GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU 記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 vLLM 安裝\n",
    "try:\n",
    "    import vllm\n",
    "    from vllm import LLM, SamplingParams\n",
    "    from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "    from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "    print(f\"✅ vLLM 版本: {vllm.__version__}\")\n",
    "    vllm_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"❌ vLLM 未安裝: {e}\")\n",
    "    print(\"請安裝: pip install vllm\")\n",
    "    vllm_available = False\n",
    "\n",
    "# 檢查 Triton Client\n",
    "try:\n",
    "    import tritonclient.http as httpclient\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    print(\"✅ Triton Client 可用\")\n",
    "    triton_client_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Triton Client 未安裝: {e}\")\n",
    "    print(\"請安裝: pip install tritonclient[all]\")\n",
    "    triton_client_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 模型倉庫結構設計\n",
    "\n",
    "### 設計 vLLM Backend 模型倉庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMModelRepository:\n",
    "    \"\"\"vLLM Backend 模型倉庫管理器\"\"\"\n",
    "    \n",
    "    def __init__(self, repository_path: str = \"./model_repository\"):\n",
    "        self.repository_path = Path(repository_path)\n",
    "        self.repository_path.mkdir(exist_ok=True)\n",
    "        logger.info(f\"模型倉庫路徑: {self.repository_path.absolute()}\")\n",
    "    \n",
    "    def create_vllm_model_config(self, \n",
    "                                model_name: str,\n",
    "                                hf_model_name: str,\n",
    "                                max_model_len: int = 2048,\n",
    "                                tensor_parallel_size: int = 1,\n",
    "                                dtype: str = \"auto\",\n",
    "                                quantization: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"創建 vLLM backend 配置\"\"\"\n",
    "        \n",
    "        config = {\n",
    "            \"name\": model_name,\n",
    "            \"backend\": \"vllm\",\n",
    "            \"max_batch_size\": 256,\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"name\": \"text_input\",\n",
    "                    \"data_type\": \"TYPE_STRING\",\n",
    "                    \"dims\": [-1]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"stream\",\n",
    "                    \"data_type\": \"TYPE_BOOL\",\n",
    "                    \"dims\": [1],\n",
    "                    \"optional\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"sampling_parameters\",\n",
    "                    \"data_type\": \"TYPE_STRING\",\n",
    "                    \"dims\": [1],\n",
    "                    \"optional\": True\n",
    "                }\n",
    "            ],\n",
    "            \"output\": [\n",
    "                {\n",
    "                    \"name\": \"text_output\",\n",
    "                    \"data_type\": \"TYPE_STRING\",\n",
    "                    \"dims\": [-1]\n",
    "                }\n",
    "            ],\n",
    "            \"instance_group\": [\n",
    "                {\n",
    "                    \"count\": 1,\n",
    "                    \"kind\": \"KIND_MODEL\"\n",
    "                }\n",
    "            ],\n",
    "            \"parameters\": {\n",
    "                \"model\": {\"string_value\": hf_model_name},\n",
    "                \"tensor_parallel_size\": {\"string_value\": str(tensor_parallel_size)},\n",
    "                \"max_model_len\": {\"string_value\": str(max_model_len)},\n",
    "                \"dtype\": {\"string_value\": dtype},\n",
    "                \"gpu_memory_utilization\": {\"string_value\": \"0.9\"},\n",
    "                \"enforce_eager\": {\"string_value\": \"false\"},\n",
    "                \"disable_log_requests\": {\"string_value\": \"false\"}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 添加量化配置\n",
    "        if quantization:\n",
    "            config[\"parameters\"][\"quantization\"] = {\"string_value\": quantization}\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def write_config_pbtxt(self, config: Dict[str, Any], model_name: str):\n",
    "        \"\"\"寫入 config.pbtxt 文件\"\"\"\n",
    "        model_dir = self.repository_path / model_name\n",
    "        model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        config_path = model_dir / \"config.pbtxt\"\n",
    "        \n",
    "        # 轉換為 protobuf 格式\n",
    "        pbtxt_content = self._dict_to_pbtxt(config)\n",
    "        \n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(pbtxt_content)\n",
    "        \n",
    "        logger.info(f\"配置文件已寫入: {config_path}\")\n",
    "        return config_path\n",
    "    \n",
    "    def create_vllm_model_py(self, model_name: str) -> Path:\n",
    "        \"\"\"創建 vLLM model.py 文件\"\"\"\n",
    "        model_dir = self.repository_path / model_name / \"1\"\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        model_py_content = '''\nimport json\nimport torch\nfrom typing import List, Dict, Any, Optional\nimport triton_python_backend_utils as pb_utils\nfrom vllm import LLM, SamplingParams\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nimport asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TritonPythonModel:\n    \"\"\"Triton vLLM Backend 模型實現\"\"\"\n    \n    def initialize(self, args):\n        \"\"\"初始化 vLLM 引擎\"\"\"\n        logger.info(\"初始化 vLLM Backend\")\n        \n        # 獲取模型配置\n        model_config = json.loads(args[\"model_config\"])\n        model_params = model_config.get(\"parameters\", {})\n        \n        # 解析 vLLM 參數\n        self.model_name = model_params.get(\"model\", {}).get(\"string_value\", \"\")\n        tensor_parallel_size = int(model_params.get(\"tensor_parallel_size\", {}).get(\"string_value\", \"1\"))\n        max_model_len = int(model_params.get(\"max_model_len\", {}).get(\"string_value\", \"2048\"))\n        dtype = model_params.get(\"dtype\", {}).get(\"string_value\", \"auto\")\n        gpu_memory_utilization = float(model_params.get(\"gpu_memory_utilization\", {}).get(\"string_value\", \"0.9\"))\n        quantization = model_params.get(\"quantization\", {}).get(\"string_value\", None)\n        \n        # 初始化 vLLM 引擎\n        try:\n            self.llm = LLM(\n                model=self.model_name,\n                tensor_parallel_size=tensor_parallel_size,\n                max_model_len=max_model_len,\n                dtype=dtype,\n                gpu_memory_utilization=gpu_memory_utilization,\n                quantization=quantization,\n                enforce_eager=False,\n                disable_log_requests=True\n            )\n            logger.info(f\"vLLM 引擎初始化成功: {self.model_name}\")\n        except Exception as e:\n            logger.error(f\"vLLM 引擎初始化失敗: {e}\")\n            raise\n    \n    def execute(self, requests):\n        \"\"\"執行推理請求\"\"\"\n        responses = []\n        \n        for request in requests:\n            try:\n                # 解析輸入\n                text_input = pb_utils.get_input_tensor_by_name(request, \"text_input\")\n                prompts = [prompt.decode('utf-8') for prompt in text_input.as_numpy()]\n                \n                # 解析 sampling parameters\n                sampling_params_tensor = pb_utils.get_input_tensor_by_name(request, \"sampling_parameters\")\n                if sampling_params_tensor is not None:\n                    sampling_params_json = sampling_params_tensor.as_numpy()[0].decode('utf-8')\n                    sampling_params_dict = json.loads(sampling_params_json)\n                    sampling_params = SamplingParams(**sampling_params_dict)\n                else:\n                    sampling_params = SamplingParams(\n                        temperature=0.7,\n                        top_p=0.9,\n                        max_tokens=512\n                    )\n                \n                # 執行推理\n                outputs = self.llm.generate(prompts, sampling_params)\n                \n                # 提取生成的文本\n                generated_texts = []\n                for output in outputs:\n                    generated_text = output.outputs[0].text\n                    generated_texts.append(generated_text)\n                \n                # 創建輸出張量\n                output_tensor = pb_utils.Tensor(\n                    \"text_output\", \n                    np.array(generated_texts, dtype=object)\n                )\n                \n                response = pb_utils.InferenceResponse(\n                    output_tensors=[output_tensor]\n                )\n                responses.append(response)\n                \n            except Exception as e:\n                logger.error(f\"推理執行失敗: {e}\")\n                error_response = pb_utils.InferenceResponse(\n                    output_tensors=[],\n                    error=pb_utils.TritonError(f\"推理失敗: {str(e)}\")\n                )\n                responses.append(error_response)\n        \n        return responses\n    \n    def finalize(self):\n        \"\"\"清理資源\"\"\"\n        logger.info(\"vLLM Backend 清理完成\")\n        pass\n'''\n        \n        model_py_path = model_dir / \"model.py\"\n        with open(model_py_path, 'w', encoding='utf-8') as f:\n            f.write(model_py_content)\n        \n        logger.info(f\"model.py 已創建: {model_py_path}\")\n        return model_py_path\n    \n    def _dict_to_pbtxt(self, config: Dict[str, Any], indent: int = 0) -> str:\n        \"\"\"將字典轉換為 protobuf 文本格式\"\"\"\n        lines = []\n        \n        for key, value in config.items():\n            if isinstance(value, dict):\n                lines.append(f\"{' ' * indent}{key} {{\")\n                lines.append(self._dict_to_pbtxt(value, indent + 2))\n                lines.append(f\"{' ' * indent}}}\")\n            elif isinstance(value, list):\n                for item in value:\n                    if isinstance(item, dict):\n                        lines.append(f\"{' ' * indent}{key} {{\")\n                        lines.append(self._dict_to_pbtxt(item, indent + 2))\n                        lines.append(f\"{' ' * indent}}}\")\n                    else:\n                        lines.append(f\"{' ' * indent}{key}: {self._format_value(item)}\")\n            else:\n                lines.append(f\"{' ' * indent}{key}: {self._format_value(value)}\")\n        \n        return '\\n'.join(lines)\n    \n    def _format_value(self, value: Any) -> str:\n        \"\"\"格式化值\"\"\"\n        if isinstance(value, str):\n            return f'\"{value}\"'\n        elif isinstance(value, bool):\n            return str(value).lower()\n        else:\n            return str(value)\n\n# 創建模型倉庫管理器\nvllm_repo = vLLMModelRepository()\nprint(f\"📁 模型倉庫路徑: {vllm_repo.repository_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 vLLM 模型部署實例\n",
    "\n",
    "### 部署小型語言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇適合的模型（根據 GPU 記憶體）\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "if gpu_memory >= 24:\n",
    "    model_choice = \"microsoft/DialoGPT-large\"\n",
    "    max_model_len = 1024\n",
    "    print(f\"🚀 選擇大型模型: {model_choice} (GPU: {gpu_memory:.1f}GB)\")\n",
    "elif gpu_memory >= 8:\n",
    "    model_choice = \"microsoft/DialoGPT-medium\"\n",
    "    max_model_len = 512\n",
    "    print(f\"🚀 選擇中型模型: {model_choice} (GPU: {gpu_memory:.1f}GB)\")\n",
    "else:\n",
    "    model_choice = \"gpt2\"\n",
    "    max_model_len = 256\n",
    "    print(f\"🚀 選擇小型模型: {model_choice} (GPU: {gpu_memory:.1f}GB)\")\n",
    "\n",
    "# 創建 vLLM 模型配置\n",
    "model_name = \"vllm_chat_model\"\n",
    "config = vllm_repo.create_vllm_model_config(\n",
    "    model_name=model_name,\n",
    "    hf_model_name=model_choice,\n",
    "    max_model_len=max_model_len,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# 寫入配置文件\n",
    "config_path = vllm_repo.write_config_pbtxt(config, model_name)\n",
    "print(f\"✅ 配置文件已創建: {config_path}\")\n",
    "\n",
    "# 創建 model.py\n",
    "model_py_path = vllm_repo.create_vllm_model_py(model_name)\n",
    "print(f\"✅ model.py 已創建: {model_py_path}\")\n",
    "\n",
    "# 顯示模型倉庫結構\n",
    "def show_repository_structure(repo_path: Path, max_depth: int = 3):\n",
    "    \"\"\"顯示模型倉庫結構\"\"\"\n",
    "    print(f\"\\n📂 模型倉庫結構: {repo_path}\")\n",
    "    \n",
    "    def print_tree(path: Path, prefix: str = \"\", depth: int = 0):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        \n",
    "        items = sorted([p for p in path.iterdir()])\n",
    "        for i, item in enumerate(items):\n",
    "            is_last = i == len(items) - 1\n",
    "            current_prefix = \"└── \" if is_last else \"├── \"\n",
    "            print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "            \n",
    "            if item.is_dir():\n",
    "                next_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "                print_tree(item, next_prefix, depth + 1)\n",
    "    \n",
    "    print_tree(repo_path)\n",
    "\n",
    "show_repository_structure(vllm_repo.repository_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Triton Server 啟動與 vLLM 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonvLLMManager:\n",
    "    \"\"\"Triton + vLLM 整合管理器\"\"\"\n",
    "    \n",
    "    def __init__(self, repository_path: str):\n",
    "        self.repository_path = repository_path\n",
    "        self.server_process = None\n",
    "        self.server_url = \"http://localhost:8000\"\n",
    "        self.grpc_url = \"localhost:8001\"\n",
    "    \n",
    "    def start_triton_server(self, log_file: str = \"triton_server.log\") -> bool:\n",
    "        \"\"\"啟動 Triton Server\"\"\"\n",
    "        try:\n",
    "            # 檢查是否已有服務器運行\n",
    "            if self.is_server_running():\n",
    "                logger.info(\"Triton Server 已在運行\")\n",
    "                return True\n",
    "            \n",
    "            # 啟動命令\n",
    "            cmd = [\n",
    "                \"tritonserver\",\n",
    "                f\"--model-repository={self.repository_path}\",\n",
    "                \"--http-port=8000\",\n",
    "                \"--grpc-port=8001\",\n",
    "                \"--metrics-port=8002\",\n",
    "                \"--log-verbose=1\",\n",
    "                \"--backend-directory=/opt/tritonserver/backends\",\n",
    "                \"--backend-config=python,shm-region-prefix-name=prefix0_\"\n",
    "            ]\n",
    "            \n",
    "            # 啟動服務器\n",
    "            logger.info(\"正在啟動 Triton Server...\")\n",
    "            with open(log_file, 'w') as f:\n",
    "                self.server_process = subprocess.Popen(\n",
    "                    cmd,\n",
    "                    stdout=f,\n",
    "                    stderr=subprocess.STDOUT,\n",
    "                    text=True\n",
    "                )\n",
    "            \n",
    "            # 等待服務器啟動\n",
    "            max_wait_time = 120  # 2分鐘\n",
    "            for i in range(max_wait_time):\n",
    "                if self.is_server_running():\n",
    "                    logger.info(f\"Triton Server 啟動成功 (等待時間: {i}秒)\")\n",
    "                    return True\n",
    "                time.sleep(1)\n",
    "            \n",
    "            logger.error(\"Triton Server 啟動超時\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"啟動 Triton Server 失敗: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def is_server_running(self) -> bool:\n",
    "        \"\"\"檢查服務器是否運行\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/v2/health/ready\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def stop_server(self):\n",
    "        \"\"\"停止服務器\"\"\"\n",
    "        if self.server_process:\n",
    "            self.server_process.terminate()\n",
    "            self.server_process.wait()\n",
    "            logger.info(\"Triton Server 已停止\")\n",
    "    \n",
    "    def get_server_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"獲取服務器狀態\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/v2\")\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"獲取服務器狀態失敗: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def list_models(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"列出所有模型\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/v2/models\")\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"獲取模型列表失敗: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_model_config(self, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"獲取模型配置\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/v2/models/{model_name}/config\")\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"獲取模型配置失敗: {e}\")\n",
    "            return {}\n",
    "\n",
    "# 創建 Triton vLLM 管理器\ntriton_vllm = TritonvLLMManager(str(vllm_repo.repository_path))\n",
    "print(\"🔧 Triton vLLM 管理器已創建\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 tritonserver 是否可用\n",
    "def check_tritonserver_availability():\n",
    "    \"\"\"檢查 tritonserver 命令是否可用\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"tritonserver\", \"--help\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        return result.returncode == 0\n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "tritonserver_available = check_tritonserver_availability()\n",
    "\n",
    "if tritonserver_available:\n",
    "    print(\"✅ Tritonserver 命令可用\")\n",
    "    \n",
    "    # 嘗試啟動 Triton Server（如果可用且 vLLM 也可用）\n",
    "    if vllm_available:\n",
    "        print(\"🚀 準備啟動 Triton Server with vLLM Backend...\")\n",
    "        # 注意：實際啟動需要模型文件，這裡僅作演示\n",
    "        print(\"⚠️  實際部署需要下載對應的 HuggingFace 模型\")\n",
    "        print(f\"   模型: {model_choice}\")\n",
    "        print(f\"   配置路徑: {config_path}\")\n",
    "        print(f\"   啟動命令: tritonserver --model-repository={vllm_repo.repository_path}\")\n",
    "    else:\n",
    "        print(\"❌ vLLM 不可用，無法啟動 vLLM Backend\")\nelse:\n",
    "    print(\"❌ Tritonserver 命令不可用\")\n",
    "    print(\"   請安裝 Triton Inference Server\")\n",
    "    print(\"   Docker: nvidia/tritonserver:xx.xx-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 vLLM Backend 客戶端測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMTritonClient:\n",
    "    \"\"\"vLLM Triton 客戶端\"\"\"\n",
    "    \n",
    "    def __init__(self, server_url: str = \"localhost:8000\"):\n",
    "        self.server_url = server_url\n",
    "        if triton_client_available:\n",
    "            self.client = httpclient.InferenceServerClient(url=server_url)\n",
    "        else:\n",
    "            self.client = None\n",
    "    \n",
    "    def is_server_live(self) -> bool:\n",
    "        \"\"\"檢查服務器是否存活\"\"\"\n",
    "        if not self.client:\n",
    "            return False\n",
    "        try:\n",
    "            return self.client.is_server_live()\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def is_server_ready(self) -> bool:\n",
    "        \"\"\"檢查服務器是否就緒\"\"\"\n",
    "        if not self.client:\n",
    "            return False\n",
    "        try:\n",
    "            return self.client.is_server_ready()\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def generate_text(self, \n",
    "                     model_name: str,\n",
    "                     prompts: List[str],\n",
    "                     sampling_params: Optional[Dict[str, Any]] = None) -> List[str]:\n",
    "        \"\"\"生成文本\"\"\"\n",
    "        if not self.client:\n",
    "            logger.error(\"Triton Client 不可用\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # 準備輸入\n",
    "            inputs = []\n",
    "            \n",
    "            # 文本輸入\n",
    "            text_input = httpclient.InferInput(\n",
    "                \"text_input\", \n",
    "                [len(prompts)], \n",
    "                \"BYTES\"\n",
    "            )\n",
    "            text_input.set_data_from_numpy(\n",
    "                np.array([prompt.encode('utf-8') for prompt in prompts], dtype=object)\n",
    "            )\n",
    "            inputs.append(text_input)\n",
    "            \n",
    "            # Sampling 參數\n",
    "            if sampling_params:\n",
    "                sampling_input = httpclient.InferInput(\n",
    "                    \"sampling_parameters\", \n",
    "                    [1], \n",
    "                    \"BYTES\"\n",
    "                )\n",
    "                sampling_json = json.dumps(sampling_params).encode('utf-8')\n",
    "                sampling_input.set_data_from_numpy(np.array([sampling_json], dtype=object))\n",
    "                inputs.append(sampling_input)\n",
    "            \n",
    "            # 準備輸出\n",
    "            outputs = [\n",
    "                httpclient.InferRequestedOutput(\"text_output\")\n",
    "            ]\n",
    "            \n",
    "            # 執行推理\n",
    "            response = self.client.infer(\n",
    "                model_name=model_name,\n",
    "                inputs=inputs,\n",
    "                outputs=outputs\n",
    "            )\n",
    "            \n",
    "            # 提取結果\n",
    "            output_data = response.as_numpy(\"text_output\")\n",
    "            results = [text.decode('utf-8') for text in output_data]\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"文本生成失敗: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def benchmark_generation(self, \n",
    "                           model_name: str,\n",
    "                           prompts: List[str],\n",
    "                           num_runs: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"性能基準測試\"\"\"\n",
    "        if not self.is_server_ready():\n",
    "            logger.error(\"服務器未就緒\")\n",
    "            return {}\n",
    "        \n",
    "        latencies = []\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            results = self.generate_text(model_name, prompts)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if results:\n",
    "                latency = end_time - start_time\n",
    "                latencies.append(latency)\n",
    "                logger.info(f\"運行 {i+1}/{num_runs}: {latency:.3f}s\")\n",
    "            else:\n",
    "                logger.warning(f\"運行 {i+1} 失敗\")\n",
    "        \n",
    "        if latencies:\n",
    "            return {\n",
    "                \"avg_latency\": np.mean(latencies),\n",
    "                \"min_latency\": np.min(latencies),\n",
    "                \"max_latency\": np.max(latencies),\n",
    "                \"std_latency\": np.std(latencies),\n",
    "                \"throughput\": len(prompts) / np.mean(latencies)\n",
    "            }\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "# 創建客戶端（僅在 Triton Client 可用時）\n",
    "if triton_client_available:\n",
    "    vllm_client = vLLMTritonClient()\n",
    "    print(\"✅ vLLM Triton 客戶端已創建\")\nelse:\n",
    "    vllm_client = None\n",
    "    print(\"❌ Triton Client 不可用，無法創建客戶端\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬測試（如果實際服務器不可用）\n",
    "def simulate_vllm_inference():\n",
    "    \"\"\"模擬 vLLM 推理測試\"\"\"\n",
    "    test_prompts = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Explain machine learning briefly.\",\n",
    "        \"Tell me about Python programming.\",\n",
    "        \"How does neural network work?\"\n",
    "    ]\n",
    "    \n",
    "    sampling_params = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "    \n",
    "    print(\"🤖 vLLM Backend 推理測試\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 檢查實際服務器狀態\n",
    "    if vllm_client and vllm_client.is_server_ready():\n",
    "        print(\"✅ Triton Server 運行中，執行實際推理\")\n",
    "        \n",
    "        # 執行實際推理\n",
    "        results = vllm_client.generate_text(\n",
    "            model_name=model_name,\n",
    "            prompts=test_prompts[:2],  # 限制測試數量\n",
    "            sampling_params=sampling_params\n",
    "        )\n",
    "        \n",
    "        for i, (prompt, result) in enumerate(zip(test_prompts[:2], results)):\n",
    "            print(f\"\\n測試 {i+1}:\")\n",
    "            print(f\"輸入: {prompt}\")\n",
    "            print(f\"輸出: {result}\")\n",
    "        \n",
    "        # 性能測試\n",
    "        print(\"\\n📊 性能基準測試\")\n",
    "        benchmark_results = vllm_client.benchmark_generation(\n",
    "            model_name=model_name,\n",
    "            prompts=test_prompts[:1],\n",
    "            num_runs=3\n",
    "        )\n",
    "        \n",
    "        for metric, value in benchmark_results.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️  Triton Server 未運行，展示模擬結果\")\n",
    "        \n",
    "        # 模擬結果\n",
    "        simulated_results = [\n",
    "            \"I'm doing well, thank you for asking! How can I help you today?\",\n",
    "            \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "            \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "            \"Python is a versatile programming language popular in AI and web development.\",\n",
    "            \"Neural networks are computing systems inspired by biological neural networks.\"\n",
    "        ]\n",
    "        \n",
    "        for i, (prompt, result) in enumerate(zip(test_prompts, simulated_results)):\n",
    "            print(f\"\\n模擬測試 {i+1}:\")\n",
    "            print(f\"輸入: {prompt}\")\n",
    "            print(f\"輸出: {result}\")\n",
    "        \n",
    "        # 模擬性能指標\n",
    "        print(\"\\n📊 模擬性能指標:\")\n",
    "        mock_metrics = {\n",
    "            \"avg_latency\": 0.234,\n",
    "            \"min_latency\": 0.201,\n",
    "            \"max_latency\": 0.267,\n",
    "            \"std_latency\": 0.024,\n",
    "            \"throughput\": 4.27\n",
    "        }\n",
    "        \n",
    "        for metric, value in mock_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n# 執行測試\nsimulate_vllm_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 PagedAttention 配置與優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedAttentionOptimizer:\n",
    "    \"\"\"PagedAttention 優化配置器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_memory = torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else 0\n",
    "        self.available_memory = self.gpu_memory * 0.9  # 保留 10% 緩衝\n",
    "    \n",
    "    def calculate_optimal_block_size(self, \n",
    "                                   model_size_gb: float,\n",
    "                                   max_seq_len: int = 2048,\n",
    "                                   head_dim: int = 128) -> Dict[str, Any]:\n",
    "        \"\"\"計算最佳塊大小配置\"\"\"\n",
    "        \n",
    "        # 估算模型記憶體需求（GB to bytes）\n",
    "        model_memory = model_size_gb * 1e9\n",
    "        \n",
    "        # 計算可用於 KV cache 的記憶體\n",
    "        kv_cache_memory = self.available_memory - model_memory\n",
    "        \n",
    "        # 計算每個 token 的 KV cache 大小\n",
    "        # (key + value) * head_dim * num_layers * dtype_size\n",
    "        # 假設 FP16 (2 bytes) 和常見的層數\n",
    "        estimated_layers = max(12, int(model_size_gb * 4))  # 粗略估計\n",
    "        kv_size_per_token = 2 * head_dim * estimated_layers * 2  # bytes\n",
    "        \n",
    "        # 計算建議的塊大小\n",
    "        target_blocks = 1000  # 目標塊數量\n",
    "        block_size = min(16, max(1, int((kv_cache_memory / target_blocks) / kv_size_per_token)))\n",
    "        \n",
    "        # 計算其他參數\n",
    "        max_num_blocks = int(kv_cache_memory / (block_size * kv_size_per_token))\n",
    "        max_concurrent_sequences = max_num_blocks * block_size // max_seq_len\n",
    "        \n",
    "        return {\n",
    "            \"block_size\": block_size,\n",
    "            \"max_num_blocks\": max_num_blocks,\n",
    "            \"max_concurrent_sequences\": max_concurrent_sequences,\n",
    "            \"estimated_memory_usage\": {\n",
    "                \"model_memory_gb\": model_memory / 1e9,\n",
    "                \"kv_cache_memory_gb\": kv_cache_memory / 1e9,\n",
    "                \"total_memory_gb\": self.gpu_memory / 1e9\n",
    "            },\n",
    "            \"recommendations\": self._generate_recommendations(block_size, max_concurrent_sequences)\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendations(self, block_size: int, max_concurrent: int) -> List[str]:\n",
    "        \"\"\"生成優化建議\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if block_size < 4:\n",
    "            recommendations.append(\"塊大小較小，考慮減少模型大小或增加 GPU 記憶體\")\n",
    "        elif block_size > 32:\n",
    "            recommendations.append(\"塊大小較大，可能導致記憶體碎片，建議調整\")\n",
    "        \n",
    "        if max_concurrent < 4:\n",
    "            recommendations.append(\"並發序列數較少，考慮優化記憶體配置\")\n",
    "        elif max_concurrent > 100:\n",
    "            recommendations.append(\"並發序列數很高，確保有足夠的計算資源\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"配置看起來合理，可以進行進一步的性能測試\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_optimized_vllm_config(self, \n",
    "                                   base_config: Dict[str, Any],\n",
    "                                   model_size_gb: float) -> Dict[str, Any]:\n",
    "        \"\"\"創建優化的 vLLM 配置\"\"\"\n",
    "        \n",
    "        # 計算最佳參數\n",
    "        optimization = self.calculate_optimal_block_size(model_size_gb)\n",
    "        \n",
    "        # 更新配置\n",
    "        optimized_config = base_config.copy()\n",
    "        \n",
    "        # 添加 PagedAttention 優化參數\n",
    "        optimized_params = {\n",
    "            \"block_size\": {\"string_value\": str(optimization[\"block_size\"])},\n",
    "            \"max_num_seqs\": {\"string_value\": str(min(256, optimization[\"max_concurrent_sequences\"]))},\n",
    "            \"gpu_memory_utilization\": {\"string_value\": \"0.85\"},\n",
    "            \"swap_space\": {\"string_value\": \"4\"},  # GB\n",
    "            \"enforce_eager\": {\"string_value\": \"false\"},\n",
    "            \"enable_chunked_prefill\": {\"string_value\": \"true\"}\n",
    "        }\n",
    "        \n",
    "        # 合併參數\n",
    "        if \"parameters\" not in optimized_config:\n",
    "            optimized_config[\"parameters\"] = {}\n",
    "        \n",
    "        optimized_config[\"parameters\"].update(optimized_params)\n",
    "        \n",
    "        return optimized_config, optimization\n",
    "\n",
    "# 創建 PagedAttention 優化器\n",
    "paged_optimizer = PagedAttentionOptimizer()\n",
    "print(\"🔧 PagedAttention 優化器已創建\")\n",
    "\n",
    "# 計算當前模型的最佳配置\n",
    "model_sizes = {\n",
    "    \"gpt2\": 0.5,\n",
    "    \"microsoft/DialoGPT-medium\": 1.5,\n",
    "    \"microsoft/DialoGPT-large\": 3.0\n",
    "}\n",
    "\n",
    "current_model_size = model_sizes.get(model_choice, 1.0)\n",
    "optimization_result = paged_optimizer.calculate_optimal_block_size(current_model_size)\n",
    "\n",
    "print(f\"\\n📊 模型: {model_choice} ({current_model_size}GB)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"建議塊大小: {optimization_result['block_size']}\")\n",
    "print(f\"最大塊數量: {optimization_result['max_num_blocks']}\")\n",
    "print(f\"最大並發序列: {optimization_result['max_concurrent_sequences']}\")\n",
    "\n",
    "print(\"\\n💾 記憶體使用估算:\")\n",
    "for key, value in optimization_result['estimated_memory_usage'].items():\n",
    "    print(f\"{key}: {value:.2f}GB\")\n",
    "\n",
    "print(\"\\n💡 優化建議:\")\n",
    "for i, rec in enumerate(optimization_result['recommendations'], 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 優化配置部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建優化後的模型配置\n",
    "optimized_model_name = \"vllm_optimized_model\"\n",
    "\n",
    "# 獲取基礎配置\n",
    "base_config = vllm_repo.create_vllm_model_config(\n",
    "    model_name=optimized_model_name,\n",
    "    hf_model_name=model_choice,\n",
    "    max_model_len=max_model_len,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# 應用 PagedAttention 優化\n",
    "optimized_config, optimization_details = paged_optimizer.create_optimized_vllm_config(\n",
    "    base_config, current_model_size\n",
    ")\n",
    "\n",
    "# 寫入優化配置\n",
    "optimized_config_path = vllm_repo.write_config_pbtxt(optimized_config, optimized_model_name)\n",
    "optimized_model_py_path = vllm_repo.create_vllm_model_py(optimized_model_name)\n",
    "\n",
    "print(f\"✅ 優化配置已創建: {optimized_config_path}\")\n",
    "print(f\"✅ 優化 model.py 已創建: {optimized_model_py_path}\")\n",
    "\n",
    "# 顯示配置對比\n",
    "def compare_configs(base_config: Dict, optimized_config: Dict):\n",
    "    \"\"\"對比配置差異\"\"\"\n",
    "    print(\"\\n⚖️  配置對比\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    base_params = base_config.get(\"parameters\", {})\n",
    "    opt_params = optimized_config.get(\"parameters\", {})\n",
    "    \n",
    "    all_keys = set(base_params.keys()) | set(opt_params.keys())\n",
    "    \n",
    "    for key in sorted(all_keys):\n",
    "        base_val = base_params.get(key, {}).get(\"string_value\", \"未設定\")\n",
    "        opt_val = opt_params.get(key, {}).get(\"string_value\", \"未設定\")\n",
    "        \n",
    "        if base_val != opt_val:\n",
    "            print(f\"{key:25} | {base_val:15} -> {opt_val:15}\")\n",
    "        else:\n",
    "            print(f\"{key:25} | {base_val:15}   (相同)\")\n",
    "\n",
    "compare_configs(base_config, optimized_config)\n",
    "\n",
    "# 顯示最終倉庫結構\n",
    "print(f\"\\n📂 最終模型倉庫結構:\")\n",
    "show_repository_structure(vllm_repo.repository_path, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 性能監控與分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMPerformanceMonitor:\n",
    "    \"\"\"vLLM 性能監控器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics_history = []\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def collect_system_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"收集系統指標\"\"\"\n",
    "        metrics = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"cpu_percent\": psutil.cpu_percent(),\n",
    "            \"memory_percent\": psutil.virtual_memory().percent,\n",
    "            \"memory_used_gb\": psutil.virtual_memory().used / 1e9\n",
    "        }\n",
    "        \n",
    "        # GPU 指標\n",
    "        if torch.cuda.is_available():\n",
    "            metrics.update({\n",
    "                \"gpu_memory_used\": torch.cuda.memory_allocated() / 1e9,\n",
    "                \"gpu_memory_cached\": torch.cuda.memory_reserved() / 1e9,\n",
    "                \"gpu_utilization\": self._get_gpu_utilization()\n",
    "            })\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _get_gpu_utilization(self) -> float:\n",
    "        \"\"\"獲取 GPU 使用率（簡化版）\"\"\"\n",
    "        try:\n",
    "            # 使用 nvidia-ml-py 會更準確，這裡用簡化估算\n",
    "            allocated = torch.cuda.memory_allocated()\n",
    "            total = torch.cuda.get_device_properties(0).total_memory\n",
    "            return (allocated / total) * 100\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def log_inference_metrics(self, \n",
    "                            batch_size: int,\n",
    "                            sequence_length: int,\n",
    "                            inference_time: float,\n",
    "                            tokens_generated: int):\n",
    "        \"\"\"記錄推理指標\"\"\"\n",
    "        system_metrics = self.collect_system_metrics()\n",
    "        \n",
    "        inference_metrics = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"inference_time\": inference_time,\n",
    "            \"tokens_generated\": tokens_generated,\n",
    "            \"tokens_per_second\": tokens_generated / inference_time if inference_time > 0 else 0,\n",
    "            \"throughput\": batch_size / inference_time if inference_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        combined_metrics = {**system_metrics, **inference_metrics}\n",
    "        self.metrics_history.append(combined_metrics)\n",
    "        \n",
    "        return combined_metrics\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"獲取性能總結\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.metrics_history)\n",
    "        \n",
    "        summary = {\n",
    "            \"total_inferences\": len(self.metrics_history),\n",
    "            \"avg_inference_time\": df[\"inference_time\"].mean(),\n",
    "            \"avg_tokens_per_second\": df[\"tokens_per_second\"].mean(),\n",
    "            \"avg_throughput\": df[\"throughput\"].mean(),\n",
    "            \"peak_memory_usage\": df[\"memory_used_gb\"].max(),\n",
    "            \"avg_cpu_usage\": df[\"cpu_percent\"].mean()\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            summary.update({\n",
    "                \"peak_gpu_memory\": df[\"gpu_memory_used\"].max(),\n",
    "                \"avg_gpu_utilization\": df[\"gpu_utilization\"].mean()\n",
    "            })\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_performance_charts(self):\n",
    "        \"\"\"繪製性能圖表\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"無性能數據可視化\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.metrics_history)\n",
    "        df['relative_time'] = df['timestamp'] - df['timestamp'].iloc[0]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('vLLM Backend 性能監控', fontsize=16)\n",
    "        \n",
    "        # 推理延遲\n",
    "        axes[0, 0].plot(df['relative_time'], df['inference_time'], 'b-', linewidth=2)\n",
    "        axes[0, 0].set_title('推理延遲 (秒)')\n",
    "        axes[0, 0].set_xlabel('時間 (秒)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 吞吐量\n",
    "        axes[0, 1].plot(df['relative_time'], df['tokens_per_second'], 'g-', linewidth=2)\n",
    "        axes[0, 1].set_title('Token 生成速率 (tokens/sec)')\n",
    "        axes[0, 1].set_xlabel('時間 (秒)')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 記憶體使用\n",
    "        axes[1, 0].plot(df['relative_time'], df['memory_used_gb'], 'r-', linewidth=2, label='CPU 記憶體')\n",
    "        if torch.cuda.is_available() and 'gpu_memory_used' in df.columns:\n",
    "            axes[1, 0].plot(df['relative_time'], df['gpu_memory_used'], 'orange', linewidth=2, label='GPU 記憶體')\n",
    "        axes[1, 0].set_title('記憶體使用 (GB)')\n",
    "        axes[1, 0].set_xlabel('時間 (秒)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # CPU/GPU 使用率\n",
    "        axes[1, 1].plot(df['relative_time'], df['cpu_percent'], 'purple', linewidth=2, label='CPU %')\n",
    "        if torch.cuda.is_available() and 'gpu_utilization' in df.columns:\n",
    "            axes[1, 1].plot(df['relative_time'], df['gpu_utilization'], 'red', linewidth=2, label='GPU %')\n",
    "        axes[1, 1].set_title('資源使用率 (%)')\n",
    "        axes[1, 1].set_xlabel('時間 (秒)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 創建性能監控器\n",
    "monitor = vLLMPerformanceMonitor()\n",
    "print(\"📊 vLLM 性能監控器已創建\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬性能監控數據\n",
    "def simulate_performance_monitoring():\n",
    "    \"\"\"模擬性能監控過程\"\"\"\n",
    "    print(\"🔄 模擬 vLLM Backend 性能監控...\")\n",
    "    \n",
    "    # 模擬多個推理請求\n",
    "    test_scenarios = [\n",
    "        {\"batch_size\": 1, \"seq_len\": 50, \"tokens\": 25, \"time\": 0.15},\n",
    "        {\"batch_size\": 2, \"seq_len\": 100, \"tokens\": 60, \"time\": 0.28},\n",
    "        {\"batch_size\": 4, \"seq_len\": 150, \"tokens\": 120, \"time\": 0.45},\n",
    "        {\"batch_size\": 1, \"seq_len\": 200, \"tokens\": 80, \"time\": 0.32},\n",
    "        {\"batch_size\": 3, \"seq_len\": 75, \"tokens\": 90, \"time\": 0.25},\n",
    "        {\"batch_size\": 2, \"seq_len\": 120, \"tokens\": 70, \"time\": 0.22},\n",
    "        {\"batch_size\": 1, \"seq_len\": 300, \"tokens\": 150, \"time\": 0.65}\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios):\n",
    "        # 模擬推理延遲\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # 記錄指標\n",
    "        metrics = monitor.log_inference_metrics(\n",
    "            batch_size=scenario[\"batch_size\"],\n",
    "            sequence_length=scenario[\"seq_len\"],\n",
    "            inference_time=scenario[\"time\"],\n",
    "            tokens_generated=scenario[\"tokens\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"推理 {i+1}: {scenario['batch_size']}x{scenario['seq_len']} -> {metrics['tokens_per_second']:.1f} tokens/s\")\n",
    "    \n",
    "    # 獲取性能總結\n",
    "    summary = monitor.get_performance_summary()\n",
    "    \n",
    "    print(\"\\n📊 性能總結\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key:25}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"{key:25}: {value}\")\n",
    "    \n",
    "    # 繪製性能圖表\n",
    "    print(\"\\n📈 性能趨勢圖表\")\n",
    "    monitor.plot_performance_charts()\n",
    "\n# 執行性能監控模擬\nsimulate_performance_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 部署腳本與最佳實踐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMDeploymentManager:\n",
    "    \"\"\"vLLM 部署管理器\"\"\"\n",
    "    \n",
    "    def __init__(self, repository_path: str):\n",
    "        self.repository_path = Path(repository_path)\n",
    "        self.deployment_scripts = {}\n",
    "    \n",
    "    def generate_docker_compose(self, \n",
    "                               model_name: str,\n",
    "                               gpu_ids: str = \"all\") -> str:\n",
    "        \"\"\"生成 Docker Compose 配置\"\"\"\n",
    "        \n",
    "        compose_content = f'''\nversion: '3.8'\n\nservices:\n  triton-vllm:\n    image: nvcr.io/nvidia/tritonserver:23.10-vllm-python-py3\n    ports:\n      - \"8000:8000\"  # HTTP\n      - \"8001:8001\"  # GRPC\n      - \"8002:8002\"  # Metrics\n    volumes:\n      - ./model_repository:/models\n      - ./logs:/logs\n    environment:\n      - CUDA_VISIBLE_DEVICES={gpu_ids}\n      - TRITON_LOG_LEVEL=INFO\n    command: >\n      tritonserver\n      --model-repository=/models\n      --http-port=8000\n      --grpc-port=8001\n      --metrics-port=8002\n      --log-verbose=1\n      --backend-directory=/opt/tritonserver/backends\n      --backend-config=vllm,max_batch_size=256\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/v2/health/ready\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n    restart: unless-stopped\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - grafana-storage:/var/lib/grafana\n      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards\n      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    restart: unless-stopped\n\nvolumes:\n  grafana-storage:\n\nnetworks:\n  default:\n    name: triton-vllm-network\n'''\n        \n        return compose_content\n    \n    def generate_kubernetes_deployment(self, \n                                     model_name: str,\n                                     replicas: int = 1,\n                                     gpu_memory: str = \"16Gi\") -> str:\n",
    "        \"\"\"生成 Kubernetes 部署配置\"\"\"\n",
    "        \n",
    "        k8s_content = f'''\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: triton-vllm-{model_name.lower().replace('_', '-')}\n  labels:\n    app: triton-vllm\n    model: {model_name}\nspec:\n  replicas: {replicas}\n  selector:\n    matchLabels:\n      app: triton-vllm\n      model: {model_name}\n  template:\n    metadata:\n      labels:\n        app: triton-vllm\n        model: {model_name}\n    spec:\n      containers:\n      - name: triton-vllm\n        image: nvcr.io/nvidia/tritonserver:23.10-vllm-python-py3\n        ports:\n        - containerPort: 8000\n          name: http\n        - containerPort: 8001\n          name: grpc\n        - containerPort: 8002\n          name: metrics\n        env:\n        - name: CUDA_VISIBLE_DEVICES\n          value: \"0\"\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n            memory: {gpu_memory}\n          requests:\n            nvidia.com/gpu: 1\n            memory: \"8Gi\"\n        volumeMounts:\n        - name: model-repository\n          mountPath: /models\n        command:\n        - tritonserver\n        - --model-repository=/models\n        - --http-port=8000\n        - --grpc-port=8001\n        - --metrics-port=8002\n        - --log-verbose=1\n        - --backend-config=vllm,max_batch_size=256\n        livenessProbe:\n          httpGet:\n            path: /v2/health/live\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /v2/health/ready\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 15\n      volumes:\n      - name: model-repository\n        configMap:\n          name: {model_name.lower().replace('_', '-')}-config\n      nodeSelector:\n        accelerator: nvidia-tesla-v100  # 根據實際 GPU 調整\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: triton-vllm-service\nspec:\n  selector:\n    app: triton-vllm\n  ports:\n  - name: http\n    port: 8000\n    targetPort: 8000\n  - name: grpc\n    port: 8001\n    targetPort: 8001\n  - name: metrics\n    port: 8002\n    targetPort: 8002\n  type: LoadBalancer\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {model_name.lower().replace('_', '-')}-config\ndata:\n  # 這裡應該包含實際的模型配置文件\n  config.pbtxt: |\n    # 模型配置內容\n'''\n        \n        return k8s_content\n    \n    def generate_monitoring_config(self) -> Dict[str, str]:\n        \"\"\"生成監控配置\"\"\"\n",
    "        \n",
    "        # Prometheus 配置\n",
    "        prometheus_config = '''\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'triton-server'\n    static_configs:\n      - targets: ['triton-vllm:8002']\n    metrics_path: '/metrics'\n    scrape_interval: 10s\n    \n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n    scrape_interval: 15s\n'''\n        \n        # Grafana Dashboard 配置\n        grafana_dashboard = '''\n{\n  \"dashboard\": {\n    \"id\": null,\n    \"title\": \"Triton vLLM Performance\",\n    \"tags\": [\"triton\", \"vllm\", \"inference\"],\n    \"timezone\": \"browser\",\n    \"panels\": [\n      {\n        \"id\": 1,\n        \"title\": \"Request Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(nv_inference_request_success_total[5m])\",\n            \"legendFormat\": \"Successful Requests/sec\"\n          }\n        ],\n        \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0}\n      },\n      {\n        \"id\": 2,\n        \"title\": \"Inference Latency\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, rate(nv_inference_request_duration_us_bucket[5m]))\",\n            \"legendFormat\": \"95th Percentile\"\n          },\n          {\n            \"expr\": \"histogram_quantile(0.50, rate(nv_inference_request_duration_us_bucket[5m]))\",\n            \"legendFormat\": \"50th Percentile\"\n          }\n        ],\n        \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 0}\n      },\n      {\n        \"id\": 3,\n        \"title\": \"GPU Memory Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"nv_gpu_memory_used_bytes / nv_gpu_memory_total_bytes * 100\",\n            \"legendFormat\": \"GPU Memory %\"\n          }\n        ],\n        \"gridPos\": {\"h\": 8, \"w\": 24, \"x\": 0, \"y\": 8}\n      }\n    ],\n    \"time\": {\n      \"from\": \"now-1h\",\n      \"to\": \"now\"\n    },\n    \"refresh\": \"10s\"\n  }\n}\n'''\n        \n        return {\n            \"prometheus.yml\": prometheus_config,\n            \"grafana_dashboard.json\": grafana_dashboard\n        }\n    \n    def generate_deployment_scripts(self, model_name: str) -> Dict[str, str]:\n        \"\"\"生成所有部署腳本\"\"\"\n",
    "        scripts = {}\n",
    "        \n",
    "        # Docker Compose\n",
    "        scripts[\"docker-compose.yml\"] = self.generate_docker_compose(model_name)\n",
    "        \n",
    "        # Kubernetes\n",
    "        scripts[\"k8s-deployment.yaml\"] = self.generate_kubernetes_deployment(model_name)\n",
    "        \n",
    "        # 監控配置\n",
    "        scripts.update(self.generate_monitoring_config())\n",
    "        \n",
    "        # 啟動腳本\n",
    "        scripts[\"start.sh\"] = f'''\n#!/bin/bash\n\nset -e\n\necho \"🚀 啟動 Triton vLLM 服務\"\n\n# 檢查 Docker\nif ! command -v docker &> /dev/null; then\n    echo \"❌ Docker 未安裝\"\n    exit 1\nfi\n\n# 檢查 NVIDIA Docker\nif ! docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi &> /dev/null; then\n    echo \"❌ NVIDIA Docker 不可用\"\n    exit 1\nfi\n\n# 創建必要目錄\nmkdir -p logs\nmkdir -p monitoring/grafana/dashboards\nmkdir -p monitoring/grafana/datasources\n\n# 複製監控配置\ncp prometheus.yml monitoring/\ncp grafana_dashboard.json monitoring/grafana/dashboards/\n\n# 啟動服務\necho \"🐳 啟動 Docker Compose 服務...\"\ndocker-compose up -d\n\n# 等待服務就緒\necho \"⏳ 等待服務啟動...\"\nsleep 30\n\n# 健康檢查\necho \"🔍 檢查服務健康狀態\"\nif curl -f http://localhost:8000/v2/health/ready; then\n    echo \"✅ Triton Server 就緒\"\nelse\n    echo \"❌ Triton Server 未就緒\"\n    docker-compose logs triton-vllm\n    exit 1\nfi\n\necho \"🎉 部署完成！\"\necho \"   - Triton Server: http://localhost:8000\"\necho \"   - Prometheus: http://localhost:9090\"\necho \"   - Grafana: http://localhost:3000 (admin/admin)\"\n'''\n        \n        # 停止腳本\n        scripts[\"stop.sh\"] = '''\n#!/bin/bash\n\necho \"🛑 停止 Triton vLLM 服務\"\ndocker-compose down\necho \"✅ 服務已停止\"\n'''\n        \n        return scripts\n    \n    def save_deployment_files(self, model_name: str, output_dir: str = \"./deployment\"):\n        \"\"\"保存部署文件\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n        \n        scripts = self.generate_deployment_scripts(model_name)\n        \n        for filename, content in scripts.items():\n            file_path = output_path / filename\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.write(content)\n            \n            # 為 shell 腳本添加執行權限\n            if filename.endswith('.sh'):\n                os.chmod(file_path, 0o755)\n        \n        logger.info(f\"部署文件已保存至: {output_path.absolute()}\")\n        return output_path\n\n# 創建部署管理器\ndeployment_manager = vLLMDeploymentManager(str(vllm_repo.repository_path))\nprint(\"🚀 vLLM 部署管理器已創建\")\n\n# 生成部署文件\ndeployment_path = deployment_manager.save_deployment_files(optimized_model_name)\nprint(f\"✅ 部署文件已生成: {deployment_path}\")\n\n# 顯示生成的文件\nprint(\"\\n📁 生成的部署文件:\")\nfor file_path in sorted(deployment_path.iterdir()):\n    print(f\"  - {file_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 最佳實踐與總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM Backend 最佳實踐指南\n",
    "best_practices = {\n",
    "    \"配置優化\": [\n",
    "        \"根據 GPU 記憶體動態調整 block_size\",\n",
    "        \"設置合理的 gpu_memory_utilization (0.85-0.9)\",\n",
    "        \"啟用 PagedAttention 以提高記憶體效率\",\n",
    "        \"配置 swap_space 以處理記憶體溢出\",\n",
    "        \"使用 continuous batching 提高吞吐量\"\n",
    "    ],\n",
    "    \"性能調優\": [\n",
    "        \"選擇適當的 tensor_parallel_size\",\n",
    "        \"啟用 CUDA Graph 以減少 kernel 啟動開銷\",\n",
    "        \"使用混合精度 (FP16) 加速推理\",\n",
    "        \"配置動態批次處理參數\",\n",
    "        \"監控 KV cache 使用情況\"\n",
    "    ],\n",
    "    \"部署策略\": [\n",
    "        \"使用 Docker 容器化部署\",\n",
    "        \"配置健康檢查和自動重啟\",\n",
    "        \"設置負載均衡和故障轉移\",\n",
    "        \"實施滾動更新策略\",\n",
    "        \"配置資源限制和請求\"\n",
    "    ],\n",
    "    \"監控運維\": [\n",
    "        \"監控推理延遲和吞吐量\",\n",
    "        \"追蹤 GPU 記憶體使用率\",\n",
    "        \"設置告警閾值\",\n",
    "        \"收集業務指標\",\n",
    "        \"定期性能基準測試\"\n",
    "    ],\n",
    "    \"安全考慮\": [\n",
    "        \"限制模型訪問權限\",\n",
    "        \"驗證輸入內容\",\n",
    "        \"設置請求速率限制\",\n",
    "        \"加密敏感配置\",\n",
    "        \"定期安全審計\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"📋 vLLM Backend 最佳實踐\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n🎯 {category}:\")\n",
    "    for i, practice in enumerate(practices, 1):\n",
    "        print(f\"   {i}. {practice}\")\n",
    "\n",
    "# 實驗總結\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎓 Lab-2.3.3 總結\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_points = [\n",
    "    \"✅ 成功設計並實現了 vLLM Backend 整合架構\",\n",
    "    \"✅ 掌握了 PagedAttention 優化配置技術\",\n",
    "    \"✅ 建立了完整的性能監控體系\",\n",
    "    \"✅ 創建了企業級部署自動化腳本\",\n",
    "    \"✅ 理解了 Triton + vLLM 的混合部署策略\"\n",
    "]\n",
    "\n",
    "for point in summary_points:\n",
    "    print(point)\n",
    "\n",
    "print(\"\\n🚀 下一步建議:\")\n",
    "next_steps = [\n",
    "    \"在實際環境中部署和測試 vLLM Backend\",\n",
    "    \"探索多模型並行部署策略\",\n",
    "    \"實現自定義 Python Backend (Lab-2.3.4)\",\n",
    "    \"集成 MLOps 工作流程\",\n",
    "    \"優化大規模生產部署\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print(f\"\\n⏱️  實驗完成時間: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"📊 實驗評估: vLLM Backend 整合 - 企業級就緒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔚 實驗室結語\n",
    "\n",
    "本實驗室深入探討了 **Triton + vLLM Backend** 的整合技術，涵蓋了:\n",
    "\n",
    "### 🏆 核心成就\n",
    "\n",
    "1. **架構設計**: 掌握了企業級 vLLM Backend 架構\n",
    "2. **性能優化**: 實現了 PagedAttention 記憶體效率最大化\n",
    "3. **監控體系**: 建立了完整的性能追蹤和分析框架\n",
    "4. **部署自動化**: 創建了生產就緒的部署管道\n",
    "5. **最佳實踐**: 總結了企業級 LLM 服務運維經驗\n",
    "\n",
    "### 🎯 技能提升\n",
    "\n",
    "- ✨ **LLM 服務化**: 從模型到服務的完整轉換\n",
    "- ✨ **記憶體優化**: PagedAttention 核心原理與實踐\n",
    "- ✨ **企業部署**: Docker/K8s 容器化部署策略\n",
    "- ✨ **性能監控**: 全方位指標收集與分析\n",
    "- ✨ **運維自動化**: DevOps 與 MLOps 整合\n",
    "\n",
    "### 🚀 實戰價值\n",
    "\n",
    "此實驗室提供的技術棧和方法論可以直接應用於:\n",
    "- **大規模 LLM 部署項目**\n",
    "- **高併發推理服務設計**\n",
    "- **企業級 AI 平台建設**\n",
    "- **MLOps 工程實施**\n",
    "\n",
    "---\n",
    "\n",
    "*下一個實驗室: **04-Custom_Python_Backend** - 自定義業務邏輯實現*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}