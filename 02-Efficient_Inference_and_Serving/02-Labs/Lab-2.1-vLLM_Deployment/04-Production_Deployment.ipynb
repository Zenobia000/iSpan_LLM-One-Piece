{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1 Part 4: Production Deployment\n",
    "\n",
    "## Objectives\n",
    "- Deploy OpenAI-compatible API server\n",
    "- Optimize performance for production\n",
    "- Set up monitoring and logging\n",
    "- Learn deployment best practices\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. OpenAI-Compatible API Server\n",
    "\n",
    "vLLM provides an OpenAI-compatible API server out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Server\n",
    "\n",
    "Run this in a **separate terminal**:\n",
    "\n",
    "```bash\n",
    "# Basic usage\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model meta-llama/Llama-2-7b-hf \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000\n",
    "\n",
    "# With more options\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model meta-llama/Llama-2-7b-hf \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --tensor-parallel-size 1 \\\n",
    "    --gpu-memory-utilization 0.9 \\\n",
    "    --max-num-seqs 32 \\\n",
    "    --max-model-len 2048\n",
    "```\n",
    "\n",
    "The server will be available at: `http://localhost:8000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test API Endpoint\n",
    "\n",
    "**Note**: Make sure the vLLM server is running before executing the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if server is running\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/health\")\n",
    "    print(f\"‚úÖ Server is running!\")\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ùå Server is not running.\")\n",
    "    print(\"Please start the server in a terminal first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test /v1/completions endpoint\n",
    "def call_completions_api(prompt: str, **kwargs):\n",
    "    \"\"\"Call vLLM completions API.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": kwargs.get(\"max_tokens\", 50),\n",
    "        \"temperature\": kwargs.get(\"temperature\", 0.8),\n",
    "        \"top_p\": kwargs.get(\"top_p\", 0.95),\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{API_URL}/v1/completions\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json=payload,\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Test\n",
    "print(\"Testing /v1/completions endpoint...\\n\")\n",
    "\n",
    "result = call_completions_api(\"The future of AI is\")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test /v1/chat/completions endpoint\n",
    "def call_chat_api(messages: list, **kwargs):\n",
    "    \"\"\"Call vLLM chat completions API.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": kwargs.get(\"max_tokens\", 100),\n",
    "        \"temperature\": kwargs.get(\"temperature\", 0.8),\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{API_URL}/v1/chat/completions\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json=payload,\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Test\n",
    "print(\"Testing /v1/chat/completions endpoint...\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
    "]\n",
    "\n",
    "result = call_chat_api(messages)\n",
    "\n",
    "if \"choices\" in result:\n",
    "    print(\"Assistant:\", result[\"choices\"][0][\"message\"][\"content\"])\n",
    "    print(f\"\\nTokens used: {result['usage']['total_tokens']}\")\n",
    "else:\n",
    "    print(\"Response:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenAI Python Client\n",
    "\n",
    "vLLM is fully compatible with OpenAI's Python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install openai if needed\n",
    "try:\n",
    "    import openai\n",
    "except ImportError:\n",
    "    print(\"Installing openai...\")\n",
    "    !pip install openai -q\n",
    "    import openai\n",
    "\n",
    "print(f\"OpenAI version: {openai.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI client to use vLLM\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"dummy-key\",  # vLLM doesn't require real API key\n",
    ")\n",
    "\n",
    "print(\"Testing with OpenAI client...\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "    ],\n",
    "    max_tokens=150,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nTokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Performance Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Parameters\n",
    "\n",
    "#### GPU Memory Utilization\n",
    "```bash\n",
    "--gpu-memory-utilization 0.9  # Use 90% of GPU memory\n",
    "```\n",
    "- Higher value ‚Üí larger batch size ‚Üí better throughput\n",
    "- Keep some memory for overhead (10%)\n",
    "\n",
    "#### Max Number of Sequences\n",
    "```bash\n",
    "--max-num-seqs 32  # Process up to 32 requests concurrently\n",
    "```\n",
    "- Higher value ‚Üí better throughput\n",
    "- Limited by GPU memory\n",
    "\n",
    "#### Max Batched Tokens\n",
    "```bash\n",
    "--max-num-batched-tokens 8192\n",
    "```\n",
    "- Controls prefill batch size\n",
    "- Affects TTFT (Time to First Token)\n",
    "\n",
    "#### Block Size\n",
    "```bash\n",
    "--block-size 16  # PagedAttention block size\n",
    "```\n",
    "- Usually 16 is optimal\n",
    "- Smaller = less waste, but more overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Testing\n",
    "\n",
    "Use `locust` or `wrk` for load testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple concurrent test\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def send_request(request_id: int):\n",
    "    \"\"\"Send a single request.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-2-7b-hf\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Tell me a fact about number {request_id}.\"}\n",
    "            ],\n",
    "            max_tokens=50,\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        return {\"id\": request_id, \"time\": elapsed, \"success\": True}\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\"id\": request_id, \"time\": elapsed, \"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Run concurrent requests\n",
    "NUM_REQUESTS = 10\n",
    "NUM_WORKERS = 5\n",
    "\n",
    "print(f\"Sending {NUM_REQUESTS} concurrent requests with {NUM_WORKERS} workers...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = [executor.submit(send_request, i) for i in range(NUM_REQUESTS)]\n",
    "    results = [f.result() for f in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Analyze results\n",
    "successful = sum(1 for r in results if r[\"success\"])\n",
    "latencies = [r[\"time\"] for r in results if r[\"success\"]]\n",
    "\n",
    "print(\"LOAD TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total requests:     {NUM_REQUESTS}\")\n",
    "print(f\"Successful:         {successful}\")\n",
    "print(f\"Failed:             {NUM_REQUESTS - successful}\")\n",
    "print(f\"Total time:         {total_time:.2f}s\")\n",
    "print(f\"Requests/sec:       {NUM_REQUESTS/total_time:.2f}\")\n",
    "print(f\"Avg latency:        {sum(latencies)/len(latencies):.3f}s\")\n",
    "print(f\"Min latency:        {min(latencies):.3f}s\")\n",
    "print(f\"Max latency:        {max(latencies):.3f}s\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Monitoring and Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prometheus Metrics\n",
    "\n",
    "vLLM exposes Prometheus metrics at `/metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Prometheus metrics\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/metrics\")\n",
    "    metrics = response.text\n",
    "    \n",
    "    print(\"Sample metrics:\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show first 20 lines\n",
    "    for line in metrics.split('\\n')[:20]:\n",
    "        if line and not line.startswith('#'):\n",
    "            print(line)\n",
    "    \n",
    "    print(\"...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error fetching metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Metrics to Monitor\n",
    "\n",
    "1. **Request Metrics**\n",
    "   - `vllm:num_requests_running` - Active requests\n",
    "   - `vllm:num_requests_waiting` - Queued requests\n",
    "   - `vllm:request_success_total` - Successful requests\n",
    "\n",
    "2. **Latency Metrics**\n",
    "   - `vllm:time_to_first_token_seconds` - TTFT\n",
    "   - `vllm:time_per_output_token_seconds` - ITL\n",
    "   - `vllm:e2e_request_latency_seconds` - End-to-end latency\n",
    "\n",
    "3. **GPU Metrics**\n",
    "   - `vllm:gpu_cache_usage_perc` - KV cache utilization\n",
    "   - `vllm:gpu_memory_usage_bytes` - GPU memory\n",
    "\n",
    "4. **Throughput Metrics**\n",
    "   - `vllm:num_preemptions_total` - Request preemptions\n",
    "   - `vllm:prompt_tokens_total` - Input tokens\n",
    "   - `vllm:generation_tokens_total` - Output tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple request logger\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def logged_request(prompt: str, **kwargs):\n",
    "    \"\"\"Make request with logging.\"\"\"\n",
    "    request_id = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    \n",
    "    logger.info(f\"Request {request_id} started\")\n",
    "    logger.info(f\"  Prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-2-7b-hf\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        tokens = response.usage.total_tokens\n",
    "        \n",
    "        logger.info(f\"Request {request_id} completed\")\n",
    "        logger.info(f\"  Time: {elapsed:.3f}s\")\n",
    "        logger.info(f\"  Tokens: {tokens}\")\n",
    "        logger.info(f\"  Throughput: {tokens/elapsed:.1f} tokens/s\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        logger.error(f\"Request {request_id} failed\")\n",
    "        logger.error(f\"  Error: {e}\")\n",
    "        logger.error(f\"  Time: {elapsed:.3f}s\")\n",
    "        raise\n",
    "\n",
    "# Test logged request\n",
    "print(\"Testing logged request:\\n\")\n",
    "response = logged_request(\"What is Python?\", max_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deployment Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Deployment\n",
    "\n",
    "#### Dockerfile Example\n",
    "\n",
    "```dockerfile\n",
    "FROM nvidia/cuda:12.1.0-devel-ubuntu22.04\n",
    "\n",
    "# Install Python\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install vLLM\n",
    "RUN pip3 install vllm\n",
    "\n",
    "# Download model (optional, can mount volume instead)\n",
    "# RUN python3 -c \"from transformers import AutoModel; AutoModel.from_pretrained('meta-llama/Llama-2-7b-hf')\"\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Start server\n",
    "CMD [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\n",
    "     \"--model\", \"meta-llama/Llama-2-7b-hf\", \\\n",
    "     \"--host\", \"0.0.0.0\", \\\n",
    "     \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "#### Build and Run\n",
    "\n",
    "```bash\n",
    "# Build\n",
    "docker build -t vllm-server .\n",
    "\n",
    "# Run\n",
    "docker run --gpus all -p 8000:8000 \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    vllm-server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubernetes Deployment\n",
    "\n",
    "#### Deployment YAML\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: vllm-server\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: vllm-server\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: vllm-server\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: vllm\n",
    "        image: vllm-server:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1\n",
    "          requests:\n",
    "            memory: \"32Gi\"\n",
    "            cpu: \"4\"\n",
    "        env:\n",
    "        - name: CUDA_VISIBLE_DEVICES\n",
    "          value: \"0\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: vllm-server\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement health check endpoint\n",
    "def check_health():\n",
    "    \"\"\"Check if vLLM server is healthy.\"\"\"\n",
    "    try:\n",
    "        # Check health endpoint\n",
    "        response = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return False, \"Health check failed\"\n",
    "        \n",
    "        # Test with simple request\n",
    "        test_response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-2-7b-hf\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "            max_tokens=5,\n",
    "        )\n",
    "        \n",
    "        return True, \"Healthy\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Run health check\n",
    "is_healthy, message = check_health()\n",
    "\n",
    "if is_healthy:\n",
    "    print(\"‚úÖ Server is healthy\")\n",
    "else:\n",
    "    print(f\"‚ùå Server health check failed: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "- [ ] **Performance Testing**\n",
    "  - [ ] Load testing completed\n",
    "  - [ ] Latency benchmarks acceptable\n",
    "  - [ ] Memory usage stable\n",
    "\n",
    "- [ ] **Monitoring**\n",
    "  - [ ] Prometheus metrics configured\n",
    "  - [ ] Grafana dashboards set up\n",
    "  - [ ] Alerts configured\n",
    "\n",
    "- [ ] **Reliability**\n",
    "  - [ ] Health checks implemented\n",
    "  - [ ] Auto-restart on failure\n",
    "  - [ ] Load balancing configured\n",
    "\n",
    "- [ ] **Security**\n",
    "  - [ ] API authentication enabled\n",
    "  - [ ] Rate limiting configured\n",
    "  - [ ] Input validation implemented\n",
    "\n",
    "- [ ] **Scalability**\n",
    "  - [ ] Horizontal scaling tested\n",
    "  - [ ] Auto-scaling configured\n",
    "  - [ ] Resource limits set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Common Issues and Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 1: Out of Memory\n",
    "\n",
    "**Symptoms**: CUDA OOM errors\n",
    "\n",
    "**Solutions**:\n",
    "```bash\n",
    "# Reduce GPU memory utilization\n",
    "--gpu-memory-utilization 0.8\n",
    "\n",
    "# Reduce max sequences\n",
    "--max-num-seqs 16\n",
    "\n",
    "# Reduce context length\n",
    "--max-model-len 1024\n",
    "```\n",
    "\n",
    "### Issue 2: High Latency\n",
    "\n",
    "**Symptoms**: Slow response times\n",
    "\n",
    "**Solutions**:\n",
    "```bash\n",
    "# Increase batch size\n",
    "--max-num-seqs 64\n",
    "\n",
    "# Increase batched tokens\n",
    "--max-num-batched-tokens 16384\n",
    "\n",
    "# Enable tensor parallelism (multi-GPU)\n",
    "--tensor-parallel-size 2\n",
    "```\n",
    "\n",
    "### Issue 3: Request Timeouts\n",
    "\n",
    "**Symptoms**: Requests timing out\n",
    "\n",
    "**Solutions**:\n",
    "- Reduce `max_tokens` in requests\n",
    "- Increase server timeout settings\n",
    "- Scale horizontally with load balancer\n",
    "\n",
    "### Issue 4: Inconsistent Performance\n",
    "\n",
    "**Symptoms**: Variable latency\n",
    "\n",
    "**Solutions**:\n",
    "- Check for competing GPU processes\n",
    "- Monitor GPU temperature throttling\n",
    "- Ensure stable power supply\n",
    "- Use dedicated GPU instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "‚úÖ **Completed Lab-2.1**:\n",
    "1. Deployed OpenAI-compatible API server\n",
    "2. Tested completions and chat APIs\n",
    "3. Performed load testing\n",
    "4. Set up monitoring and logging\n",
    "5. Learned deployment best practices\n",
    "\n",
    "üìä **Key Achievements**:\n",
    "- Production-ready vLLM deployment\n",
    "- OpenAI API compatibility\n",
    "- Performance monitoring setup\n",
    "- Understanding of optimization parameters\n",
    "\n",
    "üéì **Skills Acquired**:\n",
    "- vLLM installation and configuration\n",
    "- PagedAttention understanding\n",
    "- Batch inference optimization\n",
    "- Advanced sampling strategies\n",
    "- Production deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **Lab-2.2**: Inference Optimization Techniques\n",
    "- **Lab-2.3**: FastAPI Service Construction\n",
    "- **Lab-2.4**: Production Environment Deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [vLLM GitHub](https://github.com/vllm-project/vllm)\n",
    "- [PagedAttention Paper](https://arxiv.org/abs/2309.06180)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "print(\"‚úÖ Lab-2.1 Complete!\")\n",
    "print(\"\\nCongratulations on mastering vLLM deployment! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
