{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Lab 2.3.4 - è‡ªå®šç¾© Python Backend é–‹ç™¼\n",
    "\n",
    "## ğŸ¯ å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬å¯¦é©—å°‡æ•™æ‚¨å¦‚ä½•ï¼š\n",
    "1. é–‹ç™¼è‡ªå®šç¾© Python Backend è™•ç†è¤‡é›œé‚è¼¯\n",
    "2. å¯¦ç¾å¤šæ­¥é©Ÿæ¨ç†æµç¨‹\n",
    "3. æ•´åˆå¤–éƒ¨æœå‹™å’Œ API\n",
    "4. è™•ç†å‹•æ…‹æ‰¹æ¬¡å’Œæµå¼è¼¸å‡º\n",
    "5. ç›£æ§å’Œèª¿è©¦ Python Backend\n",
    "\n",
    "## ğŸ“‹ å‰ç½®éœ€æ±‚\n",
    "\n",
    "- å®Œæˆ Lab 2.1ï¼ˆTriton åŸºç¤è¨­ç½®ï¼‰\n",
    "- ç†Ÿæ‚‰ Python ç·¨ç¨‹å’Œç•°æ­¥è™•ç†\n",
    "- äº†è§£ REST API å’Œ gRPC å”è­°\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## ğŸ“š ç†è«–èƒŒæ™¯\n",
    "\n",
    "### Python Backend çš„å„ªå‹¢\n",
    "\n",
    "**1. éˆæ´»æ€§**\n",
    "- å¯ä»¥å¯¦ç¾ä»»æ„è¤‡é›œçš„æ¨ç†é‚è¼¯\n",
    "- æ”¯æŒå‹•æ…‹æ¨¡å‹åŠ è¼‰å’Œåˆ‡æ›\n",
    "- æ˜“æ–¼é›†æˆå¤–éƒ¨åº«å’Œæœå‹™\n",
    "\n",
    "**2. å¿«é€Ÿé–‹ç™¼**\n",
    "- Python ç”Ÿæ…‹ç³»çµ±è±å¯Œ\n",
    "- èª¿è©¦å’Œæ¸¬è©¦ä¾¿åˆ©\n",
    "- å¿«é€ŸåŸå‹é©—è­‰\n",
    "\n",
    "**3. é©ç”¨å ´æ™¯**\n",
    "- å¤šæ­¥é©Ÿæ¨ç†ç®¡é“\n",
    "- è¤‡é›œçš„é /å¾Œè™•ç†\n",
    "- é›†æˆå¤–éƒ¨ API å’Œæ•¸æ“šåº«\n",
    "- A/B æ¸¬è©¦å’Œå¯¦é©—åŠŸèƒ½\n",
    "\n",
    "### Python Backend æ¶æ§‹\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Client Request] --> B[Triton Server]\n",
    "    B --> C[Python Backend]\n",
    "    C --> D[Model Instance]\n",
    "    D --> E[Execute Function]\n",
    "    E --> F[External API]\n",
    "    E --> G[Database]\n",
    "    E --> H[Other Models]\n",
    "    E --> I[Response Processing]\n",
    "    I --> B\n",
    "    B --> A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Triton ç›¸é—œ\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# æ©Ÿå™¨å­¸ç¿’ç›¸é—œ\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# æª¢æŸ¥ç’°å¢ƒ\n",
    "print(f\"Python version: {__import__('sys').version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­ç½®å¯¦é©—è·¯å¾‘\n",
    "BASE_DIR = \"/opt/tritonserver\"\n",
    "MODEL_REPO = f\"{BASE_DIR}/models\"\n",
    "PYTHON_BACKEND_DIR = f\"{MODEL_REPO}/custom_pipeline\"\n",
    "\n",
    "# å‰µå»ºç›®éŒ„çµæ§‹\n",
    "os.makedirs(f\"{PYTHON_BACKEND_DIR}/1\", exist_ok=True)\n",
    "\n",
    "print(f\"Model repository: {MODEL_REPO}\")\n",
    "print(f\"Python backend directory: {PYTHON_BACKEND_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 1ï¼šåŸºç¤ Python Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 1.1 å‰µå»ºæ¨¡å‹é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤ Python Backend é…ç½®\n",
    "config_pbtxt = '''\n",
    "name: \"custom_pipeline\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"text_input\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"parameters\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"processed_text\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"metadata\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 2\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/python_env.tar.gz\"}\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(f\"{PYTHON_BACKEND_DIR}/config.pbtxt\", \"w\") as f:\n",
    "    f.write(config_pbtxt)\n",
    "\n",
    "print(\"âœ… åŸºç¤é…ç½®å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 1.2 å¯¦ç¾ Python Backend é‚è¼¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºç¤ Python Backend å¯¦ç¾\n",
    "python_backend_code = '''\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"\n",
    "    è‡ªå®šç¾© Python Backend æ¨¡å‹\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        æ¨¡å‹åˆå§‹åŒ–\n",
    "        \"\"\"\n",
    "        self.model_config = model_config = json.loads(args['model_config'])\n",
    "        \n",
    "        # ç²å–è¼¸å…¥è¼¸å‡ºé…ç½®\n",
    "        input_configs = pb_utils.get_input_config_by_name(\n",
    "            model_config, \"text_input\"\n",
    "        )\n",
    "        output_configs = pb_utils.get_output_config_by_name(\n",
    "            model_config, \"processed_text\"\n",
    "        )\n",
    "        \n",
    "        # åˆå§‹åŒ–è™•ç†ç®¡é“çµ„ä»¶\n",
    "        self.text_processors = {\n",
    "            \"uppercase\": lambda x: x.upper(),\n",
    "            \"lowercase\": lambda x: x.lower(),\n",
    "            \"reverse\": lambda x: x[::-1],\n",
    "            \"word_count\": lambda x: f\"Words: {len(x.split())}\",\n",
    "        }\n",
    "        \n",
    "        # çµ±è¨ˆä¿¡æ¯\n",
    "        self.request_count = 0\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        print(f\"âœ… Python Backend åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(f\"ğŸ“Š å¯ç”¨è™•ç†å™¨: {list(self.text_processors.keys())}\")\n",
    "\n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        è™•ç†æ¨ç†è«‹æ±‚\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            # è§£æè¼¸å…¥\n",
    "            text_input = pb_utils.get_input_tensor_by_name(\n",
    "                request, \"text_input\"\n",
    "            ).as_numpy().astype(str)[0]\n",
    "            \n",
    "            # è§£æåƒæ•¸ï¼ˆå¯é¸ï¼‰\n",
    "            parameters = {}\n",
    "            try:\n",
    "                param_tensor = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"parameters\"\n",
    "                )\n",
    "                if param_tensor is not None:\n",
    "                    parameters = json.loads(\n",
    "                        param_tensor.as_numpy().astype(str)[0]\n",
    "                    )\n",
    "            except:\n",
    "                parameters = {}\n",
    "            \n",
    "            # åŸ·è¡Œè™•ç†\n",
    "            processed_text, metadata = self._process_text(\n",
    "                text_input, parameters\n",
    "            )\n",
    "            \n",
    "            # å‰µå»ºè¼¸å‡ºå¼µé‡\n",
    "            processed_tensor = pb_utils.Tensor(\n",
    "                \"processed_text\",\n",
    "                np.array([processed_text], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            metadata_tensor = pb_utils.Tensor(\n",
    "                \"metadata\",\n",
    "                np.array([json.dumps(metadata)], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            # å‰µå»ºéŸ¿æ‡‰\n",
    "            response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[processed_tensor, metadata_tensor]\n",
    "            )\n",
    "            responses.append(response)\n",
    "            \n",
    "            # æ›´æ–°çµ±è¨ˆ\n",
    "            self.request_count += 1\n",
    "        \n",
    "        return responses\n",
    "\n",
    "    def _process_text(self, text: str, parameters: dict) -> tuple:\n",
    "        \"\"\"\n",
    "        æ–‡æœ¬è™•ç†é‚è¼¯\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ç²å–è™•ç†é¡å‹\n",
    "        process_type = parameters.get(\"type\", \"uppercase\")\n",
    "        \n",
    "        # åŸ·è¡Œè™•ç†\n",
    "        if process_type in self.text_processors:\n",
    "            processed = self.text_processors[process_type](text)\n",
    "        else:\n",
    "            processed = text\n",
    "        \n",
    "        # å‰µå»ºå…ƒæ•¸æ“š\n",
    "        metadata = {\n",
    "            \"original_length\": len(text),\n",
    "            \"processed_length\": len(processed),\n",
    "            \"process_type\": process_type,\n",
    "            \"processing_time_ms\": (time.time() - start_time) * 1000,\n",
    "            \"request_id\": self.request_count + 1,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        \n",
    "        return processed, metadata\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        æ¨¡å‹æ¸…ç†\n",
    "        \"\"\"\n",
    "        total_time = time.time() - self.start_time\n",
    "        print(f\"ğŸ Python Backend çµæŸ\")\n",
    "        print(f\"ğŸ“ˆ ç¸½è«‹æ±‚æ•¸: {self.request_count}\")\n",
    "        print(f\"â±ï¸  ç¸½é‹è¡Œæ™‚é–“: {total_time:.2f}s\")\n",
    "        print(f\"ğŸ“Š å¹³å‡ QPS: {self.request_count/total_time:.2f}\")\n",
    "'''\n",
    "\n",
    "with open(f\"{PYTHON_BACKEND_DIR}/1/model.py\", \"w\") as f:\n",
    "    f.write(python_backend_code)\n",
    "\n",
    "print(\"âœ… Python Backend ä»£ç¢¼å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 2ï¼šé«˜ç´š Pipeline Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºé«˜ç´šç®¡é“æ¨¡å‹ç›®éŒ„\n",
    "ADVANCED_PIPELINE_DIR = f\"{MODEL_REPO}/advanced_pipeline\"\n",
    "os.makedirs(f\"{ADVANCED_PIPELINE_DIR}/1\", exist_ok=True)\n",
    "\n",
    "# é«˜ç´šç®¡é“é…ç½®\n",
    "advanced_config = '''\n",
    "name: \"advanced_pipeline\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 4\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"query\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"context\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "    optional: true\n",
    "  },\n",
    "  {\n",
    "    name: \"config\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"answer\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"confidence\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"sources\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"pipeline_info\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus: [ 0 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(f\"{ADVANCED_PIPELINE_DIR}/config.pbtxt\", \"w\") as f:\n",
    "    f.write(advanced_config)\n",
    "\n",
    "print(\"âœ… é«˜ç´šç®¡é“é…ç½®å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜ç´šç®¡é“å¯¦ç¾\n",
    "advanced_pipeline_code = '''\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import threading\n",
    "import numpy as np\n",
    "import requests\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"\n",
    "    é«˜ç´šç®¡é“è™•ç†æ¨¡å‹\n",
    "    å¯¦ç¾å¤šæ­¥é©Ÿæ¨ç†ï¼šæª¢ç´¢ -> é‡æ’ -> ç”Ÿæˆ -> å¾Œè™•ç†\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        æ¨¡å‹åˆå§‹åŒ–\n",
    "        \"\"\"\n",
    "        self.model_config = json.loads(args['model_config'])\n",
    "        \n",
    "        # åˆå§‹åŒ–çµ„ä»¶\n",
    "        self.retriever = SimpleRetriever()\n",
    "        self.reranker = SimpleReranker()\n",
    "        self.generator = SimpleGenerator()\n",
    "        self.post_processor = PostProcessor()\n",
    "        \n",
    "        # ç·šç¨‹æ± \n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        \n",
    "        # çµ±è¨ˆä¿¡æ¯\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"avg_pipeline_time\": 0.0,\n",
    "            \"start_time\": time.time()\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸš€ é«˜ç´šç®¡é“åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(\"ğŸ“‹ ç®¡é“æ­¥é©Ÿ: æª¢ç´¢ -> é‡æ’ -> ç”Ÿæˆ -> å¾Œè™•ç†\")\n",
    "\n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œç®¡é“æ¨ç†\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            try:\n",
    "                # è§£æè¼¸å…¥\n",
    "                query = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"query\"\n",
    "                ).as_numpy().astype(str)[0]\n",
    "                \n",
    "                # è§£æä¸Šä¸‹æ–‡\n",
    "                context = []\n",
    "                context_tensor = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"context\"\n",
    "                )\n",
    "                if context_tensor is not None:\n",
    "                    context = context_tensor.as_numpy().astype(str).tolist()\n",
    "                \n",
    "                # è§£æé…ç½®\n",
    "                config = {}\n",
    "                config_tensor = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"config\"\n",
    "                )\n",
    "                if config_tensor is not None:\n",
    "                    try:\n",
    "                        config = json.loads(\n",
    "                            config_tensor.as_numpy().astype(str)[0]\n",
    "                        )\n",
    "                    except:\n",
    "                        config = {}\n",
    "                \n",
    "                # åŸ·è¡Œç®¡é“\n",
    "                result = self._execute_pipeline(query, context, config)\n",
    "                \n",
    "                # å‰µå»ºéŸ¿æ‡‰\n",
    "                response = self._create_response(result)\n",
    "                responses.append(response)\n",
    "                \n",
    "                self.stats[\"successful_requests\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è«‹æ±‚è™•ç†å¤±æ•—: {str(e)}\")\n",
    "                \n",
    "                # å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰\n",
    "                error_response = self._create_error_response(str(e))\n",
    "                responses.append(error_response)\n",
    "                \n",
    "                self.stats[\"failed_requests\"] += 1\n",
    "            \n",
    "            self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        return responses\n",
    "\n",
    "    def _execute_pipeline(self, query: str, context: List[str], config: dict) -> dict:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œå®Œæ•´æ¨ç†ç®¡é“\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        pipeline_info = {\n",
    "            \"steps\": [],\n",
    "            \"total_time\": 0.0,\n",
    "            \"query\": query,\n",
    "            \"config\": config\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # æ­¥é©Ÿ 1: æª¢ç´¢ç›¸é—œæ–‡æª”\n",
    "            step_start = time.time()\n",
    "            if not context:\n",
    "                retrieved_docs = self.retriever.retrieve(query, config.get(\"top_k\", 5))\n",
    "            else:\n",
    "                retrieved_docs = context\n",
    "            \n",
    "            step_time = (time.time() - step_start) * 1000\n",
    "            pipeline_info[\"steps\"].append({\n",
    "                \"name\": \"retrieve\",\n",
    "                \"time_ms\": step_time,\n",
    "                \"output_count\": len(retrieved_docs)\n",
    "            })\n",
    "            \n",
    "            # æ­¥é©Ÿ 2: é‡æ’åºæ–‡æª”\n",
    "            step_start = time.time()\n",
    "            reranked_docs = self.reranker.rerank(query, retrieved_docs)\n",
    "            \n",
    "            step_time = (time.time() - step_start) * 1000\n",
    "            pipeline_info[\"steps\"].append({\n",
    "                \"name\": \"rerank\",\n",
    "                \"time_ms\": step_time,\n",
    "                \"rerank_changes\": len(reranked_docs)\n",
    "            })\n",
    "            \n",
    "            # æ­¥é©Ÿ 3: ç”Ÿæˆç­”æ¡ˆ\n",
    "            step_start = time.time()\n",
    "            generation_result = self.generator.generate(\n",
    "                query, reranked_docs, config\n",
    "            )\n",
    "            \n",
    "            step_time = (time.time() - step_start) * 1000\n",
    "            pipeline_info[\"steps\"].append({\n",
    "                \"name\": \"generate\",\n",
    "                \"time_ms\": step_time,\n",
    "                \"tokens_generated\": len(generation_result[\"answer\"].split())\n",
    "            })\n",
    "            \n",
    "            # æ­¥é©Ÿ 4: å¾Œè™•ç†\n",
    "            step_start = time.time()\n",
    "            final_result = self.post_processor.process(\n",
    "                generation_result, config\n",
    "            )\n",
    "            \n",
    "            step_time = (time.time() - step_start) * 1000\n",
    "            pipeline_info[\"steps\"].append({\n",
    "                \"name\": \"post_process\",\n",
    "                \"time_ms\": step_time,\n",
    "                \"final_length\": len(final_result[\"answer\"])\n",
    "            })\n",
    "            \n",
    "            # è¨ˆç®—ç¸½æ™‚é–“\n",
    "            pipeline_info[\"total_time\"] = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # æ·»åŠ æºæ–‡æª”\n",
    "            final_result[\"sources\"] = [doc[\"title\"] for doc in reranked_docs[:3]]\n",
    "            final_result[\"pipeline_info\"] = pipeline_info\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            pipeline_info[\"error\"] = str(e)\n",
    "            pipeline_info[\"total_time\"] = (time.time() - start_time) * 1000\n",
    "            raise e\n",
    "\n",
    "    def _create_response(self, result: dict):\n",
    "        \"\"\"\n",
    "        å‰µå»ºéŸ¿æ‡‰å¼µé‡\n",
    "        \"\"\"\n",
    "        answer_tensor = pb_utils.Tensor(\n",
    "            \"answer\",\n",
    "            np.array([result[\"answer\"]], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        confidence_tensor = pb_utils.Tensor(\n",
    "            \"confidence\",\n",
    "            np.array([result[\"confidence\"]], dtype=np.float32)\n",
    "        )\n",
    "        \n",
    "        sources_tensor = pb_utils.Tensor(\n",
    "            \"sources\",\n",
    "            np.array(result[\"sources\"], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        pipeline_info_tensor = pb_utils.Tensor(\n",
    "            \"pipeline_info\",\n",
    "            np.array([json.dumps(result[\"pipeline_info\"])], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        return pb_utils.InferenceResponse(\n",
    "            output_tensors=[\n",
    "                answer_tensor, confidence_tensor,\n",
    "                sources_tensor, pipeline_info_tensor\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _create_error_response(self, error_msg: str):\n",
    "        \"\"\"\n",
    "        å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰\n",
    "        \"\"\"\n",
    "        return pb_utils.InferenceResponse(\n",
    "            output_tensors=[\n",
    "                pb_utils.Tensor(\"answer\", np.array([f\"Error: {error_msg}\"], dtype=np.object_)),\n",
    "                pb_utils.Tensor(\"confidence\", np.array([0.0], dtype=np.float32)),\n",
    "                pb_utils.Tensor(\"sources\", np.array([], dtype=np.object_)),\n",
    "                pb_utils.Tensor(\"pipeline_info\", np.array([json.dumps({\"error\": error_msg})], dtype=np.object_))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        æ¸…ç†è³‡æº\n",
    "        \"\"\"\n",
    "        self.executor.shutdown(wait=True)\n",
    "        \n",
    "        total_time = time.time() - self.stats[\"start_time\"]\n",
    "        print(f\"ğŸ é«˜ç´šç®¡é“çµæŸ\")\n",
    "        print(f\"ğŸ“Š çµ±è¨ˆä¿¡æ¯:\")\n",
    "        print(f\"   ç¸½è«‹æ±‚: {self.stats['total_requests']}\")\n",
    "        print(f\"   æˆåŠŸ: {self.stats['successful_requests']}\")\n",
    "        print(f\"   å¤±æ•—: {self.stats['failed_requests']}\")\n",
    "        print(f\"   æˆåŠŸç‡: {self.stats['successful_requests']/max(self.stats['total_requests'], 1)*100:.1f}%\")\n",
    "\n",
    "\n",
    "# è¼”åŠ©é¡å¯¦ç¾\n",
    "class SimpleRetriever:\n",
    "    def __init__(self):\n",
    "        # æ¨¡æ“¬æ–‡æª”åº«\n",
    "        self.documents = [\n",
    "            {\"id\": 1, \"title\": \"Python åŸºç¤\", \"content\": \"Python æ˜¯ä¸€ç¨®è§£é‡‹å‹ç¨‹å¼èªè¨€...\"},\n",
    "            {\"id\": 2, \"title\": \"æ©Ÿå™¨å­¸ç¿’å…¥é–€\", \"content\": \"æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„å­é ˜åŸŸ...\"},\n",
    "            {\"id\": 3, \"title\": \"æ·±åº¦å­¸ç¿’æ¡†æ¶\", \"content\": \"PyTorch å’Œ TensorFlow æ˜¯æµè¡Œçš„æ¡†æ¶...\"},\n",
    "            {\"id\": 4, \"title\": \"è‡ªç„¶èªè¨€è™•ç†\", \"content\": \"NLP è™•ç†äººé¡èªè¨€çš„è¨ˆç®—æ–¹æ³•...\"},\n",
    "            {\"id\": 5, \"title\": \"Transformer æ¶æ§‹\", \"content\": \"æ³¨æ„åŠ›æ©Ÿåˆ¶æ˜¯ Transformer çš„æ ¸å¿ƒ...\"}\n",
    "        ]\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5):\n",
    "        # ç°¡å–®çš„é—œéµè©åŒ¹é…\n",
    "        scored_docs = []\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            score = 0\n",
    "            if query_lower in doc[\"title\"].lower():\n",
    "                score += 2\n",
    "            if query_lower in doc[\"content\"].lower():\n",
    "                score += 1\n",
    "            \n",
    "            scored_docs.append((doc, score))\n",
    "        \n",
    "        # æ’åºä¸¦è¿”å› top_k\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, score in scored_docs[:top_k]]\n",
    "\n",
    "\n",
    "class SimpleReranker:\n",
    "    def rerank(self, query: str, documents: List[dict]):\n",
    "        # ç°¡å–®é‡æ’ï¼šæ ¹æ“šæ¨™é¡Œç›¸é—œæ€§\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        scored_docs = []\n",
    "        for doc in documents:\n",
    "            title_words = set(doc[\"title\"].lower().split())\n",
    "            overlap = len(query_words & title_words)\n",
    "            scored_docs.append((doc, overlap))\n",
    "        \n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, score in scored_docs]\n",
    "\n",
    "\n",
    "class SimpleGenerator:\n",
    "    def generate(self, query: str, documents: List[dict], config: dict):\n",
    "        # ç°¡å–®ç”Ÿæˆï¼šåŸºæ–¼æ¨¡æ¿\n",
    "        if not documents:\n",
    "            return {\n",
    "                \"answer\": \"æŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ç›¸é—œä¿¡æ¯ã€‚\",\n",
    "                \"confidence\": 0.1\n",
    "            }\n",
    "        \n",
    "        # ä½¿ç”¨ç¬¬ä¸€å€‹æ–‡æª”ç”Ÿæˆç­”æ¡ˆ\n",
    "        top_doc = documents[0]\n",
    "        answer = f\"æ ¹æ“š'{top_doc['title']}'ï¼Œ{top_doc['content'][:100]}...\"\n",
    "        \n",
    "        # è¨ˆç®—ç½®ä¿¡åº¦ï¼ˆåŸºæ–¼æ–‡æª”æ•¸é‡å’ŒåŒ¹é…åº¦ï¼‰\n",
    "        confidence = min(0.9, 0.3 + 0.1 * len(documents))\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "\n",
    "\n",
    "class PostProcessor:\n",
    "    def process(self, generation_result: dict, config: dict):\n",
    "        # å¾Œè™•ç†ï¼šæ ¼å¼åŒ–å’Œå„ªåŒ–\n",
    "        answer = generation_result[\"answer\"]\n",
    "        \n",
    "        # æ‡‰ç”¨é…ç½®\n",
    "        if config.get(\"format\") == \"markdown\":\n",
    "            answer = f\"**ç­”æ¡ˆ:** {answer}\"\n",
    "        \n",
    "        if config.get(\"max_length\"):\n",
    "            max_len = config[\"max_length\"]\n",
    "            if len(answer) > max_len:\n",
    "                answer = answer[:max_len-3] + \"...\"\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": generation_result[\"confidence\"]\n",
    "        }\n",
    "'''\n",
    "\n",
    "with open(f\"{ADVANCED_PIPELINE_DIR}/1/model.py\", \"w\") as f:\n",
    "    f.write(advanced_pipeline_code)\n",
    "\n",
    "print(\"âœ… é«˜ç´šç®¡é“ä»£ç¢¼å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 3ï¼šç•°æ­¥è™•ç† Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºç•°æ­¥è™•ç†æ¨¡å‹ç›®éŒ„\n",
    "ASYNC_BACKEND_DIR = f\"{MODEL_REPO}/async_processor\"\n",
    "os.makedirs(f\"{ASYNC_BACKEND_DIR}/1\", exist_ok=True)\n",
    "\n",
    "# ç•°æ­¥è™•ç†é…ç½®\n",
    "async_config = '''\n",
    "name: \"async_processor\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"requests\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"async_config\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"results\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"status\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  max_queue_delay_microseconds: 500\n",
    "  default_queue_policy {\n",
    "    timeout_action: DELAY\n",
    "    default_timeout_microseconds: 1000\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(f\"{ASYNC_BACKEND_DIR}/config.pbtxt\", \"w\") as f:\n",
    "    f.write(async_config)\n",
    "\n",
    "print(\"âœ… ç•°æ­¥è™•ç†é…ç½®å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç•°æ­¥è™•ç†å¯¦ç¾\n",
    "async_backend_code = '''\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"\n",
    "    ç•°æ­¥è™•ç† Backend\n",
    "    æ”¯æŒä¸¦ç™¼ API èª¿ç”¨å’Œæ‰¹é‡è™•ç†\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        æ¨¡å‹åˆå§‹åŒ–\n",
    "        \"\"\"\n",
    "        self.model_config = json.loads(args['model_config'])\n",
    "        \n",
    "        # å‰µå»ºäº‹ä»¶å¾ªç’°ï¼ˆåœ¨æ–°ç·šç¨‹ä¸­ï¼‰\n",
    "        self.loop = None\n",
    "        self.loop_thread = None\n",
    "        self._start_event_loop()\n",
    "        \n",
    "        # HTTP å®¢æˆ¶ç«¯æœƒè©±\n",
    "        self.session = None\n",
    "        \n",
    "        # ç·šç¨‹æ± \n",
    "        self.executor = ThreadPoolExecutor(max_workers=8)\n",
    "        \n",
    "        # çµ±è¨ˆä¿¡æ¯\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"async_requests\": 0,\n",
    "            \"batch_requests\": 0,\n",
    "            \"avg_batch_size\": 0.0,\n",
    "            \"total_async_time\": 0.0\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸ”„ ç•°æ­¥è™•ç† Backend åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(\"âš¡ æ”¯æŒä¸¦ç™¼ API èª¿ç”¨å’Œæ‰¹é‡è™•ç†\")\n",
    "\n",
    "    def _start_event_loop(self):\n",
    "        \"\"\"\n",
    "        åœ¨æ–°ç·šç¨‹ä¸­å•Ÿå‹•äº‹ä»¶å¾ªç’°\n",
    "        \"\"\"\n",
    "        import threading\n",
    "        \n",
    "        def run_loop():\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(self.loop)\n",
    "            self.loop.run_forever()\n",
    "        \n",
    "        self.loop_thread = threading.Thread(target=run_loop, daemon=True)\n",
    "        self.loop_thread.start()\n",
    "        \n",
    "        # ç­‰å¾…äº‹ä»¶å¾ªç’°å•Ÿå‹•\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œç•°æ­¥è™•ç†\n",
    "        \"\"\"\n",
    "        batch_size = len(requests)\n",
    "        self.stats[\"total_requests\"] += batch_size\n",
    "        self.stats[\"batch_requests\"] += 1\n",
    "        self.stats[\"avg_batch_size\"] = self.stats[\"total_requests\"] / self.stats[\"batch_requests\"]\n",
    "        \n",
    "        # è§£ææ‰€æœ‰è«‹æ±‚\n",
    "        parsed_requests = []\n",
    "        for request in requests:\n",
    "            try:\n",
    "                request_data = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"requests\"\n",
    "                ).as_numpy().astype(str)[0]\n",
    "                \n",
    "                config_data = {}\n",
    "                config_tensor = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"async_config\"\n",
    "                )\n",
    "                if config_tensor is not None:\n",
    "                    try:\n",
    "                        config_data = json.loads(\n",
    "                            config_tensor.as_numpy().astype(str)[0]\n",
    "                        )\n",
    "                    except:\n",
    "                        config_data = {}\n",
    "                \n",
    "                parsed_requests.append({\n",
    "                    \"data\": json.loads(request_data),\n",
    "                    \"config\": config_data\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                parsed_requests.append({\n",
    "                    \"data\": {\"error\": str(e)},\n",
    "                    \"config\": {}\n",
    "                })\n",
    "        \n",
    "        # åŸ·è¡Œç•°æ­¥è™•ç†\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.loop and not self.loop.is_closed():\n",
    "            # ä½¿ç”¨ç•°æ­¥è™•ç†\n",
    "            future = asyncio.run_coroutine_threadsafe(\n",
    "                self._async_process_batch(parsed_requests), self.loop\n",
    "            )\n",
    "            results = future.result(timeout=30)  # 30ç§’è¶…æ™‚\n",
    "            self.stats[\"async_requests\"] += batch_size\n",
    "        else:\n",
    "            # å›é€€åˆ°åŒæ­¥è™•ç†\n",
    "            results = self._sync_process_batch(parsed_requests)\n",
    "        \n",
    "        async_time = time.time() - start_time\n",
    "        self.stats[\"total_async_time\"] += async_time\n",
    "        \n",
    "        # å‰µå»ºéŸ¿æ‡‰\n",
    "        responses = []\n",
    "        for result in results:\n",
    "            result_tensor = pb_utils.Tensor(\n",
    "                \"results\",\n",
    "                np.array([json.dumps(result[\"result\"])], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            status_tensor = pb_utils.Tensor(\n",
    "                \"status\",\n",
    "                np.array([result[\"status\"]], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[result_tensor, status_tensor]\n",
    "            )\n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses\n",
    "\n",
    "    async def _async_process_batch(self, requests: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        ç•°æ­¥æ‰¹é‡è™•ç†\n",
    "        \"\"\"\n",
    "        # å‰µå»º HTTP æœƒè©±\n",
    "        if self.session is None:\n",
    "            connector = aiohttp.TCPConnector(limit=100)\n",
    "            timeout = aiohttp.ClientTimeout(total=30)\n",
    "            self.session = aiohttp.ClientSession(\n",
    "                connector=connector, timeout=timeout\n",
    "            )\n",
    "        \n",
    "        # å‰µå»ºç•°æ­¥ä»»å‹™\n",
    "        tasks = []\n",
    "        for req in requests:\n",
    "            if req[\"data\"].get(\"type\") == \"api_call\":\n",
    "                task = self._async_api_call(req[\"data\"], req[\"config\"])\n",
    "            elif req[\"data\"].get(\"type\") == \"computation\":\n",
    "                task = self._async_computation(req[\"data\"], req[\"config\"])\n",
    "            else:\n",
    "                task = self._async_default_process(req[\"data\"], req[\"config\"])\n",
    "            \n",
    "            tasks.append(task)\n",
    "        \n",
    "        # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰ä»»å‹™\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # è™•ç†çµæœ\n",
    "        processed_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, Exception):\n",
    "                processed_results.append({\n",
    "                    \"result\": {\"error\": str(result)},\n",
    "                    \"status\": \"error\"\n",
    "                })\n",
    "            else:\n",
    "                processed_results.append({\n",
    "                    \"result\": result,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "    async def _async_api_call(self, data: dict, config: dict) -> dict:\n",
    "        \"\"\"\n",
    "        ç•°æ­¥ API èª¿ç”¨\n",
    "        \"\"\"\n",
    "        url = data.get(\"url\", \"https://httpbin.org/delay/1\")\n",
    "        method = data.get(\"method\", \"GET\").upper()\n",
    "        payload = data.get(\"payload\", {})\n",
    "        \n",
    "        try:\n",
    "            if method == \"GET\":\n",
    "                async with self.session.get(url, params=payload) as response:\n",
    "                    result = await response.json()\n",
    "            else:\n",
    "                async with self.session.post(url, json=payload) as response:\n",
    "                    result = await response.json()\n",
    "            \n",
    "            return {\n",
    "                \"api_result\": result,\n",
    "                \"status_code\": response.status,\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"url\": url\n",
    "            }\n",
    "\n",
    "    async def _async_computation(self, data: dict, config: dict) -> dict:\n",
    "        \"\"\"\n",
    "        ç•°æ­¥è¨ˆç®—ä»»å‹™\n",
    "        \"\"\"\n",
    "        operation = data.get(\"operation\", \"sum\")\n",
    "        numbers = data.get(\"numbers\", [1, 2, 3, 4, 5])\n",
    "        \n",
    "        # æ¨¡æ“¬è¨ˆç®—å»¶é²\n",
    "        await asyncio.sleep(0.1)\n",
    "        \n",
    "        if operation == \"sum\":\n",
    "            result = sum(numbers)\n",
    "        elif operation == \"product\":\n",
    "            result = 1\n",
    "            for n in numbers:\n",
    "                result *= n\n",
    "        elif operation == \"average\":\n",
    "            result = sum(numbers) / len(numbers) if numbers else 0\n",
    "        else:\n",
    "            result = len(numbers)\n",
    "        \n",
    "        return {\n",
    "            \"operation\": operation,\n",
    "            \"result\": result,\n",
    "            \"input_count\": len(numbers)\n",
    "        }\n",
    "\n",
    "    async def _async_default_process(self, data: dict, config: dict) -> dict:\n",
    "        \"\"\"\n",
    "        é»˜èªç•°æ­¥è™•ç†\n",
    "        \"\"\"\n",
    "        # æ¨¡æ“¬è™•ç†æ™‚é–“\n",
    "        delay = config.get(\"delay\", 0.05)\n",
    "        await asyncio.sleep(delay)\n",
    "        \n",
    "        return {\n",
    "            \"processed_data\": data,\n",
    "            \"processing_time\": delay,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "\n",
    "    def _sync_process_batch(self, requests: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        åŒæ­¥æ‰¹é‡è™•ç†ï¼ˆå›é€€æ–¹æ¡ˆï¼‰\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = []\n",
    "            for req in requests:\n",
    "                future = executor.submit(self._sync_process_single, req)\n",
    "                futures.append(future)\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result(timeout=10)\n",
    "                    results.append({\n",
    "                        \"result\": result,\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    results.append({\n",
    "                        \"result\": {\"error\": str(e)},\n",
    "                        \"status\": \"error\"\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _sync_process_single(self, request: dict) -> dict:\n",
    "        \"\"\"\n",
    "        åŒæ­¥è™•ç†å–®å€‹è«‹æ±‚\n",
    "        \"\"\"\n",
    "        data = request[\"data\"]\n",
    "        config = request[\"config\"]\n",
    "        \n",
    "        # æ¨¡æ“¬è™•ç†\n",
    "        time.sleep(config.get(\"delay\", 0.1))\n",
    "        \n",
    "        return {\n",
    "            \"sync_processed\": data,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        æ¸…ç†è³‡æº\n",
    "        \"\"\"\n",
    "        # é—œé–‰ HTTP æœƒè©±\n",
    "        if self.session and not self.session.closed:\n",
    "            asyncio.run_coroutine_threadsafe(\n",
    "                self.session.close(), self.loop\n",
    "            ).result(timeout=5)\n",
    "        \n",
    "        # åœæ­¢äº‹ä»¶å¾ªç’°\n",
    "        if self.loop and not self.loop.is_closed():\n",
    "            self.loop.call_soon_threadsafe(self.loop.stop)\n",
    "        \n",
    "        # é—œé–‰ç·šç¨‹æ± \n",
    "        self.executor.shutdown(wait=True)\n",
    "        \n",
    "        print(f\"ğŸ ç•°æ­¥è™•ç† Backend çµæŸ\")\n",
    "        print(f\"ğŸ“Š è™•ç†çµ±è¨ˆ:\")\n",
    "        print(f\"   ç¸½è«‹æ±‚: {self.stats['total_requests']}\")\n",
    "        print(f\"   ç•°æ­¥è«‹æ±‚: {self.stats['async_requests']}\")\n",
    "        print(f\"   æ‰¹æ¬¡æ•¸é‡: {self.stats['batch_requests']}\")\n",
    "        print(f\"   å¹³å‡æ‰¹æ¬¡å¤§å°: {self.stats['avg_batch_size']:.1f}\")\n",
    "        if self.stats['async_requests'] > 0:\n",
    "            avg_async_time = self.stats['total_async_time'] / self.stats['batch_requests']\n",
    "            print(f\"   å¹³å‡ç•°æ­¥è™•ç†æ™‚é–“: {avg_async_time:.3f}s\")\n",
    "'''\n",
    "\n",
    "with open(f\"{ASYNC_BACKEND_DIR}/1/model.py\", \"w\") as f:\n",
    "    f.write(async_backend_code)\n",
    "\n",
    "print(\"âœ… ç•°æ­¥è™•ç†ä»£ç¢¼å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## ğŸ§ª æ¸¬è©¦å’Œé©—è­‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦å®¢æˆ¶ç«¯ä»£ç¢¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Backend æ¸¬è©¦å®¢æˆ¶ç«¯\n",
    "class PythonBackendTester:\n",
    "    def __init__(self, server_url=\"localhost:8000\"):\n",
    "        self.server_url = server_url\n",
    "        self.client = httpclient.InferenceServerClient(\n",
    "            url=server_url, verbose=False\n",
    "        )\n",
    "    \n",
    "    def test_basic_backend(self):\n",
    "        \"\"\"æ¸¬è©¦åŸºç¤ Python Backend\"\"\"\n",
    "        print(\"ğŸ§ª æ¸¬è©¦åŸºç¤ Python Backend...\")\n",
    "        \n",
    "        # æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"text\": \"Hello World\",\n",
    "                \"params\": {\"type\": \"uppercase\"}\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Python Programming\",\n",
    "                \"params\": {\"type\": \"reverse\"}\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Machine Learning is Amazing\",\n",
    "                \"params\": {\"type\": \"word_count\"}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, case in enumerate(test_cases):\n",
    "            try:\n",
    "                # å‰µå»ºè¼¸å…¥\n",
    "                text_input = httpclient.InferInput(\n",
    "                    \"text_input\", [1], \"BYTES\"\n",
    "                )\n",
    "                text_input.set_data_from_numpy(\n",
    "                    np.array([case[\"text\"]], dtype=np.object_)\n",
    "                )\n",
    "                \n",
    "                params_input = httpclient.InferInput(\n",
    "                    \"parameters\", [1], \"BYTES\"\n",
    "                )\n",
    "                params_input.set_data_from_numpy(\n",
    "                    np.array([json.dumps(case[\"params\"])], dtype=np.object_)\n",
    "                )\n",
    "                \n",
    "                # å‰µå»ºè¼¸å‡º\n",
    "                outputs = [\n",
    "                    httpclient.InferRequestedOutput(\"processed_text\"),\n",
    "                    httpclient.InferRequestedOutput(\"metadata\")\n",
    "                ]\n",
    "                \n",
    "                # ç™¼é€è«‹æ±‚\n",
    "                response = self.client.infer(\n",
    "                    \"custom_pipeline\",\n",
    "                    inputs=[text_input, params_input],\n",
    "                    outputs=outputs\n",
    "                )\n",
    "                \n",
    "                # è§£æçµæœ\n",
    "                processed_text = response.as_numpy(\"processed_text\")[0].decode()\n",
    "                metadata = json.loads(response.as_numpy(\"metadata\")[0].decode())\n",
    "                \n",
    "                print(f\"ğŸ“ æ¸¬è©¦æ¡ˆä¾‹ {i+1}:\")\n",
    "                print(f\"   è¼¸å…¥: '{case['text']}'\")\n",
    "                print(f\"   è™•ç†é¡å‹: {case['params']['type']}\")\n",
    "                print(f\"   çµæœ: '{processed_text}'\")\n",
    "                print(f\"   è™•ç†æ™‚é–“: {metadata['processing_time_ms']:.2f}ms\")\n",
    "                print(\"\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ æ¸¬è©¦æ¡ˆä¾‹ {i+1} å¤±æ•—: {str(e)}\")\n",
    "    \n",
    "    def test_advanced_pipeline(self):\n",
    "        \"\"\"æ¸¬è©¦é«˜ç´šç®¡é“\"\"\"\n",
    "        print(\"ğŸ§ª æ¸¬è©¦é«˜ç´šç®¡é“...\")\n",
    "        \n",
    "        try:\n",
    "            # æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "            query = \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\"\n",
    "            config = {\n",
    "                \"top_k\": 3,\n",
    "                \"format\": \"markdown\",\n",
    "                \"max_length\": 200\n",
    "            }\n",
    "            \n",
    "            # å‰µå»ºè¼¸å…¥\n",
    "            query_input = httpclient.InferInput(\"query\", [1], \"BYTES\")\n",
    "            query_input.set_data_from_numpy(\n",
    "                np.array([query], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            config_input = httpclient.InferInput(\"config\", [1], \"BYTES\")\n",
    "            config_input.set_data_from_numpy(\n",
    "                np.array([json.dumps(config)], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            # å‰µå»ºè¼¸å‡º\n",
    "            outputs = [\n",
    "                httpclient.InferRequestedOutput(\"answer\"),\n",
    "                httpclient.InferRequestedOutput(\"confidence\"),\n",
    "                httpclient.InferRequestedOutput(\"sources\"),\n",
    "                httpclient.InferRequestedOutput(\"pipeline_info\")\n",
    "            ]\n",
    "            \n",
    "            # ç™¼é€è«‹æ±‚\n",
    "            start_time = time.time()\n",
    "            response = self.client.infer(\n",
    "                \"advanced_pipeline\",\n",
    "                inputs=[query_input, config_input],\n",
    "                outputs=outputs\n",
    "            )\n",
    "            request_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # è§£æçµæœ\n",
    "            answer = response.as_numpy(\"answer\")[0].decode()\n",
    "            confidence = response.as_numpy(\"confidence\")[0]\n",
    "            sources = [s.decode() for s in response.as_numpy(\"sources\")]\n",
    "            pipeline_info = json.loads(\n",
    "                response.as_numpy(\"pipeline_info\")[0].decode()\n",
    "            )\n",
    "            \n",
    "            print(f\"â“ æŸ¥è©¢: '{query}'\")\n",
    "            print(f\"ğŸ’¡ ç­”æ¡ˆ: {answer}\")\n",
    "            print(f\"ğŸ¯ ç½®ä¿¡åº¦: {confidence:.2f}\")\n",
    "            print(f\"ğŸ“š ä¾†æº: {', '.join(sources)}\")\n",
    "            print(f\"â±ï¸  ç¸½è«‹æ±‚æ™‚é–“: {request_time:.2f}ms\")\n",
    "            print(f\"ğŸ”§ ç®¡é“ç¸½æ™‚é–“: {pipeline_info['total_time']:.2f}ms\")\n",
    "            print(\"\")\n",
    "            print(\"ğŸ“Š ç®¡é“æ­¥é©Ÿè©³æƒ…:\")\n",
    "            for step in pipeline_info['steps']:\n",
    "                print(f\"   {step['name']}: {step['time_ms']:.2f}ms\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ é«˜ç´šç®¡é“æ¸¬è©¦å¤±æ•—: {str(e)}\")\n",
    "    \n",
    "    def test_async_processor(self):\n",
    "        \"\"\"æ¸¬è©¦ç•°æ­¥è™•ç†å™¨\"\"\"\n",
    "        print(\"ğŸ§ª æ¸¬è©¦ç•°æ­¥è™•ç†å™¨...\")\n",
    "        \n",
    "        # æ¸¬è©¦ä¸åŒé¡å‹çš„ç•°æ­¥ä»»å‹™\n",
    "        test_requests = [\n",
    "            {\n",
    "                \"type\": \"api_call\",\n",
    "                \"url\": \"https://httpbin.org/delay/0.5\",\n",
    "                \"method\": \"GET\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"computation\",\n",
    "                \"operation\": \"sum\",\n",
    "                \"numbers\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"computation\",\n",
    "                \"operation\": \"product\",\n",
    "                \"numbers\": [2, 3, 4]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, req_data in enumerate(test_requests):\n",
    "            try:\n",
    "                # å‰µå»ºè¼¸å…¥\n",
    "                request_input = httpclient.InferInput(\n",
    "                    \"requests\", [1], \"BYTES\"\n",
    "                )\n",
    "                request_input.set_data_from_numpy(\n",
    "                    np.array([json.dumps(req_data)], dtype=np.object_)\n",
    "                )\n",
    "                \n",
    "                config_input = httpclient.InferInput(\n",
    "                    \"async_config\", [1], \"BYTES\"\n",
    "                )\n",
    "                config_input.set_data_from_numpy(\n",
    "                    np.array([json.dumps({\"delay\": 0.1})], dtype=np.object_)\n",
    "                )\n",
    "                \n",
    "                # å‰µå»ºè¼¸å‡º\n",
    "                outputs = [\n",
    "                    httpclient.InferRequestedOutput(\"results\"),\n",
    "                    httpclient.InferRequestedOutput(\"status\")\n",
    "                ]\n",
    "                \n",
    "                # ç™¼é€è«‹æ±‚\n",
    "                start_time = time.time()\n",
    "                response = self.client.infer(\n",
    "                    \"async_processor\",\n",
    "                    inputs=[request_input, config_input],\n",
    "                    outputs=outputs\n",
    "                )\n",
    "                request_time = (time.time() - start_time) * 1000\n",
    "                \n",
    "                # è§£æçµæœ\n",
    "                results = json.loads(response.as_numpy(\"results\")[0].decode())\n",
    "                status = response.as_numpy(\"status\")[0].decode()\n",
    "                \n",
    "                print(f\"ğŸ”„ ç•°æ­¥ä»»å‹™ {i+1} ({req_data['type']}):\")\n",
    "                print(f\"   ç‹€æ…‹: {status}\")\n",
    "                print(f\"   è«‹æ±‚æ™‚é–“: {request_time:.2f}ms\")\n",
    "                print(f\"   çµæœ: {results}\")\n",
    "                print(\"\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ç•°æ­¥ä»»å‹™ {i+1} å¤±æ•—: {str(e)}\")\n",
    "\n",
    "\n",
    "# å‰µå»ºæ¸¬è©¦å™¨å¯¦ä¾‹ï¼ˆæ³¨æ„ï¼šéœ€è¦ Triton æœå‹™å™¨é‹è¡Œï¼‰\n",
    "print(\"ğŸ§ª Python Backend æ¸¬è©¦å®¢æˆ¶ç«¯å·²æº–å‚™\")\n",
    "print(\"ğŸ’¡ ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"   tester = PythonBackendTester()\")\n",
    "print(\"   tester.test_basic_backend()\")\n",
    "print(\"   tester.test_advanced_pipeline()\")\n",
    "print(\"   tester.test_async_processor()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ€§èƒ½ç›£æ§å’Œèª¿è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Backend ç›£æ§å’Œèª¿è©¦å·¥å…·\n",
    "class PythonBackendMonitor:\n",
    "    def __init__(self, server_url=\"localhost:8000\"):\n",
    "        self.server_url = server_url\n",
    "        self.client = httpclient.InferenceServerClient(url=server_url)\n",
    "    \n",
    "    def get_model_status(self, model_name):\n",
    "        \"\"\"ç²å–æ¨¡å‹ç‹€æ…‹\"\"\"\n",
    "        try:\n",
    "            model_config = self.client.get_model_config(model_name)\n",
    "            model_stats = self.client.get_inference_statistics(model_name)\n",
    "            \n",
    "            print(f\"ğŸ“‹ æ¨¡å‹: {model_name}\")\n",
    "            print(f\"ğŸ·ï¸  Backend: {model_config['backend']}\")\n",
    "            print(f\"ğŸ“ˆ æœ€å¤§æ‰¹æ¬¡å¤§å°: {model_config['max_batch_size']}\")\n",
    "            print(f\"ğŸ”¢ å¯¦ä¾‹æ•¸é‡: {len(model_config['instance_group'])}\")\n",
    "            \n",
    "            if 'model_stats' in model_stats:\n",
    "                stats = model_stats['model_stats'][0]\n",
    "                print(f\"ğŸ“Š æ¨ç†çµ±è¨ˆ:\")\n",
    "                print(f\"   æˆåŠŸè«‹æ±‚: {stats['inference_count']}\")\n",
    "                print(f\"   åŸ·è¡Œæ™‚é–“: {stats['inference_stats']['success']['count']}\")\n",
    "                if stats['inference_stats']['success']['count'] > 0:\n",
    "                    avg_time = stats['inference_stats']['success']['total_time_ns'] / stats['inference_stats']['success']['count'] / 1000000\n",
    "                    print(f\"   å¹³å‡å»¶é²: {avg_time:.2f}ms\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç„¡æ³•ç²å–æ¨¡å‹ç‹€æ…‹: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def benchmark_model(self, model_name, test_data, num_requests=10, concurrency=1):\n",
    "        \"\"\"æ¨¡å‹æ€§èƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "        print(f\"ğŸš€ é–‹å§‹åŸºæº–æ¸¬è©¦: {model_name}\")\n",
    "        print(f\"ğŸ“‹ æ¸¬è©¦é…ç½®: {num_requests} è«‹æ±‚, ä¸¦ç™¼åº¦ {concurrency}\")\n",
    "        \n",
    "        results = {\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"response_times\": [],\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ç°¡å–®çš„ä¸²è¡Œæ¸¬è©¦ï¼ˆå¯ä»¥æ“´å±•ç‚ºä¸¦ç™¼ï¼‰\n",
    "        for i in range(num_requests):\n",
    "            try:\n",
    "                request_start = time.time()\n",
    "                \n",
    "                # æ ¹æ“šæ¨¡å‹é¡å‹æº–å‚™è¼¸å…¥\n",
    "                if model_name == \"custom_pipeline\":\n",
    "                    inputs = self._prepare_basic_inputs(test_data)\n",
    "                    outputs = [\"processed_text\", \"metadata\"]\n",
    "                elif model_name == \"advanced_pipeline\":\n",
    "                    inputs = self._prepare_advanced_inputs(test_data)\n",
    "                    outputs = [\"answer\", \"confidence\", \"sources\", \"pipeline_info\"]\n",
    "                elif model_name == \"async_processor\":\n",
    "                    inputs = self._prepare_async_inputs(test_data)\n",
    "                    outputs = [\"results\", \"status\"]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # å‰µå»ºè¼¸å‡ºå°è±¡\n",
    "                output_objects = [httpclient.InferRequestedOutput(name) for name in outputs]\n",
    "                \n",
    "                # ç™¼é€è«‹æ±‚\n",
    "                response = self.client.infer(\n",
    "                    model_name, inputs=inputs, outputs=output_objects\n",
    "                )\n",
    "                \n",
    "                request_time = (time.time() - request_start) * 1000\n",
    "                results[\"response_times\"].append(request_time)\n",
    "                results[\"successful_requests\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è«‹æ±‚ {i+1} å¤±æ•—: {str(e)}\")\n",
    "                results[\"failed_requests\"] += 1\n",
    "        \n",
    "        results[\"total_time\"] = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯\n",
    "        if results[\"response_times\"]:\n",
    "            avg_latency = np.mean(results[\"response_times\"])\n",
    "            p95_latency = np.percentile(results[\"response_times\"], 95)\n",
    "            p99_latency = np.percentile(results[\"response_times\"], 99)\n",
    "            throughput = results[\"successful_requests\"] / (results[\"total_time\"] / 1000)\n",
    "            \n",
    "            print(\"\\nğŸ“Š åŸºæº–æ¸¬è©¦çµæœ:\")\n",
    "            print(f\"   æˆåŠŸè«‹æ±‚: {results['successful_requests']}/{num_requests}\")\n",
    "            print(f\"   æˆåŠŸç‡: {results['successful_requests']/num_requests*100:.1f}%\")\n",
    "            print(f\"   å¹³å‡å»¶é²: {avg_latency:.2f}ms\")\n",
    "            print(f\"   P95 å»¶é²: {p95_latency:.2f}ms\")\n",
    "            print(f\"   P99 å»¶é²: {p99_latency:.2f}ms\")\n",
    "            print(f\"   ååé‡: {throughput:.2f} QPS\")\n",
    "            print(f\"   ç¸½æ¸¬è©¦æ™‚é–“: {results['total_time']:.2f}ms\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _prepare_basic_inputs(self, test_data):\n",
    "        \"\"\"æº–å‚™åŸºç¤æ¨¡å‹è¼¸å…¥\"\"\"\n",
    "        text_input = httpclient.InferInput(\"text_input\", [1], \"BYTES\")\n",
    "        text_input.set_data_from_numpy(\n",
    "            np.array([test_data.get(\"text\", \"Hello World\")], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        params_input = httpclient.InferInput(\"parameters\", [1], \"BYTES\")\n",
    "        params_input.set_data_from_numpy(\n",
    "            np.array([json.dumps(test_data.get(\"params\", {\"type\": \"uppercase\"}))], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        return [text_input, params_input]\n",
    "    \n",
    "    def _prepare_advanced_inputs(self, test_data):\n",
    "        \"\"\"æº–å‚™é«˜ç´šç®¡é“è¼¸å…¥\"\"\"\n",
    "        query_input = httpclient.InferInput(\"query\", [1], \"BYTES\")\n",
    "        query_input.set_data_from_numpy(\n",
    "            np.array([test_data.get(\"query\", \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\")], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        config_input = httpclient.InferInput(\"config\", [1], \"BYTES\")\n",
    "        config_input.set_data_from_numpy(\n",
    "            np.array([json.dumps(test_data.get(\"config\", {\"top_k\": 3}))], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        return [query_input, config_input]\n",
    "    \n",
    "    def _prepare_async_inputs(self, test_data):\n",
    "        \"\"\"æº–å‚™ç•°æ­¥è™•ç†è¼¸å…¥\"\"\"\n",
    "        request_input = httpclient.InferInput(\"requests\", [1], \"BYTES\")\n",
    "        request_input.set_data_from_numpy(\n",
    "            np.array([json.dumps(test_data.get(\"request\", {\"type\": \"computation\", \"operation\": \"sum\", \"numbers\": [1,2,3,4,5]}))], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        config_input = httpclient.InferInput(\"async_config\", [1], \"BYTES\")\n",
    "        config_input.set_data_from_numpy(\n",
    "            np.array([json.dumps(test_data.get(\"async_config\", {\"delay\": 0.05}))], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        return [request_input, config_input]\n",
    "\n",
    "\n",
    "print(\"ğŸ“Š Python Backend ç›£æ§å·¥å…·å·²æº–å‚™\")\n",
    "print(\"ğŸ’¡ ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"   monitor = PythonBackendMonitor()\")\n",
    "print(\"   monitor.get_model_status('custom_pipeline')\")\n",
    "print(\"   monitor.benchmark_model('custom_pipeline', {'text': 'Test'}, num_requests=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 4ï¼šéƒ¨ç½²å’Œé©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºéƒ¨ç½²è…³æœ¬\n",
    "deployment_script = '''\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"ğŸš€ éƒ¨ç½² Python Backend æ¨¡å‹...\"\n",
    "\n",
    "# è¨­ç½®ç’°å¢ƒè®Šé‡\n",
    "export MODEL_REPOSITORY=\"/opt/tritonserver/models\"\n",
    "export TRITON_LOG_LEVEL=\"INFO\"\n",
    "\n",
    "# æª¢æŸ¥æ¨¡å‹ç›®éŒ„\n",
    "echo \"ğŸ“‹ æª¢æŸ¥æ¨¡å‹ç›®éŒ„çµæ§‹...\"\n",
    "ls -la $MODEL_REPOSITORY/\n",
    "\n",
    "# æª¢æŸ¥ Python Backend æ¨¡å‹\n",
    "for model in \"custom_pipeline\" \"advanced_pipeline\" \"async_processor\"; do\n",
    "    if [ -d \"$MODEL_REPOSITORY/$model\" ]; then\n",
    "        echo \"âœ… ç™¼ç¾æ¨¡å‹: $model\"\n",
    "        ls -la \"$MODEL_REPOSITORY/$model/\"\n",
    "        \n",
    "        # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶\n",
    "        if [ -f \"$MODEL_REPOSITORY/$model/config.pbtxt\" ]; then\n",
    "            echo \"  ğŸ“„ é…ç½®æ–‡ä»¶å­˜åœ¨\"\n",
    "        else\n",
    "            echo \"  âŒ é…ç½®æ–‡ä»¶ç¼ºå¤±\"\n",
    "        fi\n",
    "        \n",
    "        if [ -f \"$MODEL_REPOSITORY/$model/1/model.py\" ]; then\n",
    "            echo \"  ğŸ Python æ¨¡å‹å­˜åœ¨\"\n",
    "        else\n",
    "            echo \"  âŒ Python æ¨¡å‹ç¼ºå¤±\"\n",
    "        fi\n",
    "        echo \"\"\n",
    "    else\n",
    "        echo \"âŒ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: $model\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "# å•Ÿå‹• Triton æœå‹™å™¨\n",
    "echo \"ğŸ–¥ï¸  å•Ÿå‹• Triton æœå‹™å™¨...\"\n",
    "tritonserver \\\n",
    "    --model-repository=$MODEL_REPOSITORY \\\n",
    "    --backend-config=python,shm-default-byte-size=134217728 \\\n",
    "    --log-verbose=1 \\\n",
    "    --allow-http=true \\\n",
    "    --allow-grpc=true \\\n",
    "    --allow-metrics=true\n",
    "'''\n",
    "\n",
    "with open(\"/tmp/deploy_python_backends.sh\", \"w\") as f:\n",
    "    f.write(deployment_script)\n",
    "\n",
    "# è¨­ç½®åŸ·è¡Œæ¬Šé™\n",
    "os.chmod(\"/tmp/deploy_python_backends.sh\", 0o755)\n",
    "\n",
    "print(\"âœ… éƒ¨ç½²è…³æœ¬å·²å‰µå»º: /tmp/deploy_python_backends.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºé©—è­‰è…³æœ¬\n",
    "validation_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def wait_for_server(url=\"http://localhost:8000/v2/health/ready\", timeout=60):\n",
    "    \"\"\"ç­‰å¾… Triton æœå‹™å™¨å°±ç·’\"\"\"\n",
    "    print(f\"â³ ç­‰å¾… Triton æœå‹™å™¨å°±ç·’...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                print(\"âœ… Triton æœå‹™å™¨å·²å°±ç·’\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(\"âŒ ç­‰å¾…æœå‹™å™¨è¶…æ™‚\")\n",
    "    return False\n",
    "\n",
    "def check_models():\n",
    "    \"\"\"æª¢æŸ¥æ¨¡å‹ç‹€æ…‹\"\"\"\n",
    "    models = [\"custom_pipeline\", \"advanced_pipeline\", \"async_processor\"]\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            response = requests.get(f\"http://localhost:8000/v2/models/{model}/ready\")\n",
    "            if response.status_code == 200:\n",
    "                print(f\"âœ… æ¨¡å‹å°±ç·’: {model}\")\n",
    "            else:\n",
    "                print(f\"âŒ æ¨¡å‹æœªå°±ç·’: {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æª¢æŸ¥æ¨¡å‹å¤±æ•— {model}: {str(e)}\")\n",
    "\n",
    "def test_basic_inference():\n",
    "    \"\"\"æ¸¬è©¦åŸºç¤æ¨ç†\"\"\"\n",
    "    print(\"ğŸ§ª æ¸¬è©¦åŸºç¤æ¨ç†...\")\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"text_input\",\n",
    "                \"shape\": [1],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": [\"Hello Python Backend\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"parameters\",\n",
    "                \"shape\": [1],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": ['{\"type\": \"uppercase\"}']\n",
    "            }\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "            {\"name\": \"processed_text\"},\n",
    "            {\"name\": \"metadata\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/v2/models/custom_pipeline/infer\",\n",
    "            json=payload\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            processed_text = result[\"outputs\"][0][\"data\"][0]\n",
    "            print(f\"âœ… åŸºç¤æ¨ç†æˆåŠŸ: '{processed_text}'\")\n",
    "        else:\n",
    "            print(f\"âŒ åŸºç¤æ¨ç†å¤±æ•—: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŸºç¤æ¨ç†ç•°å¸¸: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ” Python Backend é©—è­‰é–‹å§‹\")\n",
    "    \n",
    "    # ç­‰å¾…æœå‹™å™¨\n",
    "    if wait_for_server():\n",
    "        # æª¢æŸ¥æ¨¡å‹\n",
    "        check_models()\n",
    "        \n",
    "        # æ¸¬è©¦æ¨ç†\n",
    "        test_basic_inference()\n",
    "        \n",
    "        print(\"ğŸ é©—è­‰å®Œæˆ\")\n",
    "    else:\n",
    "        print(\"âŒ æœå‹™å™¨æœªå°±ç·’ï¼Œé©—è­‰çµ‚æ­¢\")\n",
    "'''\n",
    "\n",
    "with open(\"/tmp/validate_python_backends.py\", \"w\") as f:\n",
    "    f.write(validation_script)\n",
    "\n",
    "os.chmod(\"/tmp/validate_python_backends.py\", 0o755)\n",
    "\n",
    "print(\"âœ… é©—è­‰è…³æœ¬å·²å‰µå»º: /tmp/validate_python_backends.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## ğŸ“š æœ€ä½³å¯¦è¸å’Œæ•…éšœæ’é™¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### ğŸ¯ Python Backend é–‹ç™¼æœ€ä½³å¯¦è¸\n",
    "\n",
    "#### 1. ä»£ç¢¼çµæ§‹\n",
    "- **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šå°‡é‚è¼¯åˆ†è§£ç‚ºå¯é‡ç”¨çš„çµ„ä»¶\n",
    "- **éŒ¯èª¤è™•ç†**ï¼šå®Œå–„çš„ç•°å¸¸æ•ç²å’Œè™•ç†æ©Ÿåˆ¶\n",
    "- **æ—¥èªŒè¨˜éŒ„**ï¼šè©³ç´°çš„åŸ·è¡Œæ—¥èªŒä¾¿æ–¼èª¿è©¦\n",
    "- **é…ç½®ç®¡ç†**ï¼šéˆæ´»çš„åƒæ•¸é…ç½®ç³»çµ±\n",
    "\n",
    "#### 2. æ€§èƒ½å„ªåŒ–\n",
    "- **æ‰¹è™•ç†**ï¼šå……åˆ†åˆ©ç”¨å‹•æ…‹æ‰¹æ¬¡åŠŸèƒ½\n",
    "- **ç•°æ­¥è™•ç†**ï¼šä½¿ç”¨å”ç¨‹è™•ç† I/O å¯†é›†å‹ä»»å‹™\n",
    "- **è³‡æºæ± **ï¼šé‡ç”¨é€£æ¥å’Œè¨ˆç®—è³‡æº\n",
    "- **å…§å­˜ç®¡ç†**ï¼šåŠæ™‚é‡‹æ”¾ä¸éœ€è¦çš„å°è±¡\n",
    "\n",
    "#### 3. å®‰å…¨è€ƒæ…®\n",
    "- **è¼¸å…¥é©—è­‰**ï¼šåš´æ ¼é©—è­‰æ‰€æœ‰è¼¸å…¥æ•¸æ“š\n",
    "- **è³‡æºé™åˆ¶**ï¼šè¨­ç½®åˆç†çš„è¶…æ™‚å’Œè³‡æºé™åˆ¶\n",
    "- **éŒ¯èª¤ä¿¡æ¯**ï¼šé¿å…æš´éœ²æ•æ„Ÿä¿¡æ¯\n",
    "- **ä¾è³´ç®¡ç†**ï¼šä¿æŒä¾è³´åº«çš„å®‰å…¨æ›´æ–°\n",
    "\n",
    "#### 4. ç›£æ§å’Œèª¿è©¦\n",
    "- **æ€§èƒ½æŒ‡æ¨™**ï¼šæ”¶é›†é—œéµæ€§èƒ½æ•¸æ“š\n",
    "- **å¥åº·æª¢æŸ¥**ï¼šå¯¦ç¾æ¨¡å‹å¥åº·ç‹€æ…‹æª¢æŸ¥\n",
    "- **èª¿è©¦æ¨¡å¼**ï¼šæ”¯æŒé–‹ç™¼æ™‚çš„è©³ç´°èª¿è©¦\n",
    "- **ç‰ˆæœ¬ç®¡ç†**ï¼šæ¸…æ™°çš„æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•…éšœæ’é™¤æŒ‡å—\n",
    "troubleshooting_guide = \"\"\"\n",
    "ğŸ”§ Python Backend æ•…éšœæ’é™¤æŒ‡å—\n",
    "\n",
    "å¸¸è¦‹å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆï¼š\n",
    "\n",
    "1. æ¨¡å‹åŠ è¼‰å¤±æ•—\n",
    "   å•é¡Œï¼šæ¨¡å‹ç„¡æ³•åŠ è¼‰æˆ–åˆå§‹åŒ–å¤±æ•—\n",
    "   è§£æ±ºæ–¹æ¡ˆï¼š\n",
    "   - æª¢æŸ¥ model.py èªæ³•éŒ¯èª¤\n",
    "   - ç¢ºèªæ‰€æœ‰ä¾è³´åº«å·²å®‰è£\n",
    "   - æª¢æŸ¥ config.pbtxt é…ç½®æ­£ç¢ºæ€§\n",
    "   - æŸ¥çœ‹ Triton æœå‹™å™¨æ—¥èªŒ\n",
    "\n",
    "2. æ¨ç†è«‹æ±‚è¶…æ™‚\n",
    "   å•é¡Œï¼šæ¨ç†è«‹æ±‚éŸ¿æ‡‰æ™‚é–“éé•·\n",
    "   è§£æ±ºæ–¹æ¡ˆï¼š\n",
    "   - å„ªåŒ–æ¨¡å‹åŸ·è¡Œé‚è¼¯\n",
    "   - å¢åŠ å¯¦ä¾‹æ•¸é‡\n",
    "   - èª¿æ•´è¶…æ™‚é…ç½®\n",
    "   - ä½¿ç”¨ç•°æ­¥è™•ç†\n",
    "\n",
    "3. å…§å­˜ä½¿ç”¨éé«˜\n",
    "   å•é¡Œï¼šPython Backend æ¶ˆè€—å¤§é‡å…§å­˜\n",
    "   è§£æ±ºæ–¹æ¡ˆï¼š\n",
    "   - åŠæ™‚é‡‹æ”¾å¤§å°è±¡\n",
    "   - é¿å…å…§å­˜æ´©æ¼\n",
    "   - å„ªåŒ–æ•¸æ“šçµæ§‹\n",
    "   - é™åˆ¶æ‰¹æ¬¡å¤§å°\n",
    "\n",
    "4. ä¸¦ç™¼è™•ç†å•é¡Œ\n",
    "   å•é¡Œï¼šé«˜ä¸¦ç™¼æ™‚å‡ºç¾ç«¶æ…‹æ¢ä»¶\n",
    "   è§£æ±ºæ–¹æ¡ˆï¼š\n",
    "   - ä½¿ç”¨ç·šç¨‹å®‰å…¨çš„æ•¸æ“šçµæ§‹\n",
    "   - é¿å…å…¨å±€è®Šé‡ä¿®æ”¹\n",
    "   - åˆç†ä½¿ç”¨é–æ©Ÿåˆ¶\n",
    "   - è¨­è¨ˆç„¡ç‹€æ…‹è™•ç†é‚è¼¯\n",
    "\n",
    "5. ä¾è³´åº«è¡çª\n",
    "   å•é¡Œï¼šä¸åŒæ¨¡å‹é–“çš„ä¾è³´åº«ç‰ˆæœ¬è¡çª\n",
    "   è§£æ±ºæ–¹æ¡ˆï¼š\n",
    "   - ä½¿ç”¨ Python ç’°å¢ƒéš”é›¢\n",
    "   - å‰µå»ºå°ˆç”¨åŸ·è¡Œç’°å¢ƒ\n",
    "   - çµ±ä¸€ä¾è³´ç‰ˆæœ¬ç®¡ç†\n",
    "   - ä½¿ç”¨å®¹å™¨åŒ–éƒ¨ç½²\n",
    "\n",
    "èª¿è©¦æŠ€å·§ï¼š\n",
    "- ä½¿ç”¨ print() æˆ– logging è¼¸å‡ºèª¿è©¦ä¿¡æ¯\n",
    "- è¨­ç½®æ–·é»é€²è¡Œèª¿è©¦ï¼ˆé–‹ç™¼ç’°å¢ƒï¼‰\n",
    "- æª¢æŸ¥ Triton æœå‹™å™¨æ—¥èªŒ\n",
    "- ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·\n",
    "- ç›£æ§ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³\n",
    "\n",
    "æ—¥èªŒæŸ¥çœ‹å‘½ä»¤ï¼š\n",
    "- docker logs <triton_container_id>\n",
    "- tail -f /var/log/triton/triton.log\n",
    "- journalctl -u triton-server -f\n",
    "\"\"\"\n",
    "\n",
    "print(troubleshooting_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## ğŸ“– ç¸½çµ\n",
    "\n",
    "æœ¬å¯¦é©—å®Œæˆäº†è‡ªå®šç¾© Python Backend çš„å®Œæ•´é–‹ç™¼æµç¨‹ï¼š\n",
    "\n",
    "### ğŸ¯ å¯¦é©—æˆæœ\n",
    "1. **åŸºç¤ Python Backend** - å¯¦ç¾äº†æ–‡æœ¬è™•ç†ç®¡é“\n",
    "2. **é«˜ç´šç®¡é“ Backend** - æ§‹å»ºäº†å¤šæ­¥é©Ÿæ¨ç†æµç¨‹\n",
    "3. **ç•°æ­¥è™•ç† Backend** - é–‹ç™¼äº†ä¸¦ç™¼è™•ç†èƒ½åŠ›\n",
    "4. **ç›£æ§å’Œèª¿è©¦å·¥å…·** - æä¾›äº†å®Œæ•´çš„é‹ç¶­æ”¯æŒ\n",
    "\n",
    "### ğŸ”§ é—œéµæŠ€è¡“é»\n",
    "- Triton Python Backend API ä½¿ç”¨\n",
    "- ç•°æ­¥ç·¨ç¨‹å’Œä¸¦ç™¼è™•ç†\n",
    "- æ€§èƒ½ç›£æ§å’Œå„ªåŒ–\n",
    "- éŒ¯èª¤è™•ç†å’Œæ•…éšœæ’é™¤\n",
    "\n",
    "### ğŸš€ å¾ŒçºŒæ­¥é©Ÿ\n",
    "1. éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ\n",
    "2. é›†æˆæ›´è¤‡é›œçš„æ¥­å‹™é‚è¼¯\n",
    "3. å¯¦ç¾ A/B æ¸¬è©¦åŠŸèƒ½\n",
    "4. æ·»åŠ æ¨¡å‹ç†±æ›´æ–°èƒ½åŠ›\n",
    "\n",
    "### ğŸ’¡ å­¸ç¿’è¦é»\n",
    "- Python Backend æä¾›äº†æœ€å¤§çš„éˆæ´»æ€§\n",
    "- ç•°æ­¥è™•ç†èƒ½é¡¯è‘—æå‡æ€§èƒ½\n",
    "- ç›£æ§å’Œèª¿è©¦æ˜¯æˆåŠŸéƒ¨ç½²çš„é—œéµ\n",
    "- è‰¯å¥½çš„éŒ¯èª¤è™•ç†èƒ½æå‡ç³»çµ±ç©©å®šæ€§\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆ Lab 2.3.4ï¼**\n",
    "\n",
    "æ‚¨å·²ç¶“æŒæ¡äº† Triton Python Backend çš„é«˜ç´šé–‹ç™¼æŠ€è¡“ï¼Œå¯ä»¥æ§‹å»ºè¤‡é›œçš„æ¨ç†ç®¡é“å’Œè™•ç†é‚è¼¯ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}