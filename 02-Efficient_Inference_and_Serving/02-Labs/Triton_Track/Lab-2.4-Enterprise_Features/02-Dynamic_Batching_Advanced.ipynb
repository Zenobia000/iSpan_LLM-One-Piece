{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.2 - é«˜ç´šå‹•æ…‹æ‰¹æ¬¡è™•ç†èˆ‡æ™ºèƒ½èª¿åº¦\n",
    "\n",
    "## ğŸ¯ å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬å¯¦é©—å°‡æ•™æ‚¨å¦‚ä½•ï¼š\n",
    "1. å¯¦ç¾æ™ºèƒ½å‹•æ…‹æ‰¹æ¬¡èª¿åº¦ç®—æ³•\n",
    "2. è¨­è¨ˆå„ªå…ˆç´šè«‹æ±‚è™•ç†æ©Ÿåˆ¶\n",
    "3. å„ªåŒ–å»¶é²èˆ‡ååé‡çš„å¹³è¡¡\n",
    "4. å¯¦ç¾è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°èª¿æ•´\n",
    "5. æ§‹å»ºè«‹æ±‚æ’éšŠå’Œè² è¼‰å‡è¡¡ç­–ç•¥\n",
    "\n",
    "## ğŸ“‹ å‰ç½®éœ€æ±‚\n",
    "\n",
    "- å®Œæˆ Lab 2.1ï¼ˆTriton åŸºç¤è¨­ç½®ï¼‰\n",
    "- äº†è§£æ‰¹æ¬¡è™•ç†åŸºæœ¬æ¦‚å¿µ\n",
    "- ç†Ÿæ‚‰æ€§èƒ½ç›£æ§å’Œèª¿å„ª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç†è«–èƒŒæ™¯\n",
    "\n",
    "### å‹•æ…‹æ‰¹æ¬¡è™•ç†æŒ‘æˆ°\n",
    "\n",
    "**1. å»¶é² vs ååé‡æ¬Šè¡¡**\n",
    "- å¤§æ‰¹æ¬¡ï¼šé«˜ååé‡ï¼Œé«˜å»¶é²\n",
    "- å°æ‰¹æ¬¡ï¼šä½å»¶é²ï¼Œä½ååé‡\n",
    "- å‹•æ…‹èª¿æ•´ï¼šæ ¹æ“šè² è¼‰æ™ºèƒ½å¹³è¡¡\n",
    "\n",
    "**2. è«‹æ±‚å„ªå…ˆç´šç®¡ç†**\n",
    "- VIP ç”¨æˆ¶å„ªå…ˆè™•ç†\n",
    "- ç·Šæ€¥è«‹æ±‚å¿«é€Ÿé€šé“\n",
    "- æ‰¹æ¬¡ä»»å‹™ä½å„ªå…ˆç´š\n",
    "\n",
    "**3. è³‡æºåˆ©ç”¨ç‡æœ€ä½³åŒ–**\n",
    "- GPU è¨˜æ†¶é«”æœ‰æ•ˆåˆ©ç”¨\n",
    "- è¨ˆç®—è³‡æºå‹•æ…‹åˆ†é…\n",
    "- å¤šæ¨¡å‹ä¸¦è¡Œè™•ç†\n",
    "\n",
    "### æ™ºèƒ½èª¿åº¦æ¶æ§‹\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[è«‹æ±‚æ¥æ”¶] --> B[å„ªå…ˆç´šåˆ†é¡]\n",
    "    B --> C{è² è¼‰æª¢æ¸¬}\n",
    "    C -->|é«˜è² è¼‰| D[å¤§æ‰¹æ¬¡ç­–ç•¥]\n",
    "    C -->|ä½è² è¼‰| E[å°æ‰¹æ¬¡ç­–ç•¥]\n",
    "    C -->|ä¸­è² è¼‰| F[å‹•æ…‹èª¿æ•´]\n",
    "    \n",
    "    D --> G[æ‰¹æ¬¡èª¿åº¦å™¨]\n",
    "    E --> G\n",
    "    F --> G\n",
    "    \n",
    "    G --> H[GPU åŸ·è¡Œ]\n",
    "    H --> I[çµæœè¿”å›]\n",
    "    I --> J[æ€§èƒ½ç›£æ§]\n",
    "    J --> C\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import asyncio\n",
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import deque, defaultdict\n",
    "import heapq\n",
    "\n",
    "# æ€§èƒ½ç›£æ§\n",
    "import psutil\n",
    "import threading\n",
    "from threading import Lock, Event\n",
    "\n",
    "# Triton å®¢æˆ¶ç«¯\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# è¨­ç½®æ¨£å¼\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"ğŸš€ Dynamic Batching Lab initialized at {datetime.now()}\")\n",
    "print(f\"ğŸ“Š Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­ç½®å¯¦é©—ç’°å¢ƒ\n",
    "BASE_DIR = \"/opt/tritonserver\"\n",
    "MODEL_REPO = f\"{BASE_DIR}/models\"\n",
    "EXPERIMENT_DIR = f\"{BASE_DIR}/experiments/dynamic_batching\"\n",
    "\n",
    "# å‰µå»ºå¯¦é©—ç›®éŒ„\n",
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/metrics\", exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/configs\", exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/logs\", exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ å¯¦é©—ç›®éŒ„: {EXPERIMENT_DIR}\")\n",
    "print(f\"ğŸ“ æ¨¡å‹å€‰åº«: {MODEL_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 1ï¼šæ™ºèƒ½æ‰¹æ¬¡èª¿åº¦å™¨è¨­è¨ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 è«‹æ±‚å’Œæ‰¹æ¬¡æ•¸æ“šçµæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceRequest:\n",
    "    \"\"\"æ¨ç†è«‹æ±‚æ•¸æ“šçµæ§‹\"\"\"\n",
    "    request_id: str\n",
    "    user_id: str\n",
    "    priority: int  # 1=æœ€é«˜, 5=æœ€ä½\n",
    "    data: Any\n",
    "    created_at: datetime\n",
    "    timeout: float  # ç§’\n",
    "    callback: Optional[callable] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        \"\"\"å„ªå…ˆç´šæ¯”è¼ƒï¼ˆç”¨æ–¼å„ªå…ˆä½‡åˆ—ï¼‰\"\"\"\n",
    "        if self.priority != other.priority:\n",
    "            return self.priority < other.priority\n",
    "        return self.created_at < other.created_at\n",
    "    \n",
    "    @property\n",
    "    def age(self) -> float:\n",
    "        \"\"\"è«‹æ±‚å¹´é½¡ï¼ˆç§’ï¼‰\"\"\"\n",
    "        return (datetime.now() - self.created_at).total_seconds()\n",
    "    \n",
    "    @property\n",
    "    def is_expired(self) -> bool:\n",
    "        \"\"\"æ˜¯å¦å·²éæœŸ\"\"\"\n",
    "        return self.age > self.timeout\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BatchConfig:\n",
    "    \"\"\"æ‰¹æ¬¡é…ç½®\"\"\"\n",
    "    min_batch_size: int = 1\n",
    "    max_batch_size: int = 32\n",
    "    max_wait_time: float = 0.1  # ç§’\n",
    "    target_latency: float = 0.05  # ç§’\n",
    "    priority_boost: Dict[int, float] = field(default_factory=lambda: {\n",
    "        1: 0.8,  # VIP ç”¨æˆ¶é™ä½ 80% ç­‰å¾…æ™‚é–“\n",
    "        2: 0.6,  # é«˜å„ªå…ˆç´šé™ä½ 60%\n",
    "        3: 1.0,  # æ™®é€šå„ªå…ˆç´š\n",
    "        4: 1.2,  # ä½å„ªå…ˆç´šå¢åŠ  20%\n",
    "        5: 1.5   # æœ€ä½å„ªå…ˆç´šå¢åŠ  50%\n",
    "    })\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BatchMetrics:\n",
    "    \"\"\"æ‰¹æ¬¡æ€§èƒ½æŒ‡æ¨™\"\"\"\n",
    "    batch_id: str\n",
    "    batch_size: int\n",
    "    processing_time: float\n",
    "    wait_time: float\n",
    "    total_latency: float\n",
    "    throughput: float\n",
    "    priority_distribution: Dict[int, int]\n",
    "    timestamp: datetime\n",
    "    gpu_utilization: float = 0.0\n",
    "    memory_usage: float = 0.0\n",
    "\n",
    "\n",
    "print(\"âœ… æ•¸æ“šçµæ§‹å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ™ºèƒ½æ‰¹æ¬¡èª¿åº¦å™¨å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartBatchScheduler:\n",
    "    \"\"\"æ™ºèƒ½æ‰¹æ¬¡èª¿åº¦å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BatchConfig, model_name: str = \"text_classifier\"):\n",
    "        self.config = config\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # è«‹æ±‚ä½‡åˆ—ï¼ˆæŒ‰å„ªå…ˆç´šæ’åºï¼‰\n",
    "        self.priority_queue = []\n",
    "        self.queue_lock = Lock()\n",
    "        \n",
    "        # æ€§èƒ½çµ±è¨ˆ\n",
    "        self.metrics_history: List[BatchMetrics] = []\n",
    "        self.metrics_lock = Lock()\n",
    "        \n",
    "        # èª¿åº¦ç‹€æ…‹\n",
    "        self.running = False\n",
    "        self.scheduler_thread = None\n",
    "        self.stop_event = Event()\n",
    "        \n",
    "        # è‡ªé©æ‡‰åƒæ•¸\n",
    "        self.adaptive_config = {\n",
    "            \"current_batch_size\": config.min_batch_size,\n",
    "            \"avg_latency\": 0.0,\n",
    "            \"avg_throughput\": 0.0,\n",
    "            \"load_factor\": 0.0\n",
    "        }\n",
    "        \n",
    "        # çµ±è¨ˆè¨ˆæ•¸å™¨\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"processed_requests\": 0,\n",
    "            \"expired_requests\": 0,\n",
    "            \"total_batches\": 0,\n",
    "            \"avg_batch_size\": 0.0\n",
    "        }\n",
    "    \n",
    "    def add_request(self, request: InferenceRequest) -> bool:\n",
    "        \"\"\"æ·»åŠ æ¨ç†è«‹æ±‚\"\"\"\n",
    "        if request.is_expired:\n",
    "            self.stats[\"expired_requests\"] += 1\n",
    "            return False\n",
    "        \n",
    "        with self.queue_lock:\n",
    "            heapq.heappush(self.priority_queue, request)\n",
    "            self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_queue_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–ä½‡åˆ—ç‹€æ…‹\"\"\"\n",
    "        with self.queue_lock:\n",
    "            queue_size = len(self.priority_queue)\n",
    "            priority_counts = defaultdict(int)\n",
    "            \n",
    "            for req in self.priority_queue:\n",
    "                priority_counts[req.priority] += 1\n",
    "            \n",
    "            return {\n",
    "                \"queue_size\": queue_size,\n",
    "                \"priority_distribution\": dict(priority_counts),\n",
    "                \"oldest_request_age\": self.priority_queue[0].age if queue_size > 0 else 0\n",
    "            }\n",
    "    \n",
    "    def _calculate_optimal_batch_size(self) -> int:\n",
    "        \"\"\"è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°\"\"\"\n",
    "        with self.queue_lock:\n",
    "            queue_size = len(self.priority_queue)\n",
    "        \n",
    "        if queue_size == 0:\n",
    "            return self.config.min_batch_size\n",
    "        \n",
    "        # åŸºæ–¼æ­·å²æ€§èƒ½æ•¸æ“šçš„è‡ªé©æ‡‰èª¿æ•´\n",
    "        recent_metrics = self.metrics_history[-10:] if self.metrics_history else []\n",
    "        \n",
    "        if recent_metrics:\n",
    "            avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "            avg_throughput = np.mean([m.throughput for m in recent_metrics])\n",
    "            \n",
    "            # å¦‚æœå»¶é²éé«˜ï¼Œæ¸›å°‘æ‰¹æ¬¡å¤§å°\n",
    "            if avg_latency > self.config.target_latency * 1.5:\n",
    "                target_size = max(self.config.min_batch_size, \n",
    "                                self.adaptive_config[\"current_batch_size\"] - 2)\n",
    "            # å¦‚æœå»¶é²åˆç†ä¸”ä½‡åˆ—è¼ƒé•·ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°\n",
    "            elif avg_latency <= self.config.target_latency and queue_size > 10:\n",
    "                target_size = min(self.config.max_batch_size,\n",
    "                                self.adaptive_config[\"current_batch_size\"] + 1)\n",
    "            else:\n",
    "                target_size = self.adaptive_config[\"current_batch_size\"]\n",
    "        else:\n",
    "            # åˆå§‹éšæ®µåŸºæ–¼ä½‡åˆ—é•·åº¦\n",
    "            if queue_size >= self.config.max_batch_size:\n",
    "                target_size = self.config.max_batch_size\n",
    "            elif queue_size >= self.config.min_batch_size:\n",
    "                target_size = min(queue_size, self.config.max_batch_size)\n",
    "            else:\n",
    "                target_size = self.config.min_batch_size\n",
    "        \n",
    "        self.adaptive_config[\"current_batch_size\"] = target_size\n",
    "        return target_size\n",
    "    \n",
    "    def _calculate_wait_time(self, priority: int) -> float:\n",
    "        \"\"\"è¨ˆç®—ç­‰å¾…æ™‚é–“ï¼ˆåŸºæ–¼å„ªå…ˆç´šï¼‰\"\"\"\n",
    "        base_wait = self.config.max_wait_time\n",
    "        priority_factor = self.config.priority_boost.get(priority, 1.0)\n",
    "        return base_wait * priority_factor\n",
    "    \n",
    "    def _form_batch(self) -> List[InferenceRequest]:\n",
    "        \"\"\"çµ„æˆæ‰¹æ¬¡\"\"\"\n",
    "        batch = []\n",
    "        target_size = self._calculate_optimal_batch_size()\n",
    "        \n",
    "        with self.queue_lock:\n",
    "            # ç§»é™¤éæœŸè«‹æ±‚\n",
    "            expired_count = 0\n",
    "            while self.priority_queue and self.priority_queue[0].is_expired:\n",
    "                heapq.heappop(self.priority_queue)\n",
    "                expired_count += 1\n",
    "            \n",
    "            self.stats[\"expired_requests\"] += expired_count\n",
    "            \n",
    "            # çµ„æˆæ‰¹æ¬¡\n",
    "            while len(batch) < target_size and self.priority_queue:\n",
    "                request = heapq.heappop(self.priority_queue)\n",
    "                if not request.is_expired:\n",
    "                    batch.append(request)\n",
    "                else:\n",
    "                    self.stats[\"expired_requests\"] += 1\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def _should_process_batch(self, batch: List[InferenceRequest]) -> bool:\n",
    "        \"\"\"åˆ¤æ–·æ˜¯å¦æ‡‰è©²è™•ç†æ‰¹æ¬¡\"\"\"\n",
    "        if not batch:\n",
    "            return False\n",
    "        \n",
    "        # å¦‚æœé”åˆ°æœ€å°æ‰¹æ¬¡å¤§å°\n",
    "        if len(batch) >= self.config.min_batch_size:\n",
    "            return True\n",
    "        \n",
    "        # å¦‚æœæœ‰é«˜å„ªå…ˆç´šè«‹æ±‚ç­‰å¾…æ™‚é–“éé•·\n",
    "        for request in batch:\n",
    "            wait_threshold = self._calculate_wait_time(request.priority)\n",
    "            if request.age >= wait_threshold:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _process_batch(self, batch: List[InferenceRequest]) -> BatchMetrics:\n",
    "        \"\"\"è™•ç†æ‰¹æ¬¡ï¼ˆæ¨¡æ“¬ï¼‰\"\"\"\n",
    "        if not batch:\n",
    "            return None\n",
    "        \n",
    "        batch_id = f\"batch_{int(time.time() * 1000)}\"\n",
    "        batch_size = len(batch)\n",
    "        \n",
    "        # è¨˜éŒ„é–‹å§‹æ™‚é–“\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # è¨ˆç®—ç­‰å¾…æ™‚é–“ï¼ˆæœ€è€è«‹æ±‚çš„ç­‰å¾…æ™‚é–“ï¼‰\n",
    "        wait_time = max(req.age for req in batch)\n",
    "        \n",
    "        # æ¨¡æ“¬è™•ç†æ™‚é–“ï¼ˆåŸºæ–¼æ‰¹æ¬¡å¤§å°å’Œè¤‡é›œåº¦ï¼‰\n",
    "        base_processing_time = 0.02  # 20ms åŸºç¤è™•ç†æ™‚é–“\n",
    "        batch_overhead = batch_size * 0.001  # æ¯å€‹æ¨£æœ¬å¢åŠ  1ms\n",
    "        processing_time = base_processing_time + batch_overhead + random.uniform(0, 0.01)\n",
    "        \n",
    "        # æ¨¡æ“¬å¯¦éš›è™•ç†\n",
    "        time.sleep(processing_time)\n",
    "        \n",
    "        # è¨ˆç®—æŒ‡æ¨™\n",
    "        total_latency = time.time() - start_time + wait_time\n",
    "        throughput = batch_size / processing_time\n",
    "        \n",
    "        # å„ªå…ˆç´šåˆ†å¸ƒ\n",
    "        priority_dist = defaultdict(int)\n",
    "        for req in batch:\n",
    "            priority_dist[req.priority] += 1\n",
    "        \n",
    "        # å‰µå»ºæ‰¹æ¬¡æŒ‡æ¨™\n",
    "        metrics = BatchMetrics(\n",
    "            batch_id=batch_id,\n",
    "            batch_size=batch_size,\n",
    "            processing_time=processing_time,\n",
    "            wait_time=wait_time,\n",
    "            total_latency=total_latency,\n",
    "            throughput=throughput,\n",
    "            priority_distribution=dict(priority_dist),\n",
    "            timestamp=datetime.now(),\n",
    "            gpu_utilization=random.uniform(0.7, 0.95),  # æ¨¡æ“¬ GPU ä½¿ç”¨ç‡\n",
    "            memory_usage=random.uniform(0.4, 0.8)        # æ¨¡æ“¬è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "        )\n",
    "        \n",
    "        # æ›´æ–°çµ±è¨ˆ\n",
    "        with self.metrics_lock:\n",
    "            self.metrics_history.append(metrics)\n",
    "            self.stats[\"processed_requests\"] += batch_size\n",
    "            self.stats[\"total_batches\"] += 1\n",
    "            \n",
    "            # ä¿ç•™æœ€è¿‘ 1000 æ¢è¨˜éŒ„\n",
    "            if len(self.metrics_history) > 1000:\n",
    "                self.metrics_history = self.metrics_history[-1000:]\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _scheduler_loop(self):\n",
    "        \"\"\"èª¿åº¦å™¨ä¸»å¾ªç’°\"\"\"\n",
    "        print(\"ğŸš€ æ™ºèƒ½æ‰¹æ¬¡èª¿åº¦å™¨å·²å•Ÿå‹•\")\n",
    "        \n",
    "        while not self.stop_event.is_set():\n",
    "            try:\n",
    "                # çµ„æˆæ‰¹æ¬¡\n",
    "                batch = self._form_batch()\n",
    "                \n",
    "                # æª¢æŸ¥æ˜¯å¦éœ€è¦è™•ç†\n",
    "                if self._should_process_batch(batch):\n",
    "                    metrics = self._process_batch(batch)\n",
    "                    if metrics:\n",
    "                        print(f\"ğŸ“Š è™•ç†æ‰¹æ¬¡ {metrics.batch_id}: \"\n",
    "                              f\"å¤§å°={metrics.batch_size}, \"\n",
    "                              f\"å»¶é²={metrics.total_latency:.3f}s, \"\n",
    "                              f\"ååé‡={metrics.throughput:.1f} req/s\")\n",
    "                else:\n",
    "                    # å¦‚æœæœ‰æ‰¹æ¬¡ä½†ä¸éœ€è¦ç«‹å³è™•ç†ï¼Œå°‡è«‹æ±‚æ”¾å›ä½‡åˆ—\n",
    "                    if batch:\n",
    "                        with self.queue_lock:\n",
    "                            for req in batch:\n",
    "                                heapq.heappush(self.priority_queue, req)\n",
    "                \n",
    "                # çŸ­æš«ç­‰å¾…\n",
    "                time.sleep(0.001)  # 1ms\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ èª¿åº¦å™¨éŒ¯èª¤: {str(e)}\")\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        print(\"â¹ï¸  æ™ºèƒ½æ‰¹æ¬¡èª¿åº¦å™¨å·²åœæ­¢\")\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"å•Ÿå‹•èª¿åº¦å™¨\"\"\"\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.stop_event.clear()\n",
    "            self.scheduler_thread = threading.Thread(target=self._scheduler_loop)\n",
    "            self.scheduler_thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"åœæ­¢èª¿åº¦å™¨\"\"\"\n",
    "        if self.running:\n",
    "            self.running = False\n",
    "            self.stop_event.set()\n",
    "            if self.scheduler_thread:\n",
    "                self.scheduler_thread.join()\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–æ€§èƒ½æ‘˜è¦\"\"\"\n",
    "        with self.metrics_lock:\n",
    "            if not self.metrics_history:\n",
    "                return {\"error\": \"æ²’æœ‰æ€§èƒ½æ•¸æ“š\"}\n",
    "            \n",
    "            recent_metrics = self.metrics_history[-100:]  # æœ€è¿‘ 100 å€‹æ‰¹æ¬¡\n",
    "            \n",
    "            avg_batch_size = np.mean([m.batch_size for m in recent_metrics])\n",
    "            avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "            avg_throughput = np.mean([m.throughput for m in recent_metrics])\n",
    "            avg_gpu_util = np.mean([m.gpu_utilization for m in recent_metrics])\n",
    "            \n",
    "            return {\n",
    "                \"total_requests\": self.stats[\"total_requests\"],\n",
    "                \"processed_requests\": self.stats[\"processed_requests\"],\n",
    "                \"expired_requests\": self.stats[\"expired_requests\"],\n",
    "                \"total_batches\": self.stats[\"total_batches\"],\n",
    "                \"avg_batch_size\": avg_batch_size,\n",
    "                \"avg_latency\": avg_latency,\n",
    "                \"avg_throughput\": avg_throughput,\n",
    "                \"avg_gpu_utilization\": avg_gpu_util,\n",
    "                \"queue_status\": self.get_queue_status()\n",
    "            }\n",
    "\n",
    "\n",
    "print(\"âœ… æ™ºèƒ½æ‰¹æ¬¡èª¿åº¦å™¨å¯¦ç¾å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 2ï¼šèª¿åº¦å™¨æ¸¬è©¦èˆ‡æ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 å‰µå»ºèª¿åº¦å™¨å¯¦ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºæ‰¹æ¬¡é…ç½®\n",
    "batch_config = BatchConfig(\n",
    "    min_batch_size=2,\n",
    "    max_batch_size=16,\n",
    "    max_wait_time=0.05,  # 50ms\n",
    "    target_latency=0.03,  # 30ms\n",
    "    priority_boost={\n",
    "        1: 0.5,  # VIP ç”¨æˆ¶ç­‰å¾…æ™‚é–“æ¸›åŠ\n",
    "        2: 0.7,  # é«˜å„ªå…ˆç´šç”¨æˆ¶\n",
    "        3: 1.0,  # æ™®é€šç”¨æˆ¶\n",
    "        4: 1.3,  # ä½å„ªå…ˆç´šç”¨æˆ¶\n",
    "        5: 1.8   # æ‰¹æ¬¡è™•ç†ç”¨æˆ¶\n",
    "    }\n",
    ")\n",
    "\n",
    "# å‰µå»ºèª¿åº¦å™¨\n",
    "scheduler = SmartBatchScheduler(batch_config)\n",
    "\n",
    "print(\"âœ… æ™ºèƒ½æ‰¹æ¬¡èª¿åº¦å™¨å‰µå»ºå®Œæˆ\")\n",
    "print(f\"ğŸ“Š é…ç½®: min_batch={batch_config.min_batch_size}, \"\n",
    "      f\"max_batch={batch_config.max_batch_size}, \"\n",
    "      f\"max_wait={batch_config.max_wait_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 è«‹æ±‚ç”Ÿæˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestGenerator:\n",
    "    \"\"\"è«‹æ±‚ç”Ÿæˆå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_types = {\n",
    "            \"vip\": {\"priority\": 1, \"ratio\": 0.05, \"timeout\": 1.0},\n",
    "            \"premium\": {\"priority\": 2, \"ratio\": 0.15, \"timeout\": 2.0},\n",
    "            \"regular\": {\"priority\": 3, \"ratio\": 0.70, \"timeout\": 5.0},\n",
    "            \"batch\": {\"priority\": 4, \"ratio\": 0.08, \"timeout\": 30.0},\n",
    "            \"background\": {\"priority\": 5, \"ratio\": 0.02, \"timeout\": 60.0}\n",
    "        }\n",
    "        self.request_counter = 0\n",
    "    \n",
    "    def generate_request(self) -> InferenceRequest:\n",
    "        \"\"\"ç”Ÿæˆå–®å€‹è«‹æ±‚\"\"\"\n",
    "        self.request_counter += 1\n",
    "        \n",
    "        # éš¨æ©Ÿé¸æ“‡ç”¨æˆ¶é¡å‹\n",
    "        rand = random.random()\n",
    "        cumulative = 0\n",
    "        selected_type = \"regular\"\n",
    "        \n",
    "        for user_type, config in self.user_types.items():\n",
    "            cumulative += config[\"ratio\"]\n",
    "            if rand <= cumulative:\n",
    "                selected_type = user_type\n",
    "                break\n",
    "        \n",
    "        user_config = self.user_types[selected_type]\n",
    "        \n",
    "        # å‰µå»ºè«‹æ±‚\n",
    "        request = InferenceRequest(\n",
    "            request_id=f\"req_{self.request_counter:06d}\",\n",
    "            user_id=f\"{selected_type}_user_{random.randint(1000, 9999)}\",\n",
    "            priority=user_config[\"priority\"],\n",
    "            data=np.random.randn(224, 224, 3),  # æ¨¡æ“¬åœ–åƒæ•¸æ“š\n",
    "            created_at=datetime.now(),\n",
    "            timeout=user_config[\"timeout\"],\n",
    "            metadata={\"user_type\": selected_type}\n",
    "        )\n",
    "        \n",
    "        return request\n",
    "    \n",
    "    def generate_burst_requests(self, count: int, \n",
    "                              priority_bias: Optional[int] = None) -> List[InferenceRequest]:\n",
    "        \"\"\"ç”Ÿæˆçªç™¼è«‹æ±‚\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for _ in range(count):\n",
    "            request = self.generate_request()\n",
    "            \n",
    "            # å¦‚æœæŒ‡å®šäº†å„ªå…ˆç´šåå‘ï¼Œèª¿æ•´å„ªå…ˆç´š\n",
    "            if priority_bias is not None:\n",
    "                request.priority = priority_bias\n",
    "            \n",
    "            requests.append(request)\n",
    "        \n",
    "        return requests\n",
    "    \n",
    "    def simulate_traffic_pattern(self, duration: int, \n",
    "                                pattern: str = \"normal\") -> List[InferenceRequest]:\n",
    "        \"\"\"æ¨¡æ“¬ä¸åŒçš„æµé‡æ¨¡å¼\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        if pattern == \"normal\":\n",
    "            # æ­£å¸¸æµé‡ï¼šæ¯ç§’ 10-20 å€‹è«‹æ±‚\n",
    "            for _ in range(duration):\n",
    "                count = random.randint(10, 20)\n",
    "                requests.extend(self.generate_burst_requests(count))\n",
    "        \n",
    "        elif pattern == \"spike\":\n",
    "            # çªç™¼æµé‡ï¼šçŸ­æ™‚é–“å…§å¤§é‡è«‹æ±‚\n",
    "            for second in range(duration):\n",
    "                if second % 10 == 0:  # æ¯ 10 ç§’ä¸€å€‹å³°å€¼\n",
    "                    count = random.randint(50, 100)\n",
    "                else:\n",
    "                    count = random.randint(5, 15)\n",
    "                requests.extend(self.generate_burst_requests(count))\n",
    "        \n",
    "        elif pattern == \"priority_flood\":\n",
    "            # é«˜å„ªå…ˆç´šè«‹æ±‚çªå¢\n",
    "            for second in range(duration):\n",
    "                if second < duration // 2:\n",
    "                    # å‰åŠæ®µæ­£å¸¸æµé‡\n",
    "                    count = random.randint(10, 20)\n",
    "                    requests.extend(self.generate_burst_requests(count))\n",
    "                else:\n",
    "                    # å¾ŒåŠæ®µé«˜å„ªå…ˆç´šçªå¢\n",
    "                    normal_count = random.randint(10, 20)\n",
    "                    priority_count = random.randint(20, 40)\n",
    "                    requests.extend(self.generate_burst_requests(normal_count))\n",
    "                    requests.extend(self.generate_burst_requests(priority_count, priority_bias=1))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "\n",
    "# å‰µå»ºè«‹æ±‚ç”Ÿæˆå™¨\n",
    "request_generator = RequestGenerator()\n",
    "print(\"âœ… è«‹æ±‚ç”Ÿæˆå™¨å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 å•Ÿå‹•èª¿åº¦å™¨ä¸¦é€²è¡Œæ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•Ÿå‹•èª¿åº¦å™¨\n",
    "scheduler.start()\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹æ€§èƒ½æ¸¬è©¦...\")\n",
    "\n",
    "# æ¸¬è©¦ 1: æ­£å¸¸æµé‡æ¨¡å¼\n",
    "print(\"\\nğŸ“Š æ¸¬è©¦ 1: æ­£å¸¸æµé‡æ¨¡å¼ï¼ˆ30ç§’ï¼‰\")\n",
    "normal_requests = request_generator.simulate_traffic_pattern(30, \"normal\")\n",
    "\n",
    "# åˆ†æ‰¹æ·»åŠ è«‹æ±‚ï¼ˆæ¨¡æ“¬å¯¦æ™‚åˆ°é”ï¼‰\n",
    "for i, request in enumerate(normal_requests):\n",
    "    scheduler.add_request(request)\n",
    "    \n",
    "    # æ¯ 50 å€‹è«‹æ±‚é¡¯ç¤ºä¸€æ¬¡é€²åº¦\n",
    "    if (i + 1) % 50 == 0:\n",
    "        status = scheduler.get_queue_status()\n",
    "        print(f\"   ğŸ“ˆ å·²æ·»åŠ  {i + 1} å€‹è«‹æ±‚ï¼Œä½‡åˆ—é•·åº¦: {status['queue_size']}\")\n",
    "    \n",
    "    # æ¨¡æ“¬è«‹æ±‚é–“éš”\n",
    "    time.sleep(0.001)\n",
    "\n",
    "# ç­‰å¾…è™•ç†å®Œæˆ\n",
    "time.sleep(2)\n",
    "performance_summary = scheduler.get_performance_summary()\n",
    "print(f\"âœ… æ­£å¸¸æµé‡æ¸¬è©¦å®Œæˆ:\")\n",
    "print(f\"   ğŸ“Š è™•ç†è«‹æ±‚: {performance_summary['processed_requests']}\")\n",
    "print(f\"   ğŸ“Š å¹³å‡å»¶é²: {performance_summary['avg_latency']:.3f}s\")\n",
    "print(f\"   ğŸ“Š å¹³å‡ååé‡: {performance_summary['avg_throughput']:.1f} req/s\")\n",
    "print(f\"   ğŸ“Š å¹³å‡æ‰¹æ¬¡å¤§å°: {performance_summary['avg_batch_size']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ 2: çªç™¼æµé‡æ¨¡å¼\n",
    "print(\"\\nğŸ“Š æ¸¬è©¦ 2: çªç™¼æµé‡æ¨¡å¼ï¼ˆ20ç§’ï¼‰\")\n",
    "spike_requests = request_generator.simulate_traffic_pattern(20, \"spike\")\n",
    "\n",
    "start_time = time.time()\n",
    "for request in spike_requests:\n",
    "    scheduler.add_request(request)\n",
    "    time.sleep(0.0001)  # æ›´å¿«çš„åˆ°é”ç‡\n",
    "\n",
    "# ç­‰å¾…è™•ç†å®Œæˆ\n",
    "time.sleep(3)\n",
    "spike_performance = scheduler.get_performance_summary()\n",
    "print(f\"âœ… çªç™¼æµé‡æ¸¬è©¦å®Œæˆ:\")\n",
    "print(f\"   ğŸ“Š è™•ç†è«‹æ±‚: {spike_performance['processed_requests'] - performance_summary['processed_requests']}\")\n",
    "print(f\"   ğŸ“Š ç•¶å‰å¹³å‡å»¶é²: {spike_performance['avg_latency']:.3f}s\")\n",
    "print(f\"   ğŸ“Š ç•¶å‰å¹³å‡ååé‡: {spike_performance['avg_throughput']:.1f} req/s\")\n",
    "print(f\"   ğŸ“Š ç•¶å‰å¹³å‡æ‰¹æ¬¡å¤§å°: {spike_performance['avg_batch_size']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ 3: å„ªå…ˆç´šå£“æ¸¬\n",
    "print(\"\\nğŸ“Š æ¸¬è©¦ 3: é«˜å„ªå…ˆç´šè«‹æ±‚çªå¢ï¼ˆ15ç§’ï¼‰\")\n",
    "priority_requests = request_generator.simulate_traffic_pattern(15, \"priority_flood\")\n",
    "\n",
    "for request in priority_requests:\n",
    "    scheduler.add_request(request)\n",
    "    time.sleep(0.0005)\n",
    "\n",
    "# ç­‰å¾…è™•ç†å®Œæˆ\n",
    "time.sleep(2)\n",
    "priority_performance = scheduler.get_performance_summary()\n",
    "print(f\"âœ… å„ªå…ˆç´šæ¸¬è©¦å®Œæˆ:\")\n",
    "print(f\"   ğŸ“Š ç¸½è™•ç†è«‹æ±‚: {priority_performance['processed_requests']}\")\n",
    "print(f\"   ğŸ“Š éæœŸè«‹æ±‚: {priority_performance['expired_requests']}\")\n",
    "print(f\"   ğŸ“Š ç¸½æ‰¹æ¬¡æ•¸: {priority_performance['total_batches']}\")\n",
    "\n",
    "# æœ€çµ‚ç‹€æ…‹\n",
    "final_status = scheduler.get_queue_status()\n",
    "print(f\"\\nğŸ“ˆ æœ€çµ‚ä½‡åˆ—ç‹€æ…‹:\")\n",
    "print(f\"   ğŸ“Š å‰©é¤˜è«‹æ±‚: {final_status['queue_size']}\")\n",
    "print(f\"   ğŸ“Š å„ªå…ˆç´šåˆ†å¸ƒ: {final_status['priority_distribution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 3ï¼šæ€§èƒ½ç›£æ§èˆ‡å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 å¯¦æ™‚æ€§èƒ½ç›£æ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scheduler_performance(scheduler: SmartBatchScheduler, window_size: int = 50):\n",
    "    \"\"\"å¯è¦–åŒ–èª¿åº¦å™¨æ€§èƒ½\"\"\"\n",
    "    metrics = scheduler.metrics_history[-window_size:] if scheduler.metrics_history else []\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"âŒ æ²’æœ‰æ€§èƒ½æ•¸æ“šå¯ä¾›å¯è¦–åŒ–\")\n",
    "        return\n",
    "    \n",
    "    # æº–å‚™æ•¸æ“š\n",
    "    timestamps = [m.timestamp for m in metrics]\n",
    "    batch_sizes = [m.batch_size for m in metrics]\n",
    "    latencies = [m.total_latency * 1000 for m in metrics]  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "    throughputs = [m.throughput for m in metrics]\n",
    "    wait_times = [m.wait_time * 1000 for m in metrics]  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "    gpu_utils = [m.gpu_utilization * 100 for m in metrics]  # è½‰æ›ç‚ºç™¾åˆ†æ¯”\n",
    "    \n",
    "    # å‰µå»ºå­åœ–\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('æ™ºèƒ½æ‰¹æ¬¡èª¿åº¦å™¨æ€§èƒ½ç›£æ§', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # æ‰¹æ¬¡å¤§å°è¶¨å‹¢\n",
    "    axes[0, 0].plot(timestamps, batch_sizes, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "    axes[0, 0].set_title('æ‰¹æ¬¡å¤§å°è®ŠåŒ–', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('æ‰¹æ¬¡å¤§å°')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axhline(y=np.mean(batch_sizes), color='r', linestyle='--', alpha=0.7, label=f'å¹³å‡: {np.mean(batch_sizes):.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # å»¶é²åˆ†å¸ƒ\n",
    "    axes[0, 1].plot(timestamps, latencies, 'g-', linewidth=2, marker='s', markersize=4)\n",
    "    axes[0, 1].set_title('ç¸½å»¶é²è®ŠåŒ–', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('å»¶é² (ms)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].axhline(y=scheduler.config.target_latency * 1000, color='r', linestyle='--', alpha=0.7, label='ç›®æ¨™å»¶é²')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # ååé‡è¶¨å‹¢\n",
    "    axes[0, 2].plot(timestamps, throughputs, 'purple', linewidth=2, marker='^', markersize=4)\n",
    "    axes[0, 2].set_title('ååé‡è®ŠåŒ–', fontweight='bold')\n",
    "    axes[0, 2].set_ylabel('ååé‡ (req/s)')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].axhline(y=np.mean(throughputs), color='r', linestyle='--', alpha=0.7, label=f'å¹³å‡: {np.mean(throughputs):.1f}')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # ç­‰å¾…æ™‚é–“åˆ†æ\n",
    "    axes[1, 0].plot(timestamps, wait_times, 'orange', linewidth=2, marker='d', markersize=4)\n",
    "    axes[1, 0].set_title('è«‹æ±‚ç­‰å¾…æ™‚é–“', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('ç­‰å¾…æ™‚é–“ (ms)')\n",
    "    axes[1, 0].set_xlabel('æ™‚é–“')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # GPU åˆ©ç”¨ç‡\n",
    "    axes[1, 1].plot(timestamps, gpu_utils, 'red', linewidth=2, marker='h', markersize=4)\n",
    "    axes[1, 1].set_title('GPU åˆ©ç”¨ç‡', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('åˆ©ç”¨ç‡ (%)')\n",
    "    axes[1, 1].set_xlabel('æ™‚é–“')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_ylim(0, 100)\n",
    "    \n",
    "    # å»¶é² vs æ‰¹æ¬¡å¤§å°æ•£é»åœ–\n",
    "    scatter = axes[1, 2].scatter(batch_sizes, latencies, c=throughputs, \n",
    "                                cmap='viridis', s=50, alpha=0.7)\n",
    "    axes[1, 2].set_title('å»¶é² vs æ‰¹æ¬¡å¤§å°', fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('æ‰¹æ¬¡å¤§å°')\n",
    "    axes[1, 2].set_ylabel('å»¶é² (ms)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ é¡è‰²æ¢\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 2])\n",
    "    cbar.set_label('ååé‡ (req/s)')\n",
    "    \n",
    "    # èª¿æ•´æ™‚é–“è»¸æ¨™ç±¤\n",
    "    for ax in axes.flat:\n",
    "        if 'time' in ax.get_xlabel().lower() or len(ax.get_xticklabels()) > 10:\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # çµ±è¨ˆæ‘˜è¦\n",
    "    print(f\"\\nğŸ“Š æ€§èƒ½çµ±è¨ˆæ‘˜è¦ï¼ˆæœ€è¿‘ {len(metrics)} å€‹æ‰¹æ¬¡ï¼‰:\")\n",
    "    print(f\"   ğŸ“ˆ å¹³å‡æ‰¹æ¬¡å¤§å°: {np.mean(batch_sizes):.2f} Â± {np.std(batch_sizes):.2f}\")\n",
    "    print(f\"   â±ï¸  å¹³å‡å»¶é²: {np.mean(latencies):.2f}ms Â± {np.std(latencies):.2f}ms\")\n",
    "    print(f\"   ğŸš€ å¹³å‡ååé‡: {np.mean(throughputs):.1f} Â± {np.std(throughputs):.1f} req/s\")\n",
    "    print(f\"   â³ å¹³å‡ç­‰å¾…æ™‚é–“: {np.mean(wait_times):.2f}ms Â± {np.std(wait_times):.2f}ms\")\n",
    "    print(f\"   ğŸ’» å¹³å‡ GPU åˆ©ç”¨ç‡: {np.mean(gpu_utils):.1f}% Â± {np.std(gpu_utils):.1f}%\")\n",
    "\n",
    "\n",
    "# å¯è¦–åŒ–æ€§èƒ½\n",
    "plot_scheduler_performance(scheduler, window_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å„ªå…ˆç´šè™•ç†æ•ˆèƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_priority_performance(scheduler: SmartBatchScheduler):\n",
    "    \"\"\"åˆ†æå„ªå…ˆç´šè™•ç†æ•ˆèƒ½\"\"\"\n",
    "    metrics = scheduler.metrics_history\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"âŒ æ²’æœ‰æ•¸æ“šå¯ä¾›åˆ†æ\")\n",
    "        return\n",
    "    \n",
    "    # æ”¶é›†å„ªå…ˆç´šçµ±è¨ˆ\n",
    "    priority_stats = defaultdict(lambda: {\n",
    "        'count': 0,\n",
    "        'total_latency': 0,\n",
    "        'total_wait_time': 0,\n",
    "        'batches': []\n",
    "    })\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for priority, count in metric.priority_distribution.items():\n",
    "            priority_stats[priority]['count'] += count\n",
    "            priority_stats[priority]['total_latency'] += metric.total_latency * count\n",
    "            priority_stats[priority]['total_wait_time'] += metric.wait_time * count\n",
    "            priority_stats[priority]['batches'].append(metric)\n",
    "    \n",
    "    # è¨ˆç®—å¹³å‡å€¼\n",
    "    priority_names = {1: 'VIP', 2: 'Premium', 3: 'Regular', 4: 'Batch', 5: 'Background'}\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('å„ªå…ˆç´šè™•ç†æ•ˆèƒ½åˆ†æ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # æº–å‚™æ•¸æ“š\n",
    "    priorities = sorted(priority_stats.keys())\n",
    "    priority_labels = [priority_names.get(p, f'P{p}') for p in priorities]\n",
    "    request_counts = [priority_stats[p]['count'] for p in priorities]\n",
    "    avg_latencies = [priority_stats[p]['total_latency'] / max(priority_stats[p]['count'], 1) * 1000 \n",
    "                    for p in priorities]  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "    avg_wait_times = [priority_stats[p]['total_wait_time'] / max(priority_stats[p]['count'], 1) * 1000 \n",
    "                     for p in priorities]  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "    \n",
    "    # è«‹æ±‚æ•¸é‡åˆ†å¸ƒ\n",
    "    colors = ['gold', 'lightcoral', 'lightblue', 'lightgreen', 'lightgray']\n",
    "    bars1 = ax1.bar(priority_labels, request_counts, color=colors[:len(priorities)], alpha=0.8)\n",
    "    ax1.set_title('å„å„ªå…ˆç´šè«‹æ±‚æ•¸é‡', fontweight='bold')\n",
    "    ax1.set_ylabel('è«‹æ±‚æ•¸é‡')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for bar, count in zip(bars1, request_counts):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(request_counts)*0.01,\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # å¹³å‡å»¶é²å°æ¯”\n",
    "    bars2 = ax2.bar(priority_labels, avg_latencies, color=colors[:len(priorities)], alpha=0.8)\n",
    "    ax2.set_title('å„å„ªå…ˆç´šå¹³å‡å»¶é²', fontweight='bold')\n",
    "    ax2.set_ylabel('å¹³å‡å»¶é² (ms)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, latency in zip(bars2, avg_latencies):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_latencies)*0.01,\n",
    "                f'{latency:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # å¹³å‡ç­‰å¾…æ™‚é–“å°æ¯”\n",
    "    bars3 = ax3.bar(priority_labels, avg_wait_times, color=colors[:len(priorities)], alpha=0.8)\n",
    "    ax3.set_title('å„å„ªå…ˆç´šå¹³å‡ç­‰å¾…æ™‚é–“', fontweight='bold')\n",
    "    ax3.set_ylabel('å¹³å‡ç­‰å¾…æ™‚é–“ (ms)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, wait_time in zip(bars3, avg_wait_times):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_wait_times)*0.01,\n",
    "                f'{wait_time:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # å„ªå…ˆç´šæ•ˆèƒ½æ¯”è¼ƒï¼ˆå»¶é²é™ä½æ¯”ä¾‹ï¼‰\n",
    "    if len(avg_latencies) > 1:\n",
    "        baseline_latency = avg_latencies[2] if len(avg_latencies) > 2 else avg_latencies[-1]  # ä½¿ç”¨ Regular æˆ–æœ€å¾Œä¸€å€‹ä½œç‚ºåŸºæº–\n",
    "        latency_improvements = [(baseline_latency - lat) / baseline_latency * 100 for lat in avg_latencies]\n",
    "        \n",
    "        bars4 = ax4.bar(priority_labels, latency_improvements, \n",
    "                       color=['green' if x > 0 else 'red' for x in latency_improvements], alpha=0.8)\n",
    "        ax4.set_title('å„ªå…ˆç´šå»¶é²æ”¹å–„ (vs Regular)', fontweight='bold')\n",
    "        ax4.set_ylabel('å»¶é²æ”¹å–„ (%)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        for bar, improvement in zip(bars4, latency_improvements):\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, \n",
    "                    bar.get_height() + (5 if improvement > 0 else -8),\n",
    "                    f'{improvement:+.1f}%', ha='center', va='bottom' if improvement > 0 else 'top', \n",
    "                    fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # è©³ç´°çµ±è¨ˆå ±å‘Š\n",
    "    print(f\"\\nğŸ“Š å„ªå…ˆç´šè™•ç†æ•ˆèƒ½å ±å‘Š:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, priority in enumerate(priorities):\n",
    "        stats = priority_stats[priority]\n",
    "        name = priority_names.get(priority, f'Priority {priority}')\n",
    "        \n",
    "        print(f\"\\nğŸ·ï¸  {name} (å„ªå…ˆç´š {priority}):\")\n",
    "        print(f\"   ğŸ“Š è™•ç†è«‹æ±‚æ•¸: {stats['count']}\")\n",
    "        print(f\"   â±ï¸  å¹³å‡å»¶é²: {avg_latencies[i]:.2f}ms\")\n",
    "        print(f\"   â³ å¹³å‡ç­‰å¾…æ™‚é–“: {avg_wait_times[i]:.2f}ms\")\n",
    "        print(f\"   ğŸ“ˆ åƒèˆ‡æ‰¹æ¬¡æ•¸: {len(stats['batches'])}\")\n",
    "        \n",
    "        if i > 0:  # èˆ‡å‰ä¸€å€‹å„ªå…ˆç´šæ¯”è¼ƒ\n",
    "            latency_diff = avg_latencies[i] - avg_latencies[i-1]\n",
    "            wait_diff = avg_wait_times[i] - avg_wait_times[i-1]\n",
    "            print(f\"   ğŸ“‰ vs æ›´é«˜å„ªå…ˆç´š: å»¶é² {latency_diff:+.2f}ms, ç­‰å¾…æ™‚é–“ {wait_diff:+.2f}ms\")\n",
    "\n",
    "\n",
    "# åˆ†æå„ªå…ˆç´šæ€§èƒ½\n",
    "analyze_priority_performance(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 4ï¼šè‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°å„ªåŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 è‡ªé©æ‡‰èª¿å„ªç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveBatchOptimizer:\n",
    "    \"\"\"è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°å„ªåŒ–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, scheduler: SmartBatchScheduler):\n",
    "        self.scheduler = scheduler\n",
    "        self.optimization_history = []\n",
    "        self.best_config = None\n",
    "        self.best_score = float('-inf')\n",
    "        \n",
    "        # å„ªåŒ–åƒæ•¸\n",
    "        self.param_ranges = {\n",
    "            'min_batch_size': (1, 8),\n",
    "            'max_batch_size': (8, 64),\n",
    "            'max_wait_time': (0.01, 0.2),\n",
    "            'target_latency': (0.01, 0.1)\n",
    "        }\n",
    "        \n",
    "        # è©•åˆ†æ¬Šé‡\n",
    "        self.score_weights = {\n",
    "            'throughput': 0.4,      # ååé‡æ¬Šé‡\n",
    "            'latency': 0.3,         # å»¶é²æ¬Šé‡ï¼ˆè² ç›¸é—œï¼‰\n",
    "            'utilization': 0.2,     # è³‡æºåˆ©ç”¨ç‡æ¬Šé‡\n",
    "            'fairness': 0.1         # å…¬å¹³æ€§æ¬Šé‡\n",
    "        }\n",
    "    \n",
    "    def calculate_performance_score(self, window_size: int = 20) -> float:\n",
    "        \"\"\"è¨ˆç®—æ€§èƒ½è©•åˆ†\"\"\"\n",
    "        recent_metrics = self.scheduler.metrics_history[-window_size:]\n",
    "        \n",
    "        if not recent_metrics:\n",
    "            return 0.0\n",
    "        \n",
    "        # è¨ˆç®—å„é …æŒ‡æ¨™\n",
    "        avg_throughput = np.mean([m.throughput for m in recent_metrics])\n",
    "        avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "        avg_utilization = np.mean([m.gpu_utilization for m in recent_metrics])\n",
    "        \n",
    "        # è¨ˆç®—å…¬å¹³æ€§ï¼ˆå„ªå…ˆç´šé–“å»¶é²çš„æ¨™æº–å·®ï¼Œè¶Šå°è¶Šå…¬å¹³ï¼‰\n",
    "        priority_latencies = defaultdict(list)\n",
    "        for metric in recent_metrics:\n",
    "            for priority in metric.priority_distribution.keys():\n",
    "                priority_latencies[priority].append(metric.total_latency)\n",
    "        \n",
    "        if len(priority_latencies) > 1:\n",
    "            priority_avg_latencies = [np.mean(latencies) for latencies in priority_latencies.values()]\n",
    "            fairness_score = 1.0 / (1.0 + np.std(priority_avg_latencies))\n",
    "        else:\n",
    "            fairness_score = 1.0\n",
    "        \n",
    "        # æ¨™æº–åŒ–åˆ†æ•¸ (0-1)\n",
    "        throughput_score = min(avg_throughput / 1000.0, 1.0)  # å‡è¨­æœ€å¤§ååé‡ 1000 req/s\n",
    "        latency_score = max(0, 1.0 - avg_latency / 0.2)  # å»¶é²è¶…é 200ms å¾— 0 åˆ†\n",
    "        utilization_score = avg_utilization  # GPU åˆ©ç”¨ç‡å·²ç¶“æ˜¯ 0-1\n",
    "        \n",
    "        # åŠ æ¬Šå¹³å‡\n",
    "        total_score = (\n",
    "            self.score_weights['throughput'] * throughput_score +\n",
    "            self.score_weights['latency'] * latency_score +\n",
    "            self.score_weights['utilization'] * utilization_score +\n",
    "            self.score_weights['fairness'] * fairness_score\n",
    "        )\n",
    "        \n",
    "        return total_score\n",
    "    \n",
    "    def generate_config_variant(self, base_config: BatchConfig, mutation_rate: float = 0.2) -> BatchConfig:\n",
    "        \"\"\"ç”Ÿæˆé…ç½®è®Šé«”\"\"\"\n",
    "        new_config = BatchConfig(\n",
    "            min_batch_size=base_config.min_batch_size,\n",
    "            max_batch_size=base_config.max_batch_size,\n",
    "            max_wait_time=base_config.max_wait_time,\n",
    "            target_latency=base_config.target_latency,\n",
    "            priority_boost=base_config.priority_boost.copy()\n",
    "        )\n",
    "        \n",
    "        # éš¨æ©Ÿè®Šç•°åƒæ•¸\n",
    "        if random.random() < mutation_rate:\n",
    "            min_val, max_val = self.param_ranges['min_batch_size']\n",
    "            new_config.min_batch_size = random.randint(min_val, max_val)\n",
    "        \n",
    "        if random.random() < mutation_rate:\n",
    "            min_val, max_val = self.param_ranges['max_batch_size']\n",
    "            new_config.max_batch_size = random.randint(min_val, max_val)\n",
    "            # ç¢ºä¿ max >= min\n",
    "            new_config.max_batch_size = max(new_config.max_batch_size, new_config.min_batch_size)\n",
    "        \n",
    "        if random.random() < mutation_rate:\n",
    "            min_val, max_val = self.param_ranges['max_wait_time']\n",
    "            new_config.max_wait_time = random.uniform(min_val, max_val)\n",
    "        \n",
    "        if random.random() < mutation_rate:\n",
    "            min_val, max_val = self.param_ranges['target_latency']\n",
    "            new_config.target_latency = random.uniform(min_val, max_val)\n",
    "        \n",
    "        return new_config\n",
    "    \n",
    "    def optimize_batch_config(self, iterations: int = 5, test_duration: int = 10):\n",
    "        \"\"\"å„ªåŒ–æ‰¹æ¬¡é…ç½®\"\"\"\n",
    "        print(f\"ğŸ”§ é–‹å§‹è‡ªé©æ‡‰æ‰¹æ¬¡é…ç½®å„ªåŒ–ï¼ˆ{iterations} æ¬¡è¿­ä»£ï¼‰\")\n",
    "        \n",
    "        current_config = self.scheduler.config\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            print(f\"\\nğŸ”„ è¿­ä»£ {iteration + 1}/{iterations}\")\n",
    "            \n",
    "            # ç”Ÿæˆæ–°é…ç½®\n",
    "            if iteration == 0:\n",
    "                test_config = current_config\n",
    "            else:\n",
    "                test_config = self.generate_config_variant(current_config)\n",
    "            \n",
    "            print(f\"   ğŸ”§ æ¸¬è©¦é…ç½®: min_batch={test_config.min_batch_size}, \"\n",
    "                  f\"max_batch={test_config.max_batch_size}, \"\n",
    "                  f\"max_wait={test_config.max_wait_time:.3f}s, \"\n",
    "                  f\"target_latency={test_config.target_latency:.3f}s\")\n",
    "            \n",
    "            # æ‡‰ç”¨æ–°é…ç½®\n",
    "            old_config = self.scheduler.config\n",
    "            self.scheduler.config = test_config\n",
    "            \n",
    "            # æ¸…ç©ºæ­·å²è¨˜éŒ„ä»¥ç²å¾—ç´”æ·¨æ¸¬è©¦\n",
    "            metrics_backup = self.scheduler.metrics_history.copy()\n",
    "            self.scheduler.metrics_history.clear()\n",
    "            \n",
    "            try:\n",
    "                # ç”Ÿæˆæ¸¬è©¦æµé‡\n",
    "                test_requests = request_generator.simulate_traffic_pattern(test_duration, \"normal\")\n",
    "                \n",
    "                # æ·»åŠ è«‹æ±‚\n",
    "                for request in test_requests:\n",
    "                    self.scheduler.add_request(request)\n",
    "                    time.sleep(0.001)\n",
    "                \n",
    "                # ç­‰å¾…è™•ç†å®Œæˆ\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # è¨ˆç®—æ€§èƒ½åˆ†æ•¸\n",
    "                score = self.calculate_performance_score()\n",
    "                \n",
    "                print(f\"   ğŸ“Š æ€§èƒ½åˆ†æ•¸: {score:.4f}\")\n",
    "                \n",
    "                # è¨˜éŒ„çµæœ\n",
    "                self.optimization_history.append({\n",
    "                    'iteration': iteration + 1,\n",
    "                    'config': test_config,\n",
    "                    'score': score,\n",
    "                    'metrics_count': len(self.scheduler.metrics_history)\n",
    "                })\n",
    "                \n",
    "                # æ›´æ–°æœ€ä½³é…ç½®\n",
    "                if score > self.best_score:\n",
    "                    self.best_score = score\n",
    "                    self.best_config = test_config\n",
    "                    current_config = test_config\n",
    "                    print(f\"   âœ… ç™¼ç¾æ›´ä½³é…ç½®ï¼æ–°æœ€ä½³åˆ†æ•¸: {score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   ğŸ“‰ é…ç½®æœªæ”¹å–„ï¼Œä¿æŒç•¶å‰é…ç½®\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ æ¸¬è©¦å¤±æ•—: {str(e)}\")\n",
    "                # æ¢å¾©åŸé…ç½®\n",
    "                self.scheduler.config = old_config\n",
    "            \n",
    "            # æ¢å¾©éƒ¨åˆ†æ­·å²è¨˜éŒ„\n",
    "            self.scheduler.metrics_history = metrics_backup + self.scheduler.metrics_history\n",
    "        \n",
    "        # æ‡‰ç”¨æœ€ä½³é…ç½®\n",
    "        if self.best_config:\n",
    "            self.scheduler.config = self.best_config\n",
    "            print(f\"\\nğŸ¯ å„ªåŒ–å®Œæˆï¼æ‡‰ç”¨æœ€ä½³é…ç½®:\")\n",
    "            print(f\"   ğŸ“Š æœ€ä½³åˆ†æ•¸: {self.best_score:.4f}\")\n",
    "            print(f\"   ğŸ”§ æœ€ä½³é…ç½®: min_batch={self.best_config.min_batch_size}, \"\n",
    "                  f\"max_batch={self.best_config.max_batch_size}, \"\n",
    "                  f\"max_wait={self.best_config.max_wait_time:.3f}s, \"\n",
    "                  f\"target_latency={self.best_config.target_latency:.3f}s\")\n",
    "    \n",
    "    def plot_optimization_history(self):\n",
    "        \"\"\"å¯è¦–åŒ–å„ªåŒ–æ­·å²\"\"\"\n",
    "        if not self.optimization_history:\n",
    "            print(\"âŒ æ²’æœ‰å„ªåŒ–æ­·å²æ•¸æ“š\")\n",
    "            return\n",
    "        \n",
    "        iterations = [h['iteration'] for h in self.optimization_history]\n",
    "        scores = [h['score'] for h in self.optimization_history]\n",
    "        min_batches = [h['config'].min_batch_size for h in self.optimization_history]\n",
    "        max_batches = [h['config'].max_batch_size for h in self.optimization_history]\n",
    "        wait_times = [h['config'].max_wait_time * 1000 for h in self.optimization_history]  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('è‡ªé©æ‡‰æ‰¹æ¬¡é…ç½®å„ªåŒ–æ­·å²', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # æ€§èƒ½åˆ†æ•¸è¶¨å‹¢\n",
    "        ax1.plot(iterations, scores, 'b-o', linewidth=2, markersize=8)\n",
    "        ax1.set_title('æ€§èƒ½åˆ†æ•¸è®ŠåŒ–', fontweight='bold')\n",
    "        ax1.set_xlabel('è¿­ä»£æ¬¡æ•¸')\n",
    "        ax1.set_ylabel('æ€§èƒ½åˆ†æ•¸')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ¨™è¨˜æœ€ä½³é»\n",
    "        best_idx = scores.index(max(scores))\n",
    "        ax1.scatter(iterations[best_idx], scores[best_idx], color='red', s=100, zorder=5)\n",
    "        ax1.annotate(f'æœ€ä½³: {scores[best_idx]:.4f}', \n",
    "                    xy=(iterations[best_idx], scores[best_idx]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "        \n",
    "        # æ‰¹æ¬¡å¤§å°è®ŠåŒ–\n",
    "        ax2.plot(iterations, min_batches, 'g-s', label='æœ€å°æ‰¹æ¬¡', linewidth=2, markersize=6)\n",
    "        ax2.plot(iterations, max_batches, 'r-^', label='æœ€å¤§æ‰¹æ¬¡', linewidth=2, markersize=6)\n",
    "        ax2.set_title('æ‰¹æ¬¡å¤§å°åƒæ•¸è®ŠåŒ–', fontweight='bold')\n",
    "        ax2.set_xlabel('è¿­ä»£æ¬¡æ•¸')\n",
    "        ax2.set_ylabel('æ‰¹æ¬¡å¤§å°')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ç­‰å¾…æ™‚é–“è®ŠåŒ–\n",
    "        ax3.plot(iterations, wait_times, 'purple', marker='d', linewidth=2, markersize=6)\n",
    "        ax3.set_title('æœ€å¤§ç­‰å¾…æ™‚é–“è®ŠåŒ–', fontweight='bold')\n",
    "        ax3.set_xlabel('è¿­ä»£æ¬¡æ•¸')\n",
    "        ax3.set_ylabel('ç­‰å¾…æ™‚é–“ (ms)')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # åƒæ•¸ç›¸é—œæ€§ç†±åœ–\n",
    "        param_data = pd.DataFrame({\n",
    "            'score': scores,\n",
    "            'min_batch': min_batches,\n",
    "            'max_batch': max_batches,\n",
    "            'wait_time': wait_times\n",
    "        })\n",
    "        \n",
    "        correlation_matrix = param_data.corr()\n",
    "        im = ax4.imshow(correlation_matrix, cmap='RdYlBu', aspect='auto', vmin=-1, vmax=1)\n",
    "        ax4.set_title('åƒæ•¸ç›¸é—œæ€§', fontweight='bold')\n",
    "        ax4.set_xticks(range(len(correlation_matrix.columns)))\n",
    "        ax4.set_yticks(range(len(correlation_matrix.columns)))\n",
    "        ax4.set_xticklabels(correlation_matrix.columns, rotation=45)\n",
    "        ax4.set_yticklabels(correlation_matrix.columns)\n",
    "        \n",
    "        # æ·»åŠ ç›¸é—œä¿‚æ•¸æ–‡å­—\n",
    "        for i in range(len(correlation_matrix)):\n",
    "            for j in range(len(correlation_matrix.columns)):\n",
    "                text = ax4.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax4)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# å‰µå»ºå„ªåŒ–å™¨\n",
    "optimizer = AdaptiveBatchOptimizer(scheduler)\n",
    "print(\"âœ… è‡ªé©æ‡‰æ‰¹æ¬¡å„ªåŒ–å™¨å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 åŸ·è¡Œè‡ªé©æ‡‰å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œå„ªåŒ–\n",
    "optimizer.optimize_batch_config(iterations=6, test_duration=8)\n",
    "\n",
    "# å¯è¦–åŒ–å„ªåŒ–éç¨‹\n",
    "optimizer.plot_optimization_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 å„ªåŒ–å¾Œæ€§èƒ½é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨å„ªåŒ–å¾Œçš„é…ç½®é€²è¡Œæœ€çµ‚é©—è­‰æ¸¬è©¦\n",
    "print(\"ğŸ§ª å„ªåŒ–å¾Œé…ç½®é©—è­‰æ¸¬è©¦\")\n",
    "\n",
    "# ç”Ÿæˆå¤§é‡æ¸¬è©¦è«‹æ±‚\n",
    "validation_requests = request_generator.simulate_traffic_pattern(30, \"spike\")\n",
    "\n",
    "print(f\"ğŸ“Š é©—è­‰æ¸¬è©¦é–‹å§‹ï¼Œå…± {len(validation_requests)} å€‹è«‹æ±‚\")\n",
    "\n",
    "# æ¸…ç©ºä¹‹å‰çš„æŒ‡æ¨™\n",
    "validation_start_metrics = len(scheduler.metrics_history)\n",
    "\n",
    "# æ·»åŠ é©—è­‰è«‹æ±‚\n",
    "for i, request in enumerate(validation_requests):\n",
    "    scheduler.add_request(request)\n",
    "    time.sleep(0.0005)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"   ğŸ“ˆ å·²æ·»åŠ  {i + 1}/{len(validation_requests)} å€‹è«‹æ±‚\")\n",
    "\n",
    "# ç­‰å¾…è™•ç†å®Œæˆ\n",
    "print(\"â³ ç­‰å¾…è™•ç†å®Œæˆ...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# ç²å–é©—è­‰çµæœ\n",
    "final_performance = scheduler.get_performance_summary()\n",
    "validation_metrics = scheduler.metrics_history[validation_start_metrics:]\n",
    "\n",
    "print(f\"\\nâœ… å„ªåŒ–å¾Œæ€§èƒ½é©—è­‰çµæœ:\")\n",
    "print(f\"   ğŸ“Š è™•ç†è«‹æ±‚: {len(validation_requests)}\")\n",
    "print(f\"   ğŸ“Š å¯¦éš›è™•ç†: {final_performance['processed_requests'] - (validation_start_metrics * np.mean([m.batch_size for m in scheduler.metrics_history[:validation_start_metrics]] if validation_start_metrics > 0 else [1]))}\")\n",
    "print(f\"   ğŸ“Š éæœŸè«‹æ±‚: {final_performance['expired_requests']}\")\n",
    "print(f\"   â±ï¸  å¹³å‡å»¶é²: {final_performance['avg_latency']:.3f}s\")\n",
    "print(f\"   ğŸš€ å¹³å‡ååé‡: {final_performance['avg_throughput']:.1f} req/s\")\n",
    "print(f\"   ğŸ“ å¹³å‡æ‰¹æ¬¡å¤§å°: {final_performance['avg_batch_size']:.1f}\")\n",
    "print(f\"   ğŸ’» å¹³å‡ GPU åˆ©ç”¨ç‡: {final_performance['avg_gpu_utilization']:.1f}%\")\n",
    "\n",
    "# è¨ˆç®—æœ€çµ‚æ€§èƒ½åˆ†æ•¸\n",
    "final_score = optimizer.calculate_performance_score(50)\n",
    "print(f\"   ğŸ¯ æœ€çµ‚æ€§èƒ½åˆ†æ•¸: {final_score:.4f}\")\n",
    "\n",
    "# æ¯”è¼ƒå„ªåŒ–å‰å¾Œ\n",
    "if optimizer.optimization_history:\n",
    "    initial_score = optimizer.optimization_history[0]['score']\n",
    "    improvement = ((final_score - initial_score) / initial_score) * 100\n",
    "    print(f\"   ğŸ“ˆ æ€§èƒ½æ”¹å–„: {improvement:+.1f}%\")\n",
    "\n",
    "# å¯è¦–åŒ–æœ€çµ‚æ€§èƒ½\n",
    "plot_scheduler_performance(scheduler, window_size=min(50, len(validation_metrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 5ï¼šè² è¼‰å‡è¡¡èˆ‡æ•…éšœæ¢å¾©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 å¤šèª¿åº¦å™¨è² è¼‰å‡è¡¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadBalancer:\n",
    "    \"\"\"è² è¼‰å‡è¡¡å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, schedulers: List[SmartBatchScheduler]):\n",
    "        self.schedulers = schedulers\n",
    "        self.scheduler_weights = [1.0] * len(schedulers)  # åˆå§‹æ¬Šé‡ç›¸ç­‰\n",
    "        self.request_counts = [0] * len(schedulers)\n",
    "        self.health_status = [True] * len(schedulers)\n",
    "        \n",
    "        # è² è¼‰å‡è¡¡ç­–ç•¥\n",
    "        self.strategies = {\n",
    "            'round_robin': self._round_robin,\n",
    "            'least_connections': self._least_connections,\n",
    "            'weighted_performance': self._weighted_performance,\n",
    "            'priority_aware': self._priority_aware\n",
    "        }\n",
    "        \n",
    "        self.current_strategy = 'weighted_performance'\n",
    "        self.round_robin_index = 0\n",
    "    \n",
    "    def _round_robin(self, request: InferenceRequest) -> int:\n",
    "        \"\"\"è¼ªè©¢ç­–ç•¥\"\"\"\n",
    "        # è·³éä¸å¥åº·çš„èª¿åº¦å™¨\n",
    "        attempts = 0\n",
    "        while attempts < len(self.schedulers):\n",
    "            if self.health_status[self.round_robin_index]:\n",
    "                selected = self.round_robin_index\n",
    "                self.round_robin_index = (self.round_robin_index + 1) % len(self.schedulers)\n",
    "                return selected\n",
    "            self.round_robin_index = (self.round_robin_index + 1) % len(self.schedulers)\n",
    "            attempts += 1\n",
    "        return 0  # å‚™ç”¨æ–¹æ¡ˆ\n",
    "    \n",
    "    def _least_connections(self, request: InferenceRequest) -> int:\n",
    "        \"\"\"æœ€å°‘é€£æ¥ç­–ç•¥\"\"\"\n",
    "        min_connections = float('inf')\n",
    "        selected_idx = 0\n",
    "        \n",
    "        for i, scheduler in enumerate(self.schedulers):\n",
    "            if not self.health_status[i]:\n",
    "                continue\n",
    "            \n",
    "            queue_size = scheduler.get_queue_status()['queue_size']\n",
    "            if queue_size < min_connections:\n",
    "                min_connections = queue_size\n",
    "                selected_idx = i\n",
    "        \n",
    "        return selected_idx\n",
    "    \n",
    "    def _weighted_performance(self, request: InferenceRequest) -> int:\n",
    "        \"\"\"åŸºæ–¼æ€§èƒ½æ¬Šé‡çš„ç­–ç•¥\"\"\"\n",
    "        # æ›´æ–°æ¬Šé‡åŸºæ–¼æœ€è¿‘æ€§èƒ½\n",
    "        self._update_weights()\n",
    "        \n",
    "        # åŠ æ¬Šéš¨æ©Ÿé¸æ“‡\n",
    "        healthy_indices = [i for i, healthy in enumerate(self.health_status) if healthy]\n",
    "        if not healthy_indices:\n",
    "            return 0\n",
    "        \n",
    "        healthy_weights = [self.scheduler_weights[i] for i in healthy_indices]\n",
    "        total_weight = sum(healthy_weights)\n",
    "        \n",
    "        if total_weight == 0:\n",
    "            return random.choice(healthy_indices)\n",
    "        \n",
    "        rand_val = random.uniform(0, total_weight)\n",
    "        cumulative = 0\n",
    "        \n",
    "        for i, idx in enumerate(healthy_indices):\n",
    "            cumulative += healthy_weights[i]\n",
    "            if rand_val <= cumulative:\n",
    "                return idx\n",
    "        \n",
    "        return healthy_indices[-1]\n",
    "    \n",
    "    def _priority_aware(self, request: InferenceRequest) -> int:\n",
    "        \"\"\"å„ªå…ˆç´šæ„ŸçŸ¥ç­–ç•¥\"\"\"\n",
    "        # é«˜å„ªå…ˆç´šè«‹æ±‚é¸æ“‡æ€§èƒ½æœ€å¥½çš„èª¿åº¦å™¨\n",
    "        if request.priority <= 2:\n",
    "            best_scheduler = -1\n",
    "            best_score = -1\n",
    "            \n",
    "            for i, scheduler in enumerate(self.schedulers):\n",
    "                if not self.health_status[i]:\n",
    "                    continue\n",
    "                \n",
    "                # è¨ˆç®—èª¿åº¦å™¨åˆ†æ•¸ï¼ˆä½å»¶é² + ä½ä½‡åˆ—é•·åº¦ï¼‰\n",
    "                recent_metrics = scheduler.metrics_history[-5:]\n",
    "                if recent_metrics:\n",
    "                    avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "                    queue_size = scheduler.get_queue_status()['queue_size']\n",
    "                    score = 1.0 / (avg_latency + 0.001) - queue_size * 0.01\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_scheduler = i\n",
    "            \n",
    "            return best_scheduler if best_scheduler >= 0 else 0\n",
    "        else:\n",
    "            # ä½å„ªå…ˆç´šè«‹æ±‚ä½¿ç”¨è² è¼‰å‡è¡¡\n",
    "            return self._least_connections(request)\n",
    "    \n",
    "    def _update_weights(self):\n",
    "        \"\"\"æ›´æ–°èª¿åº¦å™¨æ¬Šé‡\"\"\"\n",
    "        for i, scheduler in enumerate(self.schedulers):\n",
    "            if not self.health_status[i]:\n",
    "                self.scheduler_weights[i] = 0.0\n",
    "                continue\n",
    "            \n",
    "            recent_metrics = scheduler.metrics_history[-10:]\n",
    "            if recent_metrics:\n",
    "                avg_throughput = np.mean([m.throughput for m in recent_metrics])\n",
    "                avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "                \n",
    "                # æ¬Šé‡ = ååé‡ / å»¶é²\n",
    "                weight = avg_throughput / max(avg_latency, 0.001)\n",
    "                self.scheduler_weights[i] = weight\n",
    "            else:\n",
    "                self.scheduler_weights[i] = 1.0\n",
    "    \n",
    "    def route_request(self, request: InferenceRequest) -> bool:\n",
    "        \"\"\"è·¯ç”±è«‹æ±‚åˆ°åˆé©çš„èª¿åº¦å™¨\"\"\"\n",
    "        strategy_func = self.strategies.get(self.current_strategy, self._round_robin)\n",
    "        selected_idx = strategy_func(request)\n",
    "        \n",
    "        if 0 <= selected_idx < len(self.schedulers) and self.health_status[selected_idx]:\n",
    "            success = self.schedulers[selected_idx].add_request(request)\n",
    "            if success:\n",
    "                self.request_counts[selected_idx] += 1\n",
    "            return success\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def check_health(self):\n",
    "        \"\"\"æª¢æŸ¥èª¿åº¦å™¨å¥åº·ç‹€æ…‹\"\"\"\n",
    "        for i, scheduler in enumerate(self.schedulers):\n",
    "            try:\n",
    "                # ç°¡å–®å¥åº·æª¢æŸ¥ï¼šæª¢æŸ¥ä½‡åˆ—ç‹€æ…‹\n",
    "                status = scheduler.get_queue_status()\n",
    "                \n",
    "                # å¦‚æœä½‡åˆ—éé•·æˆ–æœ€è€è«‹æ±‚éä¹…ï¼Œæ¨™è¨˜ç‚ºä¸å¥åº·\n",
    "                queue_too_long = status['queue_size'] > 1000\n",
    "                oldest_too_old = status['oldest_request_age'] > 60  # 60ç§’\n",
    "                \n",
    "                self.health_status[i] = not (queue_too_long or oldest_too_old)\n",
    "                \n",
    "            except Exception:\n",
    "                self.health_status[i] = False\n",
    "    \n",
    "    def get_load_distribution(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–è² è¼‰åˆ†å¸ƒç‹€æ…‹\"\"\"\n",
    "        total_requests = sum(self.request_counts)\n",
    "        \n",
    "        distribution = []\n",
    "        for i, scheduler in enumerate(self.schedulers):\n",
    "            queue_status = scheduler.get_queue_status()\n",
    "            performance = scheduler.get_performance_summary()\n",
    "            \n",
    "            distribution.append({\n",
    "                'scheduler_id': i,\n",
    "                'healthy': self.health_status[i],\n",
    "                'weight': self.scheduler_weights[i],\n",
    "                'requests_routed': self.request_counts[i],\n",
    "                'request_percentage': (self.request_counts[i] / max(total_requests, 1)) * 100,\n",
    "                'queue_size': queue_status['queue_size'],\n",
    "                'avg_latency': performance.get('avg_latency', 0),\n",
    "                'avg_throughput': performance.get('avg_throughput', 0)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'strategy': self.current_strategy,\n",
    "            'total_requests': total_requests,\n",
    "            'healthy_schedulers': sum(self.health_status),\n",
    "            'distribution': distribution\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ… è² è¼‰å‡è¡¡å™¨å¯¦ç¾å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 å‰µå»ºå¤šèª¿åº¦å™¨ç’°å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœæ­¢åŸèª¿åº¦å™¨\n",
    "scheduler.stop()\n",
    "\n",
    "# å‰µå»ºå¤šå€‹èª¿åº¦å™¨å¯¦ä¾‹\n",
    "print(\"ğŸ”§ å‰µå»ºå¤šèª¿åº¦å™¨ç’°å¢ƒ...\")\n",
    "\n",
    "# èª¿åº¦å™¨é…ç½®ï¼ˆæ¯å€‹ç•¥æœ‰ä¸åŒä»¥æ¨¡æ“¬çœŸå¯¦ç’°å¢ƒï¼‰\n",
    "scheduler_configs = [\n",
    "    BatchConfig(min_batch_size=1, max_batch_size=8, max_wait_time=0.03, target_latency=0.025),\n",
    "    BatchConfig(min_batch_size=2, max_batch_size=16, max_wait_time=0.05, target_latency=0.035),\n",
    "    BatchConfig(min_batch_size=1, max_batch_size=12, max_wait_time=0.04, target_latency=0.030),\n",
    "]\n",
    "\n",
    "# å‰µå»ºèª¿åº¦å™¨å¯¦ä¾‹\n",
    "schedulers = []\n",
    "for i, config in enumerate(scheduler_configs):\n",
    "    sched = SmartBatchScheduler(config, model_name=f\"text_classifier_replica_{i}\")\n",
    "    sched.start()\n",
    "    schedulers.append(sched)\n",
    "    print(f\"   âœ… èª¿åº¦å™¨ {i} å·²å•Ÿå‹• (max_batch={config.max_batch_size})\")\n",
    "\n",
    "# å‰µå»ºè² è¼‰å‡è¡¡å™¨\n",
    "load_balancer = LoadBalancer(schedulers)\n",
    "print(f\"âœ… è² è¼‰å‡è¡¡å™¨å·²å‰µå»ºï¼Œç®¡ç† {len(schedulers)} å€‹èª¿åº¦å™¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 è² è¼‰å‡è¡¡æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ä¸åŒè² è¼‰å‡è¡¡ç­–ç•¥\n",
    "strategies_to_test = ['round_robin', 'least_connections', 'weighted_performance', 'priority_aware']\n",
    "\n",
    "strategy_results = {}\n",
    "\n",
    "for strategy in strategies_to_test:\n",
    "    print(f\"\\nğŸ§ª æ¸¬è©¦è² è¼‰å‡è¡¡ç­–ç•¥: {strategy}\")\n",
    "    \n",
    "    # é‡ç½®çµ±è¨ˆ\n",
    "    load_balancer.current_strategy = strategy\n",
    "    load_balancer.request_counts = [0] * len(schedulers)\n",
    "    \n",
    "    # ç”Ÿæˆæ¸¬è©¦è«‹æ±‚\n",
    "    test_requests = request_generator.simulate_traffic_pattern(20, \"normal\")\n",
    "    \n",
    "    # è·¯ç”±è«‹æ±‚\n",
    "    successful_routes = 0\n",
    "    for request in test_requests:\n",
    "        if load_balancer.route_request(request):\n",
    "            successful_routes += 1\n",
    "        time.sleep(0.001)\n",
    "    \n",
    "    # ç­‰å¾…è™•ç†\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # æª¢æŸ¥å¥åº·ç‹€æ…‹\n",
    "    load_balancer.check_health()\n",
    "    \n",
    "    # ç²å–åˆ†å¸ƒç‹€æ…‹\n",
    "    distribution = load_balancer.get_load_distribution()\n",
    "    strategy_results[strategy] = distribution\n",
    "    \n",
    "    print(f\"   ğŸ“Š æˆåŠŸè·¯ç”±: {successful_routes}/{len(test_requests)} å€‹è«‹æ±‚\")\n",
    "    print(f\"   ğŸ¥ å¥åº·èª¿åº¦å™¨: {distribution['healthy_schedulers']}/{len(schedulers)}\")\n",
    "    \n",
    "    for i, sched_info in enumerate(distribution['distribution']):\n",
    "        print(f\"   ğŸ“ˆ èª¿åº¦å™¨ {i}: {sched_info['requests_routed']} è«‹æ±‚ \"\n",
    "              f\"({sched_info['request_percentage']:.1f}%), \"\n",
    "              f\"ä½‡åˆ—: {sched_info['queue_size']}, \"\n",
    "              f\"å¥åº·: {'âœ…' if sched_info['healthy'] else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 å¯è¦–åŒ–è² è¼‰å‡è¡¡æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_load_balancing_results(strategy_results: Dict[str, Dict]):\n",
    "    \"\"\"å¯è¦–åŒ–è² è¼‰å‡è¡¡çµæœ\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('è² è¼‰å‡è¡¡ç­–ç•¥æ•ˆæœå°æ¯”', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    strategies = list(strategy_results.keys())\n",
    "    num_schedulers = len(strategy_results[strategies[0]]['distribution'])\n",
    "    \n",
    "    # è«‹æ±‚åˆ†å¸ƒå°æ¯”\n",
    "    ax1 = axes[0, 0]\n",
    "    x = np.arange(num_schedulers)\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, strategy in enumerate(strategies):\n",
    "        percentages = [sched['request_percentage'] for sched in strategy_results[strategy]['distribution']]\n",
    "        ax1.bar(x + i * width, percentages, width, label=strategy, alpha=0.8)\n",
    "    \n",
    "    ax1.set_title('è«‹æ±‚åˆ†å¸ƒæ¯”ä¾‹ (%)', fontweight='bold')\n",
    "    ax1.set_xlabel('èª¿åº¦å™¨ ID')\n",
    "    ax1.set_ylabel('è«‹æ±‚æ¯”ä¾‹ (%)')\n",
    "    ax1.set_xticks(x + width * 1.5)\n",
    "    ax1.set_xticklabels([f'Sched {i}' for i in range(num_schedulers)])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å¹³å‡å»¶é²å°æ¯”\n",
    "    ax2 = axes[0, 1]\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        latencies = [sched['avg_latency'] * 1000 for sched in strategy_results[strategy]['distribution']]\n",
    "        ax2.bar(x + i * width, latencies, width, label=strategy, alpha=0.8)\n",
    "    \n",
    "    ax2.set_title('å¹³å‡å»¶é²å°æ¯” (ms)', fontweight='bold')\n",
    "    ax2.set_xlabel('èª¿åº¦å™¨ ID')\n",
    "    ax2.set_ylabel('å¹³å‡å»¶é² (ms)')\n",
    "    ax2.set_xticks(x + width * 1.5)\n",
    "    ax2.set_xticklabels([f'Sched {i}' for i in range(num_schedulers)])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ååé‡å°æ¯”\n",
    "    ax3 = axes[1, 0]\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        throughputs = [sched['avg_throughput'] for sched in strategy_results[strategy]['distribution']]\n",
    "        ax3.bar(x + i * width, throughputs, width, label=strategy, alpha=0.8)\n",
    "    \n",
    "    ax3.set_title('å¹³å‡ååé‡å°æ¯” (req/s)', fontweight='bold')\n",
    "    ax3.set_xlabel('èª¿åº¦å™¨ ID')\n",
    "    ax3.set_ylabel('ååé‡ (req/s)')\n",
    "    ax3.set_xticks(x + width * 1.5)\n",
    "    ax3.set_xticklabels([f'Sched {i}' for i in range(num_schedulers)])\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # è² è¼‰å‡è¡¡åº¦åˆ†æï¼ˆæ¨™æº–å·®ï¼‰\n",
    "    ax4 = axes[1, 1]\n",
    "    balance_scores = []\n",
    "    strategy_names = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        percentages = [sched['request_percentage'] for sched in strategy_results[strategy]['distribution']]\n",
    "        # è¨ˆç®—åˆ†å¸ƒçš„æ¨™æº–å·®ï¼ˆè¶Šå°è¶Šå‡è¡¡ï¼‰\n",
    "        balance_score = np.std(percentages)\n",
    "        balance_scores.append(balance_score)\n",
    "        strategy_names.append(strategy)\n",
    "    \n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "    bars = ax4.bar(strategy_names, balance_scores, color=colors[:len(strategies)], alpha=0.8)\n",
    "    ax4.set_title('è² è¼‰å‡è¡¡åº¦ (æ¨™æº–å·®)', fontweight='bold')\n",
    "    ax4.set_ylabel('åˆ†å¸ƒæ¨™æº–å·® (è¶Šå°è¶Šå‡è¡¡)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for bar, score in zip(bars, balance_scores):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(balance_scores)*0.01,\n",
    "                f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ç­–ç•¥æ•ˆæœæ‘˜è¦\n",
    "    print(f\"\\nğŸ“Š è² è¼‰å‡è¡¡ç­–ç•¥æ•ˆæœæ‘˜è¦:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        result = strategy_results[strategy]\n",
    "        percentages = [sched['request_percentage'] for sched in result['distribution']]\n",
    "        latencies = [sched['avg_latency'] * 1000 for sched in result['distribution']]\n",
    "        throughputs = [sched['avg_throughput'] for sched in result['distribution']]\n",
    "        \n",
    "        balance_score = np.std(percentages)\n",
    "        avg_latency = np.mean(latencies)\n",
    "        total_throughput = np.sum(throughputs)\n",
    "        \n",
    "        print(f\"\\nğŸ”§ {strategy.replace('_', ' ').title()}:\")\n",
    "        print(f\"   âš–ï¸  å‡è¡¡åº¦: {balance_score:.2f} (è¶Šå°è¶Šå¥½)\")\n",
    "        print(f\"   â±ï¸  å¹³å‡å»¶é²: {avg_latency:.2f}ms\")\n",
    "        print(f\"   ğŸš€ ç¸½ååé‡: {total_throughput:.1f} req/s\")\n",
    "        print(f\"   ğŸ¥ å¥åº·ç‡: {result['healthy_schedulers']}/{len(schedulers)}\")\n",
    "\n",
    "\n",
    "# å¯è¦–åŒ–çµæœ\n",
    "plot_load_balancing_results(strategy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š å¯¦é©—ç¸½çµèˆ‡æœ€ä½³å¯¦è¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœæ­¢æ‰€æœ‰èª¿åº¦å™¨\n",
    "for i, sched in enumerate(schedulers):\n",
    "    sched.stop()\n",
    "    print(f\"â¹ï¸  èª¿åº¦å™¨ {i} å·²åœæ­¢\")\n",
    "\n",
    "# å°å‡ºå¯¦é©—çµæœ\n",
    "def export_dynamic_batching_results():\n",
    "    \"\"\"å°å‡ºå‹•æ…‹æ‰¹æ¬¡è™•ç†å¯¦é©—çµæœ\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # ç¶œåˆçµæœ\n",
    "    experiment_results = {\n",
    "        \"experiment_name\": \"Dynamic Batching Advanced\",\n",
    "        \"timestamp\": timestamp,\n",
    "        \"optimization_history\": optimizer.optimization_history,\n",
    "        \"best_config\": {\n",
    "            \"min_batch_size\": optimizer.best_config.min_batch_size if optimizer.best_config else None,\n",
    "            \"max_batch_size\": optimizer.best_config.max_batch_size if optimizer.best_config else None,\n",
    "            \"max_wait_time\": optimizer.best_config.max_wait_time if optimizer.best_config else None,\n",
    "            \"target_latency\": optimizer.best_config.target_latency if optimizer.best_config else None,\n",
    "        },\n",
    "        \"best_score\": optimizer.best_score,\n",
    "        \"load_balancing_results\": strategy_results,\n",
    "        \"scheduler_metrics\": [\n",
    "            {\n",
    "                \"scheduler_id\": i,\n",
    "                \"total_batches\": len(sched.metrics_history),\n",
    "                \"performance_summary\": sched.get_performance_summary()\n",
    "            }\n",
    "            for i, sched in enumerate(schedulers)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # å°å‡º JSON æ–‡ä»¶\n",
    "    results_file = f\"{EXPERIMENT_DIR}/dynamic_batching_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(experiment_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"ğŸ“„ å¯¦é©—çµæœå·²å°å‡º: {results_file}\")\n",
    "    return results_file\n",
    "\n",
    "\n",
    "# å°å‡ºçµæœ\n",
    "results_file = export_dynamic_batching_results()\n",
    "\n",
    "# æœ€ä½³å¯¦è¸ç¸½çµ\n",
    "best_practices_summary = \"\"\"\n",
    "ğŸ¯ é«˜ç´šå‹•æ…‹æ‰¹æ¬¡è™•ç†æœ€ä½³å¯¦è¸ç¸½çµ\n",
    "\n",
    "ğŸ”§ æ™ºèƒ½èª¿åº¦ç­–ç•¥:\n",
    "   âœ… è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°èª¿æ•´\n",
    "   âœ… å„ªå…ˆç´šæ„ŸçŸ¥çš„ç­‰å¾…æ™‚é–“ç­–ç•¥\n",
    "   âœ… å¯¦æ™‚æ€§èƒ½ç›£æ§å’Œåé¥‹èª¿æ•´\n",
    "   âœ… åŸºæ–¼è² è¼‰çš„å‹•æ…‹é…ç½®å„ªåŒ–\n",
    "\n",
    "âš–ï¸ è² è¼‰å‡è¡¡ç­–ç•¥é¸æ“‡:\n",
    "   ğŸ“Š è¼ªè©¢ç­–ç•¥: ç°¡å–®å‡å‹»ï¼Œé©åˆåŒè³ªåŒ–ç’°å¢ƒ\n",
    "   ğŸ“Š æœ€å°‘é€£æ¥: å‹•æ…‹å‡è¡¡ï¼Œé©åˆç•°è³ªåŒ–è² è¼‰\n",
    "   ğŸ“Š æ¬Šé‡æ€§èƒ½: æ™ºèƒ½åˆ†é…ï¼Œé©åˆæ€§èƒ½å·®ç•°åŒ–\n",
    "   ğŸ“Š å„ªå…ˆç´šæ„ŸçŸ¥: æœå‹™è³ªé‡ä¿è­‰ï¼Œé©åˆSLAè¦æ±‚\n",
    "\n",
    "ğŸ¯ é—œéµæ€§èƒ½æŒ‡æ¨™:\n",
    "   â±ï¸  å»¶é²å„ªåŒ–: ç›®æ¨™ < 50ms P99 å»¶é²\n",
    "   ğŸš€ ååé‡: æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡\n",
    "   âš–ï¸  å…¬å¹³æ€§: å„ªå…ˆç´šé–“åˆç†çš„å»¶é²å·®ç•°\n",
    "   ğŸ’» è³‡æºæ•ˆç‡: GPU åˆ©ç”¨ç‡ > 80%\n",
    "\n",
    "ğŸ›¡ï¸ æ•…éšœæ¢å¾©æ©Ÿåˆ¶:\n",
    "   âœ… å¥åº·æª¢æŸ¥å’Œè‡ªå‹•æ•…éšœè½‰ç§»\n",
    "   âœ… è«‹æ±‚è¶…æ™‚å’ŒéæœŸè™•ç†\n",
    "   âœ… ä½‡åˆ—é•·åº¦ç›£æ§å’Œé™åˆ¶\n",
    "   âœ… æ€§èƒ½é€€åŒ–æª¢æ¸¬å’Œæ¢å¾©\n",
    "\n",
    "ğŸ“ˆ å„ªåŒ–å»ºè­°:\n",
    "   ğŸ”„ å®šæœŸé‡æ–°è©•ä¼°å’Œèª¿å„ªæ‰¹æ¬¡åƒæ•¸\n",
    "   ğŸ“Š åŸºæ–¼æ¥­å‹™æ¨¡å¼èª¿æ•´å„ªå…ˆç´šç­–ç•¥\n",
    "   ğŸ¯ æ ¹æ“šSLAè¦æ±‚è¨­å®šç›®æ¨™å»¶é²\n",
    "   ğŸ’¡ ä½¿ç”¨A/Bæ¸¬è©¦é©—è­‰å„ªåŒ–æ•ˆæœ\n",
    "\n",
    "ğŸš€ ç”Ÿç”¢éƒ¨ç½²è€ƒæ…®:\n",
    "   ğŸ“¦ å®¹å™¨åŒ–éƒ¨ç½²æ”¯æŒæ°´å¹³æ“´å±•\n",
    "   ğŸ“Š å®Œæ•´çš„ç›£æ§å’Œå‘Šè­¦é«”ç³»\n",
    "   ğŸ”§ é…ç½®ç†±æ›´æ–°èƒ½åŠ›\n",
    "   ğŸ“ è©³ç´°çš„æ€§èƒ½æ—¥èªŒè¨˜éŒ„\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices_summary)\n",
    "\n",
    "print(f\"\\nâœ… Lab 2.4.2 é«˜ç´šå‹•æ…‹æ‰¹æ¬¡è™•ç†å¯¦é©—å®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š å¯¦é©—æ•¸æ“šå·²ä¿å­˜è‡³: {EXPERIMENT_DIR}\")\n",
    "print(f\"ğŸ¯ æœ€ä½³é…ç½®å·²ç¢ºå®šä¸¦å¯ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– ç¸½çµ\n",
    "\n",
    "æœ¬å¯¦é©—å®Œæˆäº†é«˜ç´šå‹•æ…‹æ‰¹æ¬¡è™•ç†èˆ‡æ™ºèƒ½èª¿åº¦çš„å®Œæ•´å¯¦ç¾ï¼š\n",
    "\n",
    "### ğŸ¯ å¯¦é©—æˆæœ\n",
    "1. **æ™ºèƒ½èª¿åº¦å™¨** - å¯¦ç¾äº†è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°å’Œå„ªå…ˆç´šæ„ŸçŸ¥èª¿åº¦\n",
    "2. **æ€§èƒ½å„ªåŒ–** - é€šéè‡ªå‹•èª¿å„ªå¯¦ç¾å»¶é²èˆ‡ååé‡çš„æœ€ä½³å¹³è¡¡\n",
    "3. **è² è¼‰å‡è¡¡** - æ”¯æ´å¤šç¨®ç­–ç•¥çš„æ™ºèƒ½è«‹æ±‚è·¯ç”±\n",
    "4. **æ•…éšœæ¢å¾©** - å…·å‚™å¥åº·æª¢æŸ¥å’Œè‡ªå‹•æ•…éšœè½‰ç§»èƒ½åŠ›\n",
    "\n",
    "### ğŸ”§ é—œéµæŠ€è¡“ç‰¹é»\n",
    "- åŸºæ–¼å„ªå…ˆç´šçš„å‹•æ…‹ç­‰å¾…æ™‚é–“èª¿æ•´\n",
    "- å¯¦æ™‚æ€§èƒ½ç›£æ§å’Œåé¥‹å¼åƒæ•¸èª¿å„ª\n",
    "- å¤šèª¿åº¦å™¨å”åŒå·¥ä½œå’Œè² è¼‰åˆ†æ•£\n",
    "- è‡ªå‹•åŒ–æ•…éšœæª¢æ¸¬å’Œæ¢å¾©æ©Ÿåˆ¶\n",
    "\n",
    "### ğŸš€ å¯¦éš›æ‡‰ç”¨åƒ¹å€¼\n",
    "1. **æå‡ç³»çµ±ååé‡** - æ™ºèƒ½æ‰¹æ¬¡è™•ç†æé«˜GPUåˆ©ç”¨ç‡\n",
    "2. **é™ä½æœå‹™å»¶é²** - å„ªå…ˆç´šèª¿åº¦ä¿è­‰é—œéµè«‹æ±‚éŸ¿æ‡‰æ™‚é–“\n",
    "3. **å¢å¼·ç³»çµ±ç©©å®šæ€§** - è² è¼‰å‡è¡¡å’Œæ•…éšœæ¢å¾©æé«˜å¯ç”¨æ€§\n",
    "4. **ç°¡åŒ–é‹ç¶­ç®¡ç†** - è‡ªå‹•åŒ–èª¿å„ªæ¸›å°‘äººå·¥å¹²é \n",
    "\n",
    "### ğŸ’¡ å­¸ç¿’è¦é»\n",
    "- å‹•æ…‹æ‰¹æ¬¡è™•ç†éœ€è¦å¹³è¡¡å»¶é²èˆ‡ååé‡\n",
    "- å„ªå…ˆç´šèª¿åº¦æ˜¯ä¼æ¥­ç´šæœå‹™çš„é—œéµç‰¹æ€§\n",
    "- è² è¼‰å‡è¡¡ç­–ç•¥çš„é¸æ“‡å½±éŸ¿æ•´é«”æ€§èƒ½\n",
    "- ç›£æ§å’Œå¯è§€æ¸¬æ€§æ˜¯å„ªåŒ–çš„åŸºç¤\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆ Lab 2.4.2ï¼**\n",
    "\n",
    "æ‚¨å·²ç¶“æŒæ¡äº†é«˜ç´šå‹•æ…‹æ‰¹æ¬¡è™•ç†æŠ€è¡“ï¼Œå¯ä»¥æ§‹å»ºé«˜æ•ˆã€å¯é çš„æ™ºèƒ½æ¨ç†èª¿åº¦ç³»çµ±ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}