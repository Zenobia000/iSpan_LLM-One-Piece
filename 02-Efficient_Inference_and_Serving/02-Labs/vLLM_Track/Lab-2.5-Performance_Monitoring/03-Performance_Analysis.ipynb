{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5-03: æ€§èƒ½åˆ†æèˆ‡è¨ºæ–·\n",
    "\n",
    "## å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬ç¯€å°‡æ·±å…¥åˆ†æ vLLM çš„æ€§èƒ½æ•¸æ“šï¼ŒåŒ…æ‹¬ï¼š\n",
    "- æ­·å²æ•¸æ“šæ·±åº¦åˆ†æ\n",
    "- æ€§èƒ½ç“¶é ¸è­˜åˆ¥èˆ‡è¨ºæ–·\n",
    "- è² è¼‰æ¸¬è©¦èˆ‡åŸºæº–è©•ä¼°\n",
    "- å®¹é‡è¦åŠƒèˆ‡æ“´å±•å»ºè­°\n",
    "- æ€§èƒ½å„ªåŒ–ç­–ç•¥åˆ¶å®š\n",
    "\n",
    "## åˆ†ææ¡†æ¶\n",
    "\n",
    "### 1. å¤šç¶­åº¦æ€§èƒ½åˆ†æ\n",
    "```\n",
    "æ€§èƒ½åˆ†æç¶­åº¦:\n",
    "â”œâ”€â”€ æ™‚é–“ç¶­åº¦ (Time-based)\n",
    "â”‚   â”œâ”€â”€ è¶¨å‹¢åˆ†æ (Trend Analysis)\n",
    "â”‚   â”œâ”€â”€ é€±æœŸæ€§æ¨¡å¼ (Periodic Patterns)\n",
    "â”‚   â””â”€â”€ çªç™¼äº‹ä»¶æª¢æ¸¬ (Spike Detection)\n",
    "â”œâ”€â”€ è³‡æºç¶­åº¦ (Resource-based)\n",
    "â”‚   â”œâ”€â”€ CPU/Memory/GPU åˆ©ç”¨ç‡\n",
    "â”‚   â”œâ”€â”€ ç“¶é ¸è³‡æºè­˜åˆ¥\n",
    "â”‚   â””â”€â”€ è³‡æºé…æ¯”å„ªåŒ–\n",
    "â””â”€â”€ æ¥­å‹™ç¶­åº¦ (Business-based)\n",
    "    â”œâ”€â”€ è«‹æ±‚è™•ç†æ•ˆç‡\n",
    "    â”œâ”€â”€ ç”¨æˆ¶é«”é©—æŒ‡æ¨™\n",
    "    â””â”€â”€ æˆæœ¬æ•ˆç›Šåˆ†æ\n",
    "```\n",
    "\n",
    "### 2. æ€§èƒ½æŒ‡æ¨™é«”ç³»\n",
    "- **éŸ¿æ‡‰æ€§æŒ‡æ¨™**: å»¶é²åˆ†ä½ˆã€TTFTã€TPOT\n",
    "- **ååé‡æŒ‡æ¨™**: QPSã€TPSã€ä½µç™¼è™•ç†èƒ½åŠ›\n",
    "- **å¯é æ€§æŒ‡æ¨™**: éŒ¯èª¤ç‡ã€å¯ç”¨æ€§ã€ç©©å®šæ€§\n",
    "- **æ•ˆç‡æŒ‡æ¨™**: è³‡æºåˆ©ç”¨ç‡ã€æˆæœ¬æ•ˆç›Šæ¯”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒåˆå§‹åŒ–èˆ‡æ•¸æ“šè¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# ç§‘å­¸è¨ˆç®—èˆ‡çµ±è¨ˆ\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# æ™‚é–“åºåˆ—åˆ†æ\n",
    "try:\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    STATSMODELS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    STATSMODELS_AVAILABLE = False\n",
    "    print(\"âš ï¸  statsmodels æœªå®‰è£ï¼Œéƒ¨åˆ†æ™‚é–“åºåˆ—åˆ†æåŠŸèƒ½ä¸å¯ç”¨\")\n",
    "\n",
    "# é€²éšè¦–è¦ºåŒ–\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# è¨­å®š\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "print(\"âœ… æ€§èƒ½åˆ†æç’°å¢ƒåˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceDataLoader:\n",
    "    \"\"\"æ€§èƒ½æ•¸æ“šè¼‰å…¥å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \".\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.monitoring_data = None\n",
    "        self.load_test_data = None\n",
    "    \n",
    "    def find_monitoring_files(self) -> List[Path]:\n",
    "        \"\"\"å°‹æ‰¾ç›£æ§æ•¸æ“šæ–‡ä»¶\"\"\"\n",
    "        json_files = list(self.data_dir.glob(\"vllm_monitoring_data_*.json\"))\n",
    "        csv_files = list(self.data_dir.glob(\"vllm_monitoring_data_*.csv\"))\n",
    "        \n",
    "        return sorted(json_files + csv_files, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    def load_monitoring_data(self, file_path: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"è¼‰å…¥ç›£æ§æ•¸æ“š\"\"\"\n",
    "        if file_path is None:\n",
    "            # è‡ªå‹•å°‹æ‰¾æœ€æ–°çš„æ•¸æ“šæ–‡ä»¶\n",
    "            files = self.find_monitoring_files()\n",
    "            if not files:\n",
    "                print(\"âŒ æœªæ‰¾åˆ°ç›£æ§æ•¸æ“šæ–‡ä»¶\")\n",
    "                return self.generate_sample_data()\n",
    "            file_path = files[0]\n",
    "            print(f\"ğŸ“ ä½¿ç”¨æ•¸æ“šæ–‡ä»¶: {file_path}\")\n",
    "        \n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if file_path.suffix == '.json':\n",
    "            return self._load_json_data(file_path)\n",
    "        elif file_path.suffix == '.csv':\n",
    "            return self._load_csv_data(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}\")\n",
    "    \n",
    "    def _load_json_data(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"è¼‰å…¥ JSON æ ¼å¼æ•¸æ“š\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # å»ºç«‹ DataFrame\n",
    "        df_data = {}\n",
    "        \n",
    "        # æ™‚é–“æˆ³\n",
    "        timestamps = [datetime.fromisoformat(ts) for ts in data['timestamps']]\n",
    "        df_data['timestamp'] = timestamps\n",
    "        \n",
    "        # ç³»çµ±æŒ‡æ¨™\n",
    "        for metric_name, values in data['system_metrics'].items():\n",
    "            # ç¢ºä¿é•·åº¦ä¸€è‡´\n",
    "            padded_values = values + [None] * (len(timestamps) - len(values))\n",
    "            df_data[metric_name] = padded_values[:len(timestamps)]\n",
    "        \n",
    "        # vLLM æŒ‡æ¨™\n",
    "        for metric_name, values in data['vllm_metrics'].items():\n",
    "            padded_values = values + [None] * (len(timestamps) - len(values))\n",
    "            df_data[metric_name] = padded_values[:len(timestamps)]\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        print(f\"âœ… è¼‰å…¥ JSON æ•¸æ“š: {len(df)} è¡Œ, {len(df.columns)} åˆ—\")\n",
    "        return df\n",
    "    \n",
    "    def _load_csv_data(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"è¼‰å…¥ CSV æ ¼å¼æ•¸æ“š\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        print(f\"âœ… è¼‰å…¥ CSV æ•¸æ“š: {len(df)} è¡Œ, {len(df.columns)} åˆ—\")\n",
    "        return df\n",
    "    \n",
    "    def generate_sample_data(self, duration_hours: int = 2, interval_seconds: int = 30) -> pd.DataFrame:\n",
    "        \"\"\"ç”Ÿæˆç¤ºä¾‹æ•¸æ“šç”¨æ–¼åˆ†æ\"\"\"\n",
    "        print(\"ğŸ”§ ç”Ÿæˆç¤ºä¾‹ç›£æ§æ•¸æ“š...\")\n",
    "        \n",
    "        # æ™‚é–“ç¯„åœ\n",
    "        end_time = datetime.now()\n",
    "        start_time = end_time - timedelta(hours=duration_hours)\n",
    "        timestamps = pd.date_range(start_time, end_time, freq=f'{interval_seconds}s')\n",
    "        \n",
    "        n_points = len(timestamps)\n",
    "        \n",
    "        # ç”ŸæˆåŸºç¤å™ªéŸ³\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # æ™‚é–“å› å­ (æ¨¡æ“¬æ—¥å¸¸è®ŠåŒ–)\n",
    "        time_factor = np.sin(np.linspace(0, 4*np.pi, n_points)) * 0.2 + 1\n",
    "        \n",
    "        data = {\n",
    "            # ç³»çµ±æŒ‡æ¨™\n",
    "            'cpu_percent': np.clip(\n",
    "                30 + 20 * time_factor + np.random.normal(0, 5, n_points), 0, 100\n",
    "            ),\n",
    "            'memory_percent': np.clip(\n",
    "                60 + 10 * time_factor + np.random.normal(0, 3, n_points), 0, 100\n",
    "            ),\n",
    "            'gpu_memory_used': np.clip(\n",
    "                45 + 25 * time_factor + np.random.normal(0, 8, n_points), 0, 100\n",
    "            ),\n",
    "            'gpu_utilization': np.clip(\n",
    "                40 + 30 * time_factor + np.random.normal(0, 10, n_points), 0, 100\n",
    "            ),\n",
    "            \n",
    "            # vLLM æŒ‡æ¨™ (æ¨¡æ“¬)\n",
    "            'vllm_num_requests_running': np.random.poisson(3 * time_factor, n_points),\n",
    "            'vllm_num_requests_waiting': np.random.poisson(1 * time_factor, n_points),\n",
    "            'vllm_request_success_total': np.cumsum(np.random.poisson(2 * time_factor, n_points)),\n",
    "            'vllm_request_failure_total': np.cumsum(np.random.poisson(0.1 * time_factor, n_points)),\n",
    "            \n",
    "            # å»¶é²æŒ‡æ¨™ (ç§’)\n",
    "            'vllm_time_to_first_token_seconds_sum': np.cumsum(\n",
    "                np.random.exponential(1.5 * time_factor, n_points)\n",
    "            ),\n",
    "            'vllm_time_to_first_token_seconds_count': np.cumsum(\n",
    "                np.random.poisson(1 * time_factor, n_points)\n",
    "            ),\n",
    "        }\n",
    "        \n",
    "        # æ·»åŠ ä¸€äº›çªç™¼äº‹ä»¶\n",
    "        spike_points = np.random.choice(n_points, size=5, replace=False)\n",
    "        for point in spike_points:\n",
    "            data['cpu_percent'][point] = min(95, data['cpu_percent'][point] + 30)\n",
    "            data['gpu_utilization'][point] = min(100, data['gpu_utilization'][point] + 40)\n",
    "        \n",
    "        df = pd.DataFrame(data, index=timestamps)\n",
    "        \n",
    "        print(f\"âœ… ç”Ÿæˆç¤ºä¾‹æ•¸æ“š: {len(df)} è¡Œ, {len(df.columns)} åˆ—\")\n",
    "        return df\n",
    "\n",
    "# åˆå§‹åŒ–æ•¸æ“šè¼‰å…¥å™¨\n",
    "data_loader = PerformanceDataLoader()\n",
    "print(\"âœ… æ•¸æ“šè¼‰å…¥å™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ç›£æ§æ•¸æ“š\n",
    "df = data_loader.load_monitoring_data()\n",
    "\n",
    "# æ•¸æ“šåŸºæœ¬è³‡è¨Š\n",
    "print(\"\\nğŸ“Š æ•¸æ“šé›†åŸºæœ¬è³‡è¨Š:\")\n",
    "print(f\"   æ™‚é–“ç¯„åœ: {df.index.min()} åˆ° {df.index.max()}\")\n",
    "print(f\"   æ•¸æ“šé»æ•¸: {len(df)}\")\n",
    "print(f\"   æ™‚é–“è·¨åº¦: {(df.index.max() - df.index.min()).total_seconds():.0f} ç§’\")\n",
    "print(f\"   å¹³å‡é–“éš”: {(df.index.max() - df.index.min()).total_seconds() / (len(df) - 1):.1f} ç§’\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ å¯ç”¨æŒ‡æ¨™:\")\n",
    "system_metrics = [col for col in df.columns if not col.startswith('vllm_')]\n",
    "vllm_metrics = [col for col in df.columns if col.startswith('vllm_')]\n",
    "\n",
    "print(f\"   ç³»çµ±æŒ‡æ¨™ ({len(system_metrics)}): {', '.join(system_metrics[:5])}{'...' if len(system_metrics) > 5 else ''}\")\n",
    "print(f\"   vLLM æŒ‡æ¨™ ({len(vllm_metrics)}): {', '.join(vllm_metrics[:5])}{'...' if len(vllm_metrics) > 5 else ''}\")\n",
    "\n",
    "# æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(\"\\nâš ï¸  æ•¸æ“šç¼ºå¤±æƒ…æ³:\")\n",
    "    for col, missing in missing_data[missing_data > 0].items():\n",
    "        print(f\"   {col}: {missing} å€‹ç¼ºå¤±å€¼ ({missing/len(df)*100:.1f}%)\")\nelse:\n    print(\"\\nâœ… æ•¸æ“šå®Œæ•´ï¼Œç„¡ç¼ºå¤±å€¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç¶œåˆæ€§èƒ½åˆ†æå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensivePerformanceAnalyzer:\n",
    "    \"\"\"ç¶œåˆæ€§èƒ½åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data.copy()\n",
    "        self.data = self.data.fillna(method='ffill').fillna(0)  # è™•ç†ç¼ºå¤±å€¼\n",
    "        \n",
    "        # åˆ†é¡æŒ‡æ¨™\n",
    "        self.system_metrics = [col for col in self.data.columns if not col.startswith('vllm_')]\n",
    "        self.vllm_metrics = [col for col in self.data.columns if col.startswith('vllm_')]\n",
    "        \n",
    "        # æ€§èƒ½åŸºæº–ç·š\n",
    "        self.performance_thresholds = {\n",
    "            'cpu_percent': {'good': 60, 'warning': 80, 'critical': 95},\n",
    "            'memory_percent': {'good': 70, 'warning': 85, 'critical': 95},\n",
    "            'gpu_memory_used': {'good': 70, 'warning': 85, 'critical': 95},\n",
    "            'gpu_utilization': {'good': 80, 'warning': 90, 'critical': 98}\n",
    "        }\n",
    "    \n",
    "    def calculate_derived_metrics(self):\n",
    "        \"\"\"è¨ˆç®—è¡ç”ŸæŒ‡æ¨™\"\"\"\n",
    "        print(\"ğŸ”§ è¨ˆç®—è¡ç”ŸæŒ‡æ¨™...\")\n",
    "        \n",
    "        # è«‹æ±‚æˆåŠŸç‡\n",
    "        if 'vllm_request_success_total' in self.data.columns and 'vllm_request_failure_total' in self.data.columns:\n",
    "            total_requests = self.data['vllm_request_success_total'] + self.data['vllm_request_failure_total']\n",
    "            self.data['success_rate'] = np.where(total_requests > 0, \n",
    "                                                self.data['vllm_request_success_total'] / total_requests * 100, 100)\n",
    "        \n",
    "        # å¹³å‡ TTFT\n",
    "        if ('vllm_time_to_first_token_seconds_sum' in self.data.columns and \n",
    "            'vllm_time_to_first_token_seconds_count' in self.data.columns):\n",
    "            count = self.data['vllm_time_to_first_token_seconds_count']\n",
    "            total_time = self.data['vllm_time_to_first_token_seconds_sum']\n",
    "            self.data['avg_ttft'] = np.where(count > 0, total_time / count, 0)\n",
    "        \n",
    "        # è«‹æ±‚è™•ç†é€Ÿç‡ (QPS)\n",
    "        if 'vllm_request_success_total' in self.data.columns:\n",
    "            self.data['qps'] = self.data['vllm_request_success_total'].diff().fillna(0) / (\n",
    "                self.data.index.to_series().diff().dt.total_seconds().fillna(1)\n",
    "            )\n",
    "            self.data['qps'] = self.data['qps'].clip(lower=0)  # ç§»é™¤è² å€¼\n",
    "        \n",
    "        # ç³»çµ±è² è¼‰æŒ‡æ•¸ (ç¶œåˆæŒ‡æ¨™)\n",
    "        load_components = []\n",
    "        if 'cpu_percent' in self.data.columns:\n",
    "            load_components.append(self.data['cpu_percent'] / 100)\n",
    "        if 'memory_percent' in self.data.columns:\n",
    "            load_components.append(self.data['memory_percent'] / 100)\n",
    "        if 'gpu_utilization' in self.data.columns:\n",
    "            load_components.append(self.data['gpu_utilization'] / 100)\n",
    "        \n",
    "        if load_components:\n",
    "            self.data['system_load_index'] = np.mean(load_components, axis=0) * 100\n",
    "        \n",
    "        print(f\"âœ… è¡ç”ŸæŒ‡æ¨™è¨ˆç®—å®Œæˆï¼Œæ–°å¢ {len([col for col in self.data.columns if col in ['success_rate', 'avg_ttft', 'qps', 'system_load_index']])} å€‹æŒ‡æ¨™\")\n",
    "    \n",
    "    def performance_summary_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"æ€§èƒ½æ‘˜è¦çµ±è¨ˆ\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        # ç³»çµ±è³‡æºçµ±è¨ˆ\n",
    "        for metric in self.system_metrics:\n",
    "            if metric in self.data.columns:\n",
    "                series = self.data[metric].dropna()\n",
    "                if len(series) > 0:\n",
    "                    stats[metric] = {\n",
    "                        'mean': series.mean(),\n",
    "                        'median': series.median(),\n",
    "                        'std': series.std(),\n",
    "                        'min': series.min(),\n",
    "                        'max': series.max(),\n",
    "                        'p95': series.quantile(0.95),\n",
    "                        'p99': series.quantile(0.99)\n",
    "                    }\n",
    "        \n",
    "        # vLLM æ€§èƒ½çµ±è¨ˆ\n",
    "        derived_metrics = ['success_rate', 'avg_ttft', 'qps', 'system_load_index']\n",
    "        for metric in derived_metrics:\n",
    "            if metric in self.data.columns:\n",
    "                series = self.data[metric].dropna()\n",
    "                if len(series) > 0:\n",
    "                    stats[metric] = {\n",
    "                        'mean': series.mean(),\n",
    "                        'median': series.median(),\n",
    "                        'std': series.std(),\n",
    "                        'min': series.min(),\n",
    "                        'max': series.max(),\n",
    "                        'p95': series.quantile(0.95),\n",
    "                        'p99': series.quantile(0.99)\n",
    "                    }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def identify_performance_bottlenecks(self) -> Dict[str, Any]:\n",
    "        \"\"\"è­˜åˆ¥æ€§èƒ½ç“¶é ¸\"\"\"\n",
    "        bottlenecks = {\n",
    "            'critical_periods': [],\n",
    "            'resource_constraints': [],\n",
    "            'performance_degradation': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # æª¢æŸ¥è³‡æºä½¿ç”¨è¶…éé–¾å€¼çš„æ™‚é–“æ®µ\n",
    "        for metric, thresholds in self.performance_thresholds.items():\n",
    "            if metric in self.data.columns:\n",
    "                series = self.data[metric]\n",
    "                \n",
    "                # æª¢æŸ¥è‡¨ç•ŒæœŸé–“\n",
    "                critical_mask = series > thresholds['critical']\n",
    "                warning_mask = series > thresholds['warning']\n",
    "                \n",
    "                if critical_mask.any():\n",
    "                    critical_periods = self._find_consecutive_periods(critical_mask)\n",
    "                    for start, end, duration in critical_periods:\n",
    "                        bottlenecks['critical_periods'].append({\n",
    "                            'metric': metric,\n",
    "                            'start_time': start,\n",
    "                            'end_time': end,\n",
    "                            'duration_seconds': duration,\n",
    "                            'max_value': series[start:end].max(),\n",
    "                            'severity': 'critical'\n",
    "                        })\n",
    "                \n",
    "                # è³‡æºç´„æŸåˆ†æ\n",
    "                high_usage_pct = (series > thresholds['warning']).mean() * 100\n",
    "                if high_usage_pct > 20:  # è¶…é 20% çš„æ™‚é–“è™•æ–¼é«˜ä½¿ç”¨ç‹€æ…‹\n",
    "                    bottlenecks['resource_constraints'].append({\n",
    "                        'metric': metric,\n",
    "                        'high_usage_percentage': high_usage_pct,\n",
    "                        'avg_usage': series.mean(),\n",
    "                        'p95_usage': series.quantile(0.95)\n",
    "                    })\n",
    "        \n",
    "        # æ€§èƒ½è¡°é€€æª¢æ¸¬\n",
    "        if 'qps' in self.data.columns and len(self.data) > 100:\n",
    "            qps_series = self.data['qps'].rolling(window=20).mean()\n",
    "            if len(qps_series.dropna()) > 50:\n",
    "                early_performance = qps_series.dropna().iloc[:25].mean()\n",
    "                late_performance = qps_series.dropna().iloc[-25:].mean()\n",
    "                \n",
    "                if early_performance > 0 and (late_performance / early_performance) < 0.8:\n",
    "                    bottlenecks['performance_degradation'].append({\n",
    "                        'metric': 'qps',\n",
    "                        'early_avg': early_performance,\n",
    "                        'late_avg': late_performance,\n",
    "                        'degradation_pct': (1 - late_performance / early_performance) * 100\n",
    "                    })\n",
    "        \n",
    "        # ç”Ÿæˆå»ºè­°\n",
    "        bottlenecks['recommendations'] = self._generate_recommendations(bottlenecks)\n",
    "        \n",
    "        return bottlenecks\n",
    "    \n",
    "    def _find_consecutive_periods(self, mask: pd.Series) -> List[Tuple]:\n",
    "        \"\"\"å°‹æ‰¾é€£çºŒçš„æ™‚é–“æ®µ\"\"\"\n",
    "        periods = []\n",
    "        start = None\n",
    "        \n",
    "        for i, (timestamp, value) in enumerate(mask.items()):\n",
    "            if value and start is None:\n",
    "                start = timestamp\n",
    "            elif not value and start is not None:\n",
    "                end = timestamp\n",
    "                duration = (end - start).total_seconds()\n",
    "                periods.append((start, end, duration))\n",
    "                start = None\n",
    "        \n",
    "        # è™•ç†çµå°¾çš„æƒ…æ³\n",
    "        if start is not None:\n",
    "            end = mask.index[-1]\n",
    "            duration = (end - start).total_seconds()\n",
    "            periods.append((start, end, duration))\n",
    "        \n",
    "        return periods\n",
    "    \n",
    "    def _generate_recommendations(self, bottlenecks: Dict) -> List[str]:\n",
    "        \"\"\"ç”Ÿæˆå„ªåŒ–å»ºè­°\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # åŸºæ–¼è³‡æºç´„æŸçš„å»ºè­°\n",
    "        for constraint in bottlenecks['resource_constraints']:\n",
    "            metric = constraint['metric']\n",
    "            if metric == 'cpu_percent':\n",
    "                recommendations.append(\"CPU ä½¿ç”¨ç‡éé«˜ï¼Œå»ºè­°å„ªåŒ–ç®—æ³•æˆ–å¢åŠ  CPU æ ¸å¿ƒæ•¸\")\n",
    "            elif metric == 'memory_percent':\n",
    "                recommendations.append(\"è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜ï¼Œå»ºè­°å¢åŠ è¨˜æ†¶é«”æˆ–å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨\")\n",
    "            elif metric == 'gpu_memory_used':\n",
    "                recommendations.append(\"GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜ï¼Œå»ºè­°èª¿æ•´æ‰¹æ¬¡å¤§å°æˆ–æ¨¡å‹åˆ†ç‰‡\")\n",
    "            elif metric == 'gpu_utilization':\n",
    "                recommendations.append(\"GPU ä½¿ç”¨ç‡éé«˜ï¼Œå»ºè­°å¢åŠ  GPU æ•¸é‡æˆ–å„ªåŒ–è¨ˆç®—\")\n",
    "        \n",
    "        # åŸºæ–¼æ€§èƒ½è¡°é€€çš„å»ºè­°\n",
    "        if bottlenecks['performance_degradation']:\n",
    "            recommendations.append(\"æª¢æ¸¬åˆ°æ€§èƒ½è¡°é€€ï¼Œå»ºè­°æª¢æŸ¥æ¨¡å‹å¿«å–å’Œè¨˜æ†¶é«”æ´©æ¼\")\n",
    "        \n",
    "        # åŸºæ–¼è‡¨ç•ŒæœŸé–“çš„å»ºè­°\n",
    "        if bottlenecks['critical_periods']:\n",
    "            recommendations.append(\"æª¢æ¸¬åˆ°è³‡æºä½¿ç”¨è‡¨ç•ŒæœŸé–“ï¼Œå»ºè­°è¨­ç½®è² è¼‰å‡è¡¡å’Œè‡ªå‹•æ“´å±•\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†æå™¨\n",
    "analyzer = ComprehensivePerformanceAnalyzer(df)\n",
    "\n",
    "# è¨ˆç®—è¡ç”ŸæŒ‡æ¨™\n",
    "analyzer.calculate_derived_metrics()\n",
    "\n",
    "print(\"âœ… ç¶œåˆæ€§èƒ½åˆ†æå™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è©³ç´°æ€§èƒ½çµ±è¨ˆåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç²å–æ€§èƒ½çµ±è¨ˆ\n",
    "performance_stats = analyzer.performance_summary_statistics()\n",
    "\n",
    "# é¡¯ç¤ºè©³ç´°çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š è©³ç´°æ€§èƒ½çµ±è¨ˆåˆ†æ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ç³»çµ±è³‡æºçµ±è¨ˆ\n",
    "print(\"\\nğŸ–¥ï¸  ç³»çµ±è³‡æºä½¿ç”¨çµ±è¨ˆ:\")\n",
    "print(f\"{'æŒ‡æ¨™':<20} {'å¹³å‡å€¼':<10} {'ä¸­ä½æ•¸':<10} {'P95':<10} {'P99':<10} {'æœ€å¤§å€¼':<10} {'æ¨™æº–å·®':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in ['cpu_percent', 'memory_percent', 'gpu_memory_used', 'gpu_utilization']:\n",
    "    if metric in performance_stats:\n",
    "        stats = performance_stats[metric]\n",
    "        print(f\"{metric:<20} {stats['mean']:<10.1f} {stats['median']:<10.1f} \"\n",
    "              f\"{stats['p95']:<10.1f} {stats['p99']:<10.1f} {stats['max']:<10.1f} {stats['std']:<10.1f}\")\n",
    "\n",
    "# æ€§èƒ½æŒ‡æ¨™çµ±è¨ˆ\n",
    "print(\"\\nğŸš€ vLLM æ€§èƒ½æŒ‡æ¨™çµ±è¨ˆ:\")\n",
    "performance_metrics = ['success_rate', 'avg_ttft', 'qps', 'system_load_index']\n",
    "\n",
    "for metric in performance_metrics:\n",
    "    if metric in performance_stats:\n",
    "        stats = performance_stats[metric]\n",
    "        unit = '%' if metric in ['success_rate', 'system_load_index'] else ('s' if 'ttft' in metric else 'req/s')\n",
    "        print(f\"\\n   {metric}:\")\n",
    "        print(f\"     å¹³å‡å€¼: {stats['mean']:.3f} {unit}\")\n",
    "        print(f\"     ä¸­ä½æ•¸: {stats['median']:.3f} {unit}\")\n",
    "        print(f\"     P95: {stats['p95']:.3f} {unit}\")\n",
    "        print(f\"     P99: {stats['p99']:.3f} {unit}\")\n",
    "        print(f\"     ç¯„åœ: {stats['min']:.3f} - {stats['max']:.3f} {unit}\")\n",
    "        print(f\"     è®Šç•°ä¿‚æ•¸: {(stats['std']/stats['mean']*100):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ€§èƒ½ç“¶é ¸è­˜åˆ¥èˆ‡è¨ºæ–·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œç“¶é ¸åˆ†æ\n",
    "bottlenecks = analyzer.identify_performance_bottlenecks()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” æ€§èƒ½ç“¶é ¸è¨ºæ–·å ±å‘Š\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# è‡¨ç•ŒæœŸé–“åˆ†æ\n",
    "if bottlenecks['critical_periods']:\n",
    "    print(\"\\nâš ï¸  æª¢æ¸¬åˆ°è‡¨ç•Œæ€§èƒ½æœŸé–“:\")\n",
    "    for i, period in enumerate(bottlenecks['critical_periods'], 1):\n",
    "        print(f\"\\n   æœŸé–“ {i}:\")\n",
    "        print(f\"     æŒ‡æ¨™: {period['metric']}\")\n",
    "        print(f\"     æ™‚é–“: {period['start_time'].strftime('%H:%M:%S')} - {period['end_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"     æŒçºŒæ™‚é–“: {period['duration_seconds']:.0f} ç§’\")\n",
    "        print(f\"     æœ€å¤§å€¼: {period['max_value']:.1f}%\")\n",
    "        print(f\"     åš´é‡ç¨‹åº¦: {period['severity']}\")\nelse:\n    print(\"\\nâœ… æœªæª¢æ¸¬åˆ°è‡¨ç•Œæ€§èƒ½æœŸé–“\")\n",
    "\n",
    "# è³‡æºç´„æŸåˆ†æ\n",
    "if bottlenecks['resource_constraints']:\n",
    "    print(\"\\nğŸ“ˆ è³‡æºç´„æŸåˆ†æ:\")\n",
    "    for constraint in bottlenecks['resource_constraints']:\n",
    "        print(f\"\\n   {constraint['metric']}:\")\n",
    "        print(f\"     é«˜ä½¿ç”¨ç‡æ™‚é–“ä½”æ¯”: {constraint['high_usage_percentage']:.1f}%\")\n",
    "        print(f\"     å¹³å‡ä½¿ç”¨ç‡: {constraint['avg_usage']:.1f}%\")\n",
    "        print(f\"     P95 ä½¿ç”¨ç‡: {constraint['p95_usage']:.1f}%\")\nelse:\n    print(\"\\nâœ… æœªç™¼ç¾é¡¯è‘—çš„è³‡æºç´„æŸå•é¡Œ\")\n",
    "\n",
    "# æ€§èƒ½è¡°é€€åˆ†æ\n",
    "if bottlenecks['performance_degradation']:\n",
    "    print(\"\\nğŸ“‰ æ€§èƒ½è¡°é€€æª¢æ¸¬:\")\n",
    "    for degradation in bottlenecks['performance_degradation']:\n",
    "        print(f\"\\n   {degradation['metric']}:\")\n",
    "        print(f\"     åˆæœŸæ€§èƒ½: {degradation['early_avg']:.2f}\")\n",
    "        print(f\"     å¾ŒæœŸæ€§èƒ½: {degradation['late_avg']:.2f}\")\n",
    "        print(f\"     è¡°é€€å¹…åº¦: {degradation['degradation_pct']:.1f}%\")\nelse:\n    print(\"\\nâœ… æœªæª¢æ¸¬åˆ°é¡¯è‘—çš„æ€§èƒ½è¡°é€€\")\n",
    "\n",
    "# å„ªåŒ–å»ºè­°\n",
    "if bottlenecks['recommendations']:\n",
    "    print(\"\\nğŸ’¡ å„ªåŒ–å»ºè­°:\")\n",
    "    for i, recommendation in enumerate(bottlenecks['recommendations'], 1):\n",
    "        print(f\"   {i}. {recommendation}\")\n",
    "else:\n",
    "    print(\"\\nâœ… ç³»çµ±é‹è¡Œè‰¯å¥½ï¼Œæš«ç„¡ç‰¹æ®Šå„ªåŒ–å»ºè­°\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. é€²éšæ™‚é–“åºåˆ—åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesAnalyzer:\n",
    "    \"\"\"æ™‚é–“åºåˆ—åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "    \n",
    "    def trend_analysis(self, metric: str, window_size: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"è¶¨å‹¢åˆ†æ\"\"\"\n",
    "        if metric not in self.data.columns:\n",
    "            return {}\n",
    "        \n",
    "        series = self.data[metric].dropna()\n",
    "        if len(series) < window_size:\n",
    "            return {}\n",
    "        \n",
    "        # è¨ˆç®—ç§»å‹•å¹³å‡\n",
    "        rolling_mean = series.rolling(window=window_size).mean()\n",
    "        \n",
    "        # ç·šæ€§è¶¨å‹¢æ“¬åˆ\n",
    "        x = np.arange(len(series))\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, series)\n",
    "        \n",
    "        # è¶¨å‹¢å¼·åº¦åˆ¤æ–·\n",
    "        trend_strength = abs(r_value)\n",
    "        if trend_strength > 0.7:\n",
    "            trend_type = \"å¼·\" + (\"ä¸Šå‡\" if slope > 0 else \"ä¸‹é™\")\n",
    "        elif trend_strength > 0.3:\n",
    "            trend_type = \"ä¸­ç­‰\" + (\"ä¸Šå‡\" if slope > 0 else \"ä¸‹é™\")\n",
    "        else:\n",
    "            trend_type = \"ç„¡æ˜é¡¯è¶¨å‹¢\"\n",
    "        \n",
    "        return {\n",
    "            'slope': slope,\n",
    "            'r_squared': r_value**2,\n",
    "            'p_value': p_value,\n",
    "            'trend_type': trend_type,\n",
    "            'rolling_mean': rolling_mean,\n",
    "            'trend_line': slope * x + intercept\n",
    "        }\n",
    "    \n",
    "    def seasonality_detection(self, metric: str, period: int = None) -> Dict[str, Any]:\n",
    "        \"\"\"å­£ç¯€æ€§æª¢æ¸¬\"\"\"\n",
    "        if not STATSMODELS_AVAILABLE or metric not in self.data.columns:\n",
    "            return {}\n",
    "        \n",
    "        series = self.data[metric].dropna()\n",
    "        if len(series) < 100:  # éœ€è¦è¶³å¤ çš„æ•¸æ“šé»\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # è‡ªå‹•æª¢æ¸¬é€±æœŸ\n",
    "            if period is None:\n",
    "                # å‡è¨­æ•¸æ“šé–“éš”ç‚ºåˆ†é˜ç´šï¼Œå˜—è©¦å°æ™‚é€±æœŸ\n",
    "                period = min(60, len(series) // 4)\n",
    "            \n",
    "            # å­£ç¯€åˆ†è§£\n",
    "            decomposition = seasonal_decompose(\n",
    "                series, \n",
    "                model='additive', \n",
    "                period=period,\n",
    "                extrapolate_trend='freq'\n",
    "            )\n",
    "            \n",
    "            # å­£ç¯€æ€§å¼·åº¦\n",
    "            seasonal_strength = np.std(decomposition.seasonal) / np.std(series)\n",
    "            \n",
    "            return {\n",
    "                'has_seasonality': seasonal_strength > 0.1,\n",
    "                'seasonal_strength': seasonal_strength,\n",
    "                'period': period,\n",
    "                'trend': decomposition.trend,\n",
    "                'seasonal': decomposition.seasonal,\n",
    "                'residual': decomposition.resid\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"å­£ç¯€æ€§åˆ†æå¤±æ•—: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def anomaly_detection_advanced(self, metric: str, contamination: float = 0.1) -> Dict[str, Any]:\n",
    "        \"\"\"é€²éšç•°å¸¸æª¢æ¸¬\"\"\"\n",
    "        if metric not in self.data.columns:\n",
    "            return {}\n",
    "        \n",
    "        series = self.data[metric].dropna()\n",
    "        if len(series) < 20:\n",
    "            return {}\n",
    "        \n",
    "        # æº–å‚™æ•¸æ“š\n",
    "        X = series.values.reshape(-1, 1)\n",
    "        \n",
    "        # Isolation Forest ç•°å¸¸æª¢æ¸¬\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        \n",
    "        anomaly_labels = iso_forest.fit_predict(X)\n",
    "        anomaly_scores = iso_forest.decision_function(X)\n",
    "        \n",
    "        # çµ±è¨ˆå­¸ç•°å¸¸æª¢æ¸¬ (Z-score)\n",
    "        z_scores = np.abs(stats.zscore(series))\n",
    "        z_anomalies = z_scores > 3\n",
    "        \n",
    "        # ç¶œåˆç•°å¸¸æª¢æ¸¬çµæœ\n",
    "        anomalies = (anomaly_labels == -1) | z_anomalies\n",
    "        \n",
    "        anomaly_indices = series.index[anomalies]\n",
    "        anomaly_values = series[anomalies]\n",
    "        \n",
    "        return {\n",
    "            'anomaly_count': anomalies.sum(),\n",
    "            'anomaly_rate': anomalies.mean(),\n",
    "            'anomaly_timestamps': anomaly_indices,\n",
    "            'anomaly_values': anomaly_values,\n",
    "            'anomaly_scores': anomaly_scores,\n",
    "            'z_scores': z_scores\n",
    "        }\n",
    "    \n",
    "    def correlation_analysis(self, metrics: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"ç›¸é—œæ€§åˆ†æ\"\"\"\n",
    "        available_metrics = [m for m in metrics if m in self.data.columns]\n",
    "        \n",
    "        if len(available_metrics) < 2:\n",
    "            return {}\n",
    "        \n",
    "        # è¨ˆç®—ç›¸é—œæ€§çŸ©é™£\n",
    "        correlation_matrix = self.data[available_metrics].corr()\n",
    "        \n",
    "        # å°‹æ‰¾å¼·ç›¸é—œé—œä¿‚\n",
    "        strong_correlations = []\n",
    "        for i, metric1 in enumerate(available_metrics):\n",
    "            for j, metric2 in enumerate(available_metrics[i+1:], i+1):\n",
    "                corr_value = correlation_matrix.loc[metric1, metric2]\n",
    "                if abs(corr_value) > 0.7:  # å¼·ç›¸é—œé–¾å€¼\n",
    "                    strong_correlations.append({\n",
    "                        'metric1': metric1,\n",
    "                        'metric2': metric2,\n",
    "                        'correlation': corr_value,\n",
    "                        'strength': 'strong'\n",
    "                    })\n",
    "                elif abs(corr_value) > 0.4:  # ä¸­ç­‰ç›¸é—œé–¾å€¼\n",
    "                    strong_correlations.append({\n",
    "                        'metric1': metric1,\n",
    "                        'metric2': metric2,\n",
    "                        'correlation': corr_value,\n",
    "                        'strength': 'moderate'\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'correlation_matrix': correlation_matrix,\n",
    "            'strong_correlations': strong_correlations\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–æ™‚é–“åºåˆ—åˆ†æå™¨\n",
    "ts_analyzer = TimeSeriesAnalyzer(analyzer.data)\n",
    "print(\"âœ… æ™‚é–“åºåˆ—åˆ†æå™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œæ™‚é–“åºåˆ—åˆ†æ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ æ™‚é–“åºåˆ—æ·±åº¦åˆ†æ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# é¸æ“‡é—œéµæŒ‡æ¨™é€²è¡Œåˆ†æ\n",
    "key_metrics = ['cpu_percent', 'memory_percent', 'gpu_utilization', 'system_load_index']\n",
    "available_metrics = [m for m in key_metrics if m in analyzer.data.columns]\n",
    "\n",
    "# è¶¨å‹¢åˆ†æ\n",
    "print(\"\\nğŸ“Š è¶¨å‹¢åˆ†æçµæœ:\")\n",
    "for metric in available_metrics:\n",
    "    trend_result = ts_analyzer.trend_analysis(metric)\n",
    "    if trend_result:\n",
    "        print(f\"\\n   {metric}:\")\n",
    "        print(f\"     è¶¨å‹¢é¡å‹: {trend_result['trend_type']}\")\n",
    "        print(f\"     è¶¨å‹¢æ–œç‡: {trend_result['slope']:.4f} å–®ä½/æ™‚é–“é»\")\n",
    "        print(f\"     æ±ºå®šä¿‚æ•¸ (RÂ²): {trend_result['r_squared']:.3f}\")\n",
    "        print(f\"     çµ±è¨ˆé¡¯è‘—æ€§ (på€¼): {trend_result['p_value']:.4f}\")\n",
    "\n",
    "# ç•°å¸¸æª¢æ¸¬\n",
    "print(\"\\nğŸ” é€²éšç•°å¸¸æª¢æ¸¬:\")\n",
    "total_anomalies = 0\n",
    "for metric in available_metrics:\n",
    "    anomaly_result = ts_analyzer.anomaly_detection_advanced(metric)\n",
    "    if anomaly_result:\n",
    "        count = anomaly_result['anomaly_count']\n",
    "        rate = anomaly_result['anomaly_rate'] * 100\n",
    "        total_anomalies += count\n",
    "        \n",
    "        print(f\"\\n   {metric}:\")\n",
    "        print(f\"     ç•°å¸¸é»æ•¸é‡: {count}\")\n",
    "        print(f\"     ç•°å¸¸ç‡: {rate:.1f}%\")\n",
    "        \n",
    "        if count > 0:\n",
    "            worst_anomaly_idx = np.argmax(np.abs(anomaly_result['anomaly_scores']))\n",
    "            worst_timestamp = anomaly_result['anomaly_timestamps'][worst_anomaly_idx]\n",
    "            worst_value = anomaly_result['anomaly_values'].iloc[worst_anomaly_idx]\n",
    "            print(f\"     æœ€åš´é‡ç•°å¸¸: {worst_timestamp.strftime('%H:%M:%S')} (å€¼: {worst_value:.1f})\")\n",
    "\n",
    "print(f\"\\n   ç¸½ç•°å¸¸é»æ•¸: {total_anomalies}\")\n",
    "\n",
    "# ç›¸é—œæ€§åˆ†æ\n",
    "correlation_result = ts_analyzer.correlation_analysis(available_metrics)\n",
    "if correlation_result and correlation_result['strong_correlations']:\n",
    "    print(\"\\nğŸ”— å¼·ç›¸é—œé—œä¿‚åˆ†æ:\")\n",
    "    for corr in correlation_result['strong_correlations']:\n",
    "        print(f\"   {corr['metric1']} â†” {corr['metric2']}: {corr['correlation']:.3f} ({corr['strength']})\")\nelse:\n    print(\"\\n   æœªç™¼ç¾é¡¯è‘—çš„å¼·ç›¸é—œé—œä¿‚\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. é«˜ç´šæ€§èƒ½è¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_performance_dashboard():\n",
    "    \"\"\"å‰µå»ºç¶œåˆæ€§èƒ½åˆ†æå„€è¡¨æ¿\"\"\"\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20, 16))\n",
    "    fig.suptitle('vLLM ç¶œåˆæ€§èƒ½åˆ†æå„€è¡¨æ¿', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. ç³»çµ±è³‡æºä½¿ç”¨è¶¨å‹¢\n",
    "    ax1 = axes[0, 0]\n",
    "    for metric in ['cpu_percent', 'memory_percent', 'gpu_utilization']:\n",
    "        if metric in analyzer.data.columns:\n",
    "            ax1.plot(analyzer.data.index, analyzer.data[metric], \n",
    "                    label=metric.replace('_', ' ').title(), alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax1.set_title('ç³»çµ±è³‡æºä½¿ç”¨è¶¨å‹¢', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # 2. æ€§èƒ½æŒ‡æ¨™åˆ†ä½ˆ\n",
    "    ax2 = axes[0, 1]\n",
    "    performance_metrics = ['cpu_percent', 'memory_percent', 'gpu_utilization']\n",
    "    available_perf_metrics = [m for m in performance_metrics if m in analyzer.data.columns]\n",
    "    \n",
    "    if available_perf_metrics:\n",
    "        data_for_box = [analyzer.data[metric].dropna() for metric in available_perf_metrics]\n",
    "        labels = [m.replace('_', ' ').title() for m in available_perf_metrics]\n",
    "        \n",
    "        box_plot = ax2.boxplot(data_for_box, labels=labels, patch_artist=True)\n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "        for patch, color in zip(box_plot['boxes'], colors[:len(box_plot['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "    \n",
    "    ax2.set_title('æ€§èƒ½æŒ‡æ¨™åˆ†ä½ˆ', fontweight='bold', fontsize=14)\n",
    "    ax2.set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. vLLM è«‹æ±‚è™•ç†æ€§èƒ½\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'qps' in analyzer.data.columns:\n",
    "        qps_data = analyzer.data['qps'].rolling(window=10).mean()\n",
    "        ax3.plot(analyzer.data.index, qps_data, color='purple', linewidth=2, label='QPS (10-point avg)')\n",
    "        ax3.fill_between(analyzer.data.index, qps_data, alpha=0.3, color='purple')\n",
    "    \n",
    "    if 'vllm_num_requests_running' in analyzer.data.columns:\n",
    "        ax3_twin = ax3.twinx()\n",
    "        ax3_twin.plot(analyzer.data.index, analyzer.data['vllm_num_requests_running'], \n",
    "                     color='orange', linewidth=2, alpha=0.7, label='Running Requests')\n",
    "        ax3_twin.set_ylabel('Active Requests', color='orange')\n",
    "        ax3_twin.tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    ax3.set_title('vLLM è«‹æ±‚è™•ç†æ€§èƒ½', fontweight='bold', fontsize=14)\n",
    "    ax3.set_ylabel('QPS')\n",
    "    ax3.legend(loc='upper left')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ç³»çµ±è² è¼‰æŒ‡æ•¸èˆ‡ç•°å¸¸é»\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'system_load_index' in analyzer.data.columns:\n",
    "        load_data = analyzer.data['system_load_index']\n",
    "        ax4.plot(analyzer.data.index, load_data, color='red', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # æ¨™è¨˜ç•°å¸¸é»\n",
    "        anomaly_result = ts_analyzer.anomaly_detection_advanced('system_load_index')\n",
    "        if anomaly_result and len(anomaly_result['anomaly_timestamps']) > 0:\n",
    "            ax4.scatter(anomaly_result['anomaly_timestamps'], \n",
    "                       anomaly_result['anomaly_values'],\n",
    "                       color='red', s=50, alpha=0.8, marker='x', label=f\"ç•°å¸¸é» ({len(anomaly_result['anomaly_timestamps'])})\")\n",
    "            ax4.legend()\n",
    "        \n",
    "        # æ·»åŠ è­¦å‘Šç·š\n",
    "        ax4.axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='è­¦å‘Šç·š (80%)')\n",
    "        ax4.axhline(y=95, color='red', linestyle='--', alpha=0.7, label='è‡¨ç•Œç·š (95%)')\n",
    "    \n",
    "    ax4.set_title('ç³»çµ±è² è¼‰æŒ‡æ•¸èˆ‡ç•°å¸¸æª¢æ¸¬', fontweight='bold', fontsize=14)\n",
    "    ax4.set_ylabel('è² è¼‰æŒ‡æ•¸ (%)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. ç›¸é—œæ€§ç†±åŠ›åœ–\n",
    "    ax5 = axes[2, 0]\n",
    "    correlation_metrics = ['cpu_percent', 'memory_percent', 'gpu_utilization', 'gpu_memory_used']\n",
    "    available_corr_metrics = [m for m in correlation_metrics if m in analyzer.data.columns]\n",
    "    \n",
    "    if len(available_corr_metrics) > 1:\n",
    "        corr_matrix = analyzer.data[available_corr_metrics].corr()\n",
    "        im = ax5.imshow(corr_matrix, cmap='RdYlBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "        \n",
    "        # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "        for i in range(len(available_corr_metrics)):\n",
    "            for j in range(len(available_corr_metrics)):\n",
    "                text = ax5.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        ax5.set_xticks(range(len(available_corr_metrics)))\n",
    "        ax5.set_yticks(range(len(available_corr_metrics)))\n",
    "        ax5.set_xticklabels([m.replace('_', '\\n') for m in available_corr_metrics], rotation=45)\n",
    "        ax5.set_yticklabels([m.replace('_', '\\n') for m in available_corr_metrics])\n",
    "        \n",
    "        # æ·»åŠ é¡è‰²æ¢\n",
    "        cbar = plt.colorbar(im, ax=ax5, shrink=0.8)\n",
    "        cbar.set_label('ç›¸é—œä¿‚æ•¸', rotation=270, labelpad=20)\n",
    "    \n",
    "    ax5.set_title('æŒ‡æ¨™ç›¸é—œæ€§åˆ†æ', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # 6. æ€§èƒ½æ‘˜è¦é›·é”åœ–\n",
    "    ax6 = axes[2, 1]\n",
    "    \n",
    "    # è¨ˆç®—æ€§èƒ½è©•åˆ† (0-100)\n",
    "    scores = {}\n",
    "    labels = []\n",
    "    values = []\n",
    "    \n",
    "    if 'success_rate' in analyzer.data.columns:\n",
    "        scores['Success Rate'] = analyzer.data['success_rate'].mean()\n",
    "    \n",
    "    # CPU æ•ˆç‡ (100 - å¹³å‡ä½¿ç”¨ç‡)\n",
    "    if 'cpu_percent' in analyzer.data.columns:\n",
    "        scores['CPU Efficiency'] = max(0, 100 - analyzer.data['cpu_percent'].mean())\n",
    "    \n",
    "    # Memory æ•ˆç‡\n",
    "    if 'memory_percent' in analyzer.data.columns:\n",
    "        scores['Memory Efficiency'] = max(0, 100 - analyzer.data['memory_percent'].mean())\n",
    "    \n",
    "    # GPU æ•ˆç‡\n",
    "    if 'gpu_utilization' in analyzer.data.columns:\n",
    "        scores['GPU Efficiency'] = max(0, 100 - analyzer.data['gpu_utilization'].mean())\n",
    "    \n",
    "    # ç©©å®šæ€§ (100 - è®Šç•°ä¿‚æ•¸)\n",
    "    if 'system_load_index' in analyzer.data.columns:\n",
    "        cv = analyzer.data['system_load_index'].std() / analyzer.data['system_load_index'].mean()\n",
    "        scores['Stability'] = max(0, 100 - cv * 100)\n",
    "    \n",
    "    if scores:\n",
    "        labels = list(scores.keys())\n",
    "        values = list(scores.values())\n",
    "        \n",
    "        # é›·é”åœ–\n",
    "        angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "        values += values[:1]  # é–‰åˆåœ–å½¢\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        ax6.plot(angles, values, 'o-', linewidth=2, label='Performance Score')\n",
    "        ax6.fill(angles, values, alpha=0.25)\n",
    "        ax6.set_xticks(angles[:-1])\n",
    "        ax6.set_xticklabels(labels)\n",
    "        ax6.set_ylim(0, 100)\n",
    "        ax6.grid(True)\n",
    "        \n",
    "        # æ·»åŠ åˆ†æ•¸æ¨™ç±¤\n",
    "        for angle, value, label in zip(angles[:-1], values[:-1], labels):\n",
    "            ax6.text(angle, value + 5, f'{value:.0f}', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax6.set_title('æ€§èƒ½è©•åˆ†é›·é”åœ–', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ç”Ÿæˆç¶œåˆæ€§èƒ½å„€è¡¨æ¿\n",
    "print(\"ğŸ“Š ç”Ÿæˆç¶œåˆæ€§èƒ½åˆ†æå„€è¡¨æ¿...\")\n",
    "create_comprehensive_performance_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. è² è¼‰æ¸¬è©¦èˆ‡åŸºæº–è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTestAnalyzer:\n",
    "    \"\"\"è² è¼‰æ¸¬è©¦åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        self.baseline_metrics = self._calculate_baseline()\n",
    "    \n",
    "    def _calculate_baseline(self) -> Dict[str, float]:\n",
    "        \"\"\"è¨ˆç®—åŸºæº–ç·šæ€§èƒ½æŒ‡æ¨™\"\"\"\n",
    "        baseline = {}\n",
    "        \n",
    "        # å–å‰ 25% çš„æ•¸æ“šä½œç‚ºåŸºæº–ç·š\n",
    "        baseline_data = self.data.iloc[:len(self.data)//4]\n",
    "        \n",
    "        for col in self.data.columns:\n",
    "            if baseline_data[col].dtype in ['int64', 'float64']:\n",
    "                baseline[col] = baseline_data[col].mean()\n",
    "        \n",
    "        return baseline\n",
    "    \n",
    "    def analyze_load_patterns(self) -> Dict[str, Any]:\n",
    "        \"\"\"åˆ†æè² è¼‰æ¨¡å¼\"\"\"\n",
    "        patterns = {\n",
    "            'peak_periods': [],\n",
    "            'low_periods': [],\n",
    "            'load_distribution': {},\n",
    "            'performance_under_load': {}\n",
    "        }\n",
    "        \n",
    "        if 'system_load_index' in self.data.columns:\n",
    "            load_series = self.data['system_load_index']\n",
    "            \n",
    "            # è­˜åˆ¥é«˜è² è¼‰å’Œä½è² è¼‰æœŸé–“\n",
    "            high_load_threshold = load_series.quantile(0.8)\n",
    "            low_load_threshold = load_series.quantile(0.2)\n",
    "            \n",
    "            high_load_mask = load_series > high_load_threshold\n",
    "            low_load_mask = load_series < low_load_threshold\n",
    "            \n",
    "            # å°‹æ‰¾é€£çºŒçš„é«˜è² è¼‰æœŸé–“\n",
    "            patterns['peak_periods'] = self._find_consecutive_periods(high_load_mask)\n",
    "            patterns['low_periods'] = self._find_consecutive_periods(low_load_mask)\n",
    "            \n",
    "            # è² è¼‰åˆ†ä½ˆåˆ†æ\n",
    "            patterns['load_distribution'] = {\n",
    "                'mean': load_series.mean(),\n",
    "                'median': load_series.median(),\n",
    "                'std': load_series.std(),\n",
    "                'min': load_series.min(),\n",
    "                'max': load_series.max(),\n",
    "                'q25': load_series.quantile(0.25),\n",
    "                'q75': load_series.quantile(0.75),\n",
    "                'high_load_percentage': (high_load_mask.sum() / len(load_series)) * 100,\n",
    "                'low_load_percentage': (low_load_mask.sum() / len(load_series)) * 100\n",
    "            }\n",
    "            \n",
    "            # ä¸åŒè² è¼‰ä¸‹çš„æ€§èƒ½åˆ†æ\n",
    "            high_load_data = self.data[high_load_mask]\n",
    "            low_load_data = self.data[low_load_mask]\n",
    "            medium_load_data = self.data[~(high_load_mask | low_load_mask)]\n",
    "            \n",
    "            for load_type, data_subset in [('high', high_load_data), ('medium', medium_load_data), ('low', low_load_data)]:\n",
    "                if len(data_subset) > 0:\n",
    "                    patterns['performance_under_load'][load_type] = {\n",
    "                        'avg_cpu': data_subset['cpu_percent'].mean() if 'cpu_percent' in data_subset.columns else 0,\n",
    "                        'avg_memory': data_subset['memory_percent'].mean() if 'memory_percent' in data_subset.columns else 0,\n",
    "                        'avg_gpu': data_subset['gpu_utilization'].mean() if 'gpu_utilization' in data_subset.columns else 0,\n",
    "                        'avg_qps': data_subset['qps'].mean() if 'qps' in data_subset.columns else 0,\n",
    "                        'data_points': len(data_subset)\n",
    "                    }\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _find_consecutive_periods(self, mask: pd.Series) -> List[Dict]:\n",
    "        \"\"\"å°‹æ‰¾é€£çºŒçš„æ™‚é–“æ®µ\"\"\"\n",
    "        periods = []\n",
    "        start = None\n",
    "        \n",
    "        for timestamp, value in mask.items():\n",
    "            if value and start is None:\n",
    "                start = timestamp\n",
    "            elif not value and start is not None:\n",
    "                end = timestamp\n",
    "                duration = (end - start).total_seconds()\n",
    "                periods.append({\n",
    "                    'start': start,\n",
    "                    'end': end,\n",
    "                    'duration_seconds': duration\n",
    "                })\n",
    "                start = None\n",
    "        \n",
    "        # è™•ç†çµå°¾çš„æƒ…æ³\n",
    "        if start is not None:\n",
    "            end = mask.index[-1]\n",
    "            duration = (end - start).total_seconds()\n",
    "            periods.append({\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'duration_seconds': duration\n",
    "            })\n",
    "        \n",
    "        return periods\n",
    "    \n",
    "    def capacity_planning_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"å®¹é‡è¦åŠƒåˆ†æ\"\"\"\n",
    "        capacity_analysis = {\n",
    "            'current_utilization': {},\n",
    "            'bottleneck_analysis': {},\n",
    "            'scaling_recommendations': []\n",
    "        }\n",
    "        \n",
    "        # ç•¶å‰è³‡æºä½¿ç”¨æƒ…æ³\n",
    "        resource_metrics = ['cpu_percent', 'memory_percent', 'gpu_utilization', 'gpu_memory_used']\n",
    "        \n",
    "        for metric in resource_metrics:\n",
    "            if metric in self.data.columns:\n",
    "                series = self.data[metric]\n",
    "                capacity_analysis['current_utilization'][metric] = {\n",
    "                    'average': series.mean(),\n",
    "                    'peak': series.max(),\n",
    "                    'p95': series.quantile(0.95),\n",
    "                    'headroom': 100 - series.quantile(0.95),  # å‰©é¤˜å®¹é‡\n",
    "                    'utilization_trend': 'increasing' if series.tail(20).mean() > series.head(20).mean() else 'stable'\n",
    "                }\n",
    "        \n",
    "        # ç“¶é ¸åˆ†æ\n",
    "        bottlenecks = []\n",
    "        for metric, utilization in capacity_analysis['current_utilization'].items():\n",
    "            if utilization['p95'] > 90:\n",
    "                bottlenecks.append({\n",
    "                    'resource': metric,\n",
    "                    'severity': 'critical',\n",
    "                    'p95_usage': utilization['p95'],\n",
    "                    'headroom': utilization['headroom']\n",
    "                })\n",
    "            elif utilization['p95'] > 80:\n",
    "                bottlenecks.append({\n",
    "                    'resource': metric,\n",
    "                    'severity': 'warning',\n",
    "                    'p95_usage': utilization['p95'],\n",
    "                    'headroom': utilization['headroom']\n",
    "                })\n",
    "        \n",
    "        capacity_analysis['bottleneck_analysis'] = bottlenecks\n",
    "        \n",
    "        # æ“´å±•å»ºè­°\n",
    "        recommendations = []\n",
    "        \n",
    "        for bottleneck in bottlenecks:\n",
    "            resource = bottleneck['resource']\n",
    "            if resource == 'cpu_percent':\n",
    "                if bottleneck['severity'] == 'critical':\n",
    "                    recommendations.append(\"ç«‹å³å¢åŠ  CPU æ ¸å¿ƒæ•¸æˆ–å„ªåŒ– CPU å¯†é›†å‹é‹ç®—\")\n",
    "                else:\n",
    "                    recommendations.append(\"è€ƒæ…®åœ¨æœªä¾† 1-2 å€‹æœˆå…§å¢åŠ  CPU è³‡æº\")\n",
    "            \n",
    "            elif resource == 'memory_percent':\n",
    "                if bottleneck['severity'] == 'critical':\n",
    "                    recommendations.append(\"ç«‹å³å¢åŠ è¨˜æ†¶é«”å®¹é‡æˆ–å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨\")\n",
    "                else:\n",
    "                    recommendations.append(\"è¨ˆåŠƒå¢åŠ è¨˜æ†¶é«”å®¹é‡ä»¥æ‡‰å°æœªä¾†éœ€æ±‚\")\n",
    "            \n",
    "            elif resource in ['gpu_utilization', 'gpu_memory_used']:\n",
    "                if bottleneck['severity'] == 'critical':\n",
    "                    recommendations.append(\"ç«‹å³å¢åŠ  GPU è³‡æºæˆ–å¯¦æ–½æ¨¡å‹åˆ†ç‰‡\")\n",
    "                else:\n",
    "                    recommendations.append(\"æº–å‚™ GPU æ“´å±•æ–¹æ¡ˆä»¥æ‡‰å°è² è¼‰å¢é•·\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"ç•¶å‰è³‡æºé…ç½®å……è¶³ï¼ŒæŒçºŒç›£æ§å³å¯\")\n",
    "        \n",
    "        capacity_analysis['scaling_recommendations'] = recommendations\n",
    "        \n",
    "        return capacity_analysis\n",
    "\n",
    "# åˆå§‹åŒ–è² è¼‰æ¸¬è©¦åˆ†æå™¨\n",
    "load_analyzer = LoadTestAnalyzer(analyzer.data)\n",
    "print(\"âœ… è² è¼‰æ¸¬è©¦åˆ†æå™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œè² è¼‰æ¨¡å¼åˆ†æ\n",
    "load_patterns = load_analyzer.analyze_load_patterns()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ è² è¼‰æ¨¡å¼åˆ†æå ±å‘Š\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# è² è¼‰åˆ†ä½ˆçµ±è¨ˆ\n",
    "if 'load_distribution' in load_patterns and load_patterns['load_distribution']:\n",
    "    dist = load_patterns['load_distribution']\n",
    "    print(\"\\nğŸ” è² è¼‰åˆ†ä½ˆçµ±è¨ˆ:\")\n",
    "    print(f\"   å¹³å‡è² è¼‰: {dist['mean']:.1f}%\")\n",
    "    print(f\"   ä¸­ä½æ•¸è² è¼‰: {dist['median']:.1f}%\")\n",
    "    print(f\"   è² è¼‰ç¯„åœ: {dist['min']:.1f}% - {dist['max']:.1f}%\")\n",
    "    print(f\"   è² è¼‰è®Šç•°æ€§: {dist['std']:.1f}% (æ¨™æº–å·®)\")\n",
    "    print(f\"   é«˜è² è¼‰æ™‚é–“ä½”æ¯”: {dist['high_load_percentage']:.1f}%\")\n",
    "    print(f\"   ä½è² è¼‰æ™‚é–“ä½”æ¯”: {dist['low_load_percentage']:.1f}%\")\n",
    "\n",
    "# å³°å€¼æœŸé–“åˆ†æ\n",
    "if load_patterns['peak_periods']:\n",
    "    print(f\"\\nâš¡ æª¢æ¸¬åˆ° {len(load_patterns['peak_periods'])} å€‹é«˜è² è¼‰æœŸé–“:\")\n",
    "    for i, period in enumerate(load_patterns['peak_periods'][:5], 1):  # é¡¯ç¤ºå‰5å€‹\n",
    "        print(f\"   æœŸé–“ {i}: {period['start'].strftime('%H:%M:%S')} - {period['end'].strftime('%H:%M:%S')} \"\n",
    "              f\"(æŒçºŒ {period['duration_seconds']:.0f} ç§’)\")\nelse:\n    print(\"\\nâœ… æœªæª¢æ¸¬åˆ°é¡¯è‘—çš„é«˜è² è¼‰æœŸé–“\")\n",
    "\n",
    "# ä¸åŒè² è¼‰ä¸‹çš„æ€§èƒ½è¡¨ç¾\n",
    "if 'performance_under_load' in load_patterns:\n",
    "    print(\"\\nğŸ¯ ä¸åŒè² è¼‰ä¸‹çš„æ€§èƒ½è¡¨ç¾:\")\n",
    "    for load_type, perf in load_patterns['performance_under_load'].items():\n",
    "        if perf['data_points'] > 0:\n",
    "            print(f\"\\n   {load_type.upper()} è² è¼‰ ({perf['data_points']} å€‹æ•¸æ“šé»):\")\n",
    "            print(f\"     å¹³å‡ CPU: {perf['avg_cpu']:.1f}%\")\n",
    "            print(f\"     å¹³å‡è¨˜æ†¶é«”: {perf['avg_memory']:.1f}%\")\n",
    "            print(f\"     å¹³å‡ GPU: {perf['avg_gpu']:.1f}%\")\n",
    "            print(f\"     å¹³å‡ QPS: {perf['avg_qps']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œå®¹é‡è¦åŠƒåˆ†æ\n",
    "capacity_analysis = load_analyzer.capacity_planning_analysis()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ—ï¸  å®¹é‡è¦åŠƒåˆ†æå ±å‘Š\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ç•¶å‰è³‡æºä½¿ç”¨æƒ…æ³\n",
    "print(\"\\nğŸ“Š ç•¶å‰è³‡æºä½¿ç”¨æƒ…æ³:\")\n",
    "print(f\"{'è³‡æº':<15} {'å¹³å‡ä½¿ç”¨':<10} {'å³°å€¼ä½¿ç”¨':<10} {'P95ä½¿ç”¨':<10} {'å‰©é¤˜å®¹é‡':<10} {'ä½¿ç”¨è¶¨å‹¢':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for resource, utilization in capacity_analysis['current_utilization'].items():\n",
    "    print(f\"{resource:<15} {utilization['average']:<10.1f} {utilization['peak']:<10.1f} \"\n",
    "          f\"{utilization['p95']:<10.1f} {utilization['headroom']:<10.1f} {utilization['utilization_trend']:<10}\")\n",
    "\n",
    "# ç“¶é ¸åˆ†æ\n",
    "if capacity_analysis['bottleneck_analysis']:\n",
    "    print(\"\\nâš ï¸  è³‡æºç“¶é ¸åˆ†æ:\")\n",
    "    for bottleneck in capacity_analysis['bottleneck_analysis']:\n",
    "        severity_icon = \"ğŸ”´\" if bottleneck['severity'] == 'critical' else \"ğŸŸ¡\"\n",
    "        print(f\"   {severity_icon} {bottleneck['resource']}: {bottleneck['severity'].upper()} \"\n",
    "              f\"(P95: {bottleneck['p95_usage']:.1f}%, å‰©é¤˜: {bottleneck['headroom']:.1f}%)\")\nelse:\n    print(\"\\nâœ… æœªç™¼ç¾è³‡æºç“¶é ¸\")\n",
    "\n",
    "# æ“´å±•å»ºè­°\n",
    "print(\"\\nğŸ’¡ å®¹é‡æ“´å±•å»ºè­°:\")\n",
    "for i, recommendation in enumerate(capacity_analysis['scaling_recommendations'], 1):\n",
    "    print(f\"   {i}. {recommendation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ€§èƒ½å„ªåŒ–å»ºè­°ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceOptimizationAdvisor:\n",
    "    \"\"\"æ€§èƒ½å„ªåŒ–å»ºè­°ç”Ÿæˆå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer, bottlenecks, load_patterns, capacity_analysis):\n",
    "        self.analyzer = analyzer\n",
    "        self.bottlenecks = bottlenecks\n",
    "        self.load_patterns = load_patterns\n",
    "        self.capacity_analysis = capacity_analysis\n",
    "        \n",
    "    def generate_comprehensive_recommendations(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç”Ÿæˆç¶œåˆå„ªåŒ–å»ºè­°\"\"\"\n",
    "        recommendations = {\n",
    "            'immediate_actions': [],\n",
    "            'short_term_optimizations': [],\n",
    "            'long_term_planning': [],\n",
    "            'monitoring_improvements': [],\n",
    "            'performance_kpis': {},\n",
    "            'risk_assessment': {}\n",
    "        }\n",
    "        \n",
    "        # ç«‹å³è¡Œå‹•å»ºè­°\n",
    "        recommendations['immediate_actions'] = self._generate_immediate_actions()\n",
    "        \n",
    "        # çŸ­æœŸå„ªåŒ–å»ºè­°\n",
    "        recommendations['short_term_optimizations'] = self._generate_short_term_optimizations()\n",
    "        \n",
    "        # é•·æœŸè¦åŠƒå»ºè­°\n",
    "        recommendations['long_term_planning'] = self._generate_long_term_planning()\n",
    "        \n",
    "        # ç›£æ§æ”¹é€²å»ºè­°\n",
    "        recommendations['monitoring_improvements'] = self._generate_monitoring_improvements()\n",
    "        \n",
    "        # æ€§èƒ½ KPI å»ºè­°\n",
    "        recommendations['performance_kpis'] = self._generate_performance_kpis()\n",
    "        \n",
    "        # é¢¨éšªè©•ä¼°\n",
    "        recommendations['risk_assessment'] = self._assess_risks()\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _generate_immediate_actions(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"ç”Ÿæˆç«‹å³è¡Œå‹•å»ºè­°\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        # æª¢æŸ¥è‡¨ç•ŒæœŸé–“\n",
    "        if self.bottlenecks['critical_periods']:\n",
    "            actions.append({\n",
    "                'action': 'è¨­ç½®è‡ªå‹•å‘Šè­¦',\n",
    "                'description': 'ç‚ºè‡¨ç•Œè³‡æºä½¿ç”¨æœŸé–“è¨­ç½®å¯¦æ™‚å‘Šè­¦æ©Ÿåˆ¶',\n",
    "                'priority': 'high',\n",
    "                'estimated_effort': '1-2 å°æ™‚'\n",
    "            })\n",
    "        \n",
    "        # æª¢æŸ¥è³‡æºç´„æŸ\n",
    "        critical_resources = [c for c in self.bottlenecks['resource_constraints'] \n",
    "                            if c['high_usage_percentage'] > 80]\n",
    "        \n",
    "        if critical_resources:\n",
    "            for resource in critical_resources:\n",
    "                if resource['metric'] == 'gpu_memory_used':\n",
    "                    actions.append({\n",
    "                        'action': 'èª¿æ•´ GPU è¨˜æ†¶é«”é…ç½®',\n",
    "                        'description': 'é™ä½æ‰¹æ¬¡å¤§å°æˆ–å•Ÿç”¨æ¢¯åº¦ç´¯ç©ä¾†æ¸›å°‘ GPU è¨˜æ†¶é«”ä½¿ç”¨',\n",
    "                        'priority': 'high',\n",
    "                        'estimated_effort': '30 åˆ†é˜'\n",
    "                    })\n",
    "                elif resource['metric'] == 'cpu_percent':\n",
    "                    actions.append({\n",
    "                        'action': 'å„ªåŒ– CPU ä½¿ç”¨',\n",
    "                        'description': 'æª¢æŸ¥ä¸¦å„ªåŒ– CPU å¯†é›†å‹æ“ä½œï¼Œè€ƒæ…®ä¸¦è¡ŒåŒ–è™•ç†',\n",
    "                        'priority': 'medium',\n",
    "                        'estimated_effort': '2-4 å°æ™‚'\n",
    "                    })\n",
    "        \n",
    "        # æª¢æŸ¥ç•°å¸¸æª¢æ¸¬çµæœ\n",
    "        if not actions:  # å¦‚æœæ²’æœ‰ç·Šæ€¥å•é¡Œ\n",
    "            actions.append({\n",
    "                'action': 'é©—è­‰ç•¶å‰é…ç½®',\n",
    "                'description': 'ç³»çµ±é‹è¡Œæ­£å¸¸ï¼Œé©—è­‰ç•¶å‰é…ç½®æ˜¯å¦ç‚ºæœ€ä½³å¯¦è¸',\n",
    "                'priority': 'low',\n",
    "                'estimated_effort': '1 å°æ™‚'\n",
    "            })\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def _generate_short_term_optimizations(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"ç”ŸæˆçŸ­æœŸå„ªåŒ–å»ºè­°\"\"\"\n",
    "        optimizations = []\n",
    "        \n",
    "        # åŸºæ–¼è² è¼‰æ¨¡å¼çš„å„ªåŒ–\n",
    "        if self.load_patterns.get('peak_periods'):\n",
    "            optimizations.append({\n",
    "                'optimization': 'å¯¦æ–½è² è¼‰å‡è¡¡',\n",
    "                'description': 'åœ¨é«˜è² è¼‰æœŸé–“è‡ªå‹•åˆ†æ•£è«‹æ±‚æˆ–èª¿æ•´æœå‹™å®¹é‡',\n",
    "                'expected_benefit': 'æ¸›å°‘ 20-30% çš„éŸ¿æ‡‰å»¶é²',\n",
    "                'timeline': '1-2 é€±'\n",
    "            })\n",
    "        \n",
    "        # è¨˜æ†¶é«”å„ªåŒ–\n",
    "        if 'memory_percent' in self.analyzer.data.columns:\n",
    "            avg_memory = self.analyzer.data['memory_percent'].mean()\n",
    "            if avg_memory > 70:\n",
    "                optimizations.append({\n",
    "                    'optimization': 'è¨˜æ†¶é«”ä½¿ç”¨å„ªåŒ–',\n",
    "                    'description': 'å¯¦æ–½è¨˜æ†¶é«”æ± ç®¡ç†å’Œåƒåœ¾å›æ”¶å„ªåŒ–',\n",
    "                    'expected_benefit': 'é™ä½ 10-15% çš„è¨˜æ†¶é«”ä½¿ç”¨',\n",
    "                    'timeline': '1 é€±'\n",
    "                })\n",
    "        \n",
    "        # å¿«å–å„ªåŒ–\n",
    "        optimizations.append({\n",
    "            'optimization': 'KV å¿«å–å„ªåŒ–',\n",
    "            'description': 'èª¿æ•´ KV å¿«å–å¤§å°å’Œç­–ç•¥ä»¥æå‡æ¨ç†æ•ˆç‡',\n",
    "            'expected_benefit': 'æå‡ 15-25% çš„ååé‡',\n",
    "            'timeline': '3-5 å¤©'\n",
    "        })\n",
    "        \n",
    "        # ä¸¦ç™¼å„ªåŒ–\n",
    "        if 'qps' in self.analyzer.data.columns:\n",
    "            avg_qps = self.analyzer.data['qps'].mean()\n",
    "            if avg_qps < 10:  # å‡è¨­æœŸæœ› QPS\n",
    "                optimizations.append({\n",
    "                    'optimization': 'ä¸¦ç™¼è™•ç†å„ªåŒ–',\n",
    "                    'description': 'èª¿æ•´ä¸¦ç™¼æ•¸å’Œæ‰¹æ¬¡è™•ç†ç­–ç•¥',\n",
    "                    'expected_benefit': 'æå‡ 30-50% çš„ååé‡',\n",
    "                    'timeline': '1 é€±'\n",
    "                })\n",
    "        \n",
    "        return optimizations\n",
    "    \n",
    "    def _generate_long_term_planning(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"ç”Ÿæˆé•·æœŸè¦åŠƒå»ºè­°\"\"\"\n",
    "        planning = []\n",
    "        \n",
    "        # åŸºæ–¼å®¹é‡åˆ†æçš„è¦åŠƒ\n",
    "        bottlenecks = self.capacity_analysis.get('bottleneck_analysis', [])\n",
    "        \n",
    "        if bottlenecks:\n",
    "            planning.append({\n",
    "                'plan': 'åŸºç¤è¨­æ–½æ“´å±•',\n",
    "                'description': 'æ ¹æ“šè³‡æºç“¶é ¸åˆ†æåˆ¶å®šç¡¬é«”æ“´å±•è¨ˆåŠƒ',\n",
    "                'timeline': '3-6 å€‹æœˆ',\n",
    "                'investment': 'ä¸­ç­‰åˆ°é«˜ç­‰'\n",
    "            })\n",
    "        \n",
    "        # æ¶æ§‹å‡ç´š\n",
    "        planning.append({\n",
    "            'plan': 'åˆ†æ•£å¼æ¶æ§‹å‡ç´š',\n",
    "            'description': 'å¯¦æ–½å¤šç¯€é»åˆ†æ•£å¼æ¨ç†æ¶æ§‹ä»¥æå‡å¯æ“´å±•æ€§',\n",
    "            'timeline': '6-12 å€‹æœˆ',\n",
    "            'investment': 'é«˜ç­‰'\n",
    "        })\n",
    "        \n",
    "        # è‡ªå‹•åŒ–é‹ç¶­\n",
    "        planning.append({\n",
    "            'plan': 'DevOps è‡ªå‹•åŒ–',\n",
    "            'description': 'å»ºç«‹è‡ªå‹•åŒ–ç›£æ§ã€éƒ¨ç½²å’Œæ“´ç¸®å®¹æ©Ÿåˆ¶',\n",
    "            'timeline': '2-4 å€‹æœˆ',\n",
    "            'investment': 'ä¸­ç­‰'\n",
    "        })\n",
    "        \n",
    "        # æ¨¡å‹å„ªåŒ–\n",
    "        planning.append({\n",
    "            'plan': 'æ¨¡å‹å£“ç¸®èˆ‡é‡åŒ–',\n",
    "            'description': 'å¯¦æ–½æ¨¡å‹é‡åŒ–ã€è’¸é¤¾å’Œå‰ªæä»¥æå‡æ¨ç†æ•ˆç‡',\n",
    "            'timeline': '4-8 å€‹æœˆ',\n",
    "            'investment': 'ä¸­ç­‰'\n",
    "        })\n",
    "        \n",
    "        return planning\n",
    "    \n",
    "    def _generate_monitoring_improvements(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"ç”Ÿæˆç›£æ§æ”¹é€²å»ºè­°\"\"\"\n",
    "        improvements = [\n",
    "            {\n",
    "                'improvement': 'å¢åŠ æ¥­å‹™å±¤ç›£æ§',\n",
    "                'description': 'ç›£æ§æ¨¡å‹å‡†ç¢ºæ€§ã€ç”¨æˆ¶æ»¿æ„åº¦ç­‰æ¥­å‹™æŒ‡æ¨™',\n",
    "                'priority': 'high'\n",
    "            },\n",
    "            {\n",
    "                'improvement': 'å¯¦æ–½åˆ†æ•£å¼è¿½è¹¤',\n",
    "                'description': 'ä½¿ç”¨ Jaeger æˆ– Zipkin è¿½è¹¤è«‹æ±‚ç”Ÿå‘½é€±æœŸ',\n",
    "                'priority': 'medium'\n",
    "            },\n",
    "            {\n",
    "                'improvement': 'å»ºç«‹è‡ªå®šç¾©æŒ‡æ¨™',\n",
    "                'description': 'æ·»åŠ ç‰¹å®šæ–¼æ¥­å‹™çš„ KPI ç›£æ§æŒ‡æ¨™',\n",
    "                'priority': 'medium'\n",
    "            },\n",
    "            {\n",
    "                'improvement': 'å¼·åŒ–å‘Šè­¦æ©Ÿåˆ¶',\n",
    "                'description': 'è¨­ç½®å¤šå±¤ç´šå‘Šè­¦å’Œè‡ªå‹•éŸ¿æ‡‰æ©Ÿåˆ¶',\n",
    "                'priority': 'high'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return improvements\n",
    "    \n",
    "    def _generate_performance_kpis(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"ç”Ÿæˆæ€§èƒ½ KPI å»ºè­°\"\"\"\n",
    "        kpis = {\n",
    "            'latency_targets': {\n",
    "                'p50_response_time': 1.0,  # ç§’\n",
    "                'p95_response_time': 2.0,\n",
    "                'p99_response_time': 5.0,\n",
    "                'ttft_target': 0.5\n",
    "            },\n",
    "            'throughput_targets': {\n",
    "                'min_qps': 10,\n",
    "                'target_qps': 50,\n",
    "                'peak_qps': 100\n",
    "            },\n",
    "            'resource_targets': {\n",
    "                'max_cpu_usage': 80,  # %\n",
    "                'max_memory_usage': 85,\n",
    "                'max_gpu_usage': 90,\n",
    "                'target_gpu_utilization': 75\n",
    "            },\n",
    "            'reliability_targets': {\n",
    "                'min_success_rate': 99.0,  # %\n",
    "                'max_error_rate': 1.0,\n",
    "                'target_availability': 99.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return kpis\n",
    "    \n",
    "    def _assess_risks(self) -> Dict[str, Any]:\n",
    "        \"\"\"è©•ä¼°é¢¨éšª\"\"\"\n",
    "        risks = {\n",
    "            'high_priority': [],\n",
    "            'medium_priority': [],\n",
    "            'low_priority': []\n",
    "        }\n",
    "        \n",
    "        # æª¢æŸ¥é«˜é¢¨éšªæƒ…æ³\n",
    "        if self.bottlenecks['critical_periods']:\n",
    "            risks['high_priority'].append({\n",
    "                'risk': 'ç³»çµ±éè¼‰é¢¨éšª',\n",
    "                'description': 'æª¢æ¸¬åˆ°è‡¨ç•Œè³‡æºä½¿ç”¨æœŸé–“ï¼Œå¯èƒ½å°è‡´æœå‹™ä¸­æ–·',\n",
    "                'mitigation': 'å¯¦æ–½è‡ªå‹•æ“´ç¸®å®¹å’Œè² è¼‰å‡è¡¡'\n",
    "            })\n",
    "        \n",
    "        # æª¢æŸ¥æ€§èƒ½è¡°é€€é¢¨éšª\n",
    "        if self.bottlenecks['performance_degradation']:\n",
    "            risks['medium_priority'].append({\n",
    "                'risk': 'æ€§èƒ½è¡°é€€é¢¨éšª',\n",
    "                'description': 'æª¢æ¸¬åˆ°æ€§èƒ½ä¸‹é™è¶¨å‹¢ï¼Œå¯èƒ½å½±éŸ¿ç”¨æˆ¶é«”é©—',\n",
    "                'mitigation': 'å®šæœŸæ€§èƒ½åŸºæº–æ¸¬è©¦å’Œå„ªåŒ–'\n",
    "            })\n",
    "        \n",
    "        # æª¢æŸ¥å®¹é‡é¢¨éšª\n",
    "        critical_bottlenecks = [b for b in self.capacity_analysis.get('bottleneck_analysis', []) \n",
    "                              if b['severity'] == 'critical']\n",
    "        \n",
    "        if critical_bottlenecks:\n",
    "            risks['high_priority'].append({\n",
    "                'risk': 'å®¹é‡ä¸è¶³é¢¨éšª',\n",
    "                'description': 'é—œéµè³‡æºæ¥è¿‘å®¹é‡ä¸Šé™',\n",
    "                'mitigation': 'åˆ¶å®šç·Šæ€¥æ“´å®¹è¨ˆåŠƒ'\n",
    "            })\n",
    "        \n",
    "        # å¦‚æœæ²’æœ‰é«˜é¢¨éšªï¼Œæ·»åŠ ä¸€äº›é é˜²æ€§å»ºè­°\n",
    "        if not risks['high_priority'] and not risks['medium_priority']:\n",
    "            risks['low_priority'].append({\n",
    "                'risk': 'ç›£æ§è¦†è“‹ä¸è¶³é¢¨éšª',\n",
    "                'description': 'ç•¶å‰ç³»çµ±é‹è¡Œè‰¯å¥½ï¼Œä½†å»ºè­°åŠ å¼·ç›£æ§è¦†è“‹',\n",
    "                'mitigation': 'æ“´å±•ç›£æ§æŒ‡æ¨™å’Œå‘Šè­¦æ©Ÿåˆ¶'\n",
    "            })\n",
    "        \n",
    "        return risks\n",
    "\n",
    "# åˆå§‹åŒ–å„ªåŒ–å»ºè­°ç”Ÿæˆå™¨\n",
    "advisor = PerformanceOptimizationAdvisor(\n",
    "    analyzer, bottlenecks, load_patterns, capacity_analysis\n",
    ")\n",
    "\n",
    "# ç”Ÿæˆç¶œåˆå»ºè­°\n",
    "optimization_recommendations = advisor.generate_comprehensive_recommendations()\n",
    "\n",
    "print(\"âœ… æ€§èƒ½å„ªåŒ–å»ºè­°ç”Ÿæˆå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤ºç¶œåˆå„ªåŒ–å»ºè­°\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ vLLM æ€§èƒ½å„ªåŒ–å»ºè­°å ±å‘Š\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ç«‹å³è¡Œå‹•å»ºè­°\n",
    "print(\"\\nğŸš¨ ç«‹å³è¡Œå‹•å»ºè­°:\")\n",
    "for i, action in enumerate(optimization_recommendations['immediate_actions'], 1):\n",
    "    priority_icon = \"ğŸ”´\" if action['priority'] == 'high' else \"ğŸŸ¡\" if action['priority'] == 'medium' else \"ğŸŸ¢\"\n",
    "    print(f\"\\n   {i}. {priority_icon} {action['action']}\")\n",
    "    print(f\"      æè¿°: {action['description']}\")\n",
    "    print(f\"      å„ªå…ˆç´š: {action['priority'].upper()}\")\n",
    "    print(f\"      é ä¼°å·¥ä½œé‡: {action['estimated_effort']}\")\n",
    "\n",
    "# çŸ­æœŸå„ªåŒ–å»ºè­°\n",
    "print(\"\\nğŸ“ˆ çŸ­æœŸå„ªåŒ–å»ºè­° (1-4 é€±):\")\n",
    "for i, optimization in enumerate(optimization_recommendations['short_term_optimizations'], 1):\n",
    "    print(f\"\\n   {i}. {optimization['optimization']}\")\n",
    "    print(f\"      æè¿°: {optimization['description']}\")\n",
    "    print(f\"      é æœŸæ•ˆç›Š: {optimization['expected_benefit']}\")\n",
    "    print(f\"      æ™‚é–“ç·š: {optimization['timeline']}\")\n",
    "\n",
    "# é•·æœŸè¦åŠƒå»ºè­°\n",
    "print(\"\\nğŸ—ï¸  é•·æœŸè¦åŠƒå»ºè­° (3-12 å€‹æœˆ):\")\n",
    "for i, plan in enumerate(optimization_recommendations['long_term_planning'], 1):\n",
    "    investment_icon = \"ğŸ’°\" if plan['investment'] == 'é«˜ç­‰' else \"ğŸ’µ\" if plan['investment'] == 'ä¸­ç­‰' else \"ğŸ’´\"\n",
    "    print(f\"\\n   {i}. {investment_icon} {plan['plan']}\")\n",
    "    print(f\"      æè¿°: {plan['description']}\")\n",
    "    print(f\"      æ™‚é–“ç·š: {plan['timeline']}\")\n",
    "    print(f\"      æŠ•è³‡ç´šåˆ¥: {plan['investment']}\")\n",
    "\n",
    "# ç›£æ§æ”¹é€²å»ºè­°\n",
    "print(\"\\nğŸ“Š ç›£æ§æ”¹é€²å»ºè­°:\")\n",
    "for i, improvement in enumerate(optimization_recommendations['monitoring_improvements'], 1):\n",
    "    priority_icon = \"ğŸ”´\" if improvement['priority'] == 'high' else \"ğŸŸ¡\" if improvement['priority'] == 'medium' else \"ğŸŸ¢\"\n",
    "    print(f\"   {i}. {priority_icon} {improvement['improvement']}\")\n",
    "    print(f\"      {improvement['description']}\")\n",
    "\n",
    "# æ€§èƒ½ KPI å»ºè­°\n",
    "print(\"\\nğŸ¯ å»ºè­°æ€§èƒ½ KPI ç›®æ¨™:\")\n",
    "kpis = optimization_recommendations['performance_kpis']\n",
    "\n",
    "print(\"\\n   ğŸ“Š å»¶é²ç›®æ¨™:\")\n",
    "for metric, target in kpis['latency_targets'].items():\n",
    "    unit = 'ç§’' if 'time' in metric or 'ttft' in metric else ''\n",
    "    print(f\"     {metric}: â‰¤ {target} {unit}\")\n",
    "\n",
    "print(\"\\n   ğŸš€ ååé‡ç›®æ¨™:\")\n",
    "for metric, target in kpis['throughput_targets'].items():\n",
    "    print(f\"     {metric}: {target} QPS\")\n",
    "\n",
    "print(\"\\n   ğŸ’» è³‡æºä½¿ç”¨ç›®æ¨™:\")\n",
    "for metric, target in kpis['resource_targets'].items():\n",
    "    print(f\"     {metric}: {target}%\")\n",
    "\n",
    "# é¢¨éšªè©•ä¼°\n",
    "print(\"\\nâš ï¸  é¢¨éšªè©•ä¼°:\")\n",
    "risks = optimization_recommendations['risk_assessment']\n",
    "\n",
    "for priority in ['high_priority', 'medium_priority', 'low_priority']:\n",
    "    if risks[priority]:\n",
    "        priority_name = priority.replace('_', ' ').title()\n",
    "        priority_icon = \"ğŸ”´\" if priority == 'high_priority' else \"ğŸŸ¡\" if priority == 'medium_priority' else \"ğŸŸ¢\"\n",
    "        print(f\"\\n   {priority_icon} {priority_name} é¢¨éšª:\")\n",
    "        \n",
    "        for risk in risks[priority]:\n",
    "            print(f\"     â€¢ {risk['risk']}\")\n",
    "            print(f\"       {risk['description']}\")\n",
    "            print(f\"       ç·©è§£æªæ–½: {risk['mitigation']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ å ±å‘Šç¸½çµ: åŸºæ–¼ç•¶å‰ç›£æ§æ•¸æ“šï¼Œç³»çµ±æ•´é«”é‹è¡Œ\" + \n",
    "      (\"éœ€è¦é—œæ³¨\" if risks['high_priority'] else \"è‰¯å¥½\") + \n",
    "      \"ã€‚å»ºè­°å„ªå…ˆåŸ·è¡Œç«‹å³è¡Œå‹•å»ºè­°ï¼Œä¸¦åˆ¶å®šçŸ­æœŸå„ªåŒ–è¨ˆåŠƒã€‚\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯¦é©—ç¸½çµ\n",
    "\n",
    "æœ¬å¯¦é©—å®Œæˆäº†å…¨é¢çš„ vLLM æ€§èƒ½åˆ†æèˆ‡è¨ºæ–·ï¼Œå»ºç«‹äº†å®Œæ•´çš„æ€§èƒ½è©•ä¼°æ¡†æ¶ï¼š\n",
    "\n",
    "### âœ… æ ¸å¿ƒæˆæœ\n",
    "\n",
    "1. **æ·±åº¦æ€§èƒ½åˆ†æ**\n",
    "   - å¤šç¶­åº¦æ€§èƒ½çµ±è¨ˆåˆ†æ\n",
    "   - æ­·å²è¶¨å‹¢èˆ‡æ¨¡å¼è­˜åˆ¥\n",
    "   - ç•°å¸¸æª¢æ¸¬èˆ‡æ ¹å› åˆ†æ\n",
    "\n",
    "2. **æ™ºèƒ½ç“¶é ¸è¨ºæ–·**\n",
    "   - è‡ªå‹•è­˜åˆ¥è³‡æºç´„æŸ\n",
    "   - è‡¨ç•ŒæœŸé–“æª¢æ¸¬\n",
    "   - æ€§èƒ½è¡°é€€åˆ†æ\n",
    "\n",
    "3. **è² è¼‰æ¨¡å¼åˆ†æ**\n",
    "   - å³°å€¼èˆ‡ä½è°·æœŸé–“è­˜åˆ¥\n",
    "   - ä¸åŒè² è¼‰ä¸‹çš„æ€§èƒ½è¡¨ç¾\n",
    "   - è² è¼‰åˆ†ä½ˆçµ±è¨ˆ\n",
    "\n",
    "4. **å®¹é‡è¦åŠƒè©•ä¼°**\n",
    "   - ç•¶å‰è³‡æºåˆ©ç”¨ç‡åˆ†æ\n",
    "   - ç“¶é ¸è³‡æºè­˜åˆ¥\n",
    "   - æ“´å±•å»ºè­°ç”Ÿæˆ\n",
    "\n",
    "5. **ç¶œåˆå„ªåŒ–å»ºè­°**\n",
    "   - åˆ†å±¤ç´šçš„è¡Œå‹•å»ºè­°\n",
    "   - çŸ­æœŸèˆ‡é•·æœŸè¦åŠƒ\n",
    "   - é¢¨éšªè©•ä¼°èˆ‡ç·©è§£ç­–ç•¥\n",
    "\n",
    "### ğŸ¯ æŠ€è¡“äº®é»\n",
    "\n",
    "- **æ©Ÿå™¨å­¸ç¿’ç•°å¸¸æª¢æ¸¬**: ä½¿ç”¨ Isolation Forest é€²è¡Œæ™ºèƒ½ç•°å¸¸è­˜åˆ¥\n",
    "- **æ™‚é–“åºåˆ—åˆ†æ**: è¶¨å‹¢æª¢æ¸¬ã€å­£ç¯€æ€§åˆ†æã€ç›¸é—œæ€§è©•ä¼°\n",
    "- **çµ±è¨ˆå­¸æ–¹æ³•**: å¤šç¨®çµ±è¨ˆæŒ‡æ¨™å’Œåˆ†ä½ˆåˆ†æ\n",
    "- **è¦–è¦ºåŒ–å„€è¡¨æ¿**: å¤šé¢æ¿ç¶œåˆæ€§èƒ½è¦–è¦ºåŒ–\n",
    "- **è‡ªå‹•åŒ–å»ºè­°**: åŸºæ–¼æ•¸æ“šçš„æ™ºèƒ½å„ªåŒ–å»ºè­°ç”Ÿæˆ\n",
    "\n",
    "### ğŸ“Š åˆ†æè¦†è“‹ç¯„åœ\n",
    "\n",
    "- **ç³»çµ±å±¤é¢**: CPUã€è¨˜æ†¶é«”ã€GPU è³‡æºåˆ†æ\n",
    "- **æ‡‰ç”¨å±¤é¢**: vLLM æœå‹™æ€§èƒ½æŒ‡æ¨™\n",
    "- **æ¥­å‹™å±¤é¢**: QPSã€å»¶é²ã€æˆåŠŸç‡åˆ†æ\n",
    "- **é‹ç¶­å±¤é¢**: å®¹é‡è¦åŠƒã€é¢¨éšªè©•ä¼°\n",
    "\n",
    "### ğŸ”§ å¯¦ç”¨å·¥å…·\n",
    "\n",
    "- **ComprehensivePerformanceAnalyzer**: å…¨é¢æ€§èƒ½åˆ†æå™¨\n",
    "- **TimeSeriesAnalyzer**: æ™‚é–“åºåˆ—åˆ†æå·¥å…·\n",
    "- **LoadTestAnalyzer**: è² è¼‰æ¸¬è©¦åˆ†æå™¨\n",
    "- **PerformanceOptimizationAdvisor**: å„ªåŒ–å»ºè­°ç”Ÿæˆå™¨\n",
    "\n",
    "### ğŸ“‹ ä¸‹ä¸€æ­¥\n",
    "\n",
    "ç¹¼çºŒé€²è¡Œ **04-Alerting_and_Optimization.ipynb**ï¼Œå­¸ç¿’æ™ºèƒ½å‘Šè­¦ç³»çµ±å»ºè¨­å’Œè‡ªå‹•åŒ–å„ªåŒ–ç­–ç•¥ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**æ‡‰ç”¨åƒ¹å€¼**:\n",
    "- æä¾›æ•¸æ“šé©…å‹•çš„æ€§èƒ½å„ªåŒ–æ±ºç­–ä¾æ“š\n",
    "- å»ºç«‹æ¨™æº–åŒ–çš„æ€§èƒ½è©•ä¼°æµç¨‹\n",
    "- æ”¯æ´é é˜²æ€§ç¶­è­·å’Œå®¹é‡è¦åŠƒ\n",
    "- é™ä½ç³»çµ±æ•…éšœé¢¨éšªå’Œé‹ç¶­æˆæœ¬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}