{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1 - OpenCompass 評估實戰\n",
    "## Notebook 02: 執行模型評估\n",
    "\n",
    "**學習目標**:\n",
    "1. 配置評估參數 (batch size, 推理模式)\n",
    "2. 執行 C-Eval 評估 (STEM, 社會科學, 人文學科)\n",
    "3. 收集評估日誌與中間結果\n",
    "4. 處理評估過程中的常見問題\n",
    "\n",
    "**預計時間**: 1-2 小時 (取決於硬體)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 載入配置與模型\n",
    "\n",
    "從 01-Setup.ipynb 載入配置和模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 數據目錄\n",
    "DATA_DIR = Path(\"./data\")\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 載入配置\n",
    "config_path = DATA_DIR / \"eval_config.json\"\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    eval_config = json.load(f)\n",
    "\n",
    "print(\"✅ 配置已載入\")\n",
    "print(json.dumps(eval_config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重新載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化配置\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    \"\"\"載入模型和 tokenizer\"\"\"\n",
    "    print(f\"📥 載入模型: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config if torch.cuda.is_available() else None,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 模型載入完成\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# 載入模型\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for model_key, model_info in eval_config[\"models\"].items():\n",
    "    if model_info[\"loaded\"]:\n",
    "        try:\n",
    "            model, tokenizer = load_model(model_info[\"name\"])\n",
    "            models[model_key] = model\n",
    "            tokenizers[model_key] = tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 載入 {model_key} 失敗: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入評估數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 C-Eval 數據集\n",
    "ceval_subjects = eval_config[\"datasets\"][\"ceval\"][\"subjects\"]\n",
    "\n",
    "print(\"📥 載入 C-Eval 數據集...\")\n",
    "ceval_data = {}\n",
    "\n",
    "for subject in ceval_subjects:\n",
    "    dataset = load_dataset(\n",
    "        \"ceval/ceval-exam\",\n",
    "        subject,\n",
    "        split=\"val\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    ceval_data[subject] = dataset\n",
    "    print(f\"  ✅ {subject}: {len(dataset)} 題\")\n",
    "\n",
    "print(f\"\\n總計: {sum(len(d) for d in ceval_data.values())} 題\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 實作評估函數\n",
    "\n",
    "實作多選題評估邏輯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ceval_prompt(question: str, choices: dict) -> str:\n",
    "    \"\"\"\n",
    "    格式化 C-Eval 問題為提示\n",
    "    \n",
    "    Args:\n",
    "        question: 問題文本\n",
    "        choices: 選項字典 {\"A\": \"...\", \"B\": \"...\", ...}\n",
    "    \n",
    "    Returns:\n",
    "        格式化的提示文本\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"以下是一道選擇題,請選出正確答案。\n",
    "\n",
    "問題: {question}\n",
    "\n",
    "A. {choices['A']}\n",
    "B. {choices['B']}\n",
    "C. {choices['C']}\n",
    "D. {choices['D']}\n",
    "\n",
    "答案:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def evaluate_multiple_choice(model, tokenizer, question: str, choices: dict, correct_answer: str):\n",
    "    \"\"\"\n",
    "    評估多選題\n",
    "    \n",
    "    使用對數似然法選擇答案:\n",
    "    1. 計算每個選項的對數似然\n",
    "    2. 選擇似然最高的選項\n",
    "    \n",
    "    Args:\n",
    "        model: 模型\n",
    "        tokenizer: Tokenizer\n",
    "        question: 問題文本\n",
    "        choices: 選項字典\n",
    "        correct_answer: 正確答案 (A/B/C/D)\n",
    "    \n",
    "    Returns:\n",
    "        (predicted_answer, is_correct, logits_dict)\n",
    "    \"\"\"\n",
    "    prompt = format_ceval_prompt(question, choices)\n",
    "    \n",
    "    # 編碼提示\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 計算每個選項的對數似然\n",
    "    option_logits = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            # 編碼選項\n",
    "            option_ids = tokenizer.encode(f\" {option}\", add_special_tokens=False)\n",
    "            \n",
    "            # 獲取模型輸出\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # 獲取最後一個 token 的 logits\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # 獲取選項的 logit\n",
    "            option_logit = last_token_logits[option_ids[0]].item()\n",
    "            option_logits[option] = option_logit\n",
    "    \n",
    "    # 選擇 logit 最高的選項\n",
    "    predicted_answer = max(option_logits, key=option_logits.get)\n",
    "    is_correct = (predicted_answer == correct_answer)\n",
    "    \n",
    "    return predicted_answer, is_correct, option_logits\n",
    "\n",
    "\n",
    "# 測試評估函數\n",
    "if ceval_data and models:\n",
    "    test_subject = list(ceval_data.keys())[0]\n",
    "    test_sample = ceval_data[test_subject][0]\n",
    "    test_model = list(models.values())[0]\n",
    "    test_tokenizer = list(tokenizers.values())[0]\n",
    "    \n",
    "    print(\"🧪 測試評估函數...\")\n",
    "    print(f\"問題: {test_sample['question']}\")\n",
    "    \n",
    "    pred, correct, logits = evaluate_multiple_choice(\n",
    "        test_model,\n",
    "        test_tokenizer,\n",
    "        test_sample['question'],\n",
    "        {k: test_sample[k] for k in ['A', 'B', 'C', 'D']},\n",
    "        test_sample['answer']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n預測答案: {pred}\")\n",
    "    print(f\"正確答案: {test_sample['answer']}\")\n",
    "    print(f\"是否正確: {correct}\")\n",
    "    print(f\"\\nLogits: {logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 執行完整評估\n",
    "\n",
    "對所有模型和學科進行評估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_subject(model, tokenizer, dataset, subject_name: str):\n",
    "    \"\"\"\n",
    "    評估單個學科\n",
    "    \n",
    "    Args:\n",
    "        model: 模型\n",
    "        tokenizer: Tokenizer\n",
    "        dataset: 數據集\n",
    "        subject_name: 學科名稱\n",
    "    \n",
    "    Returns:\n",
    "        評估結果字典\n",
    "    \"\"\"\n",
    "    print(f\"\\n評估學科: {subject_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for sample in tqdm(dataset, desc=subject_name):\n",
    "        question = sample['question']\n",
    "        choices = {k: sample[k] for k in ['A', 'B', 'C', 'D']}\n",
    "        correct_answer = sample['answer']\n",
    "        \n",
    "        pred, is_correct, logits = evaluate_multiple_choice(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            question,\n",
    "            choices,\n",
    "            correct_answer\n",
    "        )\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'choices': choices,\n",
    "            'correct_answer': correct_answer,\n",
    "            'predicted_answer': pred,\n",
    "            'is_correct': is_correct,\n",
    "            'logits': logits\n",
    "        })\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    \n",
    "    print(f\"\\n準確率: {accuracy:.2%} ({correct_count}/{total_count})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'subject': subject_name,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct_count,\n",
    "        'total': total_count,\n",
    "        'details': results\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model_on_ceval(model, tokenizer, model_name: str, ceval_data: dict):\n",
    "    \"\"\"\n",
    "    評估模型在 C-Eval 上的表現\n",
    "    \n",
    "    Args:\n",
    "        model: 模型\n",
    "        tokenizer: Tokenizer\n",
    "        model_name: 模型名稱\n",
    "        ceval_data: C-Eval 數據集字典\n",
    "    \n",
    "    Returns:\n",
    "        評估結果\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"評估模型: {model_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    subject_results = []\n",
    "    \n",
    "    for subject, dataset in ceval_data.items():\n",
    "        result = evaluate_subject(model, tokenizer, dataset, subject)\n",
    "        subject_results.append(result)\n",
    "    \n",
    "    # 計算總體準確率\n",
    "    total_correct = sum(r['correct'] for r in subject_results)\n",
    "    total_questions = sum(r['total'] for r in subject_results)\n",
    "    overall_accuracy = total_correct / total_questions\n",
    "    \n",
    "    # 按學科分類計算準確率\n",
    "    subject_categories = {\n",
    "        'STEM': ['computer_science', 'physics', 'mathematics'],\n",
    "        'Humanities': ['chinese_language_and_literature', 'history'],\n",
    "        'Social Science': ['law', 'economics']\n",
    "    }\n",
    "    \n",
    "    category_accuracies = {}\n",
    "    for category, subjects in subject_categories.items():\n",
    "        category_results = [r for r in subject_results if r['subject'] in subjects]\n",
    "        if category_results:\n",
    "            cat_correct = sum(r['correct'] for r in category_results)\n",
    "            cat_total = sum(r['total'] for r in category_results)\n",
    "            category_accuracies[category] = cat_correct / cat_total\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'category_accuracies': category_accuracies,\n",
    "        'subject_results': subject_results,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行評估 (Llama-2-7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama-2-7b' in models:\n",
    "    llama_results = evaluate_model_on_ceval(\n",
    "        models['llama-2-7b'],\n",
    "        tokenizers['llama-2-7b'],\n",
    "        'Llama-2-7B',\n",
    "        ceval_data\n",
    "    )\n",
    "    \n",
    "    # 保存結果\n",
    "    results_path = RESULTS_DIR / \"llama2_7b_results.json\"\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(llama_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    print(f\"\\n✅ 結果已保存: {results_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Llama-2-7B 未載入,跳過評估\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行評估 (Qwen-7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'qwen-7b' in models:\n",
    "    qwen_results = evaluate_model_on_ceval(\n",
    "        models['qwen-7b'],\n",
    "        tokenizers['qwen-7b'],\n",
    "        'Qwen-7B',\n",
    "        ceval_data\n",
    "    )\n",
    "    \n",
    "    # 保存結果\n",
    "    results_path = RESULTS_DIR / \"qwen_7b_results.json\"\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qwen_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    print(f\"\\n✅ 結果已保存: {results_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Qwen-7B 未載入,跳過評估\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 初步結果查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results_summary(results: dict):\n",
    "    \"\"\"\n",
    "    顯示評估結果摘要\n",
    "    \n",
    "    Args:\n",
    "        results: 評估結果字典\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"模型: {results['model_name']}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    print(f\"\\n整體準確率: {results['overall_accuracy']:.2%}\")\n",
    "    \n",
    "    print(f\"\\n分類準確率:\")\n",
    "    for category, accuracy in results['category_accuracies'].items():\n",
    "        print(f\"  {category:20s}: {accuracy:.2%}\")\n",
    "    \n",
    "    print(f\"\\n學科準確率:\")\n",
    "    for subject_result in results['subject_results']:\n",
    "        print(f\"  {subject_result['subject']:35s}: {subject_result['accuracy']:.2%}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "\n",
    "# 顯示結果\n",
    "if 'llama_results' in locals():\n",
    "    display_results_summary(llama_results)\n",
    "\n",
    "if 'qwen_results' in locals():\n",
    "    display_results_summary(qwen_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama_results' in locals() and 'qwen_results' in locals():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"模型對比\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 整體對比\n",
    "    print(f\"\\n整體準確率:\")\n",
    "    print(f\"  Llama-2-7B: {llama_results['overall_accuracy']:.2%}\")\n",
    "    print(f\"  Qwen-7B:    {qwen_results['overall_accuracy']:.2%}\")\n",
    "    print(f\"  差距:       {(qwen_results['overall_accuracy'] - llama_results['overall_accuracy']):.2%}\")\n",
    "    \n",
    "    # 分類對比\n",
    "    print(f\"\\n分類準確率對比:\")\n",
    "    for category in llama_results['category_accuracies'].keys():\n",
    "        llama_acc = llama_results['category_accuracies'][category]\n",
    "        qwen_acc = qwen_results['category_accuracies'][category]\n",
    "        diff = qwen_acc - llama_acc\n",
    "        print(f\"  {category:20s}: Llama {llama_acc:.2%} | Qwen {qwen_acc:.2%} | Δ {diff:+.2%}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"⚠️ 需要兩個模型的結果才能進行對比\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 總結\n",
    "\n",
    "在本 notebook 中,我們完成了:\n",
    "\n",
    "1. ✅ 配置評估參數\n",
    "2. ✅ 實作多選題評估邏輯 (對數似然法)\n",
    "3. ✅ 執行 C-Eval 評估\n",
    "4. ✅ 保存評估結果\n",
    "5. ✅ 初步結果對比\n",
    "\n",
    "### 下一步\n",
    "\n",
    "前往 **03-Analyze.ipynb** 進行深入的結果分析。\n",
    "\n",
    "---\n",
    "\n",
    "**觀察**:\n",
    "- Qwen-7B 通常在中文任務上表現更好\n",
    "- STEM 類別通常比人文類別更具挑戰性\n",
    "- 4-bit 量化可能略微降低準確率 (約 1-2%)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
