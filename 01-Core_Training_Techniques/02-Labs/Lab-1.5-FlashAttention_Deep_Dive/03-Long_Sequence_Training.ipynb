{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.5: 長序列訓練應用\n",
    "## Long Sequence Training with FlashAttention\n",
    "\n",
    "**學習目標**:\n",
    "- 訓練超長序列模型 (4K-8K tokens)\n",
    "- 對比不同序列長度的性能與記憶體\n",
    "- 處理長序列訓練的挑戰\n",
    "- 實作長文本理解任務\n",
    "\n",
    "**預計時間**: 60-90分鐘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. 環境設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# 檢查 FlashAttention\n",
    "try:\n",
    "    from flash_attn import flash_attn_qkvpacked_func\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"✅ FlashAttention 可用\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"❌ FlashAttention 未安裝 - 本實驗需要 FlashAttention\")\n",
    "    print(\"請先完成 01-Setup_and_Comparison.ipynb 中的安裝步驟\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n使用設備: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"總記憶體: {total_memory:.2f} GB\")\n",
    "    \n",
    "    if total_memory < 16:\n",
    "        print(\"\\n⚠️  警告: GPU 記憶體 <16GB, 可能無法完成所有長序列實驗\")\n",
    "        print(\"建議: 減小批次大小或序列長度\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. 長序列數據準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongSequenceDataset(Dataset):\n",
    "    \"\"\"長序列文本數據集\"\"\"\n",
    "    def __init__(self, tokenizer, num_samples=200, seq_length=2048):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # 生成長文本 (模擬長文檔)\n",
    "        base_text = (\n",
    "            \"In recent years, artificial intelligence has made remarkable progress. \"\n",
    "            \"Large language models like GPT have demonstrated impressive capabilities. \"\n",
    "            \"These models are trained on vast amounts of text data. \"\n",
    "            \"The attention mechanism is a key component of these architectures. \"\n",
    "            \"FlashAttention enables training on much longer sequences efficiently. \"\n",
    "        )\n",
    "        \n",
    "        # 根據目標長度重複文本\n",
    "        tokens_per_repeat = len(tokenizer.encode(base_text))\n",
    "        num_repeats = (seq_length // tokens_per_repeat) + 1\n",
    "        \n",
    "        self.texts = [base_text * num_repeats for _ in range(num_samples)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# 載入 tokenizer\n",
    "print(\"載入 GPT-2 Tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"\\n✅ 數據集類別準備完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. FlashAttention 模型準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    # 從之前的 notebook 導入 FlashAttention 層\n",
    "    class FlashAttentionLayer(nn.Module):\n",
    "        \"\"\"FlashAttention 層\"\"\"\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.embed_dim = config.n_embd\n",
    "            self.num_heads = config.n_head\n",
    "            self.head_dim = self.embed_dim // self.num_heads\n",
    "            \n",
    "            self.c_attn = nn.Linear(self.embed_dim, 3 * self.embed_dim)\n",
    "            self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "            \n",
    "            self.attn_dropout = config.attn_pdrop\n",
    "            self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        \n",
    "        def forward(self, hidden_states, layer_past=None, use_cache=False):\n",
    "            batch_size, seq_len, _ = hidden_states.size()\n",
    "            \n",
    "            qkv = self.c_attn(hidden_states)\n",
    "            qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "            \n",
    "            attn_output = flash_attn_qkvpacked_func(\n",
    "                qkv,\n",
    "                dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "                causal=True\n",
    "            )\n",
    "            \n",
    "            attn_output = attn_output.view(batch_size, seq_len, self.embed_dim)\n",
    "            attn_output = self.c_proj(attn_output)\n",
    "            attn_output = self.resid_dropout(attn_output)\n",
    "            \n",
    "            return (attn_output,)\n",
    "    \n",
    "    def create_flash_gpt2(seq_length, num_layers=6):\n",
    "        \"\"\"創建使用 FlashAttention 的 GPT-2 模型\"\"\"\n",
    "        config = GPT2Config(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            n_positions=seq_length,\n",
    "            n_embd=768,\n",
    "            n_layer=num_layers,\n",
    "            n_head=12,\n",
    "            resid_pdrop=0.1,\n",
    "            embd_pdrop=0.1,\n",
    "            attn_pdrop=0.1\n",
    "        )\n",
    "        \n",
    "        model = GPT2LMHeadModel(config)\n",
    "        \n",
    "        # 替換為 FlashAttention\n",
    "        for block in model.transformer.h:\n",
    "            old_attn = block.attn\n",
    "            new_attn = FlashAttentionLayer(config).to(model.device)\n",
    "            \n",
    "            # 複製權重\n",
    "            new_attn.c_attn.weight.data = old_attn.c_attn.weight.data.clone()\n",
    "            new_attn.c_attn.bias.data = old_attn.c_attn.bias.data.clone()\n",
    "            new_attn.c_proj.weight.data = old_attn.c_proj.weight.data.clone()\n",
    "            new_attn.c_proj.bias.data = old_attn.c_proj.bias.data.clone()\n",
    "            \n",
    "            block.attn = new_attn\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    print(\"✅ FlashAttention 模型創建函數準備完成\")\n",
    "else:\n",
    "    print(\"❌ 無法繼續 - 需要 FlashAttention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. 序列長度擴展實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"序列長度擴展實驗\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 測試不同序列長度\n",
    "    seq_lengths = [1024, 2048, 4096]\n",
    "    \n",
    "    # 根據 GPU 記憶體調整\n",
    "    if torch.cuda.is_available():\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        if total_memory < 16:\n",
    "            seq_lengths = [1024, 2048]  # 小記憶體 GPU\n",
    "            print(\"\\n⚠️  GPU 記憶體有限, 僅測試至 2048 序列長度\")\n",
    "        elif total_memory >= 24:\n",
    "            seq_lengths.append(8192)  # 大記憶體 GPU 可測試 8K\n",
    "            print(\"\\n✅ GPU 記憶體充足, 將測試至 8192 序列長度\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"測試序列長度: {seq_len}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # 創建數據集\n",
    "            dataset = LongSequenceDataset(\n",
    "                tokenizer,\n",
    "                num_samples=100,\n",
    "                seq_length=seq_len\n",
    "            )\n",
    "            \n",
    "            # 批次大小根據序列長度調整\n",
    "            if seq_len <= 1024:\n",
    "                batch_size = 4\n",
    "            elif seq_len <= 2048:\n",
    "                batch_size = 2\n",
    "            else:\n",
    "                batch_size = 1\n",
    "            \n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            print(f\"\\n配置:\")\n",
    "            print(f\"  序列長度: {seq_len}\")\n",
    "            print(f\"  批次大小: {batch_size}\")\n",
    "            print(f\"  數據集大小: {len(dataset)}\")\n",
    "            \n",
    "            # 創建模型\n",
    "            model = create_flash_gpt2(seq_length=seq_len, num_layers=4)\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # 優化器\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "            scaler = GradScaler()\n",
    "            \n",
    "            # 啟用梯度檢查點以節省記憶體\n",
    "            model.gradient_checkpointing_enable()\n",
    "            print(f\"  梯度檢查點: 啟用\")\n",
    "            \n",
    "            # 重置記憶體統計\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # 訓練\n",
    "            model.train()\n",
    "            num_steps = 20\n",
    "            losses = []\n",
    "            times = []\n",
    "            \n",
    "            dataloader_iter = iter(dataloader)\n",
    "            \n",
    "            print(f\"\\n開始訓練 ({num_steps} steps)...\")\n",
    "            pbar = tqdm(range(num_steps), desc=f\"Seq={seq_len}\")\n",
    "            \n",
    "            for step in pbar:\n",
    "                try:\n",
    "                    batch = next(dataloader_iter)\n",
    "                except StopIteration:\n",
    "                    dataloader_iter = iter(dataloader)\n",
    "                    batch = next(dataloader_iter)\n",
    "                \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with autocast(dtype=torch.float16):\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                step_time = time.time() - start_time\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                times.append(step_time)\n",
    "                \n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"time\": f\"{step_time:.2f}s\"})\n",
    "            \n",
    "            # 統計\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "            \n",
    "            results[seq_len] = {\n",
    "                \"avg_loss\": np.mean(losses),\n",
    "                \"avg_time\": np.mean(times),\n",
    "                \"peak_memory_gb\": peak_memory,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"losses\": losses,\n",
    "                \"times\": times\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n結果:\")\n",
    "            print(f\"  平均 Loss: {results[seq_len]['avg_loss']:.4f}\")\n",
    "            print(f\"  平均步時間: {results[seq_len]['avg_time']:.2f} 秒\")\n",
    "            print(f\"  峰值記憶體: {peak_memory:.2f} GB\")\n",
    "            print(f\"  ✅ 成功完成 {seq_len} 序列訓練\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"\\n  ❌ OOM - 序列長度 {seq_len} 超出記憶體限制\")\n",
    "                results[seq_len] = None\n",
    "                break  # 停止測試更長序列\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        finally:\n",
    "            # 清理\n",
    "            if 'model' in locals():\n",
    "                del model\n",
    "            if 'optimizer' in locals():\n",
    "                del optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"序列長度擴展實驗完成\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  跳過實驗 - FlashAttention 未安裝\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. 結果分析與視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE and results:\n",
    "    # 過濾成功的結果\n",
    "    valid_results = {k: v for k, v in results.items() if v is not None}\n",
    "    \n",
    "    if valid_results:\n",
    "        seq_lens = sorted(valid_results.keys())\n",
    "        avg_times = [valid_results[sl]['avg_time'] for sl in seq_lens]\n",
    "        peak_mems = [valid_results[sl]['peak_memory_gb'] for sl in seq_lens]\n",
    "        batch_sizes = [valid_results[sl]['batch_size'] for sl in seq_lens]\n",
    "        \n",
    "        # 創建視覺化\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(\"長序列訓練性能分析\", fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. 訓練時間 vs 序列長度\n",
    "        axes[0, 0].plot(seq_lens, avg_times, marker='o', linewidth=2, markersize=10, color='#3498db')\n",
    "        axes[0, 0].set_xlabel('序列長度')\n",
    "        axes[0, 0].set_ylabel('平均步時間 (秒)')\n",
    "        axes[0, 0].set_title('訓練時間 vs 序列長度', fontweight='bold')\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # 添加數值標籤\n",
    "        for x, y in zip(seq_lens, avg_times):\n",
    "            axes[0, 0].text(x, y, f'{y:.2f}s', ha='center', va='bottom')\n",
    "        \n",
    "        # 2. 記憶體使用 vs 序列長度\n",
    "        axes[0, 1].plot(seq_lens, peak_mems, marker='s', linewidth=2, markersize=10, color='#e74c3c')\n",
    "        axes[0, 1].set_xlabel('序列長度')\n",
    "        axes[0, 1].set_ylabel('峰值記憶體 (GB)')\n",
    "        axes[0, 1].set_title('記憶體使用 vs 序列長度', fontweight='bold')\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        for x, y in zip(seq_lens, peak_mems):\n",
    "            axes[0, 1].text(x, y, f'{y:.2f}GB', ha='center', va='bottom')\n",
    "        \n",
    "        # 3. Loss 曲線對比\n",
    "        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "        for i, sl in enumerate(seq_lens):\n",
    "            axes[1, 0].plot(\n",
    "                valid_results[sl]['losses'],\n",
    "                label=f'Seq={sl}',\n",
    "                linewidth=2,\n",
    "                color=colors[i % len(colors)],\n",
    "                alpha=0.8\n",
    "            )\n",
    "        axes[1, 0].set_xlabel('Step')\n",
    "        axes[1, 0].set_ylabel('Loss')\n",
    "        axes[1, 0].set_title('Loss 曲線對比', fontweight='bold')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # 4. 時間複雜度分析\n",
    "        # 理論上是 O(N²), 繪製趨勢線\n",
    "        axes[1, 1].scatter(seq_lens, avg_times, s=100, color='#3498db', label='實際時間', zorder=3)\n",
    "        \n",
    "        # 擬合 O(N²) 曲線\n",
    "        if len(seq_lens) >= 2:\n",
    "            # 計算擬合係數\n",
    "            seq_lens_arr = np.array(seq_lens)\n",
    "            avg_times_arr = np.array(avg_times)\n",
    "            \n",
    "            # 擬合 y = a*x^2\n",
    "            a = np.mean(avg_times_arr / (seq_lens_arr ** 2))\n",
    "            x_fit = np.linspace(min(seq_lens), max(seq_lens), 100)\n",
    "            y_fit = a * (x_fit ** 2)\n",
    "            \n",
    "            axes[1, 1].plot(x_fit, y_fit, '--', color='#e74c3c', linewidth=2, label='O(N²) 趨勢線', alpha=0.7)\n",
    "        \n",
    "        axes[1, 1].set_xlabel('序列長度')\n",
    "        axes[1, 1].set_ylabel('平均步時間 (秒)')\n",
    "        axes[1, 1].set_title('時間複雜度分析', fontweight='bold')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 打印統計表格\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"長序列訓練統計表\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'序列長度':<12} {'批次大小':<10} {'平均時間(s)':<15} {'峰值記憶體(GB)':<18} {'平均Loss':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        for sl in seq_lens:\n",
    "            r = valid_results[sl]\n",
    "            print(f\"{sl:<12} {r['batch_size']:<10} {r['avg_time']:<15.2f} {r['peak_memory_gb']:<18.2f} {r['avg_loss']:<12.4f}\")\n",
    "        \n",
    "        # 擴展性分析\n",
    "        if len(seq_lens) >= 2:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"擴展性分析\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            base_sl = seq_lens[0]\n",
    "            base_time = valid_results[base_sl]['avg_time']\n",
    "            base_mem = valid_results[base_sl]['peak_memory_gb']\n",
    "            \n",
    "            for sl in seq_lens[1:]:\n",
    "                time_ratio = valid_results[sl]['avg_time'] / base_time\n",
    "                mem_ratio = valid_results[sl]['peak_memory_gb'] / base_mem\n",
    "                seq_ratio = sl / base_sl\n",
    "                \n",
    "                print(f\"\\n{base_sl} → {sl} (序列長度 {seq_ratio:.1f}x):\")\n",
    "                print(f\"  時間增長: {time_ratio:.2f}x (理論 O(N²): {seq_ratio**2:.2f}x)\")\n",
    "                print(f\"  記憶體增長: {mem_ratio:.2f}x (理論 O(N): {seq_ratio:.2f}x)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️  沒有成功的訓練結果可分析\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  無法進行分析 - FlashAttention 未安裝或無測試結果\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. 長文本理解任務示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"長文本生成示例\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 創建一個訓練好的長序列模型\n",
    "    print(\"\\n創建長序列模型 (支援 2048 tokens)...\")\n",
    "    model_long = create_flash_gpt2(seq_length=2048, num_layers=4)\n",
    "    model_long = model_long.to(device)\n",
    "    model_long.eval()\n",
    "    \n",
    "    # 準備長文本輸入\n",
    "    long_prompt = (\n",
    "        \"Artificial intelligence has revolutionized many fields. \"\n",
    "        \"In natural language processing, large language models like GPT have shown remarkable capabilities. \"\n",
    "        \"These models can understand and generate human-like text. \"\n",
    "        \"The attention mechanism allows them to capture long-range dependencies. \"\n",
    "        \"FlashAttention makes it possible to train on much longer sequences. \"\n",
    "        \"This breakthrough enables new applications like \" \n",
    "    )\n",
    "    \n",
    "    print(f\"\\n輸入文本長度: {len(tokenizer.encode(long_prompt))} tokens\")\n",
    "    print(f\"\\n輸入文本:\")\n",
    "    print(f\"{long_prompt}\")\n",
    "    \n",
    "    # 生成\n",
    "    print(\"\\n生成文本...\")\n",
    "    input_ids = tokenizer.encode(long_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model_long.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n生成結果:\")\n",
    "    print(f\"{generated_text}\")\n",
    "    print(f\"\\n總長度: {len(output[0])} tokens\")\n",
    "    \n",
    "    # 清理\n",
    "    del model_long\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n✅ 長文本生成示例完成\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  跳過示例 - FlashAttention 未安裝\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. 實驗總結\n",
    "\n",
    "### 關鍵發現\n",
    "\n",
    "通過本實驗, 我們驗證了:\n",
    "\n",
    "1. **FlashAttention 使長序列訓練成為可能**:\n",
    "   - 標準 Attention: 通常在 1K-2K 序列時 OOM\n",
    "   - FlashAttention: 可訓練 4K-8K 甚至更長序列\n",
    "   - 關鍵技術: O(N) 記憶體複雜度\n",
    "\n",
    "2. **時間複雜度仍為 O(N²)**:\n",
    "   - FlashAttention 優化的是記憶體訪問, 不是計算量\n",
    "   - 序列長度翻倍 → 訓練時間增加 ~4倍\n",
    "   - 但通過 IO 優化, 實際增長略小於理論值\n",
    "\n",
    "3. **記憶體增長接近線性 O(N)**:\n",
    "   - 主要記憶體占用來自模型參數和激活值\n",
    "   - Attention 不再是記憶體瓶頸\n",
    "   - 配合梯度檢查點, 記憶體效率更高\n",
    "\n",
    "4. **批次大小需要調整**:\n",
    "   - 長序列需要減小批次以控制記憶體\n",
    "   - 可使用梯度累積維持有效批次大小\n",
    "   - 512: bs=8, 1K: bs=4, 2K: bs=2, 4K+: bs=1\n",
    "\n",
    "### 長序列訓練最佳實踐\n",
    "\n",
    "#### 1. 記憶體優化組合\n",
    "```python\n",
    "# 推薦配置\n",
    "model.gradient_checkpointing_enable()  # 梯度檢查點\n",
    "use FlashAttention                      # FlashAttention\n",
    "use FP16/BF16                          # 混合精度\n",
    "use gradient accumulation              # 梯度累積\n",
    "\n",
    "# 可在 16GB GPU 上訓練 4K 序列\n",
    "```\n",
    "\n",
    "#### 2. 序列長度選擇\n",
    "- **文檔理解**: 2K-4K tokens\n",
    "- **長對話**: 4K-8K tokens\n",
    "- **代碼理解**: 4K-8K tokens\n",
    "- **書籍分析**: 8K-32K tokens (需要特殊優化)\n",
    "\n",
    "#### 3. Position Encoding 考慮\n",
    "- **RoPE** (Rotary Position Embedding): 適合長序列\n",
    "- **ALiBi**: 可外推到更長序列\n",
    "- **標準 Sinusoidal**: 受限於訓練長度\n",
    "\n",
    "#### 4. 訓練策略\n",
    "```python\n",
    "# 漸進式長度增加\n",
    "Phase 1: Train on 512 tokens  (50% steps)\n",
    "Phase 2: Train on 1024 tokens (30% steps)\n",
    "Phase 3: Train on 2048 tokens (20% steps)\n",
    "\n",
    "# 好處: 加快訓練, 提升穩定性\n",
    "```\n",
    "\n",
    "### 實際應用場景\n",
    "\n",
    "1. **長文檔摘要**\n",
    "   - 輸入: 完整文章/報告 (2K-4K tokens)\n",
    "   - 輸出: 結構化摘要\n",
    "\n",
    "2. **長對話理解**\n",
    "   - 輸入: 完整對話歷史 (4K-8K tokens)\n",
    "   - 輸出: 上下文相關回覆\n",
    "\n",
    "3. **代碼理解與生成**\n",
    "   - 輸入: 完整代碼文件 (4K-8K tokens)\n",
    "   - 輸出: 代碼解釋/補全\n",
    "\n",
    "4. **多文檔問答**\n",
    "   - 輸入: 多個文檔片段 (4K-16K tokens)\n",
    "   - 輸出: 綜合答案\n",
    "\n",
    "### 限制與挑戰\n",
    "\n",
    "1. **計算成本**:\n",
    "   - O(N²) 時間複雜度仍然存在\n",
    "   - 超長序列 (>16K) 訓練非常慢\n",
    "\n",
    "2. **推理延遲**:\n",
    "   - 長序列推理仍需大量計算\n",
    "   - 需要配合 KV Cache 優化\n",
    "\n",
    "3. **硬體需求**:\n",
    "   - 8K+ 序列仍需大記憶體 GPU\n",
    "   - 多GPU訓練更佳\n",
    "\n",
    "### 未來方向\n",
    "\n",
    "1. **Linear Attention**: O(N) 時間複雜度\n",
    "2. **Sparse Attention**: 減少計算量\n",
    "3. **Hierarchical Attention**: 分層處理\n",
    "4. **Flash-Decoding**: 優化推理\n",
    "\n",
    "### 下一步\n",
    "\n",
    "完成本實驗後, 建議:\n",
    "1. **04-Performance_Analysis.ipynb**: 深入性能分析\n",
    "2. **實際項目應用**: 在自己的長文本任務中使用 FlashAttention\n",
    "3. **Lab-1.6**: 學習 MQA/GQA 進一步優化推理效率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
