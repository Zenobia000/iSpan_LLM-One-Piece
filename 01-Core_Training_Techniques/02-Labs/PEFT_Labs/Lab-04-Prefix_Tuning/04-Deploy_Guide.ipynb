{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Prefix Tuning - Deployment Guide\n",
    "\n",
    "## ğŸ¯ å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬ Notebook æ¢è¨ **Prefix Tuning åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­çš„éƒ¨ç½²ç­–ç•¥**ã€‚èˆ‡ LoRA å’Œ IAÂ³ ç­‰å¯å®Œå…¨åˆä½µçš„æ–¹æ³•ä¸åŒ,Prefix Tuning çš„å‰ç¶´åƒæ•¸**ç„¡æ³•ç›´æ¥åˆä½µåˆ°åŸºç¤æ¨¡å‹ä¸­**,å› æ­¤éœ€è¦ç‰¹æ®Šçš„éƒ¨ç½²æ–¹æ¡ˆã€‚\n",
    "\n",
    "### é—œéµå­¸ç¿’è¦é»\n",
    "- ç†è§£ Prefix Tuning ç„¡æ³•åˆä½µçš„æŠ€è¡“åŸå› \n",
    "- æŒæ¡å‰ç¶´åƒæ•¸çš„å„ªåŒ–èˆ‡å£“ç¸®æŠ€è¡“\n",
    "- å¯¦ç¾é«˜æ•ˆçš„ Adapter è¼‰å…¥èˆ‡æ¨ç†éƒ¨ç½²\n",
    "- åˆ†æä¸åŒéƒ¨ç½²å ´æ™¯ä¸‹çš„æœ€ä½³å¯¦è¸\n",
    "- å­¸ç¿’å¤šä»»å‹™å‰ç¶´çš„ç®¡ç†ç­–ç•¥\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prefix Tuning éƒ¨ç½²ç‰¹æ€§åˆ†æ\n",
    "\n",
    "### 1.1 ç‚ºä»€éº¼ Prefix Tuning ç„¡æ³•åˆä½µ?\n",
    "\n",
    "**æŠ€è¡“åŸå› **:\n",
    "- **çµæ§‹æ€§ä¿®æ”¹**: Prefix åœ¨æ¯ä¸€å±¤çš„ Key/Value éƒ½æ³¨å…¥æ–°çš„å‘é‡,æ”¹è®Šäº†æ³¨æ„åŠ›æ©Ÿåˆ¶çš„è¨ˆç®—çµæ§‹\n",
    "- **å‹•æ…‹æ³¨å…¥**: Prefix å‘é‡åœ¨æ¨ç†æ™‚å‹•æ…‹æ‹¼æ¥åˆ°åŸå§‹åºåˆ—,ç„¡æ³•éœæ…‹èåˆåˆ°æ¬Šé‡çŸ©é™£\n",
    "- **ä½ç½®ä¾è³´**: Prefix ä½ç½®å°æ¨¡å‹è¡Œç‚ºæœ‰é¡¯è‘—å½±éŸ¿,åˆä½µæœƒç ´å£é€™ç¨®ä½ç½®é—œä¿‚\n",
    "\n",
    "**èˆ‡å¯åˆä½µæ–¹æ³•çš„å°æ¯”**:\n",
    "\n",
    "| æ–¹æ³• | æ˜¯å¦å¯åˆä½µ | åŸç† | æ¨ç†é–‹éŠ· |\n",
    "|:---|:---|:---|:---|\n",
    "| **LoRA** | âœ… å¯åˆä½µ | `W' = W + BÂ·A` çŸ©é™£åŠ æ³• | é›¶é–‹éŠ· |\n",
    "| **IAÂ³** | âœ… å¯åˆä½µ | `W' = W âŠ™ scale` å…ƒç´ ä¹˜æ³• | é›¶é–‹éŠ· |\n",
    "| **Prefix Tuning** | âŒ **ä¸å¯åˆä½µ** | å‹•æ…‹ KV æ³¨å…¥ | **æœ‰é–‹éŠ·** |\n",
    "| **Prompt Tuning** | âŒ ä¸å¯åˆä½µ | è¼¸å…¥åºåˆ—æ‹¼æ¥ | æœ‰é–‹éŠ· |\n",
    "| **Adapter** | âŒ ä¸å¯åˆä½µ | æ–°å¢æ¨¡çµ„å±¤ | æœ‰é–‹éŠ· |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç’°å¢ƒæº–å‚™èˆ‡æ¨¡å‹è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„åº«\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel, \n",
    "    PrefixTuningConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "# è¨­å®šè¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 è¼‰å…¥è¨“ç·´å¥½çš„ Prefix Tuning æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è·¯å¾‘è¨­å®š\n",
    "base_model_name = \"gpt2\"\n",
    "adapter_path = \"./gpt2-prefix-tuning-imdb\"  # å‡è¨­é€™æ˜¯è¨“ç·´å¥½çš„ adapter è·¯å¾‘\n",
    "\n",
    "# è¼‰å…¥åŸºç¤æ¨¡å‹å’Œ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"åŸºç¤æ¨¡å‹åƒæ•¸é‡: {base_model.num_parameters():,}\")\n",
    "print(f\"åŸºç¤æ¨¡å‹è¨˜æ†¶é«”ä½”ç”¨: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ Prefix Tuning adapter (å¦‚æœå­˜åœ¨)\n",
    "if os.path.exists(adapter_path):\n",
    "    # æ‰¾åˆ°æœ€æ–°çš„ checkpoint\n",
    "    checkpoints = [d for d in os.listdir(adapter_path) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(\n",
    "            [os.path.join(adapter_path, d) for d in checkpoints],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
    "        print(f\"æˆåŠŸè¼‰å…¥ Prefix Tuning adapter: {latest_checkpoint}\")\n",
    "    else:\n",
    "        peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        print(f\"æˆåŠŸè¼‰å…¥ Prefix Tuning adapter: {adapter_path}\")\n",
    "else:\n",
    "    # å¦‚æœæ²’æœ‰è¨“ç·´å¥½çš„ adapter,å‰µå»ºç¤ºç¯„é…ç½®\n",
    "    print(\"æœªæ‰¾åˆ°è¨“ç·´å¥½çš„ adapter,å‰µå»ºç¤ºç¯„ Prefix Tuning é…ç½®...\")\n",
    "    \n",
    "    prefix_config = PrefixTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        num_virtual_tokens=20,\n",
    "        prefix_projection=True,\n",
    "        encoder_hidden_size=768\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(base_model, prefix_config)\n",
    "    print(\"å‰µå»ºäº†ç¤ºç¯„ Prefix Tuning æ¨¡å‹\")\n",
    "\n",
    "# é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸çµ±è¨ˆ\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prefix Tuning çµæ§‹åˆ†æ\n",
    "\n",
    "### 3.1 å‰ç¶´åƒæ•¸çµæ§‹æ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prefix_structure(model):\n",
    "    \"\"\"åˆ†æ Prefix Tuning çš„çµæ§‹å’Œåƒæ•¸\"\"\"\n",
    "    prefix_params = {}\n",
    "    total_prefix_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prefix' in name.lower() or 'prompt' in name.lower():\n",
    "            prefix_params[name] = {\n",
    "                'shape': param.shape,\n",
    "                'dtype': param.dtype,\n",
    "                'requires_grad': param.requires_grad,\n",
    "                'device': param.device,\n",
    "                'num_params': param.numel()\n",
    "            }\n",
    "            total_prefix_params += param.numel()\n",
    "            \n",
    "            print(f\"Prefix åƒæ•¸: {name}\")\n",
    "            print(f\"  å½¢ç‹€: {param.shape}\")\n",
    "            print(f\"  æ•¸æ“šé¡å‹: {param.dtype}\")\n",
    "            print(f\"  åƒæ•¸é‡: {param.numel():,}\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"ç¸½ Prefix åƒæ•¸é‡: {total_prefix_params:,}\")\n",
    "    print(f\"ä½”åŸºç¤æ¨¡å‹åƒæ•¸æ¯”ä¾‹: {total_prefix_params / base_model.num_parameters() * 100:.4f}%\")\n",
    "    \n",
    "    return prefix_params\n",
    "\n",
    "prefix_structure = analyze_prefix_structure(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å‰ç¶´åƒæ•¸è¨˜æ†¶é«”åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prefix_memory(prefix_params):\n",
    "    \"\"\"è¨ˆç®—å‰ç¶´åƒæ•¸çš„è¨˜æ†¶é«”ä½”ç”¨\"\"\"\n",
    "    total_memory = 0\n",
    "    \n",
    "    for name, info in prefix_params.items():\n",
    "        # æ ¹æ“šæ•¸æ“šé¡å‹è¨ˆç®—è¨˜æ†¶é«”\n",
    "        dtype_size = {\n",
    "            torch.float32: 4,\n",
    "            torch.float16: 2,\n",
    "            torch.bfloat16: 2,\n",
    "            torch.int32: 4,\n",
    "            torch.int64: 8\n",
    "        }\n",
    "        \n",
    "        size_bytes = info['num_params'] * dtype_size.get(info['dtype'], 4)\n",
    "        total_memory += size_bytes\n",
    "        \n",
    "        print(f\"{name}: {size_bytes / 1024:.2f} KB\")\n",
    "    \n",
    "    print(f\"\\nç¸½å‰ç¶´åƒæ•¸è¨˜æ†¶é«”: {total_memory / 1024:.2f} KB ({total_memory / 1024**2:.4f} MB)\")\n",
    "    return total_memory\n",
    "\n",
    "prefix_memory = calculate_prefix_memory(prefix_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
    "\n",
    "### 4.1 æ€§èƒ½æ¸¬è©¦å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, test_prompts, num_runs=5):\n",
    "    \"\"\"æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                if run == 0:  # åªåœ¨ç¬¬ä¸€è¼ªä¿å­˜çµæœ\n",
    "                    results.append({\n",
    "                        'prompt': prompt,\n",
    "                        'generated': generated_text[len(prompt):].strip()\n",
    "                    })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    return avg_time, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 åŸ·è¡Œæ¨ç†æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦æç¤ºè©\n",
    "test_prompts = [\n",
    "    \"This movie was absolutely\",\n",
    "    \"I think this film was\",\n",
    "    \"The acting in this movie\",\n",
    "    \"Overall, my impression is\"\n",
    "]\n",
    "\n",
    "print(\"=== å¸¶ Prefix çš„æ¨ç†æ€§èƒ½æ¸¬è©¦ ===\")\n",
    "prefix_time, prefix_results = benchmark_inference(peft_model, tokenizer, test_prompts)\n",
    "print(f\"å¹³å‡æ¨ç†æ™‚é–“: {prefix_time:.4f} ç§’\")\n",
    "print(f\"è¨˜æ†¶é«”ä½¿ç”¨: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== ç”Ÿæˆç¤ºä¾‹ ===\")\n",
    "for i, result in enumerate(prefix_results[:2]):  # åªé¡¯ç¤ºå‰å…©å€‹ç¤ºä¾‹\n",
    "    print(f\"æç¤ºè© {i+1}: {result['prompt']}\")\n",
    "    print(f\"ç”Ÿæˆ: {result['generated']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 èˆ‡åŸºç¤æ¨¡å‹å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== åŸºç¤æ¨¡å‹æ¨ç†æ€§èƒ½æ¸¬è©¦ ===\")\n",
    "base_time, base_results = benchmark_inference(base_model, tokenizer, test_prompts)\n",
    "print(f\"å¹³å‡æ¨ç†æ™‚é–“: {base_time:.4f} ç§’\")\n",
    "print(f\"è¨˜æ†¶é«”ä½¿ç”¨: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "# è¨ˆç®—æ€§èƒ½é–‹éŠ·\n",
    "time_overhead = (prefix_time - base_time) / base_time * 100\n",
    "memory_overhead = (peft_model.get_memory_footprint() - base_model.get_memory_footprint()) / base_model.get_memory_footprint() * 100\n",
    "\n",
    "print(\"\\n=== æ€§èƒ½é–‹éŠ·åˆ†æ ===\")\n",
    "print(f\"æ¨ç†æ™‚é–“å¢åŠ : {time_overhead:.2f}%\")\n",
    "print(f\"è¨˜æ†¶é«”ä½¿ç”¨å¢åŠ : {memory_overhead:.2f}%\")\n",
    "\n",
    "print(\"\\n=== åŸºç¤æ¨¡å‹ç”Ÿæˆç¤ºä¾‹ ===\")\n",
    "for i, result in enumerate(base_results[:2]):\n",
    "    print(f\"æç¤ºè© {i+1}: {result['prompt']}\")\n",
    "    print(f\"ç”Ÿæˆ: {result['generated']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prefix åƒæ•¸å„ªåŒ–èˆ‡å£“ç¸®\n",
    "\n",
    "### 5.1 ç§»é™¤è¨“ç·´æ™‚çš„ MLP é‡åƒæ•¸åŒ–å±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_prefix_for_inference(model):\n",
    "    \"\"\"\n",
    "    å„ªåŒ– Prefix Tuning æ¨¡å‹ä»¥é€²è¡Œæ¨ç†\n",
    "    åœ¨è¨“ç·´éšæ®µ,ä½¿ç”¨ MLP é‡åƒæ•¸åŒ–ä¾†ç©©å®šè¨“ç·´\n",
    "    åœ¨æ¨ç†éšæ®µ,å¯ä»¥ç§»é™¤ MLP,ç›´æ¥ä½¿ç”¨é è¨ˆç®—çš„ prefix\n",
    "    \"\"\"\n",
    "    print(\"=== Prefix æ¨ç†å„ªåŒ– ===\")\n",
    "    print(\"æ³¨æ„: æ­¤æ­¥é©Ÿé€šå¸¸åœ¨ä¿å­˜æ¨¡å‹æ™‚è‡ªå‹•å®Œæˆ\")\n",
    "    print(\"PEFT åº«æœƒè‡ªå‹•è™•ç† MLP çš„ç§»é™¤å’Œ prefix çš„é è¨ˆç®—\")\n",
    "    \n",
    "    # é¡¯ç¤ºç•¶å‰æ¨¡å‹çµæ§‹\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nç¸½åƒæ•¸é‡: {total_params:,}\")\n",
    "    print(f\"å¯è¨“ç·´åƒæ•¸é‡: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "optimized_model = optimize_prefix_for_inference(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prefix åƒæ•¸é‡åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_prefix_parameters(model, target_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    é‡åŒ–å‰ç¶´åƒæ•¸ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨\n",
    "    æ³¨æ„: é€™æœƒä¿®æ”¹æ¨¡å‹,è«‹ç¢ºä¿å·²ä¿å­˜åŸå§‹ç‰ˆæœ¬\n",
    "    \"\"\"\n",
    "    print(f\"=== é‡åŒ– Prefix åƒæ•¸åˆ° {target_dtype} ===\")\n",
    "    \n",
    "    quantized_params = 0\n",
    "    original_memory = 0\n",
    "    quantized_memory = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prefix' in name.lower() or 'prompt' in name.lower():\n",
    "            original_dtype = param.dtype\n",
    "            original_size = param.numel() * param.element_size()\n",
    "            \n",
    "            # é‡åŒ–åƒæ•¸\n",
    "            param.data = param.data.to(target_dtype)\n",
    "            \n",
    "            quantized_size = param.numel() * param.element_size()\n",
    "            quantized_params += 1\n",
    "            original_memory += original_size\n",
    "            quantized_memory += quantized_size\n",
    "            \n",
    "            print(f\"é‡åŒ– {name}: {original_dtype} -> {target_dtype}\")\n",
    "            print(f\"  è¨˜æ†¶é«”: {original_size / 1024:.2f} KB -> {quantized_size / 1024:.2f} KB\")\n",
    "    \n",
    "    compression_ratio = (original_memory - quantized_memory) / original_memory * 100\n",
    "    print(f\"\\né‡åŒ–äº† {quantized_params} å€‹ prefix åƒæ•¸\")\n",
    "    print(f\"è¨˜æ†¶é«”å£“ç¸®ç‡: {compression_ratio:.2f}%\")\n",
    "    print(f\"ç¯€çœè¨˜æ†¶é«”: {(original_memory - quantized_memory) / 1024:.2f} KB\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# æ¼”ç¤ºé‡åŒ– (å¯¦éš›ä½¿ç”¨æ™‚è«‹è¬¹æ…,å¯èƒ½å½±éŸ¿æ€§èƒ½)\n",
    "print(\"æ³¨æ„: é‡åŒ–æœƒä¿®æ”¹æ¨¡å‹,é€™è£¡åƒ…ä½œæ¼”ç¤º\")\n",
    "print(\"å¯¦éš›éƒ¨ç½²æ™‚è«‹å…ˆåœ¨é©—è­‰é›†ä¸Šæ¸¬è©¦é‡åŒ–å¾Œçš„æ€§èƒ½\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹ä¿å­˜èˆ‡éƒ¨ç½²\n",
    "\n",
    "### 6.1 ä¿å­˜ Prefix Tuning Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šä¿å­˜è·¯å¾‘\n",
    "save_path = \"./gpt2-prefix-tuning-deployed\"\n",
    "\n",
    "print(f\"=== ä¿å­˜ Prefix Tuning Adapter åˆ°: {save_path} ===\")\n",
    "\n",
    "# ä¿å­˜ PEFT adapter (åªä¿å­˜å‰ç¶´åƒæ•¸)\n",
    "peft_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"âœ… Adapter ä¿å­˜æˆåŠŸ!\")\n",
    "\n",
    "# æª¢æŸ¥ä¿å­˜çš„æ–‡ä»¶\n",
    "saved_files = os.listdir(save_path)\n",
    "print(f\"\\nä¿å­˜çš„æ–‡ä»¶: {saved_files}\")\n",
    "\n",
    "# è¨ˆç®— adapter å¤§å°\n",
    "adapter_size = 0\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(save_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        adapter_size += size\n",
    "        print(f\"  {file}: {size / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nAdapter ç¸½å¤§å°: {adapter_size / 1024:.2f} KB ({adapter_size / 1024**2:.4f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 é©—è­‰ä¿å­˜çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== é©—è­‰ä¿å­˜çš„ Adapter ===\")\n",
    "\n",
    "# é‡æ–°è¼‰å…¥åŸºç¤æ¨¡å‹\n",
    "fresh_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# å¾ä¿å­˜è·¯å¾‘è¼‰å…¥ adapter\n",
    "reloaded_model = PeftModel.from_pretrained(fresh_base_model, save_path)\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "print(f\"âœ… æˆåŠŸè¼‰å…¥ä¿å­˜çš„ adapter\")\n",
    "\n",
    "# å¿«é€Ÿæ¸¬è©¦\n",
    "test_prompt = \"This movie is absolutely\"\n",
    "inputs = reloaded_tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=reloaded_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = reloaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\næ¸¬è©¦ç”Ÿæˆ:\")\n",
    "print(f\"æç¤ºè©: {test_prompt}\")\n",
    "print(f\"ç”Ÿæˆ: {generated_text[len(test_prompt):].strip()}\")\n",
    "print(\"\\nâœ… ä¿å­˜çš„ adapter é‹ä½œæ­£å¸¸!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²ç­–ç•¥\n",
    "\n",
    "### 7.1 éƒ¨ç½²æ¶æ§‹é¸æ“‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_strategies = {\n",
    "    \"å–®ä»»å‹™éƒ¨ç½²\": {\n",
    "        \"æè¿°\": \"ä¸€å€‹æœå‹™åªæœå‹™ä¸€å€‹ä»»å‹™\",\n",
    "        \"å„ªé»\": [\n",
    "            \"éƒ¨ç½²ç°¡å–®\",\n",
    "            \"æ¨ç†é€Ÿåº¦æœ€å¿«\",\n",
    "            \"è³‡æºéš”é›¢\"\n",
    "        ],\n",
    "        \"ç¼ºé»\": [\n",
    "            \"è³‡æºåˆ©ç”¨ç‡ä½\",\n",
    "            \"éœ€è¦å¤šå€‹æ¨¡å‹å¯¦ä¾‹\"\n",
    "        ],\n",
    "        \"é©ç”¨å ´æ™¯\": \"å–®ä¸€ä»»å‹™ã€é«˜QPSéœ€æ±‚\"\n",
    "    },\n",
    "    \"å¤šä»»å‹™å…±äº«åŸºç¤æ¨¡å‹\": {\n",
    "        \"æè¿°\": \"å…±äº«åŸºç¤æ¨¡å‹,å‹•æ…‹åˆ‡æ› prefix\",\n",
    "        \"å„ªé»\": [\n",
    "            \"è¨˜æ†¶é«”æ•ˆç‡é«˜\",\n",
    "            \"æ”¯æŒå¤šä»»å‹™\",\n",
    "            \"prefix åˆ‡æ›é–‹éŠ·å°\"\n",
    "        ],\n",
    "        \"ç¼ºé»\": [\n",
    "            \"éœ€è¦ prefix ç®¡ç†é‚è¼¯\",\n",
    "            \"æ‰¹æ¬¡è™•ç†è¤‡é›œ\"\n",
    "        ],\n",
    "        \"é©ç”¨å ´æ™¯\": \"å¤šä»»å‹™ã€ä¸­ç­‰QPS\"\n",
    "    },\n",
    "    \"æ‰¹æ¬¡æ¨ç†å„ªåŒ–\": {\n",
    "        \"æè¿°\": \"ç›¸åŒä»»å‹™æ‰¹æ¬¡è™•ç†\",\n",
    "        \"å„ªé»\": [\n",
    "            \"ååé‡æœ€å¤§åŒ–\",\n",
    "            \"GPU åˆ©ç”¨ç‡é«˜\"\n",
    "        ],\n",
    "        \"ç¼ºé»\": [\n",
    "            \"å»¶é²å¢åŠ \",\n",
    "            \"éœ€è¦è«‹æ±‚æ’éšŠ\"\n",
    "        ],\n",
    "        \"é©ç”¨å ´æ™¯\": \"é›¢ç·šæ¨ç†ã€é«˜ååé‡éœ€æ±‚\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== Prefix Tuning éƒ¨ç½²ç­–ç•¥å°æ¯” ===\")\n",
    "for strategy, details in deployment_strategies.items():\n",
    "    print(f\"\\nã€{strategy}ã€‘\")\n",
    "    print(f\"æè¿°: {details['æè¿°']}\")\n",
    "    print(f\"å„ªé»: {', '.join(details['å„ªé»'])}\")\n",
    "    print(f\"ç¼ºé»: {', '.join(details['ç¼ºé»'])}\")\n",
    "    print(f\"é©ç”¨å ´æ™¯: {details['é©ç”¨å ´æ™¯']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 å¤šä»»å‹™ Prefix ç®¡ç†ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskPrefixManager:\n",
    "    \"\"\"\n",
    "    å¤šä»»å‹™ Prefix ç®¡ç†å™¨\n",
    "    ç”¨æ–¼åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ç®¡ç†å’Œåˆ‡æ›ä¸åŒä»»å‹™çš„ prefix\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, tokenizer):\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_adapters = {}  # ä»»å‹™åç¨± -> adapter è·¯å¾‘\n",
    "        self.current_task = None\n",
    "        self.current_model = None\n",
    "        \n",
    "    def register_task(self, task_name, adapter_path):\n",
    "        \"\"\"è¨»å†Šä¸€å€‹ä»»å‹™çš„ adapter\"\"\"\n",
    "        self.task_adapters[task_name] = adapter_path\n",
    "        print(f\"è¨»å†Šä»»å‹™: {task_name} -> {adapter_path}\")\n",
    "        \n",
    "    def switch_task(self, task_name):\n",
    "        \"\"\"åˆ‡æ›åˆ°æŒ‡å®šä»»å‹™\"\"\"\n",
    "        if task_name not in self.task_adapters:\n",
    "            raise ValueError(f\"æœªçŸ¥ä»»å‹™: {task_name}\")\n",
    "            \n",
    "        if task_name == self.current_task:\n",
    "            print(f\"å·²ç¶“åœ¨ä»»å‹™ {task_name} ä¸Š,ç„¡éœ€åˆ‡æ›\")\n",
    "            return\n",
    "            \n",
    "        print(f\"åˆ‡æ›åˆ°ä»»å‹™: {task_name}\")\n",
    "        adapter_path = self.task_adapters[task_name]\n",
    "        \n",
    "        # è¼‰å…¥å°æ‡‰çš„ adapter\n",
    "        self.current_model = PeftModel.from_pretrained(\n",
    "            self.base_model, \n",
    "            adapter_path\n",
    "        )\n",
    "        self.current_task = task_name\n",
    "        \n",
    "    def generate(self, prompt, task_name=None, **generate_kwargs):\n",
    "        \"\"\"ä½¿ç”¨æŒ‡å®šä»»å‹™çš„ prefix ç”Ÿæˆæ–‡æœ¬\"\"\"\n",
    "        if task_name:\n",
    "            self.switch_task(task_name)\n",
    "            \n",
    "        if self.current_model is None:\n",
    "            raise RuntimeError(\"è«‹å…ˆåˆ‡æ›åˆ°ä¸€å€‹ä»»å‹™\")\n",
    "            \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = self.current_model.generate(\n",
    "                **inputs,\n",
    "                **generate_kwargs\n",
    "            )\n",
    "            \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def list_tasks(self):\n",
    "        \"\"\"åˆ—å‡ºæ‰€æœ‰å·²è¨»å†Šçš„ä»»å‹™\"\"\"\n",
    "        return list(self.task_adapters.keys())\n",
    "\n",
    "# æ¼”ç¤ºä½¿ç”¨\n",
    "print(\"=== å¤šä»»å‹™ Prefix ç®¡ç†å™¨ç¤ºä¾‹ ===\")\n",
    "print(\"\\né€™æ˜¯ä¸€å€‹ç®¡ç†å¤šå€‹ä»»å‹™ prefix çš„æ¡†æ¶\")\n",
    "print(\"åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­,å¯ä»¥é å…ˆè¼‰å…¥å¤šå€‹ä»»å‹™çš„ adapter\")\n",
    "print(\"æ ¹æ“šè«‹æ±‚å‹•æ…‹åˆ‡æ›,ç„¡éœ€é‡æ–°è¼‰å…¥æ•´å€‹æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºéƒ¨ç½²é…ç½®æ–‡ä»¶\n",
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"peft_method\": \"Prefix Tuning\",\n",
    "        \"num_virtual_tokens\": 20,\n",
    "        \"mergeable\": False,\n",
    "        \"adapter_size_mb\": adapter_size / 1024**2\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"inference_time_seconds\": prefix_time,\n",
    "        \"vs_base_model_overhead_percent\": time_overhead,\n",
    "        \"memory_overhead_percent\": memory_overhead,\n",
    "        \"adapter_loading_time_seconds\": \"< 1s (estimated)\"\n",
    "    },\n",
    "    \"deployment_requirements\": {\n",
    "        \"python_packages\": [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.20.0\",\n",
    "            \"peft>=0.3.0\",\n",
    "            \"tokenizers>=0.12.0\"\n",
    "        ],\n",
    "        \"minimum_gpu_memory_gb\": 4,\n",
    "        \"recommended_gpu_memory_gb\": 8\n",
    "    },\n",
    "    \"deployment_notes\": {\n",
    "        \"adapter_management\": \"éœ€è¦åŒæ™‚ç®¡ç†åŸºç¤æ¨¡å‹å’Œ adapter\",\n",
    "        \"inference_overhead\": \"å­˜åœ¨æ¨ç†é–‹éŠ·,ä¸å¯æ¶ˆé™¤\",\n",
    "        \"multi_task_support\": \"å¯ä»¥å…±äº«åŸºç¤æ¨¡å‹,å‹•æ…‹åˆ‡æ› adapter\",\n",
    "        \"scalability\": \"é©åˆå¤šä»»å‹™å ´æ™¯,å–®ä»»å‹™å»ºè­°è€ƒæ…® LoRA æˆ– IAÂ³\"\n",
    "    },\n",
    "    \"usage_example\": {\n",
    "        \"load_base_model\": f\"base_model = AutoModelForCausalLM.from_pretrained('{base_model_name}')\",\n",
    "        \"load_adapter\": f\"model = PeftModel.from_pretrained(base_model, '{save_path}')\",\n",
    "        \"inference\": \"model.generate(...)\"\n",
    "    },\n",
    "    \"optimization_tips\": [\n",
    "        \"ä½¿ç”¨ FP16 æˆ– BF16 æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨\",\n",
    "        \"ç›¸åŒä»»å‹™çš„è«‹æ±‚æ‰¹æ¬¡è™•ç†\",\n",
    "        \"é è¼‰å…¥å¸¸ç”¨ä»»å‹™çš„ adapter\",\n",
    "        \"ä½¿ç”¨ KV Cache åŠ é€Ÿç”Ÿæˆ\",\n",
    "        \"è€ƒæ…®ä½¿ç”¨ vLLM ç­‰æ¨ç†å¼•æ“å„ªåŒ–\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_path = os.path.join(save_path, \"deployment_config.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=== ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²é…ç½® ===\")\n",
    "print(json.dumps(deployment_config, indent=2, ensure_ascii=False))\n",
    "print(f\"\\né…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prefix Tuning èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„éƒ¨ç½²å°æ¯”\n",
    "\n",
    "### 8.1 éƒ¨ç½²ç‰¹æ€§å°æ¯”çŸ©é™£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_deployment_comparison = {\n",
    "    \"LoRA\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âœ… å¯å®Œå…¨åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"é›¶é–‹éŠ· (åˆä½µå¾Œ)\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä½ (å–®ä¸€æ¨¡å‹æ–‡ä»¶)\",\n",
    "        \"å¤šä»»å‹™æ”¯æŒ\": \"éœ€è¦å¤šå€‹åˆä½µå¾Œçš„æ¨¡å‹\",\n",
    "        \"é©ç”¨å ´æ™¯\": \"å–®ä»»å‹™ã€è¿½æ±‚æ¥µè‡´æ€§èƒ½\"\n",
    "    },\n",
    "    \"IAÂ³\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âœ… å¯å®Œå…¨åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"é›¶é–‹éŠ· (åˆä½µå¾Œ)\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä½ (å–®ä¸€æ¨¡å‹æ–‡ä»¶)\",\n",
    "        \"å¤šä»»å‹™æ”¯æŒ\": \"éœ€è¦å¤šå€‹åˆä½µå¾Œçš„æ¨¡å‹\",\n",
    "        \"é©ç”¨å ´æ™¯\": \"æ¥µè‡´æ•ˆç‡ã€è³‡æºå—é™\"\n",
    "    },\n",
    "    \"Prefix Tuning\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âŒ ä¸å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"æœ‰é–‹éŠ· (KV æ³¨å…¥)\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä¸­ (åŸºç¤æ¨¡å‹ + adapter)\",\n",
    "        \"å¤šä»»å‹™æ”¯æŒ\": \"âœ… å…±äº«åŸºç¤æ¨¡å‹\",\n",
    "        \"é©ç”¨å ´æ™¯\": \"å¤šä»»å‹™ã€éˆæ´»åˆ‡æ›\"\n",
    "    },\n",
    "    \"Prompt Tuning\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âŒ ä¸å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"æœ‰é–‹éŠ· (è¼¸å…¥æ‹¼æ¥)\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä¸­ (åŸºç¤æ¨¡å‹ + prompts)\",\n",
    "        \"å¤šä»»å‹™æ”¯æŒ\": \"âœ… å…±äº«åŸºç¤æ¨¡å‹\",\n",
    "        \"é©ç”¨å ´æ™¯\": \"è¶…å¤§æ¨¡å‹ã€æ¥µç°¡ adapter\"\n",
    "    },\n",
    "    \"Adapter Layers\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âŒ ä¸å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"æœ‰é–‹éŠ· (é¡å¤–å±¤)\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"é«˜ (æ¨¡å‹çµæ§‹ä¿®æ”¹)\",\n",
    "        \"å¤šä»»å‹™æ”¯æŒ\": \"âœ… æ¨¡çµ„åŒ–åˆ‡æ›\",\n",
    "        \"é©ç”¨å ´æ™¯\": \"NLU ä»»å‹™ã€æ¨¡çµ„åŒ–è¨­è¨ˆ\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== PEFT æ–¹æ³•éƒ¨ç½²ç‰¹æ€§å°æ¯” ===\")\n",
    "print(f\"{'æ–¹æ³•':<15} {'å¯åˆä½µæ€§':<15} {'æ¨ç†é–‹éŠ·':<20} {'éƒ¨ç½²è¤‡é›œåº¦':<25} {'å¤šä»»å‹™æ”¯æŒ':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for method, features in peft_deployment_comparison.items():\n",
    "    print(f\"{method:<15} {features['å¯åˆä½µæ€§']:<15} {features['æ¨ç†é–‹éŠ·']:<20} {features['éƒ¨ç½²è¤‡é›œåº¦']:<25} {features['å¤šä»»å‹™æ”¯æŒ']:<20}\")\n",
    "\n",
    "print(\"\\n=== Prefix Tuning éƒ¨ç½²å®šä½ ===\")\n",
    "print(\"âœ… å„ªå‹¢: å¤šä»»å‹™å ´æ™¯ä¸‹è¨˜æ†¶é«”æ•ˆç‡é«˜\")\n",
    "print(\"âœ… å„ªå‹¢: Adapter åˆ‡æ›éˆæ´»\")\n",
    "print(\"âš ï¸  åŠ£å‹¢: å­˜åœ¨æ¨ç†é–‹éŠ·\")\n",
    "print(\"âš ï¸  åŠ£å‹¢: éƒ¨ç½²ç›¸å°è¤‡é›œ\")\n",
    "print(\"\\nğŸ’¡ å»ºè­°: å–®ä»»å‹™é«˜æ€§èƒ½å ´æ™¯é¸æ“‡ LoRA/IAÂ³,å¤šä»»å‹™éˆæ´»å ´æ™¯é¸æ“‡ Prefix Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. éƒ¨ç½²æœ€ä½³å¯¦è¸èˆ‡å»ºè­°\n",
    "\n",
    "### 9.1 éƒ¨ç½²æª¢æŸ¥æ¸…å–®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prefix Tuning éƒ¨ç½²æª¢æŸ¥æ¸…å–® ===\")\n",
    "print(\"\")\n",
    "print(\"ğŸ“‹ éƒ¨ç½²å‰æº–å‚™:\")\n",
    "print(\"  â–¡ ç¢ºèª Prefix Tuning adapter è¨“ç·´å®Œæˆä¸¦ä¿å­˜\")\n",
    "print(\"  â–¡ åœ¨é©—è­‰é›†ä¸Šé©—è­‰ adapter æ€§èƒ½\")\n",
    "print(\"  â–¡ è¨˜éŒ„åŸºæº–æ€§èƒ½æŒ‡æ¨™ (å»¶é²ã€ååé‡)\")\n",
    "print(\"  â–¡ ç¢ºå®šéƒ¨ç½²æ¶æ§‹ (å–®ä»»å‹™ vs å¤šä»»å‹™)\")\n",
    "print(\"\")\n",
    "print(\"ğŸ”§ éƒ¨ç½²é…ç½®:\")\n",
    "print(\"  â–¡ é…ç½®åŸºç¤æ¨¡å‹è¼‰å…¥è·¯å¾‘\")\n",
    "print(\"  â–¡ é…ç½® adapter è¼‰å…¥è·¯å¾‘\")\n",
    "print(\"  â–¡ è¨­å®šæ¨ç†åƒæ•¸ (batch_size, max_length ç­‰)\")\n",
    "print(\"  â–¡ é…ç½®è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥\")\n",
    "print(\"  â–¡ è¨­å®šç›£æ§å’Œæ—¥èªŒ\")\n",
    "print(\"\")\n",
    "print(\"âœ… éƒ¨ç½²å¾Œé©—è­‰:\")\n",
    "print(\"  â–¡ æ¨ç†å»¶é²æ¸¬è©¦\")\n",
    "print(\"  â–¡ ååé‡æ¸¬è©¦\")\n",
    "print(\"  â–¡ è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§\")\n",
    "print(\"  â–¡ è¼¸å‡ºè³ªé‡æŠ½æ¨£æª¢æŸ¥\")\n",
    "print(\"  â–¡ å£“åŠ›æ¸¬è©¦\")\n",
    "print(\"  â–¡ é•·æ™‚é–“ç©©å®šæ€§æ¸¬è©¦\")\n",
    "print(\"\")\n",
    "print(\"ğŸ“š æ–‡æª”èˆ‡ç¶­è­·:\")\n",
    "print(\"  â–¡ ä¿å­˜éƒ¨ç½²é…ç½®æ–‡ä»¶\")\n",
    "print(\"  â–¡ è¨˜éŒ„æ€§èƒ½åŸºæº–æ•¸æ“š\")\n",
    "print(\"  â–¡ æº–å‚™å›æ»¾æ–¹æ¡ˆ\")\n",
    "print(\"  â–¡ æ›´æ–° API æ–‡æª”\")\n",
    "print(\"  â–¡ è¨­å®šå‘Šè­¦é–¾å€¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 å ´æ™¯é¸æ“‡å»ºè­°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prefix Tuning é©ç”¨å ´æ™¯å»ºè­° ===\")\n",
    "print(\"\")\n",
    "print(\"ğŸ¯ æœ€é©åˆçš„å ´æ™¯:\")\n",
    "print(\"  â€¢ éœ€è¦æ”¯æŒå¤šå€‹ç›¸é—œä»»å‹™\")\n",
    "print(\"  â€¢ ä»»å‹™éœ€è¦é »ç¹åˆ‡æ›\")\n",
    "print(\"  â€¢ è¨˜æ†¶é«”è³‡æºæœ‰é™,ç„¡æ³•éƒ¨ç½²å¤šå€‹å®Œæ•´æ¨¡å‹\")\n",
    "print(\"  â€¢ æ–‡æœ¬ç”Ÿæˆä»»å‹™ (å°è©±ã€æ‘˜è¦ã€é¢¨æ ¼è½‰æ›)\")\n",
    "print(\"  â€¢ å¯ä»¥æ¥å—é©åº¦çš„æ¨ç†é–‹éŠ·\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸  éœ€è¦æ¬Šè¡¡çš„å ´æ™¯:\")\n",
    "print(\"  â€¢ å°æ¨ç†å»¶é²è¦æ±‚æ¥µé«˜çš„å ´æ™¯ -> è€ƒæ…® LoRA/IAÂ³\")\n",
    "print(\"  â€¢ å–®ä¸€ä»»å‹™éƒ¨ç½² -> LoRA/IAÂ³ æ›´åˆé©\")\n",
    "print(\"  â€¢ ç†è§£é¡ä»»å‹™ (åˆ†é¡ã€NER) -> Adapter/BitFit å¯èƒ½æ›´å¥½\")\n",
    "print(\"\")\n",
    "print(\"ğŸ”„ çµ„åˆç­–ç•¥:\")\n",
    "print(\"  â€¢ Prefix Tuning + LoRA: çµåˆå…©è€…å„ªå‹¢\")\n",
    "print(\"  â€¢ é–‹ç™¼éšæ®µç”¨ Prefix Tuning å¿«é€Ÿé©—è­‰å¤šä»»å‹™\")\n",
    "print(\"  â€¢ ç”Ÿç”¢éšæ®µç‚ºé«˜é »ä»»å‹™å–®ç¨è¨“ç·´ LoRA éƒ¨ç½²\")\n",
    "print(\"  â€¢ ä½é »ä»»å‹™ä¿æŒ Prefix Tuning éˆæ´»æ€§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ç¸½çµ\n",
    "\n",
    "é€šéæœ¬å¯¦é©—,æˆ‘å€‘æ·±å…¥æ¢è¨äº† **Prefix Tuning åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­çš„éƒ¨ç½²ç­–ç•¥**ã€‚\n",
    "\n",
    "### 10.1 æ ¸å¿ƒå­¸ç¿’æ”¶ç©«\n",
    "\n",
    "1. **ç†è«–èªçŸ¥**: ç†è§£äº† Prefix Tuning ä¸å¯åˆä½µçš„æŠ€è¡“åŸå› \n",
    "2. **éƒ¨ç½²å¯¦è¸**: æŒæ¡äº† Prefix Tuning çš„ä¿å­˜ã€è¼‰å…¥å’Œéƒ¨ç½²æµç¨‹\n",
    "3. **æ€§èƒ½åˆ†æ**: é‡åŒ–äº†æ¨ç†é–‹éŠ·å’Œè¨˜æ†¶é«”ä½”ç”¨\n",
    "4. **æ¶æ§‹è¨­è¨ˆ**: å­¸ç¿’äº†å¤šä»»å‹™ Prefix ç®¡ç†ç­–ç•¥\n",
    "5. **æ–¹æ³•å°æ¯”**: ç†è§£äº†ä¸åŒ PEFT æ–¹æ³•çš„éƒ¨ç½²ç‰¹æ€§å·®ç•°\n",
    "\n",
    "### 10.2 Prefix Tuning éƒ¨ç½²çš„ç¨ç‰¹åƒ¹å€¼\n",
    "\n",
    "- **å¤šä»»å‹™å‹å¥½**: å…±äº«åŸºç¤æ¨¡å‹,éˆæ´»åˆ‡æ›ä»»å‹™\n",
    "- **è¨˜æ†¶é«”æ•ˆç‡**: Adapter æ¥µå°,å¤šä»»å‹™ä¹Ÿä¸æœƒä½”ç”¨å¤ªå¤šé¡å¤–è¨˜æ†¶é«”\n",
    "- **éƒ¨ç½²éˆæ´»æ€§**: å¯ä»¥å‹•æ…‹æ·»åŠ æ–°ä»»å‹™è€Œä¸å½±éŸ¿ç¾æœ‰æœå‹™\n",
    "- **ç†è«–å„ªé›…æ€§**: é€šéå‰ç¶´å¼•å°ç”Ÿæˆ,æ¦‚å¿µæ¸…æ™°\n",
    "\n",
    "### 10.3 éƒ¨ç½²æ¬Šè¡¡\n",
    "\n",
    "é›–ç„¶ Prefix Tuning ç„¡æ³•åƒ LoRA/IAÂ³ é‚£æ¨£å¯¦ç¾é›¶é–‹éŠ·æ¨ç†,ä½†åœ¨**å¤šä»»å‹™å ´æ™¯**ä¸‹,å…¶éˆæ´»æ€§å’Œè¨˜æ†¶é«”æ•ˆç‡ä½¿å…¶æˆç‚ºå„ªç§€çš„é¸æ“‡ã€‚é—œéµæ˜¯æ ¹æ“šå¯¦éš›éœ€æ±‚é¸æ“‡åˆé©çš„ PEFT æ–¹æ³•:\n",
    "\n",
    "- **å–®ä»»å‹™ã€è¿½æ±‚æ¥µè‡´æ€§èƒ½**: LoRA/IAÂ³\n",
    "- **å¤šä»»å‹™ã€éˆæ´»åˆ‡æ›**: Prefix Tuning\n",
    "- **ç†è§£é¡ä»»å‹™**: Adapter/BitFit\n",
    "- **è¶…å¤§æ¨¡å‹ã€æ¥µç°¡é©é…**: Prompt Tuning\n",
    "\n",
    "æ­£ç¢ºçš„éƒ¨ç½²ç­–ç•¥èƒ½å¤ æœ€å¤§åŒ– PEFT æŠ€è¡“çš„åƒ¹å€¼!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†è³‡æº\n",
    "print(\"=== æ¸…ç†å¯¦é©—è³‡æº ===\")\n",
    "del peft_model, base_model, reloaded_model, fresh_base_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"âœ… è³‡æºæ¸…ç†å®Œæˆ\")\n",
    "\n",
    "print(\"\\nğŸ‰ Lab 4 - Prefix Tuning éƒ¨ç½²æŒ‡å—å¯¦é©—å®Œæˆ!\")\n",
    "print(f\"Adapter å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "print(\"ç¾åœ¨æ‚¨å¯ä»¥åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­éƒ¨ç½²é€™å€‹ Prefix Tuning adapter äº†!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
