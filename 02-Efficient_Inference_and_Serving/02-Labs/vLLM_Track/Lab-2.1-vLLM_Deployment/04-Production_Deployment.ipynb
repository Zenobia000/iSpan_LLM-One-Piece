{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1 Part 4: Production Deployment\n",
    "\n",
    "## Objectives\n",
    "- Deploy OpenAI-compatible API server\n",
    "- Optimize performance for production\n",
    "- Set up monitoring and logging\n",
    "- Learn deployment best practices\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. OpenAI-Compatible API Server\n",
    "\n",
    "vLLM provides an OpenAI-compatible API server out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Server\n",
    "\n",
    "Run this in a **separate terminal**:\n",
    "\n",
    "```bash\n",
    "# Basic usage\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model meta-llama/Llama-2-7b-hf \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000\n",
    "\n",
    "# With more options\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model meta-llama/Llama-2-7b-hf \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --tensor-parallel-size 1 \\\n",
    "    --gpu-memory-utilization 0.9 \\\n",
    "    --max-num-seqs 32 \\\n",
    "    --max-model-len 2048\n",
    "```\n",
    "\n",
    "The server will be available at: `http://localhost:8000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test API Endpoint\n",
    "\n",
    "**Note**: Make sure the vLLM server is running before executing the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Server is running!\n",
      "Status: 200\n"
     ]
    }
   ],
   "source": [
    "# Test if server is running\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/health\")\n",
    "    print(f\"✅ Server is running!\")\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Server is not running.\")\n",
    "    print(\"Please start the server in a terminal first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /v1/completions endpoint...\n",
      "\n",
      "Response:\n",
      "{\n",
      "  \"id\": \"cmpl-442002760ddb4897bd2416e29b76ae5e\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1761616011,\n",
      "  \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"text\": \" happening now\\nThis article is the second in a series of articles highlighting the real-world impact of AI.\\nAI: The Future is Now\\nLinda Rendle\\nPresident, Fujitsu North America\\nWe have\",\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null,\n",
      "      \"prompt_logprobs\": null,\n",
      "      \"prompt_token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 7,\n",
      "    \"total_tokens\": 57,\n",
      "    \"completion_tokens\": 50,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test /v1/completions endpoint\n",
    "def call_completions_api(prompt: str, **kwargs):\n",
    "    \"\"\"Call vLLM completions API.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": kwargs.get(\"max_tokens\", 50),\n",
    "        \"temperature\": kwargs.get(\"temperature\", 0.8),\n",
    "        \"top_p\": kwargs.get(\"top_p\", 0.95),\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{API_URL}/v1/completions\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json=payload,\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Test\n",
    "print(\"Testing /v1/completions endpoint...\\n\")\n",
    "\n",
    "result = call_completions_api(\"The future of AI is\")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion Text:\n",
      " happening now\n",
      "This article is the second in a series of articles highlighting the real-world impact of AI.\n",
      "AI: The Future is Now\n",
      "Linda Rendle\n",
      "President, Fujitsu North America\n",
      "We have\n"
     ]
    }
   ],
   "source": [
    "json.dumps(result, indent=2)\n",
    "\n",
    "# 取出 choices 下的 text\n",
    "completion_text = None\n",
    "if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "    completion_text = result[\"choices\"][0].get(\"text\")\n",
    "    print(\"Completion Text:\")\n",
    "    print(completion_text)\n",
    "else:\n",
    "    print(\"No completion text found in response.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing /v1/chat/completions endpoint...\n",
      "\n",
      "Response: {'error': {'message': 'As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one. None', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n"
     ]
    }
   ],
   "source": [
    "# Test /v1/chat/completions endpoint\n",
    "def call_chat_api(messages: list, **kwargs):\n",
    "    \"\"\"Call vLLM chat completions API.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": kwargs.get(\"max_tokens\", 100),\n",
    "        \"temperature\": kwargs.get(\"temperature\", 0.8),\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{API_URL}/v1/chat/completions\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json=payload,\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Test\n",
    "print(\"Testing /v1/chat/completions endpoint...\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
    "]\n",
    "\n",
    "result = call_chat_api(messages)\n",
    "\n",
    "if \"choices\" in result:\n",
    "    print(\"Assistant:\", result[\"choices\"][0][\"message\"][\"content\"])\n",
    "    print(f\"\\nTokens used: {result['usage']['total_tokens']}\")\n",
    "else:\n",
    "    print(\"Response:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenAI Python Client\n",
    "\n",
    "vLLM is fully compatible with OpenAI's Python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "# Install openai if needed\n",
    "try:\n",
    "    import openai\n",
    "except ImportError:\n",
    "    print(\"Installing openai...\")\n",
    "    !pip install openai -q\n",
    "    import openai\n",
    "\n",
    "print(f\"OpenAI version: {openai.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with OpenAI client...\n",
      "\n",
      "Response:\n",
      "  Ah, an excellent question! *adjusts glasses* Quantum computing, my dear, is a revolutionary field that harnesses the power of quantum mechanics to perform calculations that are beyond the reach of classical computers. *nods*\n",
      "\n",
      "You see, classical computers use bits, which are either a 0 or a 1, to store and process information. But, quantum computers use quantum bits, or qubits, which can exist in multiple states simultaneously! *excitedly* This means that qubits can process an enormous amount of information all at once, making quantum computers incredibly fast and efficient. *winks*\n",
      "\n",
      "But wait, there's more! Quantum computers can also\n",
      "\n",
      "Tokens: 185\n"
     ]
    }
   ],
   "source": [
    "# ✅ 修正版：適用 vLLM + Llama-2-7b-chat-hf\n",
    "from openai import OpenAI\n",
    "\n",
    "# vLLM 預設端口通常是 http://localhost:8000/v1\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"dummy-key\",  # vLLM 不需實際 API Key\n",
    ")\n",
    "\n",
    "print(\"Testing with OpenAI client...\\n\")\n",
    "\n",
    "# ✅ 改用 Chat 版本模型（內建 chat_template）\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "    ],\n",
    "    max_tokens=150,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nTokens: {response.usage.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Performance Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Parameters\n",
    "\n",
    "#### GPU Memory Utilization\n",
    "```bash\n",
    "--gpu-memory-utilization 0.9  # Use 90% of GPU memory\n",
    "```\n",
    "- Higher value → larger batch size → better throughput\n",
    "- Keep some memory for overhead (10%)\n",
    "\n",
    "#### Max Number of Sequences\n",
    "```bash\n",
    "--max-num-seqs 32  # Process up to 32 requests concurrently\n",
    "```\n",
    "- Higher value → better throughput\n",
    "- Limited by GPU memory\n",
    "\n",
    "#### Max Batched Tokens\n",
    "```bash\n",
    "--max-num-batched-tokens 8192\n",
    "```\n",
    "- Controls prefill batch size\n",
    "- Affects TTFT (Time to First Token)\n",
    "\n",
    "#### Block Size\n",
    "```bash\n",
    "--block-size 16  # PagedAttention block size\n",
    "```\n",
    "- Usually 16 is optimal\n",
    "- Smaller = less waste, but more overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Testing\n",
    "\n",
    "Use `locust` or `wrk` for load testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 10 concurrent requests with 5 workers...\n",
      "\n",
      "LOAD TEST RESULTS\n",
      "================================================================================\n",
      "Total requests:     10\n",
      "Successful:         10\n",
      "Failed:             0\n",
      "Total time:         6.69s\n",
      "Requests/sec:       1.49\n",
      "Avg latency:        3.054s\n",
      "Min latency:        2.330s\n",
      "Max latency:        3.377s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple concurrent test\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def send_request(request_id: int):\n",
    "    \"\"\"Send a single request.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Tell me a fact about number {request_id}.\"}\n",
    "            ],\n",
    "            max_tokens=50,\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        return {\"id\": request_id, \"time\": elapsed, \"success\": True}\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\"id\": request_id, \"time\": elapsed, \"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Run concurrent requests\n",
    "NUM_REQUESTS = 10\n",
    "NUM_WORKERS = 5\n",
    "\n",
    "print(f\"Sending {NUM_REQUESTS} concurrent requests with {NUM_WORKERS} workers...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = [executor.submit(send_request, i) for i in range(NUM_REQUESTS)]\n",
    "    results = [f.result() for f in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Analyze results\n",
    "successful = sum(1 for r in results if r[\"success\"])\n",
    "latencies = [r[\"time\"] for r in results if r[\"success\"]]\n",
    "\n",
    "print(\"LOAD TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total requests:     {NUM_REQUESTS}\")\n",
    "print(f\"Successful:         {successful}\")\n",
    "print(f\"Failed:             {NUM_REQUESTS - successful}\")\n",
    "print(f\"Total time:         {total_time:.2f}s\")\n",
    "print(f\"Requests/sec:       {NUM_REQUESTS/total_time:.2f}\")\n",
    "print(f\"Avg latency:        {sum(latencies)/len(latencies):.3f}s\")\n",
    "print(f\"Min latency:        {min(latencies):.3f}s\")\n",
    "print(f\"Max latency:        {max(latencies):.3f}s\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Monitoring and Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prometheus Metrics\n",
    "\n",
    "vLLM exposes Prometheus metrics at `/metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample metrics:\n",
      "\n",
      "================================================================================\n",
      "python_gc_objects_collected_total{generation=\"0\"} 10797.0\n",
      "python_gc_objects_collected_total{generation=\"1\"} 1616.0\n",
      "python_gc_objects_collected_total{generation=\"2\"} 209.0\n",
      "python_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n",
      "python_gc_objects_uncollectable_total{generation=\"1\"} 0.0\n",
      "python_gc_objects_uncollectable_total{generation=\"2\"} 0.0\n",
      "python_gc_collections_total{generation=\"0\"} 1273.0\n",
      "python_gc_collections_total{generation=\"1\"} 114.0\n",
      "python_gc_collections_total{generation=\"2\"} 9.0\n",
      "python_info{implementation=\"CPython\",major=\"3\",minor=\"10\",patchlevel=\"12\",version=\"3.10.12\"} 1.0\n",
      "process_virtual_memory_bytes 1.032974336e+010\n",
      "process_resident_memory_bytes 9.81815296e+08\n",
      "process_start_time_seconds 1.76161721182e+09\n",
      "process_cpu_seconds_total 12.479999999999999\n",
      "process_open_fds 51.0\n",
      "process_max_fds 1.048576e+06\n",
      "vllm:num_requests_running{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:num_requests_waiting{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:kv_cache_usage_perc{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:prefix_cache_queries_total{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 205.0\n",
      "vllm:prefix_cache_queries_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.761617256692543e+09\n",
      "vllm:prefix_cache_hits_total{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:prefix_cache_hits_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566925573e+09\n",
      "vllm:num_preemptions_total{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:num_preemptions_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566925685e+09\n",
      "vllm:prompt_tokens_total{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 205.0\n",
      "vllm:prompt_tokens_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566925793e+09\n",
      "vllm:generation_tokens_total{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 607.0\n",
      "vllm:generation_tokens_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566925914e+09\n",
      "vllm:request_success_total{engine=\"0\",finished_reason=\"stop\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 4.0\n",
      "vllm:request_success_total{engine=\"0\",finished_reason=\"length\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 7.0\n",
      "vllm:request_success_total{engine=\"0\",finished_reason=\"abort\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_success_created{engine=\"0\",finished_reason=\"stop\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566926131e+09\n",
      "vllm:request_success_created{engine=\"0\",finished_reason=\"length\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566926193e+09\n",
      "vllm:request_success_created{engine=\"0\",finished_reason=\"abort\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566926246e+09\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"100.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"200.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"500.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"1000.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"2000.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prompt_tokens_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prompt_tokens_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 205.0\n",
      "vllm:request_prompt_tokens_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566926618e+09\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"100.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"200.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"500.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"1000.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"2000.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_generation_tokens_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_generation_tokens_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 607.0\n",
      "vllm:request_generation_tokens_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566927106e+09\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 150.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"8.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 245.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"16.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 245.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"32.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 248.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"64.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"128.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"256.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"512.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"1024.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"2048.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"4096.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"8192.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"16384.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 252.0\n",
      "vllm:iteration_tokens_total_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 812.0\n",
      "vllm:iteration_tokens_total_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566927407e+09\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"100.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"200.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"500.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"1000.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"2000.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_max_num_generation_tokens_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_max_num_generation_tokens_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 607.0\n",
      "vllm:request_max_num_generation_tokens_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.761617256692859e+09\n",
      "vllm:request_params_n_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_n_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_n_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_n_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_n_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_n_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_n_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_n_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_n_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566928942e+09\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"100.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"200.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"500.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"1000.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"2000.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_max_tokens_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_params_max_tokens_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 650.0\n",
      "vllm:request_params_max_tokens_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.761617256692922e+09\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.001\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.005\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.01\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.02\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.04\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.06\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.08\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 3.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.1\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 3.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.25\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.75\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"7.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"80.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"160.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"640.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"2560.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:time_to_first_token_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.2776150703430176\n",
      "vllm:time_to_first_token_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566929526e+09\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.01\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.025\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.05\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.075\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.1\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.15\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.2\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.4\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.75\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"7.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"80.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:time_per_output_token_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 38.65668998612091\n",
      "vllm:time_per_output_token_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566929924e+09\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.01\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.025\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.05\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.075\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.1\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.15\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.2\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.4\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"0.75\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"7.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"80.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 596.0\n",
      "vllm:inter_token_latency_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 38.65668998612091\n",
      "vllm:inter_token_latency_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566930294e+09\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.01\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.025\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.05\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.075\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.1\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.15\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.2\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.4\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.75\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"7.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"80.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_time_per_output_token_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.7156963857299535\n",
      "vllm:request_time_per_output_token_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.761617256693065e+09\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:e2e_request_latency_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 39.93271231651306\n",
      "vllm:e2e_request_latency_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566931028e+09\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_queue_time_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.00019648810848593712\n",
      "vllm:request_queue_time_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566931384e+09\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 2.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_inference_time_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 39.429728765040636\n",
      "vllm:request_inference_time_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566931746e+09\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_prefill_time_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.7730387789197266\n",
      "vllm:request_prefill_time_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.7616172566941261e+09\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 0.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 3.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 10.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_count{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 11.0\n",
      "vllm:request_decode_time_seconds_sum{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 38.65668998612091\n",
      "vllm:request_decode_time_seconds_created{engine=\"0\",model_name=\"meta-llama/Llama-2-7b-chat-hf\"} 1.761617256694175e+09\n",
      "vllm:cache_config_info{block_size=\"16\",cache_dtype=\"auto\",calculate_kv_scales=\"False\",cpu_kvcache_space_bytes=\"None\",cpu_offload_gb=\"0.0\",enable_prefix_caching=\"True\",engine=\"0\",gpu_memory_utilization=\"0.9\",is_attention_free=\"False\",kv_cache_memory_bytes=\"None\",kv_sharing_fast_prefill=\"False\",mamba_cache_dtype=\"auto\",mamba_page_size_padded=\"None\",mamba_ssm_cache_dtype=\"auto\",num_cpu_blocks=\"None\",num_gpu_blocks=\"153\",num_gpu_blocks_override=\"None\",prefix_caching_hash_algo=\"sha256\",sliding_window=\"None\",swap_space=\"4.0\",swap_space_bytes=\"4294967296.0\"} 1.0\n",
      "http_requests_total{handler=\"/v1/chat/completions\",method=\"POST\",status=\"2xx\"} 11.0\n",
      "http_requests_created{handler=\"/v1/chat/completions\",method=\"POST\",status=\"2xx\"} 1.761617378773575e+09\n",
      "http_request_size_bytes_count{handler=\"/v1/chat/completions\"} 11.0\n",
      "http_request_size_bytes_sum{handler=\"/v1/chat/completions\"} 1509.0\n",
      "http_request_size_bytes_created{handler=\"/v1/chat/completions\"} 1.7616173787735891e+09\n",
      "http_response_size_bytes_count{handler=\"/v1/chat/completions\"} 11.0\n",
      "http_response_size_bytes_sum{handler=\"/v1/chat/completions\"} 8995.0\n",
      "http_response_size_bytes_created{handler=\"/v1/chat/completions\"} 1.761617378773608e+09\n",
      "http_request_duration_highr_seconds_bucket{le=\"0.01\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"0.025\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"0.05\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"0.075\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"0.1\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"0.25\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"0.5\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"0.75\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"1.0\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"1.5\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"2.0\"} 0.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"2.5\"} 1.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"3.0\"} 4.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"3.5\"} 10.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"4.0\"} 10.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"4.5\"} 10.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"5.0\"} 10.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"7.5\"} 10.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"10.0\"} 10.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"30.0\"} 11.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"60.0\"} 11.0\n",
      "http_request_duration_highr_seconds_bucket{le=\"+Inf\"} 11.0\n",
      "http_request_duration_highr_seconds_count 11.0\n",
      "http_request_duration_highr_seconds_sum 42.04235151782632\n",
      "http_request_duration_highr_seconds_created 1.7616172574229198e+09\n",
      "http_request_duration_seconds_bucket{handler=\"/v1/chat/completions\",le=\"0.1\",method=\"POST\"} 0.0\n",
      "http_request_duration_seconds_bucket{handler=\"/v1/chat/completions\",le=\"0.5\",method=\"POST\"} 0.0\n",
      "http_request_duration_seconds_bucket{handler=\"/v1/chat/completions\",le=\"1.0\",method=\"POST\"} 0.0\n",
      "http_request_duration_seconds_bucket{handler=\"/v1/chat/completions\",le=\"+Inf\",method=\"POST\"} 11.0\n",
      "http_request_duration_seconds_count{handler=\"/v1/chat/completions\",method=\"POST\"} 11.0\n",
      "http_request_duration_seconds_sum{handler=\"/v1/chat/completions\",method=\"POST\"} 42.04235151782632\n",
      "http_request_duration_seconds_created{handler=\"/v1/chat/completions\",method=\"POST\"} 1.7616173787736297e+09\n",
      "...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fetch Prometheus metrics\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/metrics\")\n",
    "    metrics = response.text\n",
    "    \n",
    "    print(\"Sample metrics:\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show first 20 lines\n",
    "    for line in metrics.split('\\n')[:]:\n",
    "        if line and not line.startswith('#'):\n",
    "            print(line)\n",
    "    \n",
    "    print(\"...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error fetching metrics: {e}\")\n",
    "\n",
    "\n",
    "# Prometheus metrics output explained:\n",
    "#\n",
    "# The metrics endpoint exposes various internal statistics in a structured text format.\n",
    "# Each line either starts with '#' (metadata) or is a metric data record.\n",
    "#\n",
    "# - Lines starting with `# HELP` provide a description of the metric.\n",
    "# - Lines starting with `# TYPE` indicate the metric type (`counter`, `gauge`, `histogram`, etc.).\n",
    "# - Metrics lines themselves have the form: \n",
    "#     <metric_name>{label1=\"value1\",...} <value>\n",
    "#\n",
    "# Common structure/examples:\n",
    "#   # HELP process_resident_memory_bytes Resident memory size in bytes.\n",
    "#   # TYPE process_resident_memory_bytes gauge\n",
    "#   process_resident_memory_bytes 9.81323776e+08\n",
    "#\n",
    "# 這裡以 vLLM 以及 Python 服務常見指標說明：\n",
    "# - 指標名稱如 `python_gc_objects_collected_total{generation=\"0\"}` 表示 GC 的 objects collected 數量，gauge/counter 格式。\n",
    "# - `{generation=\"0\"}` 是該 metric 的 label，可根據不同維度切分資料。多數時候會看到例如 GPU/handler/method/endpoint 等 label。\n",
    "# - 指標數值（如 `10797.0`）是當前監控時的讀數，可用於 Prometheus 查詢/監控/告警。\n",
    "#\n",
    "# 典型 Prometheus metric log 結構：\n",
    "#   - 說明：# HELP\n",
    "#   - 型別：# TYPE\n",
    "#   - 具體資料：metric_name{labels} value\n",
    "#\n",
    "# vLLM 相關指標舉例（用於監控推理伺服器狀態）：\n",
    "#   vllm:num_requests_running         # 當前正在執行中的請求數（gauge）\n",
    "#   vllm:num_requests_waiting         # 等待排程的請求數（gauge）\n",
    "#   vllm:e2e_request_latency_seconds  # 請求端到端延遲（histogram/gauge）\n",
    "#   vllm:gpu_cache_usage_perc         # GPU KV-cache 使用率（gauge）\n",
    "#   vllm:generation_tokens_total      # 產生的token總數（counter）\n",
    "#\n",
    "# 你可以根據這些指標設計 dashboard 或 alert，例如：\n",
    "#   - 若 num_requests_waiting 過高，代表伺服器壅塞或資源不足。\n",
    "#   - 若 e2e_request_latency_seconds 顯著升高，需檢查模型、硬體或上游負載情況。\n",
    "#   - GPU cache/memory 使用率貼近上限時，易出現 OOM 或延遲。\n",
    "\n",
    "\n",
    "# ---\n",
    "# 上述 Prometheus 指標的含意說明：\n",
    "#\n",
    "# 1. `python_gc_objects_collected_total{generation=\"X\"}`：表示 Python 垃圾回收 (GC) 針對世代 X（0、1、2）累計成功回收的物件數量。GC 會將內存中的物件根據存活時間分為三個世代：0（新）、1（中）、2（老），以分層清理減少效能損耗。\n",
    "#\n",
    "# 2. `python_gc_objects_uncollectable_total{generation=\"X\"}`：世代 X 中累計無法被 GC 回收的物件數（通常因循環引用且無法正確解構）。理論上此數維持 0 代表沒有內存洩漏風險。\n",
    "#\n",
    "# 3. `python_gc_collections_total{generation=\"X\"}`：GC 對世代 X 已執行的回收次數。這能反映垃圾回收觸發頻率，值越大代表 GC 次數越多。\n",
    "#\n",
    "# 4. `python_info{implementation=\"CPython\",major=\"3\",minor=\"10\",patchlevel=\"12\",version=\"3.10.12\"} 1.0`：說明 Python 執行環境的版本資訊。此例為 CPython 3.10.12。該指標值為常數 1，僅作為標籤資訊供監控系統查詢環境。\n",
    "#\n",
    "# 這些指標可協助你監控 Python 應用的記憶體管理與垃圾回收狀況，判斷系統是否存在異常物件累積、循環引用（uncollectable）、GC 頻繁觸發等問題，進而優化資源分配及減少潛在效能瓶頸。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Metrics to Monitor\n",
    "\n",
    "1. **Request Metrics**\n",
    "   - `vllm:num_requests_running` - Active requests\n",
    "   - `vllm:num_requests_waiting` - Queued requests\n",
    "   - `vllm:request_success_total` - Successful requests\n",
    "\n",
    "2. **Latency Metrics**\n",
    "   - `vllm:time_to_first_token_seconds` - TTFT\n",
    "   - `vllm:time_per_output_token_seconds` - ITL\n",
    "   - `vllm:e2e_request_latency_seconds` - End-to-end latency\n",
    "\n",
    "3. **GPU Metrics**\n",
    "   - `vllm:gpu_cache_usage_perc` - KV cache utilization\n",
    "   - `vllm:gpu_memory_usage_bytes` - GPU memory\n",
    "\n",
    "4. **Throughput Metrics**\n",
    "   - `vllm:num_preemptions_total` - Request preemptions\n",
    "   - `vllm:prompt_tokens_total` - Input tokens\n",
    "   - `vllm:generation_tokens_total` - Output tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 10:34:50,325 - INFO - Request 20251028103450325778 started\n",
      "2025-10-28 10:34:50,326 - INFO -   Prompt: What is Python?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing logged request:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 10:34:53,546 - INFO - HTTP Request: POST http://localhost:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-28 10:34:53,547 - INFO - Request 20251028103450325778 completed\n",
      "2025-10-28 10:34:53,547 - INFO -   Time: 3.220s\n",
      "2025-10-28 10:34:53,547 - INFO -   Tokens: 62\n",
      "2025-10-28 10:34:53,548 - INFO -   Throughput: 19.3 tokens/s\n"
     ]
    }
   ],
   "source": [
    "# Simple request logger\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def logged_request(prompt: str, **kwargs):\n",
    "    \"\"\"Make request with logging.\"\"\"\n",
    "    request_id = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    \n",
    "    logger.info(f\"Request {request_id} started\")\n",
    "    logger.info(f\"  Prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        tokens = response.usage.total_tokens\n",
    "        \n",
    "        logger.info(f\"Request {request_id} completed\")\n",
    "        logger.info(f\"  Time: {elapsed:.3f}s\")\n",
    "        logger.info(f\"  Tokens: {tokens}\")\n",
    "        logger.info(f\"  Throughput: {tokens/elapsed:.1f} tokens/s\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        logger.error(f\"Request {request_id} failed\")\n",
    "        logger.error(f\"  Error: {e}\")\n",
    "        logger.error(f\"  Time: {elapsed:.3f}s\")\n",
    "        raise\n",
    "\n",
    "# Test logged request\n",
    "print(\"Testing logged request:\\n\")\n",
    "response = logged_request(\"What is Python?\", max_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deployment Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Deployment\n",
    "\n",
    "#### Dockerfile Example\n",
    "\n",
    "```dockerfile\n",
    "FROM nvidia/cuda:12.1.0-devel-ubuntu22.04\n",
    "\n",
    "# Install Python\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install vLLM\n",
    "RUN pip3 install vllm\n",
    "\n",
    "# Download model (optional, can mount volume instead)\n",
    "# RUN python3 -c \"from transformers import AutoModel; AutoModel.from_pretrained('meta-llama/Llama-2-7b-hf')\"\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Start server\n",
    "CMD [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\n",
    "     \"--model\", \"meta-llama/Llama-2-7b-hf\", \\\n",
    "     \"--host\", \"0.0.0.0\", \\\n",
    "     \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "#### Build and Run\n",
    "\n",
    "```bash\n",
    "# Build\n",
    "docker build -t vllm-server .\n",
    "\n",
    "# Run\n",
    "docker run --gpus all -p 8000:8000 \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    vllm-server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubernetes Deployment\n",
    "\n",
    "#### Deployment YAML\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: vllm-server\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: vllm-server\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: vllm-server\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: vllm\n",
    "        image: vllm-server:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1\n",
    "          requests:\n",
    "            memory: \"32Gi\"\n",
    "            cpu: \"4\"\n",
    "        env:\n",
    "        - name: CUDA_VISIBLE_DEVICES\n",
    "          value: \"0\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: vllm-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: vllm-server\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement health check endpoint\n",
    "def check_health():\n",
    "    \"\"\"Check if vLLM server is healthy.\"\"\"\n",
    "    try:\n",
    "        # Check health endpoint\n",
    "        response = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return False, \"Health check failed\"\n",
    "        \n",
    "        # Test with simple request\n",
    "        test_response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-2-7b-hf\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "            max_tokens=5,\n",
    "        )\n",
    "        \n",
    "        return True, \"Healthy\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Run health check\n",
    "is_healthy, message = check_health()\n",
    "\n",
    "if is_healthy:\n",
    "    print(\"✅ Server is healthy\")\n",
    "else:\n",
    "    print(f\"❌ Server health check failed: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "- [ ] **Performance Testing**\n",
    "  - [ ] Load testing completed\n",
    "  - [ ] Latency benchmarks acceptable\n",
    "  - [ ] Memory usage stable\n",
    "\n",
    "- [ ] **Monitoring**\n",
    "  - [ ] Prometheus metrics configured\n",
    "  - [ ] Grafana dashboards set up\n",
    "  - [ ] Alerts configured\n",
    "\n",
    "- [ ] **Reliability**\n",
    "  - [ ] Health checks implemented\n",
    "  - [ ] Auto-restart on failure\n",
    "  - [ ] Load balancing configured\n",
    "\n",
    "- [ ] **Security**\n",
    "  - [ ] API authentication enabled\n",
    "  - [ ] Rate limiting configured\n",
    "  - [ ] Input validation implemented\n",
    "\n",
    "- [ ] **Scalability**\n",
    "  - [ ] Horizontal scaling tested\n",
    "  - [ ] Auto-scaling configured\n",
    "  - [ ] Resource limits set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Common Issues and Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 1: Out of Memory\n",
    "\n",
    "**Symptoms**: CUDA OOM errors\n",
    "\n",
    "**Solutions**:\n",
    "```bash\n",
    "# Reduce GPU memory utilization\n",
    "--gpu-memory-utilization 0.8\n",
    "\n",
    "# Reduce max sequences\n",
    "--max-num-seqs 16\n",
    "\n",
    "# Reduce context length\n",
    "--max-model-len 1024\n",
    "```\n",
    "\n",
    "### Issue 2: High Latency\n",
    "\n",
    "**Symptoms**: Slow response times\n",
    "\n",
    "**Solutions**:\n",
    "```bash\n",
    "# Increase batch size\n",
    "--max-num-seqs 64\n",
    "\n",
    "# Increase batched tokens\n",
    "--max-num-batched-tokens 16384\n",
    "\n",
    "# Enable tensor parallelism (multi-GPU)\n",
    "--tensor-parallel-size 2\n",
    "```\n",
    "\n",
    "### Issue 3: Request Timeouts\n",
    "\n",
    "**Symptoms**: Requests timing out\n",
    "\n",
    "**Solutions**:\n",
    "- Reduce `max_tokens` in requests\n",
    "- Increase server timeout settings\n",
    "- Scale horizontally with load balancer\n",
    "\n",
    "### Issue 4: Inconsistent Performance\n",
    "\n",
    "**Symptoms**: Variable latency\n",
    "\n",
    "**Solutions**:\n",
    "- Check for competing GPU processes\n",
    "- Monitor GPU temperature throttling\n",
    "- Ensure stable power supply\n",
    "- Use dedicated GPU instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ **Completed Lab-2.1**:\n",
    "1. Deployed OpenAI-compatible API server\n",
    "2. Tested completions and chat APIs\n",
    "3. Performed load testing\n",
    "4. Set up monitoring and logging\n",
    "5. Learned deployment best practices\n",
    "\n",
    "📊 **Key Achievements**:\n",
    "- Production-ready vLLM deployment\n",
    "- OpenAI API compatibility\n",
    "- Performance monitoring setup\n",
    "- Understanding of optimization parameters\n",
    "\n",
    "🎓 **Skills Acquired**:\n",
    "- vLLM installation and configuration\n",
    "- PagedAttention understanding\n",
    "- Batch inference optimization\n",
    "- Advanced sampling strategies\n",
    "- Production deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **Lab-2.2**: Inference Optimization Techniques\n",
    "- **Lab-2.3**: FastAPI Service Construction\n",
    "- **Lab-2.4**: Production Environment Deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [vLLM GitHub](https://github.com/vllm-project/vllm)\n",
    "- [PagedAttention Paper](https://arxiv.org/abs/2309.06180)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "print(\"✅ Lab-2.1 Complete!\")\n",
    "print(\"\\nCongratulations on mastering vLLM deployment! 🎉\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
