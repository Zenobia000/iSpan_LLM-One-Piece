{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: Prompt Tuning - Fine-Tuning a T5 Model for Summarization\n",
        "---\n",
        "## Notebook 2: The Training Process\n",
        "\n",
        "**Goal:** In this notebook, you will fine-tune a `t5-small` model on a text summarization task using **Prompt Tuning**.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Load a dataset for a sequence-to-sequence task (summarization) and preprocess it.\n",
        "-   Load a pre-trained T5 model.\n",
        "-   Deeply understand and configure `peft.PromptTuningConfig`.\n",
        "-   Apply soft prompts to the T5 model.\n",
        "-   Fine-tune the model by training *only* the soft prompt embeddings using the `transformers.Trainer`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Dataset and Preprocess\n",
        "\n",
        "We will use the `billsum` dataset, which contains texts of US congressional bills and their corresponding summaries. This is a classic sequence-to-sequence (seq2seq) task.\n",
        "\n",
        "#### Key Hugging Face Components:\n",
        "\n",
        "-   `transformers.AutoTokenizer`: We'll load the tokenizer for `t5-small`. For T5, it's common practice to prepend a task-specific prefix to the input text (e.g., \"summarize: \").\n",
        "-   `dataset.map()`: We'll create a function to tokenize both the input text (`text`) and the target summary (`summary`). The tokenized summary will be our `labels`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# --- Load Dataset ---\n",
        "# We'll take a small slice for a quick demonstration\n",
        "dataset = load_dataset(\"billsum\", split=\"train[:500]\")\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def preprocess_function(examples):\n",
        "    # T5 expects a prefix for summarization tasks\n",
        "    inputs = [\"summarize: \" + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    \n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# --- Apply Preprocessing ---\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"summary\", \"title\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"âœ… Dataset loaded and preprocessed.\")\n",
        "print(tokenized_datasets[\"train\"][0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load the Base Model\n",
        "\n",
        "Next, we load the `t5-small` model. Since this is a seq2seq task, we can use `AutoModelForSeq2SeqLM`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "print(\"âœ… Base T5 model loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Configure Prompt Tuning\n",
        "\n",
        "Now we configure Prompt Tuning. The idea is to create a small, trainable embedding that acts as a \"soft prompt\" to guide the frozen base model.\n",
        "\n",
        "#### Key Hugging Face `peft` Components:\n",
        "\n",
        "-   `peft.PromptTuningConfig`: The configuration class for this method.\n",
        "    -   `task_type=\"SEQ_2_SEQ_LM\"`: We must specify the task type. For T5, it's sequence-to-sequence language modeling.\n",
        "    -   `prompt_tuning_init=\"TEXT\"`: How to initialize the soft prompt embeddings. `\"TEXT\"` means we'll initialize them using the vocabulary embeddings of a specific text string. This can provide a better starting point than random initialization.\n",
        "    -   `num_virtual_tokens`: The length of the soft prompt. This is the main hyperparameter to tune. It's the number of trainable embedding vectors we will create.\n",
        "    -   `prompt_tuning_init_text`: The text to use for initialization if `prompt_tuning_init=\"TEXT\"`. The vocabulary embeddings of these tokens will be averaged to create the initial soft prompt.\n",
        "    -   `tokenizer_name_or_path`: We must provide the path to the tokenizer to be used for the text initialization.\n",
        "-   `peft.get_peft_model`: As before, this function applies the configuration to our base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_model, PromptTuningConfig, TaskType\n",
        "\n",
        "# --- Prompt Tuning Configuration ---\n",
        "prompt_config = PromptTuningConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    prompt_tuning_init=\"TEXT\",\n",
        "    num_virtual_tokens=8,\n",
        "    prompt_tuning_init_text=\"Summarize the following congressional bill:\",\n",
        "    tokenizer_name_or_path=model_checkpoint,\n",
        ")\n",
        "\n",
        "# --- Create PeftModel ---\n",
        "peft_model = get_peft_model(model, prompt_config)\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Set Up Training\n",
        "\n",
        "The final step is to configure and run the training process using the `transformers.Trainer`. This is very similar to the previous labs. We will reuse the `compute_metrics` function, but this time we will use the `rouge` metric, which is standard for summarization tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5-prompt-tuning-billsum\",\n",
        "    auto_find_batch_size=True, # Automatically find a batch size that fits\n",
        "    learning_rate=1e-3, # Higher learning rate is common for PEFT methods\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# --- Create Trainer ---\n",
        "# For Seq2Seq tasks, we need a specific data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"ðŸš€ Starting training with Prompt Tuning...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
