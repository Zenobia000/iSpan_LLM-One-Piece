{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1: LoRA & QLoRA - Fine-Tuning a Llama-2 Model\n",
        "---\n",
        "## Notebook 4: Merge for Deployment\n",
        "\n",
        "**Goal:** In this final notebook, you will learn how to merge the trained LoRA adapter weights back into the base model. This creates a single, standalone model that can be easily deployed without needing the `peft` library for inference.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Reload the base model and the trained PEFT adapter.\n",
        "-   Use the `merge_and_unload()` method to combine the weights.\n",
        "-   Save the merged model and its tokenizer for future use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Reload Model and Adapter\n",
        "\n",
        "As before, we must load the base model in its original quantized configuration and then load the PEFT adapter on top of it. This ensures the model architecture is identical before we attempt to merge the weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "# --- Base Model and Tokenizer Loading ---\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Load PEFT Adapter ---\n",
        "output_dir = \"./lora-llama2-7b-guanaco\"\n",
        "latest_checkpoint = max(\n",
        "    [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")],\n",
        "    key=os.path.getmtime\n",
        ")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "print(\"âœ… Model and adapter loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Merge the Adapter into the Base Model\n",
        "\n",
        "This is the key step for deployment. Merging the adapter simplifies the inference process, as you no longer need to manage a separate set of adapter weights.\n",
        "\n",
        "#### Key Hugging Face `peft` Component:\n",
        "\n",
        "-   `peft_model.merge_and_unload()`: This powerful method performs two actions:\n",
        "    1.  **Merge**: It calculates the final weight update (approximated by `B * A` in LoRA) and adds it directly to the weights of the original `target_modules` in the base model.\n",
        "    2.  **Unload**: It removes the LoRA adapter layers from the model.\n",
        "\n",
        "The result is a standard `transformers` model (e.g., `LlamaForCausalLM`) that has the fine-tuning \"baked in.\"\n",
        "\n",
        "**Note:** Merging adapters into a quantized model (`bitsandbytes`) is a recent feature. While it works for inference, saving and reloading a merged quantized model can sometimes be tricky depending on library versions. The standard, most reliable workflow is with non-quantized models. We demonstrate the general process here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge the LoRA adapter into the base model\n",
        "print(\"ðŸš€ Merging adapter...\")\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "print(\"âœ… Adapter merged!\")\n",
        "\n",
        "# The model is now a standard transformers model\n",
        "print(f\"\\nType of merged model: {type(merged_model)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Save the Merged Model for Deployment\n",
        "\n",
        "Now that we have a single, merged model, we can save it to a directory using the standard Hugging Face method.\n",
        "\n",
        "#### Key Hugging Face `transformers` Components:\n",
        "\n",
        "-   `model.save_pretrained(directory)`: Saves the model's weights and configuration file (`config.json`) to the specified directory.\n",
        "-   `tokenizer.save_pretrained(directory)`: Saves the tokenizer's vocabulary and configuration files.\n",
        "\n",
        "The resulting directory contains a complete, self-contained model that can be easily shared or loaded elsewhere using `AutoModelForCausalLM.from_pretrained(directory)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the directory to save the merged model\n",
        "save_directory = \"./llama2-7b-guanaco-merged\"\n",
        "\n",
        "# Save the merged model and tokenizer\n",
        "print(f\"ðŸ’¾ Saving merged model to: {save_directory}\")\n",
        "merged_model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "print(\"âœ… Merged model saved successfully!\")\n",
        "\n",
        "# You can now load this model directly like any other Hugging Face model\n",
        "# from transformers import AutoModelForCausalLM\n",
        "# loaded_model = AutoModelForCausalLM.from_pretrained(save_directory)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
