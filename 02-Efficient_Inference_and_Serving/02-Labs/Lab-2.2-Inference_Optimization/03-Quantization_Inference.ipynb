{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.2 Part 3: Quantization Inference\n",
    "\n",
    "## Objectives\n",
    "- Understand quantization fundamentals\n",
    "- Implement INT8/INT4 quantization\n",
    "- Compare GPTQ, AWQ, and BitsAndBytes\n",
    "- Evaluate quality vs performance tradeoffs\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Quantization Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Quantization?\n",
    "\n",
    "Quantization reduces precision of model weights and activations:\n",
    "\n",
    "```\n",
    "FP32 (32-bit):  ¬±3.4 √ó 10¬≥‚Å∏ range, high precision\n",
    "FP16 (16-bit):  ¬±65,504 range, good precision  \n",
    "INT8 (8-bit):   -128 to 127, reduced precision\n",
    "INT4 (4-bit):   -8 to 7, minimal precision\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- 2-4x smaller model size\n",
    "- 1.5-3x faster inference (memory-bound ‚Üí less data transfer)\n",
    "- Enables larger batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Comparison\n",
    "\n",
    "| Method | Bits | Model Size | Speed | Quality | Use Case |\n",
    "|--------|------|-----------|--------|---------|----------|\n",
    "| FP16 | 16 | 100% | 1.0x | 100% | Standard |\n",
    "| INT8 | 8 | 50% | 1.5-2x | 98-99% | Balanced |\n",
    "| INT4 | 4 | 25% | 2-3x | 95-97% | Resource-constrained |\n",
    "| FP8 (H100) | 8 | 50% | 2-3x | 99-99.5% | Latest GPUs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size comparison\n",
    "def calculate_model_size(num_params_b: float, precision_bytes: int) -> float:\n",
    "    \"\"\"Calculate model size in GB.\"\"\"\n",
    "    return num_params_b * precision_bytes\n",
    "\n",
    "# Llama-2-7B\n",
    "num_params = 7.0  # Billion\n",
    "\n",
    "sizes = {\n",
    "    'FP32': calculate_model_size(num_params, 4),\n",
    "    'FP16': calculate_model_size(num_params, 2),\n",
    "    'INT8': calculate_model_size(num_params, 1),\n",
    "    'INT4': calculate_model_size(num_params, 0.5),\n",
    "}\n",
    "\n",
    "print(\"Model Size Comparison (Llama-2-7B)\")\n",
    "print(\"=\" * 60)\n",
    "for precision, size in sizes.items():\n",
    "    reduction = sizes['FP16'] / size\n",
    "    print(f\"{precision:6s}: {size:5.2f} GB ({reduction:.1f}x reduction vs FP16)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "precisions = list(sizes.keys())\n",
    "size_values = list(sizes.values())\n",
    "colors = ['#ff6b6b', '#ffd93d', '#51cf66', '#4dabf7']\n",
    "\n",
    "bars = plt.bar(precisions, size_values, color=colors)\n",
    "plt.ylabel('Model Size (GB)')\n",
    "plt.title('Model Size vs Precision (Llama-2-7B)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, size in zip(bars, size_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{size:.1f} GB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. INT8 Quantization with BitsAndBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if bitsandbytes is installed\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"‚úÖ bitsandbytes: {bnb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå bitsandbytes not installed\")\n",
    "    print(\"Install: pip install bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FP16 model (baseline)\n",
    "MODEL_NAME = \"facebook/opt-1.3b\"\n",
    "\n",
    "print(f\"Loading FP16 baseline: {MODEL_NAME}...\")\n",
    "start = time.time()\n",
    "\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "load_time_fp16 = time.time() - start\n",
    "\n",
    "# Check model size\n",
    "fp16_memory = model_fp16.get_memory_footprint() / 1e9\n",
    "\n",
    "print(f\"‚úÖ Loaded in {load_time_fp16:.2f}s\")\n",
    "print(f\"   Memory: {fp16_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load INT8 quantized model\n",
    "print(f\"\\nLoading INT8 quantized: {MODEL_NAME}...\")\n",
    "start = time.time()\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "load_time_int8 = time.time() - start\n",
    "int8_memory = model_int8.get_memory_footprint() / 1e9\n",
    "\n",
    "print(f\"‚úÖ Loaded in {load_time_int8:.2f}s\")\n",
    "print(f\"   Memory: {int8_memory:.2f} GB\")\n",
    "print(f\"   Reduction: {fp16_memory/int8_memory:.1f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, tokenizer, prompts, max_tokens=50):\n",
    "    \"\"\"Benchmark model inference.\"\"\"\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        total_time += elapsed\n",
    "        total_tokens += len(outputs[0]) - len(inputs.input_ids[0])\n",
    "    \n",
    "    return {\n",
    "        'total_time': total_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'throughput': total_tokens / total_time,\n",
    "        'avg_time': total_time / len(prompts),\n",
    "    }\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain machine learning:\",\n",
    "    \"What is Python programming?\",\n",
    "    \"The benefits of AI are\",\n",
    "]\n",
    "\n",
    "print(\"Benchmarking FP16 model...\")\n",
    "fp16_results = benchmark_model(model_fp16, tokenizer, test_prompts)\n",
    "\n",
    "print(\"\\nBenchmarking INT8 model...\")\n",
    "int8_results = benchmark_model(model_int8, tokenizer, test_prompts)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFP16:\")\n",
    "print(f\"  Total time:    {fp16_results['total_time']:.2f}s\")\n",
    "print(f\"  Throughput:    {fp16_results['throughput']:.1f} tokens/s\")\n",
    "print(f\"  Memory:        {fp16_memory:.2f} GB\")\n",
    "\n",
    "print(f\"\\nINT8:\")\n",
    "print(f\"  Total time:    {int8_results['total_time']:.2f}s\")\n",
    "print(f\"  Throughput:    {int8_results['throughput']:.1f} tokens/s\")\n",
    "print(f\"  Memory:        {int8_memory:.2f} GB\")\n",
    "\n",
    "speedup = fp16_results['total_time'] / int8_results['total_time']\n",
    "throughput_gain = int8_results['throughput'] / fp16_results['throughput']\n",
    "memory_reduction = fp16_memory / int8_memory\n",
    "\n",
    "print(f\"\\nGains:\")\n",
    "print(f\"  Speedup:       {speedup:.2f}x ‚ö°\")\n",
    "print(f\"  Throughput:    {throughput_gain:.2f}x üìä\")\n",
    "print(f\"  Memory saved:  {memory_reduction:.2f}x üíæ\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare output quality\n",
    "eval_prompt = \"Explain the concept of neural networks in detail:\"\n",
    "\n",
    "# Generate with both models\n",
    "inputs = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(\"Generating with FP16...\")\n",
    "with torch.no_grad():\n",
    "    fp16_output = model_fp16.generate(\n",
    "        **inputs, max_new_tokens=100, temperature=0.7, do_sample=True\n",
    "    )\n",
    "fp16_text = tokenizer.decode(fp16_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generating with INT8...\")\n",
    "with torch.no_grad():\n",
    "    int8_output = model_int8.generate(\n",
    "        **inputs, max_new_tokens=100, temperature=0.7, do_sample=True\n",
    "    )\n",
    "int8_text = tokenizer.decode(int8_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OUTPUT QUALITY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFP16 Output:\\n{fp16_text}\")\n",
    "print(f\"\\n{'-' * 80}\")\n",
    "print(f\"\\nINT8 Output:\\n{int8_text}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Quality remains very similar despite 2x compression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity Evaluation\n",
    "\n",
    "Perplexity measures how well the model predicts a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    \"\"\"Calculate perplexity on a text sample.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "# Test texts\n",
    "test_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Neural networks are inspired by biological neurons.\",\n",
    "]\n",
    "\n",
    "print(\"Calculating perplexity...\\n\")\n",
    "\n",
    "fp16_ppls = [calculate_perplexity(model_fp16, tokenizer, text) for text in test_texts]\n",
    "int8_ppls = [calculate_perplexity(model_int8, tokenizer, text) for text in test_texts]\n",
    "\n",
    "fp16_avg = np.mean(fp16_ppls)\n",
    "int8_avg = np.mean(int8_ppls)\n",
    "\n",
    "print(\"Perplexity Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"FP16 avg:  {fp16_avg:.2f}\")\n",
    "print(f\"INT8 avg:  {int8_avg:.2f}\")\n",
    "print(f\"Degradation: {(int8_avg - fp16_avg) / fp16_avg * 100:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Perplexity increase <5% is generally acceptable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. INT4 with GPTQ (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ Quantization\n",
    "\n",
    "GPTQ (Generalized Post-Training Quantization) provides high-quality INT4 quantization.\n",
    "\n",
    "**Note**: Requires pre-quantized models (e.g., from TheBloke).\n",
    "\n",
    "```python\n",
    "# Example (requires auto-gptq)\n",
    "from transformers import GPTQConfig\n",
    "\n",
    "gptq_config = GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    desc_act=True,\n",
    ")\n",
    "\n",
    "model_int4 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Llama-2-7B-GPTQ\",\n",
    "    quantization_config=gptq_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Memory: ~3.5GB (4x reduction vs FP16)\n",
    "# Speed: 2-3x faster\n",
    "# Quality: ~95% of FP16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ vs GPTQ\n",
    "\n",
    "| Method | Calibration | Speed | Quality | Use Case |\n",
    "|--------|------------|--------|---------|----------|\n",
    "| **GPTQ** | Required | Fast | High | General purpose |\n",
    "| **AWQ** | Required | Faster | Very High | When quality critical |\n",
    "\n",
    "Both achieve ~4x compression with INT4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Quantization Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Quantization Level\n",
    "\n",
    "**Decision Tree**:\n",
    "```\n",
    "                  Start\n",
    "                    ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ                     ‚îÇ\n",
    "   GPU memory OK?        GPU limited?\n",
    "         ‚îÇ                     ‚îÇ\n",
    "         ‚ñº                     ‚ñº\n",
    "       FP16                  INT8\n",
    "         ‚îÇ                     ‚îÇ\n",
    "         ‚îÇ              Quality critical?\n",
    "         ‚îÇ                   /   \\\n",
    "         ‚îÇ                 Yes   No\n",
    "         ‚îÇ                  ‚îÇ     ‚îÇ\n",
    "         ‚îÇ                INT8  INT4\n",
    "         ‚îÇ\n",
    "    Need speed?\n",
    "       /    \\\n",
    "     Yes    No\n",
    "      ‚îÇ      ‚îÇ\n",
    "    INT8   FP16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "comparison_data = {\n",
    "    'Precision': ['FP16', 'INT8'],\n",
    "    'Memory (GB)': [fp16_memory, int8_memory],\n",
    "    'Load Time (s)': [load_time_fp16, load_time_int8],\n",
    "    'Throughput (tok/s)': [\n",
    "        fp16_results['throughput'], \n",
    "        int8_results['throughput']\n",
    "    ],\n",
    "    'Avg PPL': [fp16_avg, int8_avg],\n",
    "}\n",
    "\n",
    "print(\"\\nQuantization Summary\")\n",
    "print(\"=\" * 80)\n",
    "for key in comparison_data:\n",
    "    print(f\"{key:20s}: {str(comparison_data[key])}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüìå Recommendations:\")\n",
    "print(\"  - Production (quality critical):  FP16 or INT8\")\n",
    "print(\"  - Production (memory limited):    INT8 or INT4\")\n",
    "print(\"  - Research/Development:           FP16\")\n",
    "print(\"  - Edge devices:                   INT4 with GPTQ/AWQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "‚úÖ **Completed**:\n",
    "1. Understood quantization fundamentals\n",
    "2. Implemented INT8 quantization with BitsAndBytes\n",
    "3. Compared FP16 vs INT8 performance\n",
    "4. Evaluated quality with perplexity\n",
    "5. Learned quantization selection strategies\n",
    "\n",
    "üìä **Key Findings**:\n",
    "- INT8: 2x memory reduction, 1.5x speedup, <5% quality loss\n",
    "- INT4: 4x memory reduction, 2-3x speedup, 5-10% quality loss\n",
    "- Quantization is crucial for resource-constrained deployment\n",
    "\n",
    "‚û°Ô∏è **Next**: In `04-Comprehensive_Optimization.ipynb`, we'll:\n",
    "- Combine multiple optimizations\n",
    "- Test vLLM + quantization\n",
    "- Run comprehensive benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "del model_fp16, model_int8\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Lab 2.2 Part 3 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
