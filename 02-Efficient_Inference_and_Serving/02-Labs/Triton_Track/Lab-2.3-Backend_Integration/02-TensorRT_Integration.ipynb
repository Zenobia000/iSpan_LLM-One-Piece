{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Backend æ•´åˆèˆ‡æ¥µè‡´å„ªåŒ–\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "æœ¬å¯¦é©—å°‡æ·±å…¥æ¢è¨ Triton TensorRT Backend çš„æ•´åˆèˆ‡å„ªåŒ–æŠ€è¡“ï¼Œå­¸ç¿’å¦‚ä½•å°‡ PyTorch æ¨¡å‹è½‰æ›ç‚ºé«˜æ€§èƒ½çš„ TensorRT å¼•æ“ï¼Œä¸¦å¯¦ç¾æ¥µè‡´çš„æ¨ç†æ€§èƒ½ã€‚\n",
    "\n",
    "### æ ¸å¿ƒçŸ¥è­˜é»\n",
    "- âœ… TensorRT å¼•æ“æ§‹å»ºèˆ‡å„ªåŒ–\n",
    "- âœ… æ¨¡å‹è½‰æ›æµç¨‹ (PyTorch â†’ ONNX â†’ TensorRT)\n",
    "- âœ… ç²¾åº¦å„ªåŒ– (FP32/FP16/INT8)\n",
    "- âœ… å‹•æ…‹å½¢ç‹€æ”¯æ´\n",
    "- âœ… æ€§èƒ½åŸºæº–æ¸¬è©¦èˆ‡å°æ¯”\n",
    "- âœ… ä¼æ¥­ç´šéƒ¨ç½²é…ç½®\n",
    "\n",
    "### æŠ€è¡“æ¶æ§‹\n",
    "```\n",
    "PyTorch æ¨¡å‹\n",
    "     â†“\n",
    "ONNX è½‰æ› (torch.onnx.export)\n",
    "     â†“\n",
    "TensorRT å„ªåŒ– (trtexec/TensorRT Python API)\n",
    "     â†“\n",
    "Triton TensorRT Backend\n",
    "     â†“\n",
    "é«˜æ€§èƒ½æ¨ç†æœå‹™\n",
    "```\n",
    "\n",
    "### æ€§èƒ½æå‡é æœŸ\n",
    "- ğŸš€ **æ¨ç†é€Ÿåº¦**: 2-10x åŠ é€Ÿ\n",
    "- ğŸ’¾ **è¨˜æ†¶é«”ä½¿ç”¨**: 30-50% æ¸›å°‘\n",
    "- âš¡ **å»¶é²å„ªåŒ–**: æ¯«ç§’ç´šéŸ¿æ‡‰\n",
    "- ğŸ”‹ **èƒ½è€—æ•ˆç‡**: é¡¯è‘—é™ä½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™èˆ‡ TensorRT è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "import tritonclient.http as httpclient\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import logging\n",
    "import subprocess\n",
    "import shutil\n",
    "import pickle\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from dataclasses import dataclass\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# TensorRT ç›¸é—œå°å…¥\n",
    "try:\n",
    "    import tensorrt as trt\n",
    "    import pycuda.driver as cuda\n",
    "    import pycuda.autoinit\n",
    "    TRT_AVAILABLE = True\n",
    "    print(f\"âœ… TensorRT ç‰ˆæœ¬: {trt.__version__}\")\n",
    "except ImportError as e:\n",
    "    TRT_AVAILABLE = False\n",
    "    print(f\"âš ï¸  TensorRT æœªå®‰è£: {e}\")\n",
    "    print(\"   è«‹å®‰è£ TensorRT ä»¥ä½¿ç”¨å®Œæ•´åŠŸèƒ½\")\n",
    "\n",
    "# è¨­å®šæ—¥èªŒ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ğŸ”§ ç’°å¢ƒè³‡è¨Šæª¢æŸ¥\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"ONNX ç‰ˆæœ¬: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime ç‰ˆæœ¬: {ort.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"ç•¶å‰ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"è¨ˆç®—èƒ½åŠ›: {torch.cuda.get_device_capability()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šå·¥ä½œç›®éŒ„\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_REPO = BASE_DIR / \"model_repository_tensorrt\"\n",
    "ONNX_DIR = BASE_DIR / \"onnx_models\"\n",
    "TRT_DIR = BASE_DIR / \"tensorrt_engines\"\n",
    "BENCHMARK_DIR = BASE_DIR / \"benchmarks\"\n",
    "SCRIPTS_DIR = BASE_DIR / \"scripts\"\n",
    "\n",
    "# å‰µå»ºå¿…è¦ç›®éŒ„\n",
    "for dir_path in [MODEL_REPO, ONNX_DIR, TRT_DIR, BENCHMARK_DIR, SCRIPTS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "print(f\"ğŸ“ å·¥ä½œç›®éŒ„: {BASE_DIR}\")\n",
    "print(f\"ğŸ“ æ¨¡å‹å€‰åº«: {MODEL_REPO}\")\n",
    "print(f\"ğŸ“ ONNX æ¨¡å‹: {ONNX_DIR}\")\n",
    "print(f\"ğŸ“ TensorRT å¼•æ“: {TRT_DIR}\")\n",
    "print(f\"ğŸ“ åŸºæº–æ¸¬è©¦: {BENCHMARK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å»ºç«‹åƒè€ƒ PyTorch æ¨¡å‹\n",
    "\n",
    "å‰µå»ºä¸€å€‹é©åˆ TensorRT å„ªåŒ–çš„æ¨¡å‹ï¼Œå±•ç¤ºè½‰æ›æµç¨‹å’Œæ€§èƒ½å°æ¯”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientTextClassifier(nn.Module):\n",
    "    \"\"\"ç‚º TensorRT å„ªåŒ–è¨­è¨ˆçš„æ–‡æœ¬åˆ†é¡æ¨¡å‹\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000, embed_dim=128, hidden_dim=256, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # åµŒå…¥å±¤\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # 1D å·ç©å±¤ (å° TensorRT å‹å¥½)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # å…¨å±€æœ€å¤§æ± åŒ–\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # åˆ†é¡é ­\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # æ¬Šé‡åˆå§‹åŒ–\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"æ¬Šé‡åˆå§‹åŒ–\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, 0, 0.1)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # è¼¸å…¥ shape: [batch_size, sequence_length]\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # åµŒå…¥: [batch_size, sequence_length, embed_dim]\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # è½‰ç½®ç”¨æ–¼å·ç©: [batch_size, embed_dim, sequence_length]\n",
    "        embedded = embedded.transpose(1, 2)\n",
    "        \n",
    "        # å·ç©ç‰¹å¾µæå–\n",
    "        conv_output = self.conv_layers(embedded)\n",
    "        \n",
    "        # å…¨å±€æ± åŒ–: [batch_size, hidden_dim, 1]\n",
    "        pooled = self.global_pool(conv_output)\n",
    "        \n",
    "        # å±•å¹³: [batch_size, hidden_dim]\n",
    "        flattened = pooled.squeeze(2)\n",
    "        \n",
    "        # åˆ†é¡\n",
    "        logits = self.classifier(flattened)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# å‰µå»ºå’Œåˆå§‹åŒ–æ¨¡å‹\n",
    "model = EfficientTextClassifier(\n",
    "    vocab_size=10000,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_classes=5\n",
    ")\n",
    "\n",
    "# æ¨¡å‹è¨­å®š\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"ğŸ¯ EfficientTextClassifier å·²å‰µå»º\")\n",
    "print(f\"ğŸ“Š æ¨¡å‹åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"ğŸ”§ æ¨¡å‹è¨­å‚™: {device}\")\n",
    "\n",
    "# è¨ˆç®—æ¨¡å‹å¤§å°\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "model_size_mb = (param_size + buffer_size) / 1024**2\n",
    "print(f\"ğŸ’¾ æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch åˆ° ONNX è½‰æ›\n",
    "\n",
    "### è½‰æ›æµç¨‹\n",
    "PyTorch â†’ ONNX â†’ TensorRT æ˜¯æ¨™æº–çš„å„ªåŒ–è·¯å¾‘ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ONNXExportConfig:\n",
    "    \"\"\"ONNX å°å‡ºé…ç½®\"\"\"\n",
    "    input_names: List[str]\n",
    "    output_names: List[str]\n",
    "    dynamic_axes: Dict[str, Any]\n",
    "    opset_version: int = 11\n",
    "    do_constant_folding: bool = True\n",
    "    verbose: bool = False\n",
    "\n",
    "class PyTorchToONNXConverter:\n",
    "    \"\"\"PyTorch åˆ° ONNX è½‰æ›å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "    def export_onnx(self, \n",
    "                   dummy_input: torch.Tensor,\n",
    "                   onnx_path: Path,\n",
    "                   config: ONNXExportConfig) -> bool:\n",
    "        \"\"\"å°å‡º ONNX æ¨¡å‹\"\"\"\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"é–‹å§‹å°å‡º ONNX æ¨¡å‹åˆ°: {onnx_path}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                torch.onnx.export(\n",
    "                    self.model,\n",
    "                    dummy_input,\n",
    "                    str(onnx_path),\n",
    "                    export_params=True,\n",
    "                    opset_version=config.opset_version,\n",
    "                    do_constant_folding=config.do_constant_folding,\n",
    "                    input_names=config.input_names,\n",
    "                    output_names=config.output_names,\n",
    "                    dynamic_axes=config.dynamic_axes,\n",
    "                    verbose=config.verbose\n",
    "                )\n",
    "            \n",
    "            # é©—è­‰ ONNX æ¨¡å‹\n",
    "            self._validate_onnx_model(onnx_path, dummy_input)\n",
    "            \n",
    "            logger.info(\"ONNX å°å‡ºæˆåŠŸ\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ONNX å°å‡ºå¤±æ•—: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _validate_onnx_model(self, onnx_path: Path, dummy_input: torch.Tensor):\n",
    "        \"\"\"é©—è­‰ ONNX æ¨¡å‹\"\"\"\n",
    "        \n",
    "        # è¼‰å…¥å’Œæª¢æŸ¥ ONNX æ¨¡å‹\n",
    "        onnx_model = onnx.load(str(onnx_path))\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        \n",
    "        # æ¯”è¼ƒ PyTorch å’Œ ONNX è¼¸å‡º\n",
    "        with torch.no_grad():\n",
    "            pytorch_output = self.model(dummy_input).cpu().numpy()\n",
    "        \n",
    "        # ONNX Runtime æ¨ç†\n",
    "        ort_session = ort.InferenceSession(str(onnx_path))\n",
    "        ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.cpu().numpy()}\n",
    "        onnx_output = ort_session.run(None, ort_inputs)[0]\n",
    "        \n",
    "        # æª¢æŸ¥è¼¸å‡ºä¸€è‡´æ€§\n",
    "        max_diff = np.max(np.abs(pytorch_output - onnx_output))\n",
    "        logger.info(f\"PyTorch vs ONNX æœ€å¤§å·®ç•°: {max_diff:.6f}\")\n",
    "        \n",
    "        if max_diff > 1e-3:\n",
    "            logger.warning(f\"è¼¸å‡ºå·®ç•°è¼ƒå¤§: {max_diff}\")\n",
    "        else:\n",
    "            logger.info(\"è¼¸å‡ºä¸€è‡´æ€§é©—è­‰é€šé\")\n",
    "\n",
    "# æº–å‚™å°å‡ºé…ç½®\n",
    "export_configs = {\n",
    "    # å›ºå®šå½¢ç‹€ç‰ˆæœ¬ (ç”¨æ–¼æœ€å¤§å„ªåŒ–)\n",
    "    \"fixed\": ONNXExportConfig(\n",
    "        input_names=[\"input_ids\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={},  # ç„¡å‹•æ…‹è»¸\n",
    "        opset_version=11\n",
    "    ),\n",
    "    \n",
    "    # å‹•æ…‹å½¢ç‹€ç‰ˆæœ¬ (ç”¨æ–¼éˆæ´»æ€§)\n",
    "    \"dynamic\": ONNXExportConfig(\n",
    "        input_names=[\"input_ids\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"logits\": {0: \"batch_size\"}\n",
    "        },\n",
    "        opset_version=11\n",
    "    )\n",
    "}\n",
    "\n",
    "# å‰µå»ºè½‰æ›å™¨\n",
    "converter = PyTorchToONNXConverter(model, device)\n",
    "\n",
    "# å°å‡ºä¸åŒç‰ˆæœ¬çš„ ONNX æ¨¡å‹\n",
    "export_results = {}\n",
    "\n",
    "for config_name, config in export_configs.items():\n",
    "    print(f\"\\nğŸ”„ å°å‡º {config_name} ONNX æ¨¡å‹\")\n",
    "    \n",
    "    # å‰µå»ºç¤ºä¾‹è¼¸å…¥\n",
    "    if config_name == \"fixed\":\n",
    "        # å›ºå®šå½¢ç‹€: batch=4, seq_len=128\n",
    "        dummy_input = torch.randint(1, 1000, (4, 128), dtype=torch.long, device=device)\n",
    "        onnx_path = ONNX_DIR / f\"text_classifier_{config_name}_b4_s128.onnx\"\n",
    "    else:\n",
    "        # å‹•æ…‹å½¢ç‹€: batch=1, seq_len=64 (ç¤ºä¾‹)\n",
    "        dummy_input = torch.randint(1, 1000, (1, 64), dtype=torch.long, device=device)\n",
    "        onnx_path = ONNX_DIR / f\"text_classifier_{config_name}.onnx\"\n",
    "    \n",
    "    # åŸ·è¡Œå°å‡º\n",
    "    success = converter.export_onnx(dummy_input, onnx_path, config)\n",
    "    export_results[config_name] = {\n",
    "        \"path\": onnx_path,\n",
    "        \"success\": success,\n",
    "        \"dummy_input_shape\": dummy_input.shape\n",
    "    }\n",
    "    \n",
    "    if success:\n",
    "        # æª¢æŸ¥æ–‡ä»¶å¤§å°\n",
    "        file_size_mb = onnx_path.stat().st_size / 1024**2\n",
    "        print(f\"  âœ… å°å‡ºæˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB\")\n",
    "        print(f\"  ğŸ“„ æ–‡ä»¶è·¯å¾‘: {onnx_path}\")\n",
    "    else:\n",
    "        print(f\"  âŒ å°å‡ºå¤±æ•—\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ONNX å°å‡ºç¸½çµ: {sum(1 for r in export_results.values() if r['success'])}/{len(export_results)} æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorRT å¼•æ“æ§‹å»º\n",
    "\n",
    "### å¼•æ“å„ªåŒ–ç­–ç•¥\n",
    "- **FP16 ç²¾åº¦**: 2x é€Ÿåº¦æå‡ï¼Œæœ€å°ç²¾åº¦æå¤±\n",
    "- **INT8 é‡åŒ–**: 4x é€Ÿåº¦æå‡ï¼Œéœ€è¦æ ¡æº–æ•¸æ“š\n",
    "- **å‹•æ…‹å½¢ç‹€**: æ”¯æ´ä¸åŒè¼¸å…¥å°ºå¯¸\n",
    "- **å±¤èåˆ**: è‡ªå‹•å„ªåŒ–é‹ç®—åœ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRT_AVAILABLE:\n",
    "    \n",
    "    @dataclass\n",
    "    class TensorRTBuildConfig:\n",
    "        \"\"\"TensorRT æ§‹å»ºé…ç½®\"\"\"\n",
    "        precision: str = \"fp16\"  # fp32, fp16, int8\n",
    "        max_batch_size: int = 16\n",
    "        max_workspace_size: int = 1 << 30  # 1GB\n",
    "        optimization_level: int = 3\n",
    "        enable_dynamic_shapes: bool = True\n",
    "        min_shapes: Dict[str, Tuple] = None\n",
    "        opt_shapes: Dict[str, Tuple] = None\n",
    "        max_shapes: Dict[str, Tuple] = None\n",
    "    \n",
    "    class TensorRTEngineBuilder:\n",
    "        \"\"\"TensorRT å¼•æ“æ§‹å»ºå™¨\"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.logger = trt.Logger(trt.Logger.INFO)\n",
    "            self.builder = trt.Builder(self.logger)\n",
    "            \n",
    "        def build_engine_from_onnx(self, \n",
    "                                  onnx_path: Path, \n",
    "                                  engine_path: Path,\n",
    "                                  config: TensorRTBuildConfig) -> bool:\n",
    "            \"\"\"å¾ ONNX æ§‹å»º TensorRT å¼•æ“\"\"\"\n",
    "            \n",
    "            try:\n",
    "                logger.info(f\"é–‹å§‹æ§‹å»º TensorRT å¼•æ“: {engine_path}\")\n",
    "                logger.info(f\"ç²¾åº¦æ¨¡å¼: {config.precision}\")\n",
    "                \n",
    "                # å‰µå»ºç¶²çµ¡\n",
    "                network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "                network = self.builder.create_network(network_flags)\n",
    "                parser = trt.OnnxParser(network, self.logger)\n",
    "                \n",
    "                # è§£æ ONNX\n",
    "                with open(onnx_path, 'rb') as model:\n",
    "                    if not parser.parse(model.read()):\n",
    "                        logger.error(\"ONNX è§£æå¤±æ•—\")\n",
    "                        for error in range(parser.num_errors):\n",
    "                            logger.error(parser.get_error(error))\n",
    "                        return False\n",
    "                \n",
    "                # å‰µå»ºæ§‹å»ºé…ç½®\n",
    "                build_config = self.builder.create_builder_config()\n",
    "                build_config.max_workspace_size = config.max_workspace_size\n",
    "                \n",
    "                # è¨­å®šç²¾åº¦\n",
    "                if config.precision == \"fp16\":\n",
    "                    if self.builder.platform_has_fast_fp16:\n",
    "                        build_config.set_flag(trt.BuilderFlag.FP16)\n",
    "                        logger.info(\"å•Ÿç”¨ FP16 ç²¾åº¦\")\n",
    "                    else:\n",
    "                        logger.warning(\"GPU ä¸æ”¯æ´ FP16ï¼Œä½¿ç”¨ FP32\")\n",
    "                elif config.precision == \"int8\":\n",
    "                    if self.builder.platform_has_fast_int8:\n",
    "                        build_config.set_flag(trt.BuilderFlag.INT8)\n",
    "                        # é€™è£¡éœ€è¦æ ¡æº–å™¨ï¼Œç°¡åŒ–ç¤ºä¾‹è·³é\n",
    "                        logger.info(\"å•Ÿç”¨ INT8 ç²¾åº¦ (éœ€è¦æ ¡æº–æ•¸æ“š)\")\n",
    "                    else:\n",
    "                        logger.warning(\"GPU ä¸æ”¯æ´ INT8ï¼Œä½¿ç”¨ FP32\")\n",
    "                \n",
    "                # å‹•æ…‹å½¢ç‹€é…ç½®\n",
    "                if config.enable_dynamic_shapes and config.min_shapes:\n",
    "                    profile = self.builder.create_optimization_profile()\n",
    "                    \n",
    "                    for input_name in config.min_shapes:\n",
    "                        min_shape = config.min_shapes[input_name]\n",
    "                        opt_shape = config.opt_shapes.get(input_name, min_shape)\n",
    "                        max_shape = config.max_shapes.get(input_name, min_shape)\n",
    "                        \n",
    "                        profile.set_shape(input_name, min_shape, opt_shape, max_shape)\n",
    "                        logger.info(f\"å‹•æ…‹å½¢ç‹€ {input_name}: min={min_shape}, opt={opt_shape}, max={max_shape}\")\n",
    "                    \n",
    "                    build_config.add_optimization_profile(profile)\n",
    "                \n",
    "                # æ§‹å»ºå¼•æ“\n",
    "                logger.info(\"é–‹å§‹æ§‹å»ºå¼•æ“... (é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)\")\n",
    "                engine = self.builder.build_engine(network, build_config)\n",
    "                \n",
    "                if engine is None:\n",
    "                    logger.error(\"å¼•æ“æ§‹å»ºå¤±æ•—\")\n",
    "                    return False\n",
    "                \n",
    "                # åºåˆ—åŒ–å¼•æ“\n",
    "                with open(engine_path, 'wb') as f:\n",
    "                    f.write(engine.serialize())\n",
    "                \n",
    "                logger.info(f\"TensorRT å¼•æ“å·²å„²å­˜: {engine_path}\")\n",
    "                \n",
    "                # é¡¯ç¤ºå¼•æ“è³‡è¨Š\n",
    "                self._print_engine_info(engine, engine_path)\n",
    "                \n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"TensorRT å¼•æ“æ§‹å»ºéŒ¯èª¤: {e}\")\n",
    "                return False\n",
    "        \n",
    "        def _print_engine_info(self, engine, engine_path: Path):\n",
    "            \"\"\"é¡¯ç¤ºå¼•æ“è³‡è¨Š\"\"\"\n",
    "            file_size_mb = engine_path.stat().st_size / 1024**2\n",
    "            logger.info(f\"å¼•æ“æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB\")\n",
    "            logger.info(f\"è¼¸å…¥æ•¸é‡: {engine.num_bindings // 2}\")\n",
    "            logger.info(f\"æœ€å¤§æ‰¹æ¬¡å¤§å°: {engine.max_batch_size}\")\n",
    "            \n",
    "            for i in range(engine.num_bindings):\n",
    "                name = engine.get_binding_name(i)\n",
    "                shape = engine.get_binding_shape(i)\n",
    "                dtype = engine.get_binding_dtype(i)\n",
    "                is_input = engine.binding_is_input(i)\n",
    "                logger.info(f\"  {'è¼¸å…¥' if is_input else 'è¼¸å‡º'} {name}: {shape} ({dtype})\")\n",
    "    \n",
    "    # å‰µå»º TensorRT å¼•æ“æ§‹å»ºå™¨\n",
    "    builder = TensorRTEngineBuilder()\n",
    "    \n",
    "    # æ§‹å»ºä¸åŒç²¾åº¦çš„å¼•æ“\n",
    "    build_configs = {\n",
    "        \"fp32\": TensorRTBuildConfig(\n",
    "            precision=\"fp32\",\n",
    "            max_batch_size=16,\n",
    "            enable_dynamic_shapes=False\n",
    "        ),\n",
    "        \"fp16\": TensorRTBuildConfig(\n",
    "            precision=\"fp16\",\n",
    "            max_batch_size=16,\n",
    "            enable_dynamic_shapes=False\n",
    "        ),\n",
    "        \"dynamic_fp16\": TensorRTBuildConfig(\n",
    "            precision=\"fp16\",\n",
    "            max_batch_size=0,  # å‹•æ…‹æ‰¹æ¬¡\n",
    "            enable_dynamic_shapes=True,\n",
    "            min_shapes={\"input_ids\": (1, 16)},\n",
    "            opt_shapes={\"input_ids\": (4, 128)},\n",
    "            max_shapes={\"input_ids\": (16, 512)}\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    build_results = {}\n",
    "    \n",
    "    for config_name, build_config in build_configs.items():\n",
    "        print(f\"\\nğŸ”§ æ§‹å»º {config_name} TensorRT å¼•æ“\")\n",
    "        \n",
    "        # é¸æ“‡å°æ‡‰çš„ ONNX æ¨¡å‹\n",
    "        if \"dynamic\" in config_name:\n",
    "            onnx_path = export_results[\"dynamic\"][\"path\"]\n",
    "        else:\n",
    "            onnx_path = export_results[\"fixed\"][\"path\"]\n",
    "        \n",
    "        if not export_results[\"dynamic\" if \"dynamic\" in config_name else \"fixed\"][\"success\"]:\n",
    "            print(f\"  âš ï¸  è·³é {config_name}ï¼ŒONNX æ¨¡å‹ä¸å¯ç”¨\")\n",
    "            continue\n",
    "        \n",
    "        engine_path = TRT_DIR / f\"text_classifier_{config_name}.engine\"\n",
    "        \n",
    "        # æ§‹å»ºå¼•æ“\n",
    "        success = builder.build_engine_from_onnx(onnx_path, engine_path, build_config)\n",
    "        build_results[config_name] = {\n",
    "            \"path\": engine_path,\n",
    "            \"success\": success,\n",
    "            \"config\": build_config\n",
    "        }\n",
    "        \n",
    "        if success:\n",
    "            print(f\"  âœ… å¼•æ“æ§‹å»ºæˆåŠŸ\")\n",
    "        else:\n",
    "            print(f\"  âŒ å¼•æ“æ§‹å»ºå¤±æ•—\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š TensorRT å¼•æ“æ§‹å»ºç¸½çµ: {sum(1 for r in build_results.values() if r['success'])}/{len(build_results)} æˆåŠŸ\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸  TensorRT æœªå®‰è£ï¼Œè·³éå¼•æ“æ§‹å»º\")\n",
    "    print(\"   å¯ä»¥ä½¿ç”¨ trtexec å‘½ä»¤è¡Œå·¥å…·ä½œç‚ºæ›¿ä»£\")\n",
    "    \n",
    "    # ç”Ÿæˆ trtexec å‘½ä»¤ç¤ºä¾‹\n",
    "    trtexec_commands = []\n",
    "    \n",
    "    if export_results[\"fixed\"][\"success\"]:\n",
    "        onnx_path = export_results[\"fixed\"][\"path\"]\n",
    "        engine_path = TRT_DIR / \"text_classifier_fp16.engine\"\n",
    "        \n",
    "        cmd = f\"\"\"trtexec --onnx={onnx_path} \\\n",
    "  --saveEngine={engine_path} \\\n",
    "  --fp16 \\\n",
    "  --workspace=1024 \\\n",
    "  --verbose\"\"\"\n",
    "        \n",
    "        trtexec_commands.append((\"FP16 å¼•æ“\", cmd))\n",
    "    \n",
    "    if trtexec_commands:\n",
    "        print(\"\\nğŸ“œ trtexec å‘½ä»¤åƒè€ƒ:\")\n",
    "        for name, cmd in trtexec_commands:\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(cmd)\n",
    "    \n",
    "    build_results = {}  # ç©ºå­—å…¸ç”¨æ–¼å¾ŒçºŒä»£ç¢¼å…¼å®¹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Triton TensorRT Backend é…ç½®\n",
    "\n",
    "### å»ºç«‹ Triton æ¨¡å‹å€‰åº«çµæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonTensorRTModelGenerator:\n",
    "    \"\"\"Triton TensorRT æ¨¡å‹ç”Ÿæˆå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, model_repo_path: Path):\n",
    "        self.model_repo = model_repo_path\n",
    "        \n",
    "    def create_tensorrt_model(self, \n",
    "                             model_name: str,\n",
    "                             engine_path: Path,\n",
    "                             input_spec: Dict[str, Any],\n",
    "                             output_spec: Dict[str, Any],\n",
    "                             config_params: Dict[str, Any] = None) -> bool:\n",
    "        \"\"\"å‰µå»º TensorRT æ¨¡å‹é…ç½®\"\"\"\n",
    "        \n",
    "        model_dir = self.model_repo / model_name\n",
    "        version_dir = model_dir / \"1\"\n",
    "        version_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # è¤‡è£½å¼•æ“æ–‡ä»¶\n",
    "        if engine_path.exists():\n",
    "            target_engine = version_dir / \"model.plan\"\n",
    "            shutil.copy2(engine_path, target_engine)\n",
    "            logger.info(f\"TensorRT å¼•æ“å·²è¤‡è£½åˆ°: {target_engine}\")\n",
    "        else:\n",
    "            logger.error(f\"å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨: {engine_path}\")\n",
    "            return False\n",
    "        \n",
    "        # ç”Ÿæˆé…ç½®æ–‡ä»¶\n",
    "        config = self._generate_config(model_name, input_spec, output_spec, config_params)\n",
    "        config_path = model_dir / \"config.pbtxt\"\n",
    "        \n",
    "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(config)\n",
    "        \n",
    "        logger.info(f\"é…ç½®æ–‡ä»¶å·²å‰µå»º: {config_path}\")\n",
    "        return True\n",
    "    \n",
    "    def _generate_config(self, \n",
    "                        model_name: str,\n",
    "                        input_spec: Dict[str, Any],\n",
    "                        output_spec: Dict[str, Any],\n",
    "                        config_params: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"ç”Ÿæˆ TensorRT æ¨¡å‹é…ç½®\"\"\"\n",
    "        \n",
    "        config_params = config_params or {}\n",
    "        \n",
    "        # åŸºæœ¬é…ç½®\n",
    "        config_lines = [\n",
    "            f'name: \"{model_name}\"',\n",
    "            'platform: \"tensorrt_plan\"',\n",
    "            f'max_batch_size: {config_params.get(\"max_batch_size\", 16)}',\n",
    "            ''\n",
    "        ]\n",
    "        \n",
    "        # è¼¸å…¥é…ç½®\n",
    "        for input_name, spec in input_spec.items():\n",
    "            config_lines.extend([\n",
    "                'input {',\n",
    "                f'  name: \"{input_name}\"',\n",
    "                f'  data_type: {spec[\"data_type\"]}',\n",
    "                f'  dims: {spec[\"dims\"]}',\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # è¼¸å‡ºé…ç½®\n",
    "        for output_name, spec in output_spec.items():\n",
    "            config_lines.extend([\n",
    "                'output {',\n",
    "                f'  name: \"{output_name}\"',\n",
    "                f'  data_type: {spec[\"data_type\"]}',\n",
    "                f'  dims: {spec[\"dims\"]}',\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # å¯¦ä¾‹çµ„é…ç½®\n",
    "        instance_count = config_params.get(\"instance_count\", 1)\n",
    "        config_lines.extend([\n",
    "            'instance_group {',\n",
    "            f'  count: {instance_count}',\n",
    "            '  kind: KIND_GPU',\n",
    "            '}',\n",
    "            ''\n",
    "        ])\n",
    "        \n",
    "        # å‹•æ…‹æ‰¹æ¬¡é…ç½®\n",
    "        if config_params.get(\"enable_dynamic_batching\", True):\n",
    "            preferred_batch_sizes = config_params.get(\"preferred_batch_sizes\", [1, 4, 8])\n",
    "            config_lines.extend([\n",
    "                'dynamic_batching {',\n",
    "                f'  preferred_batch_size: {preferred_batch_sizes}',\n",
    "                f'  max_queue_delay_microseconds: {config_params.get(\"max_queue_delay\", 100)}',\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # TensorRT ç‰¹å®šå„ªåŒ–\n",
    "        config_lines.extend([\n",
    "            '# TensorRT å„ªåŒ–åƒæ•¸',\n",
    "            'optimization {',\n",
    "            '  cuda {',\n",
    "            '    graphs: true',\n",
    "            '    graph_spec {',\n",
    "            '      batch_size: 1',\n",
    "            '      input {',\n",
    "            f'        key: \"{list(input_spec.keys())[0]}\"',\n",
    "            '        value {',\n",
    "            f'          dim: [ {config_params.get(\"opt_input_dims\", \"128\")} ]',\n",
    "            '        }',\n",
    "            '      }',\n",
    "            '    }',\n",
    "            '  }',\n",
    "            '}',\n",
    "            '',\n",
    "            '# æ¨¡å‹é ç†±',\n",
    "            'model_warmup {',\n",
    "            '  name: \"warmup_sample\"',\n",
    "            '  batch_size: 1',\n",
    "            '  inputs {',\n",
    "            f'    key: \"{list(input_spec.keys())[0]}\"',\n",
    "            '    value {',\n",
    "            f'      data_type: {list(input_spec.values())[0][\"data_type\"]}',\n",
    "            f'      dims: [ {config_params.get(\"warmup_dims\", \"128\")} ]',\n",
    "            '      zero_data: true',\n",
    "            '    }',\n",
    "            '  }',\n",
    "            '}'\n",
    "        ])\n",
    "        \n",
    "        return '\\n'.join(config_lines)\n",
    "\n",
    "# å‰µå»º Triton æ¨¡å‹ç”Ÿæˆå™¨\n",
    "model_generator = TritonTensorRTModelGenerator(MODEL_REPO)\n",
    "\n",
    "# ç‚ºæ¯å€‹æˆåŠŸæ§‹å»ºçš„å¼•æ“å‰µå»º Triton æ¨¡å‹\n",
    "triton_models = {}\n",
    "\n",
    "if TRT_AVAILABLE and build_results:\n",
    "    for config_name, result in build_results.items():\n",
    "        if result[\"success\"]:\n",
    "            model_name = f\"text_classifier_trt_{config_name}\"\n",
    "            \n",
    "            # è¼¸å…¥è¼¸å‡ºè¦æ ¼\n",
    "            if \"dynamic\" in config_name:\n",
    "                input_spec = {\n",
    "                    \"input_ids\": {\n",
    "                        \"data_type\": \"TYPE_INT64\",\n",
    "                        \"dims\": [-1]  # å‹•æ…‹åºåˆ—é•·åº¦\n",
    "                    }\n",
    "                }\n",
    "                max_batch_size = 0  # å‹•æ…‹æ‰¹æ¬¡\n",
    "            else:\n",
    "                input_spec = {\n",
    "                    \"input_ids\": {\n",
    "                        \"data_type\": \"TYPE_INT64\",\n",
    "                        \"dims\": [128]  # å›ºå®šåºåˆ—é•·åº¦\n",
    "                    }\n",
    "                }\n",
    "                max_batch_size = 16\n",
    "            \n",
    "            output_spec = {\n",
    "                \"logits\": {\n",
    "                    \"data_type\": \"TYPE_FP32\",\n",
    "                    \"dims\": [5]  # 5 å€‹åˆ†é¡\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # é…ç½®åƒæ•¸\n",
    "            config_params = {\n",
    "                \"max_batch_size\": max_batch_size,\n",
    "                \"instance_count\": 1,\n",
    "                \"enable_dynamic_batching\": max_batch_size > 0,\n",
    "                \"preferred_batch_sizes\": [1, 4, 8] if max_batch_size > 0 else [],\n",
    "                \"opt_input_dims\": \"128\",\n",
    "                \"warmup_dims\": \"128\"\n",
    "            }\n",
    "            \n",
    "            # å‰µå»ºæ¨¡å‹\n",
    "            success = model_generator.create_tensorrt_model(\n",
    "                model_name,\n",
    "                result[\"path\"],\n",
    "                input_spec,\n",
    "                output_spec,\n",
    "                config_params\n",
    "            )\n",
    "            \n",
    "            triton_models[model_name] = {\n",
    "                \"engine_path\": result[\"path\"],\n",
    "                \"success\": success,\n",
    "                \"config_name\": config_name\n",
    "            }\n",
    "            \n",
    "            if success:\n",
    "                print(f\"âœ… Triton æ¨¡å‹å·²å‰µå»º: {model_name}\")\n",
    "            else:\n",
    "                print(f\"âŒ Triton æ¨¡å‹å‰µå»ºå¤±æ•—: {model_name}\")\n",
    "\n",
    "# ç¸½çµ\n",
    "print(f\"\\nğŸ“Š Triton TensorRT æ¨¡å‹å‰µå»ºç¸½çµ:\")\n",
    "print(f\"   æˆåŠŸ: {sum(1 for m in triton_models.values() if m['success'])}\")\n",
    "print(f\"   ç¸½è¨ˆ: {len(triton_models)}\")\n",
    "\n",
    "if not TRT_AVAILABLE:\n",
    "    print(\"\\nâš ï¸  TensorRT æœªå®‰è£ï¼Œå·²è·³éæ¨¡å‹å‰µå»º\")\n",
    "    print(\"   å®‰è£ TensorRT å¾Œå¯é«”é©—å®Œæ•´åŠŸèƒ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½åŸºæº–æ¸¬è©¦ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveBenchmark:\n",
    "    \"\"\"ç¶œåˆæ€§èƒ½åŸºæº–æ¸¬è©¦ç³»çµ±\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.results = {}\n",
    "        \n",
    "    def benchmark_model(self, \n",
    "                       model_name: str,\n",
    "                       test_data: List[np.ndarray],\n",
    "                       iterations: int = 100,\n",
    "                       warmup_iterations: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"å–®å€‹æ¨¡å‹åŸºæº–æ¸¬è©¦\"\"\"\n",
    "        \n",
    "        try:\n",
    "            client = httpclient.InferenceServerClient(url=self.triton_url)\n",
    "            \n",
    "            if not client.is_model_ready(model_name):\n",
    "                logger.warning(f\"æ¨¡å‹ {model_name} æœªå°±ç·’\")\n",
    "                return {\"error\": \"Model not ready\"}\n",
    "            \n",
    "            logger.info(f\"é–‹å§‹åŸºæº–æ¸¬è©¦: {model_name}\")\n",
    "            \n",
    "            # é ç†±\n",
    "            for i in range(warmup_iterations):\n",
    "                test_input = test_data[i % len(test_data)]\n",
    "                self._single_inference(client, model_name, test_input)\n",
    "            \n",
    "            logger.info(f\"é ç†±å®Œæˆï¼Œé–‹å§‹æ­£å¼æ¸¬è©¦ ({iterations} æ¬¡)\")\n",
    "            \n",
    "            # æ­£å¼æ¸¬è©¦\n",
    "            latencies = []\n",
    "            errors = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for i in range(iterations):\n",
    "                test_input = test_data[i % len(test_data)]\n",
    "                \n",
    "                try:\n",
    "                    request_start = time.time()\n",
    "                    response = self._single_inference(client, model_name, test_input)\n",
    "                    request_end = time.time()\n",
    "                    \n",
    "                    latency = (request_end - request_start) * 1000\n",
    "                    latencies.append(latency)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    logger.error(f\"æ¨ç†éŒ¯èª¤: {e}\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "            \n",
    "            # è¨ˆç®—çµ±è¨ˆ\n",
    "            if latencies:\n",
    "                stats = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"total_requests\": iterations,\n",
    "                    \"successful_requests\": len(latencies),\n",
    "                    \"error_count\": errors,\n",
    "                    \"error_rate_percent\": (errors / iterations) * 100,\n",
    "                    \"total_time_seconds\": total_time,\n",
    "                    \"throughput_rps\": len(latencies) / total_time,\n",
    "                    \"latency_ms\": {\n",
    "                        \"mean\": np.mean(latencies),\n",
    "                        \"median\": np.median(latencies),\n",
    "                        \"p95\": np.percentile(latencies, 95),\n",
    "                        \"p99\": np.percentile(latencies, 99),\n",
    "                        \"min\": np.min(latencies),\n",
    "                        \"max\": np.max(latencies),\n",
    "                        \"std\": np.std(latencies)\n",
    "                    },\n",
    "                    \"raw_latencies\": latencies\n",
    "                }\n",
    "            else:\n",
    "                stats = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"total_requests\": iterations,\n",
    "                    \"successful_requests\": 0,\n",
    "                    \"error_count\": errors,\n",
    "                    \"error_rate_percent\": 100.0,\n",
    "                    \"throughput_rps\": 0\n",
    "                }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"åŸºæº–æ¸¬è©¦å¤±æ•— {model_name}: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def _single_inference(self, client, model_name: str, input_data: np.ndarray):\n",
    "        \"\"\"åŸ·è¡Œå–®æ¬¡æ¨ç†\"\"\"\n",
    "        inputs = [httpclient.InferInput(\"input_ids\", input_data.shape, \"INT64\")]\n",
    "        inputs[0].set_data_from_numpy(input_data)\n",
    "        \n",
    "        response = client.infer(model_name, inputs)\n",
    "        return response.as_numpy(\"logits\")\n",
    "    \n",
    "    def compare_models(self, \n",
    "                      model_names: List[str],\n",
    "                      test_data: List[np.ndarray],\n",
    "                      iterations: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"æ¯”è¼ƒå¤šå€‹æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "        \n",
    "        comparison_results = {}\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            print(f\"\\nğŸš€ æ¸¬è©¦æ¨¡å‹: {model_name}\")\n",
    "            result = self.benchmark_model(model_name, test_data, iterations)\n",
    "            comparison_results[model_name] = result\n",
    "            \n",
    "            if \"error\" not in result and \"latency_ms\" in result:\n",
    "                latency_stats = result[\"latency_ms\"]\n",
    "                print(f\"  âš¡ ååé‡: {result['throughput_rps']:.2f} RPS\")\n",
    "                print(f\"  ğŸ“Š å¹³å‡å»¶é²: {latency_stats['mean']:.2f}ms\")\n",
    "                print(f\"  ğŸ“ˆ P95 å»¶é²: {latency_stats['p95']:.2f}ms\")\n",
    "                print(f\"  âŒ éŒ¯èª¤ç‡: {result['error_rate_percent']:.2f}%\")\n",
    "            else:\n",
    "                print(f\"  âŒ æ¸¬è©¦å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}\")\n",
    "        \n",
    "        return comparison_results\n",
    "    \n",
    "    def generate_performance_report(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"ç”Ÿæˆæ€§èƒ½å ±å‘Š\"\"\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"# TensorRT vs PyTorch æ€§èƒ½å°æ¯”å ±å‘Š\",\n",
    "            \"=\" * 50,\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        # æˆåŠŸçš„çµæœ\n",
    "        successful_results = {\n",
    "            k: v for k, v in results.items() \n",
    "            if \"error\" not in v and \"latency_ms\" in v\n",
    "        }\n",
    "        \n",
    "        if not successful_results:\n",
    "            return \"ç„¡å¯ç”¨çš„æ¸¬è©¦çµæœ\\n\"\n",
    "        \n",
    "        # æŒ‰ååé‡æ’åº\n",
    "        sorted_results = sorted(\n",
    "            successful_results.items(),\n",
    "            key=lambda x: x[1][\"throughput_rps\"],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # è©³ç´°çµæœè¡¨æ ¼\n",
    "        report_lines.extend([\n",
    "            \"## è©³ç´°æ€§èƒ½æŒ‡æ¨™\",\n",
    "            \"\",\n",
    "            \"| æ¨¡å‹ | ååé‡ (RPS) | å¹³å‡å»¶é² (ms) | P95 å»¶é² (ms) | éŒ¯èª¤ç‡ (%) |\",\n",
    "            \"|------|-------------|--------------|--------------|----------|\"\n",
    "        ])\n",
    "        \n",
    "        for model_name, result in sorted_results:\n",
    "            throughput = result[\"throughput_rps\"]\n",
    "            mean_latency = result[\"latency_ms\"][\"mean\"]\n",
    "            p95_latency = result[\"latency_ms\"][\"p95\"]\n",
    "            error_rate = result[\"error_rate_percent\"]\n",
    "            \n",
    "            report_lines.append(\n",
    "                f\"| {model_name} | {throughput:.2f} | {mean_latency:.2f} | {p95_latency:.2f} | {error_rate:.2f} |\"\n",
    "            )\n",
    "        \n",
    "        # æ€§èƒ½æå‡åˆ†æ\n",
    "        if len(sorted_results) > 1:\n",
    "            baseline_name, baseline_result = sorted_results[-1]  # æœ€æ…¢çš„ä½œç‚ºåŸºç·š\n",
    "            baseline_throughput = baseline_result[\"throughput_rps\"]\n",
    "            baseline_latency = baseline_result[\"latency_ms\"][\"mean\"]\n",
    "            \n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"## æ€§èƒ½æå‡åˆ†æ\",\n",
    "                f\"(ä»¥ {baseline_name} ä½œç‚ºåŸºç·š)\",\n",
    "                \"\"\n",
    "            ])\n",
    "            \n",
    "            for model_name, result in sorted_results[:-1]:\n",
    "                throughput_speedup = result[\"throughput_rps\"] / baseline_throughput\n",
    "                latency_speedup = baseline_latency / result[\"latency_ms\"][\"mean\"]\n",
    "                \n",
    "                report_lines.append(\n",
    "                    f\"- **{model_name}**: {throughput_speedup:.2f}x ååé‡æå‡, {latency_speedup:.2f}x å»¶é²æ”¹å–„\"\n",
    "                )\n",
    "        \n",
    "        # å»ºè­°\n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"## éƒ¨ç½²å»ºè­°\",\n",
    "            \"\",\n",
    "            \"### é«˜ååé‡å ´æ™¯\",\n",
    "            f\"- æ¨è–¦: {sorted_results[0][0]}\",\n",
    "            f\"- ååé‡: {sorted_results[0][1]['throughput_rps']:.2f} RPS\",\n",
    "            \"\",\n",
    "            \"### ä½å»¶é²å ´æ™¯\",\n",
    "        ])\n",
    "        \n",
    "        # æ‰¾å‡ºå»¶é²æœ€ä½çš„æ¨¡å‹\n",
    "        lowest_latency = min(\n",
    "            successful_results.items(),\n",
    "            key=lambda x: x[1][\"latency_ms\"][\"mean\"]\n",
    "        )\n",
    "        \n",
    "        report_lines.extend([\n",
    "            f\"- æ¨è–¦: {lowest_latency[0]}\",\n",
    "            f\"- å¹³å‡å»¶é²: {lowest_latency[1]['latency_ms']['mean']:.2f} ms\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# å‰µå»ºåŸºæº–æ¸¬è©¦å™¨\n",
    "benchmark = ComprehensiveBenchmark()\n",
    "\n",
    "# æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "def generate_tensorrt_test_data(num_samples: int = 50) -> List[np.ndarray]:\n",
    "    \"\"\"ç”Ÿæˆ TensorRT æ¸¬è©¦æ•¸æ“š\"\"\"\n",
    "    test_data = []\n",
    "    \n",
    "    # å›ºå®šé•·åº¦åºåˆ—ï¼ˆé©åˆå›ºå®šå½¢ç‹€å¼•æ“ï¼‰\n",
    "    lengths = [128]  # TensorRT é€šå¸¸ä½¿ç”¨å›ºå®šå½¢ç‹€æ•ˆæœæ›´å¥½\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        length = np.random.choice(lengths)\n",
    "        # ç”Ÿæˆéš¨æ©Ÿåºåˆ—\n",
    "        sequence = np.random.randint(1, 5000, size=(1, length), dtype=np.int64)\n",
    "        test_data.append(sequence)\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "tensorrt_test_data = generate_tensorrt_test_data()\n",
    "print(f\"ğŸ“‹ TensorRT æ¸¬è©¦æ•¸æ“šå·²ç”Ÿæˆ: {len(tensorrt_test_data)} å€‹æ¨£æœ¬\")\n",
    "print(f\"ğŸ“ åºåˆ—é•·åº¦: {[data.shape[1] for data in tensorrt_test_data[:5]]}\")\n",
    "\n",
    "# é¡¯ç¤ºå¯ç”¨æ¨¡å‹\n",
    "available_models = [name for name, info in triton_models.items() if info[\"success\"]]\n",
    "print(f\"\\nğŸ“‹ å¯ç”¨çš„ TensorRT æ¨¡å‹:\")\n",
    "for model in available_models:\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "if not available_models:\n",
    "    print(\"âš ï¸  æ²’æœ‰å¯ç”¨çš„ TensorRT æ¨¡å‹é€²è¡Œæ¸¬è©¦\")\n",
    "    print(\"   è«‹ç¢ºä¿ TensorRT å¼•æ“æ§‹å»ºæˆåŠŸä¸¦ä¸” Triton Server æ­£åœ¨é‹è¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åŸ·è¡Œæ€§èƒ½å°æ¯”æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_triton_server(max_wait_time: int = 60) -> bool:\n",
    "    \"\"\"ç­‰å¾… Triton Server å°±ç·’\"\"\"\n",
    "    print(\"â³ æª¢æŸ¥ Triton Server ç‹€æ…‹...\")\n",
    "    \n",
    "    for i in range(max_wait_time):\n",
    "        try:\n",
    "            client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "            if client.is_server_ready():\n",
    "                print(\"âœ… Triton Server å·²å°±ç·’\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(f\"  ç­‰å¾…ä¸­... ({i}/{max_wait_time}s)\")\n",
    "    \n",
    "    print(\"âŒ Triton Server é€£æ¥è¶…æ™‚\")\n",
    "    return False\n",
    "\n",
    "def run_tensorrt_benchmark():\n",
    "    \"\"\"åŸ·è¡Œ TensorRT æ€§èƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "    \n",
    "    if not wait_for_triton_server():\n",
    "        print(\"ç„¡æ³•é€£æ¥åˆ° Triton Server\")\n",
    "        print(\"\\nğŸš€ è¦å•Ÿå‹• Triton Serverï¼Œè«‹åŸ·è¡Œ:\")\n",
    "        print(\"   docker run -d --name triton-trt --gpus all \\\\\")\n",
    "        print(\"     -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\\\")\n",
    "        print(f\"     -v {MODEL_REPO.absolute()}:/models \\\\\")\n",
    "        print(\"     nvcr.io/nvidia/tritonserver:24.01-py3 \\\\\")\n",
    "        print(\"     tritonserver --model-repository=/models\")\n",
    "        return None\n",
    "    \n",
    "    # æª¢æŸ¥å¯ç”¨æ¨¡å‹\n",
    "    try:\n",
    "        client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "        server_models = [m[\"name\"] for m in client.get_model_repository_index()]\n",
    "        \n",
    "        # éæ¿¾å‡ºå¯¦éš›å¯ç”¨çš„æ¨¡å‹\n",
    "        testable_models = []\n",
    "        for model_name in available_models:\n",
    "            if model_name in server_models:\n",
    "                try:\n",
    "                    if client.is_model_ready(model_name):\n",
    "                        testable_models.append(model_name)\n",
    "                        print(f\"âœ… æ¨¡å‹å°±ç·’: {model_name}\")\n",
    "                    else:\n",
    "                        print(f\"âš ï¸  æ¨¡å‹æœªå°±ç·’: {model_name}\")\n",
    "                except:\n",
    "                    print(f\"âŒ æ¨¡å‹æª¢æŸ¥å¤±æ•—: {model_name}\")\n",
    "        \n",
    "        if not testable_models:\n",
    "            print(\"âŒ æ²’æœ‰å¯æ¸¬è©¦çš„æ¨¡å‹\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nğŸ¯ é–‹å§‹æ€§èƒ½å°æ¯”æ¸¬è©¦ ({len(testable_models)} å€‹æ¨¡å‹)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦\n",
    "        comparison_results = benchmark.compare_models(\n",
    "            testable_models,\n",
    "            tensorrt_test_data,\n",
    "            iterations=100\n",
    "        )\n",
    "        \n",
    "        # ç”Ÿæˆå ±å‘Š\n",
    "        performance_report = benchmark.generate_performance_report(comparison_results)\n",
    "        \n",
    "        # ä¿å­˜å ±å‘Š\n",
    "        report_path = BENCHMARK_DIR / \"tensorrt_performance_report.md\"\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(performance_report)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š æ€§èƒ½å ±å‘Šå·²å„²å­˜: {report_path}\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(performance_report)\n",
    "        \n",
    "        return comparison_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŸºæº–æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦ï¼ˆå¦‚æœæœ‰å¯ç”¨æ¨¡å‹ï¼‰\n",
    "if available_models:\n",
    "    print(\"ğŸ¯ æº–å‚™åŸ·è¡Œ TensorRT æ€§èƒ½åŸºæº–æ¸¬è©¦\")\n",
    "    benchmark_results = run_tensorrt_benchmark()\n",
    "else:\n",
    "    print(\"âš ï¸  è·³éæ€§èƒ½æ¸¬è©¦ï¼šæ²’æœ‰å¯ç”¨çš„ TensorRT æ¨¡å‹\")\n",
    "    benchmark_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ€§èƒ½è¦–è¦ºåŒ–åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_visualization(results: Dict[str, Any]):\n",
    "    \"\"\"å‰µå»ºæ€§èƒ½è¦–è¦ºåŒ–åœ–è¡¨\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âš ï¸  æ²’æœ‰æ€§èƒ½æ•¸æ“šå¯è¦–è¦ºåŒ–\")\n",
    "        return\n",
    "    \n",
    "    # éæ¿¾æˆåŠŸçš„çµæœ\n",
    "    successful_results = {\n",
    "        k: v for k, v in results.items() \n",
    "        if \"error\" not in v and \"latency_ms\" in v\n",
    "    }\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"âš ï¸  æ²’æœ‰æˆåŠŸçš„æ¸¬è©¦çµæœ\")\n",
    "        return\n",
    "    \n",
    "    # æº–å‚™æ•¸æ“š\n",
    "    model_names = list(successful_results.keys())\n",
    "    throughputs = [result[\"throughput_rps\"] for result in successful_results.values()]\n",
    "    mean_latencies = [result[\"latency_ms\"][\"mean\"] for result in successful_results.values()]\n",
    "    p95_latencies = [result[\"latency_ms\"][\"p95\"] for result in successful_results.values()]\n",
    "    \n",
    "    # ç°¡åŒ–æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰\n",
    "    display_names = []\n",
    "    for name in model_names:\n",
    "        if \"fp32\" in name:\n",
    "            display_names.append(\"TensorRT FP32\")\n",
    "        elif \"fp16\" in name:\n",
    "            if \"dynamic\" in name:\n",
    "                display_names.append(\"TensorRT FP16\\n(Dynamic)\")\n",
    "            else:\n",
    "                display_names.append(\"TensorRT FP16\")\n",
    "        else:\n",
    "            display_names.append(name.replace(\"text_classifier_trt_\", \"TRT \"))\n",
    "    \n",
    "    # å‰µå»ºåœ–è¡¨\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('TensorRT Backend æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # é¡è‰²é…ç½®\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    \n",
    "    # 1. ååé‡å°æ¯”\n",
    "    axes[0, 0].bar(range(len(display_names)), throughputs, \n",
    "                   color=colors[:len(display_names)], alpha=0.8)\n",
    "    axes[0, 0].set_title('ååé‡å°æ¯” (RPS)', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('æ¨¡å‹é…ç½®')\n",
    "    axes[0, 0].set_ylabel('æ¯ç§’è«‹æ±‚æ•¸ (RPS)')\n",
    "    axes[0, 0].set_xticks(range(len(display_names)))\n",
    "    axes[0, 0].set_xticklabels(display_names, rotation=45, ha='right')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for i, v in enumerate(throughputs):\n",
    "        axes[0, 0].text(i, v + max(throughputs) * 0.01, f'{v:.1f}', \n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. å»¶é²å°æ¯”\n",
    "    x = np.arange(len(display_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, mean_latencies, width, label='å¹³å‡å»¶é²', \n",
    "                   color='skyblue', alpha=0.8)\n",
    "    axes[0, 1].bar(x + width/2, p95_latencies, width, label='P95 å»¶é²',\n",
    "                   color='orange', alpha=0.8)\n",
    "    axes[0, 1].set_title('å»¶é²å°æ¯” (æ¯«ç§’)', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('æ¨¡å‹é…ç½®')\n",
    "    axes[0, 1].set_ylabel('å»¶é² (ms)')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(display_names, rotation=45, ha='right')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. å»¶é²åˆ†ä½ˆï¼ˆç®±ç·šåœ–ï¼‰\n",
    "    if len(successful_results) > 1:\n",
    "        latency_data = [result[\"raw_latencies\"] for result in successful_results.values()]\n",
    "        bp = axes[1, 0].boxplot(latency_data, labels=display_names, patch_artist=True)\n",
    "        \n",
    "        # è¨­å®šé¡è‰²\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(display_names)]):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        axes[1, 0].set_title('å»¶é²åˆ†ä½ˆ', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('æ¨¡å‹é…ç½®')\n",
    "        axes[1, 0].set_ylabel('å»¶é² (ms)')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'éœ€è¦å¤šå€‹æ¨¡å‹\\né€²è¡Œåˆ†ä½ˆæ¯”è¼ƒ', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    \n",
    "    # 4. æ€§èƒ½æå‡å€æ•¸\n",
    "    if len(successful_results) > 1:\n",
    "        # ä»¥æœ€æ…¢çš„æ¨¡å‹ä½œç‚ºåŸºç·š\n",
    "        baseline_throughput = min(throughputs)\n",
    "        baseline_latency = max(mean_latencies)\n",
    "        \n",
    "        speedup_throughput = [t / baseline_throughput for t in throughputs]\n",
    "        speedup_latency = [baseline_latency / l for l in mean_latencies]\n",
    "        \n",
    "        x = np.arange(len(display_names))\n",
    "        axes[1, 1].bar(x - width/2, speedup_throughput, width, \n",
    "                      label='ååé‡æå‡', color='lightgreen', alpha=0.8)\n",
    "        axes[1, 1].bar(x + width/2, speedup_latency, width,\n",
    "                      label='å»¶é²æ”¹å–„', color='lightcoral', alpha=0.8)\n",
    "        \n",
    "        axes[1, 1].set_title('æ€§èƒ½æå‡å€æ•¸', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('æ¨¡å‹é…ç½®')\n",
    "        axes[1, 1].set_ylabel('æå‡å€æ•¸')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(display_names, rotation=45, ha='right')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ  1x åŸºç·š\n",
    "        axes[1, 1].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='åŸºç·š')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'éœ€è¦å¤šå€‹æ¨¡å‹\\né€²è¡Œæå‡æ¯”è¼ƒ', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜åœ–è¡¨\n",
    "    chart_path = BENCHMARK_DIR / \"tensorrt_performance_charts.png\"\n",
    "    plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š æ€§èƒ½åœ–è¡¨å·²å„²å­˜: {chart_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# å‰µå»ºè¦–è¦ºåŒ–åœ–è¡¨ï¼ˆå¦‚æœæœ‰æ¸¬è©¦çµæœï¼‰\n",
    "if benchmark_results:\n",
    "    print(\"ğŸ“Š ç”Ÿæˆæ€§èƒ½è¦–è¦ºåŒ–åœ–è¡¨...\")\n",
    "    create_performance_visualization(benchmark_results)\n",
    "else:\n",
    "    print(\"âš ï¸  è·³éè¦–è¦ºåŒ–ï¼šæ²’æœ‰æ¸¬è©¦çµæœ\")\n",
    "    \n",
    "    # å‰µå»ºæ¨¡æ“¬åœ–è¡¨ç”¨æ–¼æ¼”ç¤º\n",
    "    print(\"ğŸ“Š ç”Ÿæˆç¤ºä¾‹æ€§èƒ½åœ–è¡¨...\")\n",
    "    \n",
    "    # æ¨¡æ“¬æ•¸æ“š\n",
    "    mock_results = {\n",
    "        \"text_classifier_trt_fp32\": {\n",
    "            \"throughput_rps\": 85.2,\n",
    "            \"latency_ms\": {\n",
    "                \"mean\": 11.7,\n",
    "                \"p95\": 18.4\n",
    "            },\n",
    "            \"raw_latencies\": np.random.normal(11.7, 2.5, 100).tolist()\n",
    "        },\n",
    "        \"text_classifier_trt_fp16\": {\n",
    "            \"throughput_rps\": 156.8,\n",
    "            \"latency_ms\": {\n",
    "                \"mean\": 6.4,\n",
    "                \"p95\": 10.2\n",
    "            },\n",
    "            \"raw_latencies\": np.random.normal(6.4, 1.8, 100).tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    create_performance_visualization(mock_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Triton TensorRT éƒ¨ç½²è…³æœ¬ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå®Œæ•´çš„ Triton TensorRT éƒ¨ç½²è…³æœ¬\n",
    "deployment_script = f'''\n",
    "#!/bin/bash\n",
    "\n",
    "# Triton TensorRT Backend å®Œæ•´éƒ¨ç½²è…³æœ¬\n",
    "# æ—¥æœŸ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "set -e  # é‡åˆ°éŒ¯èª¤ç«‹å³é€€å‡º\n",
    "\n",
    "echo \"ğŸš€ Triton TensorRT Backend éƒ¨ç½²è…³æœ¬\"\n",
    "echo \"=====================================\"\n",
    "\n",
    "# é…ç½®è®Šæ•¸\n",
    "MODEL_REPO=\"{MODEL_REPO.absolute()}\"\n",
    "CONTAINER_NAME=\"triton-tensorrt-server\"\n",
    "TRITON_IMAGE=\"nvcr.io/nvidia/tritonserver:24.01-py3\"\n",
    "\n",
    "# æª¢æŸ¥ Docker\n",
    "if ! command -v docker &> /dev/null; then\n",
    "    echo \"âŒ Docker æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ Docker\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# æª¢æŸ¥ NVIDIA Container Toolkit\n",
    "if ! docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi &> /dev/null; then\n",
    "    echo \"âŒ NVIDIA Container Toolkit æœªå®‰è£æˆ–é…ç½®éŒ¯èª¤\"\n",
    "    echo \"   è«‹å®‰è£ NVIDIA Container Toolkit\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"âœ… Docker å’Œ GPU æ”¯æ´æª¢æŸ¥é€šé\"\n",
    "\n",
    "# åœæ­¢ä¸¦ç§»é™¤ç¾æœ‰å®¹å™¨\n",
    "echo \"ğŸ§¹ æ¸…ç†ç¾æœ‰å®¹å™¨...\"\n",
    "docker stop $CONTAINER_NAME 2>/dev/null || true\n",
    "docker rm $CONTAINER_NAME 2>/dev/null || true\n",
    "\n",
    "# æª¢æŸ¥æ¨¡å‹å€‰åº«\n",
    "if [ ! -d \"$MODEL_REPO\" ]; then\n",
    "    echo \"âŒ æ¨¡å‹å€‰åº«ä¸å­˜åœ¨: $MODEL_REPO\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"ğŸ“ æ¨¡å‹å€‰åº«: $MODEL_REPO\"\n",
    "echo \"ğŸ“‹ å¯ç”¨æ¨¡å‹:\"\n",
    "ls -la \"$MODEL_REPO\"\n",
    "\n",
    "# å•Ÿå‹• Triton Server\n",
    "echo \"ğŸš€ å•Ÿå‹• Triton TensorRT Server...\"\n",
    "docker run -d \\\n",
    "  --name $CONTAINER_NAME \\\n",
    "  --gpus all \\\n",
    "  --shm-size=1g \\\n",
    "  --ulimit memlock=-1 \\\n",
    "  --ulimit stack=67108864 \\\n",
    "  -p 8000:8000 \\\n",
    "  -p 8001:8001 \\\n",
    "  -p 8002:8002 \\\n",
    "  -v \"$MODEL_REPO:/models\" \\\n",
    "  -e CUDA_VISIBLE_DEVICES=0 \\\n",
    "  $TRITON_IMAGE \\\n",
    "  tritonserver \\\n",
    "    --model-repository=/models \\\n",
    "    --backend-directory=/opt/tritonserver/backends \\\n",
    "    --model-control-mode=explicit \\\n",
    "    --strict-model-config=false \\\n",
    "    --log-verbose=1 \\\n",
    "    --log-info=true \\\n",
    "    --log-warning=true \\\n",
    "    --log-error=true \\\n",
    "    --exit-on-error=false \\\n",
    "    --exit-timeout-secs=120 \\\n",
    "    --buffer-manager-thread-count=4 \\\n",
    "    --model-load-thread-count=8 \\\n",
    "    --backend-config=tensorrt,default-max-workspace-size=1073741824\n",
    "\n",
    "# ç­‰å¾…æœå‹™å™¨å•Ÿå‹•\n",
    "echo \"â³ ç­‰å¾… Triton Server å•Ÿå‹•...\"\n",
    "for i in {{1..30}}; do\n",
    "    if curl -s http://localhost:8000/v2/health/ready > /dev/null; then\n",
    "        echo \"âœ… Triton Server å·²å°±ç·’\"\n",
    "        break\n",
    "    fi\n",
    "    echo \"  ç­‰å¾…ä¸­... ($i/30)\"\n",
    "    sleep 2\n",
    "done\n",
    "\n",
    "# æª¢æŸ¥æœå‹™å™¨ç‹€æ…‹\n",
    "if ! curl -s http://localhost:8000/v2/health/ready > /dev/null; then\n",
    "    echo \"âŒ Triton Server å•Ÿå‹•å¤±æ•—\"\n",
    "    echo \"å®¹å™¨æ—¥èªŒ:\"\n",
    "    docker logs $CONTAINER_NAME\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# é¡¯ç¤ºæœå‹™è³‡è¨Š\n",
    "echo \"ğŸŒ Triton Server ç«¯é»:\"\n",
    "echo \"   HTTP:    http://localhost:8000\"\n",
    "echo \"   gRPC:    localhost:8001\"\n",
    "echo \"   Metrics: http://localhost:8002/metrics\"\n",
    "\n",
    "# è¼‰å…¥æ‰€æœ‰å¯ç”¨æ¨¡å‹\n",
    "echo \"ğŸ“¥ è¼‰å…¥ TensorRT æ¨¡å‹...\"\n",
    "'''\n",
    "\n",
    "# ç‚ºæ¯å€‹å¯ç”¨æ¨¡å‹æ·»åŠ è¼‰å…¥å‘½ä»¤\n",
    "for model_name in available_models:\n",
    "    deployment_script += f'''\n",
    "echo \"  è¼‰å…¥æ¨¡å‹: {model_name}\"\n",
    "curl -X POST \"http://localhost:8000/v2/repository/models/{model_name}/load\" || echo \"è¼‰å…¥å¤±æ•—: {model_name}\"\n",
    "'''\n",
    "\n",
    "deployment_script += '''\n",
    "\n",
    "# æª¢æŸ¥æ¨¡å‹ç‹€æ…‹\n",
    "echo \"ğŸ“‹ æª¢æŸ¥æ¨¡å‹ç‹€æ…‹:\"\n",
    "curl -s http://localhost:8000/v2/models | python3 -m json.tool\n",
    "\n",
    "echo \"ğŸ‰ éƒ¨ç½²å®Œæˆï¼\"\n",
    "echo \"\\nğŸ’¡ æœ‰ç”¨çš„å‘½ä»¤:\"\n",
    "echo \"   æŸ¥çœ‹å®¹å™¨æ—¥èªŒ: docker logs -f $CONTAINER_NAME\"\n",
    "echo \"   åœæ­¢æœå‹™:     docker stop $CONTAINER_NAME\"\n",
    "echo \"   é€²å…¥å®¹å™¨:     docker exec -it $CONTAINER_NAME bash\"\n",
    "echo \"   æ¨¡å‹çµ±è¨ˆ:     curl http://localhost:8000/v2/models/stats\"\n",
    "'''\n",
    "\n",
    "# å„²å­˜éƒ¨ç½²è…³æœ¬\n",
    "deploy_script_path = SCRIPTS_DIR / \"deploy_tensorrt_triton.sh\"\n",
    "with open(deploy_script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(deployment_script.strip())\n",
    "\n",
    "# è¨­å®šåŸ·è¡Œæ¬Šé™\n",
    "os.chmod(deploy_script_path, 0o755)\n",
    "\n",
    "print(f\"ğŸ“œ TensorRT éƒ¨ç½²è…³æœ¬å·²å‰µå»º: {deploy_script_path}\")\n",
    "print(\"\\nğŸš€ è¦éƒ¨ç½² Triton TensorRT Serverï¼Œè«‹åŸ·è¡Œ:\")\n",
    "print(f\"   bash {deploy_script_path}\")\n",
    "\n",
    "# å‰µå»ºæ¸¬è©¦è…³æœ¬\n",
    "test_script = f'''\n",
    "#!/usr/bin/env python3\n",
    "# TensorRT æ¨¡å‹æ¸¬è©¦è…³æœ¬\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "import time\n",
    "import json\n",
    "\n",
    "def test_tensorrt_models():\n",
    "    \"\"\"æ¸¬è©¦ TensorRT æ¨¡å‹\"\"\"\n",
    "    \n",
    "    triton_url = \"localhost:8000\"\n",
    "    client = httpclient.InferenceServerClient(url=triton_url)\n",
    "    \n",
    "    print(\"ğŸ¯ TensorRT æ¨¡å‹æ¸¬è©¦\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # æª¢æŸ¥æœå‹™å™¨ç‹€æ…‹\n",
    "    if not client.is_server_ready():\n",
    "        print(\"âŒ Triton Server æœªå°±ç·’\")\n",
    "        return\n",
    "    \n",
    "    # ç²å–æ¨¡å‹åˆ—è¡¨\n",
    "    models = client.get_model_repository_index()\n",
    "    tensorrt_models = [m[\"name\"] for m in models if \"trt\" in m[\"name\"].lower()]\n",
    "    \n",
    "    if not tensorrt_models:\n",
    "        print(\"âŒ æ²’æœ‰æ‰¾åˆ° TensorRT æ¨¡å‹\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ“‹ æ‰¾åˆ° {{len(tensorrt_models)}} å€‹ TensorRT æ¨¡å‹\")\n",
    "    \n",
    "    # æ¸¬è©¦æ¯å€‹æ¨¡å‹\n",
    "    for model_name in tensorrt_models:\n",
    "        print(f\"\\nğŸš€ æ¸¬è©¦æ¨¡å‹: {{model_name}}\")\n",
    "        \n",
    "        try:\n",
    "            # æª¢æŸ¥æ¨¡å‹ç‹€æ…‹\n",
    "            if not client.is_model_ready(model_name):\n",
    "                print(f\"  âš ï¸  æ¨¡å‹æœªå°±ç·’\")\n",
    "                continue\n",
    "            \n",
    "            # å‰µå»ºæ¸¬è©¦è¼¸å…¥\n",
    "            test_input = np.random.randint(1, 1000, size=(1, 128), dtype=np.int64)\n",
    "            \n",
    "            # æ¨ç†æ¸¬è©¦\n",
    "            inputs = [httpclient.InferInput(\"input_ids\", test_input.shape, \"INT64\")]\n",
    "            inputs[0].set_data_from_numpy(test_input)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = client.infer(model_name, inputs)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # ç²å–è¼¸å‡º\n",
    "            output = response.as_numpy(\"logits\")\n",
    "            \n",
    "            print(f\"  âœ… æ¨ç†æˆåŠŸ\")\n",
    "            print(f\"  âš¡ å»¶é²: {{(end_time - start_time) * 1000:.2f}} ms\")\n",
    "            print(f\"  ğŸ“Š è¼¸å‡ºå½¢ç‹€: {{output.shape}}\")\n",
    "            print(f\"  ğŸ¯ é æ¸¬é¡åˆ¥: {{np.argmax(output[0])}}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ æ¸¬è©¦å¤±æ•—: {{e}}\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ æ¸¬è©¦å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tensorrt_models()\n",
    "'''\n",
    "\n",
    "# å„²å­˜æ¸¬è©¦è…³æœ¬\n",
    "test_script_path = SCRIPTS_DIR / \"test_tensorrt_models.py\"\n",
    "with open(test_script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(test_script.strip())\n",
    "\n",
    "os.chmod(test_script_path, 0o755)\n",
    "\n",
    "print(f\"ğŸ§ª æ¸¬è©¦è…³æœ¬å·²å‰µå»º: {test_script_path}\")\n",
    "print(\"\\nğŸ“‹ éƒ¨ç½²å¾Œæ¸¬è©¦å‘½ä»¤:\")\n",
    "print(f\"   python3 {test_script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorRT æœ€ä½³å¯¦è¸ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensorrt_best_practices():\n",
    "    \"\"\"ç”Ÿæˆ TensorRT æœ€ä½³å¯¦è¸æŒ‡å—\"\"\"\n",
    "    \n",
    "    best_practices = {\n",
    "        \"æ¨¡å‹è½‰æ›å„ªåŒ–\": {\n",
    "            \"ONNX å°å‡º\": [\n",
    "                \"ä½¿ç”¨è¼ƒæ–°çš„ ONNX opset ç‰ˆæœ¬ (11+)\",\n",
    "                \"å•Ÿç”¨å¸¸æ•¸æ‘ºç–Š (constant folding)\",\n",
    "                \"åˆç†è¨­è¨ˆå‹•æ…‹è»¸ï¼Œé¿å…éåº¦éˆæ´»æ€§\",\n",
    "                \"é©—è­‰ PyTorch å’Œ ONNX è¼¸å‡ºä¸€è‡´æ€§\"\n",
    "            ],\n",
    "            \"TensorRT æ§‹å»º\": [\n",
    "                \"æ ¹æ“šç¡¬é«”é¸æ“‡åˆé©ç²¾åº¦ (FP16/INT8)\",\n",
    "                \"è¨­å®šè¶³å¤ çš„å·¥ä½œç©ºé–“å¤§å° (1GB+)\",\n",
    "                \"ç‚ºå¸¸ç”¨è¼¸å…¥å½¢ç‹€å‰µå»ºå„ªåŒ–é…ç½®æª”æ¡ˆ\",\n",
    "                \"ä½¿ç”¨æ‰¹æ¬¡æ¨ç†æé«˜ååé‡\"\n",
    "            ]\n",
    "        },\n",
    "        \"æ€§èƒ½å„ªåŒ–ç­–ç•¥\": {\n",
    "            \"ç²¾åº¦é¸æ“‡\": [\n",
    "                \"FP16: 2x åŠ é€Ÿï¼Œæ¥µå°ç²¾åº¦æå¤±ï¼Œæ¨è–¦é¦–é¸\",\n",
    "                \"INT8: 4x åŠ é€Ÿï¼Œéœ€è¦æ ¡æº–æ•¸æ“šï¼Œé©åˆç”Ÿç”¢ç’°å¢ƒ\",\n",
    "                \"å‹•æ…‹ç¯„åœé‡åŒ–: ç„¡æ ¡æº–æ•¸æ“šçš„ INT8 æ›¿ä»£\"\n",
    "            ],\n",
    "            \"æ‰¹æ¬¡ç­–ç•¥\": [\n",
    "                \"å›ºå®šæ‰¹æ¬¡å¤§å°ç²å¾—æœ€ä½³æ€§èƒ½\",\n",
    "                \"å‹•æ…‹æ‰¹æ¬¡æä¾›éˆæ´»æ€§ä½†çŠ§ç‰²éƒ¨åˆ†æ€§èƒ½\",\n",
    "                \"æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´æœ€å¤§æ‰¹æ¬¡å¤§å°\"\n",
    "            ],\n",
    "            \"è¨˜æ†¶é«”ç®¡ç†\": [\n",
    "                \"é åˆ†é…æ¨ç†è¨˜æ†¶é«”é¿å…å‹•æ…‹åˆ†é…\",\n",
    "                \"ä½¿ç”¨ CUDA Stream é‡ç–Šè¨ˆç®—å’Œè¨˜æ†¶é«”å‚³è¼¸\",\n",
    "                \"ç›£æ§ GPU è¨˜æ†¶é«”ä½¿ç”¨é¿å… OOM\"\n",
    "            ]\n",
    "        },\n",
    "        \"Triton æ•´åˆé…ç½®\": {\n",
    "            \"å¯¦ä¾‹çµ„è¨­å®š\": [\n",
    "                \"æ ¹æ“š GPU è¨˜æ†¶é«”å’Œå»¶é²éœ€æ±‚è¨­å®šå¯¦ä¾‹æ•¸é‡\",\n",
    "                \"å¤§æ¨¡å‹ä½¿ç”¨å–®å¯¦ä¾‹ï¼Œå°æ¨¡å‹å¯å¤šå¯¦ä¾‹ä¸¦è¡Œ\",\n",
    "                \"åˆç†é…ç½® GPU è¦ªå’Œæ€§\"\n",
    "            ],\n",
    "            \"å‹•æ…‹æ‰¹æ¬¡\": [\n",
    "                \"è¨­å®šåˆç†çš„åå¥½æ‰¹æ¬¡å¤§å°\",\n",
    "                \"èª¿æ•´ä½‡åˆ—å»¶é²å¹³è¡¡ååé‡å’ŒéŸ¿æ‡‰æ™‚é–“\",\n",
    "                \"ä½¿ç”¨å„ªå…ˆç´šè™•ç†ä¸åŒé‡è¦æ€§çš„è«‹æ±‚\"\n",
    "            ],\n",
    "            \"æ¨¡å‹é ç†±\": [\n",
    "                \"ç‚ºå¸¸ç”¨è¼¸å…¥å¤§å°é…ç½®é ç†±æ¨£æœ¬\",\n",
    "                \"åŒ…å«ä¸åŒæ‰¹æ¬¡å¤§å°çš„é ç†±é…ç½®\",\n",
    "                \"é¿å…é¦–æ¬¡æ¨ç†çš„å†·å•Ÿå‹•å»¶é²\"\n",
    "            ]\n",
    "        },\n",
    "        \"éƒ¨ç½²ç’°å¢ƒå„ªåŒ–\": {\n",
    "            \"ç¡¬é«”é…ç½®\": [\n",
    "                \"ä½¿ç”¨æ”¯æ´ Tensor Core çš„ GPU (V100/T4/RTX/A100)\",\n",
    "                \"ç¢ºä¿ CUDA ç‰ˆæœ¬èˆ‡ TensorRT ç‰ˆæœ¬å…¼å®¹\",\n",
    "                \"é…ç½®è¶³å¤ çš„ç³»çµ±è¨˜æ†¶é«”æ”¯æ´æ¨¡å‹è¼‰å…¥\"\n",
    "            ],\n",
    "            \"å®¹å™¨åŒ–éƒ¨ç½²\": [\n",
    "                \"ä½¿ç”¨å®˜æ–¹ Triton å®¹å™¨æ˜ åƒ\",\n",
    "                \"æ­£ç¢ºé…ç½® GPU è³‡æºåˆ†é…\",\n",
    "                \"è¨­å®šåˆé©çš„å…±äº«è¨˜æ†¶é«”å¤§å° (--shm-size)\",\n",
    "                \"é…ç½®è¨˜æ†¶é«”é–å®šé™åˆ¶ (--ulimit memlock=-1)\"\n",
    "            ]\n",
    "        },\n",
    "        \"ç›£æ§å‘Šè­¦\": {\n",
    "            \"é—œéµæŒ‡æ¨™\": [\n",
    "                \"GPU åˆ©ç”¨ç‡å’Œè¨˜æ†¶é«”ä½¿ç”¨ç‡\",\n",
    "                \"æ¨ç†å»¶é²åˆ†ä½ˆ (P50/P95/P99)\",\n",
    "                \"ååé‡å’ŒéŒ¯èª¤ç‡\",\n",
    "                \"æ¨¡å‹è¼‰å…¥æ™‚é–“å’Œé ç†±ç‹€æ…‹\"\n",
    "            ],\n",
    "            \"å‘Šè­¦é–¾å€¼å»ºè­°\": [\n",
    "                \"GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡ > 85%\",\n",
    "                \"P99 å»¶é²å¢é•· > 50%\",\n",
    "                \"éŒ¯èª¤ç‡ > 0.1%\",\n",
    "                \"ååé‡ä¸‹é™ > 20%\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ç”Ÿæˆå ±å‘Š\n",
    "    report_lines = [\n",
    "        \"# TensorRT Backend æœ€ä½³å¯¦è¸æŒ‡å—\",\n",
    "        \"=\" * 40,\n",
    "        f\"ç”Ÿæˆæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"\",\n",
    "        \"## ğŸ¯ æ¦‚è¿°\",\n",
    "        \"\",\n",
    "        \"æœ¬æŒ‡å—ç¸½çµäº†åœ¨ Triton Inference Server ä¸­ä½¿ç”¨ TensorRT Backend çš„æœ€ä½³å¯¦è¸ï¼Œ\",\n",
    "        \"åŒ…æ‹¬æ¨¡å‹è½‰æ›ã€æ€§èƒ½å„ªåŒ–ã€éƒ¨ç½²é…ç½®å’Œç›£æ§ç­–ç•¥ã€‚\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    for section_name, content in best_practices.items():\n",
    "        report_lines.append(f\"## ğŸ”§ {section_name}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        for subsection, items in content.items():\n",
    "            report_lines.append(f\"### ğŸ“Œ {subsection}\")\n",
    "            report_lines.append(\"\")\n",
    "            \n",
    "            for item in items:\n",
    "                report_lines.append(f\"- {item}\")\n",
    "            \n",
    "            report_lines.append(\"\")\n",
    "    \n",
    "    # æ€§èƒ½å°æ¯”ç¸½çµ\n",
    "    if benchmark_results:\n",
    "        report_lines.extend([\n",
    "            \"## ğŸ“Š æœ¬æ¬¡æ¸¬è©¦æ€§èƒ½ç¸½çµ\",\n",
    "            \"\",\n",
    "            \"| é…ç½® | ååé‡ (RPS) | å¹³å‡å»¶é² (ms) | å‚™è¨» |\",\n",
    "            \"|------|-------------|--------------|------|\"\n",
    "        ])\n",
    "        \n",
    "        for model_name, result in benchmark_results.items():\n",
    "            if \"error\" not in result and \"latency_ms\" in result:\n",
    "                throughput = result[\"throughput_rps\"]\n",
    "                latency = result[\"latency_ms\"][\"mean\"]\n",
    "                \n",
    "                # ç°¡åŒ–åç¨±å’Œæ·»åŠ å‚™è¨»\n",
    "                if \"fp16\" in model_name:\n",
    "                    config = \"TensorRT FP16\"\n",
    "                    note = \"æ¨è–¦é…ç½®\"\n",
    "                elif \"fp32\" in model_name:\n",
    "                    config = \"TensorRT FP32\"\n",
    "                    note = \"åŸºç·šé…ç½®\"\n",
    "                else:\n",
    "                    config = model_name.replace(\"text_classifier_trt_\", \"\")\n",
    "                    note = \"-\"\n",
    "                \n",
    "                report_lines.append(\n",
    "                    f\"| {config} | {throughput:.1f} | {latency:.2f} | {note} |\"\n",
    "                )\n",
    "    \n",
    "    # æ¨è–¦é…ç½®æ¨¡æ¿\n",
    "    report_lines.extend([\n",
    "        \"\",\n",
    "        \"## ğŸ—ï¸ æ¨è–¦é…ç½®æ¨¡æ¿\",\n",
    "        \"\",\n",
    "        \"### é«˜ååé‡å ´æ™¯\",\n",
    "        \"```protobuf\",\n",
    "        'name: \"model_high_throughput\"',\n",
    "        'platform: \"tensorrt_plan\"',\n",
    "        'max_batch_size: 32',\n",
    "        \"\",\n",
    "        'instance_group {',\n",
    "        '  count: 2',\n",
    "        '  kind: KIND_GPU',\n",
    "        '}',\n",
    "        \"\",\n",
    "        'dynamic_batching {',\n",
    "        '  preferred_batch_size: [ 8, 16, 32 ]',\n",
    "        '  max_queue_delay_microseconds: 1000',\n",
    "        '}',\n",
    "        \"```\",\n",
    "        \"\",\n",
    "        \"### ä½å»¶é²å ´æ™¯\",\n",
    "        \"```protobuf\",\n",
    "        'name: \"model_low_latency\"',\n",
    "        'platform: \"tensorrt_plan\"',\n",
    "        'max_batch_size: 4',\n",
    "        \"\",\n",
    "        'instance_group {',\n",
    "        '  count: 1',\n",
    "        '  kind: KIND_GPU',\n",
    "        '}',\n",
    "        \"\",\n",
    "        'dynamic_batching {',\n",
    "        '  preferred_batch_size: [ 1, 2 ]',\n",
    "        '  max_queue_delay_microseconds: 100',\n",
    "        '}',\n",
    "        \"```\",\n",
    "        \"\",\n",
    "        \"---\",\n",
    "        \"\",\n",
    "        \"ğŸ’¡ **æç¤º**: æ ¹æ“šå…·é«”æ¥­å‹™å ´æ™¯å’Œç¡¬é«”è³‡æºèª¿æ•´ä¸Šè¿°é…ç½®åƒæ•¸ã€‚\"\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "# ç”Ÿæˆæœ€ä½³å¯¦è¸æŒ‡å—\n",
    "best_practices_guide = generate_tensorrt_best_practices()\n",
    "\n",
    "# å„²å­˜æŒ‡å—\n",
    "guide_path = BASE_DIR / \"TensorRT_Best_Practices.md\"\n",
    "with open(guide_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(best_practices_guide)\n",
    "\n",
    "print(f\"ğŸ“š TensorRT æœ€ä½³å¯¦è¸æŒ‡å—å·²ç”Ÿæˆ: {guide_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(best_practices_guide[:2000] + \"...\" if len(best_practices_guide) > 2000 else best_practices_guide)\n",
    "\n",
    "# ç”Ÿæˆå¯¦é©—ç¸½çµ\n",
    "print(\"\\nğŸ‰ TensorRT Backend æ•´åˆå¯¦é©—å®Œæˆï¼\")\n",
    "print(\"\\nğŸ“Š å¯¦é©—æˆæœç¸½çµ:\")\n",
    "print(f\"  âœ… ONNX æ¨¡å‹å°å‡º: {sum(1 for r in export_results.values() if r['success'])}/{len(export_results)}\")\n",
    "\n",
    "if TRT_AVAILABLE:\n",
    "    print(f\"  âœ… TensorRT å¼•æ“æ§‹å»º: {sum(1 for r in build_results.values() if r['success'])}/{len(build_results)}\")\n",
    "    print(f\"  âœ… Triton æ¨¡å‹é…ç½®: {sum(1 for m in triton_models.values() if m['success'])}/{len(triton_models)}\")\n",
    "else:\n",
    "    print(\"  âš ï¸  TensorRT å¼•æ“æ§‹å»º: å·²è·³é (TensorRT æœªå®‰è£)\")\n",
    "    print(\"  âš ï¸  Triton æ¨¡å‹é…ç½®: å·²è·³é\")\n",
    "\n",
    "print(f\"  âœ… éƒ¨ç½²è…³æœ¬ç”Ÿæˆ: å®Œæˆ\")\n",
    "print(f\"  âœ… æœ€ä½³å¯¦è¸æŒ‡å—: å®Œæˆ\")\n",
    "\n",
    "print(\"\\nğŸ“š ä¸‹ä¸€æ­¥å­¸ç¿’å»ºè­°:\")\n",
    "print(\"1. ğŸ”§ å¯¦è¸ vLLM Backend æ•´åˆ (Notebook 03)\")\n",
    "print(\"2. ğŸ› ï¸  é–‹ç™¼è‡ªå®šç¾© Python Backend (Notebook 04)\")\n",
    "print(\"3. ğŸ“Š æ·±å…¥ INT8 é‡åŒ–å„ªåŒ–\")\n",
    "print(\"4. ğŸ¢ å¤š GPU åˆ†æ•£å¼éƒ¨ç½²\")\n",
    "print(\"5. ğŸš€ ç”Ÿç”¢ç’°å¢ƒæ€§èƒ½èª¿å„ª\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— ç›¸é—œè³‡æºèˆ‡å»¶ä¼¸é–±è®€\n",
    "\n",
    "### å®˜æ–¹æ–‡æª”\n",
    "- [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html)\n",
    "- [Triton TensorRT Backend](https://github.com/triton-inference-server/tensorrt_backend)\n",
    "- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)\n",
    "\n",
    "### æ€§èƒ½å„ªåŒ–\n",
    "- [TensorRT æœ€ä½³å¯¦è¸](https://docs.nvidia.com/deeplearning/tensorrt/best-practices/index.html)\n",
    "- [GPU æ¨ç†å„ªåŒ–æŒ‡å—](https://developer.nvidia.com/blog/optimizing-inference-performance-using-nvidia-tensorrt/)\n",
    "- [æ··åˆç²¾åº¦æ¨ç†](https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/)\n",
    "\n",
    "### å¯¦è¸æ¡ˆä¾‹\n",
    "- [TensorRT åœ¨ç”Ÿç”¢ç’°å¢ƒçš„éƒ¨ç½²](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)\n",
    "- [å¤§è¦æ¨¡æ¨ç†æœå‹™å„ªåŒ–](https://developer.nvidia.com/blog/maximizing-deep-learning-inference-performance-with-nvidia-model-analyzer/)\n",
    "\n",
    "### å·¥å…·è³‡æº\n",
    "- [Netron (æ¨¡å‹è¦–è¦ºåŒ–)](https://netron.app/)\n",
    "- [NVIDIA Model Analyzer](https://github.com/triton-inference-server/model_analyzer)\n",
    "- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) (å¤§èªè¨€æ¨¡å‹å°ˆç”¨)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ å¯¦é©—å®Œæˆæ¨™èªŒ**: TensorRT Backend æ•´åˆèˆ‡æ¥µè‡´å„ªåŒ–æŠ€è¡“æŒæ¡ âœ…\n",
    "\n",
    "**ğŸš€ æ€§èƒ½æå‡**: ç›¸æ¯” PyTorch åŸç”Ÿæ¨ç†ï¼ŒTensorRT å¯å¯¦ç¾ 2-10x çš„æ€§èƒ½æå‡ï¼ŒåŒæ™‚æ¸›å°‘ 30-50% çš„è¨˜æ†¶é«”ä½¿ç”¨ã€‚åœ¨ä¼æ¥­ç´šéƒ¨ç½²ä¸­ï¼Œé€™ç¨®å„ªåŒ–å°æ–¼é™ä½æ¨ç†æˆæœ¬å’Œæå‡ç”¨æˆ¶é«”é©—å…·æœ‰é‡å¤§æ„ç¾©ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}