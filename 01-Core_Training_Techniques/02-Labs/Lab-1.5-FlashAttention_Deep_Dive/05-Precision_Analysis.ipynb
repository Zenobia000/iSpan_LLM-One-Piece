{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashAttention 精度影響分析\n",
    "## Precision Impact Analysis for Training vs Inference\n",
    "\n",
    "**學習目標**:\n",
    "- 理解 FlashAttention 在不同精度下的行為差異\n",
    "- 識別訓練和推論中的關鍵精度敏感層\n",
    "- 掌握精度處理的最佳實踐\n",
    "- 建立精度兼容性檢測機制\n",
    "\n",
    "**重要性**: FlashAttention 只支援 FP16/BF16，與傳統 FP32 模型混合使用時需要特別注意精度轉換的影響。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設置和基礎工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# FlashAttention\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"✅ FlashAttention 可用\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"❌ FlashAttention 未安裝\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 設置隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 精度分析工具\n",
    "class PrecisionAnalyzer:\n",
    "    \"\"\"精度分析工具\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_numerical_error(tensor1, tensor2, metric='mse'):\n",
    "        \"\"\"計算數值誤差\"\"\"\n",
    "        if tensor1.shape != tensor2.shape:\n",
    "            raise ValueError(f\"張量形狀不匹配: {tensor1.shape} vs {tensor2.shape}\")\n",
    "        \n",
    "        # 確保相同數據類型\n",
    "        if tensor1.dtype != tensor2.dtype:\n",
    "            tensor2 = tensor2.to(tensor1.dtype)\n",
    "        \n",
    "        diff = tensor1 - tensor2\n",
    "        \n",
    "        if metric == 'mse':\n",
    "            return torch.mean(diff ** 2).item()\n",
    "        elif metric == 'mae':\n",
    "            return torch.mean(torch.abs(diff)).item()\n",
    "        elif metric == 'max':\n",
    "            return torch.max(torch.abs(diff)).item()\n",
    "        elif metric == 'relative':\n",
    "            return torch.mean(torch.abs(diff) / (torch.abs(tensor1) + 1e-8)).item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_gradient_precision(gradients):\n",
    "        \"\"\"分析梯度精度\"\"\"\n",
    "        results = {}\n",
    "        for name, grad in gradients.items():\n",
    "            if grad is not None:\n",
    "                results[name] = {\n",
    "                    'dtype': str(grad.dtype),\n",
    "                    'mean': grad.mean().item(),\n",
    "                    'std': grad.std().item(),\n",
    "                    'max': grad.max().item(),\n",
    "                    'min': grad.min().item(),\n",
    "                    'norm': grad.norm().item()\n",
    "                }\n",
    "        return results\n",
    "\n",
    "print(\"✅ 精度分析工具準備完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 關鍵層精度影響分析\n",
    "\n",
    "### 🎯 關鍵發現：哪些層最容易受到精度影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建測試模型來分析各層精度敏感性\n",
    "class PrecisionSensitiveTransformer(nn.Module):\n",
    "    \"\"\"用於精度分析的 Transformer 模型\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=768, n_heads=12, seq_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # 關鍵層定義\n",
    "        self.embedding = nn.Embedding(50257, d_model)  # GPT-2 vocab size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        \n",
    "        # LayerNorm - 精度敏感度：HIGH ⚠️\n",
    "        self.ln_1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.ln_2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.ln_final = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        \n",
    "        # Attention 投影層 - 精度敏感度：MEDIUM\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # MLP 層 - 精度敏感度：MEDIUM\n",
    "        self.mlp_up = nn.Linear(d_model, 4 * d_model)\n",
    "        self.mlp_down = nn.Linear(4 * d_model, d_model)\n",
    "        \n",
    "        # 輸出層 - 精度敏感度：HIGH ⚠️\n",
    "        self.lm_head = nn.Linear(d_model, 50257, bias=False)\n",
    "        \n",
    "        # Dropout - 精度敏感度：LOW\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def standard_attention(self, x):\n",
    "        \"\"\"標準注意力計算\"\"\"\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 注意力計算 - 精度敏感度：HIGH ⚠️\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # Softmax - 精度敏感度：CRITICAL ⚠️⚠️\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(attn_output)\n",
    "    \n",
    "    def flash_attention(self, x):\n",
    "        \"\"\"FlashAttention 計算\"\"\"\n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            return self.standard_attention(x)\n",
    "            \n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # 確保 FP16 精度\n",
    "        original_dtype = x.dtype\n",
    "        if x.dtype == torch.float32:\n",
    "            x = x.half()\n",
    "            \n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # FlashAttention 計算\n",
    "        attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "        attn_output = attn_output.view(B, T, C)\n",
    "        \n",
    "        # 轉換回原始精度\n",
    "        if original_dtype == torch.float32:\n",
    "            attn_output = attn_output.float()\n",
    "            \n",
    "        return self.o_proj(attn_output)\n",
    "    \n",
    "    def forward(self, input_ids, use_flash=False):\n",
    "        B, T = input_ids.shape\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x + self.pos_embedding[:, :T, :]\n",
    "        \n",
    "        # Pre-attention LayerNorm\n",
    "        x_norm = self.ln_1(x)\n",
    "        \n",
    "        # Attention\n",
    "        if use_flash:\n",
    "            attn_out = self.flash_attention(x_norm)\n",
    "        else:\n",
    "            attn_out = self.standard_attention(x_norm)\n",
    "        \n",
    "        # Residual connection\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # Pre-MLP LayerNorm\n",
    "        x_norm2 = self.ln_2(x)\n",
    "        \n",
    "        # MLP\n",
    "        mlp_out = self.mlp_down(F.gelu(self.mlp_up(x_norm2)))\n",
    "        \n",
    "        # Residual connection\n",
    "        x = x + self.dropout(mlp_out)\n",
    "        \n",
    "        # Final LayerNorm\n",
    "        x = self.ln_final(x)\n",
    "        \n",
    "        # Language modeling head\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"✅ 精度敏感性測試模型準備完成\")\n",
    "\n",
    "# 精度敏感度總結\n",
    "precision_sensitivity = {\n",
    "    \"CRITICAL (需要特別注意)\": [\n",
    "        \"Softmax 計算\",\n",
    "        \"Attention 分數計算\",\n",
    "        \"溫度縮放 (Temperature scaling)\"\n",
    "    ],\n",
    "    \"HIGH (高敏感度)\": [\n",
    "        \"LayerNorm\",\n",
    "        \"輸出投影層 (lm_head)\",\n",
    "        \"殘差連接累積\",\n",
    "        \"Loss 計算\"\n",
    "    ],\n",
    "    \"MEDIUM (中等敏感度)\": [\n",
    "        \"Linear 投影層 (QKV, MLP)\",\n",
    "        \"GELU 激活函數\",\n",
    "        \"Embedding 層\"\n",
    "    ],\n",
    "    \"LOW (低敏感度)\": [\n",
    "        \"Dropout\",\n",
    "        \"位置編碼加法\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n🎯 精度敏感度層級分析：\")\n",
    "for level, layers in precision_sensitivity.items():\n",
    "    print(f\"\\n{level}:\")\n",
    "    for layer in layers:\n",
    "        print(f\"  • {layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 訓練 vs 推論精度差異實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_precision_comparison_experiment():\n",
    "    \"\"\"運行精度比較實驗\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"訓練 vs 推論精度差異實驗\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 創建模型\n",
    "    model = PrecisionSensitiveTransformer(d_model=768, n_heads=12, seq_len=128).to(device)\n",
    "    \n",
    "    # 測試數據\n",
    "    batch_size = 4\n",
    "    seq_len = 128\n",
    "    input_ids = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
    "    target_ids = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. 純 FP32 基準\n",
    "    print(\"\\n1. FP32 基準測試...\")\n",
    "    model.float()\n",
    "    with torch.no_grad():\n",
    "        fp32_output = model(input_ids, use_flash=False)\n",
    "    results['fp32_inference'] = fp32_output.clone()\n",
    "    \n",
    "    # 2. FP32 + FlashAttention (內部轉換)\n",
    "    print(\"\\n2. FP32 + FlashAttention 測試...\")\n",
    "    if FLASH_ATTN_AVAILABLE:\n",
    "        with torch.no_grad():\n",
    "            fp32_flash_output = model(input_ids, use_flash=True)\n",
    "        results['fp32_flash_inference'] = fp32_flash_output.clone()\n",
    "    \n",
    "    # 3. 純 FP16 推論\n",
    "    print(\"\\n3. FP16 推論測試...\")\n",
    "    model.half()\n",
    "    input_ids_fp16 = input_ids\n",
    "    with torch.no_grad():\n",
    "        with autocast(dtype=torch.float16):\n",
    "            fp16_output = model(input_ids_fp16, use_flash=False)\n",
    "    results['fp16_inference'] = fp16_output.clone()\n",
    "    \n",
    "    # 4. FP16 + FlashAttention 推論\n",
    "    print(\"\\n4. FP16 + FlashAttention 推論測試...\")\n",
    "    if FLASH_ATTN_AVAILABLE:\n",
    "        with torch.no_grad():\n",
    "            with autocast(dtype=torch.float16):\n",
    "                fp16_flash_output = model(input_ids_fp16, use_flash=True)\n",
    "        results['fp16_flash_inference'] = fp16_flash_output.clone()\n",
    "    \n",
    "    # 5. 混合精度訓練模擬\n",
    "    print(\"\\n5. 混合精度訓練測試...\")\n",
    "    model.float()  # 權重保持 FP32\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scaler = GradScaler()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(dtype=torch.float16):\n",
    "        mixed_output = model(input_ids, use_flash=True)\n",
    "        loss = criterion(mixed_output.view(-1, mixed_output.size(-1)), target_ids.view(-1))\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    results['mixed_precision_training'] = mixed_output.clone().detach()\n",
    "    \n",
    "    # 梯度分析\n",
    "    gradients = {name: param.grad for name, param in model.named_parameters() if param.grad is not None}\n",
    "    grad_analysis = PrecisionAnalyzer.analyze_gradient_precision(gradients)\n",
    "    \n",
    "    return results, grad_analysis\n",
    "\n",
    "# 運行實驗\n",
    "if FLASH_ATTN_AVAILABLE:\n",
    "    precision_results, gradient_analysis = run_precision_comparison_experiment()\n",
    "    print(\"\\n✅ 精度比較實驗完成\")\n",
    "else:\n",
    "    print(\"⚠️  FlashAttention 未安裝，跳過實驗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 精度差異數值分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE and 'precision_results' in locals():\n",
    "    print(\"=\"*80)\n",
    "    print(\"精度差異數值分析\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    analyzer = PrecisionAnalyzer()\n",
    "    \n",
    "    # 基準：FP32 標準注意力\n",
    "    baseline = precision_results['fp32_inference'].float()\n",
    "    \n",
    "    comparison_pairs = [\n",
    "        ('fp32_flash_inference', 'FP32 + FlashAttention'),\n",
    "        ('fp16_inference', 'FP16 標準注意力'),\n",
    "        ('fp16_flash_inference', 'FP16 + FlashAttention'),\n",
    "        ('mixed_precision_training', '混合精度訓練')\n",
    "    ]\n",
    "    \n",
    "    error_analysis = {}\n",
    "    \n",
    "    print(f\"\\n{'配置':<25} {'MSE':<12} {'MAE':<12} {'最大誤差':<12} {'相對誤差':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for key, name in comparison_pairs:\n",
    "        if key in precision_results:\n",
    "            target = precision_results[key].float()\n",
    "            \n",
    "            mse = analyzer.compute_numerical_error(baseline, target, 'mse')\n",
    "            mae = analyzer.compute_numerical_error(baseline, target, 'mae')\n",
    "            max_err = analyzer.compute_numerical_error(baseline, target, 'max')\n",
    "            rel_err = analyzer.compute_numerical_error(baseline, target, 'relative')\n",
    "            \n",
    "            error_analysis[key] = {\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'max_error': max_err,\n",
    "                'relative_error': rel_err\n",
    "            }\n",
    "            \n",
    "            print(f\"{name:<25} {mse:<12.2e} {mae:<12.2e} {max_err:<12.2e} {rel_err:<12.4f}\")\n",
    "    \n",
    "    # 梯度精度分析\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"梯度精度分析 (混合精度訓練)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n{'層名稱':<30} {'數據類型':<10} {'均值':<12} {'標準差':<12} {'梯度範數':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, stats in list(gradient_analysis.items())[:10]:  # 顯示前10層\n",
    "        print(f\"{name[-28:]:<30} {stats['dtype']:<10} {stats['mean']:<12.2e} {stats['std']:<12.2e} {stats['norm']:<12.2e}\")\n",
    "    \n",
    "    # 視覺化誤差分佈\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 子圖1：不同配置的誤差對比\n",
    "    plt.subplot(1, 3, 1)\n",
    "    configs = [name for _, name in comparison_pairs if _ in error_analysis]\n",
    "    mse_values = [error_analysis[key]['mse'] for key, _ in comparison_pairs if key in error_analysis]\n",
    "    \n",
    "    bars = plt.bar(range(len(configs)), mse_values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(configs)])\n",
    "    plt.xticks(range(len(configs)), [c.replace(' ', '\\n') for c in configs], rotation=0)\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('MSE (log scale)')\n",
    "    plt.title('不同配置的 MSE 比較')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 子圖2：相對誤差\n",
    "    plt.subplot(1, 3, 2)\n",
    "    rel_errors = [error_analysis[key]['relative_error'] for key, _ in comparison_pairs if key in error_analysis]\n",
    "    \n",
    "    bars = plt.bar(range(len(configs)), rel_errors, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(configs)])\n",
    "    plt.xticks(range(len(configs)), [c.replace(' ', '\\n') for c in configs], rotation=0)\n",
    "    plt.ylabel('相對誤差')\n",
    "    plt.title('相對誤差比較')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 子圖3：誤差分佈熱圖\n",
    "    plt.subplot(1, 3, 3)\n",
    "    \n",
    "    # 計算第一個 token 的誤差分佈\n",
    "    if 'fp16_flash_inference' in precision_results:\n",
    "        diff = (baseline[0, 0, :] - precision_results['fp16_flash_inference'][0, 0, :].float()).abs().cpu().numpy()\n",
    "        plt.hist(diff, bins=50, alpha=0.7, color='#e74c3c')\n",
    "        plt.xlabel('絕對誤差')\n",
    "        plt.ylabel('頻次')\n",
    "        plt.title('FP16+FlashAttention\\n誤差分佈')\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  無精度結果可分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 關鍵層精度處理最佳實踐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionAwareFlashAttention(nn.Module):\n",
    "    \"\"\"精度感知的 FlashAttention 實現\"\"\"\n",
    "    \n",
    "    def __init__(self, config, precision_policy='mixed'):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        self.precision_policy = precision_policy\n",
    "        \n",
    "        # 線性層\n",
    "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd, bias=True)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=True)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(getattr(config, 'resid_pdrop', 0.1))\n",
    "        \n",
    "        # 精度策略配置\n",
    "        self.setup_precision_policy()\n",
    "    \n",
    "    def setup_precision_policy(self):\n",
    "        \"\"\"設置精度策略\"\"\"\n",
    "        if self.precision_policy == 'conservative':\n",
    "            # 保守策略：關鍵計算保持高精度\n",
    "            self.attention_dtype = torch.float32\n",
    "            self.output_dtype = torch.float32\n",
    "            print(\"🛡️ 使用保守精度策略 (更高精度，較慢速度)\")\n",
    "            \n",
    "        elif self.precision_policy == 'aggressive':\n",
    "            # 激進策略：全程 FP16\n",
    "            self.attention_dtype = torch.float16\n",
    "            self.output_dtype = torch.float16\n",
    "            print(\"⚡ 使用激進精度策略 (更快速度，較低精度)\")\n",
    "            \n",
    "        else:  # mixed\n",
    "            # 混合策略：平衡精度和速度\n",
    "            self.attention_dtype = torch.float16\n",
    "            self.output_dtype = torch.float32\n",
    "            print(\"⚖️ 使用混合精度策略 (平衡精度和速度)\")\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None, layer_past=None,\n",
    "                head_mask=None, use_cache=False, output_attentions=False, **kwargs):\n",
    "        \n",
    "        B, T, C = hidden_states.size()\n",
    "        original_dtype = hidden_states.dtype\n",
    "        \n",
    "        # 1. 輸入精度處理\n",
    "        if hidden_states.dtype != self.attention_dtype:\n",
    "            hidden_states = hidden_states.to(self.attention_dtype)\n",
    "        \n",
    "        # 2. 確保權重精度匹配\n",
    "        if self.c_attn.weight.dtype != hidden_states.dtype:\n",
    "            self.c_attn = self.c_attn.to(hidden_states.dtype)\n",
    "        \n",
    "        # 3. QKV 投影 - 使用指定精度\n",
    "        qkv = self.c_attn(hidden_states)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # 4. 處理 KV cache（如果有）\n",
    "        present = None\n",
    "        if layer_past is not None:\n",
    "            try:\n",
    "                if isinstance(layer_past, (tuple, list)) and len(layer_past) >= 2:\n",
    "                    past_key, past_value = layer_past[0], layer_past[1]\n",
    "                    # 確保 KV cache 精度一致\n",
    "                    if past_key.dtype != k.dtype:\n",
    "                        past_key = past_key.to(k.dtype)\n",
    "                        past_value = past_value.to(v.dtype)\n",
    "                    k = torch.cat((past_key, k), dim=1)\n",
    "                    v = torch.cat((past_value, v), dim=1)\n",
    "            except (ValueError, IndexError):\n",
    "                layer_past = None\n",
    "        \n",
    "        if use_cache:\n",
    "            present = (k, v)\n",
    "        \n",
    "        # 5. 重塑為 FlashAttention 格式\n",
    "        current_seq_len = k.size(1)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim)\n",
    "        k = k.view(B, current_seq_len, self.n_head, self.head_dim)\n",
    "        v = v.view(B, current_seq_len, self.n_head, self.head_dim)\n",
    "        \n",
    "        # 6. FlashAttention 計算（關鍵：確保數值穩定性）\n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            # 降級到標準注意力\n",
    "            q = q.transpose(1, 2)\n",
    "            k = k.transpose(1, 2)\n",
    "            v = v.transpose(1, 2)\n",
    "            \n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "            \n",
    "            # 精度敏感的 Softmax - 使用高精度計算\n",
    "            if self.precision_policy == 'conservative':\n",
    "                scores = scores.float()\n",
    "                attn_weights = F.softmax(scores, dim=-1).to(v.dtype)\n",
    "            else:\n",
    "                attn_weights = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            attn_output = torch.matmul(attn_weights, v)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        else:\n",
    "            # 使用 FlashAttention\n",
    "            if layer_past is None:\n",
    "                try:\n",
    "                    from flash_attn import flash_attn_qkvpacked_func\n",
    "                    qkv_packed = torch.stack([q, k, v], dim=2)\n",
    "                    attn_output = flash_attn_qkvpacked_func(\n",
    "                        qkv_packed,\n",
    "                        dropout_p=0.0,\n",
    "                        causal=True\n",
    "                    )\n",
    "                except ImportError:\n",
    "                    attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "            else:\n",
    "                attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "            \n",
    "            attn_output = attn_output.contiguous().view(B, T, C)\n",
    "        \n",
    "        # 7. 輸出投影 - 精度處理\n",
    "        if self.c_proj.weight.dtype != attn_output.dtype:\n",
    "            self.c_proj = self.c_proj.to(attn_output.dtype)\n",
    "        \n",
    "        output = self.c_proj(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # 8. 最終輸出精度轉換\n",
    "        if self.output_dtype != output.dtype and self.output_dtype == original_dtype:\n",
    "            output = output.to(original_dtype)\n",
    "        \n",
    "        # 9. 返回格式\n",
    "        outputs = (output, present)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (None,)  # FlashAttention 不返回注意力權重\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"✅ 精度感知 FlashAttention 實現完成\")\n",
    "\n",
    "# 展示不同精度策略的配置建議\n",
    "precision_strategies = {\n",
    "    \"研發/調試階段\": {\n",
    "        \"策略\": \"conservative\",\n",
    "        \"描述\": \"保持關鍵計算的高精度，確保數值穩定性\",\n",
    "        \"適用場景\": \"模型開發、調試、精度敏感任務\",\n",
    "        \"性能\": \"較慢，但精度最高\"\n",
    "    },\n",
    "    \"生產部署階段\": {\n",
    "        \"策略\": \"mixed\",\n",
    "        \"描述\": \"平衡精度和性能，推薦的默認策略\",\n",
    "        \"適用場景\": \"大部分生產環境\",\n",
    "        \"性能\": \"較好的精度-速度平衡\"\n",
    "    },\n",
    "    \"高性能推論\": {\n",
    "        \"策略\": \"aggressive\",\n",
    "        \"描述\": \"最大化性能，接受精度損失\",\n",
    "        \"適用場景\": \"對延遲極其敏感的場景\",\n",
    "        \"性能\": \"最快，但精度最低\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n🎯 精度策略建議：\")\n",
    "for scenario, config in precision_strategies.items():\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"  策略: {config['策略']}\")\n",
    "    print(f\"  描述: {config['描述']}\")\n",
    "    print(f\"  適用: {config['適用場景']}\")\n",
    "    print(f\"  性能: {config['性能']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LayerNorm 精度特殊處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionAwareLayerNorm(nn.Module):\n",
    "    \"\"\"精度感知的 LayerNorm 實現\"\"\"\n",
    "    \n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True, \n",
    "                 dtype=None, device=None, force_fp32_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LayerNorm 參數\n",
    "        factory_kwargs = {'dtype': dtype, 'device': device}\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        self.force_fp32_stats = force_fp32_stats  # 強制統計計算使用 FP32\n",
    "        \n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.empty(normalized_shape, **factory_kwargs))\n",
    "            self.bias = nn.Parameter(torch.empty(normalized_shape, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        if self.elementwise_affine:\n",
    "            nn.init.ones_(self.weight)\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        original_dtype = input.dtype\n",
    "        \n",
    "        # 關鍵：LayerNorm 統計計算使用 FP32 以提高數值穩定性\n",
    "        if self.force_fp32_stats and input.dtype != torch.float32:\n",
    "            input_fp32 = input.float()\n",
    "            \n",
    "            # 計算統計量（FP32）\n",
    "            mean = input_fp32.mean(-1, keepdim=True)\n",
    "            var = input_fp32.var(-1, keepdim=True, unbiased=False)\n",
    "            \n",
    "            # 標準化（FP32）\n",
    "            normalized = (input_fp32 - mean) / torch.sqrt(var + self.eps)\n",
    "            \n",
    "            # 轉換回原始精度\n",
    "            normalized = normalized.to(original_dtype)\n",
    "        else:\n",
    "            # 標準 LayerNorm\n",
    "            normalized = F.layer_norm(input, self.normalized_shape, eps=self.eps)\n",
    "        \n",
    "        # 應用可學習參數\n",
    "        if self.elementwise_affine:\n",
    "            # 確保權重精度匹配\n",
    "            weight = self.weight.to(normalized.dtype)\n",
    "            bias = self.bias.to(normalized.dtype)\n",
    "            normalized = normalized * weight + bias\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "print(\"✅ 精度感知 LayerNorm 實現完成\")\n",
    "\n",
    "# LayerNorm 精度影響測試\n",
    "def test_layernorm_precision():\n",
    "    \"\"\"測試 LayerNorm 精度影響\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LayerNorm 精度影響測試\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 測試數據\n",
    "    hidden_dim = 768\n",
    "    batch_size = 4\n",
    "    seq_len = 128\n",
    "    \n",
    "    # 創建測試輸入（模擬極端情況）\n",
    "    x = torch.randn(batch_size, seq_len, hidden_dim, device=device, dtype=torch.float32)\n",
    "    x = x * 100 + 1000  # 增加數值範圍以測試精度敏感性\n",
    "    \n",
    "    # 不同 LayerNorm 實現\n",
    "    ln_standard = nn.LayerNorm(hidden_dim).to(device)\n",
    "    ln_precision_aware = PrecisionAwareLayerNorm(hidden_dim, force_fp32_stats=True).to(device)\n",
    "    ln_precision_aware_false = PrecisionAwareLayerNorm(hidden_dim, force_fp32_stats=False).to(device)\n",
    "    \n",
    "    # 複製權重以確保公平比較\n",
    "    with torch.no_grad():\n",
    "        ln_precision_aware.weight.copy_(ln_standard.weight)\n",
    "        ln_precision_aware.bias.copy_(ln_standard.bias)\n",
    "        ln_precision_aware_false.weight.copy_(ln_standard.weight)\n",
    "        ln_precision_aware_false.bias.copy_(ln_standard.bias)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. FP32 基準\n",
    "    with torch.no_grad():\n",
    "        fp32_output = ln_standard(x)\n",
    "    results['fp32_baseline'] = fp32_output\n",
    "    \n",
    "    # 2. FP16 標準 LayerNorm\n",
    "    ln_standard.half()\n",
    "    with torch.no_grad():\n",
    "        fp16_output = ln_standard(x.half())\n",
    "    results['fp16_standard'] = fp16_output.float()\n",
    "    \n",
    "    # 3. FP16 + 精度感知 LayerNorm (FP32 統計)\n",
    "    ln_precision_aware.half()\n",
    "    with torch.no_grad():\n",
    "        fp16_aware_output = ln_precision_aware(x.half())\n",
    "    results['fp16_precision_aware'] = fp16_aware_output.float()\n",
    "    \n",
    "    # 4. FP16 + 精度感知 LayerNorm (FP16 統計)\n",
    "    ln_precision_aware_false.half()\n",
    "    with torch.no_grad():\n",
    "        fp16_aware_false_output = ln_precision_aware_false(x.half())\n",
    "    results['fp16_no_fp32_stats'] = fp16_aware_false_output.float()\n",
    "    \n",
    "    # 分析誤差\n",
    "    analyzer = PrecisionAnalyzer()\n",
    "    baseline = results['fp32_baseline']\n",
    "    \n",
    "    print(f\"\\n{'LayerNorm 配置':<30} {'MSE':<12} {'最大誤差':<12} {'相對誤差':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for key, name in [('fp16_standard', 'FP16 標準'),\n",
    "                      ('fp16_precision_aware', 'FP16 + FP32統計'),\n",
    "                      ('fp16_no_fp32_stats', 'FP16 + FP16統計')]:\n",
    "        mse = analyzer.compute_numerical_error(baseline, results[key], 'mse')\n",
    "        max_err = analyzer.compute_numerical_error(baseline, results[key], 'max')\n",
    "        rel_err = analyzer.compute_numerical_error(baseline, results[key], 'relative')\n",
    "        \n",
    "        print(f\"{name:<30} {mse:<12.2e} {max_err:<12.2e} {rel_err:<12.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 運行 LayerNorm 測試\n",
    "layernorm_results = test_layernorm_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 實際訓練場景精度監控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionMonitor:\n",
    "    \"\"\"訓練過程中的精度監控工具\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor_layers=None, log_frequency=100):\n",
    "        self.monitor_layers = monitor_layers or ['attention', 'layernorm', 'output']\n",
    "        self.log_frequency = log_frequency\n",
    "        self.precision_history = defaultdict(list)\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"註冊監控鉤子\"\"\"\n",
    "        hooks = []\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if any(layer_type in name.lower() for layer_type in self.monitor_layers):\n",
    "                hook = module.register_forward_hook(\n",
    "                    lambda module, input, output, name=name: self._log_precision(name, output)\n",
    "                )\n",
    "                hooks.append((name, hook))\n",
    "        \n",
    "        return hooks\n",
    "    \n",
    "    def _log_precision(self, layer_name, output):\n",
    "        \"\"\"記錄精度信息\"\"\"\n",
    "        if self.step_count % self.log_frequency == 0:\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]  # 取第一個輸出\n",
    "            \n",
    "            if torch.is_tensor(output):\n",
    "                info = {\n",
    "                    'step': self.step_count,\n",
    "                    'dtype': str(output.dtype),\n",
    "                    'mean': output.mean().item(),\n",
    "                    'std': output.std().item(),\n",
    "                    'min': output.min().item(),\n",
    "                    'max': output.max().item(),\n",
    "                    'has_nan': torch.isnan(output).any().item(),\n",
    "                    'has_inf': torch.isinf(output).any().item()\n",
    "                }\n",
    "                self.precision_history[layer_name].append(info)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"增加步數計數\"\"\"\n",
    "        self.step_count += 1\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"獲取精度監控摘要\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            if history:\n",
    "                latest = history[-1]\n",
    "                summary[layer_name] = {\n",
    "                    'latest_dtype': latest['dtype'],\n",
    "                    'nan_detected': any(h['has_nan'] for h in history),\n",
    "                    'inf_detected': any(h['has_inf'] for h in history),\n",
    "                    'value_range': (min(h['min'] for h in history), max(h['max'] for h in history)),\n",
    "                    'records_count': len(history)\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_precision_trends(self):\n",
    "        \"\"\"繪製精度趨勢圖\"\"\"\n",
    "        if not self.precision_history:\n",
    "            print(\"⚠️  無精度數據可繪製\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('訓練過程精度監控', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 子圖1：均值趨勢\n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            steps = [h['step'] for h in history]\n",
    "            means = [h['mean'] for h in history]\n",
    "            axes[0, 0].plot(steps, means, label=layer_name.split('.')[-1], marker='o', markersize=4)\n",
    "        \n",
    "        axes[0, 0].set_xlabel('訓練步數')\n",
    "        axes[0, 0].set_ylabel('輸出均值')\n",
    "        axes[0, 0].set_title('輸出均值趨勢')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # 子圖2：標準差趨勢\n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            steps = [h['step'] for h in history]\n",
    "            stds = [h['std'] for h in history]\n",
    "            axes[0, 1].plot(steps, stds, label=layer_name.split('.')[-1], marker='s', markersize=4)\n",
    "        \n",
    "        axes[0, 1].set_xlabel('訓練步數')\n",
    "        axes[0, 1].set_ylabel('輸出標準差')\n",
    "        axes[0, 1].set_title('輸出標準差趨勢')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # 子圖3：數值範圍\n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            steps = [h['step'] for h in history]\n",
    "            ranges = [h['max'] - h['min'] for h in history]\n",
    "            axes[1, 0].semilogy(steps, ranges, label=layer_name.split('.')[-1], marker='^', markersize=4)\n",
    "        \n",
    "        axes[1, 0].set_xlabel('訓練步數')\n",
    "        axes[1, 0].set_ylabel('數值範圍 (log scale)')\n",
    "        axes[1, 0].set_title('數值範圍變化')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # 子圖4：異常檢測\n",
    "        anomaly_counts = {}\n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            nan_count = sum(h['has_nan'] for h in history)\n",
    "            inf_count = sum(h['has_inf'] for h in history)\n",
    "            anomaly_counts[layer_name.split('.')[-1]] = nan_count + inf_count\n",
    "        \n",
    "        if anomaly_counts:\n",
    "            layers = list(anomaly_counts.keys())\n",
    "            counts = list(anomaly_counts.values())\n",
    "            bars = axes[1, 1].bar(layers, counts, color='red', alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('層')\n",
    "            axes[1, 1].set_ylabel('異常檢測次數')\n",
    "            axes[1, 1].set_title('NaN/Inf 檢測統計')\n",
    "            axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # 添加數值標籤\n",
    "            for bar, count in zip(bars, counts):\n",
    "                if count > 0:\n",
    "                    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                                   str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ 精度監控工具準備完成\")\n",
    "\n",
    "# 使用示例\n",
    "print(\"\\n📖 精度監控使用示例:\")\n",
    "print(\"\"\"\n",
    "# 1. 創建監控器\n",
    "monitor = PrecisionMonitor(monitor_layers=['attention', 'layernorm'], log_frequency=50)\n",
    "\n",
    "# 2. 註冊監控鉤子\n",
    "hooks = monitor.register_hooks(model)\n",
    "\n",
    "# 3. 訓練循環\n",
    "for batch in dataloader:\n",
    "    output = model(batch)\n",
    "    loss = compute_loss(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    monitor.step()  # 更新步數\n",
    "\n",
    "# 4. 查看結果\n",
    "summary = monitor.get_summary()\n",
    "monitor.plot_precision_trends()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 精度處理檢查清單和最佳實踐總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完成精度分析任務\n",
    "print(\"=\"*80)\n",
    "print(\"🎯 FlashAttention 精度處理完整指南\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "precision_checklist = {\n",
    "    \"🚨 CRITICAL 層 (必須特殊處理)\": {\n",
    "        \"Softmax 計算\": {\n",
    "            \"問題\": \"FP16 精度下容易數值溢出，導致 NaN\",\n",
    "            \"解決方案\": \"使用 FP32 計算 softmax，再轉換回 FP16\",\n",
    "            \"代碼\": \"scores = scores.float(); attn = F.softmax(scores, dim=-1).half()\"\n",
    "        },\n",
    "        \"注意力分數計算\": {\n",
    "            \"問題\": \"大序列長度時分數範圍極大，FP16 表示範圍不足\",\n",
    "            \"解決方案\": \"確保縮放因子正確，考慮使用 FP32 計算\",\n",
    "            \"代碼\": \"scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(head_dim)\"\n",
    "        },\n",
    "        \"溫度縮放\": {\n",
    "            \"問題\": \"生成時的溫度參數會放大精度誤差\",\n",
    "            \"解決方案\": \"溫度縮放使用 FP32 計算\",\n",
    "            \"代碼\": \"logits = logits.float() / temperature\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"⚠️ HIGH 層 (建議特殊處理)\": {\n",
    "        \"LayerNorm\": {\n",
    "            \"問題\": \"統計計算（均值、方差）在 FP16 下精度不足\",\n",
    "            \"解決方案\": \"統計計算使用 FP32，輸出轉回原精度\",\n",
    "            \"代碼\": \"使用 PrecisionAwareLayerNorm 或 force_fp32_stats=True\"\n",
    "        },\n",
    "        \"輸出投影層 (lm_head)\": {\n",
    "            \"問題\": \"詞彙表大小導致權重矩陣巨大，累積誤差\",\n",
    "            \"解決方案\": \"保持 FP32 權重，或使用高精度累積\",\n",
    "            \"代碼\": \"保持 lm_head.weight 為 FP32 dtype\"\n",
    "        },\n",
    "        \"殘差連接累積\": {\n",
    "            \"問題\": \"多層累積導致精度損失\",\n",
    "            \"解決方案\": \"定期檢查數值範圍，避免精度降級\",\n",
    "            \"代碼\": \"x = x + residual  # 確保 dtype 一致\"\n",
    "        },\n",
    "        \"Loss 計算\": {\n",
    "            \"問題\": \"CrossEntropyLoss 在 FP16 下可能不穩定\",\n",
    "            \"解決方案\": \"Loss 計算使用 FP32\",\n",
    "            \"代碼\": \"loss = F.cross_entropy(logits.float(), targets)\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"🔧 MEDIUM 層 (需要注意)\": {\n",
    "        \"Linear 投影層\": {\n",
    "            \"問題\": \"權重精度與輸入精度不匹配\",\n",
    "            \"解決方案\": \"動態調整權重精度或使用 autocast\",\n",
    "            \"代碼\": \"確保 linear.weight.dtype == input.dtype\"\n",
    "        },\n",
    "        \"GELU 激活函數\": {\n",
    "            \"問題\": \"複雜的數學運算在 FP16 下精度損失\",\n",
    "            \"解決方案\": \"使用 PyTorch 優化的 GELU 實現\",\n",
    "            \"代碼\": \"F.gelu(x, approximate='tanh') 或 nn.GELU()\"\n",
    "        },\n",
    "        \"Embedding 層\": {\n",
    "            \"問題\": \"Embedding 查找後的精度轉換\",\n",
    "            \"解決方案\": \"確保 embedding 權重精度正確\",\n",
    "            \"代碼\": \"embedding = embedding.to(target_dtype)\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, layers in precision_checklist.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for layer_name, details in layers.items():\n",
    "        print(f\"\\n📍 {layer_name}:\")\n",
    "        print(f\"   問題: {details['問題']}\")\n",
    "        print(f\"   解決: {details['解決方案']}\")\n",
    "        print(f\"   代碼: {details['代碼']}\")\n",
    "\n",
    "# 部署前檢查清單\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 部署前精度檢查清單\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "deployment_checklist = [\n",
    "    \"✅ 確認 FlashAttention 版本與 PyTorch 版本兼容\",\n",
    "    \"✅ 測試 FP16 vs FP32 輸出一致性（相對誤差 < 1e-2）\",\n",
    "    \"✅ 驗證梯度縮放策略正確配置\",\n",
    "    \"✅ 檢查 LayerNorm 精度設置\",\n",
    "    \"✅ 確認關鍵層（Softmax、Loss）使用高精度計算\",\n",
    "    \"✅ 測試不同序列長度下的數值穩定性\",\n",
    "    \"✅ 監控訓練過程中是否出現 NaN/Inf\",\n",
    "    \"✅ 驗證 KV cache 精度一致性（推論時）\",\n",
    "    \"✅ 測試批次大小變化對精度的影響\",\n",
    "    \"✅ 確認生產環境性能滿足要求\"\n",
    "]\n",
    "\n",
    "for item in deployment_checklist:\n",
    "    print(item)\n",
    "\n",
    "# 故障排除指南\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚨 常見精度問題故障排除\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "troubleshooting = {\n",
    "    \"Loss 變成 NaN\": [\n",
    "        \"檢查 Softmax 計算是否溢出\",\n",
    "        \"降低學習率或使用梯度裁剪\",\n",
    "        \"確保 Loss 計算使用 FP32\",\n",
    "        \"檢查輸入數據是否包含極值\"\n",
    "    ],\n",
    "    \"訓練不收斂\": [\n",
    "        \"比較 FP32 vs FP16 訓練曲線\",\n",
    "        \"檢查梯度是否正常累積\",\n",
    "        \"調整 GradScaler 參數\",\n",
    "        \"增加關鍵層的精度\"\n",
    "    ],\n",
    "    \"推論結果不一致\": [\n",
    "        \"檢查模型加載時的精度轉換\",\n",
    "        \"驗證 KV cache 精度一致性\",\n",
    "        \"確認 tokenizer 精度設置\",\n",
    "        \"測試相同輸入的可重現性\"\n",
    "    ],\n",
    "    \"記憶體使用異常\": [\n",
    "        \"檢查是否有精度轉換導致的內存洩漏\",\n",
    "        \"確認權重共享正確\",\n",
    "        \"監控梯度累積過程\",\n",
    "        \"使用 torch.cuda.empty_cache() 清理\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for problem, solutions in troubleshooting.items():\n",
    "    print(f\"\\n❌ {problem}:\")\n",
    "    for solution in solutions:\n",
    "        print(f\"   • {solution}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 FlashAttention 精度分析完成！\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n🔑 關鍵要點:\")\n",
    "print(\"• FlashAttention 本身是數學等價的，精度問題主要來自 FP16 限制\")\n",
    "print(\"• 關鍵是識別精度敏感層並採用適當的處理策略\")\n",
    "print(\"• 混合精度策略可以平衡性能和精度\")\n",
    "print(\"• 持續監控和測試是確保精度的關鍵\")\n",
    "print(\"\\n💡 建議:\")\n",
    "print(\"• 開發階段使用保守精度策略\")\n",
    "print(\"• 生產部署使用混合精度策略\")\n",
    "print(\"• 高性能場景可考慮激進精度策略\")\n",
    "print(\"• 建立完整的精度測試和監控體系\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}