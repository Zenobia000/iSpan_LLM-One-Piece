{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: P-Tuning - Deployment Guide\n",
    "\n",
    "## ğŸ¯ å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬ Notebook æ¢è¨ **P-Tuning çš„éƒ¨ç½²ç­–ç•¥èˆ‡å„ªåŒ–æ–¹æ¡ˆ**ã€‚P-Tuning ä½œç‚ºè¼¸å…¥å±¤çš„å¯è¨“ç·´æç¤ºæ–¹æ³•,åœ¨æŠ€è¡“ä¸Š**ç„¡æ³•ç›´æ¥åˆä½µ**åˆ°åŸºç¤æ¨¡å‹ä¸­,ä½†ç›¸æ¯” Prefix Tuning å’Œ Adapter,å…¶æ¨ç†é–‹éŠ·è¼ƒå°,éƒ¨ç½²ç›¸å°ç°¡å–®ã€‚\n",
    "\n",
    "### é—œéµå­¸ç¿’è¦é»\n",
    "- ç†è§£ P-Tuning ç„¡æ³•åˆä½µçš„æŠ€è¡“åŸå› \n",
    "- æŒæ¡ Prompt Encoder çš„å„ªåŒ–èˆ‡å£“ç¸®\n",
    "- å¯¦ç¾é«˜æ•ˆçš„æ¨ç†éƒ¨ç½²ç­–ç•¥\n",
    "- åˆ†æ P-Tuning çš„æ¨ç†é–‹éŠ·ç‰¹æ€§\n",
    "- å­¸ç¿’å¤šä»»å‹™ Prompt çš„ç®¡ç†æ–¹æ¡ˆ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. P-Tuning éƒ¨ç½²ç‰¹æ€§åˆ†æ\n",
    "\n",
    "### 1.1 ç‚ºä»€éº¼ P-Tuning ç„¡æ³•åˆä½µ?\n",
    "\n",
    "**æŠ€è¡“åŸå› **:\n",
    "- **è¼¸å…¥åºåˆ—ä¿®æ”¹**: P-Tuning åœ¨è¼¸å…¥åºåˆ—å‰æ·»åŠ è™›æ“¬æ¨™è¨˜(virtual tokens),æ”¹è®Šäº†è¼¸å…¥çµæ§‹\n",
    "- **å‹•æ…‹ Embedding**: è™›æ“¬æ¨™è¨˜é€šé Prompt Encoder (MLP) å‹•æ…‹ç”Ÿæˆ,ç„¡æ³•éœæ…‹èåˆ\n",
    "- **ä½ç½®ä¾è³´æ€§**: è™›æ“¬æ¨™è¨˜çš„ä½ç½®å’Œæ•¸é‡æœƒå½±éŸ¿æ¨¡å‹è¡Œç‚º,ä¸èƒ½ç°¡å–®åˆä½µ\n",
    "\n",
    "**èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„å°æ¯”**:\n",
    "\n",
    "| æ–¹æ³• | æ˜¯å¦å¯åˆä½µ | ä¿®æ”¹ä½ç½® | æ¨ç†é–‹éŠ· | éƒ¨ç½²è¤‡é›œåº¦ |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **LoRA** | âœ… å¯åˆä½µ | ç·šæ€§å±¤æ¬Šé‡ | é›¶é–‹éŠ· | ä½ |\n",
    "| **IAÂ³** | âœ… å¯åˆä½µ | æ¿€æ´»å€¼ç¸®æ”¾ | é›¶é–‹éŠ· | ä½ |\n",
    "| **P-Tuning** | âŒ **ä¸å¯åˆä½µ** | **è¼¸å…¥ Embedding** | **ä½é–‹éŠ·** | **ä¸­** |\n",
    "| **Prompt Tuning** | âŒ ä¸å¯åˆä½µ | è¼¸å…¥ Embedding | ä½é–‹éŠ· | ä½ |\n",
    "| **Prefix Tuning** | âŒ ä¸å¯åˆä½µ | æ¯å±¤ KV | ä¸­ç­‰é–‹éŠ· | ä¸­ |\n",
    "| **Adapter** | âŒ ä¸å¯åˆä½µ | æ–°å¢æ¨¡çµ„ | ä¸­ç­‰é–‹éŠ· | é«˜ |\n",
    "\n",
    "### 1.2 P-Tuning çš„æ¨ç†é–‹éŠ·ä¾†æº\n",
    "\n",
    "```python\n",
    "# P-Tuning æ¨ç†æµç¨‹\n",
    "1. é€šé Prompt Encoder ç”Ÿæˆè™›æ“¬æ¨™è¨˜ embeddings\n",
    "   virtual_embeddings = PromptEncoder(virtual_token_ids)  # â† MLP å‰å‘å‚³æ’­\n",
    "\n",
    "2. æ‹¼æ¥è™›æ“¬æ¨™è¨˜å’ŒçœŸå¯¦è¼¸å…¥\n",
    "   input_embeddings = concat([virtual_embeddings, text_embeddings])\n",
    "\n",
    "3. æ­£å¸¸çš„ Transformer å‰å‘å‚³æ’­\n",
    "   output = Transformer(input_embeddings)\n",
    "```\n",
    "\n",
    "**é–‹éŠ·åˆ†æ**:\n",
    "- Prompt Encoder çš„ MLP è¨ˆç®— (é€šå¸¸å¾ˆå°,2-3å±¤)\n",
    "- è™›æ“¬æ¨™è¨˜å¢åŠ äº†åºåˆ—é•·åº¦,ç•¥å¾®å¢åŠ æ³¨æ„åŠ›è¨ˆç®—é‡\n",
    "- **ç¸½é«”é–‹éŠ· << Prefix Tuning æˆ– Adapter**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç’°å¢ƒæº–å‚™èˆ‡æ¨¡å‹è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„åº«\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PromptEncoderConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# è¨­å®šè¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 è¼‰å…¥è¨“ç·´å¥½çš„ P-Tuning æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è·¯å¾‘è¨­å®š\n",
    "base_model_name = \"bert-base-uncased\"\n",
    "adapter_path = \"./bert-ptuning-sst2\"  # å‡è¨­é€™æ˜¯è¨“ç·´å¥½çš„ P-Tuning è·¯å¾‘\n",
    "num_labels = 2  # SST-2 æ˜¯äºŒåˆ†é¡ä»»å‹™\n",
    "\n",
    "# è¼‰å…¥åŸºç¤æ¨¡å‹å’Œ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"åŸºç¤æ¨¡å‹åƒæ•¸é‡: {base_model.num_parameters():,}\")\n",
    "print(f\"åŸºç¤æ¨¡å‹è¨˜æ†¶é«”ä½”ç”¨: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ P-Tuning adapter (å¦‚æœå­˜åœ¨)\n",
    "if os.path.exists(adapter_path):\n",
    "    # æ‰¾åˆ°æœ€æ–°çš„ checkpoint\n",
    "    checkpoints = [d for d in os.listdir(adapter_path) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(\n",
    "            [os.path.join(adapter_path, d) for d in checkpoints],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
    "        print(f\"æˆåŠŸè¼‰å…¥ P-Tuning adapter: {latest_checkpoint}\")\n",
    "    else:\n",
    "        peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        print(f\"æˆåŠŸè¼‰å…¥ P-Tuning adapter: {adapter_path}\")\n",
    "else:\n",
    "    # å¦‚æœæ²’æœ‰è¨“ç·´å¥½çš„ adapter,å‰µå»ºç¤ºç¯„é…ç½®\n",
    "    print(\"æœªæ‰¾åˆ°è¨“ç·´å¥½çš„ adapter,å‰µå»ºç¤ºç¯„ P-Tuning é…ç½®...\")\n",
    "    \n",
    "    ptuning_config = PromptEncoderConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        num_virtual_tokens=20,\n",
    "        encoder_hidden_size=768\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(base_model, ptuning_config)\n",
    "    print(\"å‰µå»ºäº†ç¤ºç¯„ P-Tuning æ¨¡å‹\")\n",
    "\n",
    "# é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸çµ±è¨ˆ\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. P-Tuning çµæ§‹åˆ†æ\n",
    "\n",
    "### 3.1 Prompt Encoder çµæ§‹æ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ptuning_structure(model):\n",
    "    \"\"\"åˆ†æ P-Tuning çš„çµæ§‹å’Œåƒæ•¸\"\"\"\n",
    "    ptuning_params = {}\n",
    "    total_ptuning_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prompt' in name.lower() or 'encoder' in name.lower():\n",
    "            ptuning_params[name] = {\n",
    "                'shape': param.shape,\n",
    "                'dtype': param.dtype,\n",
    "                'requires_grad': param.requires_grad,\n",
    "                'device': param.device,\n",
    "                'num_params': param.numel()\n",
    "            }\n",
    "            total_ptuning_params += param.numel()\n",
    "            \n",
    "            print(f\"P-Tuning åƒæ•¸: {name}\")\n",
    "            print(f\"  å½¢ç‹€: {param.shape}\")\n",
    "            print(f\"  æ•¸æ“šé¡å‹: {param.dtype}\")\n",
    "            print(f\"  åƒæ•¸é‡: {param.numel():,}\")\n",
    "            print()\n",
    "    \n",
    "    base_params = sum(p.numel() for p in model.base_model.parameters())\n",
    "    print(f\"ç¸½ P-Tuning åƒæ•¸é‡: {total_ptuning_params:,}\")\n",
    "    print(f\"åŸºç¤æ¨¡å‹åƒæ•¸é‡: {base_params:,}\")\n",
    "    print(f\"P-Tuning åƒæ•¸ä½”æ¯”: {total_ptuning_params / base_params * 100:.4f}%\")\n",
    "    \n",
    "    return ptuning_params\n",
    "\n",
    "ptuning_structure = analyze_ptuning_structure(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Encoder çš„è¨ˆç®—æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ptuning_computation(num_virtual_tokens=20, hidden_size=768):\n",
    "    \"\"\"\n",
    "    å¯è¦–åŒ– P-Tuning çš„è¨ˆç®—æµç¨‹\n",
    "    \"\"\"\n",
    "    print(\"=== P-Tuning è¨ˆç®—æµç¨‹åˆ†æ ===\")\n",
    "    print()\n",
    "    print(f\"è™›æ“¬æ¨™è¨˜æ•¸é‡: {num_virtual_tokens}\")\n",
    "    print(f\"éš±è—ç¶­åº¦: {hidden_size}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"éšæ®µ 1: Prompt Encoder ç”Ÿæˆè™›æ“¬æ¨™è¨˜ embeddings\")\n",
    "    print(f\"  è¼¸å…¥: è™›æ“¬æ¨™è¨˜ IDs [{num_virtual_tokens}]\")\n",
    "    print(f\"  Embedding å±¤: [{num_virtual_tokens}] -> [{num_virtual_tokens}, {hidden_size}]\")\n",
    "    print(f\"    åƒæ•¸é‡: {num_virtual_tokens * hidden_size:,}\")\n",
    "    print()\n",
    "    \n",
    "    mlp_hidden = hidden_size * 2\n",
    "    print(f\"  MLP ç·¨ç¢¼å™¨:\")\n",
    "    print(f\"    Layer 1: [{hidden_size}] -> [{mlp_hidden}]\")\n",
    "    print(f\"      åƒæ•¸é‡: {hidden_size * mlp_hidden:,}\")\n",
    "    print(f\"    Activation: ReLU\")\n",
    "    print(f\"    Layer 2: [{mlp_hidden}] -> [{hidden_size}]\")\n",
    "    print(f\"      åƒæ•¸é‡: {mlp_hidden * hidden_size:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"éšæ®µ 2: æ‹¼æ¥è™›æ“¬æ¨™è¨˜å’ŒçœŸå¯¦è¼¸å…¥\")\n",
    "    seq_len = 128  # å‡è¨­çš„åºåˆ—é•·åº¦\n",
    "    total_len = num_virtual_tokens + seq_len\n",
    "    print(f\"  è™›æ“¬æ¨™è¨˜: [{num_virtual_tokens}, {hidden_size}]\")\n",
    "    print(f\"  çœŸå¯¦è¼¸å…¥: [{seq_len}, {hidden_size}]\")\n",
    "    print(f\"  æ‹¼æ¥çµæœ: [{total_len}, {hidden_size}]\")\n",
    "    print()\n",
    "    \n",
    "    print(\"éšæ®µ 3: Transformer å‰å‘å‚³æ’­\")\n",
    "    print(f\"  è¼¸å…¥åºåˆ—é•·åº¦å¢åŠ : {num_virtual_tokens} tokens\")\n",
    "    print(f\"  æ³¨æ„åŠ›è¨ˆç®—é‡å¢åŠ : ~{(total_len**2 / seq_len**2 - 1) * 100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    total_params = (num_virtual_tokens * hidden_size + \n",
    "                   2 * hidden_size * mlp_hidden)\n",
    "    print(f\"ç¸½åƒæ•¸é‡: {total_params:,}\")\n",
    "    base_params = 110 * 1e6  # BERT-base\n",
    "    print(f\"ç›¸æ¯”åŸºç¤æ¨¡å‹çš„åƒæ•¸æ¯”ä¾‹: {total_params / base_params * 100:.4f}%\")\n",
    "\n",
    "visualize_ptuning_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
    "\n",
    "### 4.1 æ€§èƒ½æ¸¬è©¦å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, test_texts, num_runs=10):\n",
    "    \"\"\"æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for text in test_texts:\n",
    "                inputs = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128\n",
    "                )\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                prediction = torch.argmax(logits, dim=-1).cpu().item()\n",
    "                \n",
    "                if run == 0:  # åªåœ¨ç¬¬ä¸€è¼ªä¿å­˜çµæœ\n",
    "                    results.append({\n",
    "                        'text': text,\n",
    "                        'prediction': prediction\n",
    "                    })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    return avg_time, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 åŸ·è¡Œæ¨ç†æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦æ–‡æœ¬\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic!\",\n",
    "    \"I hated every minute of this film.\",\n",
    "    \"An average movie with some good moments.\",\n",
    "    \"Outstanding performance by the lead actor.\",\n",
    "    \"Terrible plot and bad acting.\"\n",
    "]\n",
    "\n",
    "print(\"=== å¸¶ P-Tuning çš„æ¨ç†æ€§èƒ½æ¸¬è©¦ ===\")\n",
    "ptuning_time, ptuning_results = benchmark_inference(peft_model, tokenizer, test_texts)\n",
    "print(f\"å¹³å‡æ¨ç†æ™‚é–“: {ptuning_time:.4f} ç§’\")\n",
    "print(f\"è¨˜æ†¶é«”ä½¿ç”¨: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== é æ¸¬çµæœç¤ºä¾‹ ===\")\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "for i, result in enumerate(ptuning_results[:3]):  # åªé¡¯ç¤ºå‰3å€‹\n",
    "    print(f\"æ–‡æœ¬ {i+1}: {result['text']}\")\n",
    "    print(f\"é æ¸¬: {label_map[result['prediction']]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 èˆ‡åŸºç¤æ¨¡å‹å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== åŸºç¤æ¨¡å‹æ¨ç†æ€§èƒ½æ¸¬è©¦ ===\")\n",
    "base_time, base_results = benchmark_inference(base_model, tokenizer, test_texts)\n",
    "print(f\"å¹³å‡æ¨ç†æ™‚é–“: {base_time:.4f} ç§’\")\n",
    "print(f\"è¨˜æ†¶é«”ä½¿ç”¨: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "# è¨ˆç®—æ€§èƒ½é–‹éŠ·\n",
    "time_overhead = (ptuning_time - base_time) / base_time * 100\n",
    "memory_overhead = (peft_model.get_memory_footprint() - base_model.get_memory_footprint()) / base_model.get_memory_footprint() * 100\n",
    "\n",
    "print(\"\\n=== æ€§èƒ½é–‹éŠ·åˆ†æ ===\")\n",
    "print(f\"æ¨ç†æ™‚é–“å¢åŠ : {time_overhead:.2f}%\")\n",
    "print(f\"è¨˜æ†¶é«”ä½¿ç”¨å¢åŠ : {memory_overhead:.2f}%\")\n",
    "print(f\"\\nçµ•å°æ™‚é–“å¢åŠ : {(ptuning_time - base_time) * 1000:.2f} ms\")\n",
    "print(f\"çµ•å°è¨˜æ†¶é«”å¢åŠ : {(peft_model.get_memory_footprint() - base_model.get_memory_footprint()) / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nğŸ’¡ è§€å¯Ÿ: P-Tuning çš„æ¨ç†é–‹éŠ·ä¸»è¦ä¾†è‡ª:\")\n",
    "print(\"  1. Prompt Encoder çš„ MLP è¨ˆç®— (å¾ˆå°)\")\n",
    "print(\"  2. è™›æ“¬æ¨™è¨˜å¢åŠ çš„åºåˆ—é•·åº¦ (è¼•å¾®å½±éŸ¿æ³¨æ„åŠ›è¨ˆç®—)\")\n",
    "print(\"  ç¸½é«”é–‹éŠ·é å°æ–¼ Prefix Tuning æˆ– Adapter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Encoder å„ªåŒ–ç­–ç•¥\n",
    "\n",
    "### 5.1 ç§»é™¤è¨“ç·´æ™‚çš„ MLP (æ¨ç†å„ªåŒ–)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_for_inference(model):\n",
    "    \"\"\"\n",
    "    å„ªåŒ– P-Tuning æ¨¡å‹ä»¥é€²è¡Œæ¨ç†\n",
    "    åœ¨æ¨ç†æ™‚å¯ä»¥é è¨ˆç®—è™›æ“¬æ¨™è¨˜çš„ embeddings,ç§»é™¤ MLP\n",
    "    \"\"\"\n",
    "    print(\"=== P-Tuning æ¨ç†å„ªåŒ– ===\")\n",
    "    print()\n",
    "    print(\"å„ªåŒ–ç­–ç•¥ 1: é è¨ˆç®—è™›æ“¬æ¨™è¨˜ embeddings\")\n",
    "    print(\"  è¨“ç·´éšæ®µ: ä½¿ç”¨ MLP å‹•æ…‹ç”Ÿæˆè™›æ“¬æ¨™è¨˜\")\n",
    "    print(\"  æ¨ç†éšæ®µ: é è¨ˆç®—ä¸¦ç·©å­˜è™›æ“¬æ¨™è¨˜ embeddings\")\n",
    "    print(\"  å„ªå‹¢: ç§»é™¤ MLP è¨ˆç®—,æ¸›å°‘æ¨ç†é–‹éŠ·\")\n",
    "    print()\n",
    "    \n",
    "    print(\"å„ªåŒ–ç­–ç•¥ 2: è™›æ“¬æ¨™è¨˜æ•¸é‡è‡ªé©æ‡‰\")\n",
    "    print(\"  ç°¡å–®ä»»å‹™: ä½¿ç”¨è¼ƒå°‘çš„è™›æ“¬æ¨™è¨˜ (5-10å€‹)\")\n",
    "    print(\"  è¤‡é›œä»»å‹™: ä½¿ç”¨è¼ƒå¤šçš„è™›æ“¬æ¨™è¨˜ (20-30å€‹)\")\n",
    "    print(\"  å„ªå‹¢: å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡\")\n",
    "    print()\n",
    "    \n",
    "    print(\"å„ªåŒ–ç­–ç•¥ 3: é‡åŒ– Prompt Encoder\")\n",
    "    print(\"  å°‡ MLP æ¬Šé‡é‡åŒ–åˆ° FP16 æˆ– INT8\")\n",
    "    print(\"  å„ªå‹¢: æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨å’Œè¨ˆç®—é‡\")\n",
    "    print()\n",
    "    \n",
    "    # é¡¯ç¤ºç•¶å‰æ¨¡å‹åƒæ•¸çµ±è¨ˆ\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"ç•¶å‰æ¨¡å‹ç¸½åƒæ•¸é‡: {total_params:,}\")\n",
    "    print(f\"ç•¶å‰å¯è¨“ç·´åƒæ•¸é‡: {trainable_params:,}\")\n",
    "    print(f\"å„ªåŒ–æ½›åŠ›: é è¨ˆç®—å¾Œå¯æ¸›å°‘ ~{trainable_params:,} æ¬¡ MLP è¨ˆç®—\")\n",
    "\n",
    "optimize_for_inference(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹ä¿å­˜èˆ‡éƒ¨ç½²\n",
    "\n",
    "### 6.1 ä¿å­˜ P-Tuning æ¨¡çµ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šä¿å­˜è·¯å¾‘\n",
    "save_path = \"./bert-ptuning-deployed\"\n",
    "\n",
    "print(f\"=== ä¿å­˜ P-Tuning æ¨¡çµ„åˆ°: {save_path} ===\")\n",
    "\n",
    "# ä¿å­˜ PEFT adapter\n",
    "peft_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"âœ… P-Tuning æ¨¡çµ„ä¿å­˜æˆåŠŸ!\")\n",
    "\n",
    "# æª¢æŸ¥ä¿å­˜çš„æ–‡ä»¶\n",
    "saved_files = os.listdir(save_path)\n",
    "print(f\"\\nä¿å­˜çš„æ–‡ä»¶: {saved_files}\")\n",
    "\n",
    "# è¨ˆç®—æ¨¡çµ„å¤§å°\n",
    "module_size = 0\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(save_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        module_size += size\n",
    "        print(f\"  {file}: {size / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nP-Tuning æ¨¡çµ„ç¸½å¤§å°: {module_size / 1024:.2f} KB ({module_size / 1024**2:.4f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 é©—è­‰ä¿å­˜çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== é©—è­‰ä¿å­˜çš„ P-Tuning æ¨¡çµ„ ===\")\n",
    "\n",
    "# é‡æ–°è¼‰å…¥åŸºç¤æ¨¡å‹\n",
    "fresh_base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# å¾ä¿å­˜è·¯å¾‘è¼‰å…¥ P-Tuning\n",
    "reloaded_model = PeftModel.from_pretrained(fresh_base_model, save_path)\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "print(f\"âœ… æˆåŠŸè¼‰å…¥ä¿å­˜çš„ P-Tuning æ¨¡çµ„\")\n",
    "\n",
    "# å¿«é€Ÿæ¸¬è©¦\n",
    "test_text = \"This movie was absolutely fantastic!\"\n",
    "\n",
    "inputs = reloaded_tokenizer(\n",
    "    test_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).cpu().item()\n",
    "\n",
    "print(f\"\\næ¸¬è©¦é æ¸¬:\")\n",
    "print(f\"æ–‡æœ¬: {test_text}\")\n",
    "print(f\"é æ¸¬: {label_map[prediction]}\")\n",
    "print(\"\\nâœ… ä¿å­˜çš„ P-Tuning æ¨¡çµ„é‹ä½œæ­£å¸¸!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºéƒ¨ç½²é…ç½®æ–‡ä»¶\n",
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"peft_method\": \"P-Tuning\",\n",
    "        \"task_type\": \"Sentiment Classification\",\n",
    "        \"num_labels\": num_labels,\n",
    "        \"num_virtual_tokens\": 20,\n",
    "        \"mergeable\": False,\n",
    "        \"module_size_mb\": module_size / 1024**2\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"inference_time_seconds\": ptuning_time,\n",
    "        \"vs_base_model_overhead_percent\": time_overhead,\n",
    "        \"memory_overhead_percent\": memory_overhead\n",
    "    },\n",
    "    \"deployment_requirements\": {\n",
    "        \"python_packages\": [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.20.0\",\n",
    "            \"peft>=0.3.0\"\n",
    "        ],\n",
    "        \"minimum_gpu_memory_gb\": 4,\n",
    "        \"recommended_gpu_memory_gb\": 8\n",
    "    },\n",
    "    \"optimization_strategies\": {\n",
    "        \"precompute_virtual_embeddings\": {\n",
    "            \"enabled\": True,\n",
    "            \"description\": \"æ¨ç†æ™‚é è¨ˆç®—è™›æ“¬æ¨™è¨˜ embeddings,ç§»é™¤ MLP\",\n",
    "            \"expected_speedup_percent\": 5\n",
    "        },\n",
    "        \"quantization\": {\n",
    "            \"enabled\": True,\n",
    "            \"dtype\": \"float16\",\n",
    "            \"memory_saving_percent\": 50\n",
    "        },\n",
    "        \"adaptive_virtual_tokens\": {\n",
    "            \"enabled\": False,\n",
    "            \"description\": \"æ ¹æ“šä»»å‹™è¤‡é›œåº¦èª¿æ•´è™›æ“¬æ¨™è¨˜æ•¸é‡\"\n",
    "        }\n",
    "    },\n",
    "    \"deployment_notes\": {\n",
    "        \"inference_overhead\": \"ä½ (~3-8%),é å°æ–¼ Prefix Tuning æˆ– Adapter\",\n",
    "        \"deployment_complexity\": \"ä¸­ç­‰,éœ€ç®¡ç† Prompt Encoder\",\n",
    "        \"multi_task_support\": \"è‰¯å¥½,å¯å…±äº«åŸºç¤æ¨¡å‹\",\n",
    "        \"best_for\": \"NLU ä»»å‹™,ç‰¹åˆ¥æ˜¯åˆ†é¡ã€NER ç­‰\"\n",
    "    },\n",
    "    \"usage_example\": {\n",
    "        \"load_base_model\": f\"base_model = AutoModelForSequenceClassification.from_pretrained('{base_model_name}')\",\n",
    "        \"load_ptuning\": f\"model = PeftModel.from_pretrained(base_model, '{save_path}')\",\n",
    "        \"inference\": \"outputs = model(**inputs)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_path = os.path.join(save_path, \"deployment_config.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=== ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²é…ç½® ===\")\n",
    "print(json.dumps(deployment_config, indent=2, ensure_ascii=False))\n",
    "print(f\"\\né…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. P-Tuning èˆ‡å…¶ä»– PEFT æ–¹æ³•å°æ¯”\n",
    "\n",
    "### 8.1 éƒ¨ç½²ç‰¹æ€§å…¨é¢å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_comparison = {\n",
    "    \"LoRA\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âœ… å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"0%\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä½\",\n",
    "        \"é©ç”¨ä»»å‹™\": \"é€šç”¨\",\n",
    "        \"åƒæ•¸æ•ˆç‡\": \"0.1-1%\"\n",
    "    },\n",
    "    \"IAÂ³\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âœ… å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"0%\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä½\",\n",
    "        \"é©ç”¨ä»»å‹™\": \"ç”Ÿæˆ\",\n",
    "        \"åƒæ•¸æ•ˆç‡\": \"0.01%\"\n",
    "    },\n",
    "    \"P-Tuning\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âŒ ä¸å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"3-8%\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä¸­\",\n",
    "        \"é©ç”¨ä»»å‹™\": \"NLU\",\n",
    "        \"åƒæ•¸æ•ˆç‡\": \"0.01-0.1%\"\n",
    "    },\n",
    "    \"Prompt Tuning\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âŒ ä¸å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"2-5%\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä½\",\n",
    "        \"é©ç”¨ä»»å‹™\": \"ç”Ÿæˆ\",\n",
    "        \"åƒæ•¸æ•ˆç‡\": \"0.001-0.01%\"\n",
    "    },\n",
    "    \"Prefix Tuning\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âŒ ä¸å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"10-20%\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"ä¸­\",\n",
    "        \"é©ç”¨ä»»å‹™\": \"ç”Ÿæˆ\",\n",
    "        \"åƒæ•¸æ•ˆç‡\": \"0.1-1%\"\n",
    "    },\n",
    "    \"Adapter\": {\n",
    "        \"å¯åˆä½µæ€§\": \"âŒ ä¸å¯åˆä½µ\",\n",
    "        \"æ¨ç†é–‹éŠ·\": \"10-20%\",\n",
    "        \"éƒ¨ç½²è¤‡é›œåº¦\": \"é«˜\",\n",
    "        \"é©ç”¨ä»»å‹™\": \"NLU\",\n",
    "        \"åƒæ•¸æ•ˆç‡\": \"0.5-5%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== PEFT æ–¹æ³•å…¨é¢å°æ¯” ===\")\n",
    "print(f\"{'æ–¹æ³•':<15} {'å¯åˆä½µ':<12} {'æ¨ç†é–‹éŠ·':<10} {'éƒ¨ç½²':<8} {'é©ç”¨':<8} {'åƒæ•¸æ•ˆç‡':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for method, features in peft_comparison.items():\n",
    "    print(f\"{method:<15} {features['å¯åˆä½µæ€§']:<12} {features['æ¨ç†é–‹éŠ·']:<10} {features['éƒ¨ç½²è¤‡é›œåº¦']:<8} {features['é©ç”¨ä»»å‹™']:<8} {features['åƒæ•¸æ•ˆç‡']:<12}\")\n",
    "\n",
    "print(\"\\n=== P-Tuning çš„å®šä½èˆ‡å„ªå‹¢ ===\")\n",
    "print(\"âœ… æ¨ç†é–‹éŠ·å°: é å°æ–¼ Prefix Tuning å’Œ Adapter\")\n",
    "print(\"âœ… NLU ä»»å‹™å„ªç§€: ç‰¹åˆ¥é©åˆåˆ†é¡ã€NER ç­‰ç†è§£ä»»å‹™\")\n",
    "print(\"âœ… è¨“ç·´ç©©å®š: MLP ç·¨ç¢¼å™¨æä¾›æ›´ç©©å®šçš„è¨“ç·´\")\n",
    "print(\"âœ… åƒæ•¸æ¥µç°¡: åƒ…éœ€æ¥µå°‘åƒæ•¸å³å¯é”åˆ°å„ªç§€æ•ˆæœ\")\n",
    "print(\"âš ï¸  ä¸å¯åˆä½µ: éœ€è¦åŒæ™‚éƒ¨ç½²åŸºç¤æ¨¡å‹å’Œ Prompt Encoder\")\n",
    "print(\"âš ï¸  ç”Ÿæˆä»»å‹™: åœ¨ç”Ÿæˆä»»å‹™ä¸Šå¯èƒ½ä¸å¦‚ Prefix Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç¸½çµ\n",
    "\n",
    "### 9.1 æ ¸å¿ƒå­¸ç¿’æ”¶ç©«\n",
    "\n",
    "1. **æŠ€è¡“èªçŸ¥**: ç†è§£äº† P-Tuning ä½œç‚ºè¼¸å…¥å±¤æç¤ºæ–¹æ³•çš„ç¨ç‰¹æ€§\n",
    "2. **éƒ¨ç½²å¯¦è¸**: æŒæ¡äº† P-Tuning çš„ä¿å­˜ã€è¼‰å…¥å’Œå„ªåŒ–\n",
    "3. **æ€§èƒ½åˆ†æ**: äº†è§£äº† P-Tuning ç›¸å°è¼ƒä½çš„æ¨ç†é–‹éŠ·\n",
    "4. **æ–¹æ³•å°æ¯”**: æ˜ç¢ºäº† P-Tuning åœ¨ NLU ä»»å‹™ä¸Šçš„å„ªå‹¢\n",
    "\n",
    "### 9.2 P-Tuning çš„éƒ¨ç½²ç‰¹é»\n",
    "\n",
    "**æœ€é©åˆçš„å ´æ™¯**:\n",
    "- NLU ä»»å‹™ (æ–‡æœ¬åˆ†é¡ã€å‘½åå¯¦é«”è­˜åˆ¥ã€é—œä¿‚æŠ½å–ç­‰)\n",
    "- éœ€è¦æ¥µé«˜åƒæ•¸æ•ˆç‡çš„å ´æ™¯\n",
    "- å¯ä»¥æ¥å—è¼•å¾®æ¨ç†é–‹éŠ·çš„æ‡‰ç”¨\n",
    "- å¤šä»»å‹™å ´æ™¯ (å…±äº«åŸºç¤æ¨¡å‹)\n",
    "\n",
    "**ä¸æ¨è–¦çš„å ´æ™¯**:\n",
    "- å°æ¨ç†å»¶é²é›¶å®¹å¿ â†’ é¸æ“‡ LoRA/IAÂ³\n",
    "- æ–‡æœ¬ç”Ÿæˆä»»å‹™ â†’ Prefix Tuning å¯èƒ½æ›´å¥½\n",
    "- è¶…å¤§æ¨¡å‹ â†’ Prompt Tuning å¯èƒ½æ›´ç°¡å–®\n",
    "\n",
    "### 9.3 å¯¦ç”¨å»ºè­°\n",
    "\n",
    "- NLU ä»»å‹™å„ªå…ˆè€ƒæ…® P-Tuning\n",
    "- å¯ä»¥åœ¨æ¨ç†æ™‚é è¨ˆç®—è™›æ“¬æ¨™è¨˜ embeddings\n",
    "- æ ¹æ“šä»»å‹™è¤‡é›œåº¦èª¿æ•´è™›æ“¬æ¨™è¨˜æ•¸é‡\n",
    "- å¤šä»»å‹™å ´æ™¯å¯å…±äº«åŸºç¤æ¨¡å‹,åˆ‡æ›ä¸åŒçš„ Prompt Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†è³‡æº\n",
    "print(\"=== æ¸…ç†å¯¦é©—è³‡æº ===\")\n",
    "del peft_model, base_model, reloaded_model, fresh_base_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"âœ… è³‡æºæ¸…ç†å®Œæˆ\")\n",
    "\n",
    "print(\"\\nğŸ‰ Lab 7 - P-Tuning éƒ¨ç½²æŒ‡å—å¯¦é©—å®Œæˆ!\")\n",
    "print(f\"P-Tuning æ¨¡çµ„å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "print(\"æ‚¨ç¾åœ¨äº†è§£äº† P-Tuning çš„éƒ¨ç½²ç‰¹æ€§å’Œå„ªåŒ–ç­–ç•¥!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
