# Lab 0: 分散式訓練基礎 - 並行策略的基石

## 概述

隨著大型語言模型 (LLM) 的參數規模達到千億甚至萬億級別，單一加速器（如 GPU）的記憶體和計算能力已遠遠無法滿足訓練需求。**分散式訓練 (Distributed Training)**，即利用多個計算設備協同訓練單一模型，已從一項優化技術演變為訓練前沿模型的唯一途徑。

本章節旨在為您奠定分散式訓練的理論基石，深入剖析從經典到前沿的各種並行策略。我們將系統性地拆解數據並行、模型並行（張量並行、流水線並行）、優化器並行乃至專家並行等核心技術的原理、權衡與適用場景，為後續的實戰實驗打下堅實的基礎。

---

## 1. 為什麼需要分散式訓練？—— 規模的挑戰

訓練一個現代大型模型（如 GPT-3 175B）主要面臨兩大核心挑戰：

1.  **記憶體瓶頸 (Memory Wall)**：模型的權重、梯度、優化器狀態和中間激活值所佔據的記憶體遠超單張 GPU 的數十 GB 顯存。例如，一個 175B 參數的模型若使用混合精度訓練，其總記憶體需求可達 1TB 以上。
2.  **計算瓶頸 (Compute Wall)**：完成一次訓練所需的浮點運算量 (FLOPs) 極其龐大，單卡需要數十年甚至更久才能完成。

為突破這些限制，我們必須將模型的計算和儲存負載**分割 (Partition)** 到多個設備上。不同的分割方式，衍生出了下文將要探討的各種並行策略。

---

## 2. 數據並行 (Data Parallelism)：最簡單的擴展方式

**數據並行**是分散式訓練中最直觀、最常用的策略。其核心思想是：**將模型完整地複製到多個設備上，每個設備分配到數據集的一個不同子集 (data shard)，並行計算梯度，最後匯總梯度以更新所有設備上的模型權重。**

![數據並行原理示意圖](https://pica.zhimg.com/v2-f423736563792f5b614b33b07346f2b0_1440w.jpg)

*圖 1：數據並行工作流程。每個設備都擁有完整的模型副本，處理不同的數據批次，並在反向傳播後同步梯度。*

#### 核心流程：
1.  **分發 (Distribute)**：在每個訓練步驟開始時，將完整的模型權重複製到所有 `N` 個設備上。
2.  **劃分 (Split)**：將一個全局批次 (Global Batch) 的數據劃分成 `N` 個微批次 (Micro-batches)。
3.  **並行計算 (Parallel Forward/Backward)**：每個設備獨立對其分配到的微批次數據執行前向和反向傳播，計算出局部梯度。
4.  **同步 (Synchronize)**：使用 `All-Reduce` 通信操作，將所有設備上的局部梯度進行平均，得到全局梯度。
5.  **更新 (Update)**：每個設備使用相同的全局梯度，獨立更新本地的模型權重，從而確保所有副本在下一步開始前保持一致。

| 優點 | 缺點 |
| :--- | :--- |
| ✅ **實現簡單**：主流框架（如 PyTorch DDP）提供成熟封裝。 | ❌ **無法解決記憶體瓶頸**：模型大小受限於單張卡的記憶體上限。 |
| ✅ **訓練加速比高**：在帶寬充足時，可以實現近線性的訓練加速。 | ❌ **通信開銷**：`All-Reduce` 操作的通信量與模型大小成正比，設備增多時成為瓶頸。 |
| ✅ **極佳的負載均衡**：只要數據劃分均勻，計算負載自然均衡。 | ❌ **對網路帶寬敏感**：尤其在多節點場景下。 |

---

## 3. 模型並行 (Model Parallelism)：分割模型以突破記憶體牆

當模型大到單卡無法容納時，就必須使用**模型並行**。其核心思想是：**將模型的單個副本分割到不同的設備上，讓每個設備只承載模型的一部分。** 模型並行主要有兩種實現路徑：張量並行和流水線並行。

### 3.1 張量並行 (Tensor Parallelism)

**張量並行**，也稱為**運算子內並行 (Intra-Operator Parallelism)**，是將模型中的單個大型運算（如矩陣乘法）分割到多個設備上協同完成。這通常應用於 Transformer 中的線性層和注意力頭。

![張量並行原理示意圖](https://pic3.zhimg.com/v2-fa37fb30f9591efcaf21ee4882ce0810_1440w.jpg)
*圖 2：一個線性層的張量並行實現。(左) 按列切分權重矩陣。(右) 按行切分權重矩陣。*

#### 原理詳解 (以 Megatron-LM 為例):
一個 Transformer 塊包含一個多頭注意力 (Multi-Head Attention) 模組和一個 MLP 模組。張量並行可以如下實現：
1.  **MLP 層**：
    *   第一個線性層的權重按**列**切分，並行計算後，結果通過 `All-Reduce` 匯總。
    *   第二個線性層的權重按**行**切分，輸入需要在設備間 `All-Gather`，計算後得到的分片結果直接拼接即可。
2.  **注意力層**：
    *   可以將 Q, K, V 的投影矩陣和注意力頭 (Attention Heads) 均勻地分配到多個 GPU 上，每個 GPU 獨立計算一部分頭的注意力得分，最後將結果匯總。

| 優點 | 缺點 |
| :--- | :--- |
| ✅ **高效的記憶體節省**：直接減少了單卡上的權重、梯度和優化器狀態。 | ❌ **大量的通信開銷**：幾乎每個並行化的運算子都需要同步通信。 |
| ✅ **計算負載均衡**：只要切分均勻，計算負載自然平衡。 | ❌ **實現極其複雜**：需要深入模型內部修改運算子，且不易擴展。 |

### 3.2 流水線並行 (Pipeline Parallelism)

**流水線並行**，也稱為**運算子間並行 (Inter-Operator Parallelism)**，它將模型的不同層（一組連續的運算子）劃分到不同的設備上，構成一個計算流水線。

![流水線並行示意圖](https://picx.zhimg.com/v2-0d17ea761373f05f24d8f30c99e51c59_1440w.jpg)
*圖 3：流水線並行。模型被切分為4個階段(Stage)，分別置於4個GPU上。數據被切分為多個微批次(Micro-batches)以提高設備利用率。*

#### 流水線氣泡 (Pipeline Bubble) 與優化：
簡單的流水線會導致大量的設備閒置時間（即氣泡）。為了緩解這個問題，GPipe 等框架引入了**微批次 (Micro-batching)** 的概念，將一個大的訓練批次切分成多個小的微批次，讓各階段的計算可以重疊起來。

![流水線調度示意圖](https://pica.zhimg.com/v2-3d0b0e09863818e41468c3e96d603538_1440w.jpg)
*圖 4：(a) 樸素的流水線調度會產生巨大的氣泡。(b) GPipe 使用的同步微批次流水線，有效減少了氣泡大小。*

| 優點 | 缺點 |
| :--- | :--- |
| ✅ **通信量可控**：僅在相鄰的流水線階段之間傳遞激活值。 | ❌ **存在流水線氣泡**：設備利用率無法達到100%。 |
| ✅ **實現相對簡單**：無需修改運算子內部，只需按層切分模型。 | ❌ **負載均衡困難**：很難將模型完美切分成計算量相等的階段。 |

---

## 4. 優化器並行 (Optimizer Parallelism) - ZeRO

數據並行雖然簡單，但其記憶體冗餘問題嚴重。微軟 DeepSpeed 團隊提出的 **ZeRO (Zero Redundancy Optimizer)** 是一種針對數據並行記憶體冗餘的優化方案，它在概念上可以被視為一種**優化器狀態和梯度的分片策略**。

![ZeRO 記憶體優化示意圖](https://pic2.zhimg.com/v2-0767b38b6144986667975d2b99d02bc3_1440w.jpg)
*圖 5：ZeRO 通過對模型狀態（優化器狀態、梯度、模型參數）進行分片，極大降低了單卡的記憶體佔用。*

ZeRO 提供了三個遞進的優化階段：

-   **ZeRO-Stage 1 (優化器狀態分片)**：將優化器狀態（如 Adam 的動量和方差）均勻分片到各個數據並行設備上。
-   **ZeRO-Stage 2 (梯度與優化器狀態分片)**：在 Stage 1 的基礎上，進一步對梯度也進行分片。
-   **ZeRO-Stage 3 (參數、梯度與優化器狀態分片)**：對所有模型狀態（模型參數、梯度、優化器狀態）都進行分片。每個設備只持有模型的一部分參數，在需要時通過 `All-Gather` 從其他設備獲取。

![ZeRO 不同階段對比](https://picx.zhimg.com/v2-502ecc042a5f2fbc6611f929997b8b17_1440w.jpg)
*圖 6：ZeRO 不同階段的記憶體分佈。從左到右，分片的內容越來越多，單卡記憶體佔用越來越低。*

| 策略 | 優化器狀態 (Optimizer States) | 梯度 (Gradients) | 模型參數 (Parameters) | 記憶體節省 |
| :--- | :--- | :--- | :--- | :--- |
| **標準數據並行** | 每個 GPU 複製 | 每個 GPU 複製 | 每個 GPU 複製 | 1x |
| **ZeRO-Stage 1** | **分片** | 每個 GPU 複製 | 每個 GPU 複製 | 4x |
| **ZeRO-Stage 2** | **分片** | **分片** | 每個 GPU 複製 | 8x |
| **ZeRO-Stage 3** | **分片** | **分片** | **分片** | 與 GPU 數量成正比 |

---

## 5. 專家並行 (Expert Parallelism for MoE)

**混合專家模型 (Mixture of Experts, MoE)** 是一種新興的模型架構，它在模型的某些層中設置多個「專家」網路（通常是 FFN），並通過一個可訓練的門控網路 (Gating Network) 來為每個輸入 token 動態地選擇激活哪些專家。

**專家並行**是為 MoE 模型量身定制的並行策略。其核心思想是：**將不同的專家分配到不同的設備上，而模型其他部分（非專家層）則在所有設備間複製。**

![MoE 與專家並行示意圖](https://pica.zhimg.com/v2-2765e8be3de6094de5e1be2e4a480a52_1440w.jpg)
*圖 7：專家並行。每個 GPU 上都有一份基礎模型和門控網路，但只承載一部分專家網路。Token 在設備間通過 `All-to-All` 通信被路由到對應的專家。*

專家並行在巨大模型（如 Switch Transformer 1.6T）的訓練中至關重要，它允許模型參數量極大地擴展，而單個 token 的計算成本卻保持不變。

---

## 6. 多維混合並行 (Multi-Dimensional Hybrid Parallelism)

為了訓練當今最龐大的模型，通常需要將上述多種並行策略組合起來，形成**多維混合並行**。

![3D 並行示意圖](https://pic1.zhimg.com/v2-2e4aa6d0ebfee86158b0c172a1119e48_1440w.jpg)
*圖 8：一個經典的 3D 並行方案，結合了數據並行、流水線並行和張量並行。*

一個典型的 3D 並行配置如下：
1.  **數據並行維度**：將設備劃分成多個小組，每個小組執行相同的模型並行策略，處理不同的數據。
2.  **流水線並行維度**：在每個模型並行小組內部，將模型層切分到不同設備上。
3.  **張量並行維度**：在流水線的每個階段內部，再使用張量並行將單層的計算分割到多個設備上。

![不同混合並行策略](https://pic4.zhimg.com/v2-a2b89bee6c614c112e67a0fe72c62cbb_1440w.jpg)
*圖 9：針對不同硬體拓撲的混合並行策略。*

- **2D 並行 (張量+流水線)**：適用於單節點內多 GPU，利用高速 NVLink 進行張量並行。
- **3D 並行 (數據+張量+流水線)**：適用於多節點集群，節點間使用帶寬較低的網路進行數據並行或流水線並行。

---

## 7. 異構系統並行與自動並行

### 7.1 異構系統並行
傳統的分散式訓練假設所有計算設備是同構的（如都是 A100 GPU）。然而，一個系統中可能包含 CPU、多種型號的 GPU、甚至專用 AI 晶片。**異構系統並行**旨在高效利用所有這些資源，例如將適合 CPU 的計算（如數據預處理）放在 CPU 上，將記憶體密集型操作放在顯存大的 GPU 上。

![異構系統並行](https://pica.zhimg.com/v2-02bd303f4c2c42ef84007da0acbceabe_1440w.jpg)
*圖 10：在一個包含 CPU 和 GPU 的異構系統中進行並行訓練。*

### 7.2 自動並行 (Auto-Parallelism)
手動設計和調優一個高效的混合並行策略是一項極其複雜的工程任務。**自動並行**框架（如 Alpa, Colossal-AI）旨在解決這個問題。它們可以自動分析模型計算圖和硬體拓撲，從一個巨大的潛在並行策略空間中搜索出最優的組合方案。

![自動並行框架工作流程](https://pic1.zhimg.com/v2-6cae4555918e8d999c6624872a7e7a4a_1440w.jpg)
*圖 11：自動並行框架通過對計算圖進行分析、切分和設備放置，自動生成高效的並行執行計劃。*

---

## 8. 實驗設計與未來展望

理解這些基礎並行策略是進行後續更複雜分散式訓練實驗的基礎。在 `Distributed_Training_Labs` 系列中，我們將依次深入以下課題：

1.  **Lab-01 - 單節點多 GPU**：探索最基礎的數據並行 `DataParallel (DP)` 與 `DistributedDataParallel (DDP)`。
2.  **Lab-02 - 流水線並行**：使用框架（如 `torch.distributed.pipeline`）實現流水線並行，並觀察其氣泡開銷。
3.  **Lab-03 - 張量並行**：基於 Megatron-LM 的思想，手動實現一個 Transformer 層的張量並行。
4.  **Lab-04 - ZeRO 與 FSDP**：實踐 DeepSpeed ZeRO 和 PyTorch FSDP，體驗優化器並行帶來的記憶體節省。

---

## 9. 參考資料與延伸閱讀

- **[知乎] 大模型训练的并行技术**: [https://zhuanlan.zhihu.com/p/598714869](https://zhuanlan.zhihu.com/p/598714869)
- **[Megatron-LM Paper]**: Shoeybi, M., et al. (2019). *Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism*.
- **[GPipe Paper]**: Huang, Y., et al. (2019). *GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism*.
- **[ZeRO Paper]**: Rajbhandari, S., et al. (2020). *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models*.
- **[Switch Transformers Paper]**: Fedus, W., et al. (2021). *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*.
- **Hugging Face 分散式訓練文檔**: [https://huggingface.co/docs/transformers/main/en/parallelism](https://huggingface.co/docs/transformers/main/en/parallelism)
