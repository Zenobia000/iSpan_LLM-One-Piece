{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.3 Part 2: Async Processing\n",
    "\n",
    "## Objectives\n",
    "- Master async/await patterns\n",
    "- Implement concurrent request handling\n",
    "- Build streaming responses (SSE)\n",
    "- Create WebSocket endpoints\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Async Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding async/await\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "# Synchronous version\n",
    "def sync_task(n):\n",
    "    print(f\"Task {n} started\")\n",
    "    time.sleep(1)  # Simulate work\n",
    "    print(f\"Task {n} completed\")\n",
    "    return n\n",
    "\n",
    "# Async version\n",
    "async def async_task(n):\n",
    "    print(f\"Task {n} started\")\n",
    "    await asyncio.sleep(1)  # Simulate work (non-blocking)\n",
    "    print(f\"Task {n} completed\")\n",
    "    return n\n",
    "\n",
    "# Compare execution time\n",
    "print(\"Synchronous execution:\")\n",
    "start = time.time()\n",
    "for i in range(3):\n",
    "    sync_task(i)\n",
    "sync_time = time.time() - start\n",
    "print(f\"Time: {sync_time:.2f}s\\n\")\n",
    "\n",
    "print(\"Async execution:\")\n",
    "start = time.time()\n",
    "await asyncio.gather(*[async_task(i) for i in range(3)])\n",
    "async_time = time.time() - start\n",
    "print(f\"Time: {async_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nSpeedup: {sync_time/async_time:.1f}x ⚡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Async FastAPI Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_async.py\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Dict\n",
    "import uuid\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Job storage\n",
    "jobs: Dict[str, dict] = {}\n",
    "\n",
    "class JobRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 100\n",
    "\n",
    "class JobResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: str\n",
    "\n",
    "class JobResult(BaseModel):\n",
    "    job_id: str\n",
    "    status: str\n",
    "    result: str = None\n",
    "\n",
    "async def process_generation(job_id: str, prompt: str, max_tokens: int):\n",
    "    \"\"\"Background task for generation.\"\"\"\n",
    "    jobs[job_id][\"status\"] = \"processing\"\n",
    "    \n",
    "    # Simulate generation (replace with actual model)\n",
    "    await asyncio.sleep(2)\n",
    "    \n",
    "    jobs[job_id][\"status\"] = \"completed\"\n",
    "    jobs[job_id][\"result\"] = f\"Generated text for: {prompt}\"\n",
    "\n",
    "@app.post(\"/jobs/generate\", response_model=JobResponse)\n",
    "async def create_job(request: JobRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"Create async generation job.\"\"\"\n",
    "    job_id = str(uuid.uuid4())\n",
    "    \n",
    "    jobs[job_id] = {\n",
    "        \"status\": \"queued\",\n",
    "        \"prompt\": request.prompt,\n",
    "        \"result\": None,\n",
    "    }\n",
    "    \n",
    "    # Add to background tasks\n",
    "    background_tasks.add_task(\n",
    "        process_generation,\n",
    "        job_id,\n",
    "        request.prompt,\n",
    "        request.max_tokens\n",
    "    )\n",
    "    \n",
    "    return JobResponse(job_id=job_id, status=\"queued\")\n",
    "\n",
    "@app.get(\"/jobs/{job_id}\", response_model=JobResult)\n",
    "async def get_job(job_id: str):\n",
    "    \"\"\"Get job status and result.\"\"\"\n",
    "    if job_id not in jobs:\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    \n",
    "    job = jobs[job_id]\n",
    "    return JobResult(\n",
    "        job_id=job_id,\n",
    "        status=job[\"status\"],\n",
    "        result=job.get(\"result\")\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Streaming Responses (SSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_streaming.py\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class StreamRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 50\n",
    "\n",
    "async def generate_stream(prompt: str, max_tokens: int):\n",
    "    \"\"\"Simulate streaming generation.\"\"\"\n",
    "    words = [\n",
    "        \"The\", \"future\", \"of\", \"artificial\", \"intelligence\",\n",
    "        \"is\", \"very\", \"promising\", \"and\", \"exciting\"\n",
    "    ]\n",
    "    \n",
    "    for i, word in enumerate(words[:max_tokens]):\n",
    "        await asyncio.sleep(0.1)  # Simulate generation delay\n",
    "        \n",
    "        data = {\n",
    "            \"token\": word,\n",
    "            \"index\": i,\n",
    "            \"done\": i == len(words) - 1\n",
    "        }\n",
    "        \n",
    "        # SSE format\n",
    "        yield f\"data: {json.dumps(data)}\\n\\n\"\n",
    "    \n",
    "    # End signal\n",
    "    yield \"data: [DONE]\\n\\n\"\n",
    "\n",
    "@app.post(\"/v1/completions/stream\")\n",
    "async def stream_generate(request: StreamRequest):\n",
    "    \"\"\"Stream generation with Server-Sent Events.\"\"\"\n",
    "    return StreamingResponse(\n",
    "        generate_stream(request.prompt, request.max_tokens),\n",
    "        media_type=\"text/event-stream\",\n",
    "        headers={\n",
    "            \"Cache-Control\": \"no-cache\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Streaming\n",
    "\n",
    "**Note**: Streaming requires special client handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming endpoint\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def test_streaming(prompt: str):\n",
    "    \"\"\"Test SSE streaming.\"\"\"\n",
    "    url = \"http://localhost:8003/v1/completions/stream\"\n",
    "    \n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, stream=True)\n",
    "        \n",
    "        print(f\"Streaming response for: '{prompt}'\\n\")\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line = line.decode('utf-8')\n",
    "                if line.startswith('data: '):\n",
    "                    data = line[6:]  # Remove 'data: ' prefix\n",
    "                    if data == '[DONE]':\n",
    "                        print(\"\\n\\n✅ Stream complete\")\n",
    "                        break\n",
    "                    else:\n",
    "                        token_data = json.loads(data)\n",
    "                        print(token_data['token'], end=' ', flush=True)\n",
    "    \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"❌ Server not running on port 8003\")\n",
    "\n",
    "# Test\n",
    "# test_streaming(\"The future of AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. WebSocket Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_websocket.py\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.websocket(\"/ws/generate\")\n",
    "async def websocket_generate(websocket: WebSocket):\n",
    "    \"\"\"WebSocket endpoint for real-time generation.\"\"\"\n",
    "    await websocket.accept()\n",
    "    \n",
    "    try:\n",
    "        # Receive request\n",
    "        data = await websocket.receive_json()\n",
    "        prompt = data.get(\"prompt\", \"\")\n",
    "        max_tokens = data.get(\"max_tokens\", 50)\n",
    "        \n",
    "        # Simulate streaming generation\n",
    "        words = prompt.split() + [\n",
    "            \"is\", \"a\", \"fascinating\", \"topic\", \"that\", \"requires\", \"further\", \"study\"\n",
    "        ]\n",
    "        \n",
    "        for i, word in enumerate(words[:max_tokens]):\n",
    "            await asyncio.sleep(0.1)\n",
    "            \n",
    "            await websocket.send_json({\n",
    "                \"type\": \"token\",\n",
    "                \"data\": word,\n",
    "                \"index\": i\n",
    "            })\n",
    "        \n",
    "        # Send completion\n",
    "        await websocket.send_json({\"type\": \"done\"})\n",
    "        \n",
    "    except WebSocketDisconnect:\n",
    "        print(\"Client disconnected\")\n",
    "    finally:\n",
    "        await websocket.close()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"WebSocket server running on /ws/generate\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebSocket Client Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebSocket client test (conceptual)\n",
    "# Requires: pip install websockets\n",
    "\n",
    "example_code = '''\n",
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "\n",
    "async def test_websocket():\n",
    "    uri = \"ws://localhost:8004/ws/generate\"\n",
    "    \n",
    "    async with websockets.connect(uri) as websocket:\n",
    "        # Send request\n",
    "        await websocket.send(json.dumps({\n",
    "            \"prompt\": \"Machine learning\",\n",
    "            \"max_tokens\": 10\n",
    "        }))\n",
    "        \n",
    "        # Receive tokens\n",
    "        while True:\n",
    "            message = await websocket.recv()\n",
    "            data = json.loads(message)\n",
    "            \n",
    "            if data[\"type\"] == \"done\":\n",
    "                break\n",
    "            \n",
    "            print(data[\"data\"], end=\" \", flush=True)\n",
    "\n",
    "# Run: asyncio.run(test_websocket())\n",
    "'''\n",
    "\n",
    "print(\"WebSocket Client Example:\")\n",
    "print(\"=\" * 60)\n",
    "print(example_code)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Concurrent Request Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_concurrent.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 50\n",
    "\n",
    "# Request queue\n",
    "request_queue = deque(maxlen=100)\n",
    "active_requests = 0\n",
    "MAX_CONCURRENT = 10\n",
    "\n",
    "async def generate_text(prompt: str, max_tokens: int) -> str:\n",
    "    \"\"\"Simulate LLM generation.\"\"\"\n",
    "    global active_requests\n",
    "    \n",
    "    # Wait if too many active requests\n",
    "    while active_requests >= MAX_CONCURRENT:\n",
    "        await asyncio.sleep(0.1)\n",
    "    \n",
    "    active_requests += 1\n",
    "    \n",
    "    try:\n",
    "        # Simulate generation\n",
    "        await asyncio.sleep(1)\n",
    "        return f\"Generated: {prompt} (simulated)\"\n",
    "    finally:\n",
    "        active_requests -= 1\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: GenerateRequest):\n",
    "    \"\"\"Handle generation with concurrency control.\"\"\"\n",
    "    result = await generate_text(request.prompt, request.max_tokens)\n",
    "    \n",
    "    return {\n",
    "        \"text\": result,\n",
    "        \"queue_size\": len(request_queue),\n",
    "        \"active_requests\": active_requests\n",
    "    }\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "async def stats():\n",
    "    return {\n",
    "        \"active_requests\": active_requests,\n",
    "        \"queue_size\": len(request_queue),\n",
    "        \"max_concurrent\": MAX_CONCURRENT\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Concurrent Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrent load test\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "async def send_request(session, url, prompt, request_id):\n",
    "    \"\"\"Send async request.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        async with session.post(url, json={\"prompt\": prompt}) as response:\n",
    "            result = await response.json()\n",
    "            elapsed = time.time() - start\n",
    "            return {\n",
    "                \"id\": request_id,\n",
    "                \"time\": elapsed,\n",
    "                \"success\": True\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"id\": request_id,\n",
    "            \"time\": time.time() - start,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "async def load_test(num_requests=20):\n",
    "    \"\"\"Run concurrent load test.\"\"\"\n",
    "    url = \"http://localhost:8005/generate\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [\n",
    "            send_request(session, url, f\"Test prompt {i}\", i)\n",
    "            for i in range(num_requests)\n",
    "        ]\n",
    "        \n",
    "        start = time.time()\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        total_time = time.time() - start\n",
    "    \n",
    "    # Analyze results\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    latencies = [r[\"time\"] for r in results if r[\"success\"]]\n",
    "    \n",
    "    print(f\"\\nLoad Test Results ({num_requests} concurrent requests):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total time:     {total_time:.2f}s\")\n",
    "    print(f\"Successful:     {successful}/{num_requests}\")\n",
    "    print(f\"Requests/sec:   {num_requests/total_time:.2f}\")\n",
    "    print(f\"Avg latency:    {np.mean(latencies):.3f}s\")\n",
    "    print(f\"Min latency:    {np.min(latencies):.3f}s\")\n",
    "    print(f\"Max latency:    {np.max(latencies):.3f}s\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run test (if server is running)\n",
    "# await load_test(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ **Completed**:\n",
    "1. Mastered async/await patterns\n",
    "2. Implemented background tasks\n",
    "3. Built streaming responses (SSE)\n",
    "4. Created WebSocket endpoints\n",
    "5. Handled concurrent requests\n",
    "\n",
    "📚 **Key Patterns**:\n",
    "- `async def` for async functions\n",
    "- `await` for async operations\n",
    "- `asyncio.gather()` for concurrency\n",
    "- `StreamingResponse` for SSE\n",
    "- `WebSocket` for bidirectional communication\n",
    "\n",
    "💡 **Best Practices**:\n",
    "- Use async for I/O-bound operations\n",
    "- Limit concurrent requests to avoid overload\n",
    "- Implement proper error handling\n",
    "- Use streaming for better UX\n",
    "\n",
    "➡️ **Next**: In `03-Integration_with_vLLM.ipynb`, we'll:\n",
    "- Integrate AsyncLLMEngine\n",
    "- Build production-ready endpoints\n",
    "- Optimize concurrent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Lab 2.3 Part 2 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
