{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1 Part 1: vLLM Setup and Installation\n",
    "\n",
    "## Objectives\n",
    "- Verify environment (CUDA, GPU)\n",
    "- Install vLLM\n",
    "- Run basic inference test\n",
    "- Understand PagedAttention basics\n",
    "\n",
    "## Estimated Time: 30-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"  Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: No CUDA GPU detected!\")\n",
    "    print(\"vLLM requires a CUDA-compatible GPU to run efficiently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed GPU memory check\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        \n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved:  {reserved:.2f} GB\")\n",
    "        print(f\"  Free:      {total - reserved:.2f} GB\")\n",
    "        print(f\"  Total:     {total:.2f} GB\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install vLLM\n",
    "\n",
    "vLLM can be installed via pip. For CUDA 12.1+:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if vLLM is already installed\n",
    "try:\n",
    "    import vllm\n",
    "    print(f\"‚úÖ vLLM is already installed: v{vllm.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå vLLM is not installed.\")\n",
    "    print(\"\\nInstalling vLLM...\")\n",
    "    print(\"Run this in terminal:\")\n",
    "    print(\"  pip install vllm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation Command\n",
    "\n",
    "If vLLM is not installed, run in terminal:\n",
    "\n",
    "```bash\n",
    "# Basic installation\n",
    "pip install vllm\n",
    "\n",
    "# Or specify CUDA version\n",
    "pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Basic Inference Test\n",
    "\n",
    "Let's test vLLM with a small model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vLLM\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ vLLM imported successfully!\")\n",
    "print(f\"Version: {vllm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Small Model\n",
    "\n",
    "We'll use GPT-2 for quick testing (124M parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vLLM with GPT-2\n",
    "print(\"Loading GPT-2 model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"gpt2\",\n",
    "    gpu_memory_utilization=0.3,  # Use 30% GPU memory for testing\n",
    "    max_model_len=512,           # Limit context length\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"‚úÖ Model loaded in {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=50,\n",
    ")\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"Once upon a time in a distant land,\",\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Python is a programming language that\",\n",
    "]\n",
    "\n",
    "print(\"Generating text...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "for i, output in enumerate(outputs):\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"‚è±Ô∏è  Average time per prompt: {generation_time/len(prompts):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PagedAttention Overview\n",
    "\n",
    "PagedAttention is vLLM's key innovation for efficient KV cache management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional KV Cache Problem\n",
    "\n",
    "Traditional approach allocates contiguous memory:\n",
    "\n",
    "```\n",
    "Request 1 (len=1024): ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (allocated 2048, used 1024)\n",
    "Request 2 (len=512):  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (allocated 2048, used 512)\n",
    "\n",
    "Memory waste: ~60%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PagedAttention Solution\n",
    "\n",
    "PagedAttention uses paging (like virtual memory):\n",
    "\n",
    "```\n",
    "Physical blocks: [P0][P1][P2][P3][P4][P5]...\n",
    "\n",
    "Request 1: P0 ‚Üí P1 ‚Üí P2 ‚Üí P3 (1024 tokens, 4 blocks)\n",
    "Request 2: P4 ‚Üí P5           (512 tokens, 2 blocks)\n",
    "\n",
    "Memory waste: ~0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory efficiency\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated data\n",
    "approaches = ['Traditional', 'PagedAttention']\n",
    "memory_used = [40, 95]  # Percentage\n",
    "memory_wasted = [60, 5]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Memory utilization\n",
    "ax1.bar(approaches, memory_used, color=['#ff6b6b', '#51cf66'])\n",
    "ax1.set_ylabel('Memory Utilization (%)')\n",
    "ax1.set_title('Memory Utilization Comparison')\n",
    "ax1.set_ylim(0, 100)\n",
    "for i, v in enumerate(memory_used):\n",
    "    ax1.text(i, v + 2, f\"{v}%\", ha='center', fontweight='bold')\n",
    "\n",
    "# Memory waste\n",
    "ax2.bar(approaches, memory_wasted, color=['#ff6b6b', '#51cf66'])\n",
    "ax2.set_ylabel('Memory Waste (%)')\n",
    "ax2.set_title('Memory Waste Comparison')\n",
    "ax2.set_ylim(0, 100)\n",
    "for i, v in enumerate(memory_wasted):\n",
    "    ax2.text(i, v + 2, f\"{v}%\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä PagedAttention improves memory utilization from 40% to 95%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compare with HuggingFace\n",
    "\n",
    "Let's compare vLLM with standard HuggingFace inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace baseline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading HuggingFace GPT-2...\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ HuggingFace model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace generation\n",
    "test_prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"Testing HuggingFace...\")\n",
    "inputs = hf_tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    hf_outputs = hf_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "    )\n",
    "hf_time = time.time() - start_time\n",
    "\n",
    "hf_text = hf_tokenizer.decode(hf_outputs[0], skip_special_tokens=True)\n",
    "print(f\"HuggingFace output: {hf_text}\")\n",
    "print(f\"Time: {hf_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM generation (same prompt)\n",
    "print(\"\\nTesting vLLM...\")\n",
    "start_time = time.time()\n",
    "vllm_outputs = llm.generate([test_prompt], sampling_params)\n",
    "vllm_time = time.time() - start_time\n",
    "\n",
    "vllm_text = vllm_outputs[0].outputs[0].text\n",
    "print(f\"vLLM output: {test_prompt}{vllm_text}\")\n",
    "print(f\"Time: {vllm_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "speedup = hf_time / vllm_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"HuggingFace:  {hf_time:.3f}s\")\n",
    "print(f\"vLLM:         {vllm_time:.3f}s\")\n",
    "print(f\"Speedup:      {speedup:.2f}x faster ‚ö°\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Check vLLM Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect vLLM engine configuration\n",
    "print(\"vLLM Engine Configuration:\")\n",
    "print(f\"  Model: {llm.llm_engine.model_config.model}\")\n",
    "print(f\"  Max model length: {llm.llm_engine.model_config.max_model_len}\")\n",
    "print(f\"  GPU memory utilization: {llm.llm_engine.cache_config.gpu_memory_utilization}\")\n",
    "print(f\"  Block size: {llm.llm_engine.cache_config.block_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "‚úÖ **Completed**:\n",
    "1. Verified CUDA and GPU environment\n",
    "2. Installed vLLM\n",
    "3. Ran basic inference test\n",
    "4. Understood PagedAttention benefits\n",
    "5. Compared vLLM vs HuggingFace\n",
    "\n",
    "üìä **Key Findings**:\n",
    "- vLLM provides significant speedup over HuggingFace\n",
    "- PagedAttention improves memory utilization from ~40% to ~95%\n",
    "- Simple API similar to HuggingFace\n",
    "\n",
    "‚û°Ô∏è **Next**: In `02-Basic_Inference.ipynb`, we'll explore:\n",
    "- Batch inference\n",
    "- Advanced sampling strategies\n",
    "- Memory profiling\n",
    "- Throughput optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Try different models**: Replace GPT-2 with other models (e.g., `facebook/opt-125m`)\n",
    "2. **Adjust parameters**: Experiment with `gpu_memory_utilization` and `max_model_len`\n",
    "3. **Measure memory**: Use `nvidia-smi` to monitor GPU memory usage\n",
    "4. **Batch size**: Test with different numbers of prompts (1, 4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import gc\n",
    "\n",
    "del llm\n",
    "del hf_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Cleaned up GPU memory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
