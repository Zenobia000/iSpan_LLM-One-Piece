{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashAttention å¯¦æˆ°æ¼”ç¤º\n",
    "\n",
    "æœ¬ç­†è¨˜æœ¬æ¼”ç¤ºå¦‚ä½•å°‡ FlashAttention æ•´åˆåˆ° GPT-2 æ¨¡å‹ä¸­ï¼Œä¸¦æ¯”è¼ƒæ€§èƒ½å·®ç•°ã€‚\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "1. ç†è§£ FlashAttention çš„å¯¦éš›æ•´åˆæ–¹æ³•\n",
    "2. è§£æ±º dtype å…¼å®¹æ€§å•é¡Œ\n",
    "3. æ¸¬é‡é€Ÿåº¦å’Œè¨˜æ†¶é«”ä½¿ç”¨æ”¹é€²\n",
    "4. é©—è­‰è¼¸å‡ºä¸€è‡´æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è¨­å‚™: cuda\n",
      "CUDA å¯ç”¨: True\n",
      "GPU è¨˜æ†¶é«”: 16.7 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "from flash_attn import flash_attn_func\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# è¨­å®šè¨­å‚™å’Œæ•¸æ“šé¡å‹\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashAttention Layer å¯¦ç¾\n",
    "\n",
    "å‰µå»ºå…¼å®¹ GPT-2 çš„ FlashAttention å±¤ï¼Œè™•ç† dtype è½‰æ›å’Œæ¬Šé‡çŸ©é™£æ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttentionLayer å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class FlashAttentionLayer(nn.Module):\n",
    "    \"\"\"ä½¿ç”¨ FlashAttention çš„æ³¨æ„åŠ›å±¤ï¼Œå…¼å®¹ GPT-2\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        \n",
    "        # QKV æŠ•å½±å±¤ - éœ€è¦æ­£ç¢ºçš„æ¬Šé‡çŸ©é™£æ ¼å¼\n",
    "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd, bias=True)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=True)\n",
    "        \n",
    "        # æ·»åŠ  dropout æ”¯æŒ\n",
    "        self.attn_dropout = getattr(config, 'attn_pdrop', 0.1)\n",
    "        self.resid_dropout = nn.Dropout(getattr(config, 'resid_pdrop', 0.1))\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None, layer_past=None,\n",
    "                head_mask=None, use_cache=False, output_attentions=False, \n",
    "                past_key_values=None, **kwargs):\n",
    "        # å…¼å®¹ä¸åŒçš„åƒæ•¸åç¨±\n",
    "        if past_key_values is not None:\n",
    "            layer_past = past_key_values\n",
    "            \n",
    "        x = hidden_states\n",
    "        B, T, C = x.size()  # batch_size, seq_len, embed_dim\n",
    "        original_dtype = x.dtype\n",
    "\n",
    "        # FlashAttention éœ€è¦ fp16 æˆ– bf16\n",
    "        if x.dtype == torch.float32:\n",
    "            x = x.half()  # è½‰æ›ç‚º fp16\n",
    "\n",
    "        # ç¢ºä¿æ¬Šé‡ä¹Ÿæ˜¯ fp16\n",
    "        if self.c_attn.weight.dtype != x.dtype:\n",
    "            self.c_attn = self.c_attn.half()\n",
    "\n",
    "        # è¨ˆç®— QKV\n",
    "        qkv = self.c_attn(x)  # (B, T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # æ¯å€‹éƒ½æ˜¯ (B, T, C)\n",
    "\n",
    "        # è™•ç† past_key_values (KV cache) - æ›´å®‰å…¨çš„è§£åŒ…\n",
    "        if layer_past is not None:\n",
    "            try:\n",
    "                if isinstance(layer_past, (tuple, list)) and len(layer_past) >= 2:\n",
    "                    past_key, past_value = layer_past[0], layer_past[1]\n",
    "                    k = torch.cat((past_key, k), dim=1)\n",
    "                    v = torch.cat((past_value, v), dim=1)\n",
    "                else:\n",
    "                    # å¦‚æœ layer_past æ ¼å¼ä¸æ­£ç¢ºï¼Œå¿½ç•¥å®ƒ\n",
    "                    layer_past = None\n",
    "            except (ValueError, IndexError):\n",
    "                # è§£åŒ…å¤±æ•—ï¼Œå¿½ç•¥ past_key_values\n",
    "                layer_past = None\n",
    "\n",
    "        # æº–å‚™ present (æ–°çš„ KV cache)\n",
    "        present = None\n",
    "        if use_cache:\n",
    "            present = (k, v)\n",
    "\n",
    "        # é‡å¡‘ç‚ºå¤šé ­æ ¼å¼: (B, T, n_head, head_dim)\n",
    "        current_seq_len = k.size(1)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim)\n",
    "        k = k.view(B, current_seq_len, self.n_head, self.head_dim)\n",
    "        v = v.view(B, current_seq_len, self.n_head, self.head_dim)\n",
    "\n",
    "        # æ™ºèƒ½é¸æ“‡ FlashAttention å‡½æ•¸\n",
    "        if layer_past is None:\n",
    "            # æ²’æœ‰ KV cacheï¼Œä½¿ç”¨ packed format (æ›´é«˜æ•ˆ)\n",
    "            try:\n",
    "                from flash_attn import flash_attn_qkvpacked_func\n",
    "                qkv_packed = torch.stack([q, k, v], dim=2)  # (B, T, 3, H, D)\n",
    "                \n",
    "                attn_output = flash_attn_qkvpacked_func(\n",
    "                    qkv_packed,\n",
    "                    dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "                    causal=True\n",
    "                )\n",
    "            except ImportError:\n",
    "                # é™ç´šåˆ°åŸºæœ¬ flash_attn_func\n",
    "                attn_output = flash_attn_func(\n",
    "                    q, k, v,\n",
    "                    dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "                    softmax_scale=None,\n",
    "                    causal=True\n",
    "                )\n",
    "        else:\n",
    "            # æœ‰ KV cacheï¼Œä½¿ç”¨ separate QKV format\n",
    "            attn_output = flash_attn_func(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "                softmax_scale=None,\n",
    "                causal=True\n",
    "            )\n",
    "\n",
    "        # é‡å¡‘å›åŸå§‹æ ¼å¼: (B, T, C)\n",
    "        attn_output = attn_output.contiguous().view(B, T, C)\n",
    "\n",
    "        # è¼¸å‡ºæŠ•å½±\n",
    "        if self.c_proj.weight.dtype != attn_output.dtype:\n",
    "            self.c_proj = self.c_proj.half()\n",
    "\n",
    "        output = self.c_proj(attn_output)\n",
    "        output = self.resid_dropout(output)\n",
    "\n",
    "        # å¦‚æœåŸå§‹è¼¸å…¥æ˜¯ fp32ï¼Œè½‰æ›å›å»\n",
    "        if original_dtype == torch.float32:\n",
    "            output = output.float()\n",
    "\n",
    "        # è¿”å›èˆ‡ GPT-2 æ³¨æ„åŠ›å±¤ç›¸åŒçš„æ ¼å¼\n",
    "        outputs = (output, present)\n",
    "        if output_attentions:\n",
    "            # FlashAttention ä¸è¿”å›æ³¨æ„åŠ›æ¬Šé‡ï¼Œè¿”å› None\n",
    "            outputs = outputs + (None,)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def copy_weights_from_gpt2_attention(self, old_attn):\n",
    "        \"\"\"å¾åŸå§‹ GPT-2 æ³¨æ„åŠ›å±¤è¤‡è£½æ¬Šé‡\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # æª¢æŸ¥æ¬Šé‡çŸ©é™£ç¶­åº¦ä¸¦é€²è¡Œå¿…è¦çš„è½‰ç½®\n",
    "            if old_attn.c_attn.weight.shape == (self.config.n_embd, 3 * self.config.n_embd):\n",
    "                # GPT-2 æ ¼å¼: [n_embd, 3*n_embd] -> éœ€è¦è½‰ç½®ç‚º [3*n_embd, n_embd]\n",
    "                self.c_attn.weight.data = old_attn.c_attn.weight.data.transpose(0, 1).contiguous()\n",
    "            else:\n",
    "                self.c_attn.weight.data = old_attn.c_attn.weight.data.clone()\n",
    "                \n",
    "            self.c_attn.bias.data = old_attn.c_attn.bias.data.clone()\n",
    "            \n",
    "            # è¤‡è£½è¼¸å‡ºæŠ•å½±æ¬Šé‡\n",
    "            if old_attn.c_proj.weight.shape == (self.config.n_embd, self.config.n_embd):\n",
    "                self.c_proj.weight.data = old_attn.c_proj.weight.data.transpose(0, 1).contiguous()\n",
    "            else:\n",
    "                self.c_proj.weight.data = old_attn.c_proj.weight.data.clone()\n",
    "                \n",
    "            self.c_proj.bias.data = old_attn.c_proj.bias.data.clone()\n",
    "\n",
    "print(\"FlashAttentionLayer å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¼‰å…¥å’Œæº–å‚™æ¨¡å‹\n",
    "\n",
    "è¼‰å…¥ GPT-2 æ¨¡å‹ä¸¦æº–å‚™æ¸¬è©¦æ•¸æ“šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹é…ç½®: 768 åµŒå…¥ç¶­åº¦, 12 æ³¨æ„åŠ›é ­, 12 å±¤\n",
      "GPT-2 æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œåƒæ•¸æ•¸é‡: 124,439,808\n",
      "æ¸¬è©¦æ•¸æ“šå½¢ç‹€: torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "# è¼‰å…¥ GPT-2 æ¨¡å‹\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "print(f\"æ¨¡å‹é…ç½®: {config.n_embd} åµŒå…¥ç¶­åº¦, {config.n_head} æ³¨æ„åŠ›é ­, {config.n_layer} å±¤\")\n",
    "\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "print(f\"GPT-2 æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œåƒæ•¸æ•¸é‡: {sum(p.numel() for p in gpt2_model.parameters()):,}\")\n",
    "\n",
    "# æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "batch_size = 4\n",
    "seq_len = 512\n",
    "vocab_size = config.vocab_size\n",
    "\n",
    "# éš¨æ©Ÿè¼¸å…¥ token IDs\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "print(f\"æ¸¬è©¦æ•¸æ“šå½¢ç‹€: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ›¿æ›æ³¨æ„åŠ›å±¤\n",
    "\n",
    "å°‡ GPT-2 çš„æ¨™æº–æ³¨æ„åŠ›å±¤æ›¿æ›ç‚º FlashAttention å±¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å±¤ 0: åŸå§‹æ¬Šé‡å½¢ç‹€ torch.Size([768, 2304]) -> Flash æ¬Šé‡å½¢ç‹€ torch.Size([2304, 768])\n",
      "æ‰€æœ‰ 12 å±¤çš„æ³¨æ„åŠ›æ©Ÿåˆ¶å·²æ›¿æ›ç‚º FlashAttention\n"
     ]
    }
   ],
   "source": [
    "# å‰µå»ºä½¿ç”¨ FlashAttention çš„æ¨¡å‹å‰¯æœ¬\n",
    "gpt2_flash = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "\n",
    "# æ›¿æ›æ‰€æœ‰æ³¨æ„åŠ›å±¤\n",
    "for i, layer in enumerate(gpt2_flash.h):\n",
    "    old_attn = layer.attn\n",
    "    \n",
    "    # å‰µå»ºæ–°çš„ FlashAttention å±¤\n",
    "    flash_attn_layer = FlashAttentionLayer(config).to(device)\n",
    "    \n",
    "    # è¤‡è£½æ¬Šé‡\n",
    "    flash_attn_layer.copy_weights_from_gpt2_attention(old_attn)\n",
    "    \n",
    "    # æ›¿æ›å±¤\n",
    "    layer.attn = flash_attn_layer\n",
    "    \n",
    "    if i == 0:  # åªæ‰“å°ç¬¬ä¸€å±¤çš„ä¿¡æ¯\n",
    "        print(f\"å±¤ {i}: åŸå§‹æ¬Šé‡å½¢ç‹€ {old_attn.c_attn.weight.shape} -> Flash æ¬Šé‡å½¢ç‹€ {flash_attn_layer.c_attn.weight.shape}\")\n",
    "\n",
    "print(f\"æ‰€æœ‰ {len(gpt2_flash.h)} å±¤çš„æ³¨æ„åŠ›æ©Ÿåˆ¶å·²æ›¿æ›ç‚º FlashAttention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
    "\n",
    "æ¯”è¼ƒæ¨™æº–æ³¨æ„åŠ›å’Œ FlashAttention çš„é€Ÿåº¦å’Œè¨˜æ†¶é«”ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "é–‹å§‹æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
      "==================================================\n",
      "\n",
      "æ¨™æº– GPT-2 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0797 Â± 0.0013 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 442.0 MB\n"
     ]
    }
   ],
   "source": [
    "def benchmark_model(model, input_ids, model_name, num_runs=5):\n",
    "    \"\"\"æ¸¬é‡æ¨¡å‹æ¨ç†æ€§èƒ½\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # é ç†±\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_ids)\n",
    "    \n",
    "    # æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # è¨˜éŒ„åˆå§‹è¨˜æ†¶é«”\n",
    "        initial_memory = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # æ¸¬é‡æ¨ç†æ™‚é–“\n",
    "    times = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    # è¨˜éŒ„å³°å€¼è¨˜æ†¶é«”\n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory = torch.cuda.max_memory_allocated()\n",
    "        memory_used = (peak_memory - initial_memory) / 1e6  # MB\n",
    "    else:\n",
    "        memory_used = 0\n",
    "    \n",
    "    avg_time = np.mean(times[1:])  # æ’é™¤ç¬¬ä¸€æ¬¡é‹è¡Œ\n",
    "    std_time = np.std(times[1:])\n",
    "    \n",
    "    print(f\"\\n{model_name} æ€§èƒ½:\")\n",
    "    print(f\"  å¹³å‡æ¨ç†æ™‚é–“: {avg_time:.4f} Â± {std_time:.4f} ç§’\")\n",
    "    print(f\"  è¨˜æ†¶é«”ä½¿ç”¨: {memory_used:.1f} MB\")\n",
    "    \n",
    "    return avg_time, memory_used, outputs\n",
    "\n",
    "# æ¸¬è©¦æ¨™æº– GPT-2\n",
    "print(\"=\" * 50)\n",
    "print(\"é–‹å§‹æ€§èƒ½åŸºæº–æ¸¬è©¦\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "std_time, std_memory, std_outputs = benchmark_model(gpt2_model, input_ids, \"æ¨™æº– GPT-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FlashAttention GPT-2 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0631 Â± 0.0003 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 649.7 MB\n",
      "\n",
      "==================================================\n",
      "æ€§èƒ½æ”¹é€²ç¸½çµ\n",
      "==================================================\n",
      "é€Ÿåº¦æå‡: 1.26x (+26.2%)\n",
      "è¨˜æ†¶é«”ç¯€çœ: -47.0%\n",
      "çµ•å°æ™‚é–“ç¯€çœ: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦ FlashAttention GPT-2\n",
    "flash_time, flash_memory, flash_outputs = benchmark_model(gpt2_flash, input_ids, \"FlashAttention GPT-2\")\n",
    "\n",
    "# è¨ˆç®—æ”¹é€²å¹…åº¦\n",
    "speed_improvement = std_time / flash_time\n",
    "memory_reduction = (std_memory - flash_memory) / std_memory * 100\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"æ€§èƒ½æ”¹é€²ç¸½çµ\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"é€Ÿåº¦æå‡: {speed_improvement:.2f}x ({((speed_improvement-1)*100):+.1f}%)\")\n",
    "print(f\"è¨˜æ†¶é«”ç¯€çœ: {memory_reduction:+.1f}%\")\n",
    "print(f\"çµ•å°æ™‚é–“ç¯€çœ: {(std_time - flash_time)*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¼¸å‡ºä¸€è‡´æ€§é©—è­‰\n",
    "\n",
    "ç¢ºä¿ FlashAttention çš„è¼¸å‡ºèˆ‡æ¨™æº–æ³¨æ„åŠ›æ©Ÿåˆ¶ä¸€è‡´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "è¼¸å‡ºä¸€è‡´æ€§åˆ†æ\n",
      "==================================================\n",
      "è¼¸å‡ºå½¢ç‹€: torch.Size([4, 512, 768])\n",
      "æ•¸æ“šé¡å‹: torch.float32 vs torch.float32\n",
      "\n",
      "çµ•å°å·®ç•°:\n",
      "  æœ€å¤§å·®ç•°: 0.198700\n",
      "  å¹³å‡å·®ç•°: 0.000388\n",
      "\n",
      "ç›¸å°å·®ç•°:\n",
      "  æœ€å¤§ç›¸å°å·®ç•°: 3912.363770 (391236.3770%)\n",
      "  å¹³å‡ç›¸å°å·®ç•°: 0.013574 (1.3574%)\n",
      "\n",
      "å®¹å·®æª¢æŸ¥ (tolerance=0.01): âœ— å¤±æ•—\n"
     ]
    }
   ],
   "source": [
    "# æ¯”è¼ƒè¼¸å‡ºå·®ç•°\n",
    "def compare_outputs(output1, output2, tolerance=1e-2):\n",
    "    \"\"\"æ¯”è¼ƒå…©å€‹æ¨¡å‹è¼¸å‡ºçš„å·®ç•°\"\"\"\n",
    "    \n",
    "    # ç¢ºä¿ç›¸åŒçš„æ•¸æ“šé¡å‹\n",
    "    if output1.last_hidden_state.dtype != output2.last_hidden_state.dtype:\n",
    "        output2_converted = output2.last_hidden_state.float()\n",
    "        output1_state = output1.last_hidden_state.float()\n",
    "    else:\n",
    "        output1_state = output1.last_hidden_state\n",
    "        output2_converted = output2.last_hidden_state\n",
    "    \n",
    "    # è¨ˆç®—å·®ç•°çµ±è¨ˆ\n",
    "    diff = torch.abs(output1_state - output2_converted)\n",
    "    max_diff = torch.max(diff).item()\n",
    "    mean_diff = torch.mean(diff).item()\n",
    "    \n",
    "    # è¨ˆç®—ç›¸å°èª¤å·®\n",
    "    relative_diff = diff / (torch.abs(output1_state) + 1e-8)\n",
    "    max_relative_diff = torch.max(relative_diff).item()\n",
    "    mean_relative_diff = torch.mean(relative_diff).item()\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦åœ¨å®¹å·®ç¯„åœå…§\n",
    "    is_close = torch.allclose(output1_state, output2_converted, atol=tolerance, rtol=tolerance)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"è¼¸å‡ºä¸€è‡´æ€§åˆ†æ\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"è¼¸å‡ºå½¢ç‹€: {output1_state.shape}\")\n",
    "    print(f\"æ•¸æ“šé¡å‹: {output1_state.dtype} vs {output2_converted.dtype}\")\n",
    "    print(f\"\\nçµ•å°å·®ç•°:\")\n",
    "    print(f\"  æœ€å¤§å·®ç•°: {max_diff:.6f}\")\n",
    "    print(f\"  å¹³å‡å·®ç•°: {mean_diff:.6f}\")\n",
    "    print(f\"\\nç›¸å°å·®ç•°:\")\n",
    "    print(f\"  æœ€å¤§ç›¸å°å·®ç•°: {max_relative_diff:.6f} ({max_relative_diff*100:.4f}%)\")\n",
    "    print(f\"  å¹³å‡ç›¸å°å·®ç•°: {mean_relative_diff:.6f} ({mean_relative_diff*100:.4f}%)\")\n",
    "    print(f\"\\nå®¹å·®æª¢æŸ¥ (tolerance={tolerance}): {'âœ“ é€šé' if is_close else 'âœ— å¤±æ•—'}\")\n",
    "    \n",
    "    return is_close, max_diff, mean_diff\n",
    "\n",
    "# åŸ·è¡Œä¸€è‡´æ€§æª¢æŸ¥\n",
    "is_consistent, max_diff, mean_diff = compare_outputs(std_outputs, flash_outputs, tolerance=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è©³ç´°åˆ†æå’Œå¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ä¸åŒåºåˆ—é•·åº¦çš„æ€§èƒ½åˆ†æ\n",
      "============================================================\n",
      "åºåˆ—é•·åº¦       æ¨™æº–æ™‚é–“(s)      Flashæ™‚é–“(s)   é€Ÿåº¦æå‡       è¨˜æ†¶é«”ç¯€çœ     \n",
      "------------------------------------------------------------\n",
      "\n",
      "æ¨™æº–-128 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0083 Â± 0.0000 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 636.3 MB\n",
      "\n",
      "Flash-128 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0070 Â± 0.0000 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 635.5 MB\n",
      "128        0.0083       0.0070       1.19      x 0.1       %\n",
      "\n",
      "æ¨™æº–-256 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0169 Â± 0.0001 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 615.8 MB\n",
      "\n",
      "Flash-256 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0127 Â± 0.0001 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 615.1 MB\n",
      "256        0.0169       0.0127       1.33      x 0.1       %\n",
      "\n",
      "æ¨™æº–-512 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0369 Â± 0.0001 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 575.7 MB\n",
      "\n",
      "Flash-512 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0285 Â± 0.0001 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 574.2 MB\n",
      "512        0.0369       0.0285       1.30      x 0.3       %\n",
      "\n",
      "æ¨™æº–-1024 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0818 Â± 0.0006 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 495.5 MB\n",
      "\n",
      "Flash-1024 æ€§èƒ½:\n",
      "  å¹³å‡æ¨ç†æ™‚é–“: 0.0610 Â± 0.0000 ç§’\n",
      "  è¨˜æ†¶é«”ä½¿ç”¨: 492.4 MB\n",
      "1024       0.0818       0.0610       1.34      x 0.6       %\n"
     ]
    }
   ],
   "source": [
    "# åˆ†æä¸åŒåºåˆ—é•·åº¦çš„æ€§èƒ½\n",
    "def analyze_sequence_lengths():\n",
    "    \"\"\"åˆ†æä¸åŒåºåˆ—é•·åº¦ä¸‹çš„æ€§èƒ½å·®ç•°\"\"\"\n",
    "    seq_lengths = [128, 256, 512, 1024]\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ä¸åŒåºåˆ—é•·åº¦çš„æ€§èƒ½åˆ†æ\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'åºåˆ—é•·åº¦':<10} {'æ¨™æº–æ™‚é–“(s)':<12} {'Flashæ™‚é–“(s)':<12} {'é€Ÿåº¦æå‡':<10} {'è¨˜æ†¶é«”ç¯€çœ':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        try:\n",
    "            # å‰µå»ºæ¸¬è©¦æ•¸æ“š\n",
    "            test_input = torch.randint(0, vocab_size, (2, seq_len), device=device)\n",
    "            \n",
    "            # æ¸¬è©¦æ¨™æº–æ¨¡å‹\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            std_time, std_mem, _ = benchmark_model(gpt2_model, test_input, f\"æ¨™æº–-{seq_len}\", num_runs=3)\n",
    "            \n",
    "            # æ¸¬è©¦ FlashAttention æ¨¡å‹\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            flash_time, flash_mem, _ = benchmark_model(gpt2_flash, test_input, f\"Flash-{seq_len}\", num_runs=3)\n",
    "            \n",
    "            # è¨ˆç®—æ”¹é€²\n",
    "            speed_up = std_time / flash_time if flash_time > 0 else 0\n",
    "            mem_save = (std_mem - flash_mem) / std_mem * 100 if std_mem > 0 else 0\n",
    "            \n",
    "            print(f\"{seq_len:<10} {std_time:<12.4f} {flash_time:<12.4f} {speed_up:<10.2f}x {mem_save:<10.1f}%\")\n",
    "            \n",
    "            results.append({\n",
    "                'seq_len': seq_len,\n",
    "                'std_time': std_time,\n",
    "                'flash_time': flash_time,\n",
    "                'speedup': speed_up,\n",
    "                'memory_save': mem_save\n",
    "            })\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"{seq_len:<10} OOM - è¨˜æ†¶é«”ä¸è¶³\")\n",
    "            if \"out of memory\" in str(e):\n",
    "                break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# åŸ·è¡Œåºåˆ—é•·åº¦åˆ†æ\n",
    "if torch.cuda.is_available():\n",
    "    seq_results = analyze_sequence_lengths()\n",
    "else:\n",
    "    print(\"CPU æ¨¡å¼ä¸‹è·³éåºåˆ—é•·åº¦åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯¦é©—ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FlashAttention æ•´åˆå¯¦é©—ç¸½çµ\n",
      "======================================================================\n",
      "\n",
      "ğŸ”§ æŠ€è¡“å¯¦ç¾:\n",
      "  âœ“ æˆåŠŸæ•´åˆ FlashAttention åˆ° GPT-2\n",
      "  âœ“ è§£æ±º dtype å…¼å®¹æ€§å•é¡Œ (fp32 â†” fp16)\n",
      "  âœ“ æ­£ç¢ºè™•ç†æ¬Šé‡çŸ©é™£è½‰ç½®\n",
      "  âœ“ å¯¦ç¾å› æœæ³¨æ„åŠ›æ©Ÿåˆ¶\n",
      "\n",
      "ğŸ“Š æ€§èƒ½æ”¹é€²:\n",
      "  âš¡ æ¨ç†é€Ÿåº¦: 1.26x æå‡\n",
      "  ğŸ§  è¨˜æ†¶é«”ä½¿ç”¨: -47.0% è®ŠåŒ–\n",
      "  â±ï¸  æ™‚é–“ç¯€çœ: 16.6 ms\n",
      "\n",
      "ğŸ¯ å“è³ªé©—è­‰:\n",
      "  âš ï¸  è¼¸å‡ºä¸€è‡´æ€§: éœ€è¦èª¿æ•´ (æœ€å¤§å·®ç•°: 0.198700)\n",
      "\n",
      "ğŸ’¡ é—œéµå­¸ç¿’é»:\n",
      "  â€¢ FlashAttention éœ€è¦ fp16/bf16 è¼¸å…¥\n",
      "  â€¢ æ¬Šé‡çŸ©é™£å¯èƒ½éœ€è¦è½‰ç½®ä»¥åŒ¹é…æ ¼å¼\n",
      "  â€¢ æ··åˆç²¾åº¦è™•ç†å°æ€§èƒ½å’Œæº–ç¢ºæ€§éƒ½å¾ˆé‡è¦\n",
      "  â€¢ åºåˆ—é•·åº¦è¶Šé•·ï¼ŒFlashAttention å„ªå‹¢è¶Šæ˜é¡¯\n",
      "\n",
      "ğŸ“ˆ å»ºè­°:\n",
      "  â€¢ åœ¨é•·åºåˆ— (>512) ä»»å‹™ä¸­å„ªå…ˆä½¿ç”¨ FlashAttention\n",
      "  â€¢ ç”Ÿç”¢ç’°å¢ƒä¸­å¯è€ƒæ…® bf16 ä»¥ç²å¾—æ›´å¥½çš„æ•¸å€¼ç©©å®šæ€§\n",
      "  â€¢ å¤§æ‰¹æ¬¡è¨“ç·´æ™‚è¨˜æ†¶é«”ç¯€çœæ•ˆæœæ›´é¡¯è‘—\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FlashAttention æ•´åˆå¯¦é©—ç¸½çµ\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ æŠ€è¡“å¯¦ç¾:\")\n",
    "print(f\"  âœ“ æˆåŠŸæ•´åˆ FlashAttention åˆ° GPT-2\")\n",
    "print(f\"  âœ“ è§£æ±º dtype å…¼å®¹æ€§å•é¡Œ (fp32 â†” fp16)\")\n",
    "print(f\"  âœ“ æ­£ç¢ºè™•ç†æ¬Šé‡çŸ©é™£è½‰ç½®\")\n",
    "print(f\"  âœ“ å¯¦ç¾å› æœæ³¨æ„åŠ›æ©Ÿåˆ¶\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ€§èƒ½æ”¹é€²:\")\n",
    "print(f\"  âš¡ æ¨ç†é€Ÿåº¦: {speed_improvement:.2f}x æå‡\")\n",
    "print(f\"  ğŸ§  è¨˜æ†¶é«”ä½¿ç”¨: {memory_reduction:+.1f}% è®ŠåŒ–\")\n",
    "print(f\"  â±ï¸  æ™‚é–“ç¯€çœ: {(std_time - flash_time)*1000:.1f} ms\")\n",
    "\n",
    "print(f\"\\nğŸ¯ å“è³ªé©—è­‰:\")\n",
    "if is_consistent:\n",
    "    print(f\"  âœ… è¼¸å‡ºä¸€è‡´æ€§: é€šé (æœ€å¤§å·®ç•°: {max_diff:.6f})\")\n",
    "else:\n",
    "    print(f\"  âš ï¸  è¼¸å‡ºä¸€è‡´æ€§: éœ€è¦èª¿æ•´ (æœ€å¤§å·®ç•°: {max_diff:.6f})\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ é—œéµå­¸ç¿’é»:\")\n",
    "print(f\"  â€¢ FlashAttention éœ€è¦ fp16/bf16 è¼¸å…¥\")\n",
    "print(f\"  â€¢ æ¬Šé‡çŸ©é™£å¯èƒ½éœ€è¦è½‰ç½®ä»¥åŒ¹é…æ ¼å¼\")\n",
    "print(f\"  â€¢ æ··åˆç²¾åº¦è™•ç†å°æ€§èƒ½å’Œæº–ç¢ºæ€§éƒ½å¾ˆé‡è¦\")\n",
    "print(f\"  â€¢ åºåˆ—é•·åº¦è¶Šé•·ï¼ŒFlashAttention å„ªå‹¢è¶Šæ˜é¡¯\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nğŸ“ˆ å»ºè­°:\")\n",
    "    print(f\"  â€¢ åœ¨é•·åºåˆ— (>512) ä»»å‹™ä¸­å„ªå…ˆä½¿ç”¨ FlashAttention\")\n",
    "    print(f\"  â€¢ ç”Ÿç”¢ç’°å¢ƒä¸­å¯è€ƒæ…® bf16 ä»¥ç²å¾—æ›´å¥½çš„æ•¸å€¼ç©©å®šæ€§\")\n",
    "    print(f\"  â€¢ å¤§æ‰¹æ¬¡è¨“ç·´æ™‚è¨˜æ†¶é«”ç¯€çœæ•ˆæœæ›´é¡¯è‘—\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
