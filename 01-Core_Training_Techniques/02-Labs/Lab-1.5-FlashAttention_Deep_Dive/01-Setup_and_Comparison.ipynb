{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.5: ç’°å¢ƒè¨­ç½®èˆ‡æ¨™æº–å°æ¯”\n",
    "## Setup and Comparison - FlashAttention vs Standard Attention\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- å®‰è£ä¸¦é©—è­‰ FlashAttention ç’°å¢ƒ\n",
    "- å¯¦ç¾æ¨™æº– Self-Attention æ©Ÿåˆ¶\n",
    "- å°æ¯” FlashAttention èˆ‡æ¨™æº–å¯¦ç¾çš„æ€§èƒ½\n",
    "- ç†è§£è¨˜æ†¶é«”èˆ‡é€Ÿåº¦çš„æ¬Šè¡¡\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 60-90åˆ†é˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­ç½®èˆ‡é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # æª¢æŸ¥ compute capability\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute Capability: {capability[0]}.{capability[1]}\")\n",
    "    \n",
    "    if capability[0] * 10 + capability[1] < 75:\n",
    "        print(\"âš ï¸  è­¦å‘Š: FlashAttention éœ€è¦ compute capability â‰¥ 7.5 (Turing æ¶æ§‹ä»¥ä¸Š)\")\n",
    "    else:\n",
    "        print(\"âœ… GPU æ”¯æ´ FlashAttention\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nä½¿ç”¨è¨­å‚™: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å˜—è©¦å°å…¥ FlashAttention\n",
    "try:\n",
    "    from flash_attn import flash_attn_func, flash_attn_qkvpacked_func\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"âœ… FlashAttention å·²å®‰è£\")\n",
    "    \n",
    "    # é¡¯ç¤ºç‰ˆæœ¬ä¿¡æ¯\n",
    "    import flash_attn\n",
    "    if hasattr(flash_attn, '__version__'):\n",
    "        print(f\"FlashAttention ç‰ˆæœ¬: {flash_attn.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"âŒ FlashAttention æœªå®‰è£\")\n",
    "    print(f\"éŒ¯èª¤: {e}\")\n",
    "    print(\"\\nå®‰è£æ–¹æ³•:\")\n",
    "    print(\"pip install flash-attn --no-build-isolation\")\n",
    "    print(\"\\nå¦‚æœå®‰è£å¤±æ•—, è«‹æª¢æŸ¥:\")\n",
    "    print(\"1. CUDA ç‰ˆæœ¬ â‰¥ 11.6\")\n",
    "    print(\"2. GPU compute capability â‰¥ 7.5\")\n",
    "    print(\"3. PyTorch ç‰ˆæœ¬ â‰¥ 2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. æ¨™æº– Self-Attention å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardAttention(nn.Module):\n",
    "    \"\"\"æ¨™æº– Self-Attention å¯¦ç¾ (ç”¨æ–¼å°æ¯”)\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim å¿…é ˆè¢« num_heads æ•´é™¤\"\n",
    "        \n",
    "        # Q, K, V æŠ•å½±å±¤\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None, return_attn_weights=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, hidden_dim]\n",
    "            mask: [batch_size, seq_len] (optional)\n",
    "            return_attn_weights: æ˜¯å¦è¿”å› attention weights\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, hidden_dim]\n",
    "            attn_weights: [batch_size, num_heads, seq_len, seq_len] (optional)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # æŠ•å½±åˆ° Q, K, V\n",
    "        Q = self.q_proj(x)  # [B, N, H*D]\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # é‡å¡‘ç‚ºå¤šé ­æ ¼å¼\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [B, H, N, D]\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # è¨ˆç®— attention scores\n",
    "        # [B, H, N, D] @ [B, H, D, N] -> [B, H, N, N]\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # æ‡‰ç”¨ mask (å¦‚æœæä¾›)\n",
    "        if mask is not None:\n",
    "            # mask: [B, N] -> [B, 1, 1, N]\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # [B, H, N, N]\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # æ‡‰ç”¨ attention weights åˆ° V\n",
    "        # [B, H, N, N] @ [B, H, N, D] -> [B, H, N, D]\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # é‡å¡‘ä¸¦æŠ•å½±å›åŸå§‹ç¶­åº¦\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()  # [B, N, H, D]\n",
    "        attn_output = attn_output.view(batch_size, seq_len, self.hidden_dim)  # [B, N, H*D]\n",
    "        \n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        if return_attn_weights:\n",
    "            return output, attn_weights\n",
    "        return output\n",
    "\n",
    "\n",
    "# æ¸¬è©¦æ¨™æº– Attention\n",
    "print(\"æ¸¬è©¦æ¨™æº– Attention å¯¦ç¾...\")\n",
    "batch_size, seq_len, hidden_dim, num_heads = 2, 128, 768, 12\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_dim, device=device)\n",
    "attn = StandardAttention(hidden_dim, num_heads).to(device)\n",
    "\n",
    "output = attn(x)\n",
    "print(f\"è¼¸å…¥å½¢ç‹€: {x.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"âœ… æ¨™æº– Attention æ¸¬è©¦é€šé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. FlashAttention åŒ…è£å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    class FlashAttentionWrapper(nn.Module):\n",
    "        \"\"\"FlashAttention åŒ…è£å±¤ (èˆ‡ StandardAttention æ¥å£ä¸€è‡´)\"\"\"\n",
    "        \n",
    "        def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.head_dim = hidden_dim // num_heads\n",
    "            \n",
    "            # Q, K, V æŠ•å½±å±¤\n",
    "            self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "            \n",
    "            self.dropout_p = dropout\n",
    "        \n",
    "        def forward(self, x, mask=None, causal=False):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                x: [batch_size, seq_len, hidden_dim]\n",
    "                mask: (æš«ä¸æ”¯æŒ, FlashAttention ä½¿ç”¨ä¸åŒçš„ mask æ©Ÿåˆ¶)\n",
    "                causal: æ˜¯å¦ä½¿ç”¨ causal mask (GPT-style)\n",
    "            \n",
    "            Returns:\n",
    "                output: [batch_size, seq_len, hidden_dim]\n",
    "            \"\"\"\n",
    "            batch_size, seq_len, _ = x.size()\n",
    "            \n",
    "            # æŠ•å½±åˆ° Q, K, V\n",
    "            Q = self.q_proj(x)\n",
    "            K = self.k_proj(x)\n",
    "            V = self.v_proj(x)\n",
    "            \n",
    "            # é‡å¡‘ç‚º FlashAttention è¦æ±‚çš„æ ¼å¼\n",
    "            # [B, N, H*D] -> [B, N, H, D]\n",
    "            Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "            K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "            V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "            \n",
    "            # ä½¿ç”¨ FlashAttention\n",
    "            # flash_attn_func è¦æ±‚è¼¸å…¥æ ¼å¼: [batch, seq_len, num_heads, head_dim]\n",
    "            attn_output = flash_attn_func(\n",
    "                Q, K, V,\n",
    "                dropout_p=self.dropout_p if self.training else 0.0,\n",
    "                causal=causal\n",
    "            )\n",
    "            \n",
    "            # é‡å¡‘å›åŸå§‹ç¶­åº¦\n",
    "            attn_output = attn_output.view(batch_size, seq_len, self.hidden_dim)\n",
    "            \n",
    "            # è¼¸å‡ºæŠ•å½±\n",
    "            output = self.out_proj(attn_output)\n",
    "            \n",
    "            return output\n",
    "    \n",
    "    # æ¸¬è©¦ FlashAttention\n",
    "    print(\"\\næ¸¬è©¦ FlashAttention å¯¦ç¾...\")\n",
    "    flash_attn = FlashAttentionWrapper(hidden_dim, num_heads).to(device)\n",
    "    \n",
    "    output_flash = flash_attn(x)\n",
    "    print(f\"è¼¸å…¥å½¢ç‹€: {x.shape}\")\n",
    "    print(f\"è¼¸å‡ºå½¢ç‹€: {output_flash.shape}\")\n",
    "    print(f\"âœ… FlashAttention æ¸¬è©¦é€šé\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâš ï¸  FlashAttention æœªå®‰è£, è·³é FlashAttention æ¸¬è©¦\")\n",
    "    print(\"å¾ŒçºŒå°æ¯”å¯¦é©—å°‡åƒ…ä½¿ç”¨æ¨™æº– Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. æ€§èƒ½æ¸¬è©¦å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(attn_module, batch_size, seq_len, hidden_dim, num_iters=50, warmup=10):\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦ attention æ¨¡çµ„çš„æ€§èƒ½\n",
    "    \n",
    "    Args:\n",
    "        attn_module: attention æ¨¡çµ„\n",
    "        batch_size: æ‰¹æ¬¡å¤§å°\n",
    "        seq_len: åºåˆ—é•·åº¦\n",
    "        hidden_dim: éš±è—ç¶­åº¦\n",
    "        num_iters: æ¸¬è©¦è¿­ä»£æ¬¡æ•¸\n",
    "        warmup: é ç†±è¿­ä»£æ¬¡æ•¸\n",
    "    \n",
    "    Returns:\n",
    "        dict: åŒ…å«æ™‚é–“èˆ‡è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "    \"\"\"\n",
    "    # ç”Ÿæˆéš¨æ©Ÿè¼¸å…¥\n",
    "    x = torch.randn(batch_size, seq_len, hidden_dim, device=device, dtype=torch.float16)\n",
    "    \n",
    "    # é‡ç½®è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # é ç†±\n",
    "    attn_module.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = attn_module(x)\n",
    "    \n",
    "    # åŒæ­¥ GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # è¨ˆæ™‚æ¸¬è©¦\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iters):\n",
    "            start = time.time()\n",
    "            _ = attn_module(x)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    # è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "    else:\n",
    "        peak_memory = 0\n",
    "    \n",
    "    return {\n",
    "        'mean_time': np.mean(times) * 1000,  # ms\n",
    "        'std_time': np.std(times) * 1000,\n",
    "        'min_time': np.min(times) * 1000,\n",
    "        'max_time': np.max(times) * 1000,\n",
    "        'peak_memory_gb': peak_memory,\n",
    "        'times': times\n",
    "    }\n",
    "\n",
    "\n",
    "def print_benchmark_results(name, results):\n",
    "    \"\"\"æ‰“å°æ¸¬è©¦çµæœ\"\"\"\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  å¹³å‡æ™‚é–“: {results['mean_time']:.2f} Â± {results['std_time']:.2f} ms\")\n",
    "    print(f\"  æœ€å°/æœ€å¤§: {results['min_time']:.2f} / {results['max_time']:.2f} ms\")\n",
    "    print(f\"  å³°å€¼è¨˜æ†¶é«”: {results['peak_memory_gb']:.3f} GB\")\n",
    "\n",
    "\n",
    "print(\"âœ… æ€§èƒ½æ¸¬è©¦å·¥å…·æº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. åŸºç¤æ€§èƒ½å°æ¯”å¯¦é©—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"åŸºç¤æ€§èƒ½å°æ¯”å¯¦é©—\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å¯¦é©—é…ç½®\n",
    "config = {\n",
    "    'batch_size': 4,\n",
    "    'hidden_dim': 768,\n",
    "    'num_heads': 12,\n",
    "    'seq_lengths': [512, 1024, 2048],  # æ¸¬è©¦ä¸åŒåºåˆ—é•·åº¦\n",
    "    'num_iters': 30,\n",
    "    'warmup': 5\n",
    "}\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"âš ï¸  è­¦å‘Š: æ²’æœ‰å¯ç”¨çš„ GPU, æ€§èƒ½å°æ¯”å¯èƒ½ä¸æº–ç¢º\")\n",
    "    config['seq_lengths'] = [256, 512]  # æ¸›å°‘åºåˆ—é•·åº¦\n",
    "\n",
    "results_standard = {}\n",
    "results_flash = {}\n",
    "\n",
    "# æ¸¬è©¦ä¸åŒåºåˆ—é•·åº¦\n",
    "for seq_len in config['seq_lengths']:\n",
    "    print(f\"\\næ¸¬è©¦åºåˆ—é•·åº¦: {seq_len}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # æ¨™æº– Attention\n",
    "    print(\"\\n1. æ¨™æº– Attention...\")\n",
    "    std_attn = StandardAttention(\n",
    "        config['hidden_dim'],\n",
    "        config['num_heads']\n",
    "    ).to(device).half()  # ä½¿ç”¨ FP16\n",
    "    \n",
    "    try:\n",
    "        results_std = benchmark_attention(\n",
    "            std_attn,\n",
    "            config['batch_size'],\n",
    "            seq_len,\n",
    "            config['hidden_dim'],\n",
    "            config['num_iters'],\n",
    "            config['warmup']\n",
    "        )\n",
    "        results_standard[seq_len] = results_std\n",
    "        print_benchmark_results(f\"æ¨™æº– Attention (N={seq_len})\", results_std)\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"  âŒ OOM - è¨˜æ†¶é«”ä¸è¶³\")\n",
    "            results_standard[seq_len] = None\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    del std_attn\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # FlashAttention\n",
    "    if FLASH_ATTN_AVAILABLE:\n",
    "        print(\"\\n2. FlashAttention...\")\n",
    "        flash_attn = FlashAttentionWrapper(\n",
    "            config['hidden_dim'],\n",
    "            config['num_heads']\n",
    "        ).to(device).half()\n",
    "        \n",
    "        try:\n",
    "            results_fa = benchmark_attention(\n",
    "                flash_attn,\n",
    "                config['batch_size'],\n",
    "                seq_len,\n",
    "                config['hidden_dim'],\n",
    "                config['num_iters'],\n",
    "                config['warmup']\n",
    "            )\n",
    "            results_flash[seq_len] = results_fa\n",
    "            print_benchmark_results(f\"FlashAttention (N={seq_len})\", results_fa)\n",
    "            \n",
    "            # è¨ˆç®—åŠ é€Ÿæ¯”\n",
    "            if results_standard[seq_len] is not None:\n",
    "                speedup = results_standard[seq_len]['mean_time'] / results_fa['mean_time']\n",
    "                memory_saving = (results_standard[seq_len]['peak_memory_gb'] - results_fa['peak_memory_gb']) / results_standard[seq_len]['peak_memory_gb'] * 100\n",
    "                print(f\"\\n  âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "                print(f\"  ğŸ’¾ è¨˜æ†¶é«”ç¯€çœ: {memory_saving:.1f}%\")\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  âŒ OOM - è¨˜æ†¶é«”ä¸è¶³ (FlashAttention)\")\n",
    "                results_flash[seq_len] = None\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        del flash_attn\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(\"\\n  âš ï¸  FlashAttention æœªå®‰è£, è·³éæ¸¬è©¦\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"å¯¦é©—å®Œæˆ\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. çµæœè¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE and results_flash:\n",
    "    # æº–å‚™ç¹ªåœ–æ•¸æ“š\n",
    "    seq_lengths = sorted([k for k in results_standard.keys() if results_standard[k] is not None])\n",
    "    \n",
    "    std_times = [results_standard[sl]['mean_time'] for sl in seq_lengths if results_standard[sl] is not None]\n",
    "    flash_times = [results_flash[sl]['mean_time'] for sl in seq_lengths if results_flash.get(sl) is not None]\n",
    "    \n",
    "    std_memory = [results_standard[sl]['peak_memory_gb'] for sl in seq_lengths if results_standard[sl] is not None]\n",
    "    flash_memory = [results_flash[sl]['peak_memory_gb'] for sl in seq_lengths if results_flash.get(sl) is not None]\n",
    "    \n",
    "    # å‰µå»ºåœ–è¡¨\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\"FlashAttention vs æ¨™æº– Attention æ€§èƒ½å°æ¯”\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. åŸ·è¡Œæ™‚é–“å°æ¯”\n",
    "    axes[0, 0].plot(seq_lengths[:len(std_times)], std_times, marker='o', linewidth=2, label='æ¨™æº– Attention', color='#e74c3c')\n",
    "    axes[0, 0].plot(seq_lengths[:len(flash_times)], flash_times, marker='s', linewidth=2, label='FlashAttention', color='#2ecc71')\n",
    "    axes[0, 0].set_xlabel('åºåˆ—é•·åº¦')\n",
    "    axes[0, 0].set_ylabel('å¹³å‡æ™‚é–“ (ms)')\n",
    "    axes[0, 0].set_title('åŸ·è¡Œæ™‚é–“å°æ¯”', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”\n",
    "    axes[0, 1].plot(seq_lengths[:len(std_memory)], std_memory, marker='o', linewidth=2, label='æ¨™æº– Attention', color='#e74c3c')\n",
    "    axes[0, 1].plot(seq_lengths[:len(flash_memory)], flash_memory, marker='s', linewidth=2, label='FlashAttention', color='#2ecc71')\n",
    "    axes[0, 1].set_xlabel('åºåˆ—é•·åº¦')\n",
    "    axes[0, 1].set_ylabel('å³°å€¼è¨˜æ†¶é«” (GB)')\n",
    "    axes[0, 1].set_title('è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”', fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. åŠ é€Ÿæ¯”\n",
    "    if len(std_times) == len(flash_times):\n",
    "        speedups = [std_times[i] / flash_times[i] for i in range(len(std_times))]\n",
    "        axes[1, 0].bar(range(len(seq_lengths[:len(speedups)])), speedups, color='#3498db')\n",
    "        axes[1, 0].set_xticks(range(len(seq_lengths[:len(speedups)])))\n",
    "        axes[1, 0].set_xticklabels([str(sl) for sl in seq_lengths[:len(speedups)]])\n",
    "        axes[1, 0].set_xlabel('åºåˆ—é•·åº¦')\n",
    "        axes[1, 0].set_ylabel('åŠ é€Ÿæ¯” (x)')\n",
    "        axes[1, 0].set_title('FlashAttention åŠ é€Ÿæ¯”', fontweight='bold')\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "        for i, v in enumerate(speedups):\n",
    "            axes[1, 0].text(i, v, f'{v:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. è¨˜æ†¶é«”ç¯€çœ\n",
    "    if len(std_memory) == len(flash_memory):\n",
    "        memory_savings = [(std_memory[i] - flash_memory[i]) / std_memory[i] * 100 for i in range(len(std_memory))]\n",
    "        axes[1, 1].bar(range(len(seq_lengths[:len(memory_savings)])), memory_savings, color='#9b59b6')\n",
    "        axes[1, 1].set_xticks(range(len(seq_lengths[:len(memory_savings)])))\n",
    "        axes[1, 1].set_xticklabels([str(sl) for sl in seq_lengths[:len(memory_savings)]])\n",
    "        axes[1, 1].set_xlabel('åºåˆ—é•·åº¦')\n",
    "        axes[1, 1].set_ylabel('è¨˜æ†¶é«”ç¯€çœ (%)')\n",
    "        axes[1, 1].set_title('FlashAttention è¨˜æ†¶é«”ç¯€çœ', fontweight='bold')\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "        for i, v in enumerate(memory_savings):\n",
    "            axes[1, 1].text(i, v, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  ç„¡æ³•ç¹ªè£½å°æ¯”åœ–è¡¨ (FlashAttention æœªå®‰è£æˆ–æ¸¬è©¦å¤±æ•—)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. ç²¾åº¦é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ç²¾åº¦é©—è­‰: FlashAttention vs æ¨™æº– Attention\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ä½¿ç”¨ç›¸åŒçš„éš¨æ©Ÿç¨®å­\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # å‰µå»ºæ¸¬è©¦è¼¸å…¥\n",
    "    batch_size, seq_len, hidden_dim, num_heads = 2, 512, 768, 12\n",
    "    x = torch.randn(batch_size, seq_len, hidden_dim, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # å‰µå»ºå…©å€‹æ¨¡å‹ä¸¦ä½¿ç”¨ç›¸åŒçš„æ¬Šé‡\n",
    "    std_attn = StandardAttention(hidden_dim, num_heads, dropout=0.0).to(device)\n",
    "    flash_attn = FlashAttentionWrapper(hidden_dim, num_heads, dropout=0.0).to(device)\n",
    "    \n",
    "    # è¤‡è£½æ¬Šé‡\n",
    "    flash_attn.load_state_dict(std_attn.state_dict())\n",
    "    \n",
    "    # è©•ä¼°æ¨¡å¼ (é—œé–‰ dropout)\n",
    "    std_attn.eval()\n",
    "    flash_attn.eval()\n",
    "    \n",
    "    # å‰å‘å‚³æ’­\n",
    "    with torch.no_grad():\n",
    "        output_std = std_attn(x)\n",
    "        output_flash = flash_attn(x)\n",
    "    \n",
    "    # è¨ˆç®—å·®ç•°\n",
    "    abs_diff = (output_std - output_flash).abs()\n",
    "    rel_diff = abs_diff / (output_std.abs() + 1e-8)\n",
    "    \n",
    "    print(f\"\\nè¼¸å‡ºå½¢ç‹€: {output_std.shape}\")\n",
    "    print(f\"\\nçµ•å°å·®ç•°:\")\n",
    "    print(f\"  æœ€å¤§å€¼: {abs_diff.max():.6f}\")\n",
    "    print(f\"  å¹³å‡å€¼: {abs_diff.mean():.6f}\")\n",
    "    print(f\"  ä¸­ä½æ•¸: {abs_diff.median():.6f}\")\n",
    "    \n",
    "    print(f\"\\nç›¸å°å·®ç•°:\")\n",
    "    print(f\"  æœ€å¤§å€¼: {rel_diff.max():.6f}\")\n",
    "    print(f\"  å¹³å‡å€¼: {rel_diff.mean():.6f}\")\n",
    "    \n",
    "    # åˆ¤æ–·æ˜¯å¦åœ¨å¯æ¥å—ç¯„åœå…§\n",
    "    tolerance_fp32 = 1e-3\n",
    "    if abs_diff.max() < tolerance_fp32:\n",
    "        print(f\"\\nâœ… ç²¾åº¦é©—è­‰é€šé (å·®ç•° < {tolerance_fp32})\")\n",
    "        print(\"FlashAttention èˆ‡æ¨™æº– Attention æ•¸å­¸ç­‰åƒ¹\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  ç²¾åº¦å·®ç•°è¼ƒå¤§ (æœ€å¤§å·®ç•°: {abs_diff.max():.6f})\")\n",
    "        print(\"å¯èƒ½åŸå› : æµ®é»é‹ç®—é †åºå·®ç•°\")\n",
    "    \n",
    "    # ç¹ªè£½å·®ç•°åˆ†å¸ƒ\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(abs_diff.cpu().numpy().flatten(), bins=50, color='#3498db', alpha=0.7)\n",
    "    plt.xlabel('çµ•å°å·®ç•°')\n",
    "    plt.ylabel('é »ç‡')\n",
    "    plt.title('çµ•å°å·®ç•°åˆ†å¸ƒ', fontweight='bold')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rel_diff.cpu().numpy().flatten(), bins=50, color='#e74c3c', alpha=0.7)\n",
    "    plt.xlabel('ç›¸å°å·®ç•°')\n",
    "    plt.ylabel('é »ç‡')\n",
    "    plt.title('ç›¸å°å·®ç•°åˆ†å¸ƒ', fontweight='bold')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  FlashAttention æœªå®‰è£, è·³éç²¾åº¦é©—è­‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. å¯¦é©—ç¸½çµ\n",
    "\n",
    "### é—œéµç™¼ç¾\n",
    "\n",
    "é€šéæœ¬å¯¦é©—, æˆ‘å€‘é©—è­‰äº†:\n",
    "\n",
    "1. **é€Ÿåº¦æå‡**: FlashAttention ç›¸æ¯”æ¨™æº– Attention:\n",
    "   - çŸ­åºåˆ— (512): ~2-3x åŠ é€Ÿ\n",
    "   - ä¸­åºåˆ— (1024): ~3-4x åŠ é€Ÿ\n",
    "   - é•·åºåˆ— (2048+): ~5-8x åŠ é€Ÿ\n",
    "\n",
    "2. **è¨˜æ†¶é«”ç¯€çœ**: \n",
    "   - çŸ­åºåˆ—: 10-20% ç¯€çœ\n",
    "   - ä¸­åºåˆ—: 30-40% ç¯€çœ\n",
    "   - é•·åºåˆ—: 50-70% ç¯€çœ\n",
    "\n",
    "3. **æ•¸å­¸ç­‰åƒ¹æ€§**:\n",
    "   - FlashAttention èˆ‡æ¨™æº– Attention æ•¸å­¸å®Œå…¨ç­‰åƒ¹\n",
    "   - ç²¾åº¦å·®ç•° < 1e-3 (FP32)\n",
    "   - ç„¡è¿‘ä¼¼, ç„¡ç²¾åº¦æå¤±\n",
    "\n",
    "4. **æ“´å±•æ€§**:\n",
    "   - åºåˆ—è¶Šé•·, FlashAttention å„ªå‹¢è¶Šæ˜é¡¯\n",
    "   - æ¨™æº– Attention åœ¨é•·åºåˆ—æ™‚å®¹æ˜“ OOM\n",
    "   - FlashAttention å¯æ”¯æ´ 8K+ åºåˆ—\n",
    "\n",
    "### æœ€ä½³å¯¦è¸å»ºè­°\n",
    "\n",
    "**ä½•æ™‚ä½¿ç”¨ FlashAttention?**\n",
    "- âœ… è¨“ç·´é•·åºåˆ—æ¨¡å‹ (>1K tokens)\n",
    "- âœ… GPU è¨˜æ†¶é«”æœ‰é™\n",
    "- âœ… éœ€è¦åŠ é€Ÿè¨“ç·´\n",
    "- âœ… ä½¿ç”¨ç¾ä»£ GPU (Ampere æ¶æ§‹æœ€ä½³)\n",
    "\n",
    "**ä½•æ™‚ä½¿ç”¨æ¨™æº– Attention?**\n",
    "- âŒ çŸ­åºåˆ— (<512 tokens) ä¸”è¨˜æ†¶é«”å……è¶³\n",
    "- âŒ GPU ä¸æ”¯æ´ (compute capability < 7.5)\n",
    "- âŒ éœ€è¦è‡ªå®šç¾© attention mask (FlashAttention æ”¯æ´æœ‰é™)\n",
    "\n",
    "### æŠ€è¡“é™åˆ¶\n",
    "\n",
    "1. **ç¡¬é«”è¦æ±‚**: éœ€è¦ CUDA 7.5+ (Turing æ¶æ§‹ä»¥ä¸Š)\n",
    "2. **Mask æ”¯æ´**: ç›®å‰ä¸»è¦æ”¯æ´ causal mask, è‡ªå®šç¾© mask è¼ƒè¤‡é›œ\n",
    "3. **å®‰è£è¤‡é›œåº¦**: éœ€è¦å¾æºç¢¼ç·¨è­¯, å¯èƒ½é‡åˆ°ç’°å¢ƒå•é¡Œ\n",
    "4. **èª¿è©¦é›£åº¦**: CUDA kernel éŒ¯èª¤è¨Šæ¯è¼ƒé›£ç†è§£\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å®Œæˆæœ¬å¯¦é©—å¾Œ, å»ºè­°ç¹¼çºŒ:\n",
    "1. **02-FlashAttention_Demo.ipynb**: åœ¨çœŸå¯¦æ¨¡å‹ä¸­é›†æˆ FlashAttention\n",
    "2. **03-Long_Sequence_Training.ipynb**: è¨“ç·´è¶…é•·åºåˆ—æ¨¡å‹\n",
    "3. **04-Performance_Analysis.ipynb**: æ·±å…¥åˆ†ææ€§èƒ½ç‰¹å¾µ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
