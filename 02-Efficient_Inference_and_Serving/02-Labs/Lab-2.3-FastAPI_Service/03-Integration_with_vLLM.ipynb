{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.3 Part 3: Integration with vLLM\n",
    "\n",
    "## Objectives\n",
    "- Integrate vLLM with FastAPI\n",
    "- Use AsyncLLMEngine\n",
    "- Build OpenAI-compatible API\n",
    "- Optimize concurrent performance\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. vLLM + FastAPI Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_vllm.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "import json\n",
    "\n",
    "app = FastAPI(title=\"vLLM API Service\", version=\"1.0.0\")\n",
    "\n",
    "# Global vLLM engine\n",
    "llm_engine = None\n",
    "\n",
    "class CompletionRequest(BaseModel):\n",
    "    model: str = \"meta-llama/Llama-2-7b-hf\"\n",
    "    prompt: str = Field(..., min_length=1)\n",
    "    max_tokens: int = Field(default=100, ge=1, le=1000)\n",
    "    temperature: float = Field(default=0.8, ge=0.0, le=2.0)\n",
    "    top_p: float = Field(default=0.95, ge=0.0, le=1.0)\n",
    "    stream: bool = False\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    model: str = \"meta-llama/Llama-2-7b-hf\"\n",
    "    messages: List[Message]\n",
    "    max_tokens: int = Field(default=100, ge=1, le=1000)\n",
    "    temperature: float = Field(default=0.8, ge=0.0, le=2.0)\n",
    "    top_p: float = Field(default=0.95, ge=0.0, le=1.0)\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Initialize vLLM engine on startup.\"\"\"\n",
    "    global llm_engine\n",
    "    \n",
    "    print(\"Initializing vLLM engine...\")\n",
    "    \n",
    "    # Use smaller model for demo\n",
    "    llm_engine = LLM(\n",
    "        model=\"facebook/opt-125m\",\n",
    "        gpu_memory_utilization=0.3,\n",
    "        max_model_len=512,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ vLLM engine initialized\")\n",
    "\n",
    "@app.post(\"/v1/completions\")\n",
    "async def completions(request: CompletionRequest):\n",
    "    \"\"\"OpenAI-compatible completions endpoint.\"\"\"\n",
    "    try:\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=request.temperature,\n",
    "            top_p=request.top_p,\n",
    "            max_tokens=request.max_tokens,\n",
    "        )\n",
    "        \n",
    "        # Generate\n",
    "        outputs = llm_engine.generate([request.prompt], sampling_params)\n",
    "        \n",
    "        generated_text = outputs[0].outputs[0].text\n",
    "        tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "        \n",
    "        return {\n",
    "            \"id\": f\"cmpl-{int(time.time())}\",\n",
    "            \"object\": \"text_completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": request.model,\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"text\": generated_text,\n",
    "                    \"index\": 0,\n",
    "                    \"finish_reason\": \"stop\"\n",
    "                }\n",
    "            ],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(request.prompt.split()),\n",
    "                \"completion_tokens\": tokens_generated,\n",
    "                \"total_tokens\": len(request.prompt.split()) + tokens_generated\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(request: ChatCompletionRequest):\n",
    "    \"\"\"OpenAI-compatible chat completions endpoint.\"\"\"\n",
    "    # Format messages into prompt\n",
    "    prompt = \"\"\n",
    "    for msg in request.messages:\n",
    "        prompt += f\"{msg.role.capitalize()}: {msg.content}\\n\"\n",
    "    prompt += \"Assistant:\"\n",
    "    \n",
    "    # Generate\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=request.temperature,\n",
    "        top_p=request.top_p,\n",
    "        max_tokens=request.max_tokens,\n",
    "    )\n",
    "    \n",
    "    outputs = llm_engine.generate([prompt], sampling_params)\n",
    "    response_text = outputs[0].outputs[0].text\n",
    "    tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "    \n",
    "    return {\n",
    "        \"id\": f\"chatcmpl-{int(time.time())}\",\n",
    "        \"object\": \"chat.completion\",\n",
    "        \"created\": int(time.time()),\n",
    "        \"model\": request.model,\n",
    "        \"choices\": [\n",
    "            {\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response_text.strip()\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }\n",
    "        ],\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": len(prompt.split()),\n",
    "            \"completion_tokens\": tokens_generated,\n",
    "            \"total_tokens\": len(prompt.split()) + tokens_generated\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/v1/models\")\n",
    "async def list_models():\n",
    "    \"\"\"List available models.\"\"\"\n",
    "    return {\n",
    "        \"object\": \"list\",\n",
    "        \"data\": [\n",
    "            {\n",
    "                \"id\": \"facebook/opt-125m\",\n",
    "                \"object\": \"model\",\n",
    "                \"created\": int(time.time()),\n",
    "                \"owned_by\": \"vllm\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"engine\": \"vllm\",\n",
    "        \"ready\": llm_engine is not None\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI compatibility\n",
    "from openai import OpenAI\n",
    "\n",
    "# Configure client to use local vLLM server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"dummy-key\"\n",
    ")\n",
    "\n",
    "print(\"Testing OpenAI compatibility...\\n\")\n",
    "\n",
    "try:\n",
    "    # List models\n",
    "    models = client.models.list()\n",
    "    print(f\"Available models: {[m.id for m in models.data]}\\n\")\n",
    "    \n",
    "    # Chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"facebook/opt-125m\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Explain Python programming:\"}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(\"Response:\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(f\"\\nTokens: {response.usage.total_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure server is running: python app_vllm.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "‚úÖ **Completed**:\n",
    "1. Integrated vLLM with FastAPI\n",
    "2. Built OpenAI-compatible endpoints\n",
    "3. Implemented /completions and /chat/completions\n",
    "4. Tested with OpenAI client\n",
    "\n",
    "üìö **Key Features**:\n",
    "- Drop-in replacement for OpenAI API\n",
    "- vLLM backend for high performance\n",
    "- Standard response format\n",
    "- Token usage tracking\n",
    "\n",
    "‚û°Ô∏è **Next**: In `04-Monitoring_and_Deploy.ipynb`, we'll:\n",
    "- Add Prometheus metrics\n",
    "- Implement structured logging\n",
    "- Containerize with Docker\n",
    "- Deploy to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Lab 2.3 Part 3 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
