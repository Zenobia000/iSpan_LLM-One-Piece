{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.5: 性能深度分析\n",
    "## Performance Analysis - Deep Dive into FlashAttention\n",
    "\n",
    "**學習目標**:\n",
    "- 使用 PyTorch Profiler 深入分析 FlashAttention\n",
    "- 理解記憶體訪問模式與性能瓶頸\n",
    "- 優化不同場景的超參數配置\n",
    "- 建立性能預測模型\n",
    "\n",
    "**預計時間**: 45-60分鐘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. 環境設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from torch.cuda.amp import autocast\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# FlashAttention\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"✅ FlashAttention 可用\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"❌ FlashAttention 未安裝\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n使用設備: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute Capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Profiler 工具設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "def profile_attention(attn_func, Q, K, V, name=\"Attention\", num_iters=10):\n    \"\"\"\n    使用 PyTorch Profiler 分析 attention 性能\n    \n    Args:\n        attn_func: attention 函數\n        Q, K, V: 輸入張量\n        name: 函數名稱\n        num_iters: 測試次數\n    \"\"\"\n    # 預熱\n    for _ in range(3):\n        _ = attn_func(Q, K, V)\n    \n    # Profiling\n    with profile(\n        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n        record_shapes=True,\n        profile_memory=True,\n        with_flops=True\n    ) as prof:\n        with record_function(name):\n            for _ in range(num_iters):\n                _ = attn_func(Q, K, V)\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n    \n    return prof\n\n\ndef print_profiler_summary(prof, sort_by=\"cuda_time_total\", row_limit=10):\n    \"\"\"打印 Profiler 摘要\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"Profiler 摘要 (排序: {sort_by}, 前 {row_limit} 項)\")\n    print(\"=\"*80)\n    \n    # 嘗試不同的排序鍵，以兼容不同版本的 PyTorch\n    available_sort_keys = [\"cuda_time_total\", \"device_time_total\", \"self_cuda_time_total\", \"self_device_time_total\"]\n    \n    used_sort_key = sort_by\n    for key in available_sort_keys:\n        try:\n            # 測試是否支持這個排序鍵\n            table = prof.key_averages().table(sort_by=key, row_limit=1)\n            used_sort_key = key\n            break\n        except (AttributeError, KeyError):\n            continue\n    \n    try:\n        print(prof.key_averages().table(\n            sort_by=used_sort_key,\n            row_limit=row_limit\n        ))\n    except Exception as e:\n        print(f\"⚠️  無法顯示詳細表格: {e}\")\n        print(\"基本統計:\")\n        events = prof.key_averages()\n        print(f\"  總事件數: {len(events)}\")\n        for i, evt in enumerate(events[:5]):\n            print(f\"  {i+1}. {evt.key}\")\n\n\nprint(\"✅ Profiler 工具準備完成\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. 標準 Attention 性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def standard_attention(Q, K, V):\n",
    "    \"\"\"標準 Self-Attention 實現\"\"\"\n",
    "    # Q, K, V: [batch, seq_len, num_heads, head_dim]\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # 轉換為 [batch, num_heads, seq_len, head_dim]\n",
    "    Q = Q.transpose(1, 2)\n",
    "    K = K.transpose(1, 2)\n",
    "    V = V.transpose(1, 2)\n",
    "    \n",
    "    # Attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attn_weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    # 轉回 [batch, seq_len, num_heads, head_dim]\n",
    "    output = output.transpose(1, 2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"標準 Attention 性能分析\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 測試配置\n",
    "batch_size, seq_len, num_heads, head_dim = 4, 512, 12, 64\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "K = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "V = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "\n",
    "print(f\"\\n測試配置:\")\n",
    "print(f\"  Batch Size: {batch_size}\")\n",
    "print(f\"  Sequence Length: {seq_len}\")\n",
    "print(f\"  Num Heads: {num_heads}\")\n",
    "print(f\"  Head Dim: {head_dim}\")\n",
    "\n",
    "# Profile 標準 Attention\n",
    "prof_std = profile_attention(\n",
    "    standard_attention,\n",
    "    Q, K, V,\n",
    "    name=\"Standard_Attention\",\n",
    "    num_iters=10\n",
    ")\n",
    "\n",
    "print_profiler_summary(prof_std, sort_by=\"cuda_time_total\", row_limit=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. FlashAttention 性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "if FLASH_ATTN_AVAILABLE:\n    print(\"=\"*70)\n    print(\"FlashAttention 性能分析\")\n    print(\"=\"*70)\n    \n    def flash_attention_wrapper(Q, K, V):\n        \"\"\"FlashAttention 包裝 (匹配標準格式)\"\"\"\n        return flash_attn_func(Q, K, V, causal=False)\n    \n    # Profile FlashAttention\n    prof_flash = profile_attention(\n        flash_attention_wrapper,\n        Q, K, V,\n        name=\"FlashAttention\",\n        num_iters=10\n    )\n    \n    print_profiler_summary(prof_flash, sort_by=\"cuda_time_total\", row_limit=15)\n    \n    # 對比分析\n    print(\"\\n\" + \"=\"*70)\n    print(\"性能對比\")\n    print(\"=\"*70)\n    \n    std_events = prof_std.key_averages()\n    flash_events = prof_flash.key_averages()\n    \n    # 兼容不同版本的 PyTorch profiler API\n    def get_cuda_time(events):\n        total_time = 0\n        for evt in events:\n            # 嘗試不同的屬性名\n            if hasattr(evt, 'cuda_time_total'):\n                total_time += evt.cuda_time_total\n            elif hasattr(evt, 'device_time_total'):\n                total_time += evt.device_time_total\n            elif hasattr(evt, 'self_cuda_time_total'):\n                total_time += evt.self_cuda_time_total\n            elif hasattr(evt, 'self_device_time_total'):\n                total_time += evt.self_device_time_total\n        return total_time\n    \n    std_cuda_time = get_cuda_time(std_events)\n    flash_cuda_time = get_cuda_time(flash_events)\n    \n    print(f\"\\n標準 Attention CUDA 時間: {std_cuda_time / 1e3:.2f} ms\")\n    print(f\"FlashAttention CUDA 時間: {flash_cuda_time / 1e3:.2f} ms\")\n    \n    if flash_cuda_time > 0:\n        print(f\"\\n⚡ 加速比: {std_cuda_time / flash_cuda_time:.2f}x\")\n    else:\n        print(f\"\\n⚠️  無法計算加速比 (FlashAttention 時間為 0)\")\n    \nelse:\n    print(\"\\n⚠️  FlashAttention 未安裝, 跳過性能分析\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. 記憶體訪問模式分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_memory_pattern(prof, name=\"Model\"):\n    \"\"\"分析記憶體訪問模式\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(f\"{name} 記憶體分析\")\n    print(\"=\"*70)\n    \n    events = prof.key_averages()\n    \n    # 兼容不同版本的 PyTorch profiler API\n    def get_memory_usage(evt):\n        cpu_mem = 0\n        cuda_mem = 0\n        \n        # 嘗試不同的屬性名\n        if hasattr(evt, 'cpu_memory_usage'):\n            cpu_mem = evt.cpu_memory_usage\n        elif hasattr(evt, 'self_cpu_memory_usage'):\n            cpu_mem = evt.self_cpu_memory_usage\n            \n        if hasattr(evt, 'cuda_memory_usage'):\n            cuda_mem = evt.cuda_memory_usage\n        elif hasattr(evt, 'self_cuda_memory_usage'):\n            cuda_mem = evt.self_cuda_memory_usage\n        elif hasattr(evt, 'device_memory_usage'):\n            cuda_mem = evt.device_memory_usage\n        elif hasattr(evt, 'self_device_memory_usage'):\n            cuda_mem = evt.self_device_memory_usage\n            \n        return cpu_mem, cuda_mem\n    \n    # 按記憶體使用排序\n    memory_events = []\n    for evt in events:\n        cpu_mem, cuda_mem = get_memory_usage(evt)\n        if cpu_mem > 0 or cuda_mem > 0:\n            memory_events.append((evt, cpu_mem, cuda_mem))\n    \n    # 按 CUDA 記憶體使用排序\n    memory_events.sort(key=lambda x: abs(x[2]), reverse=True)\n    \n    print(f\"\\n{'操作':<40} {'CPU記憶體(MB)':<18} {'CUDA記憶體(MB)':<18}\")\n    print(\"-\"*76)\n    \n    for evt, cpu_mem, cuda_mem in memory_events[:10]:\n        cpu_mem_mb = cpu_mem / 1e6\n        cuda_mem_mb = cuda_mem / 1e6\n        print(f\"{evt.key[:38]:<40} {cpu_mem_mb:>15.2f} {cuda_mem_mb:>15.2f}\")\n\n\n# 分析標準 Attention 記憶體\nanalyze_memory_pattern(prof_std, \"標準 Attention\")\n\n# 分析 FlashAttention 記憶體\nif FLASH_ATTN_AVAILABLE:\n    analyze_memory_pattern(prof_flash, \"FlashAttention\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. 不同序列長度的性能縮放分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"序列長度縮放分析\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    seq_lengths = [256, 512, 1024, 2048]\n",
    "    batch_size = 4\n",
    "    num_heads = 12\n",
    "    head_dim = 64\n",
    "    num_iters = 20\n",
    "    \n",
    "    results = {\n",
    "        \"seq_len\": [],\n",
    "        \"std_time\": [],\n",
    "        \"flash_time\": [],\n",
    "        \"std_memory\": [],\n",
    "        \"flash_memory\": []\n",
    "    }\n",
    "    \n",
    "    for seq_len in tqdm(seq_lengths, desc=\"測試不同序列長度\"):\n",
    "        print(f\"\\n測試序列長度: {seq_len}\")\n",
    "        \n",
    "        Q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "        K = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "        V = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "        \n",
    "        results[\"seq_len\"].append(seq_len)\n",
    "        \n",
    "        # 標準 Attention\n",
    "        try:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            times = []\n",
    "            for _ in range(num_iters):\n",
    "                start = time.time()\n",
    "                _ = standard_attention(Q, K, V)\n",
    "                torch.cuda.synchronize()\n",
    "                times.append(time.time() - start)\n",
    "            \n",
    "            std_time = np.mean(times) * 1000  # ms\n",
    "            std_mem = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "            \n",
    "            results[\"std_time\"].append(std_time)\n",
    "            results[\"std_memory\"].append(std_mem)\n",
    "            \n",
    "            print(f\"  標準: {std_time:.2f}ms, {std_mem:.2f}GB\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                results[\"std_time\"].append(None)\n",
    "                results[\"std_memory\"].append(None)\n",
    "                print(f\"  標準: OOM\")\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        # FlashAttention\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        times = []\n",
    "        for _ in range(num_iters):\n",
    "            start = time.time()\n",
    "            _ = flash_attn_func(Q, K, V, causal=False)\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "        \n",
    "        flash_time = np.mean(times) * 1000  # ms\n",
    "        flash_mem = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "        \n",
    "        results[\"flash_time\"].append(flash_time)\n",
    "        results[\"flash_memory\"].append(flash_mem)\n",
    "        \n",
    "        print(f\"  Flash: {flash_time:.2f}ms, {flash_mem:.2f}GB\")\n",
    "    \n",
    "    # 轉換為 DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"縮放性能表\")\n",
    "    print(\"=\"*70)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  FlashAttention 未安裝, 跳過縮放分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. 性能縮放視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE and 'results' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\"FlashAttention 性能縮放分析\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    seq_lens = results[\"seq_len\"]\n",
    "    \n",
    "    # 1. 執行時間縮放\n",
    "    std_times = [t for t in results[\"std_time\"] if t is not None]\n",
    "    flash_times = results[\"flash_time\"]\n",
    "    valid_seq_lens = seq_lens[:len(std_times)]\n",
    "    \n",
    "    axes[0, 0].plot(valid_seq_lens, std_times, 'o-', linewidth=2, markersize=8, label='標準 Attention', color='#e74c3c')\n",
    "    axes[0, 0].plot(seq_lens, flash_times, 's-', linewidth=2, markersize=8, label='FlashAttention', color='#2ecc71')\n",
    "    axes[0, 0].set_xlabel('序列長度')\n",
    "    axes[0, 0].set_ylabel('執行時間 (ms)')\n",
    "    axes[0, 0].set_title('執行時間 vs 序列長度', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # 2. 記憶體縮放\n",
    "    std_mems = [m for m in results[\"std_memory\"] if m is not None]\n",
    "    flash_mems = results[\"flash_memory\"]\n",
    "    \n",
    "    axes[0, 1].plot(valid_seq_lens, std_mems, 'o-', linewidth=2, markersize=8, label='標準 Attention', color='#e74c3c')\n",
    "    axes[0, 1].plot(seq_lens, flash_mems, 's-', linewidth=2, markersize=8, label='FlashAttention', color='#2ecc71')\n",
    "    axes[0, 1].set_xlabel('序列長度')\n",
    "    axes[0, 1].set_ylabel('峰值記憶體 (GB)')\n",
    "    axes[0, 1].set_title('記憶體使用 vs 序列長度', fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. 加速比\n",
    "    speedups = [std_times[i] / flash_times[i] for i in range(len(std_times))]\n",
    "    axes[1, 0].bar(range(len(valid_seq_lens)), speedups, color='#3498db')\n",
    "    axes[1, 0].set_xticks(range(len(valid_seq_lens)))\n",
    "    axes[1, 0].set_xticklabels([str(sl) for sl in valid_seq_lens])\n",
    "    axes[1, 0].set_xlabel('序列長度')\n",
    "    axes[1, 0].set_ylabel('加速比 (x)')\n",
    "    axes[1, 0].set_title('FlashAttention 加速比', fontweight='bold')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 添加數值標籤\n",
    "    for i, v in enumerate(speedups):\n",
    "        axes[1, 0].text(i, v, f'{v:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. 複雜度分析 (Log-Log plot)\n",
    "    axes[1, 1].loglog(seq_lens, flash_times, 's-', linewidth=2, markersize=8, label='FlashAttention 實際', color='#2ecc71')\n",
    "    \n",
    "    # 擬合 O(N²) 曲線\n",
    "    seq_lens_arr = np.array(seq_lens)\n",
    "    flash_times_arr = np.array(flash_times)\n",
    "    \n",
    "    # y = a*N^2\n",
    "    a = np.mean(flash_times_arr / (seq_lens_arr ** 2))\n",
    "    theoretical = a * (seq_lens_arr ** 2)\n",
    "    \n",
    "    axes[1, 1].loglog(seq_lens, theoretical, '--', linewidth=2, label='O(N²) 理論', color='#e74c3c', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('序列長度 (log scale)')\n",
    "    axes[1, 1].set_ylabel('執行時間 (ms, log scale)')\n",
    "    axes[1, 1].set_title('時間複雜度分析 (Log-Log)', fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3, which='both')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  無法繪製圖表 - FlashAttention 未安裝或無測試結果\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. 批次大小影響分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"批次大小影響分析\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    seq_len = 1024\n",
    "    num_heads = 12\n",
    "    head_dim = 64\n",
    "    batch_sizes = [1, 2, 4, 8, 16]\n",
    "    \n",
    "    batch_results = {\n",
    "        \"batch_size\": [],\n",
    "        \"time_ms\": [],\n",
    "        \"memory_gb\": [],\n",
    "        \"throughput\": []  # tokens/sec\n",
    "    }\n",
    "    \n",
    "    for bs in tqdm(batch_sizes, desc=\"測試不同批次大小\"):\n",
    "        try:\n",
    "            Q = torch.randn(bs, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "            K = torch.randn(bs, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "            V = torch.randn(bs, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "            \n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # 測試時間\n",
    "            times = []\n",
    "            for _ in range(20):\n",
    "                start = time.time()\n",
    "                _ = flash_attn_func(Q, K, V, causal=False)\n",
    "                torch.cuda.synchronize()\n",
    "                times.append(time.time() - start)\n",
    "            \n",
    "            avg_time = np.mean(times) * 1000  # ms\n",
    "            peak_mem = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "            throughput = (bs * seq_len) / (avg_time / 1000)  # tokens/sec\n",
    "            \n",
    "            batch_results[\"batch_size\"].append(bs)\n",
    "            batch_results[\"time_ms\"].append(avg_time)\n",
    "            batch_results[\"memory_gb\"].append(peak_mem)\n",
    "            batch_results[\"throughput\"].append(throughput)\n",
    "            \n",
    "            print(f\"  Batch={bs}: {avg_time:.2f}ms, {peak_mem:.2f}GB, {throughput:.0f} tokens/s\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  Batch={bs}: OOM\")\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # 視覺化\n",
    "    if batch_results[\"batch_size\"]:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        fig.suptitle(\"批次大小影響分析\", fontsize=14, fontweight='bold')\n",
    "        \n",
    "        bs_list = batch_results[\"batch_size\"]\n",
    "        \n",
    "        # 時間\n",
    "        axes[0].plot(bs_list, batch_results[\"time_ms\"], 'o-', linewidth=2, markersize=8, color='#3498db')\n",
    "        axes[0].set_xlabel('批次大小')\n",
    "        axes[0].set_ylabel('執行時間 (ms)')\n",
    "        axes[0].set_title('時間 vs 批次大小', fontweight='bold')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # 記憶體\n",
    "        axes[1].plot(bs_list, batch_results[\"memory_gb\"], 's-', linewidth=2, markersize=8, color='#e74c3c')\n",
    "        axes[1].set_xlabel('批次大小')\n",
    "        axes[1].set_ylabel('峰值記憶體 (GB)')\n",
    "        axes[1].set_title('記憶體 vs 批次大小', fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        # 吞吐量\n",
    "        axes[2].plot(bs_list, batch_results[\"throughput\"], '^-', linewidth=2, markersize=8, color='#2ecc71')\n",
    "        axes[2].set_xlabel('批次大小')\n",
    "        axes[2].set_ylabel('吞吐量 (tokens/sec)')\n",
    "        axes[2].set_title('吞吐量 vs 批次大小', fontweight='bold')\n",
    "        axes[2].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  FlashAttention 未安裝, 跳過批次大小分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. 最佳配置建議生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimal_config(gpu_memory_gb, seq_length, model_size=\"medium\"):\n",
    "    \"\"\"\n",
    "    根據硬體配置生成最佳訓練配置\n",
    "    \n",
    "    Args:\n",
    "        gpu_memory_gb: GPU 記憶體 (GB)\n",
    "        seq_length: 目標序列長度\n",
    "        model_size: 模型大小 (small, medium, large)\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"FlashAttention 最佳配置建議\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n輸入配置:\")\n",
    "    print(f\"  GPU 記憶體: {gpu_memory_gb} GB\")\n",
    "    print(f\"  序列長度: {seq_length}\")\n",
    "    print(f\"  模型大小: {model_size}\")\n",
    "    \n",
    "    # 基於序列長度推薦批次大小\n",
    "    if seq_length <= 512:\n",
    "        recommended_bs = min(16, int(gpu_memory_gb // 2))\n",
    "    elif seq_length <= 1024:\n",
    "        recommended_bs = min(8, int(gpu_memory_gb // 3))\n",
    "    elif seq_length <= 2048:\n",
    "        recommended_bs = min(4, int(gpu_memory_gb // 4))\n",
    "    elif seq_length <= 4096:\n",
    "        recommended_bs = min(2, int(gpu_memory_gb // 8))\n",
    "    else:\n",
    "        recommended_bs = 1\n",
    "    \n",
    "    # 梯度累積建議\n",
    "    effective_bs = 32  # 目標有效批次\n",
    "    accumulation_steps = max(1, effective_bs // recommended_bs)\n",
    "    \n",
    "    # 其他優化建議\n",
    "    use_amp = True\n",
    "    use_grad_checkpoint = seq_length > 1024 or model_size == \"large\"\n",
    "    use_flash_attn = True\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"推薦配置\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n核心配置:\")\n",
    "    print(f\"  批次大小: {recommended_bs}\")\n",
    "    print(f\"  梯度累積步數: {accumulation_steps}\")\n",
    "    print(f\"  有效批次大小: {recommended_bs * accumulation_steps}\")\n",
    "    \n",
    "    print(f\"\\n優化技術:\")\n",
    "    print(f\"  混合精度訓練: {'✅ 啟用' if use_amp else '❌ 關閉'}\")\n",
    "    print(f\"  梯度檢查點: {'✅ 啟用' if use_grad_checkpoint else '❌ 關閉'}\")\n",
    "    print(f\"  FlashAttention: {'✅ 啟用' if use_flash_attn else '❌ 關閉'}\")\n",
    "    \n",
    "    # 代碼範例\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"配置代碼範例\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    code = f\"\"\"\n",
    "# 訓練配置\n",
    "config = {{\n",
    "    'batch_size': {recommended_bs},\n",
    "    'accumulation_steps': {accumulation_steps},\n",
    "    'seq_length': {seq_length},\n",
    "    'use_amp': {use_amp},\n",
    "    'use_grad_checkpoint': {use_grad_checkpoint},\n",
    "}}\n",
    "\n",
    "# 模型配置\n",
    "model = create_model(seq_length={seq_length})\n",
    "if config['use_grad_checkpoint']:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# 訓練循環\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler() if config['use_amp'] else None\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    if step % config['accumulation_steps'] == 0:\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    if config['use_amp']:\n",
    "        with autocast(dtype=torch.float16):\n",
    "            loss = model(**batch).loss / config['accumulation_steps']\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss = model(**batch).loss / config['accumulation_steps']\n",
    "        loss.backward()\n",
    "    \n",
    "    if (step + 1) % config['accumulation_steps'] == 0:\n",
    "        if config['use_amp']:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\"\"\"\n",
    "    print(code)\n",
    "    \n",
    "    # 預期性能\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"預期性能\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if seq_length <= 1024:\n",
    "        speedup_range = \"2-3x\"\n",
    "        memory_saving = \"30-40%\"\n",
    "    elif seq_length <= 2048:\n",
    "        speedup_range = \"3-5x\"\n",
    "        memory_saving = \"40-50%\"\n",
    "    else:\n",
    "        speedup_range = \"5-8x\"\n",
    "        memory_saving = \"50-70%\"\n",
    "    \n",
    "    print(f\"\\n相對於標準 Attention:\")\n",
    "    print(f\"  速度提升: {speedup_range}\")\n",
    "    print(f\"  記憶體節省: {memory_saving}\")\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    generate_optimal_config(gpu_mem, seq_length=2048, model_size=\"medium\")\n",
    "else:\n",
    "    print(\"示例配置:\")\n",
    "    generate_optimal_config(16, seq_length=2048, model_size=\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. 實驗總結\n",
    "\n",
    "### 關鍵發現\n",
    "\n",
    "通過深入的性能分析, 我們發現:\n",
    "\n",
    "1. **FlashAttention 的核心優勢**:\n",
    "   - **IO 優化**: 減少 HBM 訪問次數, 提升記憶體頻寬利用率\n",
    "   - **融合內核**: 單個 CUDA kernel 完成所有計算\n",
    "   - **線性記憶體**: O(N) 記憶體複雜度 vs 標準的 O(N²)\n",
    "   - **數學等價**: 完全等價於標準 attention, 無近似\n",
    "\n",
    "2. **性能縮放規律**:\n",
    "   - 時間複雜度: 仍為 O(N²), 但常數項更小\n",
    "   - 記憶體複雜度: O(N), 線性增長\n",
    "   - 加速比: 隨序列長度增加而提升 (2x → 8x)\n",
    "\n",
    "3. **批次大小影響**:\n",
    "   - 更大批次: 更好的 GPU 利用率, 但記憶體需求增加\n",
    "   - 吞吐量: 批次增大時接近線性提升\n",
    "   - 最佳配置: 取決於 GPU 記憶體和序列長度\n",
    "\n",
    "4. **Profiler 洞察**:\n",
    "   - 標準 Attention: 大量時間花在 matmul 和 softmax\n",
    "   - FlashAttention: 時間主要在融合 kernel\n",
    "   - 記憶體訪問: FlashAttention 大幅減少 HBM 讀寫\n",
    "\n",
    "### 最佳實踐總結\n",
    "\n",
    "#### 1. 何時使用 FlashAttention?\n",
    "\n",
    "**強烈推薦**:\n",
    "- ✅ 訓練 Transformer 模型\n",
    "- ✅ 序列長度 >512 tokens\n",
    "- ✅ GPU 記憶體受限\n",
    "- ✅ 需要加速訓練\n",
    "\n",
    "**謹慎考慮**:\n",
    "- ⚠️  短序列 (<256 tokens): 加速效果有限\n",
    "- ⚠️  需要複雜自定義 mask: FlashAttention 支援有限\n",
    "- ⚠️  舊 GPU (compute capability <7.5): 不支援\n",
    "\n",
    "#### 2. 配置優化建議\n",
    "\n",
    "```python\n",
    "# 小 GPU (8GB)\n",
    "config_8gb = {\n",
    "    'max_seq_len': 1024,\n",
    "    'batch_size': 2,\n",
    "    'accumulation': 16,\n",
    "    'use_grad_checkpoint': True\n",
    "}\n",
    "\n",
    "# 中 GPU (16GB)\n",
    "config_16gb = {\n",
    "    'max_seq_len': 2048,\n",
    "    'batch_size': 4,\n",
    "    'accumulation': 8,\n",
    "    'use_grad_checkpoint': True\n",
    "}\n",
    "\n",
    "# 大 GPU (24GB+)\n",
    "config_24gb = {\n",
    "    'max_seq_len': 4096,\n",
    "    'batch_size': 8,\n",
    "    'accumulation': 4,\n",
    "    'use_grad_checkpoint': False  # 記憶體充足時可關閉\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3. 性能調優技巧\n",
    "\n",
    "**記憶體優化**:\n",
    "1. FlashAttention (必須)\n",
    "2. 混合精度訓練 (FP16/BF16)\n",
    "3. 梯度檢查點 (長序列)\n",
    "4. 梯度累積 (小批次)\n",
    "\n",
    "**速度優化**:\n",
    "1. 增大批次大小 (記憶體允許)\n",
    "2. 使用 BF16 (Ampere 架構)\n",
    "3. 優化數據載入 (num_workers, pin_memory)\n",
    "4. 減少同步點\n",
    "\n",
    "### Profiling 工作流\n",
    "\n",
    "```python\n",
    "# 1. 基礎性能測試\n",
    "benchmark_attention(model, seq_lengths=[512, 1024, 2048])\n",
    "\n",
    "# 2. 詳細 Profiling\n",
    "with profile(activities=[ProfilerActivity.CUDA]) as prof:\n",
    "    train_step(model, batch)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n",
    "\n",
    "# 3. 記憶體分析\n",
    "torch.cuda.memory._record_memory_history()\n",
    "# ... 訓練\n",
    "torch.cuda.memory._dump_snapshot(\"memory.pickle\")\n",
    "\n",
    "# 4. 優化迭代\n",
    "# 根據 profiling 結果調整配置\n",
    "```\n",
    "\n",
    "### 進階主題\n",
    "\n",
    "1. **Flash-Decoding**: 推理優化版本\n",
    "2. **PagedAttention**: vLLM 的 KV Cache 優化\n",
    "3. **Ring Attention**: 超長序列 (100K+)\n",
    "4. **Multi-Query/Grouped-Query Attention**: 減少 KV heads\n",
    "\n",
    "### 資源與工具\n",
    "\n",
    "- [FlashAttention 論文](https://arxiv.org/abs/2205.14135)\n",
    "- [官方實現](https://github.com/Dao-AILab/flash-attention)\n",
    "- [PyTorch Profiler 文檔](https://pytorch.org/docs/stable/profiler.html)\n",
    "- [CUDA 性能優化指南](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)\n",
    "\n",
    "### 結語\n",
    "\n",
    "FlashAttention 是現代 LLM 訓練的基石技術。通過本實驗室的學習:\n",
    "- ✅ 理解了 FlashAttention 的原理與實現\n",
    "- ✅ 掌握了性能分析與優化方法\n",
    "- ✅ 學會了配置最佳訓練參數\n",
    "- ✅ 可以在實際項目中應用 FlashAttention\n",
    "\n",
    "恭喜完成 Lab-1.5 所有實驗! 🎉\n",
    "\n",
    "建議下一步:\n",
    "- **Lab-1.6**: MQA/GQA - 進一步優化推理效率\n",
    "- **Lab-1.7**: DPO Alignment - 模型對齊技術\n",
    "- **實際應用**: 在自己的 LLM 項目中集成 FlashAttention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}