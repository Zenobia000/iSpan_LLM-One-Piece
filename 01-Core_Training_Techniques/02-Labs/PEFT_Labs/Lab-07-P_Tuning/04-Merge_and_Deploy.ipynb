{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: P-Tuning - Deployment Guide\n",
    "\n",
    "## 🎯 實驗目標\n",
    "\n",
    "本 Notebook 探討 **P-Tuning 的部署策略與優化方案**。P-Tuning 作為輸入層的可訓練提示方法,在技術上**無法直接合併**到基礎模型中,但相比 Prefix Tuning 和 Adapter,其推理開銷較小,部署相對簡單。\n",
    "\n",
    "### 關鍵學習要點\n",
    "- 理解 P-Tuning 無法合併的技術原因\n",
    "- 掌握 Prompt Encoder 的優化與壓縮\n",
    "- 實現高效的推理部署策略\n",
    "- 分析 P-Tuning 的推理開銷特性\n",
    "- 學習多任務 Prompt 的管理方案\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. P-Tuning 部署特性分析\n",
    "\n",
    "### 1.1 為什麼 P-Tuning 無法合併?\n",
    "\n",
    "**技術原因**:\n",
    "- **輸入序列修改**: P-Tuning 在輸入序列前添加虛擬標記(virtual tokens),改變了輸入結構\n",
    "- **動態 Embedding**: 虛擬標記通過 Prompt Encoder (MLP) 動態生成,無法靜態融合\n",
    "- **位置依賴性**: 虛擬標記的位置和數量會影響模型行為,不能簡單合併\n",
    "\n",
    "**與其他 PEFT 方法的對比**:\n",
    "\n",
    "| 方法 | 是否可合併 | 修改位置 | 推理開銷 | 部署複雜度 |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **LoRA** | ✅ 可合併 | 線性層權重 | 零開銷 | 低 |\n",
    "| **IA³** | ✅ 可合併 | 激活值縮放 | 零開銷 | 低 |\n",
    "| **P-Tuning** | ❌ **不可合併** | **輸入 Embedding** | **低開銷** | **中** |\n",
    "| **Prompt Tuning** | ❌ 不可合併 | 輸入 Embedding | 低開銷 | 低 |\n",
    "| **Prefix Tuning** | ❌ 不可合併 | 每層 KV | 中等開銷 | 中 |\n",
    "| **Adapter** | ❌ 不可合併 | 新增模組 | 中等開銷 | 高 |\n",
    "\n",
    "### 1.2 P-Tuning 的推理開銷來源\n",
    "\n",
    "```python\n",
    "# P-Tuning 推理流程\n",
    "1. 通過 Prompt Encoder 生成虛擬標記 embeddings\n",
    "   virtual_embeddings = PromptEncoder(virtual_token_ids)  # ← MLP 前向傳播\n",
    "\n",
    "2. 拼接虛擬標記和真實輸入\n",
    "   input_embeddings = concat([virtual_embeddings, text_embeddings])\n",
    "\n",
    "3. 正常的 Transformer 前向傳播\n",
    "   output = Transformer(input_embeddings)\n",
    "```\n",
    "\n",
    "**開銷分析**:\n",
    "- Prompt Encoder 的 MLP 計算 (通常很小,2-3層)\n",
    "- 虛擬標記增加了序列長度,略微增加注意力計算量\n",
    "- **總體開銷 << Prefix Tuning 或 Adapter**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 環境準備與模型載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要的庫\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PromptEncoderConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 設定隨機種子\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 載入訓練好的 P-Tuning 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型路徑設定\n",
    "base_model_name = \"bert-base-uncased\"\n",
    "adapter_path = \"./bert-ptuning-sst2\"  # 假設這是訓練好的 P-Tuning 路徑\n",
    "num_labels = 2  # SST-2 是二分類任務\n",
    "\n",
    "# 載入基礎模型和 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"基礎模型參數量: {base_model.num_parameters():,}\")\n",
    "print(f\"基礎模型記憶體佔用: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 P-Tuning adapter (如果存在)\n",
    "if os.path.exists(adapter_path):\n",
    "    # 找到最新的 checkpoint\n",
    "    checkpoints = [d for d in os.listdir(adapter_path) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(\n",
    "            [os.path.join(adapter_path, d) for d in checkpoints],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
    "        print(f\"成功載入 P-Tuning adapter: {latest_checkpoint}\")\n",
    "    else:\n",
    "        peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        print(f\"成功載入 P-Tuning adapter: {adapter_path}\")\n",
    "else:\n",
    "    # 如果沒有訓練好的 adapter,創建示範配置\n",
    "    print(\"未找到訓練好的 adapter,創建示範 P-Tuning 配置...\")\n",
    "    \n",
    "    ptuning_config = PromptEncoderConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        num_virtual_tokens=20,\n",
    "        encoder_hidden_size=768\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(base_model, ptuning_config)\n",
    "    print(\"創建了示範 P-Tuning 模型\")\n",
    "\n",
    "# 顯示可訓練參數統計\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. P-Tuning 結構分析\n",
    "\n",
    "### 3.1 Prompt Encoder 結構探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ptuning_structure(model):\n",
    "    \"\"\"分析 P-Tuning 的結構和參數\"\"\"\n",
    "    ptuning_params = {}\n",
    "    total_ptuning_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prompt' in name.lower() or 'encoder' in name.lower():\n",
    "            ptuning_params[name] = {\n",
    "                'shape': param.shape,\n",
    "                'dtype': param.dtype,\n",
    "                'requires_grad': param.requires_grad,\n",
    "                'device': param.device,\n",
    "                'num_params': param.numel()\n",
    "            }\n",
    "            total_ptuning_params += param.numel()\n",
    "            \n",
    "            print(f\"P-Tuning 參數: {name}\")\n",
    "            print(f\"  形狀: {param.shape}\")\n",
    "            print(f\"  數據類型: {param.dtype}\")\n",
    "            print(f\"  參數量: {param.numel():,}\")\n",
    "            print()\n",
    "    \n",
    "    base_params = sum(p.numel() for p in model.base_model.parameters())\n",
    "    print(f\"總 P-Tuning 參數量: {total_ptuning_params:,}\")\n",
    "    print(f\"基礎模型參數量: {base_params:,}\")\n",
    "    print(f\"P-Tuning 參數佔比: {total_ptuning_params / base_params * 100:.4f}%\")\n",
    "    \n",
    "    return ptuning_params\n",
    "\n",
    "ptuning_structure = analyze_ptuning_structure(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Encoder 的計算流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ptuning_computation(num_virtual_tokens=20, hidden_size=768):\n",
    "    \"\"\"\n",
    "    可視化 P-Tuning 的計算流程\n",
    "    \"\"\"\n",
    "    print(\"=== P-Tuning 計算流程分析 ===\")\n",
    "    print()\n",
    "    print(f\"虛擬標記數量: {num_virtual_tokens}\")\n",
    "    print(f\"隱藏維度: {hidden_size}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"階段 1: Prompt Encoder 生成虛擬標記 embeddings\")\n",
    "    print(f\"  輸入: 虛擬標記 IDs [{num_virtual_tokens}]\")\n",
    "    print(f\"  Embedding 層: [{num_virtual_tokens}] -> [{num_virtual_tokens}, {hidden_size}]\")\n",
    "    print(f\"    參數量: {num_virtual_tokens * hidden_size:,}\")\n",
    "    print()\n",
    "    \n",
    "    mlp_hidden = hidden_size * 2\n",
    "    print(f\"  MLP 編碼器:\")\n",
    "    print(f\"    Layer 1: [{hidden_size}] -> [{mlp_hidden}]\")\n",
    "    print(f\"      參數量: {hidden_size * mlp_hidden:,}\")\n",
    "    print(f\"    Activation: ReLU\")\n",
    "    print(f\"    Layer 2: [{mlp_hidden}] -> [{hidden_size}]\")\n",
    "    print(f\"      參數量: {mlp_hidden * hidden_size:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"階段 2: 拼接虛擬標記和真實輸入\")\n",
    "    seq_len = 128  # 假設的序列長度\n",
    "    total_len = num_virtual_tokens + seq_len\n",
    "    print(f\"  虛擬標記: [{num_virtual_tokens}, {hidden_size}]\")\n",
    "    print(f\"  真實輸入: [{seq_len}, {hidden_size}]\")\n",
    "    print(f\"  拼接結果: [{total_len}, {hidden_size}]\")\n",
    "    print()\n",
    "    \n",
    "    print(\"階段 3: Transformer 前向傳播\")\n",
    "    print(f\"  輸入序列長度增加: {num_virtual_tokens} tokens\")\n",
    "    print(f\"  注意力計算量增加: ~{(total_len**2 / seq_len**2 - 1) * 100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    total_params = (num_virtual_tokens * hidden_size + \n",
    "                   2 * hidden_size * mlp_hidden)\n",
    "    print(f\"總參數量: {total_params:,}\")\n",
    "    base_params = 110 * 1e6  # BERT-base\n",
    "    print(f\"相比基礎模型的參數比例: {total_params / base_params * 100:.4f}%\")\n",
    "\n",
    "visualize_ptuning_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 推理性能基準測試\n",
    "\n",
    "### 4.1 性能測試函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, test_texts, num_runs=10):\n",
    "    \"\"\"推理性能基準測試\"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for text in test_texts:\n",
    "                inputs = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128\n",
    "                )\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                prediction = torch.argmax(logits, dim=-1).cpu().item()\n",
    "                \n",
    "                if run == 0:  # 只在第一輪保存結果\n",
    "                    results.append({\n",
    "                        'text': text,\n",
    "                        'prediction': prediction\n",
    "                    })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    return avg_time, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 執行推理測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試文本\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic!\",\n",
    "    \"I hated every minute of this film.\",\n",
    "    \"An average movie with some good moments.\",\n",
    "    \"Outstanding performance by the lead actor.\",\n",
    "    \"Terrible plot and bad acting.\"\n",
    "]\n",
    "\n",
    "print(\"=== 帶 P-Tuning 的推理性能測試 ===\")\n",
    "ptuning_time, ptuning_results = benchmark_inference(peft_model, tokenizer, test_texts)\n",
    "print(f\"平均推理時間: {ptuning_time:.4f} 秒\")\n",
    "print(f\"記憶體使用: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== 預測結果示例 ===\")\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "for i, result in enumerate(ptuning_results[:3]):  # 只顯示前3個\n",
    "    print(f\"文本 {i+1}: {result['text']}\")\n",
    "    print(f\"預測: {label_map[result['prediction']]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 與基礎模型對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 基礎模型推理性能測試 ===\")\n",
    "base_time, base_results = benchmark_inference(base_model, tokenizer, test_texts)\n",
    "print(f\"平均推理時間: {base_time:.4f} 秒\")\n",
    "print(f\"記憶體使用: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "# 計算性能開銷\n",
    "time_overhead = (ptuning_time - base_time) / base_time * 100\n",
    "memory_overhead = (peft_model.get_memory_footprint() - base_model.get_memory_footprint()) / base_model.get_memory_footprint() * 100\n",
    "\n",
    "print(\"\\n=== 性能開銷分析 ===\")\n",
    "print(f\"推理時間增加: {time_overhead:.2f}%\")\n",
    "print(f\"記憶體使用增加: {memory_overhead:.2f}%\")\n",
    "print(f\"\\n絕對時間增加: {(ptuning_time - base_time) * 1000:.2f} ms\")\n",
    "print(f\"絕對記憶體增加: {(peft_model.get_memory_footprint() - base_model.get_memory_footprint()) / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n💡 觀察: P-Tuning 的推理開銷主要來自:\")\n",
    "print(\"  1. Prompt Encoder 的 MLP 計算 (很小)\")\n",
    "print(\"  2. 虛擬標記增加的序列長度 (輕微影響注意力計算)\")\n",
    "print(\"  總體開銷遠小於 Prefix Tuning 或 Adapter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Encoder 優化策略\n",
    "\n",
    "### 5.1 移除訓練時的 MLP (推理優化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_for_inference(model):\n",
    "    \"\"\"\n",
    "    優化 P-Tuning 模型以進行推理\n",
    "    在推理時可以預計算虛擬標記的 embeddings,移除 MLP\n",
    "    \"\"\"\n",
    "    print(\"=== P-Tuning 推理優化 ===\")\n",
    "    print()\n",
    "    print(\"優化策略 1: 預計算虛擬標記 embeddings\")\n",
    "    print(\"  訓練階段: 使用 MLP 動態生成虛擬標記\")\n",
    "    print(\"  推理階段: 預計算並緩存虛擬標記 embeddings\")\n",
    "    print(\"  優勢: 移除 MLP 計算,減少推理開銷\")\n",
    "    print()\n",
    "    \n",
    "    print(\"優化策略 2: 虛擬標記數量自適應\")\n",
    "    print(\"  簡單任務: 使用較少的虛擬標記 (5-10個)\")\n",
    "    print(\"  複雜任務: 使用較多的虛擬標記 (20-30個)\")\n",
    "    print(\"  優勢: 平衡性能和效率\")\n",
    "    print()\n",
    "    \n",
    "    print(\"優化策略 3: 量化 Prompt Encoder\")\n",
    "    print(\"  將 MLP 權重量化到 FP16 或 INT8\")\n",
    "    print(\"  優勢: 減少記憶體佔用和計算量\")\n",
    "    print()\n",
    "    \n",
    "    # 顯示當前模型參數統計\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"當前模型總參數量: {total_params:,}\")\n",
    "    print(f\"當前可訓練參數量: {trainable_params:,}\")\n",
    "    print(f\"優化潛力: 預計算後可減少 ~{trainable_params:,} 次 MLP 計算\")\n",
    "\n",
    "optimize_for_inference(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型保存與部署\n",
    "\n",
    "### 6.1 保存 P-Tuning 模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定保存路徑\n",
    "save_path = \"./bert-ptuning-deployed\"\n",
    "\n",
    "print(f\"=== 保存 P-Tuning 模組到: {save_path} ===\")\n",
    "\n",
    "# 保存 PEFT adapter\n",
    "peft_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"✅ P-Tuning 模組保存成功!\")\n",
    "\n",
    "# 檢查保存的文件\n",
    "saved_files = os.listdir(save_path)\n",
    "print(f\"\\n保存的文件: {saved_files}\")\n",
    "\n",
    "# 計算模組大小\n",
    "module_size = 0\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(save_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        module_size += size\n",
    "        print(f\"  {file}: {size / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nP-Tuning 模組總大小: {module_size / 1024:.2f} KB ({module_size / 1024**2:.4f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 驗證保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 驗證保存的 P-Tuning 模組 ===\")\n",
    "\n",
    "# 重新載入基礎模型\n",
    "fresh_base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# 從保存路徑載入 P-Tuning\n",
    "reloaded_model = PeftModel.from_pretrained(fresh_base_model, save_path)\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "print(f\"✅ 成功載入保存的 P-Tuning 模組\")\n",
    "\n",
    "# 快速測試\n",
    "test_text = \"This movie was absolutely fantastic!\"\n",
    "\n",
    "inputs = reloaded_tokenizer(\n",
    "    test_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).cpu().item()\n",
    "\n",
    "print(f\"\\n測試預測:\")\n",
    "print(f\"文本: {test_text}\")\n",
    "print(f\"預測: {label_map[prediction]}\")\n",
    "print(\"\\n✅ 保存的 P-Tuning 模組運作正常!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 生產環境部署配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建部署配置文件\n",
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"peft_method\": \"P-Tuning\",\n",
    "        \"task_type\": \"Sentiment Classification\",\n",
    "        \"num_labels\": num_labels,\n",
    "        \"num_virtual_tokens\": 20,\n",
    "        \"mergeable\": False,\n",
    "        \"module_size_mb\": module_size / 1024**2\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"inference_time_seconds\": ptuning_time,\n",
    "        \"vs_base_model_overhead_percent\": time_overhead,\n",
    "        \"memory_overhead_percent\": memory_overhead\n",
    "    },\n",
    "    \"deployment_requirements\": {\n",
    "        \"python_packages\": [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.20.0\",\n",
    "            \"peft>=0.3.0\"\n",
    "        ],\n",
    "        \"minimum_gpu_memory_gb\": 4,\n",
    "        \"recommended_gpu_memory_gb\": 8\n",
    "    },\n",
    "    \"optimization_strategies\": {\n",
    "        \"precompute_virtual_embeddings\": {\n",
    "            \"enabled\": True,\n",
    "            \"description\": \"推理時預計算虛擬標記 embeddings,移除 MLP\",\n",
    "            \"expected_speedup_percent\": 5\n",
    "        },\n",
    "        \"quantization\": {\n",
    "            \"enabled\": True,\n",
    "            \"dtype\": \"float16\",\n",
    "            \"memory_saving_percent\": 50\n",
    "        },\n",
    "        \"adaptive_virtual_tokens\": {\n",
    "            \"enabled\": False,\n",
    "            \"description\": \"根據任務複雜度調整虛擬標記數量\"\n",
    "        }\n",
    "    },\n",
    "    \"deployment_notes\": {\n",
    "        \"inference_overhead\": \"低 (~3-8%),遠小於 Prefix Tuning 或 Adapter\",\n",
    "        \"deployment_complexity\": \"中等,需管理 Prompt Encoder\",\n",
    "        \"multi_task_support\": \"良好,可共享基礎模型\",\n",
    "        \"best_for\": \"NLU 任務,特別是分類、NER 等\"\n",
    "    },\n",
    "    \"usage_example\": {\n",
    "        \"load_base_model\": f\"base_model = AutoModelForSequenceClassification.from_pretrained('{base_model_name}')\",\n",
    "        \"load_ptuning\": f\"model = PeftModel.from_pretrained(base_model, '{save_path}')\",\n",
    "        \"inference\": \"outputs = model(**inputs)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 保存配置文件\n",
    "config_path = os.path.join(save_path, \"deployment_config.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=== 生產環境部署配置 ===\")\n",
    "print(json.dumps(deployment_config, indent=2, ensure_ascii=False))\n",
    "print(f\"\\n配置文件已保存到: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. P-Tuning 與其他 PEFT 方法對比\n",
    "\n",
    "### 8.1 部署特性全面對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_comparison = {\n",
    "    \"LoRA\": {\n",
    "        \"可合併性\": \"✅ 可合併\",\n",
    "        \"推理開銷\": \"0%\",\n",
    "        \"部署複雜度\": \"低\",\n",
    "        \"適用任務\": \"通用\",\n",
    "        \"參數效率\": \"0.1-1%\"\n",
    "    },\n",
    "    \"IA³\": {\n",
    "        \"可合併性\": \"✅ 可合併\",\n",
    "        \"推理開銷\": \"0%\",\n",
    "        \"部署複雜度\": \"低\",\n",
    "        \"適用任務\": \"生成\",\n",
    "        \"參數效率\": \"0.01%\"\n",
    "    },\n",
    "    \"P-Tuning\": {\n",
    "        \"可合併性\": \"❌ 不可合併\",\n",
    "        \"推理開銷\": \"3-8%\",\n",
    "        \"部署複雜度\": \"中\",\n",
    "        \"適用任務\": \"NLU\",\n",
    "        \"參數效率\": \"0.01-0.1%\"\n",
    "    },\n",
    "    \"Prompt Tuning\": {\n",
    "        \"可合併性\": \"❌ 不可合併\",\n",
    "        \"推理開銷\": \"2-5%\",\n",
    "        \"部署複雜度\": \"低\",\n",
    "        \"適用任務\": \"生成\",\n",
    "        \"參數效率\": \"0.001-0.01%\"\n",
    "    },\n",
    "    \"Prefix Tuning\": {\n",
    "        \"可合併性\": \"❌ 不可合併\",\n",
    "        \"推理開銷\": \"10-20%\",\n",
    "        \"部署複雜度\": \"中\",\n",
    "        \"適用任務\": \"生成\",\n",
    "        \"參數效率\": \"0.1-1%\"\n",
    "    },\n",
    "    \"Adapter\": {\n",
    "        \"可合併性\": \"❌ 不可合併\",\n",
    "        \"推理開銷\": \"10-20%\",\n",
    "        \"部署複雜度\": \"高\",\n",
    "        \"適用任務\": \"NLU\",\n",
    "        \"參數效率\": \"0.5-5%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== PEFT 方法全面對比 ===\")\n",
    "print(f\"{'方法':<15} {'可合併':<12} {'推理開銷':<10} {'部署':<8} {'適用':<8} {'參數效率':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for method, features in peft_comparison.items():\n",
    "    print(f\"{method:<15} {features['可合併性']:<12} {features['推理開銷']:<10} {features['部署複雜度']:<8} {features['適用任務']:<8} {features['參數效率']:<12}\")\n",
    "\n",
    "print(\"\\n=== P-Tuning 的定位與優勢 ===\")\n",
    "print(\"✅ 推理開銷小: 遠小於 Prefix Tuning 和 Adapter\")\n",
    "print(\"✅ NLU 任務優秀: 特別適合分類、NER 等理解任務\")\n",
    "print(\"✅ 訓練穩定: MLP 編碼器提供更穩定的訓練\")\n",
    "print(\"✅ 參數極簡: 僅需極少參數即可達到優秀效果\")\n",
    "print(\"⚠️  不可合併: 需要同時部署基礎模型和 Prompt Encoder\")\n",
    "print(\"⚠️  生成任務: 在生成任務上可能不如 Prefix Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 總結\n",
    "\n",
    "### 9.1 核心學習收穫\n",
    "\n",
    "1. **技術認知**: 理解了 P-Tuning 作為輸入層提示方法的獨特性\n",
    "2. **部署實踐**: 掌握了 P-Tuning 的保存、載入和優化\n",
    "3. **性能分析**: 了解了 P-Tuning 相對較低的推理開銷\n",
    "4. **方法對比**: 明確了 P-Tuning 在 NLU 任務上的優勢\n",
    "\n",
    "### 9.2 P-Tuning 的部署特點\n",
    "\n",
    "**最適合的場景**:\n",
    "- NLU 任務 (文本分類、命名實體識別、關係抽取等)\n",
    "- 需要極高參數效率的場景\n",
    "- 可以接受輕微推理開銷的應用\n",
    "- 多任務場景 (共享基礎模型)\n",
    "\n",
    "**不推薦的場景**:\n",
    "- 對推理延遲零容忍 → 選擇 LoRA/IA³\n",
    "- 文本生成任務 → Prefix Tuning 可能更好\n",
    "- 超大模型 → Prompt Tuning 可能更簡單\n",
    "\n",
    "### 9.3 實用建議\n",
    "\n",
    "- NLU 任務優先考慮 P-Tuning\n",
    "- 可以在推理時預計算虛擬標記 embeddings\n",
    "- 根據任務複雜度調整虛擬標記數量\n",
    "- 多任務場景可共享基礎模型,切換不同的 Prompt Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理資源\n",
    "print(\"=== 清理實驗資源 ===\")\n",
    "del peft_model, base_model, reloaded_model, fresh_base_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"✅ 資源清理完成\")\n",
    "\n",
    "print(\"\\n🎉 Lab 7 - P-Tuning 部署指南實驗完成!\")\n",
    "print(f\"P-Tuning 模組已保存到: {save_path}\")\n",
    "print(\"您現在了解了 P-Tuning 的部署特性和優化策略!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
