# Common Utilities ä½¿ç”¨æŒ‡å—
## PEFT å¯¦é©—å®¤å…±ç”¨å·¥å…·æ¨¡çµ„

**ç‰ˆæœ¬**: v1.1.0
**æœ€å¾Œæ›´æ–°**: 2025-10-09

---

## ğŸ“š æ¨¡çµ„æ¦‚è¿°

`common_utils` æä¾›æ‰€æœ‰ PEFT å¯¦é©—å®¤å…±ç”¨çš„å·¥å…·å‡½æ•¸ï¼ŒåŒ…å«æ¨¡å‹è¼‰å…¥ã€æ•¸æ“šè™•ç†ã€è¦–è¦ºåŒ–ã€è¨“ç·´è¼”åŠ©ç­‰åŠŸèƒ½ï¼Œç¢ºä¿ä»£ç¢¼ä¸€è‡´æ€§èˆ‡å¯ç¶­è­·æ€§ã€‚

### æ¨¡çµ„çµæ§‹

```
common_utils/
â”œâ”€â”€ __init__.py              # æ¨¡çµ„å…¥å£
â”œâ”€â”€ model_helpers.py         # æ¨¡å‹ç®¡ç†å·¥å…· (878è¡Œ)
â”œâ”€â”€ data_loaders.py          # æ•¸æ“šè¼‰å…¥å·¥å…· (996è¡Œ)
â”œâ”€â”€ visualization.py         # è¦–è¦ºåŒ–å·¥å…· (æ–°å¢, 370è¡Œ)
â”œâ”€â”€ training_helpers.py      # è¨“ç·´è¼”åŠ©å·¥å…· (æ–°å¢, 380è¡Œ)
â””â”€â”€ README.md               # æœ¬æ–‡æª”
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºæœ¬å°å…¥

```python
# å°å…¥æ‰€æœ‰å·¥å…·
from common_utils import *

# æˆ–é¸æ“‡æ€§å°å…¥
from common_utils import load_model_with_peft, plot_training_curves
from common_utils.visualization import PEFT_COLORS
```

### å…¸å‹ä½¿ç”¨æµç¨‹

```python
# 1. æª¢æŸ¥ GPU
gpu_info = check_gpu_availability()
device = get_device()

# 2. è¼‰å…¥æ¨¡å‹
model = load_model_with_peft(
    model_name="meta-llama/Llama-2-7b-hf",
    peft_method=PEFTMethod.LORA,
    peft_config={'r': 8, 'lora_alpha': 16}
)

# 3. è¼‰å…¥æ•¸æ“š
dataset = load_alpaca_dataset(num_samples=1000)

# 4. è¨“ç·´å‰æª¢æŸ¥
passed, issues = pre_training_checklist(model, dataset, "./output")

# 5. è¨“ç·´ï¼ˆä½¿ç”¨ HuggingFace Trainerï¼‰
trainer = Trainer(...)
trainer.train()

# 6. è¦–è¦ºåŒ–çµæœ
plot_training_curves(trainer.state.log_history)

# 7. åˆ†æçµæœ
results = analyze_training_results(trainer, "./output")
```

---

## ğŸ“¦ æ¨¡çµ„è©³ç´°èªªæ˜

### 1. model_helpers.py

**ä¸»è¦åŠŸèƒ½**: æ¨¡å‹è¼‰å…¥ã€PEFT é…ç½®ã€é‡åŒ–ã€è¨˜æ†¶é«”ç›£æ§

#### æ ¸å¿ƒé¡åˆ¥

**ModelType (Enum)**
```python
class ModelType(Enum):
    LLAMA = "llama"
    MISTRAL = "mistral"
    QWEN = "qwen"
    GEMMA = "gemma"
    BERT = "bert"
    GPT2 = "gpt2"
```

**PEFTMethod (Enum)**
```python
class PEFTMethod(Enum):
    LORA = "lora"
    QLORA = "qlora"
    ADAPTER = "adapter"
    PREFIX_TUNING = "prefix_tuning"
    # ... å…¶ä»–æ–¹æ³•
```

#### ä¸»è¦å‡½æ•¸

**load_model_with_peft()**
```python
model = load_model_with_peft(
    model_name="meta-llama/Llama-2-7b-hf",
    peft_method=PEFTMethod.LORA,
    peft_config={'r': 8, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']},
    quantization_config={'load_in_4bit': True}
)
```

**create_peft_config()**
```python
config = create_peft_config(
    method=PEFTMethod.LORA,
    task_type="CAUSAL_LM",
    r=8,
    lora_alpha=16
)
```

---

### 2. data_loaders.py

**ä¸»è¦åŠŸèƒ½**: æŒ‡ä»¤æ•¸æ“šé›†è™•ç†ã€å¤šæ ¼å¼æç¤ºæ¨¡æ¿ã€æ‰¹æ¬¡æ”¶é›†

#### æ ¸å¿ƒé¡åˆ¥

**InstructionDataset**
```python
dataset = InstructionDataset(
    data=alpaca_data,
    tokenizer=tokenizer,
    template_type=PromptTemplate.ALPACA,
    max_length=512
)
```

**InstructionDataCollator**
```python
collator = InstructionDataCollator(
    tokenizer=tokenizer,
    padding=True,
    max_length=512
)
```

#### æç¤ºæ¨¡æ¿

æ”¯æ´å¤šç¨®æ ¼å¼:
- `PromptTemplate.ALPACA`: Alpaca æ ¼å¼
- `PromptTemplate.DOLLY`: Dolly æ ¼å¼
- `PromptTemplate.CHATML`: ChatML æ ¼å¼
- `PromptTemplate.CUSTOM`: è‡ªå®šç¾©æ ¼å¼

---

### 3. visualization.py â­ æ–°å¢

**ä¸»è¦åŠŸèƒ½**: çµ±ä¸€çš„è¦–è¦ºåŒ–å·¥å…·ï¼Œç¢ºä¿æ‰€æœ‰å¯¦é©—å®¤åœ–è¡¨é¢¨æ ¼ä¸€è‡´

#### é…è‰²æ–¹æ¡ˆ

```python
PEFT_COLORS = {
    'LoRA': '#FF6B6B',
    'Adapter': '#4ECDC4',
    'IA3': '#45B7D1',
    'train': '#3498db',
    'eval': '#e74c3c'
}
```

#### æ ¸å¿ƒå‡½æ•¸

**plot_training_curves()** - è¨“ç·´æ›²ç·š
```python
history = {
    'train_loss': [3.2, 2.8, 2.5, 2.3],
    'eval_loss': [3.5, 3.0, 2.7, 2.6],
    'eval_perplexity': [33.1, 20.1, 14.9, 13.5]
}

plot_training_curves(history, save_path="training_curves.png")
```

**plot_peft_comparison()** - PEFT æ–¹æ³•å°æ¯”
```python
results = [
    {'method': 'LoRA', 'trainable_params_%': 0.5, 'performance': 85.2},
    {'method': 'Adapter', 'trainable_params_%': 2.0, 'performance': 86.1},
    {'method': 'IA3', 'trainable_params_%': 0.01, 'performance': 84.5}
]

plot_peft_comparison(results, metrics=['trainable_params_%', 'performance'])
```

**plot_parameter_distribution()** - åƒæ•¸åˆ†ä½ˆ
```python
stats = print_trainable_parameters(model, verbose=False)
plot_parameter_distribution(stats)
```

**å¿«é€Ÿä½¿ç”¨**: Trainer æ•´åˆ
```python
from common_utils.visualization import quick_plot_trainer_history

trainer.train()
quick_plot_trainer_history(trainer, save_dir="./output")
```

---

### 4. training_helpers.py â­ æ–°å¢

**ä¸»è¦åŠŸèƒ½**: éŒ¯èª¤è™•ç†ã€è³‡æºç®¡ç†ã€è¨“ç·´ç›£æ§

#### GPU ç®¡ç†

**check_gpu_availability()**
```python
gpu_info = check_gpu_availability(verbose=True)
# è¼¸å‡º:
# ============================================================
# GPU ç’°å¢ƒæª¢æŸ¥
# ============================================================
# âœ… CUDA å¯ç”¨
# GPU æ•¸é‡: 1
# GPU å‹è™Ÿ: NVIDIA RTX 4060 Ti
# ç¸½è¨˜æ†¶é«”: 16.00 GB
# CUDA ç‰ˆæœ¬: 12.1
```

**get_device()** - è‡ªå‹•é¸æ“‡è¨­å‚™
```python
device = get_device()  # è‡ªå‹•é¸æ“‡ cuda/mps/cpu
```

#### æª¢æŸ¥é»ç®¡ç†

**load_latest_checkpoint()**
```python
checkpoint_path = load_latest_checkpoint(
    output_dir="./output",
    prefix="checkpoint-"
)
# âœ… è¼‰å…¥æª¢æŸ¥é»: ./output/checkpoint-1000
```

#### è¨“ç·´é©—è­‰

**validate_training_config()**
```python
config = {
    'learning_rate': 5e-5,
    'num_train_epochs': 3,
    'per_device_train_batch_size': 4
}

is_valid, warnings = validate_training_config(config)
for warning in warnings:
    print(warning)
```

**pre_training_checklist()**
```python
passed, issues = pre_training_checklist(
    model=model,
    train_dataset=dataset,
    output_dir="./output"
)

if passed:
    # é–‹å§‹è¨“ç·´
    trainer.train()
```

#### è¨“ç·´ç›£æ§

**TrainingMonitor**
```python
monitor = TrainingMonitor(log_interval=100)

for step, batch in enumerate(dataloader):
    loss = train_step(batch)
    monitor.log_step(loss, metrics={'lr': current_lr})

history = monitor.get_history()
plot_training_curves(history)
```

---

## ğŸ’¡ æœ€ä½³å¯¦è¸

### åœ¨å¯¦é©—å®¤ä¸­ä½¿ç”¨

**æ¨è–¦çš„å°å…¥çµæ§‹** (åœ¨ notebook é ‚éƒ¨):

```python
# 01-Setup.ipynb
from common_utils import (
    check_gpu_availability,
    get_device,
    safe_load_model,
    safe_load_dataset
)

# 02-Train.ipynb
from common_utils import (
    load_model_with_peft,
    create_peft_config,
    load_alpaca_dataset,
    InstructionDataCollator,
    print_trainable_parameters,
    plot_training_curves,
    analyze_training_results
)

# 03-Inference.ipynb
from common_utils import (
    load_latest_checkpoint,
    plot_inference_benchmark
)

# 04-Merge_and_Deploy.ipynb
from common_utils import (
    merge_and_save_model,
    plot_parameter_distribution
)
```

### æ¨™æº–åŒ–è¨“ç·´æµç¨‹

```python
# Step 1: ç’°å¢ƒæª¢æŸ¥
check_gpu_availability()
device = get_device()

# Step 2: è¼‰å…¥æ¨¡å‹èˆ‡æ•¸æ“š
model = load_model_with_peft(...)
dataset = load_alpaca_dataset(...)

# Step 3: è¨“ç·´å‰æª¢æŸ¥
passed, issues = pre_training_checklist(model, dataset, output_dir)
if not passed:
    for issue in issues:
        print(f"âš ï¸  {issue}")
    # æ±ºå®šæ˜¯å¦ç¹¼çºŒ

# Step 4: è¨“ç·´
trainer = Trainer(...)
trainer.train()

# Step 5: è¦–è¦ºåŒ–èˆ‡åˆ†æ
plot_training_curves(trainer.state.log_history)
results = analyze_training_results(trainer, output_dir)
```

---

## ğŸ”§ é«˜ç´šç”¨æ³•

### è‡ªå®šç¾©è¦–è¦ºåŒ–

```python
from common_utils.visualization import plt, PEFT_COLORS

# ä½¿ç”¨çµ±ä¸€é…è‰²
plt.plot(data, color=PEFT_COLORS['LoRA'], label='LoRA r=8')
plt.plot(data2, color=PEFT_COLORS['train'], label='Training')
```

### è¨˜æ†¶é«”ç›£æ§

```python
# è¨“ç·´å‰
print_gpu_memory_usage("è¨“ç·´å‰: ")

# è¨“ç·´ä¸­
for step in range(num_steps):
    loss = train_step()
    if step % 100 == 0:
        print_gpu_memory_usage(f"Step {step}: ")

# è¨“ç·´å¾Œ
clear_gpu_cache()
```

### çµ„åˆä½¿ç”¨ç¯„ä¾‹

```python
# å®Œæ•´çš„è¨“ç·´èˆ‡åˆ†ææµç¨‹
def train_and_analyze(model, dataset, config):
    # é©—è­‰é…ç½®
    is_valid, warnings = validate_training_config(config)

    # è¨“ç·´å‰æª¢æŸ¥
    passed, issues = pre_training_checklist(model, dataset, config['output_dir'])

    # è¨“ç·´
    trainer = Trainer(model, args=config, train_dataset=dataset)
    trainer.train()

    # è¦–è¦ºåŒ–
    plot_training_curves(trainer.state.log_history)

    # åˆ†æ
    results = analyze_training_results(trainer, config['output_dir'])

    return results
```

---

## ğŸ“Š æ¨¡çµ„çµ±è¨ˆ

| æ¨¡çµ„ | ä»£ç¢¼è¡Œæ•¸ | å‡½æ•¸æ•¸ | é¡åˆ¥æ•¸ | ç”¨é€” |
|------|---------|--------|--------|------|
| model_helpers.py | 878 | 15+ | 3 | æ¨¡å‹ç®¡ç† |
| data_loaders.py | 996 | 10+ | 3 | æ•¸æ“šè™•ç† |
| visualization.py | 370 | 12 | 0 | è¦–è¦ºåŒ– |
| training_helpers.py | 380 | 15+ | 1 | è¨“ç·´è¼”åŠ© |
| **ç¸½è¨ˆ** | **2,624** | **52+** | **7** | - |

---

## ğŸ¯ æ›´æ–°æ—¥èªŒ

### v1.1.0 (2025-10-09)
- âœ… æ–°å¢ `visualization.py` - çµ±ä¸€è¦–è¦ºåŒ–å·¥å…·
- âœ… æ–°å¢ `training_helpers.py` - è¨“ç·´è¼”åŠ©èˆ‡éŒ¯èª¤è™•ç†
- âœ… æ›´æ–° `__init__.py` - æ•´åˆæ–°æ¨¡çµ„
- âœ… æ–°å¢æœ¬ README æ–‡æª”

### v1.0.0 (2025-10-08)
- âœ… å®Œæˆ `model_helpers.py` - æ¨¡å‹è¼‰å…¥èˆ‡ PEFT é…ç½®
- âœ… å®Œæˆ `data_loaders.py` - æŒ‡ä»¤æ•¸æ“šé›†è™•ç†

---

## ğŸ“ è²¢ç»æŒ‡å—

### æ–°å¢å·¥å…·å‡½æ•¸

1. ç¢ºå®šæ‰€å±¬æ¨¡çµ„ (model/data/viz/training)
2. éµå¾ªç¾æœ‰å‘½åè¦ç¯„
3. æ·»åŠ å®Œæ•´çš„ docstring
4. åŒ…å«ä½¿ç”¨ç¯„ä¾‹
5. æ›´æ–° `__init__.py` çš„ `__all__`

### ä»£ç¢¼é¢¨æ ¼

- éµå¾ª PEP 8
- ä½¿ç”¨ Type Hints
- ä¸­æ–‡è¨»è§£ + è‹±æ–‡ docstring
- å®Œæ•´çš„éŒ¯èª¤è™•ç†

---

## ğŸ†˜ å¸¸è¦‹å•é¡Œ

### Q: ç‚ºä»€éº¼å°å…¥å¤±æ•—?
```python
# éŒ¯èª¤
from common_utils import plot_training_curves  # ModuleNotFoundError

# è§£æ±ºæ–¹æ¡ˆ: ç¢ºä¿åœ¨æ­£ç¢ºç›®éŒ„
import sys
sys.path.append('/path/to/iSpan_LLM-One-Piece')
from common_utils import plot_training_curves
```

### Q: å¦‚ä½•è‡ªå®šç¾©è¦–è¦ºåŒ–é¢¨æ ¼?
```python
from common_utils.visualization import PEFT_COLORS

# ä¿®æ”¹é…è‰²
PEFT_COLORS['LoRA'] = '#YOUR_COLOR'

# æˆ–ä½¿ç”¨è‡ªå·±çš„é…è‰²
my_colors = {'method1': '#color1', 'method2': '#color2'}
```

### Q: GPU è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦?
```python
# ä½¿ç”¨å·¥å…·å‡½æ•¸è¨ºæ–·
print_gpu_memory_usage()

# æ¸…ç©ºç·©å­˜
clear_gpu_cache()

# æª¢æŸ¥é…ç½®
validate_training_config(config)  # æœƒçµ¦å‡ºè¨˜æ†¶é«”è­¦å‘Š
```

---

## ğŸ”— ç›¸é—œè³‡æº

- **PEFT Labs**: `01-Core_Training_Techniques/02-Labs/PEFT_Labs/`
- **ç†è«–æ–‡ä»¶**: `01-Core_Training_Techniques/01-Theory/1.1-PEFT.md`
- **å°ˆæ¡ˆæ–‡æª”**: `docs/`

---

**ç¶­è­·è€…**: LLM æ•™å­¸å°ˆæ¡ˆåœ˜éšŠ
**æˆæ¬Š**: MIT License
**è²¢ç»**: æ­¡è¿æäº¤ PR æ”¹é€²å·¥å…·å‡½æ•¸
