# Lab 4.2: é«˜æ•ˆæ•¸æ“šç¯©é¸èˆ‡å„ªåŒ–

**é›£åº¦**: â­â­â­â­  
**é è¨ˆæ™‚é–“**: 4-6 å°æ™‚  
**æ›´æ–°æ—¥æœŸ**: 2025-10-17

---

## ğŸ“‹ ç›®éŒ„

1. [å¯¦é©—æ¦‚è¿°](#1-å¯¦é©—æ¦‚è¿°)
2. [ç†è«–èƒŒæ™¯](#2-ç†è«–èƒŒæ™¯)
3. [æŠ€è¡“æ¶æ§‹](#3-æŠ€è¡“æ¶æ§‹)
4. [ç’°å¢ƒéœ€æ±‚](#4-ç’°å¢ƒéœ€æ±‚)
5. [å¿«é€Ÿé–‹å§‹](#5-å¿«é€Ÿé–‹å§‹)
6. [æ ¸å¿ƒæ¦‚å¿µ](#6-æ ¸å¿ƒæ¦‚å¿µ)
7. [å¯¦ä½œæµç¨‹](#7-å¯¦ä½œæµç¨‹)
8. [é—œéµæŠ€è¡“é»](#8-é—œéµæŠ€è¡“é»)
9. [å¯¦é©—çµæœ](#9-å¯¦é©—çµæœ)
10. [æ•…éšœæ’é™¤](#10-æ•…éšœæ’é™¤)
11. [å»¶ä¼¸é–±è®€](#11-å»¶ä¼¸é–±è®€)

---

## 1. å¯¦é©—æ¦‚è¿°

### 1.1 å­¸ç¿’ç›®æ¨™

æœ¬å¯¦é©—å°‡å¸¶ä½ æŒæ¡ LLM æ•¸æ“šå·¥ç¨‹çš„æ ¸å¿ƒæŠ€è¡“,ç‰¹åˆ¥æ˜¯å¦‚ä½•ä½¿ç”¨ **IFD** å’Œ **DEITA** æ–¹æ³•è‡ªå‹•ç¯©é¸é«˜è³ªé‡å¾®èª¿æ•¸æ“šã€‚

**æ ¸å¿ƒèƒ½åŠ›**:
- âœ… ç†è§£æ•¸æ“šè³ªé‡å°æ¨¡å‹æ€§èƒ½çš„é—œéµå½±éŸ¿
- âœ… æŒæ¡ IFD (Instruction Following Difficulty) æŒ‡æ¨™çš„è¨ˆç®—èˆ‡æ‡‰ç”¨
- âœ… èƒ½å¤ ä½¿ç”¨ DEITA æ–¹æ³•è‡ªå‹•åŒ–æ•¸æ“šç¯©é¸
- âœ… å»ºç«‹ç«¯åˆ°ç«¯çš„æ•¸æ“šè™•ç†ç®¡ç·š
- âœ… é©—è­‰æ•¸æ“šç¯©é¸å°æ¨¡å‹æ€§èƒ½çš„æå‡æ•ˆæœ

### 1.2 ç‚ºä»€éº¼éœ€è¦æ•¸æ“šç¯©é¸?

**ç¶“å…¸ç ”ç©¶æ¡ˆä¾‹**:

#### LIMA (Less Is More for Alignment)
- åƒ…ç”¨ **1,000 æ¢**ç²¾å¿ƒç¯©é¸çš„é«˜è³ªé‡æ•¸æ“š
- é”åˆ°èˆ‡ **52,000 æ¢**æ•¸æ“šè¨“ç·´çš„æ¨¡å‹ç›¸ç•¶çš„æ€§èƒ½
- **æ ¸å¿ƒæ´å¯Ÿ**: æ•¸æ“šè³ªé‡ > æ•¸æ“šæ•¸é‡

#### DEITA ç ”ç©¶
- ç¯©é¸ **6K æ•¸æ“š**(å¾ 52K Alpaca)
- åœ¨å¤šå€‹åŸºæº–ä¸Šè¶…è¶Šå…¨é‡æ•¸æ“šè¨“ç·´çš„æ¨¡å‹
- **è¨“ç·´æ™‚é–“**: æ¸›å°‘ **85%**

#### Phi-1.5 (Microsoft Research)
- 1.3B åƒæ•¸çš„å°å‹æ¨¡å‹
- åƒ…ä½¿ç”¨ 7B tokens çš„é«˜è³ªé‡ã€Œæ•™ç§‘æ›¸ç´šã€æ•¸æ“š
- åœ¨æ¨ç†ä»»å‹™ä¸Šè¶…è¶Šè¨±å¤š 10B+ åƒæ•¸æ¨¡å‹

**é—œéµçµè«–**:
> "1000 æ¢é«˜è³ªé‡æ•¸æ“š > 10000 æ¢ä½è³ªé‡æ•¸æ“š"

### 1.3 å¯¦é©—è¨­è¨ˆ

æœ¬å¯¦é©—å°‡é€šéä»¥ä¸‹æ­¥é©Ÿé©—è­‰æ•¸æ“šè³ªé‡çš„é‡è¦æ€§:

1. **æ•¸æ“šæº–å‚™**: åŠ è¼‰ Alpaca æŒ‡ä»¤æ•¸æ“šé›† (52K æ¨£æœ¬)
2. **æ•¸æ“šç¯©é¸**: ä½¿ç”¨ IFD + DEITA ç¯©é¸å‡º 30% é«˜è³ªé‡æ•¸æ“š
3. **æ•ˆæœé©—è­‰**: å°æ¯”å…¨é‡æ•¸æ“š vs ç¯©é¸æ•¸æ“šçš„è¨“ç·´æ•ˆæœ
4. **ç®¡ç·šå»ºç«‹**: å»ºç«‹è‡ªå‹•åŒ–æ•¸æ“šè™•ç†ç®¡ç·š

**é æœŸæˆæœ**:
- æ•¸æ“šé‡æ¸›å°‘ **70%**
- è¨“ç·´æ™‚é–“æ¸›å°‘ **65%**
- æ¨¡å‹æ€§èƒ½æå‡æˆ–æŒå¹³

---

## 2. ç†è«–èƒŒæ™¯

### 2.1 æ•¸æ“šè³ªé‡çš„ä¸‰å€‹ç¶­åº¦

#### 2.1.1 è¤‡é›œåº¦ (Complexity)
è¡¡é‡ä»»å‹™çš„é›£åº¦èˆ‡æ¨ç†æ·±åº¦ã€‚

**ç‰¹å¾µ**:
- å¤šæ­¥é©Ÿæ¨ç†è¦æ±‚
- æ¢ä»¶åˆ†æ”¯æ•¸é‡
- é ˜åŸŸçŸ¥è­˜æ·±åº¦
- èªè¨€è¡¨é”ç²¾ç¢ºæ€§

**ç¯„ä¾‹å°æ¯”**:
```
ä½è¤‡é›œåº¦ (åˆ†æ•¸: 2/10):
  æŒ‡ä»¤: "å°‡ 'hello' ç¿»è­¯æˆä¸­æ–‡"
  å›æ‡‰: "ä½ å¥½"

é«˜è¤‡é›œåº¦ (åˆ†æ•¸: 8/10):
  æŒ‡ä»¤: "åˆ†æé‡å­è¨ˆç®—å°å¯†ç¢¼å­¸çš„æ½›åœ¨å½±éŸ¿,ä¸¦è¨è«–å¾Œé‡å­å¯†ç¢¼çš„ç™¼å±•è¶¨å‹¢"
  å›æ‡‰: [éœ€è¦å¤šæ®µè½æ·±å…¥åˆ†æ]
```

#### 2.1.2 å¤šæ¨£æ€§ (Diversity)
è¡¡é‡æ•¸æ“šé›†çš„è¦†è“‹å»£åº¦ã€‚

**é‡è¦æ€§**:
- é¿å…æ¨¡å‹éåº¦æ“¬åˆç‰¹å®šä»»å‹™
- æå‡æ³›åŒ–èƒ½åŠ›
- ç¢ºä¿å‡è¡¡çš„é ˜åŸŸè¦†è“‹

**æ¸¬é‡æ–¹æ³•**:
- K-means èšé¡è­˜åˆ¥æ•¸æ“šåˆ†å¸ƒ
- è¨ˆç®—åµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦çŸ©é™£
- è©•ä¼°ä»»å‹™é¡å‹åˆ†å¸ƒ

#### 2.1.3 æº–ç¢ºæ€§ (Accuracy)
è¡¡é‡å›æ‡‰çš„æ­£ç¢ºæ€§èˆ‡è³ªé‡ã€‚

**è©•ä¼°ç¶­åº¦**:
- äº‹å¯¦æº–ç¢ºæ€§
- é‚è¼¯ä¸€è‡´æ€§
- æ ¼å¼è¦ç¯„æ€§
- å®Œæ•´æ€§

### 2.2 IFD (Instruction Following Difficulty)

#### æ ¸å¿ƒæ€æƒ³
æ¸¬é‡æŒ‡ä»¤èˆ‡å›æ‡‰ä¹‹é–“çš„**èªç¾©è·é›¢**,è·é›¢è¶Šå¤§è¡¨ç¤ºä»»å‹™è¶Šå›°é›£ã€‚

**æ•¸å­¸è¡¨ç¤º**:
```
IFD = 1 - cosine_similarity(embedding(instruction), embedding(response))
```

**å‡è¨­**:
- **ç°¡å–®ä»»å‹™**: å›æ‡‰èˆ‡æŒ‡ä»¤é«˜åº¦ç›¸é—œ (å¦‚ã€Œç¿»è­¯ã€ã€ã€Œé‡è¤‡ã€)
  - æŒ‡ä»¤: "ç¸½çµé€™æ®µæ–‡å­—"
  - å›æ‡‰: [åŒ…å«åŸæ–‡é—œéµè©çš„ç¸½çµ]
  - IFD: ä½ (0.1-0.3)

- **å›°é›£ä»»å‹™**: å›æ‡‰éœ€è¦æ¨ç†,èˆ‡æŒ‡ä»¤èªç¾©è·é›¢è¼ƒå¤§ (å¦‚ã€Œåˆ†æã€ã€ã€Œå‰µé€ ã€)
  - æŒ‡ä»¤: "åˆ†æé‡å­è¨ˆç®—çš„æœªä¾†ç™¼å±•"
  - å›æ‡‰: [éœ€è¦èƒŒæ™¯çŸ¥è­˜ã€å¤šæ­¥é©Ÿæ¨ç†çš„åˆ†æ]
  - IFD: é«˜ (0.6-0.9)

**IFD åˆ†ä½ˆç‰¹æ€§**:
```
IFD < 0.3:  ç°¡å–®ä»»å‹™ (ç¿»è­¯ã€æ ¼å¼è½‰æ›) - éæ¿¾æ‰
0.3 â‰¤ IFD < 0.6:  ä¸­ç­‰ä»»å‹™ (æ‘˜è¦ã€æ¯”è¼ƒ) - ä¿ç•™
0.6 â‰¤ IFD < 0.9:  å›°é›£ä»»å‹™ (åˆ†æã€æ¨ç†) - ä¿ç•™
IFD â‰¥ 0.9:  éæ–¼ä¸ç›¸é—œæˆ–å™ªè² - éæ¿¾æ‰
```

#### å„ªå‹¢èˆ‡é™åˆ¶

**å„ªå‹¢**:
- âœ… ç„¡éœ€è¨“ç·´,ç›´æ¥è¨ˆç®—
- âœ… è¨ˆç®—æ•ˆç‡é«˜ (æ‰¹é‡åµŒå…¥)
- âœ… é©ç”¨æ–¼å„ç¨®èªè¨€
- âœ… ä¸ä¾è³´å¤–éƒ¨ API

**é™åˆ¶**:
- âŒ ä¾è³´åµŒå…¥æ¨¡å‹è³ªé‡
- âŒ ç„¡æ³•æ•æ‰é‚è¼¯éŒ¯èª¤
- âŒ å°æŸäº›ä»»å‹™é¡å‹ä¸æº–ç¢º (å¦‚å‰µæ„å¯«ä½œ)

### 2.3 DEITA (Data-Efficient Instruction Tuning)

#### ç¶œåˆè©•åˆ†æ¡†æ¶

DEITA çµåˆä¸‰å€‹ç¶­åº¦é€²è¡Œå…¨é¢è©•ä¼°:

**è©•åˆ†å…¬å¼**:
```
DEITA_score = Î± Ã— Complexity + Î² Ã— Quality + Î³ Ã— Diversity

å¸¸ç”¨æ¬Šé‡: Î±=0.4, Î²=0.4, Î³=0.2
```

#### ä¸‰å€‹ç¶­åº¦çš„è¨ˆç®—

**1. Complexity (è¤‡é›œåº¦)**:
- ä½¿ç”¨ LLM (å¦‚ GPT-4) è©•ä¼°æŒ‡ä»¤è¤‡é›œåº¦
- 1-10 æ‰“åˆ†åˆ¶
- è€ƒæ…®æ¨ç†æ­¥é©Ÿã€çŸ¥è­˜æ·±åº¦ã€æ¢ä»¶åˆ†æ”¯

```python
prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹æŒ‡ä»¤çš„è¤‡é›œåº¦,å¾ 1 åˆ° 10 æ‰“åˆ†:
1-3: ç°¡å–® (ç¿»è­¯å–®è©ã€åŸºç¤è¨ˆç®—)
4-6: ä¸­ç­‰ (æ®µè½ç¿»è­¯ã€å¤šæ­¥é©Ÿæ¨ç†)
7-10: å›°é›£ (è¤‡é›œåˆ†æã€å‰µæ„å¯«ä½œ)

æŒ‡ä»¤: {instruction}
"""
```

**2. Quality (è³ªé‡)**:
- ä½¿ç”¨ LLM è©•ä¼°å›æ‡‰è³ªé‡
- è©•ä¼°æº–ç¢ºæ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ã€å¯¦ç”¨æ€§

```python
prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹å›æ‡‰çš„è³ªé‡,å¾ 1 åˆ° 10 æ‰“åˆ†:
- æº–ç¢ºæ€§: å›æ‡‰æ˜¯å¦æ­£ç¢º
- å®Œæ•´æ€§: æ˜¯å¦å……åˆ†å›ç­”å•é¡Œ
- æ¸…æ™°åº¦: è¡¨é”æ˜¯å¦æ¸…æ™°
- å¯¦ç”¨æ€§: æ˜¯å¦æœ‰å¯¦éš›åƒ¹å€¼

æŒ‡ä»¤: {instruction}
å›æ‡‰: {output}
"""
```

**3. Diversity (å¤šæ¨£æ€§)**:
- è¨ˆç®—æ¨£æœ¬èˆ‡å·²é¸æ“‡æ¨£æœ¬çš„æœ€å°ç›¸ä¼¼åº¦
- ä½¿ç”¨ Sentence-BERT ç”ŸæˆåµŒå…¥
- é¼“å‹µé¸æ“‡èˆ‡å·²æœ‰æ•¸æ“šä¸åŒçš„æ¨£æœ¬

```python
diversity = 1 - max(cosine_similarity(sample, selected_samples))
```

#### DEITA é¸æ“‡æµç¨‹

```
1. è¨ˆç®—æ‰€æœ‰æ¨£æœ¬çš„ Complexity åˆ†æ•¸
   â†“
2. è¿­ä»£é¸æ“‡æ¨£æœ¬:
   - è¨ˆç®—ç•¶å‰æ¨£æœ¬çš„ Quality åˆ†æ•¸
   - è¨ˆç®—ç•¶å‰æ¨£æœ¬èˆ‡å·²é¸æ¨£æœ¬çš„ Diversity åˆ†æ•¸
   - è¨ˆç®— DEITA ç¶œåˆåˆ†æ•¸
   â†“
3. é¸æ“‡ DEITA åˆ†æ•¸æœ€é«˜çš„ k å€‹æ¨£æœ¬
   â†“
4. è¼¸å‡ºç¯©é¸å¾Œçš„æ•¸æ“šé›†
```

### 2.4 LESS (Low-Effort Score Sampling)

#### åŸºæ–¼æ¢¯åº¦çš„æ•¸æ“šé¸æ“‡

**æ ¸å¿ƒæ€æƒ³**:
- å°æ¨¡å‹å½±éŸ¿å¤§çš„æ¨£æœ¬ = é«˜æ¢¯åº¦ç¯„æ•¸
- é¸æ“‡æ¢¯åº¦ç¯„æ•¸æœ€å¤§çš„ k å€‹æ¨£æœ¬

**å„ªå‹¢**:
- âœ… ç›´æ¥åŸºæ–¼æ¨¡å‹è¨“ç·´ä¿¡è™Ÿ
- âœ… ä¸ä¾è³´å¤–éƒ¨ LLM
- âœ… è¨ˆç®—æ•ˆç‡é«˜ (å–®æ¬¡å‰å‘+åå‘)

**è¨ˆç®—æµç¨‹**:
```python
for sample in dataset:
    loss = model(sample)
    loss.backward()
    gradient_norm = compute_gradient_norm(model)
    scores.append((sample, gradient_norm))

selected = top_k(scores, k)
```

---

## 3. æŠ€è¡“æ¶æ§‹

### 3.1 æ•´é«”æµç¨‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     åŸå§‹æ•¸æ“šé›† (52K)                          â”‚
â”‚                 (Alpaca / Dolly / ShareGPT)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              01-Setup: æ•¸æ“šåŠ è¼‰èˆ‡çµ±è¨ˆåˆ†æ                      â”‚
â”‚  âœ“ åŠ è¼‰åŸå§‹æ•¸æ“šé›†                                             â”‚
â”‚  âœ“ åŸºç¤çµ±è¨ˆ (é•·åº¦ã€é ˜åŸŸåˆ†å¸ƒ)                                   â”‚
â”‚  âœ“ æº–å‚™è©•ä¼°æ¨¡å‹ (Sentence-BERT)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           02-Filter: IFD + DEITA ç¯©é¸                        â”‚
â”‚  âœ“ IFD è¨ˆç®— (èªç¾©è·é›¢)                                        â”‚
â”‚  âœ“ DEITA è©•åˆ† (è¤‡é›œåº¦ + è³ªé‡ + å¤šæ¨£æ€§)                         â”‚
â”‚  âœ“ æ‡‰ç”¨ç¯©é¸é–¾å€¼ (ä¿ç•™ Top-30%)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ç¯©é¸å¾Œæ•¸æ“šé›† (15.6K, 30%)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              03-Validate: ç¯©é¸æ•ˆæœé©—è­‰                         â”‚
â”‚  âœ“ å°æ¯”å¯¦é©— (å…¨é‡ vs ç¯©é¸)                                    â”‚
â”‚  âœ“ è¨“ç·´è³‡æºå°æ¯” (æ™‚é–“ã€GPU)                                   â”‚
â”‚  âœ“ è©•ä¼°æŒ‡æ¨™å°æ¯” (C-Eval)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          04-Pipeline: è‡ªå‹•åŒ–æ•¸æ“šç®¡ç·š                           â”‚
â”‚  âœ“ ç«¯åˆ°ç«¯ç®¡ç·šå»ºç«‹                                             â”‚
â”‚  âœ“ å¢é‡æ•¸æ“šè™•ç†                                               â”‚
â”‚  âœ“ æ•¸æ“šç‰ˆæœ¬ç®¡ç†                                               â”‚
â”‚  âœ“ è³ªé‡ç›£æ§å„€è¡¨æ¿                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æŠ€è¡“æ£§

| çµ„ä»¶ | æŠ€è¡“ | ç”¨é€” |
|:---|:---|:---|
| **èªç¾©åµŒå…¥** | Sentence-BERT | ç”ŸæˆæŒ‡ä»¤èˆ‡å›æ‡‰çš„å‘é‡è¡¨ç¤º |
| **ç›¸ä¼¼åº¦è¨ˆç®—** | scikit-learn | è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ |
| **èšé¡åˆ†æ** | K-means | è­˜åˆ¥æ•¸æ“šåˆ†å¸ƒèˆ‡å¤šæ¨£æ€§ |
| **LLM è©•ä¼°** | OpenAI API / æœ¬åœ°æ¨¡å‹ | è©•ä¼°è¤‡é›œåº¦èˆ‡è³ªé‡ |
| **æ•¸æ“šè™•ç†** | pandas, numpy | æ•¸æ“šæ¸…æ´—èˆ‡çµ±è¨ˆåˆ†æ |
| **å¯è¦–åŒ–** | matplotlib, seaborn | æ•¸æ“šåˆ†å¸ƒèˆ‡è©•ä¼°çµæœå¯è¦–åŒ– |
| **æ¨¡å‹è¨“ç·´** | Transformers, PEFT | é©—è­‰ç¯©é¸æ•ˆæœ |

---

## 4. ç’°å¢ƒéœ€æ±‚

### 4.1 ç¡¬é«”éœ€æ±‚

| çµ„ä»¶ | æœ€ä½é…ç½® | æ¨è–¦é…ç½® |
|:---|:---|:---|
| **GPU** | ç„¡ (CPU å¯åŸ·è¡Œç¯©é¸) | RTX 3090 / A100 (ç”¨æ–¼é©—è­‰è¨“ç·´) |
| **VRAM** | - | 24GB+ (ç”¨æ–¼é©—è­‰è¨“ç·´) |
| **RAM** | 16GB | 32GB+ |
| **ç£ç¢Ÿç©ºé–“** | 10GB | 50GB+ |

**èªªæ˜**:
- æ•¸æ“šç¯©é¸éšæ®µ (01-02) å¯åœ¨ CPU ä¸ŠåŸ·è¡Œ
- é©—è­‰è¨“ç·´éšæ®µ (03) éœ€è¦ GPU
- å¦‚ç„¡ GPU,å¯ä½¿ç”¨é è¨ˆç®—çš„é©—è­‰çµæœ

### 4.2 è»Ÿé«”ä¾è³´

```bash
# æ ¸å¿ƒä¾è³´
sentence-transformers>=2.2.0
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0

# è¨“ç·´é©—è­‰ (å¯é¸)
transformers>=4.35.0
peft>=0.6.0
accelerate>=0.24.0
torch>=2.0.0

# å¯è¦–åŒ–å¢å¼·
plotly>=5.17.0
ipywidgets>=8.1.0
```

### 4.3 å®‰è£æŒ‡ä»¤

```bash
# æ–¹æ³• 1: ä½¿ç”¨ Poetry (æ¨è–¦)
cd 00-Course_Setup/
poetry install --all-extras

# æ–¹æ³• 2: ä½¿ç”¨ pip
pip install sentence-transformers scikit-learn pandas numpy \
            matplotlib seaborn transformers peft accelerate
```

---

## 5. å¿«é€Ÿé–‹å§‹

### 5.1 å®Œæ•´æµç¨‹ (4-6 å°æ™‚)

```bash
# 1. å•Ÿå‹• Jupyter Lab
cd 04-Evaluation_and_Data_Engineering/02-Labs/Lab-4.2-Efficient_Data_Filtering/
jupyter lab

# 2. æŒ‰é †åºåŸ·è¡Œ Notebooks
# 01-Setup.ipynb        (30-45 åˆ†é˜)
# 02-Filter.ipynb       (1-1.5 å°æ™‚)
# 03-Validate.ipynb     (2-3 å°æ™‚) - éœ€è¦ GPU
# 04-Pipeline.ipynb     (30-45 åˆ†é˜)
```

### 5.2 å¿«é€Ÿæ¸¬è©¦ (åƒ…æ•¸æ“šç¯©é¸,30 åˆ†é˜)

å¦‚æœæ²’æœ‰ GPU æˆ–æ™‚é–“æœ‰é™,å¯ä»¥åªåŸ·è¡Œæ•¸æ“šç¯©é¸éƒ¨åˆ†:

```bash
# åŸ·è¡Œ 01-Setup.ipynb å’Œ 02-Filter.ipynb
# è·³é 03-Validate.ipynb (è¨“ç·´é©—è­‰)
# æŸ¥çœ‹ 02-Filter.ipynb çš„ç¯©é¸çµæœ
```

### 5.3 é æœŸè¼¸å‡º

å®Œæˆæ‰€æœ‰ notebooks å¾Œ,ä½ å°‡ç²å¾—:

```
Lab-4.2-Efficient_Data_Filtering/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ alpaca_raw.json              (52K åŸå§‹æ•¸æ“š)
â”‚   â””â”€â”€ alpaca_filtered.json         (15.6K ç¯©é¸å¾Œæ•¸æ“š)
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ ifd_distribution.png         (IFD åˆ†å¸ƒåœ–)
â”‚   â”œâ”€â”€ deita_scores.csv             (DEITA è©•åˆ†è¡¨)
â”‚   â”œâ”€â”€ quality_comparison.png       (è³ªé‡å°æ¯”åœ–)
â”‚   â””â”€â”€ filtering_summary.md         (ç¯©é¸æ‘˜è¦å ±å‘Š)
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ training_logs/               (è¨“ç·´æ—¥èªŒ)
â”‚   â”œâ”€â”€ full_data_results.json       (å…¨é‡æ•¸æ“šçµæœ)
â”‚   â”œâ”€â”€ filtered_data_results.json   (ç¯©é¸æ•¸æ“šçµæœ)
â”‚   â””â”€â”€ comparison_report.md         (å°æ¯”å ±å‘Š)
â””â”€â”€ pipeline/
    â”œâ”€â”€ data_pipeline.py             (è‡ªå‹•åŒ–ç®¡ç·šä»£ç¢¼)
    â””â”€â”€ quality_dashboard.html       (è³ªé‡ç›£æ§å„€è¡¨æ¿)
```

---

## 6. æ ¸å¿ƒæ¦‚å¿µ

### 6.1 IFD è¨ˆç®—ç¯„ä¾‹

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# ç¯„ä¾‹ 1: ç°¡å–®ä»»å‹™ (ä½ IFD)
instruction_1 = "å°‡ 'hello' ç¿»è­¯æˆä¸­æ–‡"
response_1 = "ä½ å¥½"

emb_i1 = model.encode([instruction_1])
emb_r1 = model.encode([response_1])
similarity_1 = cosine_similarity(emb_i1, emb_r1)[0][0]
ifd_1 = 1 - similarity_1

print(f"ç°¡å–®ä»»å‹™ IFD: {ifd_1:.3f}")  # é æœŸ: 0.2-0.4

# ç¯„ä¾‹ 2: å›°é›£ä»»å‹™ (é«˜ IFD)
instruction_2 = "åˆ†æé‡å­è¨ˆç®—å°ç¾ä»£å¯†ç¢¼å­¸çš„å½±éŸ¿"
response_2 = """é‡å­è¨ˆç®—çš„ç™¼å±•å°ç¾ä»£å¯†ç¢¼å­¸æ§‹æˆäº†é‡å¤§æŒ‘æˆ°...
[è©³ç´°åˆ†æå…§å®¹]"""

emb_i2 = model.encode([instruction_2])
emb_r2 = model.encode([response_2])
similarity_2 = cosine_similarity(emb_i2, emb_r2)[0][0]
ifd_2 = 1 - similarity_2

print(f"å›°é›£ä»»å‹™ IFD: {ifd_2:.3f}")  # é æœŸ: 0.6-0.8
```

### 6.2 DEITA è©•åˆ†ç¯„ä¾‹

```python
def calculate_deita_score(sample, reference_samples, alpha=0.4, beta=0.4, gamma=0.2):
    """
    è¨ˆç®— DEITA ç¶œåˆè©•åˆ†
    
    Args:
        sample: ç•¶å‰æ¨£æœ¬
        reference_samples: å·²é¸æ“‡çš„åƒè€ƒæ¨£æœ¬
        alpha, beta, gamma: æ¬Šé‡åƒæ•¸
    
    Returns:
        DEITA åˆ†æ•¸ (0-1)
    """
    # 1. è¤‡é›œåº¦è©•åˆ† (ä½¿ç”¨ LLM æˆ–è¦å‰‡)
    complexity = evaluate_complexity(sample['instruction'])
    
    # 2. è³ªé‡è©•åˆ† (ä½¿ç”¨ LLM æˆ–è¦å‰‡)
    quality = evaluate_quality(sample['instruction'], sample['output'])
    
    # 3. å¤šæ¨£æ€§è©•åˆ† (èˆ‡å·²é¸æ¨£æœ¬çš„æœ€å°ç›¸ä¼¼åº¦)
    if not reference_samples:
        diversity = 1.0
    else:
        sample_emb = model.encode([sample['instruction'] + ' ' + sample['output']])
        ref_embs = model.encode([
            s['instruction'] + ' ' + s['output'] for s in reference_samples
        ])
        similarities = cosine_similarity(sample_emb, ref_embs)[0]
        diversity = 1 - np.max(similarities)
    
    # 4. åŠ æ¬Šæ±‚å’Œ
    deita_score = alpha * complexity + beta * quality + gamma * diversity
    
    return deita_score
```

### 6.3 æ•¸æ“šç¯©é¸æ±ºç­–æ¨¹

```
é–‹å§‹è©•ä¼°æ¨£æœ¬
    â”‚
    â”œâ”€ è¨ˆç®— IFD åˆ†æ•¸
    â”‚    â”‚
    â”‚    â”œâ”€ IFD < 0.3? â†’ å¤ªç°¡å–®,éæ¿¾æ‰
    â”‚    â”œâ”€ IFD > 0.9? â†’ ä¸ç›¸é—œ,éæ¿¾æ‰
    â”‚    â””â”€ 0.3 â‰¤ IFD â‰¤ 0.9? â†’ ç¹¼çºŒ
    â”‚
    â”œâ”€ è¨ˆç®— DEITA åˆ†æ•¸
    â”‚    â”‚
    â”‚    â”œâ”€ è¤‡é›œåº¦ < 3? â†’ éæ¿¾æ‰
    â”‚    â”œâ”€ è³ªé‡ < 5? â†’ éæ¿¾æ‰
    â”‚    â””â”€ åˆ†æ•¸ â‰¥ é–¾å€¼? â†’ ä¿ç•™
    â”‚
    â””â”€ æª¢æŸ¥å¤šæ¨£æ€§
         â”‚
         â”œâ”€ èˆ‡å·²é¸æ¨£æœ¬ç›¸ä¼¼åº¦ > 0.9? â†’ éæ¿¾æ‰ (é‡è¤‡)
         â””â”€ å¦å‰‡ â†’ åŠ å…¥ç¯©é¸é›†
```

---

## 7. å¯¦ä½œæµç¨‹

### 7.1 Notebook 01: æ•¸æ“šæº–å‚™èˆ‡ç’°å¢ƒé…ç½®

**ç›®æ¨™**: åŠ è¼‰åŸå§‹æ•¸æ“šé›†,é€²è¡Œçµ±è¨ˆåˆ†æ,æº–å‚™è©•ä¼°æ¨¡å‹

**æ ¸å¿ƒä»»å‹™**:
1. åŠ è¼‰ Alpaca æ•¸æ“šé›† (52K æ¨£æœ¬)
2. åŸºç¤çµ±è¨ˆåˆ†æ:
   - æŒ‡ä»¤é•·åº¦åˆ†å¸ƒ
   - å›æ‡‰é•·åº¦åˆ†å¸ƒ
   - ä»»å‹™é¡å‹åˆ†å¸ƒ
3. æº–å‚™ Sentence-BERT æ¨¡å‹
4. è¨­å®šç¯©é¸ç›®æ¨™ (ä¿ç•™ 30%)

**é æœŸè¼¸å‡º**:
```
æ•¸æ“šçµ±è¨ˆ:
  ç¸½æ¨£æœ¬æ•¸: 52,002
  å¹³å‡æŒ‡ä»¤é•·åº¦: 15.3 è©
  å¹³å‡å›æ‡‰é•·åº¦: 78.4 è©
  ä»»å‹™é¡å‹: 
    - é–‹æ”¾å¼ QA: 35%
    - å‰µæ„å¯«ä½œ: 25%
    - åˆ†é¡/æ¨™è¨»: 20%
    - å…¶ä»–: 20%
```

### 7.2 Notebook 02: æ•¸æ“šç¯©é¸èˆ‡è©•åˆ†

**ç›®æ¨™**: ä½¿ç”¨ IFD å’Œ DEITA æ–¹æ³•ç¯©é¸é«˜è³ªé‡æ•¸æ“š

**æ ¸å¿ƒä»»å‹™**:

#### æ­¥é©Ÿ 1: IFD è¨ˆç®—
```python
# æ‰¹é‡è¨ˆç®— IFD
ifd_calculator = IFDCalculator()
samples_with_ifd = ifd_calculator.calculate_batch_ifd(raw_data)

# IFD éæ¿¾ (ä¿ç•™ 0.3-0.9)
ifd_filtered = [
    s for s, ifd in samples_with_ifd 
    if 0.3 <= ifd <= 0.9
]

print(f"IFD éæ¿¾: {len(raw_data)} â†’ {len(ifd_filtered)}")
# é æœŸ: 52,002 â†’ 41,000 (éæ¿¾æ‰ 21%)
```

#### æ­¥é©Ÿ 2: DEITA è©•åˆ†
```python
# è¨ˆç®— DEITA åˆ†æ•¸
deita_scorer = DEITAScorer(
    alpha=0.4,  # è¤‡é›œåº¦æ¬Šé‡
    beta=0.4,   # è³ªé‡æ¬Šé‡
    gamma=0.2   # å¤šæ¨£æ€§æ¬Šé‡
)

scored_samples = []
for sample in ifd_filtered:
    score = deita_scorer.score_sample(sample, scored_samples)
    scored_samples.append(score)

# æ’åºä¸¦é¸æ“‡ Top-30%
scored_samples.sort(key=lambda x: x['deita_score'], reverse=True)
final_filtered = scored_samples[:int(len(scored_samples) * 0.3)]

print(f"DEITA ç¯©é¸: {len(ifd_filtered)} â†’ {len(final_filtered)}")
# é æœŸ: 41,000 â†’ 15,600 (ä¿ç•™ 30%)
```

**é æœŸè¼¸å‡º**:
```
ç¯©é¸çµæœ:
  åŸå§‹æ•¸æ“š: 52,002 æ¨£æœ¬
  IFD éæ¿¾å¾Œ: 41,000 æ¨£æœ¬ (éæ¿¾ 21%)
  DEITA ç¯©é¸å¾Œ: 15,600 æ¨£æœ¬ (ä¿ç•™ 30%)

è³ªé‡æå‡:
  å¹³å‡ IFD: 0.23 â†’ 0.41 (+78%)
  å¹³å‡è¤‡é›œåº¦: 2.1 â†’ 3.7 (+76%)
  å¤šæ¨£æ€§ (èšé¡æ•¸): 87% â†’ 94%
```

### 7.3 Notebook 03: ç¯©é¸æ•ˆæœé©—è­‰

**ç›®æ¨™**: é€šéå°æ¯”è¨“ç·´å¯¦é©—é©—è­‰ç¯©é¸æ•ˆæœ

**æ ¸å¿ƒä»»å‹™**:

#### å¯¦é©—è¨­è¨ˆ
```
åŸºç·šå¯¦é©— (Baseline):
  æ•¸æ“š: å…¨é‡ 52K
  æ¨¡å‹: Llama-2-7B
  è¨“ç·´è¼ªæ•¸: 3 epochs
  è©•ä¼°: C-Eval

å°æ¯”å¯¦é©— (Filtered):
  æ•¸æ“š: ç¯©é¸å¾Œ 15.6K (30%)
  æ¨¡å‹: Llama-2-7B (ç›¸åŒåˆå§‹æ¬Šé‡)
  è¨“ç·´è¼ªæ•¸: 3 epochs
  è©•ä¼°: C-Eval (ç›¸åŒæ¸¬è©¦é›†)
```

#### å°æ¯”ç¶­åº¦
1. **æ¨¡å‹æ€§èƒ½**: C-Eval æº–ç¢ºç‡
2. **è¨“ç·´æ•ˆç‡**: è¨“ç·´æ™‚é–“ã€GPU åˆ©ç”¨ç‡
3. **æ”¶æ–‚é€Ÿåº¦**: Loss æ›²ç·š
4. **æ³›åŒ–èƒ½åŠ›**: é©—è­‰é›†æ€§èƒ½

**é æœŸè¼¸å‡º**:
```
å°æ¯”çµæœ:

æ¨¡å‹æ€§èƒ½:
  å…¨é‡æ•¸æ“š (52K): C-Eval 45.3%
  ç¯©é¸æ•¸æ“š (15.6K): C-Eval 47.1% (+1.8%)

è¨“ç·´æ•ˆç‡:
  å…¨é‡æ•¸æ“š: 12 å°æ™‚, GPU 90%
  ç¯©é¸æ•¸æ“š: 3.5 å°æ™‚, GPU 88%
  æ•ˆç‡æå‡: 3.4x æ›´å¿«

çµè«–:
  âœ… æ•¸æ“šé‡æ¸›å°‘ 70%
  âœ… è¨“ç·´æ™‚é–“æ¸›å°‘ 71%
  âœ… æ¨¡å‹æ€§èƒ½æå‡ 1.8%
```

### 7.4 Notebook 04: è‡ªå‹•åŒ–æ•¸æ“šç®¡ç·š

**ç›®æ¨™**: å»ºç«‹ç«¯åˆ°ç«¯çš„è‡ªå‹•åŒ–æ•¸æ“šè™•ç†ç®¡ç·š

**æ ¸å¿ƒä»»å‹™**:

#### ç®¡ç·šè¨­è¨ˆ
```python
class DataFilteringPipeline:
    def __init__(self, config):
        self.ifd_calculator = IFDCalculator()
        self.deita_scorer = DEITAScorer()
        self.config = config
    
    def run(self, input_data):
        # 1. IFD éæ¿¾
        ifd_filtered = self.ifd_filter(input_data)
        
        # 2. DEITA è©•åˆ†
        scored = self.deita_score(ifd_filtered)
        
        # 3. Top-K é¸æ“‡
        selected = self.select_top_k(scored, k=self.config['target_size'])
        
        # 4. ä¿å­˜çµæœ
        self.save_results(selected)
        
        return selected
```

#### å¢é‡è™•ç†
```python
# è™•ç†æ–°å¢æ•¸æ“š
new_data = load_new_data()
existing_data = load_filtered_data()

# åƒ…å°æ–°æ•¸æ“šé€²è¡Œç¯©é¸
new_filtered = pipeline.run(new_data)

# åˆä½µä¸¦é‡æ–°è©•ä¼°å¤šæ¨£æ€§
combined = combine_and_rerank(existing_data, new_filtered)
```

---

## 8. é—œéµæŠ€è¡“é»

### 8.1 åµŒå…¥æ¨¡å‹é¸æ“‡

| æ¨¡å‹ | åƒæ•¸é‡ | é€Ÿåº¦ | è³ªé‡ | æ¨è–¦å ´æ™¯ |
|:---|---:|:---:|:---:|:---|
| `all-MiniLM-L6-v2` | 22M | â­â­â­â­â­ | â­â­â­ | å¿«é€Ÿå¯¦é©— |
| `all-mpnet-base-v2` | 110M | â­â­â­â­ | â­â­â­â­ | å¹³è¡¡é¸æ“‡ (æ¨è–¦) |
| `text-embedding-ada-002` | - | â­â­â­ | â­â­â­â­â­ | æœ€ä½³è³ªé‡ (éœ€ API) |

**æ¨è–¦**: `all-mpnet-base-v2` (è³ªé‡èˆ‡é€Ÿåº¦å¹³è¡¡)

### 8.2 æ‰¹é‡è™•ç†å„ªåŒ–

```python
# âœ… å¥½çš„åšæ³•: æ‰¹é‡ç·¨ç¢¼
embeddings = model.encode(
    texts,
    batch_size=64,
    show_progress_bar=True,
    convert_to_numpy=True
)

# âŒ ä¸å¥½çš„åšæ³•: é€å€‹ç·¨ç¢¼
embeddings = [model.encode(text) for text in texts]  # æ…¢ 10-50 å€
```

### 8.3 è¨˜æ†¶é«”å„ªåŒ–

```python
# åˆ†æ‰¹è™•ç†å¤§å‹æ•¸æ“šé›†
def process_large_dataset(dataset, batch_size=1000):
    results = []
    
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        batch_results = process_batch(batch)
        results.extend(batch_results)
        
        # é‡‹æ”¾è¨˜æ†¶é«”
        del batch_results
        gc.collect()
    
    return results
```

### 8.4 å¤šæ¨£æ€§ä¿è­‰

ä½¿ç”¨**è²ªå©ªé¸æ“‡æ¼”ç®—æ³•**ç¢ºä¿å¤šæ¨£æ€§:

```python
def greedy_selection(scored_samples, k):
    """
    è²ªå©ªé¸æ“‡æ¼”ç®—æ³•,ç¢ºä¿å¤šæ¨£æ€§
    
    Args:
        scored_samples: å·²è©•åˆ†çš„æ¨£æœ¬åˆ—è¡¨
        k: ç›®æ¨™æ•¸é‡
    
    Returns:
        é¸æ“‡çš„æ¨£æœ¬åˆ—è¡¨
    """
    selected = []
    
    # 1. é¸æ“‡åˆ†æ•¸æœ€é«˜çš„ç¬¬ä¸€å€‹æ¨£æœ¬
    selected.append(scored_samples[0])
    remaining = scored_samples[1:]
    
    # 2. è¿­ä»£é¸æ“‡
    while len(selected) < k and remaining:
        # é‡æ–°è¨ˆç®—æ¯å€‹å€™é¸æ¨£æœ¬çš„å¤šæ¨£æ€§åˆ†æ•¸
        for sample in remaining:
            diversity = calculate_diversity(sample, selected)
            sample['final_score'] = (
                0.6 * sample['deita_score'] + 
                0.4 * diversity
            )
        
        # é¸æ“‡åˆ†æ•¸æœ€é«˜çš„
        remaining.sort(key=lambda x: x['final_score'], reverse=True)
        selected.append(remaining[0])
        remaining = remaining[1:]
    
    return selected
```

---

## 9. å¯¦é©—çµæœ

### 9.1 é æœŸçµæœ

åŸºæ–¼ DEITA è«–æ–‡çš„çµæœ,æˆ‘å€‘é æœŸ:

| æŒ‡æ¨™ | å…¨é‡æ•¸æ“š (52K) | ç¯©é¸æ•¸æ“š (15.6K) | è®ŠåŒ– |
|:---|---:|---:|:---:|
| **æ•¸æ“šé‡** | 52,002 | 15,600 | -70% |
| **å¹³å‡ IFD** | 0.23 | 0.41 | +78% |
| **å¹³å‡è¤‡é›œåº¦** | 2.1 | 3.7 | +76% |
| **å¤šæ¨£æ€§** | 87% | 94% | +7% |
| **C-Eval æº–ç¢ºç‡** | 45.3% | 47.1% | +1.8% |
| **è¨“ç·´æ™‚é–“** | 12h | 3.5h | -71% |
| **GPU è¨˜æ†¶é«”** | 22GB | 22GB | - |

### 9.2 é—œéµè§€å¯Ÿ

**æ•¸æ“šè³ªé‡æå‡**:
- IFD åˆ†æ•¸é¡¯è‘—æé«˜ (+78%),è¡¨ç¤ºä¿ç•™äº†æ›´å›°é›£çš„ä»»å‹™
- è¤‡é›œåº¦æå‡ (+76%),éæ¿¾æ‰ç°¡å–®é‡è¤‡çš„ä»»å‹™
- å¤šæ¨£æ€§æ”¹å–„ (+7%),ç¢ºä¿é ˜åŸŸè¦†è“‹

**è¨“ç·´æ•ˆç‡**:
- è¨“ç·´æ™‚é–“æ¸›å°‘ 71%,å¤§å¹…æå‡è¿­ä»£é€Ÿåº¦
- æ¨¡å‹æ€§èƒ½åè€Œæå‡ 1.8%,é©—è­‰ã€Œè³ªé‡ > æ•¸é‡ã€

**æˆæœ¬æ•ˆç›Š**:
- æ¯æ¬¡è¨“ç·´æˆæœ¬é™ä½ 70%
- å¯¦é©—è¿­ä»£é€Ÿåº¦æå‡ 3.4 å€
- ROI é¡¯è‘—æå‡

---

## 10. æ•…éšœæ’é™¤

### 10.1 å¸¸è¦‹å•é¡Œ

#### Q1: Sentence-BERT è¨˜æ†¶é«”ä¸è¶³

**ç—‡ç‹€**: `CUDA out of memory` æˆ– `RuntimeError: out of memory`

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# æ–¹æ³• 1: æ¸›å°æ‰¹æ¬¡å¤§å°
embeddings = model.encode(texts, batch_size=32)  # é™ä½è‡³ 32 æˆ– 16

# æ–¹æ³• 2: ä½¿ç”¨ CPU
model = SentenceTransformer('all-mpnet-base-v2', device='cpu')

# æ–¹æ³• 3: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
model = SentenceTransformer('all-MiniLM-L6-v2')  # 22M åƒæ•¸
```

#### Q2: DEITA è©•åˆ†é€Ÿåº¦æ…¢

**ç—‡ç‹€**: è™•ç† 52K æ¨£æœ¬éœ€è¦è¶…é 10 å°æ™‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# æ–¹æ³• 1: ä¸ä½¿ç”¨ LLM,åƒ…ä½¿ç”¨è¦å‰‡è©•åˆ†
def rule_based_complexity(instruction):
    # åŸºæ–¼é•·åº¦ã€é—œéµè©ç­‰è¦å‰‡
    score = calculate_rule_score(instruction)
    return score

# æ–¹æ³• 2: ä½¿ç”¨æœ¬åœ°æ¨¡å‹è€Œé API
local_model = AutoModelForCausalLM.from_pretrained("Qwen-7B")

# æ–¹æ³• 3: å…ˆç”¨ IFD éæ¿¾,æ¸›å°‘éœ€è¦è©•åˆ†çš„æ¨£æœ¬
ifd_filtered = ifd_filter(raw_data, min_ifd=0.3, max_ifd=0.9)
deita_scored = deita_score(ifd_filtered)  # æ•¸é‡æ¸›å°‘ ~20%
```

#### Q3: è¨“ç·´é©—è­‰å¯¦é©—ç„¡ GPU

**ç—‡ç‹€**: æ²’æœ‰ GPU ç„¡æ³•åŸ·è¡Œ 03-Validate.ipynb

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# é¸é … 1: ä½¿ç”¨é è¨ˆç®—çš„çµæœ
# ä¸‹è¼‰é è¨ˆç®—çš„é©—è­‰çµæœ
!wget https://example.com/validation_results.json

# é¸é … 2: ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (CPU å¯è¡Œ)
model_name = "EleutherAI/pythia-160m"  # 160M åƒæ•¸,CPU å¯è¨“ç·´

# é¸é … 3: è·³éé©—è­‰,æŸ¥çœ‹ Lab æä¾›çš„çµæœæ‘˜è¦
```

### 10.2 é™¤éŒ¯æŠ€å·§

#### é€æ­¥æª¢æŸ¥æ•¸æ“šè³ªé‡
```python
# æª¢æŸ¥ IFD åˆ†å¸ƒ
import matplotlib.pyplot as plt

ifd_scores = [s['ifd'] for s in samples_with_ifd]
plt.hist(ifd_scores, bins=50)
plt.xlabel('IFD Score')
plt.ylabel('Count')
plt.title('IFD Distribution')
plt.show()

# é æœŸ: æ­£æ…‹åˆ†å¸ƒ,å³°å€¼åœ¨ 0.4-0.6
```

#### æª¢æŸ¥ç¯©é¸é‚è¼¯
```python
# æ‰“å°ç¯©é¸çµ±è¨ˆ
print(f"åŸå§‹æ•¸æ“š: {len(raw_data)}")
print(f"IFD < 0.3 (éæ¿¾): {sum(1 for s, ifd in samples_with_ifd if ifd < 0.3)}")
print(f"IFD > 0.9 (éæ¿¾): {sum(1 for s, ifd in samples_with_ifd if ifd > 0.9)}")
print(f"0.3 â‰¤ IFD â‰¤ 0.9 (ä¿ç•™): {len(ifd_filtered)}")
```

---

## 11. å»¶ä¼¸é–±è®€

### 11.1 æ ¸å¿ƒè«–æ–‡

**DEITA**:
- æ¨™é¡Œ: "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning"
- ä½œè€…: Liu et al. (HKUST)
- arXiv: [2312.15685](https://arxiv.org/abs/2312.15685)
- GitHub: [https://github.com/hkust-nlp/deita](https://github.com/hkust-nlp/deita)

**LIMA**:
- æ¨™é¡Œ: "LIMA: Less Is More for Alignment"
- ä½œè€…: Zhou et al. (Meta AI)
- arXiv: [2305.11206](https://arxiv.org/abs/2305.11206)

**IFD**:
- æ¨™é¡Œ: "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"
- arXiv: [2308.12032](https://arxiv.org/abs/2308.12032)

**LESS**:
- æ¨™é¡Œ: "LESS: Selecting Influential Data for Targeted Instruction Tuning"
- arXiv: [2402.04333](https://arxiv.org/abs/2402.04333)

### 11.2 ç›¸é—œè³‡æº

**æ•¸æ“šé›†**:
- Alpaca: [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- Dolly: [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)
- ShareGPT: [https://sharegpt.com/](https://sharegpt.com/)

**å·¥å…·èˆ‡æ¡†æ¶**:
- Sentence-Transformers: [https://www.sbert.net/](https://www.sbert.net/)
- OpenCompass: [https://opencompass.org.cn/](https://opencompass.org.cn/)

### 11.3 é€²éšä¸»é¡Œ

**å¤šæ¨¡æ…‹æ•¸æ“šç¯©é¸**:
- åœ–æ–‡æŒ‡ä»¤æ•¸æ“šçš„è³ªé‡è©•ä¼°
- è·¨æ¨¡æ…‹ç›¸ä¼¼åº¦è¨ˆç®—

**å¼·åŒ–å­¸ç¿’æ•¸æ“šé¸æ“‡**:
- ä½¿ç”¨ RL å‹•æ…‹èª¿æ•´ç¯©é¸ç­–ç•¥
- åœ¨ç·šå­¸ç¿’èˆ‡æ•¸æ“šé¸æ“‡

**è¯é‚¦å­¸ç¿’å ´æ™¯**:
- åˆ†æ•£å¼æ•¸æ“šç¯©é¸
- éš±ç§ä¿è­·çš„æ•¸æ“šè³ªé‡è©•ä¼°

---

## ğŸ“ ç¸½çµ

æœ¬å¯¦é©—å°‡å¸¶ä½ å®Œæ•´æŒæ¡ LLM æ•¸æ“šå·¥ç¨‹çš„æ ¸å¿ƒæŠ€è¡“:

1. **ç†è«–ç†è§£**: IFDã€DEITAã€LESS çš„åŸç†èˆ‡æ‡‰ç”¨
2. **å¯¦ä½œèƒ½åŠ›**: å¾æ•¸æ“šç¯©é¸åˆ°é©—è­‰çš„å®Œæ•´æµç¨‹
3. **å·¥ç¨‹ç¶“é©—**: è‡ªå‹•åŒ–ç®¡ç·šå»ºç«‹èˆ‡å„ªåŒ–æŠ€å·§
4. **å¯¦é©—åˆ†æ**: æ•¸æ“šè³ªé‡å°æ¨¡å‹æ€§èƒ½çš„é‡åŒ–å½±éŸ¿

**æ ¸å¿ƒæ´å¯Ÿ**:
> "åœ¨ LLM è¨“ç·´ä¸­,æ•¸æ“šè³ªé‡å¾€å¾€æ¯”æ•¸é‡æ›´é‡è¦ã€‚é€šéç§‘å­¸çš„æ•¸æ“šç¯©é¸æ–¹æ³•,æˆ‘å€‘å¯ä»¥ç”¨ 30% çš„æ•¸æ“šé”åˆ°ç”šè‡³è¶…è¶Šå…¨é‡æ•¸æ“šçš„è¨“ç·´æ•ˆæœ,åŒæ™‚å¤§å¹…é™ä½è¨ˆç®—æˆæœ¬ã€‚"

**ä¸‹ä¸€æ­¥**:
- å˜—è©¦ä¸åŒçš„ç¯©é¸é–¾å€¼ (10%, 20%, 40%)
- æ‡‰ç”¨åˆ°è‡ªå·±çš„æ•¸æ“šé›†
- æ¢ç´¢å…¶ä»–æ•¸æ“šé¸æ“‡æ–¹æ³• (LESS, MoDS)
- å»ºç«‹ç”Ÿç”¢ç´šæ•¸æ“šç®¡ç·š

---

**ç¥å­¸ç¿’é †åˆ©!** ğŸ‰

å¦‚æœ‰å•é¡Œ,è«‹åƒè€ƒ:
- [ç†è«–æ–‡ä»¶: 4.2-Data_Engineering.md](../../01-Theory/4.2-Data_Engineering.md)
- [æ•…éšœæ’é™¤](#10-æ•…éšœæ’é™¤)
- [GitHub Issues](https://github.com/Zenobia000/iSpan_LLM-One-Piece/issues)
