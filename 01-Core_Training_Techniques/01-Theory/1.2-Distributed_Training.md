# 大模型訓練技術 1.2 分散式訓練 (Distributed Training)

本教學模組深入探討大型語言模型 (LLM) 分散式訓練的核心技術，涵蓋從基礎原理到實務應用的完整知識體系。

**🚨 重要環境限制**: 本課程設計考慮到單 GPU 環境限制，重點提供理論教學與架構分析。

| 層次 | 時間 | 學習目標 | 核心內容 | 產出 |
| :--- | :--- | :--- | :--- | :--- |
| **Fundamentals** | 基礎 | 理解分散式訓練基本原理 | 記憶體瓶頸與平行化策略 | 掌握基本概念與術語 |
| **First Principles** | 原理 | 掌握數學原理與通訊機制 | 通訊複雜度與最佳化理論 | 理解底層技術實現原理 |
| **Body of Knowledge** | 實務 | 分散式 LLM 訓練框架精通 | 主流框架 DeepSpeed/Megatron | 具備架構設計與調優能力 |

---

## 1. Fundamentals (基礎)

### 1.1 分散式訓練動機

大型語言模型面臨的根本挑戰：

1. **模型規模** 175B 參數的 GPT-3 需要約 700GB 記憶體 (FP32)
2. **記憶體限制** 單GPU記憶體容量有限，無法容納大型模型
3. **計算效率** 單GPU訓練耗時過長，影響研發效率

**分散式訓練**是將模型訓練任務分散到多個GPU上執行的技術，是訓練大型模型的核心解決方案。

### 1.2 平行化策略概覽

現代大模型訓練採用多種平行化技術：

#### 1.2.1 資料平行 (Data Parallelism, DP)

**基本概念**：資料集分割到不同GPU，模型複製到每個GPU。

**執行流程**
1. 每個GPU處理不同資料批次 (mini-batch)
2. GPU獨立進行前向和反向傳播
3. 同步梯度 (All-Reduce)
4. 更新模型參數
5. 複製更新後的參數到所有GPU

**優點**
- 實現簡單直觀
- 線性擴展性良好
- 適用於中小型模型

**限制**
- 每個GPU都需要完整模型，記憶體利用率低
- 通訊開銷隨GPU數量增加而增大
- 無法處理超大模型 (175B+)

**數學表示：**
假設有 $N$ 個GPU，每個GPU的批次大小為 $B$，則有效批次大小為 $B \times N$：
$$\nabla W = \frac{1}{N} \sum_{i=1}^{N} \nabla W_i$$

#### 1.2.2 模型平行 (Model Parallelism, MP)

**基本概念**：將模型的不同部分放置在不同GPU上。

##### 張量平行 (Tensor Parallelism)
將Transformer層內部的張量矩陣分割到多個GPU。

**注意力機制平行化**
- Multi-Head Attention的頭部分配到不同GPU
- 每個GPU計算部分注意力頭，最後聚合結果

**前饋網路平行化**
- FFN的權重矩陣分割
- 行切割或列切割策略

**數學表示：**
對於矩陣乘法 $Y = XW$，其中 $W \in \mathbb{R}^{d \times h}$
分割為 $W = [W_1, W_2, ..., W_p]$，則 $Y = X[W_1, W_2, ..., W_p] = [XW_1, XW_2, ..., XW_p]$

##### 流水線平行 (Pipeline Parallelism)
將模型的不同層分配到不同GPU，形成流水線執行。

**執行流程**
1. Transformer的層劃分為多個階段 (stage)
2. 每個GPU負責特定層的計算
3. 資料在GPU間流水線傳遞
4. 微批次 (micro-batch) 提高GPU利用率

#### 1.2.3 混合平行化策略

大型模型通常需要結合多種平行化技術。

**3D平行化 (3D Parallelism)**
- **資料平行** (Data) + **張量平行** (Tensor) + **流水線平行** (Pipeline)
- 線性擴展到數千個GPU

**ZeRO (Zero Redundancy Optimizer)**
- 消除優化器狀態的冗餘記憶體
- ZeRO-1：劃分優化器狀態
- ZeRO-2：劃分優化器狀態 + 梯度
- ZeRO-3：劃分優化器狀態 + 梯度 + 參數

#### 1.2.4 專家混合平行 (MoE Parallelism)

**基本概念**：Mixture of Experts (MoE) 模型的專門平行化策略。

**平行化策略**
- **專家平行**：專家分配到不同GPU
- **路由平行**：路由器採用張量平行
- **動態負載平衡**：處理專家之間的負載不均

---

## 2. First Principles (原理)

### 2.1 記憶體使用分析

#### 記憶體組成

大型語言模型的記憶體需求包括：

**模型參數 (Model Parameters)**
- FP32: $4 \times P$ bytes
- FP16/BF16: $2 \times P$ bytes
- INT8: $P$ bytes

**優化器狀態 (Optimizer States)**
- Adam 優化器：$8 \times P$ bytes (動量 + 方差)
- SGD：$4 \times P$ bytes (僅動量)

**梯度 (Gradients)**
- 與模型參數相同：$2 \times P$ bytes (FP16)

**激活值 (Activations)**
- 依賴批次大小和序列長度
- 約 $B \times S \times H \times L$ bytes

**總記憶體估算**
對於175B參數的模型（使用Adam優化器）：
$Memory = 2P + 8P + 2P + Activations = 12P + Activations \approx 2.1TB$

#### 通訊開銷

**資料平行通訊**
- All-Reduce操作需要 $2P$ bytes per iteration
- 通訊時間與 iteration 成正比

**模型平行通訊**
- 張量平行：層間需要同步中間結果
- 流水線平行：批次間需要傳遞激活值

### 2.2 平行化效率理論

#### Amdahl定律在分散式訓練中的應用

假設平行化比例為 $p$，使用 $n$ 個處理器：
$$Speedup = \frac{1}{(1-p) + \frac{p}{n}}$$

#### 通訊複雜度分析

**算術強度 (Arithmetic Intensity)**
$$AI = \frac{FLOPs}{Memory\_Access}$$

**通訊時間模型**
$$T_{comm} = \alpha + \beta \times Message\_Size$$
其中 $\alpha$ 為延遲，$\beta$ 為頻寬倒數

---

## 3. Body of Knowledge (實務知識)

### 3.1 主流分散式訓練框架

#### 3.1.1 PyTorch Distributed Data Parallel (DDP)

**技術特點**
- 基於資料平行的分散式框架
- 採用Ring All-Reduce進行梯度同步
- 原生支援GPU和CPU

**基本使用**
```python
# 初始化分散式環境
import torch.distributed as dist
import torch.nn.parallel

# 初始化進程組
dist.init_process_group("nccl")

# 包裝模型
model = torch.nn.parallel.DistributedDataParallel(model)

# 分散式資料載入
sampler = torch.utils.data.distributed.DistributedSampler(dataset)
dataloader = DataLoader(dataset, sampler=sampler)
```

**核心機制**
- **All-Reduce**：高效的梯度同步演算法
- **Bucket化**：梯度分組減少通訊次數
- **梯度壓縮**：減少通訊資料量

#### 3.1.2 Megatron-LM

**核心理念**：專為Transformer模型設計的模型平行框架。

**主要特性**
- **張量平行**：注意力和FFN平行化
- **流水線平行**：支援GPipe和PipeDream策略
- **序列平行**：LayerNorm和Dropout的平行化

**技術優勢**
- 針對Transformer架構優化
- 支援混合精度訓練
- 高度優化的GPU核心

#### 3.1.3 DeepSpeed

**設計目標**
- **ZeRO優化器**：消除記憶體冗餘
- **3D平行化**：資料+張量+流水線結合
- **異構訓練**：支援CPU+GPU混合

**ZeRO階段詳解**

**ZeRO-1 (Optimizer State Partitioning)**
- 將Adam等優化器狀態分片存儲
- 記憶體節省：$4 \times$ reduction

**ZeRO-2 (Gradient Partitioning)**
- 額外分片梯度存儲
- 記憶體節省：$8 \times$ reduction

**ZeRO-3 (Parameter Partitioning)**
- 分片模型參數本身
- 按需載入參數到GPU

**Offload機制**
- **ZeRO-Offload**：優化器狀態載入到CPU
- **ZeRO-Infinity**：支援NVMe SSD存儲

#### 3.1.4 Megatron-DeepSpeed整合

**整合優勢**
- 結合Megatron的模型平行和DeepSpeed的記憶體優化
- 支援超大規模模型訓練
- 優秀的可擴展性

### 3.2 進階平行化技術

#### 3.2.1 序列平行 (Sequence Parallelism)

**核心思想**：除了張量平行外，進一步分散序列維度的計算。

**實現細節**
- 將序列長度分割到多個GPU
- LayerNorm和Dropout等操作實現平行化
- 減少激活值記憶體佔用

#### 3.2.2 自動平行化

**4D平行化配置**
- **資料平行** (Data)
- **張量平行** (Tensor)
- **流水線平行** (Pipeline)
- **專家平行** (Expert)

**配置範例**
假設有1024個GPU的配置：
- 資料平行：4
- 張量平行：8
- 流水線平行：8
- 專家平行：4
- 總GPU數：4 × 8 × 8 × 4 = 1024

#### 3.2.3 智能平行化 (Auto Parallelism)

**核心理念**：自動決定最佳平行化策略。

**技術組件**
- **成本模型**：預測平行化配置的性能
- **搜索算法**：配置空間中尋找最優解
- **動態調整**：根據運行時資訊動態調整策略

---

## 4. 實際配置範例

### 4.1 硬體環境考量

#### GPU互連硬體

**NVLink**
- 頻寬：300GB/s (A100)
- 用途：節點內GPU互連
- 最適合：張量平行等需要高頻寬的策略

**InfiniBand**
- 頻寬：200Gb/s (HDR)
- 用途：節點間網路
- 最適合：資料平行等對延遲要求較低的配置

#### 記憶體層次設計

**GPU記憶體**
- HBM高頻寬記憶體：存儲模型參數和激活值
- 最適合：模型參數和中間結果

**CPU記憶體**
- DDR記憶體：優化器狀態存儲 (ZeRO-Offload)
- 最適合：減少CPU-GPU通訊開銷

**存儲系統**
- NVMe SSD：存儲模型檢查點
- 最適合：大規模檢查點存儲和載入

### 4.2 配置範例解析

#### DeepSpeed配置實例

```json
{
  "train_batch_size": 32,
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 8,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 1e-4,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "offload_param": {
      "device": "cpu",
      "pin_memory": true
    }
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "activation_checkpointing": {
    "partition_activations": true,
    "cpu_checkpointing": true,
    "contiguous_memory_optimization": false,
    "number_checkpoints": 4
  }
}
```

#### Megatron平行化配置

```bash
# 平行化配置參數
WORLD_SIZE=64
TENSOR_MODEL_PARALLEL_SIZE=8
PIPELINE_MODEL_PARALLEL_SIZE=8
DATA_PARALLEL_SIZE=1

# 關係驗證
# WORLD_SIZE = TENSOR_MP * PIPELINE_MP * DATA_MP
# 64 = 8 * 8 * 1
```

### 4.3 性能調優策略

#### 計算優化

**批次大小調整**
- 平衡計算效率
- 記憶體使用率
- 收斂性 vs 通訊開銷

**混合精度**
- FP16加速計算
- FP32保證數值穩定性
- 動態損失縮放

#### 通訊優化

**梯度壓縮**
- 減少通訊資料量
- 收斂性 vs 通訊效率

**重疊計算與通訊**
- 前向傳播使用FP16精度
- 通訊延遲隱藏技術

---

## 5. 分散式訓練趨勢

### 5.1 新興平行化技術

#### 專家混合模型 (MoE) 擴展

**Switch Transformer**
- 模型稀疏化
- 條件激活

**GLaM (Generalist Language Model)**
- 超大規模專家網路
- 高效推理

#### 異構平行化

**CPU+GPU協同**
- CPU負責控制流
- GPU專注計算核心

**GPU+TPU混合**
- 發揮各自優勢

### 5.2 自動化趨勢

#### 自動平行化框架

**XLA (Accelerated Linear Algebra)**
- 編譯時平行化策略
- 跨硬體平台優化

**TorchScript平行化**
- 靜態圖分析
- 自動平行化推理

#### 神經架構搜索平行化

**神經架構搜索 (NAS) for 平行化**
- 自動搜索最佳平行化配置
- 硬體感知的策略設計

---

## 6. 常見問題與故障排除

### 6.1 典型問題診斷

#### 記憶體問題

**現象**：GPU顯存不足錯誤
**原因**：模型太大或批次大小過大
**解決方案**：
- 採用ZeRO配置
- 降低批次大小
- 啟用CPU Offload

#### 收斂問題

**現象**：CUDA Out of Memory錯誤
**原因**：模型無法正確收斂
**解決方案**：
- 檢查梯度同步ZeRO配置
- 調整學習率和批次大小
- 檢查CPU Offload

#### 負載不均

**現象**：GPU利用率參差不齊
**原因**：資料分布不均或平行化策略不當
**解決方案**：
- 使用DistributedSampler確保資料均勻分布
- 調整流水線平行的批次切分
- 動態負載平衡策略

### 6.2 性能分析

#### 分散式指標

**吞吐量**
- **計算吞吐量**：samples/second或tokens/second
- **GPU利用率**：各GPU的計算利用率
- **記憶體效率**：記憶體使用率 vs 記憶體容量

**可擴展性**
- **強擴展性**：固定問題規模的可擴展性
- **弱擴展性**：平行度與問題規模等比例
- **通訊效率**：計算時間與通訊時間比例

#### 監控工具

**TensorBoard監控**
```python
# 分散式訓練監控
if torch.distributed.get_rank() == 0:
    writer.add_scalar('train/loss', loss.item(), step)
    writer.add_scalar('train/lr', scheduler.get_lr()[0], step)
```

**Profile分析工具**
- **PyTorch Profiler**：深入的記憶體分析
- **NVIDIA Nsight**：GPU核心級性能分析

---

## 總結

分散式訓練是超大型語言模型的基礎技術，解決了單GPU無法處理大模型的根本問題。通過資料平行、模型平行等多種策略的組合，現代框架能夠將模型訓練擴展到數千個GPU。

### 技術要點總結

1. **平行化策略選擇**：根據模型規模和硬體配置選擇合適的平行化方法
2. **記憶體優化**：ZeRO等技術顯著降低記憶體使用量
3. **通訊優化**：高效的梯度同步和通訊策略
4. **自動化**：配置自動化和問題診斷的重要性

### 未來發展方向

- **智能調度**：動態配置和自動平行化
- **異構整合**：多種硬體架構的協同
- **算法創新**：更高效的分散式訓練算法
- **自動平行化**：消除手動配置的複雜性

分散式訓練技術將持續演進，為更大規模的模型和更複雜的應用場景提供基礎支撐。

**🚨 單GPU環境限制說明**：
1. 掌握分散式訓練基本理論和原理
2. 理解主流框架的配置方法
3. 具備架構設計思維
4. 為未來多GPU環境應用奠定理論基礎

透過理論學習，您將具備扎實的分散式訓練知識基礎。