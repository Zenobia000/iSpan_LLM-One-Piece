# Lab-1.8: ORPO å°é½Šå„ªåŒ–
## Odds Ratio Preference Optimization (ORPO)

**å¯¦é©—å®¤é¡å‹**: æ¨¡å‹å°é½ŠæŠ€è¡“
**é›£åº¦ç­‰ç´š**: â­â­â­â­â­ (é«˜ç´š)
**é ä¼°æ™‚é–“**: 4-6å°æ™‚
**é©ç”¨GPU**: 16GB+ VRAM

---

## ğŸ“š å¯¦é©—å®¤æ¦‚è¿°

ORPO (Odds Ratio Preference Optimization) æ˜¯æ¯” DPO æ›´å…ˆé€²çš„å°é½ŠæŠ€è¡“ï¼Œé€šéå–®éšæ®µè¨“ç·´åŒæ™‚å®Œæˆ instruction tuning å’Œåå¥½å°é½Šï¼Œç„¡éœ€é å…ˆè¨“ç·´ SFT æ¨¡å‹ï¼Œå¤§å¹…ç°¡åŒ–å°é½Šæµç¨‹ä¸¦æå‡æ•ˆç‡ã€‚

### å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š
- âœ… ç†è§£ ORPO ç›¸æ¯” DPO çš„å‰µæ–°é»
- âœ… æŒæ¡ Odds Ratio æå¤±å‡½æ•¸åŸç†
- âœ… å¯¦ç¾å–®éšæ®µå°é½Šè¨“ç·´
- âœ… å°æ¯” SFT+DPO vs ORPO çš„æ•ˆæœ
- âœ… å„ªåŒ– ORPO è¨“ç·´è¶…åƒæ•¸

---

## ğŸ¯ ORPO æ ¸å¿ƒå‰µæ–°

### DPO çš„å±€é™

**DPO æµç¨‹**:
```
Phase 1: SFT (Supervised Fine-Tuning)
  â†’ è¨“ç·´åŸºç¤èƒ½åŠ›
  â†’ éœ€è¦ instruction dataset

Phase 2: DPO (Direct Preference Optimization)
  â†’ å°é½Šäººé¡åå¥½
  â†’ éœ€è¦ preference dataset

å•é¡Œ:
  - éœ€è¦å…©å€‹éšæ®µ
  - éœ€è¦å…©ç¨®æ•¸æ“šé›†
  - SFT å’Œ DPO ç›®æ¨™å¯èƒ½è¡çª
```

### ORPO çš„è§£æ±ºæ–¹æ¡ˆ

**ORPO å–®éšæ®µè¨“ç·´**:
```
Single Phase: ORPO
  â†’ åŒæ™‚å­¸ç¿’ instruction following + preference alignment
  â†’ åªéœ€è¦ preference dataset (åŒ…å« instruction)

å„ªå‹¢:
  âœ… å–®éšæ®µè¨“ç·´ (ç„¡éœ€ SFT)
  âœ… è¨“ç·´æ™‚é–“æ¸›å°‘ 50%
  âœ… é¿å… SFT-DPO ç›®æ¨™è¡çª
  âœ… çµ±ä¸€å„ªåŒ–ç›®æ¨™
```

### ORPO æå¤±å‡½æ•¸

**çµ„åˆæå¤±**:
```
L_ORPO = L_SFT + Î» Ã— L_OR

å…¶ä¸­:
  L_SFT = æ¨™æº–èªè¨€æ¨¡å‹æå¤± (å° chosen responses)
  L_OR = Odds Ratio åå¥½æå¤±
  Î» = å¹³è¡¡æ¬Šé‡ (é€šå¸¸ 0.1-1.0)
```

**Odds Ratio æå¤±**:
```python
# Odds Ratio: è¡¡é‡ chosen vs rejected çš„ç›¸å°æ©Ÿç‡
odds_ratio = (P(chosen) / (1 - P(chosen))) / (P(rejected) / (1 - P(rejected)))

# Log Odds Ratio
log_odds = log(P(chosen) / (1 - P(chosen))) - log(P(rejected) / (1 - P(rejected)))
         = log(P(chosen)) - log(P(rejected)) - log(1 - P(chosen)) + log(1 - P(rejected))

# ORPO æå¤± (æœ€å¤§åŒ– log odds)
L_OR = -log_sigmoid(log_odds)
```

### ORPO vs DPO å°æ¯”

| ç‰¹æ€§ | DPO | ORPO | å„ªå‹¢ |
|------|-----|------|------|
| **è¨“ç·´éšæ®µ** | 2éšæ®µ (SFT + DPO) | 1éšæ®µ | ORPO â¬† |
| **æ‰€éœ€æ•¸æ“š** | Instruction + Preference | Preference (å« instruction) | ORPO â¬† |
| **åƒè€ƒæ¨¡å‹** | éœ€è¦ (SFT model) | ä¸éœ€è¦ | ORPO â¬† |
| **è¨“ç·´æ™‚é–“** | 100% | 50-60% | ORPO â¬† |
| **è¨˜æ†¶é«”å ç”¨** | 2x (policy + ref) | 1x | ORPO â¬† |
| **å°é½Šæ•ˆæœ** | å¼· | æ›´å¼· | ORPO â¬† |
| **å¯¦ç¾è¤‡é›œåº¦** | ä¸­ç­‰ | ç°¡å–® | ORPO â¬† |

---

## ğŸ“‚ å¯¦é©—å®¤çµæ§‹

```
Lab-1.8-ORPO_Alignment/
â”œâ”€â”€ README.md                         # æœ¬æ–‡æª”
â”œâ”€â”€ 01-Setup.ipynb                   # ç’°å¢ƒè¨­ç½®èˆ‡æ•¸æ“šæº–å‚™
â”œâ”€â”€ 02-ORPO_Training.ipynb           # ORPO å–®éšæ®µè¨“ç·´
â”œâ”€â”€ 03-Compare_with_DPO.ipynb        # vs DPO å°æ¯”å¯¦é©—
â””â”€â”€ 04-Production_Deploy.ipynb       # ç”Ÿç”¢éƒ¨ç½²æŒ‡å—
```

---

## ğŸ”§ æŠ€è¡“åŸç†è©³è§£

### Odds Ratio æ•¸å­¸åŸç†

**å®šç¾©**:
```
Odds(äº‹ä»¶) = P(äº‹ä»¶ç™¼ç”Ÿ) / P(äº‹ä»¶ä¸ç™¼ç”Ÿ)
           = P / (1 - P)

Odds Ratio = Odds(chosen) / Odds(rejected)
```

**åœ¨ ORPO ä¸­çš„æ‡‰ç”¨**:
```python
# è¨ˆç®—æ¯å€‹ response çš„ odds
def compute_odds(logits, labels):
    """è¨ˆç®—åºåˆ—çš„ odds"""
    # P(sequence) = âˆ P(token_i | context)
    log_probs = F.log_softmax(logits, dim=-1)
    token_log_probs = torch.gather(log_probs, -1, labels.unsqueeze(-1)).squeeze(-1)

    # Log P(sequence)
    log_p = token_log_probs.sum(dim=-1)

    # Log odds = log(P / (1-P))
    # è¿‘ä¼¼: log_odds â‰ˆ log_p (when P << 1)
    log_odds = log_p - torch.log1p(-torch.exp(log_p))

    return log_odds

# ORPO åå¥½æå¤±
log_odds_chosen = compute_odds(logits_chosen, chosen_labels)
log_odds_rejected = compute_odds(logits_rejected, rejected_labels)

L_OR = -F.logsigmoid(log_odds_chosen - log_odds_rejected).mean()
```

### å®Œæ•´ ORPO ç®—æ³•

```python
def orpo_loss(model, batch, lambda_or=0.5):
    """
    ORPO æå¤±å‡½æ•¸

    Args:
        model: è¨“ç·´ä¸­çš„æ¨¡å‹
        batch: {'prompt', 'chosen', 'rejected'}
        lambda_or: OR æå¤±æ¬Šé‡

    Returns:
        total_loss, metrics
    """
    # 1. SFT æå¤± (æ¨™æº– LM loss on chosen)
    prompt_chosen = torch.cat([batch['prompt'], batch['chosen']], dim=1)
    outputs_chosen = model(prompt_chosen, labels=prompt_chosen)
    L_SFT = outputs_chosen.loss

    # 2. Odds Ratio æå¤±
    # Chosen
    chosen_logits = outputs_chosen.logits[:, batch['prompt'].size(1)-1:-1, :]
    chosen_labels = batch['chosen']
    log_odds_chosen = compute_odds(chosen_logits, chosen_labels)

    # Rejected
    prompt_rejected = torch.cat([batch['prompt'], batch['rejected']], dim=1)
    outputs_rejected = model(prompt_rejected)
    rejected_logits = outputs_rejected.logits[:, batch['prompt'].size(1)-1:-1, :]
    rejected_labels = batch['rejected']
    log_odds_rejected = compute_odds(rejected_logits, rejected_labels)

    # OR æå¤±
    L_OR = -F.logsigmoid(log_odds_chosen - log_odds_rejected).mean()

    # 3. ç¸½æå¤±
    total_loss = L_SFT + lambda_or * L_OR

    metrics = {
        'loss': total_loss.item(),
        'sft_loss': L_SFT.item(),
        'or_loss': L_OR.item(),
        'log_odds_margin': (log_odds_chosen - log_odds_rejected).mean().item()
    }

    return total_loss, metrics
```

---

## ğŸ“Š å¯¦é©—å…§å®¹è©³è§£

### Notebook 1: ç’°å¢ƒè¨­ç½® (01-Setup.ipynb)
**æ™‚é–“**: 30-45åˆ†é˜

**å…§å®¹**:
- ORPO ç’°å¢ƒé©—è­‰
- åå¥½æ•¸æ“šé›†æº–å‚™
- æ•¸æ“šé è™•ç†èˆ‡çµ±è¨ˆ

### Notebook 2: ORPO è¨“ç·´ (02-ORPO_Training.ipynb)
**æ™‚é–“**: 90-120åˆ†é˜

**å…§å®¹**:
- ORPO æå¤±å‡½æ•¸å¯¦ç¾
- å–®éšæ®µè¨“ç·´
- è¨“ç·´æŒ‡æ¨™ç›£æ§
- æ¨¡å‹æª¢æŸ¥é»ä¿å­˜

**é—œéµä»£ç¢¼**:
```python
from trl import ORPOTrainer, ORPOConfig

config = ORPOConfig(
    beta=0.1,              # OR æº«åº¦
    lambda_or=0.5,         # OR æå¤±æ¬Šé‡
    learning_rate=5e-6,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=1
)

trainer = ORPOTrainer(
    model=model,
    args=config,
    train_dataset=preference_dataset,
    tokenizer=tokenizer
)

trainer.train()
```

### Notebook 3: vs DPO å°æ¯” (03-Compare_with_DPO.ipynb)
**æ™‚é–“**: 60-75åˆ†é˜

**å°æ¯”ç¶­åº¦**:
- è¨“ç·´æ™‚é–“
- è¨˜æ†¶é«”å ç”¨
- å°é½Šæ•ˆæœ
- æ¨¡å‹è³ªé‡

### Notebook 4: ç”Ÿç”¢éƒ¨ç½² (04-Production_Deploy.ipynb)
**æ™‚é–“**: 45-60åˆ†é˜

**å…§å®¹**:
- æ¨¡å‹å°å‡ºèˆ‡é‡åŒ–
- æ¨ç†å„ªåŒ–
- éƒ¨ç½²æœ€ä½³å¯¦è¸
- æŒçºŒæ”¹é€²ç­–ç•¥

---

## ğŸ“ˆ æ€§èƒ½é æœŸ

### ORPO vs DPO vs SFT

| æ–¹æ³• | è¨“ç·´éšæ®µ | GPU æ™‚ | Win Rate | Helpfulness | ç›¸å°æˆæœ¬ |
|------|---------|-------|---------|-------------|---------|
| **SFT** | 1 | 20 | 50% | 3.5/5 | 20% |
| **SFT+DPO** | 2 | 40 | 68% | 4.1/5 | 100% |
| **ORPO** | 1 | 25 | 70% | 4.2/5 | 62% |

**ORPO å„ªå‹¢**:
- âœ… æ•ˆæœæœ€ä½³
- âœ… æˆæœ¬æœ€ä½ (å–®éšæ®µ)
- âœ… è¨“ç·´æœ€ç°¡å–®

---

## ğŸ’¡ æœ€ä½³å¯¦è¸

### è¶…åƒæ•¸å»ºè­°

```python
# æ¨è–¦é…ç½®
orpo_config = {
    'beta': 0.1,           # OR æº«åº¦ (0.05-0.2)
    'lambda_or': 0.5,      # OR æå¤±æ¬Šé‡ (0.1-1.0)
    'learning_rate': 5e-6, # è¼ƒå°å­¸ç¿’ç‡
    'warmup_ratio': 0.1,
    'max_length': 512
}
```

### è¨“ç·´æŠ€å·§

1. **Lambda èª¿å„ª**: æ§åˆ¶ SFT vs OR å¹³è¡¡
2. **ç›£æ§ odds margin**: æ‡‰è©²æŒçºŒå¢é•·
3. **æ—©åœç­–ç•¥**: é¿å…éæ“¬åˆ

---

## ğŸ“ å­¸ç¿’æª¢æŸ¥æ¸…å–®

- [ ] ç†è§£ Odds Ratio æ¦‚å¿µ
- [ ] å¯¦ç¾ ORPO æå¤±å‡½æ•¸
- [ ] è¨“ç·´ ORPO æ¨¡å‹
- [ ] å°æ¯” ORPO vs DPO æ•ˆæœ

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œ:
- æŒæ¡æœ€å…ˆé€²çš„å°é½ŠæŠ€è¡“
- å¯éƒ¨ç½²ç”Ÿç”¢ç´šå°é½Šæ¨¡å‹
- äº†è§£å°é½ŠæŠ€è¡“çš„å‰æ²¿ç™¼å±•

---

**å¯¦é©—å®¤ç‹€æ…‹**: ğŸ”„ é–‹ç™¼ä¸­
**æœ€å¾Œæ›´æ–°**: 2025-10-08

**ç›¸é—œè«–æ–‡**:
- [ORPO: Monolithic Preference Optimization](https://arxiv.org/abs/2403.07691) - 2024
- [DPO: Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
