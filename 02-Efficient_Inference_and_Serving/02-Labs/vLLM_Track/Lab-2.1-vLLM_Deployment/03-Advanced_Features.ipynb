{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1 Part 3: Advanced Features\n",
    "\n",
    "## Objectives\n",
    "- Understand Continuous Batching\n",
    "- Master advanced sampling strategies\n",
    "- Handle long context inputs\n",
    "- Manage multiple models\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import asyncio\n",
    "\n",
    "print(f\"vLLM: {vllm.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "# MODEL_NAME = \"facebook/opt-1.3b\"  # Alternative\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=2048,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"✅ Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Continuous Batching\n",
    "\n",
    "vLLM's killer feature: dynamic request scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Static Batching Problem\n",
    "\n",
    "```\n",
    "Batch: [Req1, Req2, Req3, Req4]\n",
    "\n",
    "Req1: ████████░░░░░░░░░░░░ (done at step 8, waits)\n",
    "Req2: ████████████░░░░░░░░ (done at step 12, waits)\n",
    "Req3: ██████████████████░░ (done at step 18, waits)\n",
    "Req4: ████████████████████ (done at step 20)\n",
    "       └── Must wait for slowest request ──┘\n",
    "\n",
    "Wasted time: ~40%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Batching Solution\n",
    "\n",
    "```\n",
    "Req1: ████████              (done, removed immediately)\n",
    "Req5:         ██████        (new request added)\n",
    "Req2: ████████████          (done, removed)\n",
    "Req6:             ████      (new request added)\n",
    "Req3: ██████████████████    (done, removed)\n",
    "Req4: ████████████████████\n",
    "\n",
    "Throughput: 2-3x higher!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate continuous batching with varied length requests\n",
    "varied_prompts = [\n",
    "    \"Hi\",  # Very short\n",
    "    \"What is Python?\",  # Short\n",
    "    \"Explain machine learning in detail:\",  # Medium\n",
    "    \"Write a comprehensive guide about artificial intelligence, covering history, techniques, and applications:\",  # Long\n",
    "]\n",
    "\n",
    "# Different max_tokens for each\n",
    "varied_params = [\n",
    "    SamplingParams(max_tokens=10, temperature=0.8),\n",
    "    SamplingParams(max_tokens=30, temperature=0.8),\n",
    "    SamplingParams(max_tokens=100, temperature=0.8),\n",
    "    SamplingParams(max_tokens=200, temperature=0.8),\n",
    "]\n",
    "\n",
    "print(\"Testing with varied-length requests...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "# vLLM automatically handles continuous batching\n",
    "outputs = llm.generate(varied_prompts, varied_params[0])  # Use same params for simplicity\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    tokens = len(output.outputs[0].token_ids)\n",
    "    print(f\"Request {i+1}: {tokens:3d} tokens\")\n",
    "\n",
    "print(f\"\\nTotal time: {elapsed:.2f}s\")\n",
    "print(\"\\n✅ Continuous batching handled varied lengths efficiently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure TTFT and ITL\n",
    "\n",
    "- **TTFT**: Time to First Token\n",
    "- **ITL**: Inter-Token Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TTFT/ITL measurement, we need streaming API (async)\n",
    "# This is a simplified demonstration\n",
    "\n",
    "test_prompt = \"Explain quantum computing:\"\n",
    "test_params = SamplingParams(\n",
    "    max_tokens=50,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "print(\"Measuring generation latency...\")\n",
    "start = time.time()\n",
    "output = llm.generate([test_prompt], test_params)[0]\n",
    "total_time = time.time() - start\n",
    "\n",
    "num_tokens = len(output.outputs[0].token_ids)\n",
    "avg_token_latency = total_time / num_tokens\n",
    "\n",
    "print(f\"\\nTotal time: {total_time:.3f}s\")\n",
    "print(f\"Tokens: {num_tokens}\")\n",
    "print(f\"Avg latency per token: {avg_token_latency*1000:.1f}ms\")\n",
    "print(f\"Throughput: {num_tokens/total_time:.1f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Advanced Sampling Strategies\n",
    "\n",
    "vLLM supports various sampling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Temperature and Top-p Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of artificial intelligence will\"\n",
    "\n",
    "# Different temperature values\n",
    "temperatures = [0.1, 0.5, 0.8, 1.2]\n",
    "\n",
    "print(\"Testing different temperatures:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for temp in temperatures:\n",
    "    params = SamplingParams(\n",
    "        temperature=temp,\n",
    "        max_tokens=30,\n",
    "    )\n",
    "    \n",
    "    output = llm.generate([prompt], params)[0]\n",
    "    text = output.outputs[0].text\n",
    "    \n",
    "    print(f\"Temperature {temp:.1f}:\")\n",
    "    print(f\"  {text}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n💡 Lower temperature = more deterministic\")\n",
    "print(\"💡 Higher temperature = more creative/random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-p (nucleus sampling)\n",
    "print(\"Testing different top_p values:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_p_values = [0.5, 0.8, 0.95, 1.0]\n",
    "\n",
    "for top_p in top_p_values:\n",
    "    params = SamplingParams(\n",
    "        temperature=0.8,\n",
    "        top_p=top_p,\n",
    "        max_tokens=30,\n",
    "    )\n",
    "    \n",
    "    output = llm.generate([prompt], params)[0]\n",
    "    text = output.outputs[0].text\n",
    "    \n",
    "    print(f\"Top-p {top_p:.2f}:\")\n",
    "    print(f\"  {text}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n💡 Lower top_p = more focused on likely tokens\")\n",
    "print(\"💡 Higher top_p = more diverse vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search for more deterministic output\n",
    "beam_params = SamplingParams(\n",
    "    n=3,  # Generate 3 candidates\n",
    "    best_of=3,  # Return best of 3\n",
    "    temperature=0.8,\n",
    "    max_tokens=50,\n",
    "    use_beam_search=False,  # vLLM uses sampling by default\n",
    ")\n",
    "\n",
    "print(\"Generating multiple candidates...\\n\")\n",
    "outputs = llm.generate([prompt], beam_params)\n",
    "\n",
    "print(f\"Generated {len(outputs[0].outputs)} outputs:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, completion in enumerate(outputs[0].outputs):\n",
    "    print(f\"\\nCandidate {i+1}:\")\n",
    "    print(completion.text)\n",
    "    print(f\"Cumulative logprob: {completion.cumulative_logprob:.2f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Repetition Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetition penalty to avoid repetitive text\n",
    "repetition_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=100,\n",
    "    repetition_penalty=1.2,  # Penalize repetitions\n",
    ")\n",
    "\n",
    "long_prompt = \"Machine learning is a field of artificial intelligence that\"\n",
    "\n",
    "print(\"Testing repetition penalty...\\n\")\n",
    "output = llm.generate([long_prompt], repetition_params)[0]\n",
    "print(output.outputs[0].text)\n",
    "print(\"\\n✅ Repetition penalty helps avoid repeated phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stop Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop generation at specific sequences\n",
    "stop_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    stop=[\"\\n\\n\", \"However\", \"In conclusion\"],  # Stop tokens\n",
    ")\n",
    "\n",
    "prompt_with_stop = \"Here are three benefits of exercise:\\n1.\"\n",
    "\n",
    "print(\"Testing stop sequences...\\n\")\n",
    "output = llm.generate([prompt_with_stop], stop_params)[0]\n",
    "print(f\"Prompt: {prompt_with_stop}\")\n",
    "print(f\"Generated: {output.outputs[0].text}\")\n",
    "print(f\"\\nStop reason: {output.outputs[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Long Context Handling\n",
    "\n",
    "Test vLLM with longer input contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a long context\n",
    "long_context = \"\"\"The history of artificial intelligence began in antiquity with myths and stories \n",
    "of artificial beings endowed with intelligence. Modern AI research started in the 1950s, when \n",
    "researchers began to explore the possibility that human intelligence could be so precisely \n",
    "described that a machine could simulate it. The field was founded on the claim that a central \n",
    "property of humans, intelligence—the sapience of Homo sapiens—can be so precisely described \n",
    "that it can be simulated by a machine.\n",
    "\n",
    "The early years of AI were marked by significant optimism. Researchers believed that machines \n",
    "would soon be able to perform any task that a human could. However, progress was slower than \n",
    "expected, and the field experienced several periods known as AI winters, during which funding \n",
    "and interest declined.\n",
    "\n",
    "In the 21st century, AI has experienced a renaissance, driven by advances in machine learning, \n",
    "particularly deep learning. Neural networks with many layers have proven remarkably effective \n",
    "at tasks like image recognition, natural language processing, and game playing.\n",
    "\n",
    "Based on the above history, answer: What caused the AI renaissance in the 21st century?\"\"\"\n",
    "\n",
    "print(f\"Context length: {len(long_context.split())} words\\n\")\n",
    "\n",
    "long_context_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(\"Processing long context...\\n\")\n",
    "start = time.time()\n",
    "output = llm.generate([long_context], long_context_params)[0]\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Answer: {output.outputs[0].text}\")\n",
    "print(f\"\\nProcessing time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Maximum Context Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test near max context (2048 tokens)\n",
    "# Generate a very long prompt\n",
    "repeated_text = \"The quick brown fox jumps over the lazy dog. \" * 100  # ~1000 words\n",
    "\n",
    "max_context_prompt = repeated_text + \"\\n\\nSummarize the above text:\"\n",
    "\n",
    "print(f\"Testing with ~1000 word context...\")\n",
    "print(f\"Estimated tokens: ~1500\\n\")\n",
    "\n",
    "try:\n",
    "    start = time.time()\n",
    "    output = llm.generate([max_context_prompt], long_context_params)[0]\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"✅ Success!\")\n",
    "    print(f\"Processing time: {elapsed:.2f}s\")\n",
    "    print(f\"Output: {output.outputs[0].text[:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Context might be too long for current max_model_len setting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Multi-Model Management\n",
    "\n",
    "Load and switch between multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Loading multiple models simultaneously requires sufficient GPU memory\n",
    "# For demonstration, we'll load one small model\n",
    "\n",
    "print(\"Loading a second model (GPT-2)...\")\n",
    "small_model = LLM(\n",
    "    model=\"gpt2\",\n",
    "    gpu_memory_utilization=0.2,\n",
    "    max_model_len=512,\n",
    ")\n",
    "print(\"✅ Second model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outputs from different models\n",
    "comparison_prompt = \"The future of AI is\"\n",
    "comparison_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=50,\n",
    ")\n",
    "\n",
    "print(\"Comparing model outputs:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Large model\n",
    "large_output = llm.generate([comparison_prompt], comparison_params)[0]\n",
    "print(f\"Llama-2-7B:\")\n",
    "print(f\"  {large_output.outputs[0].text}\")\n",
    "print()\n",
    "\n",
    "# Small model\n",
    "small_output = small_model.generate([comparison_prompt], comparison_params)[0]\n",
    "print(f\"GPT-2 (124M):\")\n",
    "print(f\"  {small_output.outputs[0].text}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n💡 Larger models generally produce more coherent outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(prompt: str, complexity: str = \"auto\"):\n",
    "    \"\"\"\n",
    "    Select model based on task complexity.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        complexity: 'simple', 'complex', or 'auto'\n",
    "    \"\"\"\n",
    "    if complexity == \"auto\":\n",
    "        # Simple heuristic: check prompt length and keywords\n",
    "        if len(prompt.split()) > 50 or any(kw in prompt.lower() for kw in \n",
    "                                            ['explain', 'analyze', 'complex', 'detail']):\n",
    "            complexity = \"complex\"\n",
    "        else:\n",
    "            complexity = \"simple\"\n",
    "    \n",
    "    if complexity == \"complex\":\n",
    "        return llm, \"Llama-2-7B\"\n",
    "    else:\n",
    "        return small_model, \"GPT-2\"\n",
    "\n",
    "# Test model selection\n",
    "test_prompts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Explain the theory of relativity in detail:\",\n",
    "]\n",
    "\n",
    "print(\"Testing automatic model selection:\\n\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    model, model_name = select_model(prompt)\n",
    "    output = model.generate([prompt], comparison_params)[0]\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Selected: {model_name}\")\n",
    "    print(f\"Output: {output.outputs[0].text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Streaming Output (Conceptual)\n",
    "\n",
    "vLLM supports streaming for real-time token generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming with AsyncLLMEngine\n",
    "\n",
    "For production streaming, use `AsyncLLMEngine`:\n",
    "\n",
    "```python\n",
    "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "\n",
    "async def stream_generate(prompt: str):\n",
    "    async for output in engine.generate(prompt, sampling_params):\n",
    "        # Process token as it's generated\n",
    "        yield output\n",
    "```\n",
    "\n",
    "This enables:\n",
    "- Real-time token output (typewriter effect)\n",
    "- Lower perceived latency\n",
    "- Better user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate streaming by showing tokens progressively\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "streaming_prompt = \"Write a short poem about AI:\"\n",
    "streaming_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {streaming_prompt}\\n\")\n",
    "print(\"Generating (simulated streaming):\\n\")\n",
    "\n",
    "output = llm.generate([streaming_prompt], streaming_params)[0]\n",
    "full_text = output.outputs[0].text\n",
    "\n",
    "# Simulate token-by-token display\n",
    "words = full_text.split()\n",
    "displayed = \"\"\n",
    "\n",
    "for word in words:\n",
    "    displayed += word + \" \"\n",
    "    print(f\"\\r{displayed}\", end=\"\", flush=True)\n",
    "    time.sleep(0.1)  # Simulate generation delay\n",
    "\n",
    "print(\"\\n\\n✅ Streaming simulation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Performance Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance test\n",
    "def run_benchmark(\n",
    "    model,\n",
    "    num_prompts: int = 10,\n",
    "    max_tokens: int = 50,\n",
    ") -> dict:\n",
    "    \"\"\"Run benchmark and return metrics.\"\"\"\n",
    "    prompts = [f\"Test prompt {i}: Tell me about topic {i}.\" for i in range(num_prompts)]\n",
    "    params = SamplingParams(temperature=0.8, max_tokens=max_tokens)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = model.generate([prompts[0]], params)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    outputs = model.generate(prompts, params)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Metrics\n",
    "    total_tokens = sum(len(o.outputs[0].token_ids) for o in outputs)\n",
    "    \n",
    "    return {\n",
    "        'num_prompts': num_prompts,\n",
    "        'total_time': elapsed,\n",
    "        'total_tokens': total_tokens,\n",
    "        'throughput': total_tokens / elapsed,\n",
    "        'time_per_prompt': elapsed / num_prompts,\n",
    "    }\n",
    "\n",
    "print(\"Running comprehensive benchmark...\\n\")\n",
    "\n",
    "results = run_benchmark(llm, num_prompts=10, max_tokens=50)\n",
    "\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Prompts processed:    {results['num_prompts']}\")\n",
    "print(f\"Total time:           {results['total_time']:.2f}s\")\n",
    "print(f\"Time per prompt:      {results['time_per_prompt']:.3f}s\")\n",
    "print(f\"Total tokens:         {results['total_tokens']}\")\n",
    "print(f\"Throughput:           {results['throughput']:.1f} tokens/s\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ **Completed**:\n",
    "1. Explored Continuous Batching benefits\n",
    "2. Mastered advanced sampling strategies:\n",
    "   - Temperature and top-p\n",
    "   - Beam search\n",
    "   - Repetition penalty\n",
    "   - Stop sequences\n",
    "3. Tested long context handling\n",
    "4. Managed multiple models\n",
    "5. Understood streaming concepts\n",
    "6. Ran comprehensive benchmarks\n",
    "\n",
    "📊 **Key Takeaways**:\n",
    "- Continuous batching improves throughput 2-3x\n",
    "- Sampling strategies greatly affect output quality\n",
    "- vLLM handles long contexts efficiently\n",
    "- Multiple models can serve different use cases\n",
    "\n",
    "➡️ **Next**: In `04-Production_Deployment.ipynb`, we'll learn:\n",
    "- Deploy OpenAI-compatible API server\n",
    "- Performance tuning for production\n",
    "- Monitoring and logging\n",
    "- Deployment best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "del llm, small_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
