{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Prefix Tuning - Deployment Guide\n",
    "\n",
    "## 🎯 實驗目標\n",
    "\n",
    "本 Notebook 探討 **Prefix Tuning 在生產環境中的部署策略**。與 LoRA 和 IA³ 等可完全合併的方法不同,Prefix Tuning 的前綴參數**無法直接合併到基礎模型中**,因此需要特殊的部署方案。\n",
    "\n",
    "### 關鍵學習要點\n",
    "- 理解 Prefix Tuning 無法合併的技術原因\n",
    "- 掌握前綴參數的優化與壓縮技術\n",
    "- 實現高效的 Adapter 載入與推理部署\n",
    "- 分析不同部署場景下的最佳實踐\n",
    "- 學習多任務前綴的管理策略\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prefix Tuning 部署特性分析\n",
    "\n",
    "### 1.1 為什麼 Prefix Tuning 無法合併?\n",
    "\n",
    "**技術原因**:\n",
    "- **結構性修改**: Prefix 在每一層的 Key/Value 都注入新的向量,改變了注意力機制的計算結構\n",
    "- **動態注入**: Prefix 向量在推理時動態拼接到原始序列,無法靜態融合到權重矩陣\n",
    "- **位置依賴**: Prefix 位置對模型行為有顯著影響,合併會破壞這種位置關係\n",
    "\n",
    "**與可合併方法的對比**:\n",
    "\n",
    "| 方法 | 是否可合併 | 原理 | 推理開銷 |\n",
    "|:---|:---|:---|:---|\n",
    "| **LoRA** | ✅ 可合併 | `W' = W + B·A` 矩陣加法 | 零開銷 |\n",
    "| **IA³** | ✅ 可合併 | `W' = W ⊙ scale` 元素乘法 | 零開銷 |\n",
    "| **Prefix Tuning** | ❌ **不可合併** | 動態 KV 注入 | **有開銷** |\n",
    "| **Prompt Tuning** | ❌ 不可合併 | 輸入序列拼接 | 有開銷 |\n",
    "| **Adapter** | ❌ 不可合併 | 新增模組層 | 有開銷 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 環境準備與模型載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要的庫\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel, \n",
    "    PrefixTuningConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 設定隨機種子\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 載入訓練好的 Prefix Tuning 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型路徑設定\n",
    "base_model_name = \"gpt2\"\n",
    "adapter_path = \"./gpt2-prefix-tuning-imdb\"  # 假設這是訓練好的 adapter 路徑\n",
    "\n",
    "# 載入基礎模型和 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"基礎模型參數量: {base_model.num_parameters():,}\")\n",
    "print(f\"基礎模型記憶體佔用: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 Prefix Tuning adapter (如果存在)\n",
    "if os.path.exists(adapter_path):\n",
    "    # 找到最新的 checkpoint\n",
    "    checkpoints = [d for d in os.listdir(adapter_path) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(\n",
    "            [os.path.join(adapter_path, d) for d in checkpoints],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
    "        print(f\"成功載入 Prefix Tuning adapter: {latest_checkpoint}\")\n",
    "    else:\n",
    "        peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        print(f\"成功載入 Prefix Tuning adapter: {adapter_path}\")\n",
    "else:\n",
    "    # 如果沒有訓練好的 adapter,創建示範配置\n",
    "    print(\"未找到訓練好的 adapter,創建示範 Prefix Tuning 配置...\")\n",
    "    \n",
    "    prefix_config = PrefixTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        num_virtual_tokens=20,\n",
    "        prefix_projection=True,\n",
    "        encoder_hidden_size=768\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(base_model, prefix_config)\n",
    "    print(\"創建了示範 Prefix Tuning 模型\")\n",
    "\n",
    "# 顯示可訓練參數統計\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prefix Tuning 結構分析\n",
    "\n",
    "### 3.1 前綴參數結構探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prefix_structure(model):\n",
    "    \"\"\"分析 Prefix Tuning 的結構和參數\"\"\"\n",
    "    prefix_params = {}\n",
    "    total_prefix_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prefix' in name.lower() or 'prompt' in name.lower():\n",
    "            prefix_params[name] = {\n",
    "                'shape': param.shape,\n",
    "                'dtype': param.dtype,\n",
    "                'requires_grad': param.requires_grad,\n",
    "                'device': param.device,\n",
    "                'num_params': param.numel()\n",
    "            }\n",
    "            total_prefix_params += param.numel()\n",
    "            \n",
    "            print(f\"Prefix 參數: {name}\")\n",
    "            print(f\"  形狀: {param.shape}\")\n",
    "            print(f\"  數據類型: {param.dtype}\")\n",
    "            print(f\"  參數量: {param.numel():,}\")\n",
    "            print()\n",
    "    \n",
    "    print(f\"總 Prefix 參數量: {total_prefix_params:,}\")\n",
    "    print(f\"佔基礎模型參數比例: {total_prefix_params / base_model.num_parameters() * 100:.4f}%\")\n",
    "    \n",
    "    return prefix_params\n",
    "\n",
    "prefix_structure = analyze_prefix_structure(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 前綴參數記憶體分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prefix_memory(prefix_params):\n",
    "    \"\"\"計算前綴參數的記憶體佔用\"\"\"\n",
    "    total_memory = 0\n",
    "    \n",
    "    for name, info in prefix_params.items():\n",
    "        # 根據數據類型計算記憶體\n",
    "        dtype_size = {\n",
    "            torch.float32: 4,\n",
    "            torch.float16: 2,\n",
    "            torch.bfloat16: 2,\n",
    "            torch.int32: 4,\n",
    "            torch.int64: 8\n",
    "        }\n",
    "        \n",
    "        size_bytes = info['num_params'] * dtype_size.get(info['dtype'], 4)\n",
    "        total_memory += size_bytes\n",
    "        \n",
    "        print(f\"{name}: {size_bytes / 1024:.2f} KB\")\n",
    "    \n",
    "    print(f\"\\n總前綴參數記憶體: {total_memory / 1024:.2f} KB ({total_memory / 1024**2:.4f} MB)\")\n",
    "    return total_memory\n",
    "\n",
    "prefix_memory = calculate_prefix_memory(prefix_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 推理性能基準測試\n",
    "\n",
    "### 4.1 性能測試函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, test_prompts, num_runs=5):\n",
    "    \"\"\"推理性能基準測試\"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                if run == 0:  # 只在第一輪保存結果\n",
    "                    results.append({\n",
    "                        'prompt': prompt,\n",
    "                        'generated': generated_text[len(prompt):].strip()\n",
    "                    })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    return avg_time, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 執行推理測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試提示詞\n",
    "test_prompts = [\n",
    "    \"This movie was absolutely\",\n",
    "    \"I think this film was\",\n",
    "    \"The acting in this movie\",\n",
    "    \"Overall, my impression is\"\n",
    "]\n",
    "\n",
    "print(\"=== 帶 Prefix 的推理性能測試 ===\")\n",
    "prefix_time, prefix_results = benchmark_inference(peft_model, tokenizer, test_prompts)\n",
    "print(f\"平均推理時間: {prefix_time:.4f} 秒\")\n",
    "print(f\"記憶體使用: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== 生成示例 ===\")\n",
    "for i, result in enumerate(prefix_results[:2]):  # 只顯示前兩個示例\n",
    "    print(f\"提示詞 {i+1}: {result['prompt']}\")\n",
    "    print(f\"生成: {result['generated']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 與基礎模型對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 基礎模型推理性能測試 ===\")\n",
    "base_time, base_results = benchmark_inference(base_model, tokenizer, test_prompts)\n",
    "print(f\"平均推理時間: {base_time:.4f} 秒\")\n",
    "print(f\"記憶體使用: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "# 計算性能開銷\n",
    "time_overhead = (prefix_time - base_time) / base_time * 100\n",
    "memory_overhead = (peft_model.get_memory_footprint() - base_model.get_memory_footprint()) / base_model.get_memory_footprint() * 100\n",
    "\n",
    "print(\"\\n=== 性能開銷分析 ===\")\n",
    "print(f\"推理時間增加: {time_overhead:.2f}%\")\n",
    "print(f\"記憶體使用增加: {memory_overhead:.2f}%\")\n",
    "\n",
    "print(\"\\n=== 基礎模型生成示例 ===\")\n",
    "for i, result in enumerate(base_results[:2]):\n",
    "    print(f\"提示詞 {i+1}: {result['prompt']}\")\n",
    "    print(f\"生成: {result['generated']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prefix 參數優化與壓縮\n",
    "\n",
    "### 5.1 移除訓練時的 MLP 重參數化層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_prefix_for_inference(model):\n",
    "    \"\"\"\n",
    "    優化 Prefix Tuning 模型以進行推理\n",
    "    在訓練階段,使用 MLP 重參數化來穩定訓練\n",
    "    在推理階段,可以移除 MLP,直接使用預計算的 prefix\n",
    "    \"\"\"\n",
    "    print(\"=== Prefix 推理優化 ===\")\n",
    "    print(\"注意: 此步驟通常在保存模型時自動完成\")\n",
    "    print(\"PEFT 庫會自動處理 MLP 的移除和 prefix 的預計算\")\n",
    "    \n",
    "    # 顯示當前模型結構\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n總參數量: {total_params:,}\")\n",
    "    print(f\"可訓練參數量: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "optimized_model = optimize_prefix_for_inference(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prefix 參數量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_prefix_parameters(model, target_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    量化前綴參數以減少記憶體佔用\n",
    "    注意: 這會修改模型,請確保已保存原始版本\n",
    "    \"\"\"\n",
    "    print(f\"=== 量化 Prefix 參數到 {target_dtype} ===\")\n",
    "    \n",
    "    quantized_params = 0\n",
    "    original_memory = 0\n",
    "    quantized_memory = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prefix' in name.lower() or 'prompt' in name.lower():\n",
    "            original_dtype = param.dtype\n",
    "            original_size = param.numel() * param.element_size()\n",
    "            \n",
    "            # 量化參數\n",
    "            param.data = param.data.to(target_dtype)\n",
    "            \n",
    "            quantized_size = param.numel() * param.element_size()\n",
    "            quantized_params += 1\n",
    "            original_memory += original_size\n",
    "            quantized_memory += quantized_size\n",
    "            \n",
    "            print(f\"量化 {name}: {original_dtype} -> {target_dtype}\")\n",
    "            print(f\"  記憶體: {original_size / 1024:.2f} KB -> {quantized_size / 1024:.2f} KB\")\n",
    "    \n",
    "    compression_ratio = (original_memory - quantized_memory) / original_memory * 100\n",
    "    print(f\"\\n量化了 {quantized_params} 個 prefix 參數\")\n",
    "    print(f\"記憶體壓縮率: {compression_ratio:.2f}%\")\n",
    "    print(f\"節省記憶體: {(original_memory - quantized_memory) / 1024:.2f} KB\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 演示量化 (實際使用時請謹慎,可能影響性能)\n",
    "print(\"注意: 量化會修改模型,這裡僅作演示\")\n",
    "print(\"實際部署時請先在驗證集上測試量化後的性能\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型保存與部署\n",
    "\n",
    "### 6.1 保存 Prefix Tuning Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定保存路徑\n",
    "save_path = \"./gpt2-prefix-tuning-deployed\"\n",
    "\n",
    "print(f\"=== 保存 Prefix Tuning Adapter 到: {save_path} ===\")\n",
    "\n",
    "# 保存 PEFT adapter (只保存前綴參數)\n",
    "peft_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"✅ Adapter 保存成功!\")\n",
    "\n",
    "# 檢查保存的文件\n",
    "saved_files = os.listdir(save_path)\n",
    "print(f\"\\n保存的文件: {saved_files}\")\n",
    "\n",
    "# 計算 adapter 大小\n",
    "adapter_size = 0\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(save_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        adapter_size += size\n",
    "        print(f\"  {file}: {size / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nAdapter 總大小: {adapter_size / 1024:.2f} KB ({adapter_size / 1024**2:.4f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 驗證保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 驗證保存的 Adapter ===\")\n",
    "\n",
    "# 重新載入基礎模型\n",
    "fresh_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# 從保存路徑載入 adapter\n",
    "reloaded_model = PeftModel.from_pretrained(fresh_base_model, save_path)\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "print(f\"✅ 成功載入保存的 adapter\")\n",
    "\n",
    "# 快速測試\n",
    "test_prompt = \"This movie is absolutely\"\n",
    "inputs = reloaded_tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=reloaded_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = reloaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\n測試生成:\")\n",
    "print(f\"提示詞: {test_prompt}\")\n",
    "print(f\"生成: {generated_text[len(test_prompt):].strip()}\")\n",
    "print(\"\\n✅ 保存的 adapter 運作正常!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 生產環境部署策略\n",
    "\n",
    "### 7.1 部署架構選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_strategies = {\n",
    "    \"單任務部署\": {\n",
    "        \"描述\": \"一個服務只服務一個任務\",\n",
    "        \"優點\": [\n",
    "            \"部署簡單\",\n",
    "            \"推理速度最快\",\n",
    "            \"資源隔離\"\n",
    "        ],\n",
    "        \"缺點\": [\n",
    "            \"資源利用率低\",\n",
    "            \"需要多個模型實例\"\n",
    "        ],\n",
    "        \"適用場景\": \"單一任務、高QPS需求\"\n",
    "    },\n",
    "    \"多任務共享基礎模型\": {\n",
    "        \"描述\": \"共享基礎模型,動態切換 prefix\",\n",
    "        \"優點\": [\n",
    "            \"記憶體效率高\",\n",
    "            \"支持多任務\",\n",
    "            \"prefix 切換開銷小\"\n",
    "        ],\n",
    "        \"缺點\": [\n",
    "            \"需要 prefix 管理邏輯\",\n",
    "            \"批次處理複雜\"\n",
    "        ],\n",
    "        \"適用場景\": \"多任務、中等QPS\"\n",
    "    },\n",
    "    \"批次推理優化\": {\n",
    "        \"描述\": \"相同任務批次處理\",\n",
    "        \"優點\": [\n",
    "            \"吞吐量最大化\",\n",
    "            \"GPU 利用率高\"\n",
    "        ],\n",
    "        \"缺點\": [\n",
    "            \"延遲增加\",\n",
    "            \"需要請求排隊\"\n",
    "        ],\n",
    "        \"適用場景\": \"離線推理、高吞吐量需求\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== Prefix Tuning 部署策略對比 ===\")\n",
    "for strategy, details in deployment_strategies.items():\n",
    "    print(f\"\\n【{strategy}】\")\n",
    "    print(f\"描述: {details['描述']}\")\n",
    "    print(f\"優點: {', '.join(details['優點'])}\")\n",
    "    print(f\"缺點: {', '.join(details['缺點'])}\")\n",
    "    print(f\"適用場景: {details['適用場景']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 多任務 Prefix 管理示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskPrefixManager:\n",
    "    \"\"\"\n",
    "    多任務 Prefix 管理器\n",
    "    用於在生產環境中管理和切換不同任務的 prefix\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, tokenizer):\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_adapters = {}  # 任務名稱 -> adapter 路徑\n",
    "        self.current_task = None\n",
    "        self.current_model = None\n",
    "        \n",
    "    def register_task(self, task_name, adapter_path):\n",
    "        \"\"\"註冊一個任務的 adapter\"\"\"\n",
    "        self.task_adapters[task_name] = adapter_path\n",
    "        print(f\"註冊任務: {task_name} -> {adapter_path}\")\n",
    "        \n",
    "    def switch_task(self, task_name):\n",
    "        \"\"\"切換到指定任務\"\"\"\n",
    "        if task_name not in self.task_adapters:\n",
    "            raise ValueError(f\"未知任務: {task_name}\")\n",
    "            \n",
    "        if task_name == self.current_task:\n",
    "            print(f\"已經在任務 {task_name} 上,無需切換\")\n",
    "            return\n",
    "            \n",
    "        print(f\"切換到任務: {task_name}\")\n",
    "        adapter_path = self.task_adapters[task_name]\n",
    "        \n",
    "        # 載入對應的 adapter\n",
    "        self.current_model = PeftModel.from_pretrained(\n",
    "            self.base_model, \n",
    "            adapter_path\n",
    "        )\n",
    "        self.current_task = task_name\n",
    "        \n",
    "    def generate(self, prompt, task_name=None, **generate_kwargs):\n",
    "        \"\"\"使用指定任務的 prefix 生成文本\"\"\"\n",
    "        if task_name:\n",
    "            self.switch_task(task_name)\n",
    "            \n",
    "        if self.current_model is None:\n",
    "            raise RuntimeError(\"請先切換到一個任務\")\n",
    "            \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = self.current_model.generate(\n",
    "                **inputs,\n",
    "                **generate_kwargs\n",
    "            )\n",
    "            \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def list_tasks(self):\n",
    "        \"\"\"列出所有已註冊的任務\"\"\"\n",
    "        return list(self.task_adapters.keys())\n",
    "\n",
    "# 演示使用\n",
    "print(\"=== 多任務 Prefix 管理器示例 ===\")\n",
    "print(\"\\n這是一個管理多個任務 prefix 的框架\")\n",
    "print(\"在生產環境中,可以預先載入多個任務的 adapter\")\n",
    "print(\"根據請求動態切換,無需重新載入整個模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 生產環境部署配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建部署配置文件\n",
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"peft_method\": \"Prefix Tuning\",\n",
    "        \"num_virtual_tokens\": 20,\n",
    "        \"mergeable\": False,\n",
    "        \"adapter_size_mb\": adapter_size / 1024**2\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"inference_time_seconds\": prefix_time,\n",
    "        \"vs_base_model_overhead_percent\": time_overhead,\n",
    "        \"memory_overhead_percent\": memory_overhead,\n",
    "        \"adapter_loading_time_seconds\": \"< 1s (estimated)\"\n",
    "    },\n",
    "    \"deployment_requirements\": {\n",
    "        \"python_packages\": [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.20.0\",\n",
    "            \"peft>=0.3.0\",\n",
    "            \"tokenizers>=0.12.0\"\n",
    "        ],\n",
    "        \"minimum_gpu_memory_gb\": 4,\n",
    "        \"recommended_gpu_memory_gb\": 8\n",
    "    },\n",
    "    \"deployment_notes\": {\n",
    "        \"adapter_management\": \"需要同時管理基礎模型和 adapter\",\n",
    "        \"inference_overhead\": \"存在推理開銷,不可消除\",\n",
    "        \"multi_task_support\": \"可以共享基礎模型,動態切換 adapter\",\n",
    "        \"scalability\": \"適合多任務場景,單任務建議考慮 LoRA 或 IA³\"\n",
    "    },\n",
    "    \"usage_example\": {\n",
    "        \"load_base_model\": f\"base_model = AutoModelForCausalLM.from_pretrained('{base_model_name}')\",\n",
    "        \"load_adapter\": f\"model = PeftModel.from_pretrained(base_model, '{save_path}')\",\n",
    "        \"inference\": \"model.generate(...)\"\n",
    "    },\n",
    "    \"optimization_tips\": [\n",
    "        \"使用 FP16 或 BF16 減少記憶體佔用\",\n",
    "        \"相同任務的請求批次處理\",\n",
    "        \"預載入常用任務的 adapter\",\n",
    "        \"使用 KV Cache 加速生成\",\n",
    "        \"考慮使用 vLLM 等推理引擎優化\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 保存配置文件\n",
    "config_path = os.path.join(save_path, \"deployment_config.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=== 生產環境部署配置 ===\")\n",
    "print(json.dumps(deployment_config, indent=2, ensure_ascii=False))\n",
    "print(f\"\\n配置文件已保存到: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prefix Tuning 與其他 PEFT 方法的部署對比\n",
    "\n",
    "### 8.1 部署特性對比矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_deployment_comparison = {\n",
    "    \"LoRA\": {\n",
    "        \"可合併性\": \"✅ 可完全合併\",\n",
    "        \"推理開銷\": \"零開銷 (合併後)\",\n",
    "        \"部署複雜度\": \"低 (單一模型文件)\",\n",
    "        \"多任務支持\": \"需要多個合併後的模型\",\n",
    "        \"適用場景\": \"單任務、追求極致性能\"\n",
    "    },\n",
    "    \"IA³\": {\n",
    "        \"可合併性\": \"✅ 可完全合併\",\n",
    "        \"推理開銷\": \"零開銷 (合併後)\",\n",
    "        \"部署複雜度\": \"低 (單一模型文件)\",\n",
    "        \"多任務支持\": \"需要多個合併後的模型\",\n",
    "        \"適用場景\": \"極致效率、資源受限\"\n",
    "    },\n",
    "    \"Prefix Tuning\": {\n",
    "        \"可合併性\": \"❌ 不可合併\",\n",
    "        \"推理開銷\": \"有開銷 (KV 注入)\",\n",
    "        \"部署複雜度\": \"中 (基礎模型 + adapter)\",\n",
    "        \"多任務支持\": \"✅ 共享基礎模型\",\n",
    "        \"適用場景\": \"多任務、靈活切換\"\n",
    "    },\n",
    "    \"Prompt Tuning\": {\n",
    "        \"可合併性\": \"❌ 不可合併\",\n",
    "        \"推理開銷\": \"有開銷 (輸入拼接)\",\n",
    "        \"部署複雜度\": \"中 (基礎模型 + prompts)\",\n",
    "        \"多任務支持\": \"✅ 共享基礎模型\",\n",
    "        \"適用場景\": \"超大模型、極簡 adapter\"\n",
    "    },\n",
    "    \"Adapter Layers\": {\n",
    "        \"可合併性\": \"❌ 不可合併\",\n",
    "        \"推理開銷\": \"有開銷 (額外層)\",\n",
    "        \"部署複雜度\": \"高 (模型結構修改)\",\n",
    "        \"多任務支持\": \"✅ 模組化切換\",\n",
    "        \"適用場景\": \"NLU 任務、模組化設計\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== PEFT 方法部署特性對比 ===\")\n",
    "print(f\"{'方法':<15} {'可合併性':<15} {'推理開銷':<20} {'部署複雜度':<25} {'多任務支持':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for method, features in peft_deployment_comparison.items():\n",
    "    print(f\"{method:<15} {features['可合併性']:<15} {features['推理開銷']:<20} {features['部署複雜度']:<25} {features['多任務支持']:<20}\")\n",
    "\n",
    "print(\"\\n=== Prefix Tuning 部署定位 ===\")\n",
    "print(\"✅ 優勢: 多任務場景下記憶體效率高\")\n",
    "print(\"✅ 優勢: Adapter 切換靈活\")\n",
    "print(\"⚠️  劣勢: 存在推理開銷\")\n",
    "print(\"⚠️  劣勢: 部署相對複雜\")\n",
    "print(\"\\n💡 建議: 單任務高性能場景選擇 LoRA/IA³,多任務靈活場景選擇 Prefix Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 部署最佳實踐與建議\n",
    "\n",
    "### 9.1 部署檢查清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prefix Tuning 部署檢查清單 ===\")\n",
    "print(\"\")\n",
    "print(\"📋 部署前準備:\")\n",
    "print(\"  □ 確認 Prefix Tuning adapter 訓練完成並保存\")\n",
    "print(\"  □ 在驗證集上驗證 adapter 性能\")\n",
    "print(\"  □ 記錄基準性能指標 (延遲、吞吐量)\")\n",
    "print(\"  □ 確定部署架構 (單任務 vs 多任務)\")\n",
    "print(\"\")\n",
    "print(\"🔧 部署配置:\")\n",
    "print(\"  □ 配置基礎模型載入路徑\")\n",
    "print(\"  □ 配置 adapter 載入路徑\")\n",
    "print(\"  □ 設定推理參數 (batch_size, max_length 等)\")\n",
    "print(\"  □ 配置記憶體管理策略\")\n",
    "print(\"  □ 設定監控和日誌\")\n",
    "print(\"\")\n",
    "print(\"✅ 部署後驗證:\")\n",
    "print(\"  □ 推理延遲測試\")\n",
    "print(\"  □ 吞吐量測試\")\n",
    "print(\"  □ 記憶體使用監控\")\n",
    "print(\"  □ 輸出質量抽樣檢查\")\n",
    "print(\"  □ 壓力測試\")\n",
    "print(\"  □ 長時間穩定性測試\")\n",
    "print(\"\")\n",
    "print(\"📚 文檔與維護:\")\n",
    "print(\"  □ 保存部署配置文件\")\n",
    "print(\"  □ 記錄性能基準數據\")\n",
    "print(\"  □ 準備回滾方案\")\n",
    "print(\"  □ 更新 API 文檔\")\n",
    "print(\"  □ 設定告警閾值\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 場景選擇建議"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prefix Tuning 適用場景建議 ===\")\n",
    "print(\"\")\n",
    "print(\"🎯 最適合的場景:\")\n",
    "print(\"  • 需要支持多個相關任務\")\n",
    "print(\"  • 任務需要頻繁切換\")\n",
    "print(\"  • 記憶體資源有限,無法部署多個完整模型\")\n",
    "print(\"  • 文本生成任務 (對話、摘要、風格轉換)\")\n",
    "print(\"  • 可以接受適度的推理開銷\")\n",
    "print(\"\")\n",
    "print(\"⚠️  需要權衡的場景:\")\n",
    "print(\"  • 對推理延遲要求極高的場景 -> 考慮 LoRA/IA³\")\n",
    "print(\"  • 單一任務部署 -> LoRA/IA³ 更合適\")\n",
    "print(\"  • 理解類任務 (分類、NER) -> Adapter/BitFit 可能更好\")\n",
    "print(\"\")\n",
    "print(\"🔄 組合策略:\")\n",
    "print(\"  • Prefix Tuning + LoRA: 結合兩者優勢\")\n",
    "print(\"  • 開發階段用 Prefix Tuning 快速驗證多任務\")\n",
    "print(\"  • 生產階段為高頻任務單獨訓練 LoRA 部署\")\n",
    "print(\"  • 低頻任務保持 Prefix Tuning 靈活性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 總結\n",
    "\n",
    "通過本實驗,我們深入探討了 **Prefix Tuning 在生產環境中的部署策略**。\n",
    "\n",
    "### 10.1 核心學習收穫\n",
    "\n",
    "1. **理論認知**: 理解了 Prefix Tuning 不可合併的技術原因\n",
    "2. **部署實踐**: 掌握了 Prefix Tuning 的保存、載入和部署流程\n",
    "3. **性能分析**: 量化了推理開銷和記憶體佔用\n",
    "4. **架構設計**: 學習了多任務 Prefix 管理策略\n",
    "5. **方法對比**: 理解了不同 PEFT 方法的部署特性差異\n",
    "\n",
    "### 10.2 Prefix Tuning 部署的獨特價值\n",
    "\n",
    "- **多任務友好**: 共享基礎模型,靈活切換任務\n",
    "- **記憶體效率**: Adapter 極小,多任務也不會佔用太多額外記憶體\n",
    "- **部署靈活性**: 可以動態添加新任務而不影響現有服務\n",
    "- **理論優雅性**: 通過前綴引導生成,概念清晰\n",
    "\n",
    "### 10.3 部署權衡\n",
    "\n",
    "雖然 Prefix Tuning 無法像 LoRA/IA³ 那樣實現零開銷推理,但在**多任務場景**下,其靈活性和記憶體效率使其成為優秀的選擇。關鍵是根據實際需求選擇合適的 PEFT 方法:\n",
    "\n",
    "- **單任務、追求極致性能**: LoRA/IA³\n",
    "- **多任務、靈活切換**: Prefix Tuning\n",
    "- **理解類任務**: Adapter/BitFit\n",
    "- **超大模型、極簡適配**: Prompt Tuning\n",
    "\n",
    "正確的部署策略能夠最大化 PEFT 技術的價值!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理資源\n",
    "print(\"=== 清理實驗資源 ===\")\n",
    "del peft_model, base_model, reloaded_model, fresh_base_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"✅ 資源清理完成\")\n",
    "\n",
    "print(\"\\n🎉 Lab 4 - Prefix Tuning 部署指南實驗完成!\")\n",
    "print(f\"Adapter 已保存到: {save_path}\")\n",
    "print(\"現在您可以在生產環境中部署這個 Prefix Tuning adapter 了!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
