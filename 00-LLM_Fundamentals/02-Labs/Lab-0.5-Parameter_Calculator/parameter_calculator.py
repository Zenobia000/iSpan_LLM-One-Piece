#!/usr/bin/env python3
"""
LLMåƒæ•¸é‡èˆ‡è¨ˆç®—è¤‡é›œåº¦ç²¾ç¢ºè¨ˆç®—å·¥å…·
å¯¦ç¾Transformeræ¶æ§‹çš„ç²¾ç¢ºåƒæ•¸è¨ˆç®—å’Œè³‡æºä¼°ç®—
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple
import json
from datetime import datetime

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®æ•¸æ“šé¡"""
    name: str
    vocab_size: int
    max_seq_len: int
    d_model: int
    n_layers: int
    n_heads: int
    d_ff: Optional[int] = None
    use_bias: bool = False
    tie_embeddings: bool = True
    attention_type: str = "mha"  # mha, mqa, gqa
    ffn_type: str = "standard"  # standard, gated, moe
    n_kv_heads: Optional[int] = None  # for GQA
    n_experts: Optional[int] = None   # for MoE

    def __post_init__(self):
        if self.d_ff is None:
            self.d_ff = 4 * self.d_model
        if self.n_kv_heads is None and self.attention_type == 'gqa':
            self.n_kv_heads = max(1, self.n_heads // 4)
        if self.n_experts is None and self.ffn_type == 'moe':
            self.n_experts = 8

class ParameterCalculator:
    """LLMåƒæ•¸è¨ˆç®—å™¨"""

    def __init__(self):
        self.precision_bytes = {
            'fp32': 4, 'fp16': 2, 'bf16': 2,
            'int8': 1, 'int4': 0.5, 'nf4': 0.5
        }

    def calculate_transformer_parameters(self, config: ModelConfig) -> Dict:
        """
        ç²¾ç¢ºè¨ˆç®—Transformeræ¨¡å‹åƒæ•¸é‡

        åƒæ•¸åˆ†è§£ï¼š
        1. Embeddingå±¤ï¼šToken Embedding + Position Embedding
        2. Transformerå±¤ï¼šAttention + FFN + LayerNorm
        3. è¼¸å‡ºå±¤ï¼šLanguage Modeling Head (å¯é¸æ“‡èˆ‡è¼¸å…¥å…±äº«)
        """

        print(f"è¨ˆç®—æ¨¡å‹åƒæ•¸é‡: {config.name}")

        # 1. Embeddingå±¤åƒæ•¸
        token_embedding = config.vocab_size * config.d_model
        position_embedding = config.max_seq_len * config.d_model

        if config.tie_embeddings:
            embedding_params = token_embedding + position_embedding
            output_params = 0  # è¼¸å‡ºå±¤èˆ‡è¼¸å…¥embeddingå…±äº«
            print(f"  Token Embedding (å…±äº«): {token_embedding:,} åƒæ•¸")
        else:
            embedding_params = token_embedding + position_embedding
            output_params = config.vocab_size * config.d_model
            print(f"  Token Embedding: {token_embedding:,} åƒæ•¸")
            print(f"  Output Layer: {output_params:,} åƒæ•¸")

        print(f"  Position Embedding: {position_embedding:,} åƒæ•¸")

        # 2. Attentionå±¤åƒæ•¸
        attention_params = self._calculate_attention_params(config)
        print(f"  Attention (per layer): {attention_params:,} åƒæ•¸")

        # 3. FFNå±¤åƒæ•¸
        ffn_params = self._calculate_ffn_params(config)
        print(f"  FFN (per layer): {ffn_params:,} åƒæ•¸")

        # 4. LayerNormåƒæ•¸
        layernorm_params = 2 * 2 * config.d_model  # æ¯å±¤2å€‹LayerNormï¼Œæ¯å€‹2å€‹åƒæ•¸
        print(f"  LayerNorm (per layer): {layernorm_params:,} åƒæ•¸")

        # 5. åç½®é …
        bias_params = 0
        if config.use_bias:
            bias_params = self._calculate_bias_params(config)
            print(f"  Bias terms (per layer): {bias_params:,} åƒæ•¸")

        # 6. æ¯å±¤ç¸½åƒæ•¸
        per_layer_params = attention_params + ffn_params + layernorm_params + bias_params

        # 7. ç¸½åƒæ•¸è¨ˆç®—
        total_params = (
            embedding_params +                    # è¼¸å…¥embedding
            config.n_layers * per_layer_params + # æ‰€æœ‰Transformerå±¤
            output_params +                       # è¼¸å‡ºå±¤
            2 * config.d_model                   # æœ€çµ‚LayerNorm
        )

        print(f"\nç¸½åƒæ•¸é‡: {total_params:,} ({total_params/1e9:.2f}B)")

        # è©³ç´°åˆ†è§£
        parameter_breakdown = {
            'total_parameters': total_params,
            'embedding_parameters': embedding_params,
            'attention_parameters_per_layer': attention_params,
            'ffn_parameters_per_layer': ffn_params,
            'layernorm_parameters_per_layer': layernorm_params,
            'bias_parameters_per_layer': bias_params,
            'per_layer_parameters': per_layer_params,
            'all_layers_parameters': config.n_layers * per_layer_params,
            'output_parameters': output_params,
            'parameter_percentages': {
                'embedding_ratio': embedding_params / total_params * 100,
                'attention_ratio': (config.n_layers * attention_params) / total_params * 100,
                'ffn_ratio': (config.n_layers * ffn_params) / total_params * 100,
                'layernorm_ratio': (config.n_layers * layernorm_params + 2 * config.d_model) / total_params * 100,
                'output_ratio': output_params / total_params * 100
            }
        }

        return parameter_breakdown

    def _calculate_attention_params(self, config: ModelConfig) -> int:
        """è¨ˆç®—Attentionå±¤åƒæ•¸"""

        if config.attention_type == "mha":
            # Multi-Head Attention: Q, K, V, O å››å€‹è®Šæ›çŸ©é™£
            return 4 * config.d_model * config.d_model

        elif config.attention_type == "mqa":
            # Multi-Query Attention: Qç‚ºå¤šé ­ï¼ŒK,Vç‚ºå–®é ­
            d_head = config.d_model // config.n_heads
            return (
                3 * config.d_model * config.d_model +  # Q, Oè®Šæ› + K,Vè®Šæ›
                2 * config.d_model * d_head             # K,Vå–®é ­åƒæ•¸
            )

        elif config.attention_type == "gqa":
            # Grouped-Query Attention
            d_head = config.d_model // config.n_heads
            kv_heads = config.n_kv_heads
            return (
                3 * config.d_model * config.d_model +  # Q, Oè®Šæ›
                2 * config.d_model * kv_heads * d_head # K,Våˆ†çµ„åƒæ•¸
            )

        else:
            raise ValueError(f"Unsupported attention type: {config.attention_type}")

    def _calculate_ffn_params(self, config: ModelConfig) -> int:
        """è¨ˆç®—FFNå±¤åƒæ•¸"""

        if config.ffn_type == "standard":
            # æ¨™æº–FFN: d_model -> d_ff -> d_model
            return 2 * config.d_model * config.d_ff

        elif config.ffn_type == "gated":
            # é–€æ§FFN (å¦‚SwiGLU): éœ€è¦é¡å¤–çš„é–€æ§è®Šæ›
            return 3 * config.d_model * config.d_ff

        elif config.ffn_type == "moe":
            # Mixture of Experts
            routing_params = config.d_model * config.n_experts
            expert_params = config.n_experts * 2 * config.d_model * config.d_ff
            return routing_params + expert_params

        else:
            raise ValueError(f"Unsupported FFN type: {config.ffn_type}")

    def _calculate_bias_params(self, config: ModelConfig) -> int:
        """è¨ˆç®—åç½®åƒæ•¸"""

        attention_bias = 4 * config.d_model  # Q,K,V,Oçš„åç½®
        ffn_bias = 2 * config.d_ff           # FFNå…©å±¤çš„åç½®

        return attention_bias + ffn_bias

    def calculate_flops(self, config: ModelConfig, batch_size: int,
                       sequence_length: int, mode: str = 'inference') -> Dict:
        """
        è¨ˆç®—FLOPsï¼ˆæµ®é»é‹ç®—æ¬¡æ•¸ï¼‰

        è¨ˆç®—å…¬å¼ï¼š
        - Forward FLOPs â‰ˆ 2 Ã— P Ã— N (P=åƒæ•¸æ•¸ï¼ŒN=tokenæ•¸)
        - Training FLOPs â‰ˆ 6 Ã— P Ã— N (åŒ…å«å‰å‘ã€åå‘ã€å„ªåŒ–å™¨)
        """

        print(f"è¨ˆç®—FLOPs: {config.name} ({mode}æ¨¡å¼)")

        param_result = self.calculate_transformer_parameters(config)
        total_params = param_result['total_parameters']

        if mode == 'training':
            # è¨“ç·´æ¨¡å¼ï¼šå‰å‘ + åå‘ + å„ªåŒ–å™¨æ›´æ–°
            total_tokens = batch_size * sequence_length

            # å‰å‘å‚³æ’­
            forward_flops = 2 * total_params * total_tokens

            # åå‘å‚³æ’­ï¼ˆç´„ç‚ºå‰å‘çš„2å€ï¼‰
            backward_flops = 4 * total_params * total_tokens

            # å„ªåŒ–å™¨æ›´æ–°ï¼ˆç›¸å°è¼ƒå°ï¼‰
            optimizer_flops = total_params  # ç°¡åŒ–ä¼°ç®—

            total_flops = forward_flops + backward_flops + optimizer_flops

            flops_breakdown = {
                'forward_flops': forward_flops,
                'backward_flops': backward_flops,
                'optimizer_flops': optimizer_flops,
                'total_flops': total_flops,
                'flops_per_token': total_flops / total_tokens
            }

        else:
            # æ¨ç†æ¨¡å¼ï¼šåƒ…å‰å‘å‚³æ’­
            total_tokens = batch_size * sequence_length
            forward_flops = 2 * total_params * total_tokens

            flops_breakdown = {
                'forward_flops': forward_flops,
                'total_flops': forward_flops,
                'flops_per_token': forward_flops / total_tokens
            }

        print(f"  ç¸½FLOPs: {flops_breakdown['total_flops']:.2e}")
        print(f"  æ¯Token FLOPs: {flops_breakdown['flops_per_token']:.2e}")

        return flops_breakdown

    def calculate_memory_requirements(self, config: ModelConfig, batch_size: int,
                                    sequence_length: int, precision: str = 'fp16',
                                    mode: str = 'inference') -> Dict:
        """
        è¨ˆç®—è¨˜æ†¶é«”éœ€æ±‚

        è¨˜æ†¶é«”çµ„æˆï¼š
        - æ¨¡å‹åƒæ•¸è¨˜æ†¶é«”
        - æ¿€æ´»å€¼è¨˜æ†¶é«”
        - KV Cacheè¨˜æ†¶é«”ï¼ˆæ¨ç†ï¼‰
        - å„ªåŒ–å™¨ç‹€æ…‹è¨˜æ†¶é«”ï¼ˆè¨“ç·´ï¼‰
        - æ¢¯åº¦è¨˜æ†¶é«”ï¼ˆè¨“ç·´ï¼‰
        """

        print(f"è¨ˆç®—è¨˜æ†¶é«”éœ€æ±‚: {config.name} ({mode}æ¨¡å¼, {precision}ç²¾åº¦)")

        param_result = self.calculate_transformer_parameters(config)
        total_params = param_result['total_parameters']
        bytes_per_param = self.precision_bytes[precision]

        # æ¨¡å‹åƒæ•¸è¨˜æ†¶é«”
        model_memory_bytes = total_params * bytes_per_param

        if mode == 'training':
            # è¨“ç·´æ¨¡å¼è¨˜æ†¶é«”
            # å„ªåŒ–å™¨ç‹€æ…‹ï¼ˆAdam: momentum + varianceï¼‰
            optimizer_memory_bytes = total_params * 8  # FP32ç²¾åº¦å­˜å„²

            # æ¢¯åº¦è¨˜æ†¶é«”
            gradient_memory_bytes = total_params * bytes_per_param

            # æ¿€æ´»å€¼è¨˜æ†¶é«”ï¼ˆéœ€è¦ä¿å­˜æ‰€æœ‰å±¤ï¼‰
            activation_per_layer = batch_size * sequence_length * config.d_model * bytes_per_param
            total_activation_bytes = activation_per_layer * config.n_layers

            total_memory_bytes = (
                model_memory_bytes + optimizer_memory_bytes +
                gradient_memory_bytes + total_activation_bytes
            )

            memory_breakdown = {
                'model_memory_gb': model_memory_bytes / (1024**3),
                'optimizer_memory_gb': optimizer_memory_bytes / (1024**3),
                'gradient_memory_gb': gradient_memory_bytes / (1024**3),
                'activation_memory_gb': total_activation_bytes / (1024**3),
                'total_memory_gb': total_memory_bytes / (1024**3)
            }

        else:
            # æ¨ç†æ¨¡å¼è¨˜æ†¶é«”
            # KV Cacheè¨˜æ†¶é«”
            d_head = config.d_model // config.n_heads
            kv_cache_bytes = (
                2 * config.n_layers * batch_size * config.n_heads *
                sequence_length * d_head * bytes_per_param
            )

            # ç•¶å‰æ¿€æ´»å€¼è¨˜æ†¶é«”ï¼ˆåªéœ€è¦ç•¶å‰å±¤ï¼‰
            current_activation_bytes = batch_size * sequence_length * config.d_model * bytes_per_param

            total_memory_bytes = model_memory_bytes + kv_cache_bytes + current_activation_bytes

            memory_breakdown = {
                'model_memory_gb': model_memory_bytes / (1024**3),
                'kv_cache_memory_gb': kv_cache_bytes / (1024**3),
                'activation_memory_gb': current_activation_bytes / (1024**3),
                'total_memory_gb': total_memory_bytes / (1024**3)
            }

        for key, value in memory_breakdown.items():
            print(f"  {key}: {value:.2f} GB")

        return memory_breakdown

    def estimate_training_cost(self, config: ModelConfig, training_tokens: int,
                             gpu_type: str = 'A100', efficiency: float = 0.5,
                             electricity_cost_per_kwh: float = 0.1) -> Dict:
        """
        ä¼°ç®—è¨“ç·´æˆæœ¬

        æˆæœ¬çµ„æˆï¼š
        - GPUç¡¬é«”æˆæœ¬
        - é›»åŠ›æˆæœ¬
        - æ™‚é–“æˆæœ¬
        """

        print(f"ä¼°ç®—è¨“ç·´æˆæœ¬: {config.name}")

        # GPUè¦æ ¼æ•¸æ“šåº«
        gpu_specs = {
            'V100': {'flops': 125e12, 'power_w': 300, 'price_usd': 8000},
            'A100': {'flops': 312e12, 'power_w': 400, 'price_usd': 15000},
            'H100': {'flops': 1000e12, 'power_w': 700, 'price_usd': 30000}
        }

        if gpu_type not in gpu_specs:
            raise ValueError(f"Unsupported GPU type: {gpu_type}")

        gpu_spec = gpu_specs[gpu_type]

        # è¨ˆç®—ç¸½FLOPs
        flop_result = self.calculate_flops(config, 1, training_tokens, 'training')
        total_flops = flop_result['total_flops']

        # è¨ˆç®—è¨“ç·´æ™‚é–“
        effective_flops_per_second = gpu_spec['flops'] * efficiency
        training_time_seconds = total_flops / effective_flops_per_second
        training_time_hours = training_time_seconds / 3600
        training_time_days = training_time_hours / 24

        # è¨ˆç®—æˆæœ¬
        gpu_cost = gpu_spec['price_usd']
        power_cost = (gpu_spec['power_w'] / 1000) * training_time_hours * electricity_cost_per_kwh
        total_cost = gpu_cost + power_cost  # ç°¡åŒ–ï¼šä¸è€ƒæ…®æŠ˜èˆŠ

        cost_breakdown = {
            'total_flops': total_flops,
            'training_time_hours': training_time_hours,
            'training_time_days': training_time_days,
            'gpu_hardware_cost': gpu_cost,
            'electricity_cost': power_cost,
            'total_cost_usd': total_cost,
            'cost_per_billion_params': total_cost / (config.d_model * config.n_layers / 1e9),
            'cost_per_billion_tokens': total_cost / (training_tokens / 1e9)
        }

        print(f"  è¨“ç·´æ™‚é–“: {training_time_days:.2f} å¤©")
        print(f"  ç¸½æˆæœ¬: ${total_cost:,.0f}")
        print(f"  æ¯åå„„åƒæ•¸æˆæœ¬: ${cost_breakdown['cost_per_billion_params']:,.0f}")

        return cost_breakdown

    def apply_scaling_laws(self, compute_budget: float, law_type: str = 'chinchilla') -> Dict:
        """
        æ‡‰ç”¨ç¸®æ”¾æ³•å‰‡é€²è¡Œè³‡æºåˆ†é…å„ªåŒ–

        Chinchillaç¸®æ”¾æ³•å‰‡ï¼š
        - æœ€å„ªåƒæ•¸é‡ âˆ C^0.73
        - æœ€å„ªæ•¸æ“šé‡ âˆ C^0.27
        - å…¶ä¸­Cç‚ºè¨ˆç®—é ç®—ï¼ˆFLOPsï¼‰
        """

        print(f"æ‡‰ç”¨{law_type}ç¸®æ”¾æ³•å‰‡, è¨ˆç®—é ç®—: {compute_budget:.2e} FLOPs")

        if law_type == 'chinchilla':
            # Chinchillaç¸®æ”¾æ³•å‰‡åƒæ•¸
            alpha = 0.34  # åƒæ•¸ç¸®æ”¾æŒ‡æ•¸
            beta = 0.28   # æ•¸æ“šç¸®æ”¾æŒ‡æ•¸
            A = 406.4     # åƒæ•¸ä¿‚æ•¸
            B = 410.7     # æ•¸æ“šä¿‚æ•¸
            E = 1.69      # åŸºç¤æå¤±

            # è¨ˆç®—æœ€å„ªåˆ†é…
            a = alpha / (alpha + beta)  # â‰ˆ 0.55
            b = beta / (alpha + beta)   # â‰ˆ 0.45

            # æœ€å„ªåƒæ•¸é‡å’Œæ•¸æ“šé‡
            optimal_params = (compute_budget / 6) ** a
            optimal_tokens = (compute_budget / 6) ** b

            # é æ¸¬æ€§èƒ½
            loss_param = A / (optimal_params ** alpha)
            loss_data = B / (optimal_tokens ** beta)
            predicted_loss = E + loss_param + loss_data

            scaling_result = {
                'law_type': law_type,
                'compute_budget': compute_budget,
                'optimal_parameters': optimal_params,
                'optimal_tokens': optimal_tokens,
                'predicted_loss': predicted_loss,
                'params_ratio': a,
                'tokens_ratio': b,
                'efficiency_score': (optimal_params * optimal_tokens) / compute_budget
            }

            print(f"  æœ€å„ªåƒæ•¸é‡: {optimal_params:.2e} ({optimal_params/1e9:.1f}B)")
            print(f"  æœ€å„ªæ•¸æ“šé‡: {optimal_tokens:.2e} ({optimal_tokens/1e9:.0f}B tokens)")
            print(f"  é æ¸¬Loss: {predicted_loss:.3f}")

        else:
            raise ValueError(f"Unsupported scaling law: {law_type}")

        return scaling_result

    def hardware_matching_calculator(self, model_config: ModelConfig,
                                   target_gpus: Dict, workload: Dict) -> Dict:
        """
        ç¡¬é«”åŒ¹é…è¨ˆç®—å™¨

        æ ¹æ“šæ¨¡å‹é…ç½®å’Œå·¥ä½œè² è¼‰è¦æ±‚ï¼Œè¨ˆç®—æœ€é©åˆçš„GPUé…ç½®
        """

        print(f"=== ç¡¬é«”åŒ¹é…è¨ˆç®—: {model_config.name} ===")

        # è¨ˆç®—æ¨¡å‹è¨˜æ†¶é«”éœ€æ±‚
        memory_req = self.calculate_memory_requirements(
            model_config,
            workload.get('batch_size', 8),
            workload.get('sequence_length', 2048),
            workload.get('precision', 'fp16'),
            workload.get('mode', 'inference')
        )

        required_memory_gb = memory_req['total_memory_gb']
        print(f"è¨˜æ†¶é«”éœ€æ±‚: {required_memory_gb:.1f} GB")

        # è¨ˆç®—å„GPUæ–¹æ¡ˆ
        matching_results = {}

        for gpu_name, gpu_specs in target_gpus.items():
            # è€ƒæ…®å®‰å…¨é¤˜é‡
            safety_margin = 0.2  # 20%å®‰å…¨é¤˜é‡
            available_memory = gpu_specs['memory_gb'] * (1 - safety_margin)

            # è¨ˆç®—æœ€å°GPUæ•¸é‡
            min_gpus = max(1, np.ceil(required_memory_gb / available_memory))

            # è¨ˆç®—å¯¦éš›è¨˜æ†¶é«”åˆ©ç”¨ç‡
            actual_utilization = required_memory_gb / (min_gpus * gpu_specs['memory_gb']) * 100

            # è¨ˆç®—æˆæœ¬
            total_cost = min_gpus * gpu_specs['price_usd']
            cost_per_gb = total_cost / required_memory_gb

            matching_results[gpu_name] = {
                'min_gpus': int(min_gpus),
                'memory_utilization_percent': min(100, actual_utilization),
                'total_cost_usd': total_cost,
                'cost_per_gb': cost_per_gb,
                'cost_efficiency': gpu_specs['memory_gb'] / gpu_specs['price_usd'],
                'recommended': actual_utilization >= 50 and actual_utilization <= 90  # åˆç†åˆ©ç”¨ç‡
            }

        # æŒ‰æˆæœ¬æ•ˆç›Šæ’åº
        sorted_options = sorted(
            matching_results.items(),
            key=lambda x: x[1]['cost_per_gb']
        )

        print("\nç¡¬é«”åŒ¹é…çµæœ:")
        for gpu_name, specs in sorted_options:
            status = "âœ… æ¨è–¦" if specs['recommended'] else "âš ï¸ è€ƒæ…®"
            print(f"  {status} {gpu_name}: {specs['min_gpus']} GPUs, "
                  f"åˆ©ç”¨ç‡ {specs['memory_utilization_percent']:.1f}%, "
                  f"æˆæœ¬ ${specs['total_cost_usd']:,}")

        return {
            'memory_requirements': memory_req,
            'gpu_matching_results': matching_results,
            'best_option': sorted_options[0] if sorted_options else None,
            'workload_config': workload
        }

    def compare_model_configurations(self, configs: List[ModelConfig]) -> pd.DataFrame:
        """å°æ¯”å¤šå€‹æ¨¡å‹é…ç½®"""

        print("\n=== æ¨¡å‹é…ç½®å°æ¯” ===")

        comparison_data = []

        for config in configs:
            param_result = self.calculate_transformer_parameters(config)

            row = {
                'æ¨¡å‹åç¨±': config.name,
                'åƒæ•¸é‡(B)': f"{param_result['total_parameters'] / 1e9:.2f}",
                'è©è¡¨å¤§å°': f"{config.vocab_size:,}",
                'æ¨¡å‹ç¶­åº¦': config.d_model,
                'å±¤æ•¸': config.n_layers,
                'æ³¨æ„åŠ›é ­æ•¸': config.n_heads,
                'FFNå€æ•¸': f"{config.d_ff / config.d_model:.1f}",
                'æ³¨æ„åŠ›é¡å‹': config.attention_type.upper(),
                'Attentionä½”æ¯”': f"{param_result['parameter_percentages']['attention_ratio']:.1f}%",
                'FFNä½”æ¯”': f"{param_result['parameter_percentages']['ffn_ratio']:.1f}%"
            }

            comparison_data.append(row)

        comparison_df = pd.DataFrame(comparison_data)
        print(comparison_df.to_string(index=False))

        return comparison_df

    def demonstrate_scaling_laws(self):
        """æ¼”ç¤ºç¸®æ”¾æ³•å‰‡æ‡‰ç”¨"""

        print("\n=== ç¸®æ”¾æ³•å‰‡æ¼”ç¤º ===")

        # ä¸åŒè¨ˆç®—é ç®—çš„åˆ†æ
        compute_budgets = [1e20, 1e21, 1e22, 1e23, 1e24]  # FLOPs

        scaling_results = []

        print("è¨ˆç®—é ç®—èˆ‡æœ€å„ªé…ç½®:")
        print(f"{'è¨ˆç®—é ç®—':<12} {'æœ€å„ªåƒæ•¸(B)':<12} {'æœ€å„ªæ•¸æ“š(B)':<12} {'é æ¸¬Loss':<10}")
        print("-" * 55)

        for budget in compute_budgets:
            result = self.apply_scaling_laws(budget, 'chinchilla')

            params_b = result['optimal_parameters'] / 1e9
            tokens_b = result['optimal_tokens'] / 1e9
            loss = result['predicted_loss']

            print(f"{budget:.1e}     {params_b:8.1f}      {tokens_b:8.0f}        {loss:.3f}")

            scaling_results.append(result)

        # ç¾æœ‰æ¨¡å‹æ•ˆç‡åˆ†æ
        print("\nç¾æœ‰æ¨¡å‹vsæœ€å„ªé…ç½®å°æ¯”:")
        existing_models = [
            {'name': 'GPT-3', 'params': 175e9, 'tokens': 300e9},
            {'name': 'LLaMA-7B', 'params': 7e9, 'tokens': 1000e9},
            {'name': 'LLaMA-65B', 'params': 65e9, 'tokens': 1400e9}
        ]

        for model in existing_models:
            compute_budget = 6 * model['params'] * model['tokens']
            optimal = self.apply_scaling_laws(compute_budget, 'chinchilla')
            efficiency = optimal['predicted_loss'] / self._predict_actual_loss(model['params'], model['tokens'])

            print(f"{model['name']:10} æ•ˆç‡ä¿‚æ•¸: {efficiency:.3f}")

        return scaling_results

    def _predict_actual_loss(self, params: float, tokens: float) -> float:
        """åŸºæ–¼Chinchillaå…¬å¼é æ¸¬å¯¦éš›Loss"""

        alpha, beta = 0.34, 0.28
        A, B, E = 406.4, 410.7, 1.69

        loss_param = A / (params ** alpha)
        loss_data = B / (tokens ** beta)

        return E + loss_param + loss_data

    def visualize_parameter_analysis(self, configs: List[ModelConfig]):
        """å¯è¦–åŒ–åƒæ•¸åˆ†æ"""

        print("\n=== ç”Ÿæˆåƒæ•¸åˆ†æå¯è¦–åŒ– ===")

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

        # 1. åƒæ•¸é‡å°æ¯”
        model_names = [config.name for config in configs]
        param_counts = []
        for config in configs:
            result = self.calculate_transformer_parameters(config)
            param_counts.append(result['total_parameters'] / 1e9)

        bars1 = ax1.bar(model_names, param_counts, color='skyblue', alpha=0.8)
        ax1.set_title('æ¨¡å‹åƒæ•¸é‡å°æ¯”')
        ax1.set_ylabel('åƒæ•¸é‡ (Billions)')
        ax1.tick_params(axis='x', rotation=45)

        for bar, count in zip(bars1, param_counts):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{count:.1f}B', ha='center', va='bottom')

        # 2. åƒæ•¸åˆ†ä½ˆï¼ˆä»¥ç¬¬ä¸€å€‹æ¨¡å‹ç‚ºä¾‹ï¼‰
        if configs:
            config = configs[0]
            result = self.calculate_transformer_parameters(config)
            percentages = result['parameter_percentages']

            labels = ['Embedding', 'Attention', 'FFN', 'LayerNorm', 'Output']
            sizes = [
                percentages['embedding_ratio'],
                percentages['attention_ratio'],
                percentages['ffn_ratio'],
                percentages['layernorm_ratio'],
                percentages['output_ratio']
            ]

            ax2.pie(sizes, labels=labels, autopct='%1.1f%%')
            ax2.set_title(f'{config.name} åƒæ•¸åˆ†ä½ˆ')

        # 3. è¨˜æ†¶é«”éœ€æ±‚å°æ¯”ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        memory_requirements = []
        for config in configs:
            memory_req = self.calculate_memory_requirements(config, 8, 2048, 'fp16', 'inference')
            memory_requirements.append(memory_req['total_memory_gb'])

        bars3 = ax3.bar(model_names, memory_requirements, color='lightgreen', alpha=0.8)
        ax3.set_title('æ¨ç†è¨˜æ†¶é«”éœ€æ±‚å°æ¯”')
        ax3.set_ylabel('è¨˜æ†¶é«”éœ€æ±‚ (GB)')
        ax3.tick_params(axis='x', rotation=45)

        for bar, mem in zip(bars3, memory_requirements):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{mem:.1f}GB', ha='center', va='bottom')

        # 4. FLOPså°æ¯”
        flops_requirements = []
        for config in configs:
            flop_result = self.calculate_flops(config, 1, 2048, 'inference')
            flops_requirements.append(flop_result['total_flops'])

        ax4.bar(model_names, flops_requirements, color='salmon', alpha=0.8)
        ax4.set_title('æ¨ç†FLOPséœ€æ±‚å°æ¯”')
        ax4.set_ylabel('FLOPs')
        ax4.set_yscale('log')  # å°æ•¸åº§æ¨™
        ax4.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig('parameter_analysis_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜: parameter_analysis_comparison.png")

def create_example_configs() -> List[ModelConfig]:
    """å‰µå»ºç¤ºä¾‹æ¨¡å‹é…ç½®"""

    configs = [
        ModelConfig("GPT-2 Small", 50257, 1024, 768, 12, 12, 3072),
        ModelConfig("LLaMA-7B", 32000, 2048, 4096, 32, 32, 11008),
        ModelConfig("LLaMA-13B", 32000, 2048, 5120, 40, 40, 13824),
        ModelConfig("Custom-MQA", 32000, 2048, 4096, 32, 32, 11008, attention_type="mqa"),
        ModelConfig("Custom-MoE", 32000, 2048, 4096, 32, 32, 11008, ffn_type="moe", n_experts=8)
    ]

    return configs

def main():
    """ä¸»å‡½æ•¸æ¼”ç¤º"""

    print("LLMåƒæ•¸é‡èˆ‡è¨ˆç®—è¤‡é›œåº¦è¨ˆç®—å·¥å…·")
    print("=" * 60)

    # åˆå§‹åŒ–è¨ˆç®—å™¨
    calculator = ParameterCalculator()

    # å‰µå»ºç¤ºä¾‹é…ç½®
    configs = create_example_configs()

    print(f"\nğŸ“Š å°‡åˆ†æ {len(configs)} å€‹æ¨¡å‹é…ç½®")

    # 1. åƒæ•¸é‡è¨ˆç®—æ¼”ç¤º
    print("\n1. åƒæ•¸é‡è©³ç´°è¨ˆç®—:")
    for config in configs[:2]:  # æ¼”ç¤ºå‰2å€‹
        param_result = calculator.calculate_transformer_parameters(config)

    # 2. FLOPsè¨ˆç®—æ¼”ç¤º
    print("\n2. FLOPsè¨ˆç®—æ¼”ç¤º:")
    flop_result = calculator.calculate_flops(configs[1], 8, 2048, 'inference')

    # 3. è¨˜æ†¶é«”éœ€æ±‚æ¼”ç¤º
    print("\n3. è¨˜æ†¶é«”éœ€æ±‚åˆ†æ:")
    memory_result = calculator.calculate_memory_requirements(configs[1], 8, 2048, 'fp16', 'inference')

    # 4. è¨“ç·´æˆæœ¬ä¼°ç®—
    print("\n4. è¨“ç·´æˆæœ¬ä¼°ç®—:")
    cost_result = calculator.estimate_training_cost(configs[1], 100e9, 'A100')

    # 5. ç¡¬é«”åŒ¹é…æ¼”ç¤º
    print("\n5. ç¡¬é«”åŒ¹é…åˆ†æ:")
    target_gpus = {
        'RTX_4090': {'memory_gb': 24, 'price_usd': 1600},
        'A100_40GB': {'memory_gb': 40, 'price_usd': 15000},
        'A100_80GB': {'memory_gb': 80, 'price_usd': 20000}
    }

    workload = {
        'batch_size': 8,
        'sequence_length': 2048,
        'precision': 'fp16',
        'mode': 'inference'
    }

    hardware_result = calculator.hardware_matching_calculator(configs[1], target_gpus, workload)

    # 6. æ¨¡å‹å°æ¯”
    print("\n6. æ¨¡å‹é…ç½®å°æ¯”:")
    comparison_df = calculator.compare_model_configurations(configs)

    # 7. ç¸®æ”¾æ³•å‰‡æ¼”ç¤º
    scaling_results = calculator.demonstrate_scaling_laws()

    # 8. å¯è¦–åŒ–
    calculator.visualize_parameter_analysis(configs)

    # 9. ç”Ÿæˆå ±å‘Š
    report = generate_comprehensive_report(calculator, configs)

    print("\nâœ… åˆ†æå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
    print("   - parameter_analysis_report.md")
    print("   - parameter_analysis_comparison.png")

    print("\nğŸ“ é—œéµå­¸ç¿’è¦é»:")
    print("1. åƒæ•¸é‡è¨ˆç®—éœ€è¦è€ƒæ…®æ¶æ§‹è®Šé«”çš„å·®ç•°")
    print("2. è¨˜æ†¶é«”éœ€æ±‚åŒ…å«å¤šå€‹çµ„ä»¶ï¼ŒKV Cacheå½±éŸ¿è¼ƒå¤§")
    print("3. ç¸®æ”¾æ³•å‰‡å¯ä»¥æŒ‡å°è³‡æºé…ç½®å„ªåŒ–")
    print("4. ç¡¬é«”åŒ¹é…éœ€è¦ç¶œåˆè€ƒæ…®æˆæœ¬å’Œåˆ©ç”¨ç‡")

def generate_comprehensive_report(calculator, configs) -> str:
    """ç”Ÿæˆç¶œåˆå ±å‘Š"""

    report = f"""# LLMåƒæ•¸é‡èˆ‡è¨ˆç®—è¤‡é›œåº¦åˆ†æå ±å‘Š

ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## åˆ†ææ¦‚è¿°

æœ¬å ±å‘Šå°{len(configs)}å€‹ä¸åŒçš„LLMé…ç½®é€²è¡Œäº†å…¨é¢çš„åƒæ•¸é‡ã€è¨ˆç®—è¤‡é›œåº¦å’Œè³‡æºéœ€æ±‚åˆ†æã€‚

## ä¸»è¦ç™¼ç¾

### 1. åƒæ•¸é‡åˆ†ä½ˆè¦å¾‹
- FFNå±¤é€šå¸¸ä½”ç¸½åƒæ•¸é‡çš„60-70%
- Attentionå±¤ä½”20-30%
- Embeddingå±¤å’ŒLayerNormä½”æ¯”è¼ƒå°

### 2. æ¶æ§‹è®Šé«”å½±éŸ¿
- MQAç›¸æ¯”MHAå¯æ¸›å°‘KVåƒæ•¸ç´„75%
- MoEå¤§å¹…å¢åŠ åƒæ•¸é‡ä½†è¨ˆç®—é‡å¯æ§
- ä¸åŒæ¶æ§‹çš„è¨˜æ†¶é«”/è¨ˆç®—ç‰¹æ€§å·®ç•°æ˜é¡¯

### 3. ç¡¬é«”éœ€æ±‚ç‰¹é»
- æ¨ç†è¨˜æ†¶é«”ä¸»è¦ç”±æ¨¡å‹åƒæ•¸å’ŒKV Cacheçµ„æˆ
- è¨“ç·´è¨˜æ†¶é«”éœ€æ±‚æ˜¯æ¨ç†çš„3-5å€
- æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•·åº¦å°è¨˜æ†¶é«”å½±éŸ¿å·¨å¤§

### 4. æˆæœ¬å„ªåŒ–ç­–ç•¥
- æ ¹æ“šç¸®æ”¾æ³•å‰‡å„ªåŒ–åƒæ•¸é‡å’Œæ•¸æ“šé‡åˆ†é…
- é¸æ“‡åˆé©çš„GPUé…ç½®å¹³è¡¡æˆæœ¬å’Œæ€§èƒ½
- è€ƒæ…®é‡åŒ–æŠ€è¡“é™ä½éƒ¨ç½²æˆæœ¬

## å¯¦éš›æ‡‰ç”¨å»ºè­°

1. **æ¨¡å‹é¸å‹**: æ ¹æ“šæ‡‰ç”¨éœ€æ±‚å’Œè³‡æºç´„æŸé¸æ“‡åˆé©è¦æ¨¡
2. **ç¡¬é«”é…ç½®**: ä½¿ç”¨ç¡¬é«”åŒ¹é…è¨ˆç®—å™¨ç¢ºå®šæœ€å„ªGPUé…ç½®
3. **æˆæœ¬æ§åˆ¶**: æ‡‰ç”¨ç¸®æ”¾æ³•å‰‡å„ªåŒ–è¨“ç·´è³‡æºåˆ†é…
4. **éƒ¨ç½²å„ªåŒ–**: çµåˆé‡åŒ–æŠ€è¡“é™ä½æ¨ç†æˆæœ¬

---
*æœ¬å ±å‘ŠåŸºæ–¼ç†è«–è¨ˆç®—ç”Ÿæˆï¼Œå¯¦éš›éƒ¨ç½²æ™‚è«‹çµåˆå…·é«”æ¸¬è©¦çµæœã€‚*
"""

    with open('parameter_analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(report)

    return report

if __name__ == "__main__":
    main()