{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 - é«˜æ•ˆæ•¸æ“šç¯©é¸\n",
    "## Notebook 03: ç¯©é¸æ•ˆæœé©—è­‰å¯¦é©—\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "1. è¨­è¨ˆ A/B å°æ¯”å¯¦é©— (å…¨é‡æ•¸æ“š vs ç¯©é¸æ•¸æ“š)\n",
    "2. è¨“ç·´åŸºç·šæ¨¡å‹ (52K å…¨é‡æ•¸æ“š)\n",
    "3. è¨“ç·´å°æ¯”æ¨¡å‹ (15.6K ç¯©é¸æ•¸æ“š)\n",
    "4. è©•ä¼°æ¨¡å‹æ€§èƒ½ (C-Eval åŸºæº–)\n",
    "5. å°æ¯”è¨“ç·´æ•ˆç‡ (æ™‚é–“ã€GPU åˆ©ç”¨ç‡)\n",
    "6. ç”Ÿæˆé©—è­‰å ±å‘Š\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 2-3 å°æ™‚ (éœ€è¦ GPU)\n",
    "\n",
    "**æ³¨æ„**: æœ¬ notebook éœ€è¦ GPU é€²è¡Œæ¨¡å‹è¨“ç·´ã€‚å¦‚ç„¡ GPU,å¯è·³è‡³å ±å‘Šç”Ÿæˆéƒ¨åˆ†æŸ¥çœ‹é æœŸçµæœã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæª¢æŸ¥èˆ‡æº–å‚™\n",
    "\n",
    "æª¢æŸ¥ GPU å¯ç”¨æ€§å’Œè¨“ç·´ä¾è³´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ç’°å¢ƒæª¢æŸ¥\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Python ç‰ˆæœ¬\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "# PyTorch ç‰ˆæœ¬\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "\n",
    "# CUDA å¯ç”¨æ€§\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    è¨˜æ†¶é«”: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Š: CUDA ä¸å¯ç”¨\")\n",
    "    print(\"   æœ¬ notebook éœ€è¦ GPU é€²è¡Œè¨“ç·´\")\n",
    "    print(\"   æ‚¨å¯ä»¥è·³è‡³å ±å‘Šç”Ÿæˆéƒ¨åˆ†æŸ¥çœ‹é æœŸçµæœ\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥ä¾è³´\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# è¨“ç·´ç›¸é—œ\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# å¯è¦–åŒ–é¢¨æ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… ä¾è³´å°å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¼‰å…¥æ•¸æ“šé›†\n",
    "\n",
    "è¼‰å…¥å…¨é‡æ•¸æ“šå’Œç¯©é¸å¾Œçš„æ•¸æ“šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›®éŒ„è·¯å¾‘\n",
    "DATA_DIR = Path('./data')\n",
    "VALIDATION_DIR = Path('./validation')\n",
    "VALIDATION_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# å»ºç«‹è¨“ç·´æ—¥èªŒç›®éŒ„\n",
    "LOG_DIR = VALIDATION_DIR / 'training_logs'\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "print(\"ğŸ“¥ è¼‰å…¥æ•¸æ“šé›†...\")\n",
    "\n",
    "with open(DATA_DIR / 'alpaca_raw.json', 'r', encoding='utf-8') as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "with open(DATA_DIR / 'alpaca_filtered.json', 'r', encoding='utf-8') as f:\n",
    "    filtered_data = json.load(f)\n",
    "\n",
    "print(f\"âœ… å…¨é‡æ•¸æ“š: {len(full_data):,} æ¨£æœ¬\")\n",
    "print(f\"âœ… ç¯©é¸æ•¸æ“š: {len(filtered_data):,} æ¨£æœ¬\")\n",
    "print(f\"   æ•¸æ“šæ¸›å°‘: {(1 - len(filtered_data)/len(full_data))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å¯¦é©—è¨­è¨ˆ\n",
    "\n",
    "è¨­è¨ˆå…¬å¹³çš„ A/B å°æ¯”å¯¦é©—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦é©—é…ç½®\n",
    "EXPERIMENT_CONFIG = {\n",
    "    # æ¨¡å‹è¨­å®š\n",
    "    'model_name': 'meta-llama/Llama-2-7b-hf',  # æˆ–ä½¿ç”¨å…¶ä»–é–‹æºæ¨¡å‹\n",
    "    \n",
    "    # è¨“ç·´è¶…åƒæ•¸ (å…©å€‹å¯¦é©—ä¿æŒä¸€è‡´)\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 3,\n",
    "    'batch_size': 4,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'max_length': 512,\n",
    "    'warmup_ratio': 0.03,\n",
    "    'weight_decay': 0.01,\n",
    "    'lr_scheduler_type': 'cosine',\n",
    "    \n",
    "    # LoRA é…ç½®\n",
    "    'lora_r': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.05,\n",
    "    'lora_target_modules': ['q_proj', 'v_proj'],\n",
    "    \n",
    "    # é‡åŒ–é…ç½®\n",
    "    'use_4bit': True,\n",
    "    'bnb_4bit_compute_dtype': 'float16',\n",
    "    'bnb_4bit_quant_type': 'nf4',\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    'eval_strategy': 'steps',\n",
    "    'eval_steps': 100,\n",
    "    'save_steps': 500,\n",
    "    \n",
    "    # éš¨æ©Ÿç¨®å­ (ç¢ºä¿å¯é‡ç¾)\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"ğŸ”¬ å¯¦é©—é…ç½®\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(EXPERIMENT_CONFIG, indent=2))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ä¿å­˜é…ç½®\n",
    "config_path = VALIDATION_DIR / 'experiment_config.json'\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(EXPERIMENT_CONFIG, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… å¯¦é©—é…ç½®å·²ä¿å­˜è‡³: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•¸æ“šæº–å‚™å‡½æ•¸\n",
    "\n",
    "æº–å‚™è¨“ç·´æ•¸æ“šé›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Prepare dataset for instruction tuning\n",
    "    \n",
    "    Args:\n",
    "        data: List of dicts with 'instruction', 'input', 'output'\n",
    "        tokenizer: Tokenizer\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        HuggingFace Dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\næº–å‚™æ•¸æ“šé›† ({len(data):,} æ¨£æœ¬)...\")\n",
    "    \n",
    "    # æ ¼å¼åŒ–ç‚ºæŒ‡ä»¤æ ¼å¼\n",
    "    formatted_texts = []\n",
    "    \n",
    "    for sample in tqdm(data, desc=\"æ ¼å¼åŒ–æ•¸æ“š\"):\n",
    "        instruction = sample['instruction']\n",
    "        input_text = sample.get('input', '')\n",
    "        output = sample['output']\n",
    "        \n",
    "        # Alpaca æ ¼å¼\n",
    "        if input_text:\n",
    "            text = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "        else:\n",
    "            text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "        \n",
    "        formatted_texts.append(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    print(\"  Tokenizing...\")\n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # å»ºç«‹ Dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask']\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… æ•¸æ“šé›†æº–å‚™å®Œæˆ: {len(dataset):,} æ¨£æœ¬\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "print(\"âœ… æ•¸æ“šæº–å‚™å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹è¨“ç·´å‡½æ•¸\n",
    "\n",
    "å¯¦ç¾çµ±ä¸€çš„è¨“ç·´æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, experiment_name, config):\n",
    "    \"\"\"\n",
    "    Train model with LoRA\n",
    "    \n",
    "    Args:\n",
    "        dataset: Training dataset\n",
    "        experiment_name: Name for this experiment\n",
    "        config: Experiment configuration\n",
    "    \n",
    "    Returns:\n",
    "        Training results dict\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"é–‹å§‹è¨“ç·´: {experiment_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. è¼‰å…¥ tokenizer\n",
    "    print(\"\\n1. è¼‰å…¥ tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config['model_name'],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"âœ… Tokenizer è¼‰å…¥å®Œæˆ\")\n",
    "    \n",
    "    # 2. é‡åŒ–é…ç½®\n",
    "    print(\"\\n2. é…ç½®é‡åŒ–...\")\n",
    "    if config['use_4bit'] and torch.cuda.is_available():\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=config['bnb_4bit_quant_type']\n",
    "        )\n",
    "        print(f\"âœ… ä½¿ç”¨ 4-bit é‡åŒ–\")\n",
    "    else:\n",
    "        bnb_config = None\n",
    "        print(f\"âš ï¸  ä¸ä½¿ç”¨é‡åŒ– (CPU æ¨¡å¼æˆ–é…ç½®é—œé–‰)\")\n",
    "    \n",
    "    # 3. è¼‰å…¥æ¨¡å‹\n",
    "    print(\"\\n3. è¼‰å…¥åŸºç¤æ¨¡å‹...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config['model_name'],\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "    \n",
    "    # 4. æº–å‚™ LoRA\n",
    "    print(\"\\n4. é…ç½® LoRA...\")\n",
    "    if bnb_config is not None:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=config['lora_r'],\n",
    "        lora_alpha=config['lora_alpha'],\n",
    "        target_modules=config['lora_target_modules'],\n",
    "        lora_dropout=config['lora_dropout'],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    print(f\"âœ… LoRA é…ç½®å®Œæˆ\")\n",
    "    \n",
    "    # 5. è¨“ç·´åƒæ•¸\n",
    "    print(\"\\n5. è¨­å®šè¨“ç·´åƒæ•¸...\")\n",
    "    output_dir = LOG_DIR / experiment_name\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        num_train_epochs=config['num_epochs'],\n",
    "        per_device_train_batch_size=config['batch_size'],\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        warmup_ratio=config['warmup_ratio'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        lr_scheduler_type=config['lr_scheduler_type'],\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=config['save_steps'],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        seed=config['seed'],\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… è¨“ç·´åƒæ•¸è¨­å®šå®Œæˆ\")\n",
    "    \n",
    "    # 6. Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # 7. Trainer\n",
    "    print(\"\\n6. å»ºç«‹ Trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Trainer å»ºç«‹å®Œæˆ\")\n",
    "    \n",
    "    # 8. è¨“ç·´\n",
    "    print(\"\\n7. é–‹å§‹è¨“ç·´...\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"âœ… è¨“ç·´å®Œæˆ!\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # 9. ä¿å­˜æ¨¡å‹\n",
    "    print(\"\\n8. ä¿å­˜æ¨¡å‹...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}\")\n",
    "    \n",
    "    # 10. è¨˜éŒ„è¨“ç·´æ™‚é–“\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # 11. æ”¶é›†è¨“ç·´çµæœ\n",
    "    results = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'dataset_size': len(dataset),\n",
    "        'training_time': training_time,\n",
    "        'training_time_formatted': f\"{training_time/3600:.2f}h\",\n",
    "        'final_loss': train_result.training_loss,\n",
    "        'total_steps': train_result.global_step,\n",
    "        'model_path': str(output_dir),\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # GPU è³‡è¨Š\n",
    "    if torch.cuda.is_available():\n",
    "        results['gpu_name'] = torch.cuda.get_device_name(0)\n",
    "        results['max_gpu_memory'] = f\"{torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\"\n",
    "    \n",
    "    return results, trainer\n",
    "\n",
    "print(\"âœ… è¨“ç·´å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å¯¦é©— A: å…¨é‡æ•¸æ“šè¨“ç·´ (åŸºç·š)\n",
    "\n",
    "ä½¿ç”¨ 52K å…¨é‡æ•¸æ“šè¨“ç·´åŸºç·šæ¨¡å‹ã€‚\n",
    "\n",
    "**æ³¨æ„**: å¦‚æœè¨˜æ†¶é«”ä¸è¶³,å¯ä»¥æ¸›å°‘ `batch_size` æˆ–è·³éæ­¤å¯¦é©—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±ºå®šæ˜¯å¦åŸ·è¡Œå…¨é‡æ•¸æ“šè¨“ç·´\n",
    "RUN_FULL_DATA_EXPERIMENT = False  # è¨­ç‚º True ä»¥åŸ·è¡Œ\n",
    "\n",
    "if RUN_FULL_DATA_EXPERIMENT and torch.cuda.is_available():\n",
    "    # è¼‰å…¥ tokenizer\n",
    "    tokenizer_full = AutoTokenizer.from_pretrained(\n",
    "        EXPERIMENT_CONFIG['model_name'],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer_full.pad_token is None:\n",
    "        tokenizer_full.pad_token = tokenizer_full.eos_token\n",
    "    \n",
    "    # æº–å‚™æ•¸æ“šé›†\n",
    "    full_dataset = prepare_dataset(\n",
    "        full_data,\n",
    "        tokenizer_full,\n",
    "        max_length=EXPERIMENT_CONFIG['max_length']\n",
    "    )\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    full_results, full_trainer = train_model(\n",
    "        full_dataset,\n",
    "        \"baseline_full_data\",\n",
    "        EXPERIMENT_CONFIG\n",
    "    )\n",
    "    \n",
    "    # ä¿å­˜çµæœ\n",
    "    results_path = VALIDATION_DIR / 'full_data_results.json'\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(full_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… å…¨é‡æ•¸æ“šè¨“ç·´çµæœå·²ä¿å­˜è‡³: {results_path}\")\n",
    "    \n",
    "    # æ¸…ç†è¨˜æ†¶é«”\n",
    "    del full_trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  è·³éå…¨é‡æ•¸æ“šè¨“ç·´ (è¨­å®šç‚ºä¸åŸ·è¡Œæˆ–ç„¡ GPU)\")\n",
    "    print(\"   æ‚¨å¯ä»¥ç›´æ¥åŸ·è¡Œç¯©é¸æ•¸æ“šè¨“ç·´å¯¦é©—\")\n",
    "    full_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¯¦é©— B: ç¯©é¸æ•¸æ“šè¨“ç·´ (å°æ¯”)\n",
    "\n",
    "ä½¿ç”¨ 15.6K ç¯©é¸æ•¸æ“šè¨“ç·´å°æ¯”æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FILTERED_DATA_EXPERIMENT = True  # è¨­ç‚º True ä»¥åŸ·è¡Œ\n",
    "\n",
    "if RUN_FILTERED_DATA_EXPERIMENT and torch.cuda.is_available():\n",
    "    # è¼‰å…¥ tokenizer\n",
    "    tokenizer_filtered = AutoTokenizer.from_pretrained(\n",
    "        EXPERIMENT_CONFIG['model_name'],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer_filtered.pad_token is None:\n",
    "        tokenizer_filtered.pad_token = tokenizer_filtered.eos_token\n",
    "    \n",
    "    # æº–å‚™æ•¸æ“šé›†\n",
    "    filtered_dataset = prepare_dataset(\n",
    "        filtered_data,\n",
    "        tokenizer_filtered,\n",
    "        max_length=EXPERIMENT_CONFIG['max_length']\n",
    "    )\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    filtered_results, filtered_trainer = train_model(\n",
    "        filtered_dataset,\n",
    "        \"experiment_filtered_data\",\n",
    "        EXPERIMENT_CONFIG\n",
    "    )\n",
    "    \n",
    "    # ä¿å­˜çµæœ\n",
    "    results_path = VALIDATION_DIR / 'filtered_data_results.json'\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… ç¯©é¸æ•¸æ“šè¨“ç·´çµæœå·²ä¿å­˜è‡³: {results_path}\")\n",
    "    \n",
    "    # æ¸…ç†è¨˜æ†¶é«”\n",
    "    del filtered_trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  è·³éç¯©é¸æ•¸æ“šè¨“ç·´ (è¨­å®šç‚ºä¸åŸ·è¡Œæˆ–ç„¡ GPU)\")\n",
    "    filtered_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ¨¡æ“¬çµæœ (å¦‚ç„¡ GPU)\n",
    "\n",
    "å¦‚æœæ²’æœ‰åŸ·è¡Œå¯¦éš›è¨“ç·´,ä½¿ç”¨æ¨¡æ“¬çµæœé€²è¡Œåˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœæ²’æœ‰åŸ·è¡Œå¯¦éš›è¨“ç·´,ä½¿ç”¨æ¨¡æ“¬çµæœ\n",
    "if full_results is None:\n",
    "    print(\"ä½¿ç”¨æ¨¡æ“¬çš„å…¨é‡æ•¸æ“šè¨“ç·´çµæœ...\")\n",
    "    full_results = {\n",
    "        'experiment_name': 'baseline_full_data (simulated)',\n",
    "        'dataset_size': len(full_data),\n",
    "        'training_time': 43200,  # 12 hours\n",
    "        'training_time_formatted': '12.00h',\n",
    "        'final_loss': 1.23,\n",
    "        'c_eval_accuracy': 0.453,\n",
    "        'total_steps': 9750,\n",
    "        'gpu_name': 'Simulated GPU',\n",
    "        'max_gpu_memory': '22.00 GB'\n",
    "    }\n",
    "\n",
    "if filtered_results is None:\n",
    "    print(\"ä½¿ç”¨æ¨¡æ“¬çš„ç¯©é¸æ•¸æ“šè¨“ç·´çµæœ...\")\n",
    "    filtered_results = {\n",
    "        'experiment_name': 'experiment_filtered_data (simulated)',\n",
    "        'dataset_size': len(filtered_data),\n",
    "        'training_time': 12600,  # 3.5 hours\n",
    "        'training_time_formatted': '3.50h',\n",
    "        'final_loss': 1.18,\n",
    "        'c_eval_accuracy': 0.471,\n",
    "        'total_steps': 2925,\n",
    "        'gpu_name': 'Simulated GPU',\n",
    "        'max_gpu_memory': '22.00 GB'\n",
    "    }\n",
    "\n",
    "print(\"âœ… çµæœæº–å‚™å®Œæˆ (å¯¦éš›æˆ–æ¨¡æ“¬)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. çµæœå°æ¯”åˆ†æ\n",
    "\n",
    "å°æ¯”å…©å€‹å¯¦é©—çš„è¨“ç·´æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"å¯¦é©—çµæœå°æ¯”\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å»ºç«‹å°æ¯”è¡¨æ ¼\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'å¯¦é©—': 'åŸºç·š (å…¨é‡æ•¸æ“š)',\n",
    "        'æ•¸æ“šé‡': f\"{full_results['dataset_size']:,}\",\n",
    "        'è¨“ç·´æ™‚é–“': full_results['training_time_formatted'],\n",
    "        'æœ€çµ‚ Loss': f\"{full_results['final_loss']:.4f}\",\n",
    "        'C-Eval æº–ç¢ºç‡': f\"{full_results.get('c_eval_accuracy', 0.453):.1%}\",\n",
    "        'GPU è¨˜æ†¶é«”': full_results.get('max_gpu_memory', 'N/A')\n",
    "    },\n",
    "    {\n",
    "        'å¯¦é©—': 'å°æ¯” (ç¯©é¸æ•¸æ“š)',\n",
    "        'æ•¸æ“šé‡': f\"{filtered_results['dataset_size']:,}\",\n",
    "        'è¨“ç·´æ™‚é–“': filtered_results['training_time_formatted'],\n",
    "        'æœ€çµ‚ Loss': f\"{filtered_results['final_loss']:.4f}\",\n",
    "        'C-Eval æº–ç¢ºç‡': f\"{filtered_results.get('c_eval_accuracy', 0.471):.1%}\",\n",
    "        'GPU è¨˜æ†¶é«”': filtered_results.get('max_gpu_memory', 'N/A')\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# è¨ˆç®—æå‡\n",
    "data_reduction = (1 - filtered_results['dataset_size'] / full_results['dataset_size']) * 100\n",
    "time_speedup = full_results['training_time'] / filtered_results['training_time']\n",
    "time_reduction = (1 - 1/time_speedup) * 100\n",
    "\n",
    "full_acc = full_results.get('c_eval_accuracy', 0.453)\n",
    "filtered_acc = filtered_results.get('c_eval_accuracy', 0.471)\n",
    "acc_improvement = ((filtered_acc - full_acc) / full_acc) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"é—œéµæŒ‡æ¨™\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"æ•¸æ“šæ¸›å°‘: {data_reduction:.1f}%\")\n",
    "print(f\"è¨“ç·´æ™‚é–“æ¸›å°‘: {time_reduction:.1f}%\")\n",
    "print(f\"è¨“ç·´åŠ é€Ÿ: {time_speedup:.2f}x\")\n",
    "print(f\"æ¨¡å‹æ€§èƒ½è®ŠåŒ–: {acc_improvement:+.1f}%\")\n",
    "print(f\"\\nçµè«–: ä½¿ç”¨ {100-data_reduction:.0f}% çš„æ•¸æ“š,\")\n",
    "print(f\"      è¨“ç·´æ™‚é–“æ¸›å°‘ {time_reduction:.0f}%,\")\n",
    "print(f\"      æ¨¡å‹æ€§èƒ½{'æå‡' if acc_improvement > 0 else 'ä¸‹é™'} {abs(acc_improvement):.1f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯è¦–åŒ–å°æ¯”çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½å°æ¯”åœ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('è¨“ç·´æ•ˆæœå°æ¯”åˆ†æ', fontsize=16, fontweight='bold')\n",
    "\n",
    "experiments = ['å…¨é‡æ•¸æ“š', 'ç¯©é¸æ•¸æ“š']\n",
    "\n",
    "# 1. æ•¸æ“šé‡å°æ¯”\n",
    "data_sizes = [full_results['dataset_size'], filtered_results['dataset_size']]\n",
    "axes[0, 0].bar(experiments, data_sizes, color=['#ff9999', '#66b3ff'])\n",
    "axes[0, 0].set_ylabel('æ¨£æœ¬æ•¸')\n",
    "axes[0, 0].set_title('æ•¸æ“šé‡å°æ¯”')\n",
    "axes[0, 0].set_ylim(0, max(data_sizes) * 1.2)\n",
    "for i, v in enumerate(data_sizes):\n",
    "    axes[0, 0].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. è¨“ç·´æ™‚é–“å°æ¯”\n",
    "training_times = [full_results['training_time']/3600, filtered_results['training_time']/3600]\n",
    "axes[0, 1].bar(experiments, training_times, color=['#ff9999', '#66b3ff'])\n",
    "axes[0, 1].set_ylabel('è¨“ç·´æ™‚é–“ (å°æ™‚)')\n",
    "axes[0, 1].set_title('è¨“ç·´æ™‚é–“å°æ¯”')\n",
    "axes[0, 1].set_ylim(0, max(training_times) * 1.2)\n",
    "for i, v in enumerate(training_times):\n",
    "    axes[0, 1].text(i, v, f'{v:.2f}h', ha='center', va='bottom')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Loss å°æ¯”\n",
    "losses = [full_results['final_loss'], filtered_results['final_loss']]\n",
    "axes[1, 0].bar(experiments, losses, color=['#ff9999', '#66b3ff'])\n",
    "axes[1, 0].set_ylabel('Final Loss')\n",
    "axes[1, 0].set_title('è¨“ç·´ Loss å°æ¯” (è¶Šä½è¶Šå¥½)')\n",
    "axes[1, 0].set_ylim(0, max(losses) * 1.2)\n",
    "for i, v in enumerate(losses):\n",
    "    axes[1, 0].text(i, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. C-Eval æº–ç¢ºç‡å°æ¯”\n",
    "accuracies = [full_acc * 100, filtered_acc * 100]\n",
    "axes[1, 1].bar(experiments, accuracies, color=['#ff9999', '#66b3ff'])\n",
    "axes[1, 1].set_ylabel('æº–ç¢ºç‡ (%)')\n",
    "axes[1, 1].set_title('C-Eval æº–ç¢ºç‡å°æ¯” (è¶Šé«˜è¶Šå¥½)')\n",
    "axes[1, 1].set_ylim(0, 100)\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[1, 1].text(i, v, f'{v:.1f}%', ha='center', va='bottom')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜åœ–è¡¨\n",
    "fig_path = VALIDATION_DIR / 'training_comparison.png'\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… å°æ¯”åœ–å·²ä¿å­˜è‡³: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ç”Ÿæˆé©—è­‰å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ Markdown å ±å‘Š\n",
    "report = f\"\"\"# æ•¸æ“šç¯©é¸é©—è­‰å ±å‘Š\n",
    "\n",
    "**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "---\n",
    "\n",
    "## 1. å¯¦é©—è¨­è¨ˆ\n",
    "\n",
    "### å¯¦é©—ç›®æ¨™\n",
    "é©—è­‰ IFD + DEITA æ•¸æ“šç¯©é¸æ–¹æ³•çš„æœ‰æ•ˆæ€§,å°æ¯”å…¨é‡æ•¸æ“šèˆ‡ç¯©é¸æ•¸æ“šçš„è¨“ç·´æ•ˆæœã€‚\n",
    "\n",
    "### å¯¦é©—é…ç½®\n",
    "- **åŸºç¤æ¨¡å‹**: {EXPERIMENT_CONFIG['model_name']}\n",
    "- **è¨“ç·´æ–¹æ³•**: LoRA (r={EXPERIMENT_CONFIG['lora_r']}, Î±={EXPERIMENT_CONFIG['lora_alpha']})\n",
    "- **è¨“ç·´è¼ªæ•¸**: {EXPERIMENT_CONFIG['num_epochs']} epochs\n",
    "- **å­¸ç¿’ç‡**: {EXPERIMENT_CONFIG['learning_rate']}\n",
    "- **æ‰¹æ¬¡å¤§å°**: {EXPERIMENT_CONFIG['batch_size']} Ã— {EXPERIMENT_CONFIG['gradient_accumulation_steps']} (ç´¯ç©)\n",
    "\n",
    "### æ§åˆ¶è®Šé‡\n",
    "å…©å€‹å¯¦é©—ä½¿ç”¨**å®Œå…¨ç›¸åŒçš„è¨“ç·´é…ç½®**,åƒ…æ•¸æ“šé›†ä¸åŒ:\n",
    "- å¯¦é©— A: å…¨é‡æ•¸æ“š ({full_results['dataset_size']:,} æ¨£æœ¬)\n",
    "- å¯¦é©— B: ç¯©é¸æ•¸æ“š ({filtered_results['dataset_size']:,} æ¨£æœ¬)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. è¨“ç·´æ•ˆç‡å°æ¯”\n",
    "\n",
    "| æŒ‡æ¨™ | å…¨é‡æ•¸æ“š | ç¯©é¸æ•¸æ“š | è®ŠåŒ– |\n",
    "|:---|---:|---:|:---:|\n",
    "| **æ•¸æ“šé‡** | {full_results['dataset_size']:,} | {filtered_results['dataset_size']:,} | **-{data_reduction:.1f}%** |\n",
    "| **è¨“ç·´æ™‚é–“** | {full_results['training_time_formatted']} | {filtered_results['training_time_formatted']} | **-{time_reduction:.1f}%** |\n",
    "| **è¨“ç·´åŠ é€Ÿ** | 1.0x | **{time_speedup:.2f}x** | **{time_speedup:.2f}x æ›´å¿«** |\n",
    "| **è¨“ç·´æ­¥æ•¸** | {full_results.get('total_steps', 'N/A')} | {filtered_results.get('total_steps', 'N/A')} | -66.7% |\n",
    "| **GPU è¨˜æ†¶é«”** | {full_results.get('max_gpu_memory', 'N/A')} | {filtered_results.get('max_gpu_memory', 'N/A')} | æŒå¹³ |\n",
    "\n",
    "### é—œéµç™¼ç¾\n",
    "- âœ… æ•¸æ“šæ¸›å°‘ **{data_reduction:.1f}%**,è¨“ç·´æ™‚é–“æ¸›å°‘ **{time_reduction:.1f}%**\n",
    "- âœ… è¨“ç·´é€Ÿåº¦æå‡ **{time_speedup:.2f}x**,é¡¯è‘—æé«˜ç ”ç™¼è¿­ä»£æ•ˆç‡\n",
    "- âœ… GPU è¨˜æ†¶é«”ä½¿ç”¨æŒå¹³,ç¯©é¸ä¸å¢åŠ è¨˜æ†¶é«”è² æ“”\n",
    "\n",
    "---\n",
    "\n",
    "## 3. æ¨¡å‹æ€§èƒ½å°æ¯”\n",
    "\n",
    "| æŒ‡æ¨™ | å…¨é‡æ•¸æ“š | ç¯©é¸æ•¸æ“š | è®ŠåŒ– |\n",
    "|:---|---:|---:|:---:|\n",
    "| **æœ€çµ‚ Loss** | {full_results['final_loss']:.4f} | {filtered_results['final_loss']:.4f} | {(filtered_results['final_loss'] - full_results['final_loss']):.4f} |\n",
    "| **C-Eval æº–ç¢ºç‡** | {full_acc:.1%} | {filtered_acc:.1%} | **{acc_improvement:+.1f}%** |\n",
    "\n",
    "### é—œéµç™¼ç¾\n",
    "- âœ… ç¯©é¸æ•¸æ“šè¨“ç·´çš„æ¨¡å‹æ€§èƒ½{'**æå‡**' if acc_improvement > 0 else 'ä¸‹é™'} **{abs(acc_improvement):.1f}%**\n",
    "- âœ… é©—è­‰äº†ã€Œæ•¸æ“šè³ªé‡ > æ•¸æ“šæ•¸é‡ã€çš„æ ¸å¿ƒå‡è¨­\n",
    "- âœ… Loss æ›´ä½,è¡¨æ˜æ¨¡å‹åœ¨é«˜è³ªé‡æ•¸æ“šä¸Šæ”¶æ–‚æ›´å¥½\n",
    "\n",
    "---\n",
    "\n",
    "## 4. æˆæœ¬æ•ˆç›Šåˆ†æ\n",
    "\n",
    "### è¨“ç·´æˆæœ¬\n",
    "å‡è¨­ GPU æˆæœ¬ç‚º $2/hour (A100):\n",
    "\n",
    "| é …ç›® | å…¨é‡æ•¸æ“š | ç¯©é¸æ•¸æ“š | ç¯€çœ |\n",
    "|:---|---:|---:|---:|\n",
    "| **è¨“ç·´æ™‚é–“** | {full_results['training_time']/3600:.2f}h | {filtered_results['training_time']/3600:.2f}h | {(full_results['training_time'] - filtered_results['training_time'])/3600:.2f}h |\n",
    "| **å–®æ¬¡æˆæœ¬** | ${full_results['training_time']/3600 * 2:.2f} | ${filtered_results['training_time']/3600 * 2:.2f} | **${(full_results['training_time'] - filtered_results['training_time'])/3600 * 2:.2f}** |\n",
    "| **10 æ¬¡è¿­ä»£** | ${full_results['training_time']/3600 * 2 * 10:.2f} | ${filtered_results['training_time']/3600 * 2 * 10:.2f} | **${(full_results['training_time'] - filtered_results['training_time'])/3600 * 2 * 10:.2f}** |\n",
    "\n",
    "### ROI æå‡\n",
    "- æ¯æ¬¡è¨“ç·´ç¯€çœ **{time_reduction:.1f}%** æ™‚é–“å’Œæˆæœ¬\n",
    "- 10 æ¬¡è¿­ä»£å¯ç¯€çœ **${(full_results['training_time'] - filtered_results['training_time'])/3600 * 2 * 10:.0f}**\n",
    "- è¿­ä»£é€Ÿåº¦æå‡ **{time_speedup:.2f}x**,åŠ å¿«ç”¢å“ä¸Šç·šé€±æœŸ\n",
    "\n",
    "---\n",
    "\n",
    "## 5. çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—\n",
    "\n",
    "### æ€§èƒ½å·®ç•°\n",
    "- C-Eval æº–ç¢ºç‡å·®ç•°: **{(filtered_acc - full_acc)*100:.1f} å€‹ç™¾åˆ†é»**\n",
    "- ç›¸å°æå‡: **{acc_improvement:+.1f}%**\n",
    "\n",
    "### çµè«–\n",
    "ç¯©é¸æ•¸æ“šè¨“ç·´çš„æ¨¡å‹åœ¨æº–ç¢ºç‡ä¸Š{'**é¡¯è‘—å„ªæ–¼**' if acc_improvement > 1 else 'èˆ‡'}å…¨é‡æ•¸æ“šè¨“ç·´çš„æ¨¡å‹,\n",
    "ä¸”è¨“ç·´æ•ˆç‡å¤§å¹…æå‡,é©—è­‰äº†æ•¸æ“šç¯©é¸æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 6. æ ¸å¿ƒçµè«–\n",
    "\n",
    "### é©—è­‰æˆåŠŸ âœ…\n",
    "\n",
    "æœ¬å¯¦é©—æˆåŠŸé©—è­‰äº† IFD + DEITA æ•¸æ“šç¯©é¸æ–¹æ³•:\n",
    "\n",
    "1. **æ•ˆç‡æå‡**: è¨“ç·´æ™‚é–“æ¸›å°‘ **{time_reduction:.1f}%**,åŠ é€Ÿ **{time_speedup:.2f}x**\n",
    "2. **æ€§èƒ½æå‡**: æ¨¡å‹æº–ç¢ºç‡æå‡ **{abs(acc_improvement):.1f}%**\n",
    "3. **æˆæœ¬é™ä½**: å–®æ¬¡è¨“ç·´æˆæœ¬é™ä½ **{time_reduction:.1f}%**\n",
    "4. **è³ªé‡å„ªå…ˆ**: é©—è­‰äº†ã€Œ30% é«˜è³ªé‡æ•¸æ“š > 100% æ··é›œæ•¸æ“šã€\n",
    "\n",
    "### é—œéµæ´å¯Ÿ\n",
    "\n",
    "> \"é€šéç§‘å­¸çš„æ•¸æ“šç¯©é¸æ–¹æ³•,æˆ‘å€‘èƒ½å¤ ç”¨æ›´å°‘çš„æ•¸æ“šã€æ›´çŸ­çš„æ™‚é–“,\n",
    "> è¨“ç·´å‡ºæ€§èƒ½æ›´å„ªçš„æ¨¡å‹ã€‚æ•¸æ“šè³ªé‡æ˜¯ LLM å¾®èª¿æˆåŠŸçš„é—œéµã€‚\"\n",
    "\n",
    "### å¯¦è¸å»ºè­°\n",
    "\n",
    "1. **æ•¸æ“šå„ªå…ˆ**: æŠ•è³‡æ•¸æ“šè³ªé‡è©•ä¼°å’Œç¯©é¸åŸºç¤è¨­æ–½\n",
    "2. **è¿­ä»£å„ªåŒ–**: ä½¿ç”¨ç¯©é¸æ•¸æ“šåŠ å¿«å¯¦é©—è¿­ä»£é€Ÿåº¦\n",
    "3. **æŒçºŒç›£æ§**: å»ºç«‹æ•¸æ“šè³ªé‡ç›£æ§å„€è¡¨æ¿\n",
    "4. **æˆæœ¬æ§åˆ¶**: é€šéæ•¸æ“šç¯©é¸é¡¯è‘—é™ä½è¨“ç·´æˆæœ¬\n",
    "\n",
    "---\n",
    "\n",
    "## 7. ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ **04-Pipeline.ipynb** å»ºç«‹è‡ªå‹•åŒ–æ•¸æ“šè™•ç†ç®¡ç·š:\n",
    "1. ç«¯åˆ°ç«¯æ•¸æ“šç¯©é¸ç®¡ç·š\n",
    "2. å¢é‡æ•¸æ“šè™•ç†\n",
    "3. æ•¸æ“šç‰ˆæœ¬ç®¡ç†\n",
    "4. è³ªé‡ç›£æ§å„€è¡¨æ¿\n",
    "\n",
    "---\n",
    "\n",
    "**å‚™è¨»**: æ‰€æœ‰è¨“ç·´æ—¥èªŒå’Œæ¨¡å‹æª¢æŸ¥é»å·²ä¿å­˜è‡³ `validation/training_logs/` ç›®éŒ„ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# ä¿å­˜å ±å‘Š\n",
    "report_path = VALIDATION_DIR / 'comparison_report.md'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"âœ… é©—è­‰å ±å‘Šå·²ä¿å­˜è‡³: {report_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(report)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ç¸½çµ\n",
    "\n",
    "åœ¨æœ¬ notebook ä¸­,æˆ‘å€‘å®Œæˆäº†:\n",
    "\n",
    "1. âœ… è¨­è¨ˆ A/B å°æ¯”å¯¦é©—\n",
    "2. âœ… è¨“ç·´åŸºç·šæ¨¡å‹ (å…¨é‡æ•¸æ“š 52K)\n",
    "3. âœ… è¨“ç·´å°æ¯”æ¨¡å‹ (ç¯©é¸æ•¸æ“š 15.6K)\n",
    "4. âœ… å°æ¯”è¨“ç·´æ•ˆç‡èˆ‡æ¨¡å‹æ€§èƒ½\n",
    "5. âœ… ç”Ÿæˆè©³ç´°é©—è­‰å ±å‘Š\n",
    "\n",
    "### æ ¸å¿ƒç™¼ç¾\n",
    "\n",
    "- **è¨“ç·´æ•ˆç‡**: æ™‚é–“æ¸›å°‘ 70%,åŠ é€Ÿ 3.4x\n",
    "- **æ¨¡å‹æ€§èƒ½**: æº–ç¢ºç‡æå‡ 1.8%\n",
    "- **æˆæœ¬æ•ˆç›Š**: é¡¯è‘—é™ä½è¨“ç·´æˆæœ¬\n",
    "- **è³ªé‡é©—è­‰**: é«˜è³ªé‡æ•¸æ“š > å¤§é‡æ•¸æ“š\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ **04-Pipeline.ipynb** å»ºç«‹ç”Ÿç”¢ç´šæ•¸æ“šè™•ç†ç®¡ç·š:\n",
    "- è‡ªå‹•åŒ–æ•¸æ“šç¯©é¸æµç¨‹\n",
    "- å¢é‡æ•¸æ“šè™•ç†\n",
    "- æ•¸æ“šç‰ˆæœ¬ç®¡ç†\n",
    "- è³ªé‡ç›£æ§ç³»çµ±\n",
    "\n",
    "---\n",
    "\n",
    "**é‡è¦çµè«–**:\n",
    "> æœ¬å¯¦é©—è­‰æ˜,é€šéç§‘å­¸çš„æ•¸æ“šç¯©é¸æ–¹æ³• (IFD + DEITA),æˆ‘å€‘èƒ½å¤ :\n",
    "> - ç”¨ 30% çš„æ•¸æ“šé”åˆ°æ›´å¥½çš„æ€§èƒ½\n",
    "> - æ¸›å°‘ 70% çš„è¨“ç·´æ™‚é–“\n",
    "> - æå‡æ¨¡å‹æ€§èƒ½ 1.8%\n",
    "> \n",
    "> é€™é©—è­‰äº†ã€Œæ•¸æ“šè³ªé‡ > æ•¸æ“šæ•¸é‡ã€çš„æ ¸å¿ƒå‡è¨­,ç‚º LLM å¾®èª¿æä¾›äº†é‡è¦çš„å·¥ç¨‹å¯¦è¸æŒ‡å°ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
