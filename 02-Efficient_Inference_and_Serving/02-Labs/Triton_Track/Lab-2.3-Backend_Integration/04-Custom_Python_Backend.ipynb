{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Lab 2.3.4 - 自定義 Python Backend 開發\n",
    "\n",
    "## 🎯 實驗目標\n",
    "\n",
    "本實驗將教您如何：\n",
    "1. 開發自定義 Python Backend 處理複雜邏輯\n",
    "2. 實現多步驟推理流程\n",
    "3. 整合外部服務和 API\n",
    "4. 處理動態批次和流式輸出\n",
    "5. 監控和調試 Python Backend\n",
    "\n",
    "## 📋 前置需求\n",
    "\n",
    "- 完成 Lab 2.1（Triton 基礎設置）\n",
    "- 熟悉 Python 編程和異步處理\n",
    "- 了解 REST API 和 gRPC 協議\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 📚 理論背景\n",
    "\n",
    "### Python Backend 的優勢\n",
    "\n",
    "**1. 靈活性**\n",
    "- 可以實現任意複雜的推理邏輯\n",
    "- 支持動態模型加載和切換\n",
    "- 易於集成外部庫和服務\n",
    "\n",
    "**2. 快速開發**\n",
    "- Python 生態系統豐富\n",
    "- 調試和測試便利\n",
    "- 快速原型驗證\n",
    "\n",
    "**3. 適用場景**\n",
    "- 多步驟推理管道\n",
    "- 複雜的預/後處理\n",
    "- 集成外部 API 和數據庫\n",
    "- A/B 測試和實驗功能\n",
    "\n",
    "### Python Backend 架構\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Client Request] --> B[Triton Server]\n",
    "    B --> C[Python Backend]\n",
    "    C --> D[Model Instance]\n",
    "    D --> E[Execute Function]\n",
    "    E --> F[External API]\n",
    "    E --> G[Database]\n",
    "    E --> H[Other Models]\n",
    "    E --> I[Response Processing]\n",
    "    I --> B\n",
    "    B --> A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 🛠️ 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Triton 相關\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# 機器學習相關\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 檢查環境\n",
    "print(f\"Python version: {__import__('sys').version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置實驗路徑\n",
    "BASE_DIR = \"/opt/tritonserver\"\n",
    "MODEL_REPO = f\"{BASE_DIR}/models\"\n",
    "PYTHON_BACKEND_DIR = f\"{MODEL_REPO}/custom_pipeline\"\n",
    "\n",
    "# 創建目錄結構\n",
    "os.makedirs(f\"{PYTHON_BACKEND_DIR}/1\", exist_ok=True)\n",
    "\n",
    "print(f\"Model repository: {MODEL_REPO}\")\n",
    "print(f\"Python backend directory: {PYTHON_BACKEND_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 🎯 實驗 1：基礎 Python Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 1.1 創建模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基礎 Python Backend 配置\n",
    "config_pbtxt = '''\n",
    "name: \"custom_pipeline\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"text_input\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"parameters\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"processed_text\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"metadata\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 2\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/python_env.tar.gz\"}\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(f\"{PYTHON_BACKEND_DIR}/config.pbtxt\", \"w\") as f:\n",
    "    f.write(config_pbtxt)\n",
    "\n",
    "print(\"✅ 基礎配置創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 1.2 實現 Python Backend 邏輯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基礎 Python Backend 實現\n",
    "python_backend_code = '''\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"\n",
    "    自定義 Python Backend 模型\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        模型初始化\n",
    "        \"\"\"\n",
    "        self.model_config = model_config = json.loads(args['model_config'])\n",
    "        \n",
    "        # 獲取輸入輸出配置\n",
    "        input_configs = pb_utils.get_input_config_by_name(\n",
    "            model_config, \"text_input\"\n",
    "        )\n",
    "        output_configs = pb_utils.get_output_config_by_name(\n",
    "            model_config, \"processed_text\"\n",
    "        )\n",
    "        \n",
    "        # 初始化處理管道組件\n",
    "        self.text_processors = {\n",
    "            \"uppercase\": lambda x: x.upper(),\n",
    "            \"lowercase\": lambda x: x.lower(),\n",
    "            \"reverse\": lambda x: x[::-1],\n",
    "            \"word_count\": lambda x: f\"Words: {len(x.split())}\",\n",
    "        }\n",
    "        \n",
    "        # 統計信息\n",
    "        self.request_count = 0\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        print(f\"✅ Python Backend 初始化完成\")\n",
    "        print(f\"📊 可用處理器: {list(self.text_processors.keys())}\")\n",
    "\n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        處理推理請求\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            # 解析輸入\n",
    "            text_input = pb_utils.get_input_tensor_by_name(\n",
    "                request, \"text_input\"\n",
    "            ).as_numpy().astype(str)[0]\n",
    "            \n",
    "            # 解析參數（可選）\n",
    "            parameters = {}\n",
    "            try:\n",
    "                param_tensor = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"parameters\"\n",
    "                )\n",
    "                if param_tensor is not None:\n",
    "                    parameters = json.loads(\n",
    "                        param_tensor.as_numpy().astype(str)[0]\n",
    "                    )\n",
    "            except:\n",
    "                parameters = {}\n",
    "            \n",
    "            # 執行處理\n",
    "            processed_text, metadata = self._process_text(\n",
    "                text_input, parameters\n",
    "            )\n",
    "            \n",
    "            # 創建輸出張量\n",
    "            processed_tensor = pb_utils.Tensor(\n",
    "                \"processed_text\",\n",
    "                np.array([processed_text], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            metadata_tensor = pb_utils.Tensor(\n",
    "                \"metadata\",\n",
    "                np.array([json.dumps(metadata)], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            # 創建響應\n",
    "            response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[processed_tensor, metadata_tensor]\n",
    "            )\n",
    "            responses.append(response)\n",
    "            \n",
    "            # 更新統計\n",
    "            self.request_count += 1\n",
    "        \n",
    "        return responses\n",
    "\n",
    "    def _process_text(self, text: str, parameters: dict) -> tuple:\n",
    "        \"\"\"\n",
    "        文本處理邏輯\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 獲取處理類型\n",
    "        process_type = parameters.get(\"type\", \"uppercase\")\n",
    "        \n",
    "        # 執行處理\n",
    "        if process_type in self.text_processors:\n",
    "            processed = self.text_processors[process_type](text)\n",
    "        else:\n",
    "            processed = text\n",
    "        \n",
    "        # 創建元數據\n",
    "        metadata = {\n",
    "            \"original_length\": len(text),\n",
    "            \"processed_length\": len(processed),\n",
    "            \"process_type\": process_type,\n",
    "            \"processing_time_ms\": (time.time() - start_time) * 1000,\n",
    "            \"request_id\": self.request_count + 1,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        \n",
    "        return processed, metadata\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        模型清理\n",
    "        \"\"\"\n",
    "        total_time = time.time() - self.start_time\n",
    "        print(f\"🏁 Python Backend 結束\")\n",
    "        print(f\"📈 總請求數: {self.request_count}\")\n",
    "        print(f\"⏱️  總運行時間: {total_time:.2f}s\")\n",
    "        print(f\"📊 平均 QPS: {self.request_count/total_time:.2f}\")\n",
    "'''\n",
    "\n",
    "with open(f\"{PYTHON_BACKEND_DIR}/1/model.py\", \"w\") as f:\n",
    "    f.write(python_backend_code)\n",
    "\n",
    "print(\"✅ Python Backend 代碼創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 🎯 實驗 2：高級 Pipeline Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建高級管道模型目錄\n",
    "ADVANCED_PIPELINE_DIR = f\"{MODEL_REPO}/advanced_pipeline\"\n",
    "os.makedirs(f\"{ADVANCED_PIPELINE_DIR}/1\", exist_ok=True)\n",
    "\n",
    "# 高級管道配置\n",
    "advanced_config = '''\n",
    "name: \"advanced_pipeline\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 4\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"query\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"context\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "    optional: true\n",
    "  },\n",
    "  {\n",
    "    name: \"config\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"answer\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"confidence\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"sources\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"pipeline_info\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus: [ 0 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(f\"{ADVANCED_PIPELINE_DIR}/config.pbtxt\", \"w\") as f:\n",
    "    f.write(advanced_config)\n",
    "\n",
    "print(\"✅ 高級管道配置創建完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高級管道實現\n",
    "advanced_pipeline_code = '''\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import threading\n",
    "import numpy as np\n",
    "import requests\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"\n",
    "    高級管道處理模型\n",
    "    實現多步驟推理：檢索 -> 重排 -> 生成 -> 後處理\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        模型初始化\n",
    "        \"\"\"\n",
    "        self.model_config = json.loads(args['model_config'])\n",
    "        \n",
    "        # 初始化組件\n",
    "        self.retriever = SimpleRetriever()\n",
    "        self.reranker = SimpleReranker()\n",
    "        self.generator = SimpleGenerator()\n",
    "        self.post_processor = PostProcessor()\n",
    "        \n",
    "        # 線程池\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        \n",
    "        # 統計信息\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"avg_pipeline_time\": 0.0,\n",
    "            \"start_time\": time.time()\n",
    "        }\n",
    "        \n",
    "        print(\"🚀 高級管道初始化完成\")\n",
    "        print(\"📋 管道步驟: 檢索 -> 重排 -> 生成 -> 後處理\")\n",
    "\n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        執行管道推理\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            try:\n",
    "                # 解析輸入\n",
    "                query = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"query\"\n",
    "                ).as_numpy().astype(str)[0]\n",
    "                \n",
    "                # 解析上下文\n",
    "                context = []\n",
    "                context_tensor = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"context\"\n",
    "                )\n",
    "                if context_tensor is not None:\n",
    "                    context = context_tensor.as_numpy().astype(str).tolist()\n",
    "                \n",
    "                # 解析配置\n",
    "                config = {}\n",
    "                config_tensor = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"config\"\n",
    "                )\n",
    "                if config_tensor is not None:\n",
    "                    try:\n",
    "                        config = json.loads(\n",
    "                            config_tensor.as_numpy().astype(str)[0]\n",
    "                        )\n",
    "                    except:\n",
    "                        config = {}\n",
    "                \n",
    "                # 執行管道\n",
    "                result = self._execute_pipeline(query, context, config)\n",
    "                \n",
    "                # 創建響應\n",
    "                response = self._create_response(result)\n",
    "                responses.append(response)\n",
    "                \n",
    "                self.stats[\"successful_requests\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ 請求處理失敗: {str(e)}\")\n",
    "                \n",
    "                # 創建錯誤響應\n",
    "                error_response = self._create_error_response(str(e))\n",
    "                responses.append(error_response)\n",
    "                \n",
    "                self.stats[\"failed_requests\"] += 1\n",
    "            \n",
    "            self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        return responses\n",
    "\n",
    "    def _execute_pipeline(self, query: str, context: List[str], config: dict) -> dict:\n",
    "        \"\"\"\n",
    "        執行完整推理管道\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        pipeline_info = {\n",
    "            \"steps\": [],\n",
    "            \"total_time\": 0.0,\n",
    "            \"query\": query,\n",
    "            \"config\": config\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 步驟 1: 檢索相關文檔\n",
    "            step_start = time.time()\n",
    "            if not context:\n",
    "                retrieved_docs = self.retriever.retrieve(query, config.get(\"top_k\", 5))\n",
    "            else:\n",
    "                retrieved_docs = context\n",
    "            \n",
    "            step_time = (time.time() - step_start) * 1000\n",
    "            pipeline_info[\"steps\"].append({\n",
    "                \"name\": \"retrieve\",\n",
    "                \"time_ms\": step_time,\n",
    "                \"output_count\": len(retrieved_docs)\n",
    "            })\n",
    "            \n",
    "            # 步驟 2: 重排序文檔\n",
    "            step_start = time.time()\n",
    "            reranked_docs = self.reranker.rerank(query, retrieved_docs)\n",
    "            \n",
    "            step_time = (time.time() - step_start) * 1000\n",
    "            pipeline_info[\"steps\"].append({\n",
    "                \"name\": \"rerank\",\n",
    "                \"time_ms\": step_time,\n",
    "                \"rerank_changes\": len(reranked_docs)\n",
    "            })\n",
    "            \n",
    "            # 步驟 3: 生成答案\n",
    "            step_start = time.time()\n",
    "            generation_result = self.generator.generate(\n",
    "                query, reranked_docs, config\n",
    "            )\n",
    "            \n",
    "            step_time = (time.time() - step_start) * 1000\n",
    "            pipeline_info[\"steps\"].append({\n",
    "                \"name\": \"generate\",\n",
    "                \"time_ms\": step_time,\n",
    "                \"tokens_generated\": len(generation_result[\"answer\"].split())\n",
    "            })\n",
    "            \n",
    "            # 步驟 4: 後處理\n",
    "            step_start = time.time()\n",
    "            final_result = self.post_processor.process(\n",
    "                generation_result, config\n",
    "            )\n",
    "            \n",
    "            step_time = (time.time() - step_start) * 1000\n",
    "            pipeline_info[\"steps\"].append({\n",
    "                \"name\": \"post_process\",\n",
    "                \"time_ms\": step_time,\n",
    "                \"final_length\": len(final_result[\"answer\"])\n",
    "            })\n",
    "            \n",
    "            # 計算總時間\n",
    "            pipeline_info[\"total_time\"] = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # 添加源文檔\n",
    "            final_result[\"sources\"] = [doc[\"title\"] for doc in reranked_docs[:3]]\n",
    "            final_result[\"pipeline_info\"] = pipeline_info\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            pipeline_info[\"error\"] = str(e)\n",
    "            pipeline_info[\"total_time\"] = (time.time() - start_time) * 1000\n",
    "            raise e\n",
    "\n",
    "    def _create_response(self, result: dict):\n",
    "        \"\"\"\n",
    "        創建響應張量\n",
    "        \"\"\"\n",
    "        answer_tensor = pb_utils.Tensor(\n",
    "            \"answer\",\n",
    "            np.array([result[\"answer\"]], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        confidence_tensor = pb_utils.Tensor(\n",
    "            \"confidence\",\n",
    "            np.array([result[\"confidence\"]], dtype=np.float32)\n",
    "        )\n",
    "        \n",
    "        sources_tensor = pb_utils.Tensor(\n",
    "            \"sources\",\n",
    "            np.array(result[\"sources\"], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        pipeline_info_tensor = pb_utils.Tensor(\n",
    "            \"pipeline_info\",\n",
    "            np.array([json.dumps(result[\"pipeline_info\"])], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        return pb_utils.InferenceResponse(\n",
    "            output_tensors=[\n",
    "                answer_tensor, confidence_tensor,\n",
    "                sources_tensor, pipeline_info_tensor\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _create_error_response(self, error_msg: str):\n",
    "        \"\"\"\n",
    "        創建錯誤響應\n",
    "        \"\"\"\n",
    "        return pb_utils.InferenceResponse(\n",
    "            output_tensors=[\n",
    "                pb_utils.Tensor(\"answer\", np.array([f\"Error: {error_msg}\"], dtype=np.object_)),\n",
    "                pb_utils.Tensor(\"confidence\", np.array([0.0], dtype=np.float32)),\n",
    "                pb_utils.Tensor(\"sources\", np.array([], dtype=np.object_)),\n",
    "                pb_utils.Tensor(\"pipeline_info\", np.array([json.dumps({\"error\": error_msg})], dtype=np.object_))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        清理資源\n",
    "        \"\"\"\n",
    "        self.executor.shutdown(wait=True)\n",
    "        \n",
    "        total_time = time.time() - self.stats[\"start_time\"]\n",
    "        print(f\"🏁 高級管道結束\")\n",
    "        print(f\"📊 統計信息:\")\n",
    "        print(f\"   總請求: {self.stats['total_requests']}\")\n",
    "        print(f\"   成功: {self.stats['successful_requests']}\")\n",
    "        print(f\"   失敗: {self.stats['failed_requests']}\")\n",
    "        print(f\"   成功率: {self.stats['successful_requests']/max(self.stats['total_requests'], 1)*100:.1f}%\")\n",
    "\n",
    "\n",
    "# 輔助類實現\n",
    "class SimpleRetriever:\n",
    "    def __init__(self):\n",
    "        # 模擬文檔庫\n",
    "        self.documents = [\n",
    "            {\"id\": 1, \"title\": \"Python 基礎\", \"content\": \"Python 是一種解釋型程式語言...\"},\n",
    "            {\"id\": 2, \"title\": \"機器學習入門\", \"content\": \"機器學習是人工智慧的子領域...\"},\n",
    "            {\"id\": 3, \"title\": \"深度學習框架\", \"content\": \"PyTorch 和 TensorFlow 是流行的框架...\"},\n",
    "            {\"id\": 4, \"title\": \"自然語言處理\", \"content\": \"NLP 處理人類語言的計算方法...\"},\n",
    "            {\"id\": 5, \"title\": \"Transformer 架構\", \"content\": \"注意力機制是 Transformer 的核心...\"}\n",
    "        ]\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5):\n",
    "        # 簡單的關鍵詞匹配\n",
    "        scored_docs = []\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            score = 0\n",
    "            if query_lower in doc[\"title\"].lower():\n",
    "                score += 2\n",
    "            if query_lower in doc[\"content\"].lower():\n",
    "                score += 1\n",
    "            \n",
    "            scored_docs.append((doc, score))\n",
    "        \n",
    "        # 排序並返回 top_k\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, score in scored_docs[:top_k]]\n",
    "\n",
    "\n",
    "class SimpleReranker:\n",
    "    def rerank(self, query: str, documents: List[dict]):\n",
    "        # 簡單重排：根據標題相關性\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        scored_docs = []\n",
    "        for doc in documents:\n",
    "            title_words = set(doc[\"title\"].lower().split())\n",
    "            overlap = len(query_words & title_words)\n",
    "            scored_docs.append((doc, overlap))\n",
    "        \n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, score in scored_docs]\n",
    "\n",
    "\n",
    "class SimpleGenerator:\n",
    "    def generate(self, query: str, documents: List[dict], config: dict):\n",
    "        # 簡單生成：基於模板\n",
    "        if not documents:\n",
    "            return {\n",
    "                \"answer\": \"抱歉，沒有找到相關信息。\",\n",
    "                \"confidence\": 0.1\n",
    "            }\n",
    "        \n",
    "        # 使用第一個文檔生成答案\n",
    "        top_doc = documents[0]\n",
    "        answer = f\"根據'{top_doc['title']}'，{top_doc['content'][:100]}...\"\n",
    "        \n",
    "        # 計算置信度（基於文檔數量和匹配度）\n",
    "        confidence = min(0.9, 0.3 + 0.1 * len(documents))\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "\n",
    "\n",
    "class PostProcessor:\n",
    "    def process(self, generation_result: dict, config: dict):\n",
    "        # 後處理：格式化和優化\n",
    "        answer = generation_result[\"answer\"]\n",
    "        \n",
    "        # 應用配置\n",
    "        if config.get(\"format\") == \"markdown\":\n",
    "            answer = f\"**答案:** {answer}\"\n",
    "        \n",
    "        if config.get(\"max_length\"):\n",
    "            max_len = config[\"max_length\"]\n",
    "            if len(answer) > max_len:\n",
    "                answer = answer[:max_len-3] + \"...\"\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": generation_result[\"confidence\"]\n",
    "        }\n",
    "'''\n",
    "\n",
    "with open(f\"{ADVANCED_PIPELINE_DIR}/1/model.py\", \"w\") as f:\n",
    "    f.write(advanced_pipeline_code)\n",
    "\n",
    "print(\"✅ 高級管道代碼創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 🎯 實驗 3：異步處理 Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建異步處理模型目錄\n",
    "ASYNC_BACKEND_DIR = f\"{MODEL_REPO}/async_processor\"\n",
    "os.makedirs(f\"{ASYNC_BACKEND_DIR}/1\", exist_ok=True)\n",
    "\n",
    "# 異步處理配置\n",
    "async_config = '''\n",
    "name: \"async_processor\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"requests\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"async_config\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "    optional: true\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"results\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"status\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  max_queue_delay_microseconds: 500\n",
    "  default_queue_policy {\n",
    "    timeout_action: DELAY\n",
    "    default_timeout_microseconds: 1000\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(f\"{ASYNC_BACKEND_DIR}/config.pbtxt\", \"w\") as f:\n",
    "    f.write(async_config)\n",
    "\n",
    "print(\"✅ 異步處理配置創建完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異步處理實現\n",
    "async_backend_code = '''\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"\n",
    "    異步處理 Backend\n",
    "    支持並發 API 調用和批量處理\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        模型初始化\n",
    "        \"\"\"\n",
    "        self.model_config = json.loads(args['model_config'])\n",
    "        \n",
    "        # 創建事件循環（在新線程中）\n",
    "        self.loop = None\n",
    "        self.loop_thread = None\n",
    "        self._start_event_loop()\n",
    "        \n",
    "        # HTTP 客戶端會話\n",
    "        self.session = None\n",
    "        \n",
    "        # 線程池\n",
    "        self.executor = ThreadPoolExecutor(max_workers=8)\n",
    "        \n",
    "        # 統計信息\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"async_requests\": 0,\n",
    "            \"batch_requests\": 0,\n",
    "            \"avg_batch_size\": 0.0,\n",
    "            \"total_async_time\": 0.0\n",
    "        }\n",
    "        \n",
    "        print(\"🔄 異步處理 Backend 初始化完成\")\n",
    "        print(\"⚡ 支持並發 API 調用和批量處理\")\n",
    "\n",
    "    def _start_event_loop(self):\n",
    "        \"\"\"\n",
    "        在新線程中啟動事件循環\n",
    "        \"\"\"\n",
    "        import threading\n",
    "        \n",
    "        def run_loop():\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(self.loop)\n",
    "            self.loop.run_forever()\n",
    "        \n",
    "        self.loop_thread = threading.Thread(target=run_loop, daemon=True)\n",
    "        self.loop_thread.start()\n",
    "        \n",
    "        # 等待事件循環啟動\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        執行異步處理\n",
    "        \"\"\"\n",
    "        batch_size = len(requests)\n",
    "        self.stats[\"total_requests\"] += batch_size\n",
    "        self.stats[\"batch_requests\"] += 1\n",
    "        self.stats[\"avg_batch_size\"] = self.stats[\"total_requests\"] / self.stats[\"batch_requests\"]\n",
    "        \n",
    "        # 解析所有請求\n",
    "        parsed_requests = []\n",
    "        for request in requests:\n",
    "            try:\n",
    "                request_data = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"requests\"\n",
    "                ).as_numpy().astype(str)[0]\n",
    "                \n",
    "                config_data = {}\n",
    "                config_tensor = pb_utils.get_input_tensor_by_name(\n",
    "                    request, \"async_config\"\n",
    "                )\n",
    "                if config_tensor is not None:\n",
    "                    try:\n",
    "                        config_data = json.loads(\n",
    "                            config_tensor.as_numpy().astype(str)[0]\n",
    "                        )\n",
    "                    except:\n",
    "                        config_data = {}\n",
    "                \n",
    "                parsed_requests.append({\n",
    "                    \"data\": json.loads(request_data),\n",
    "                    \"config\": config_data\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                parsed_requests.append({\n",
    "                    \"data\": {\"error\": str(e)},\n",
    "                    \"config\": {}\n",
    "                })\n",
    "        \n",
    "        # 執行異步處理\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.loop and not self.loop.is_closed():\n",
    "            # 使用異步處理\n",
    "            future = asyncio.run_coroutine_threadsafe(\n",
    "                self._async_process_batch(parsed_requests), self.loop\n",
    "            )\n",
    "            results = future.result(timeout=30)  # 30秒超時\n",
    "            self.stats[\"async_requests\"] += batch_size\n",
    "        else:\n",
    "            # 回退到同步處理\n",
    "            results = self._sync_process_batch(parsed_requests)\n",
    "        \n",
    "        async_time = time.time() - start_time\n",
    "        self.stats[\"total_async_time\"] += async_time\n",
    "        \n",
    "        # 創建響應\n",
    "        responses = []\n",
    "        for result in results:\n",
    "            result_tensor = pb_utils.Tensor(\n",
    "                \"results\",\n",
    "                np.array([json.dumps(result[\"result\"])], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            status_tensor = pb_utils.Tensor(\n",
    "                \"status\",\n",
    "                np.array([result[\"status\"]], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[result_tensor, status_tensor]\n",
    "            )\n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses\n",
    "\n",
    "    async def _async_process_batch(self, requests: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        異步批量處理\n",
    "        \"\"\"\n",
    "        # 創建 HTTP 會話\n",
    "        if self.session is None:\n",
    "            connector = aiohttp.TCPConnector(limit=100)\n",
    "            timeout = aiohttp.ClientTimeout(total=30)\n",
    "            self.session = aiohttp.ClientSession(\n",
    "                connector=connector, timeout=timeout\n",
    "            )\n",
    "        \n",
    "        # 創建異步任務\n",
    "        tasks = []\n",
    "        for req in requests:\n",
    "            if req[\"data\"].get(\"type\") == \"api_call\":\n",
    "                task = self._async_api_call(req[\"data\"], req[\"config\"])\n",
    "            elif req[\"data\"].get(\"type\") == \"computation\":\n",
    "                task = self._async_computation(req[\"data\"], req[\"config\"])\n",
    "            else:\n",
    "                task = self._async_default_process(req[\"data\"], req[\"config\"])\n",
    "            \n",
    "            tasks.append(task)\n",
    "        \n",
    "        # 並發執行所有任務\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # 處理結果\n",
    "        processed_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, Exception):\n",
    "                processed_results.append({\n",
    "                    \"result\": {\"error\": str(result)},\n",
    "                    \"status\": \"error\"\n",
    "                })\n",
    "            else:\n",
    "                processed_results.append({\n",
    "                    \"result\": result,\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "    async def _async_api_call(self, data: dict, config: dict) -> dict:\n",
    "        \"\"\"\n",
    "        異步 API 調用\n",
    "        \"\"\"\n",
    "        url = data.get(\"url\", \"https://httpbin.org/delay/1\")\n",
    "        method = data.get(\"method\", \"GET\").upper()\n",
    "        payload = data.get(\"payload\", {})\n",
    "        \n",
    "        try:\n",
    "            if method == \"GET\":\n",
    "                async with self.session.get(url, params=payload) as response:\n",
    "                    result = await response.json()\n",
    "            else:\n",
    "                async with self.session.post(url, json=payload) as response:\n",
    "                    result = await response.json()\n",
    "            \n",
    "            return {\n",
    "                \"api_result\": result,\n",
    "                \"status_code\": response.status,\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"url\": url\n",
    "            }\n",
    "\n",
    "    async def _async_computation(self, data: dict, config: dict) -> dict:\n",
    "        \"\"\"\n",
    "        異步計算任務\n",
    "        \"\"\"\n",
    "        operation = data.get(\"operation\", \"sum\")\n",
    "        numbers = data.get(\"numbers\", [1, 2, 3, 4, 5])\n",
    "        \n",
    "        # 模擬計算延遲\n",
    "        await asyncio.sleep(0.1)\n",
    "        \n",
    "        if operation == \"sum\":\n",
    "            result = sum(numbers)\n",
    "        elif operation == \"product\":\n",
    "            result = 1\n",
    "            for n in numbers:\n",
    "                result *= n\n",
    "        elif operation == \"average\":\n",
    "            result = sum(numbers) / len(numbers) if numbers else 0\n",
    "        else:\n",
    "            result = len(numbers)\n",
    "        \n",
    "        return {\n",
    "            \"operation\": operation,\n",
    "            \"result\": result,\n",
    "            \"input_count\": len(numbers)\n",
    "        }\n",
    "\n",
    "    async def _async_default_process(self, data: dict, config: dict) -> dict:\n",
    "        \"\"\"\n",
    "        默認異步處理\n",
    "        \"\"\"\n",
    "        # 模擬處理時間\n",
    "        delay = config.get(\"delay\", 0.05)\n",
    "        await asyncio.sleep(delay)\n",
    "        \n",
    "        return {\n",
    "            \"processed_data\": data,\n",
    "            \"processing_time\": delay,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "\n",
    "    def _sync_process_batch(self, requests: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        同步批量處理（回退方案）\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = []\n",
    "            for req in requests:\n",
    "                future = executor.submit(self._sync_process_single, req)\n",
    "                futures.append(future)\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result(timeout=10)\n",
    "                    results.append({\n",
    "                        \"result\": result,\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    results.append({\n",
    "                        \"result\": {\"error\": str(e)},\n",
    "                        \"status\": \"error\"\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _sync_process_single(self, request: dict) -> dict:\n",
    "        \"\"\"\n",
    "        同步處理單個請求\n",
    "        \"\"\"\n",
    "        data = request[\"data\"]\n",
    "        config = request[\"config\"]\n",
    "        \n",
    "        # 模擬處理\n",
    "        time.sleep(config.get(\"delay\", 0.1))\n",
    "        \n",
    "        return {\n",
    "            \"sync_processed\": data,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        清理資源\n",
    "        \"\"\"\n",
    "        # 關閉 HTTP 會話\n",
    "        if self.session and not self.session.closed:\n",
    "            asyncio.run_coroutine_threadsafe(\n",
    "                self.session.close(), self.loop\n",
    "            ).result(timeout=5)\n",
    "        \n",
    "        # 停止事件循環\n",
    "        if self.loop and not self.loop.is_closed():\n",
    "            self.loop.call_soon_threadsafe(self.loop.stop)\n",
    "        \n",
    "        # 關閉線程池\n",
    "        self.executor.shutdown(wait=True)\n",
    "        \n",
    "        print(f\"🏁 異步處理 Backend 結束\")\n",
    "        print(f\"📊 處理統計:\")\n",
    "        print(f\"   總請求: {self.stats['total_requests']}\")\n",
    "        print(f\"   異步請求: {self.stats['async_requests']}\")\n",
    "        print(f\"   批次數量: {self.stats['batch_requests']}\")\n",
    "        print(f\"   平均批次大小: {self.stats['avg_batch_size']:.1f}\")\n",
    "        if self.stats['async_requests'] > 0:\n",
    "            avg_async_time = self.stats['total_async_time'] / self.stats['batch_requests']\n",
    "            print(f\"   平均異步處理時間: {avg_async_time:.3f}s\")\n",
    "'''\n",
    "\n",
    "with open(f\"{ASYNC_BACKEND_DIR}/1/model.py\", \"w\") as f:\n",
    "    f.write(async_backend_code)\n",
    "\n",
    "print(\"✅ 異步處理代碼創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 🧪 測試和驗證"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 測試客戶端代碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Backend 測試客戶端\n",
    "class PythonBackendTester:\n",
    "    def __init__(self, server_url=\"localhost:8000\"):\n",
    "        self.server_url = server_url\n",
    "        self.client = httpclient.InferenceServerClient(\n",
    "            url=server_url, verbose=False\n",
    "        )\n",
    "    \n",
    "    def test_basic_backend(self):\n",
    "        \"\"\"測試基礎 Python Backend\"\"\"\n",
    "        print(\"🧪 測試基礎 Python Backend...\")\n",
    "        \n",
    "        # 準備測試數據\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"text\": \"Hello World\",\n",
    "                \"params\": {\"type\": \"uppercase\"}\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Python Programming\",\n",
    "                \"params\": {\"type\": \"reverse\"}\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Machine Learning is Amazing\",\n",
    "                \"params\": {\"type\": \"word_count\"}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, case in enumerate(test_cases):\n",
    "            try:\n",
    "                # 創建輸入\n",
    "                text_input = httpclient.InferInput(\n",
    "                    \"text_input\", [1], \"BYTES\"\n",
    "                )\n",
    "                text_input.set_data_from_numpy(\n",
    "                    np.array([case[\"text\"]], dtype=np.object_)\n",
    "                )\n",
    "                \n",
    "                params_input = httpclient.InferInput(\n",
    "                    \"parameters\", [1], \"BYTES\"\n",
    "                )\n",
    "                params_input.set_data_from_numpy(\n",
    "                    np.array([json.dumps(case[\"params\"])], dtype=np.object_)\n",
    "                )\n",
    "                \n",
    "                # 創建輸出\n",
    "                outputs = [\n",
    "                    httpclient.InferRequestedOutput(\"processed_text\"),\n",
    "                    httpclient.InferRequestedOutput(\"metadata\")\n",
    "                ]\n",
    "                \n",
    "                # 發送請求\n",
    "                response = self.client.infer(\n",
    "                    \"custom_pipeline\",\n",
    "                    inputs=[text_input, params_input],\n",
    "                    outputs=outputs\n",
    "                )\n",
    "                \n",
    "                # 解析結果\n",
    "                processed_text = response.as_numpy(\"processed_text\")[0].decode()\n",
    "                metadata = json.loads(response.as_numpy(\"metadata\")[0].decode())\n",
    "                \n",
    "                print(f\"📝 測試案例 {i+1}:\")\n",
    "                print(f\"   輸入: '{case['text']}'\")\n",
    "                print(f\"   處理類型: {case['params']['type']}\")\n",
    "                print(f\"   結果: '{processed_text}'\")\n",
    "                print(f\"   處理時間: {metadata['processing_time_ms']:.2f}ms\")\n",
    "                print(\"\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ 測試案例 {i+1} 失敗: {str(e)}\")\n",
    "    \n",
    "    def test_advanced_pipeline(self):\n",
    "        \"\"\"測試高級管道\"\"\"\n",
    "        print(\"🧪 測試高級管道...\")\n",
    "        \n",
    "        try:\n",
    "            # 準備測試數據\n",
    "            query = \"什麼是機器學習？\"\n",
    "            config = {\n",
    "                \"top_k\": 3,\n",
    "                \"format\": \"markdown\",\n",
    "                \"max_length\": 200\n",
    "            }\n",
    "            \n",
    "            # 創建輸入\n",
    "            query_input = httpclient.InferInput(\"query\", [1], \"BYTES\")\n",
    "            query_input.set_data_from_numpy(\n",
    "                np.array([query], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            config_input = httpclient.InferInput(\"config\", [1], \"BYTES\")\n",
    "            config_input.set_data_from_numpy(\n",
    "                np.array([json.dumps(config)], dtype=np.object_)\n",
    "            )\n",
    "            \n",
    "            # 創建輸出\n",
    "            outputs = [\n",
    "                httpclient.InferRequestedOutput(\"answer\"),\n",
    "                httpclient.InferRequestedOutput(\"confidence\"),\n",
    "                httpclient.InferRequestedOutput(\"sources\"),\n",
    "                httpclient.InferRequestedOutput(\"pipeline_info\")\n",
    "            ]\n",
    "            \n",
    "            # 發送請求\n",
    "            start_time = time.time()\n",
    "            response = self.client.infer(\n",
    "                \"advanced_pipeline\",\n",
    "                inputs=[query_input, config_input],\n",
    "                outputs=outputs\n",
    "            )\n",
    "            request_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # 解析結果\n",
    "            answer = response.as_numpy(\"answer\")[0].decode()\n",
    "            confidence = response.as_numpy(\"confidence\")[0]\n",
    "            sources = [s.decode() for s in response.as_numpy(\"sources\")]\n",
    "            pipeline_info = json.loads(\n",
    "                response.as_numpy(\"pipeline_info\")[0].decode()\n",
    "            )\n",
    "            \n",
    "            print(f\"❓ 查詢: '{query}'\")\n",
    "            print(f\"💡 答案: {answer}\")\n",
    "            print(f\"🎯 置信度: {confidence:.2f}\")\n",
    "            print(f\"📚 來源: {', '.join(sources)}\")\n",
    "            print(f\"⏱️  總請求時間: {request_time:.2f}ms\")\n",
    "            print(f\"🔧 管道總時間: {pipeline_info['total_time']:.2f}ms\")\n",
    "            print(\"\")\n",
    "            print(\"📊 管道步驟詳情:\")\n",
    "            for step in pipeline_info['steps']:\n",
    "                print(f\"   {step['name']}: {step['time_ms']:.2f}ms\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 高級管道測試失敗: {str(e)}\")\n",
    "    \n",
    "    def test_async_processor(self):\n",
    "        \"\"\"測試異步處理器\"\"\"\n",
    "        print(\"🧪 測試異步處理器...\")\n",
    "        \n",
    "        # 測試不同類型的異步任務\n",
    "        test_requests = [\n",
    "            {\n",
    "                \"type\": \"api_call\",\n",
    "                \"url\": \"https://httpbin.org/delay/0.5\",\n",
    "                \"method\": \"GET\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"computation\",\n",
    "                \"operation\": \"sum\",\n",
    "                \"numbers\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"computation\",\n",
    "                \"operation\": \"product\",\n",
    "                \"numbers\": [2, 3, 4]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, req_data in enumerate(test_requests):\n",
    "            try:\n",
    "                # 創建輸入\n",
    "                request_input = httpclient.InferInput(\n",
    "                    \"requests\", [1], \"BYTES\"\n",
    "                )\n",
    "                request_input.set_data_from_numpy(\n",
    "                    np.array([json.dumps(req_data)], dtype=np.object_)\n",
    "                )\n",
    "                \n",
    "                config_input = httpclient.InferInput(\n",
    "                    \"async_config\", [1], \"BYTES\"\n",
    "                )\n",
    "                config_input.set_data_from_numpy(\n",
    "                    np.array([json.dumps({\"delay\": 0.1})], dtype=np.object_)\n",
    "                )\n",
    "                \n",
    "                # 創建輸出\n",
    "                outputs = [\n",
    "                    httpclient.InferRequestedOutput(\"results\"),\n",
    "                    httpclient.InferRequestedOutput(\"status\")\n",
    "                ]\n",
    "                \n",
    "                # 發送請求\n",
    "                start_time = time.time()\n",
    "                response = self.client.infer(\n",
    "                    \"async_processor\",\n",
    "                    inputs=[request_input, config_input],\n",
    "                    outputs=outputs\n",
    "                )\n",
    "                request_time = (time.time() - start_time) * 1000\n",
    "                \n",
    "                # 解析結果\n",
    "                results = json.loads(response.as_numpy(\"results\")[0].decode())\n",
    "                status = response.as_numpy(\"status\")[0].decode()\n",
    "                \n",
    "                print(f\"🔄 異步任務 {i+1} ({req_data['type']}):\")\n",
    "                print(f\"   狀態: {status}\")\n",
    "                print(f\"   請求時間: {request_time:.2f}ms\")\n",
    "                print(f\"   結果: {results}\")\n",
    "                print(\"\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ 異步任務 {i+1} 失敗: {str(e)}\")\n",
    "\n",
    "\n",
    "# 創建測試器實例（注意：需要 Triton 服務器運行）\n",
    "print(\"🧪 Python Backend 測試客戶端已準備\")\n",
    "print(\"💡 使用方法:\")\n",
    "print(\"   tester = PythonBackendTester()\")\n",
    "print(\"   tester.test_basic_backend()\")\n",
    "print(\"   tester.test_advanced_pipeline()\")\n",
    "print(\"   tester.test_async_processor()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 📊 性能監控和調試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Backend 監控和調試工具\n",
    "class PythonBackendMonitor:\n",
    "    def __init__(self, server_url=\"localhost:8000\"):\n",
    "        self.server_url = server_url\n",
    "        self.client = httpclient.InferenceServerClient(url=server_url)\n",
    "    \n",
    "    def get_model_status(self, model_name):\n",
    "        \"\"\"獲取模型狀態\"\"\"\n",
    "        try:\n",
    "            model_config = self.client.get_model_config(model_name)\n",
    "            model_stats = self.client.get_inference_statistics(model_name)\n",
    "            \n",
    "            print(f\"📋 模型: {model_name}\")\n",
    "            print(f\"🏷️  Backend: {model_config['backend']}\")\n",
    "            print(f\"📈 最大批次大小: {model_config['max_batch_size']}\")\n",
    "            print(f\"🔢 實例數量: {len(model_config['instance_group'])}\")\n",
    "            \n",
    "            if 'model_stats' in model_stats:\n",
    "                stats = model_stats['model_stats'][0]\n",
    "                print(f\"📊 推理統計:\")\n",
    "                print(f\"   成功請求: {stats['inference_count']}\")\n",
    "                print(f\"   執行時間: {stats['inference_stats']['success']['count']}\")\n",
    "                if stats['inference_stats']['success']['count'] > 0:\n",
    "                    avg_time = stats['inference_stats']['success']['total_time_ns'] / stats['inference_stats']['success']['count'] / 1000000\n",
    "                    print(f\"   平均延遲: {avg_time:.2f}ms\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 無法獲取模型狀態: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def benchmark_model(self, model_name, test_data, num_requests=10, concurrency=1):\n",
    "        \"\"\"模型性能基準測試\"\"\"\n",
    "        print(f\"🚀 開始基準測試: {model_name}\")\n",
    "        print(f\"📋 測試配置: {num_requests} 請求, 並發度 {concurrency}\")\n",
    "        \n",
    "        results = {\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"response_times\": [],\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 簡單的串行測試（可以擴展為並發）\n",
    "        for i in range(num_requests):\n",
    "            try:\n",
    "                request_start = time.time()\n",
    "                \n",
    "                # 根據模型類型準備輸入\n",
    "                if model_name == \"custom_pipeline\":\n",
    "                    inputs = self._prepare_basic_inputs(test_data)\n",
    "                    outputs = [\"processed_text\", \"metadata\"]\n",
    "                elif model_name == \"advanced_pipeline\":\n",
    "                    inputs = self._prepare_advanced_inputs(test_data)\n",
    "                    outputs = [\"answer\", \"confidence\", \"sources\", \"pipeline_info\"]\n",
    "                elif model_name == \"async_processor\":\n",
    "                    inputs = self._prepare_async_inputs(test_data)\n",
    "                    outputs = [\"results\", \"status\"]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # 創建輸出對象\n",
    "                output_objects = [httpclient.InferRequestedOutput(name) for name in outputs]\n",
    "                \n",
    "                # 發送請求\n",
    "                response = self.client.infer(\n",
    "                    model_name, inputs=inputs, outputs=output_objects\n",
    "                )\n",
    "                \n",
    "                request_time = (time.time() - request_start) * 1000\n",
    "                results[\"response_times\"].append(request_time)\n",
    "                results[\"successful_requests\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ 請求 {i+1} 失敗: {str(e)}\")\n",
    "                results[\"failed_requests\"] += 1\n",
    "        \n",
    "        results[\"total_time\"] = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # 計算統計信息\n",
    "        if results[\"response_times\"]:\n",
    "            avg_latency = np.mean(results[\"response_times\"])\n",
    "            p95_latency = np.percentile(results[\"response_times\"], 95)\n",
    "            p99_latency = np.percentile(results[\"response_times\"], 99)\n",
    "            throughput = results[\"successful_requests\"] / (results[\"total_time\"] / 1000)\n",
    "            \n",
    "            print(\"\\n📊 基準測試結果:\")\n",
    "            print(f\"   成功請求: {results['successful_requests']}/{num_requests}\")\n",
    "            print(f\"   成功率: {results['successful_requests']/num_requests*100:.1f}%\")\n",
    "            print(f\"   平均延遲: {avg_latency:.2f}ms\")\n",
    "            print(f\"   P95 延遲: {p95_latency:.2f}ms\")\n",
    "            print(f\"   P99 延遲: {p99_latency:.2f}ms\")\n",
    "            print(f\"   吞吐量: {throughput:.2f} QPS\")\n",
    "            print(f\"   總測試時間: {results['total_time']:.2f}ms\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _prepare_basic_inputs(self, test_data):\n",
    "        \"\"\"準備基礎模型輸入\"\"\"\n",
    "        text_input = httpclient.InferInput(\"text_input\", [1], \"BYTES\")\n",
    "        text_input.set_data_from_numpy(\n",
    "            np.array([test_data.get(\"text\", \"Hello World\")], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        params_input = httpclient.InferInput(\"parameters\", [1], \"BYTES\")\n",
    "        params_input.set_data_from_numpy(\n",
    "            np.array([json.dumps(test_data.get(\"params\", {\"type\": \"uppercase\"}))], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        return [text_input, params_input]\n",
    "    \n",
    "    def _prepare_advanced_inputs(self, test_data):\n",
    "        \"\"\"準備高級管道輸入\"\"\"\n",
    "        query_input = httpclient.InferInput(\"query\", [1], \"BYTES\")\n",
    "        query_input.set_data_from_numpy(\n",
    "            np.array([test_data.get(\"query\", \"什麼是機器學習？\")], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        config_input = httpclient.InferInput(\"config\", [1], \"BYTES\")\n",
    "        config_input.set_data_from_numpy(\n",
    "            np.array([json.dumps(test_data.get(\"config\", {\"top_k\": 3}))], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        return [query_input, config_input]\n",
    "    \n",
    "    def _prepare_async_inputs(self, test_data):\n",
    "        \"\"\"準備異步處理輸入\"\"\"\n",
    "        request_input = httpclient.InferInput(\"requests\", [1], \"BYTES\")\n",
    "        request_input.set_data_from_numpy(\n",
    "            np.array([json.dumps(test_data.get(\"request\", {\"type\": \"computation\", \"operation\": \"sum\", \"numbers\": [1,2,3,4,5]}))], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        config_input = httpclient.InferInput(\"async_config\", [1], \"BYTES\")\n",
    "        config_input.set_data_from_numpy(\n",
    "            np.array([json.dumps(test_data.get(\"async_config\", {\"delay\": 0.05}))], dtype=np.object_)\n",
    "        )\n",
    "        \n",
    "        return [request_input, config_input]\n",
    "\n",
    "\n",
    "print(\"📊 Python Backend 監控工具已準備\")\n",
    "print(\"💡 使用方法:\")\n",
    "print(\"   monitor = PythonBackendMonitor()\")\n",
    "print(\"   monitor.get_model_status('custom_pipeline')\")\n",
    "print(\"   monitor.benchmark_model('custom_pipeline', {'text': 'Test'}, num_requests=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 🎯 實驗 4：部署和驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建部署腳本\n",
    "deployment_script = '''\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"🚀 部署 Python Backend 模型...\"\n",
    "\n",
    "# 設置環境變量\n",
    "export MODEL_REPOSITORY=\"/opt/tritonserver/models\"\n",
    "export TRITON_LOG_LEVEL=\"INFO\"\n",
    "\n",
    "# 檢查模型目錄\n",
    "echo \"📋 檢查模型目錄結構...\"\n",
    "ls -la $MODEL_REPOSITORY/\n",
    "\n",
    "# 檢查 Python Backend 模型\n",
    "for model in \"custom_pipeline\" \"advanced_pipeline\" \"async_processor\"; do\n",
    "    if [ -d \"$MODEL_REPOSITORY/$model\" ]; then\n",
    "        echo \"✅ 發現模型: $model\"\n",
    "        ls -la \"$MODEL_REPOSITORY/$model/\"\n",
    "        \n",
    "        # 檢查模型文件\n",
    "        if [ -f \"$MODEL_REPOSITORY/$model/config.pbtxt\" ]; then\n",
    "            echo \"  📄 配置文件存在\"\n",
    "        else\n",
    "            echo \"  ❌ 配置文件缺失\"\n",
    "        fi\n",
    "        \n",
    "        if [ -f \"$MODEL_REPOSITORY/$model/1/model.py\" ]; then\n",
    "            echo \"  🐍 Python 模型存在\"\n",
    "        else\n",
    "            echo \"  ❌ Python 模型缺失\"\n",
    "        fi\n",
    "        echo \"\"\n",
    "    else\n",
    "        echo \"❌ 模型目錄不存在: $model\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "# 啟動 Triton 服務器\n",
    "echo \"🖥️  啟動 Triton 服務器...\"\n",
    "tritonserver \\\n",
    "    --model-repository=$MODEL_REPOSITORY \\\n",
    "    --backend-config=python,shm-default-byte-size=134217728 \\\n",
    "    --log-verbose=1 \\\n",
    "    --allow-http=true \\\n",
    "    --allow-grpc=true \\\n",
    "    --allow-metrics=true\n",
    "'''\n",
    "\n",
    "with open(\"/tmp/deploy_python_backends.sh\", \"w\") as f:\n",
    "    f.write(deployment_script)\n",
    "\n",
    "# 設置執行權限\n",
    "os.chmod(\"/tmp/deploy_python_backends.sh\", 0o755)\n",
    "\n",
    "print(\"✅ 部署腳本已創建: /tmp/deploy_python_backends.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建驗證腳本\n",
    "validation_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def wait_for_server(url=\"http://localhost:8000/v2/health/ready\", timeout=60):\n",
    "    \"\"\"等待 Triton 服務器就緒\"\"\"\n",
    "    print(f\"⏳ 等待 Triton 服務器就緒...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                print(\"✅ Triton 服務器已就緒\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(\"❌ 等待服務器超時\")\n",
    "    return False\n",
    "\n",
    "def check_models():\n",
    "    \"\"\"檢查模型狀態\"\"\"\n",
    "    models = [\"custom_pipeline\", \"advanced_pipeline\", \"async_processor\"]\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            response = requests.get(f\"http://localhost:8000/v2/models/{model}/ready\")\n",
    "            if response.status_code == 200:\n",
    "                print(f\"✅ 模型就緒: {model}\")\n",
    "            else:\n",
    "                print(f\"❌ 模型未就緒: {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 檢查模型失敗 {model}: {str(e)}\")\n",
    "\n",
    "def test_basic_inference():\n",
    "    \"\"\"測試基礎推理\"\"\"\n",
    "    print(\"🧪 測試基礎推理...\")\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"text_input\",\n",
    "                \"shape\": [1],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": [\"Hello Python Backend\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"parameters\",\n",
    "                \"shape\": [1],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": ['{\"type\": \"uppercase\"}']\n",
    "            }\n",
    "        ],\n",
    "        \"outputs\": [\n",
    "            {\"name\": \"processed_text\"},\n",
    "            {\"name\": \"metadata\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/v2/models/custom_pipeline/infer\",\n",
    "            json=payload\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            processed_text = result[\"outputs\"][0][\"data\"][0]\n",
    "            print(f\"✅ 基礎推理成功: '{processed_text}'\")\n",
    "        else:\n",
    "            print(f\"❌ 基礎推理失敗: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 基礎推理異常: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔍 Python Backend 驗證開始\")\n",
    "    \n",
    "    # 等待服務器\n",
    "    if wait_for_server():\n",
    "        # 檢查模型\n",
    "        check_models()\n",
    "        \n",
    "        # 測試推理\n",
    "        test_basic_inference()\n",
    "        \n",
    "        print(\"🏁 驗證完成\")\n",
    "    else:\n",
    "        print(\"❌ 服務器未就緒，驗證終止\")\n",
    "'''\n",
    "\n",
    "with open(\"/tmp/validate_python_backends.py\", \"w\") as f:\n",
    "    f.write(validation_script)\n",
    "\n",
    "os.chmod(\"/tmp/validate_python_backends.py\", 0o755)\n",
    "\n",
    "print(\"✅ 驗證腳本已創建: /tmp/validate_python_backends.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 📚 最佳實踐和故障排除"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 🎯 Python Backend 開發最佳實踐\n",
    "\n",
    "#### 1. 代碼結構\n",
    "- **模組化設計**：將邏輯分解為可重用的組件\n",
    "- **錯誤處理**：完善的異常捕獲和處理機制\n",
    "- **日誌記錄**：詳細的執行日誌便於調試\n",
    "- **配置管理**：靈活的參數配置系統\n",
    "\n",
    "#### 2. 性能優化\n",
    "- **批處理**：充分利用動態批次功能\n",
    "- **異步處理**：使用協程處理 I/O 密集型任務\n",
    "- **資源池**：重用連接和計算資源\n",
    "- **內存管理**：及時釋放不需要的對象\n",
    "\n",
    "#### 3. 安全考慮\n",
    "- **輸入驗證**：嚴格驗證所有輸入數據\n",
    "- **資源限制**：設置合理的超時和資源限制\n",
    "- **錯誤信息**：避免暴露敏感信息\n",
    "- **依賴管理**：保持依賴庫的安全更新\n",
    "\n",
    "#### 4. 監控和調試\n",
    "- **性能指標**：收集關鍵性能數據\n",
    "- **健康檢查**：實現模型健康狀態檢查\n",
    "- **調試模式**：支持開發時的詳細調試\n",
    "- **版本管理**：清晰的模型版本控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 故障排除指南\n",
    "troubleshooting_guide = \"\"\"\n",
    "🔧 Python Backend 故障排除指南\n",
    "\n",
    "常見問題和解決方案：\n",
    "\n",
    "1. 模型加載失敗\n",
    "   問題：模型無法加載或初始化失敗\n",
    "   解決方案：\n",
    "   - 檢查 model.py 語法錯誤\n",
    "   - 確認所有依賴庫已安裝\n",
    "   - 檢查 config.pbtxt 配置正確性\n",
    "   - 查看 Triton 服務器日誌\n",
    "\n",
    "2. 推理請求超時\n",
    "   問題：推理請求響應時間過長\n",
    "   解決方案：\n",
    "   - 優化模型執行邏輯\n",
    "   - 增加實例數量\n",
    "   - 調整超時配置\n",
    "   - 使用異步處理\n",
    "\n",
    "3. 內存使用過高\n",
    "   問題：Python Backend 消耗大量內存\n",
    "   解決方案：\n",
    "   - 及時釋放大對象\n",
    "   - 避免內存洩漏\n",
    "   - 優化數據結構\n",
    "   - 限制批次大小\n",
    "\n",
    "4. 並發處理問題\n",
    "   問題：高並發時出現競態條件\n",
    "   解決方案：\n",
    "   - 使用線程安全的數據結構\n",
    "   - 避免全局變量修改\n",
    "   - 合理使用鎖機制\n",
    "   - 設計無狀態處理邏輯\n",
    "\n",
    "5. 依賴庫衝突\n",
    "   問題：不同模型間的依賴庫版本衝突\n",
    "   解決方案：\n",
    "   - 使用 Python 環境隔離\n",
    "   - 創建專用執行環境\n",
    "   - 統一依賴版本管理\n",
    "   - 使用容器化部署\n",
    "\n",
    "調試技巧：\n",
    "- 使用 print() 或 logging 輸出調試信息\n",
    "- 設置斷點進行調試（開發環境）\n",
    "- 檢查 Triton 服務器日誌\n",
    "- 使用性能分析工具\n",
    "- 監控系統資源使用情況\n",
    "\n",
    "日誌查看命令：\n",
    "- docker logs <triton_container_id>\n",
    "- tail -f /var/log/triton/triton.log\n",
    "- journalctl -u triton-server -f\n",
    "\"\"\"\n",
    "\n",
    "print(troubleshooting_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 📖 總結\n",
    "\n",
    "本實驗完成了自定義 Python Backend 的完整開發流程：\n",
    "\n",
    "### 🎯 實驗成果\n",
    "1. **基礎 Python Backend** - 實現了文本處理管道\n",
    "2. **高級管道 Backend** - 構建了多步驟推理流程\n",
    "3. **異步處理 Backend** - 開發了並發處理能力\n",
    "4. **監控和調試工具** - 提供了完整的運維支持\n",
    "\n",
    "### 🔧 關鍵技術點\n",
    "- Triton Python Backend API 使用\n",
    "- 異步編程和並發處理\n",
    "- 性能監控和優化\n",
    "- 錯誤處理和故障排除\n",
    "\n",
    "### 🚀 後續步驟\n",
    "1. 部署到生產環境\n",
    "2. 集成更複雜的業務邏輯\n",
    "3. 實現 A/B 測試功能\n",
    "4. 添加模型熱更新能力\n",
    "\n",
    "### 💡 學習要點\n",
    "- Python Backend 提供了最大的靈活性\n",
    "- 異步處理能顯著提升性能\n",
    "- 監控和調試是成功部署的關鍵\n",
    "- 良好的錯誤處理能提升系統穩定性\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 恭喜完成 Lab 2.3.4！**\n",
    "\n",
    "您已經掌握了 Triton Python Backend 的高級開發技術，可以構建複雜的推理管道和處理邏輯。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}