{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab-1.7: DPO (Direct Preference Optimization) ç’°å¢ƒèˆ‡æ•¸æ“šæº–å‚™\n",
    "\n",
    "**å¯¦é©—ç›®æ¨™**: æº–å‚™ DPO è¨“ç·´ç’°å¢ƒèˆ‡åå¥½æ•¸æ“šé›†\n",
    "\n",
    "**Direct Preference Optimization (DPO)** æ˜¯ä¸€ç¨®æ–°ç©Žçš„ RLHF æ›¿ä»£æ–¹æ¡ˆï¼Œç”± Stanford æå‡ºï¼Œèƒ½å¤ ç›´æŽ¥å¾žåå¥½æ•¸æ“šä¸­å„ªåŒ–èªžè¨€æ¨¡åž‹ï¼Œç„¡éœ€è¨“ç·´çŽå‹µæ¨¡åž‹æˆ–åŸ·è¡Œå¼·åŒ–å­¸ç¿’ã€‚\n",
    "\n",
    "## æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "DPO çš„ä¸»è¦å„ªå‹¢ï¼š\n",
    "- **ç„¡éœ€çŽå‹µæ¨¡åž‹**: ç›´æŽ¥å¾žåå¥½æ•¸æ“šè¨“ç·´ï¼Œç°¡åŒ– RLHF æµç¨‹\n",
    "- **ç©©å®šè¨“ç·´**: é¿å…å¼·åŒ–å­¸ç¿’ä¸­çš„ä¸ç©©å®šæ€§å•é¡Œ\n",
    "- **é«˜æ•ˆå¯¦ç¾**: çµåˆ PEFT æŠ€è¡“ï¼Œåœ¨å–® GPU ä¸Šå¯¦ç¾å¤§æ¨¡åž‹å°é½Š\n",
    "- **ç†è«–ä¿è­‰**: åŸºæ–¼ Bradley-Terry æ¨¡åž‹çš„ç†è«–åŸºç¤Ž\n",
    "\n",
    "## å¯¦é©—æµç¨‹\n",
    "\n",
    "1. **ç’°å¢ƒæº–å‚™**: å®‰è£ TRLã€PEFT ç­‰å¿…è¦åº«\n",
    "2. **æ•¸æ“šæº–å‚™**: è¼‰å…¥åå¥½æ•¸æ“šé›†ä¸¦é€²è¡Œé è™•ç†\n",
    "3. **æ¨¡åž‹è¼‰å…¥**: æº–å‚™åŸºç¤Žæ¨¡åž‹å’Œ tokenizer\n",
    "4. **æ•¸æ“šæ ¼å¼åŒ–**: å°‡åå¥½å°è½‰æ›ç‚º DPO è¨“ç·´æ ¼å¼\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1: ç’°å¢ƒæª¢æŸ¥èˆ‡å®‰è£\n",
    "\n",
    "é¦–å…ˆæª¢æŸ¥ GPU ç’°å¢ƒä¸¦å®‰è£å¿…è¦çš„å¥—ä»¶ã€‚DPO è¨“ç·´éœ€è¦ï¼š\n",
    "- `trl`: Transformer Reinforcement Learning åº«\n",
    "- `peft`: åƒæ•¸é«˜æ•ˆå¾®èª¿\n",
    "- `bitsandbytes`: é‡åŒ–æ”¯æŒ\n",
    "- `accelerate`: åˆ†æ•£å¼è¨“ç·´æ”¯æŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ GPU ç’°å¢ƒ\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(f'Python ç‰ˆæœ¬: {sys.version}')\n",
    "print(f'PyTorch ç‰ˆæœ¬: {torch.__version__}')\n",
    "print(f'CUDA å¯ç”¨: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU è¨­å‚™: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "    print(f'ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "# è¨­ç½®éš¨æ©Ÿç¨®å­ç¢ºä¿å¯¦é©—å¯é‡ç¾\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶ (å¦‚æžœå°šæœªå®‰è£)\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def install_and_import(package, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = package\n",
    "    \n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f'âœ… {package} å·²å®‰è£')\n",
    "    except ImportError:\n",
    "        print(f'âš ï¸  å®‰è£ {package}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f'âœ… {package} å®‰è£å®Œæˆ')\n",
    "\n",
    "# æª¢æŸ¥å¿…è¦å¥—ä»¶\n",
    "install_and_import('trl')\n",
    "install_and_import('peft')\n",
    "install_and_import('bitsandbytes')\n",
    "install_and_import('accelerate')\n",
    "install_and_import('transformers')\n",
    "install_and_import('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°Žå…¥å¿…è¦åº«ä¸¦æª¢æŸ¥ç‰ˆæœ¬\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import datasets\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "\n",
    "print('=== å¥—ä»¶ç‰ˆæœ¬è³‡è¨Š ===')\n",
    "print(f'ðŸ¤— Transformers: {transformers.__version__}')\n",
    "print(f'ðŸŽ¯ PEFT: {peft.__version__}')\n",
    "print(f'ðŸš€ TRL: {trl.__version__}')\n",
    "print(f'ðŸ“Š Datasets: {datasets.__version__}')\n",
    "print(f'âš¡ Accelerate: {accelerate.__version__}')\n",
    "\n",
    "print('âœ… DPO ç’°å¢ƒæº–å‚™å®Œæˆï¼')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2: åå¥½æ•¸æ“šé›†è¼‰å…¥èˆ‡æŽ¢ç´¢\n",
    "\n",
    "DPO éœ€è¦åå¥½æ•¸æ“šï¼Œæ ¼å¼ç‚º (prompt, chosen, rejected) ä¸‰å…ƒçµ„ï¼š\n",
    "- **prompt**: è¼¸å…¥æç¤º\n",
    "- **chosen**: åå¥½çš„å›žæ‡‰ (é«˜å“è³ª)\n",
    "- **rejected**: éžåå¥½çš„å›žæ‡‰ (ä½Žå“è³ª)\n",
    "\n",
    "æˆ‘å€‘ä½¿ç”¨ Anthropic çš„ HH-RLHF æ•¸æ“šé›†ï¼Œé€™æ˜¯äººé¡žåå¥½æ•¸æ“šçš„ç¶“å…¸æ•¸æ“šé›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# è¼‰å…¥åå¥½æ•¸æ“šé›†\n",
    "print('ðŸ“¥ è¼‰å…¥ Anthropic HH-RLHF æ•¸æ“šé›†...')\n",
    "\n",
    "try:\n",
    "    # å…ˆè¼‰å…¥å°æ¨£æœ¬é€²è¡Œæ¸¬è©¦\n",
    "    dataset = load_dataset('Anthropic/hh-rlhf', split='train[:100]')\n",
    "    print(f'âœ… æˆåŠŸè¼‰å…¥æ•¸æ“šé›†ï¼Œæ¨£æœ¬æ•¸: {len(dataset)}')\n",
    "    \n",
    "    # æª¢æŸ¥æ•¸æ“šæ ¼å¼\n",
    "    print('\\n=== æ•¸æ“šé›†çµæ§‹ ===')\n",
    "    print(f'æ¬„ä½: {list(dataset.features.keys())}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'âŒ æ•¸æ“šé›†è¼‰å…¥å¤±æ•—: {e}')\n",
    "    print('ä½¿ç”¨æœ¬åœ°æ¨¡æ“¬æ•¸æ“š...')\n",
    "    \n",
    "    # å‰µå»ºæ¨¡æ“¬åå¥½æ•¸æ“š\n",
    "    mock_data = {\n",
    "        'chosen': [\n",
    "            'Human: è«‹è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?\\n\\nAssistant: æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒè®“è¨ˆç®—æ©Ÿèƒ½å¤ å¾žæ•¸æ“šä¸­å­¸ç¿’ä¸¦åšå‡ºæ±ºç­–ï¼Œè€Œç„¡éœ€æ˜Žç¢ºç·¨ç¨‹ã€‚é€šéŽç®—æ³•åˆ†æžå¤§é‡æ•¸æ“šï¼Œç³»çµ±å¯ä»¥è­˜åˆ¥æ¨¡å¼ä¸¦æé«˜æ€§èƒ½ã€‚',\n",
    "            'Human: å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆ?\\n\\nAssistant: å­¸ç¿’ç¨‹å¼è¨­è¨ˆå»ºè­°å¾žåŸºç¤Žèªžæ³•é–‹å§‹ï¼Œé¸æ“‡ä¸€é–€é©åˆçš„èªžè¨€å¦‚Pythonï¼Œå¤šåšç·´ç¿’é …ç›®ï¼Œåƒèˆ‡é–‹æºå°ˆæ¡ˆï¼Œä¸¦æŒçºŒå­¸ç¿’æ–°æŠ€è¡“ã€‚å¯¦ä½œæ˜¯æœ€é‡è¦çš„å­¸ç¿’æ–¹å¼ã€‚'\n",
    "        ],\n",
    "        'rejected': [\n",
    "            'Human: è«‹è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?\\n\\nAssistant: æ©Ÿå™¨å­¸ç¿’å°±æ˜¯è®“æ©Ÿå™¨è®Šè°æ˜Žã€‚',\n",
    "            'Human: å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆ?\\n\\nAssistant: å­¸ç¨‹å¼å°±æ˜¯å¯«codeã€‚'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_dict(mock_data)\n",
    "    print(f'âœ… å‰µå»ºæ¨¡æ“¬æ•¸æ“šé›†ï¼Œæ¨£æœ¬æ•¸: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŽ¢ç´¢æ•¸æ“šé›†å…§å®¹\n",
    "print('=== æ•¸æ“šæ¨£æœ¬ç¤ºä¾‹ ===')\n",
    "\n",
    "for i in range(min(2, len(dataset))):\n",
    "    sample = dataset[i]\n",
    "    print(f'\\nðŸ“ æ¨£æœ¬ {i+1}:')\n",
    "    \n",
    "    if 'chosen' in sample:\n",
    "        print('âœ… åå¥½å›žæ‡‰ (chosen):')\n",
    "        print(sample['chosen'][:200] + '...' if len(sample['chosen']) > 200 else sample['chosen'])\n",
    "        print()\n",
    "        \n",
    "    if 'rejected' in sample:\n",
    "        print('âŒ éžåå¥½å›žæ‡‰ (rejected):')\n",
    "        print(sample['rejected'][:200] + '...' if len(sample['rejected']) > 200 else sample['rejected'])\n",
    "        print()\n",
    "    \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3: æ¨¡åž‹èˆ‡ Tokenizer æº–å‚™\n",
    "\n",
    "DPO è¨“ç·´éœ€è¦ä¸€å€‹é è¨“ç·´çš„åŸºç¤Žæ¨¡åž‹ã€‚æˆ‘å€‘ä½¿ç”¨è¼ƒå°çš„æ¨¡åž‹é€²è¡Œå¯¦é©—ï¼Œä¸¦é…ç½®é‡åŒ–ä»¥ç¯€çœè¨˜æ†¶é«”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# é¸æ“‡æ¨¡åž‹ (ä½¿ç”¨è¼ƒå°çš„æ¨¡åž‹é€²è¡Œå¯¦é©—)\n",
    "MODEL_NAME = 'microsoft/DialoGPT-medium'  # æˆ–è€…ä½¿ç”¨ 'gpt2' é€²è¡Œå¿«é€Ÿæ¸¬è©¦\n",
    "\n",
    "print(f'ðŸ“¦ è¼‰å…¥æ¨¡åž‹: {MODEL_NAME}')\n",
    "\n",
    "# é…ç½®é‡åŒ– (å¯é¸ï¼Œç”¨æ–¼å¤§æ¨¡åž‹)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    # è¼‰å…¥ tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # è¨­ç½® pad token (DPO éœ€è¦)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f'âœ… Tokenizer è¼‰å…¥æˆåŠŸ')\n",
    "    print(f'è©žå½™å¤§å°: {len(tokenizer)}')\n",
    "    print(f'ç‰¹æ®Š tokens: EOS={tokenizer.eos_token}, PAD={tokenizer.pad_token}')\n",
    "    \n",
    "    # è¼‰å…¥æ¨¡åž‹ (ä¸é‡åŒ–é€²è¡Œå¿«é€Ÿæ¸¬è©¦)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    "        # quantization_config=quantization_config  # å¦‚éœ€é‡åŒ–è«‹å–æ¶ˆè¨»è§£\n",
    "    )\n",
    "    \n",
    "    print(f'âœ… æ¨¡åž‹è¼‰å…¥æˆåŠŸ')\n",
    "    print(f'æ¨¡åž‹åƒæ•¸é‡: {model.num_parameters():,}')\n",
    "    print(f'æ¨¡åž‹è¨­å‚™: {next(model.parameters()).device}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'âŒ æ¨¡åž‹è¼‰å…¥å¤±æ•—: {e}')\n",
    "    print('è«‹æª¢æŸ¥ç¶²è·¯é€£æŽ¥æˆ–å˜—è©¦å…¶ä»–æ¨¡åž‹')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4: æ•¸æ“šé è™•ç†èˆ‡æ ¼å¼åŒ–\n",
    "\n",
    "å°‡åå¥½æ•¸æ“šè½‰æ›ç‚º DPO è¨“ç·´æ‰€éœ€çš„æ ¼å¼ã€‚DPO éœ€è¦ promptã€chosenã€rejected ä¸‰å€‹æ¬„ä½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_and_response(text):\n",
    "    \"\"\"å¾žå°è©±æ–‡æœ¬ä¸­æå–æç¤ºå’Œå›žæ‡‰\"\"\"\n",
    "    if 'Human:' in text and 'Assistant:' in text:\n",
    "        parts = text.split('Assistant:')\n",
    "        if len(parts) >= 2:\n",
    "            prompt = parts[0].replace('Human:', '').strip()\n",
    "            response = parts[1].strip()\n",
    "            return prompt, response\n",
    "    return None, None\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"é è™•ç†æ•¸æ“šé›†ç‚º DPO æ ¼å¼\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # æå– chosen å’Œ rejected çš„ prompt å’Œ response\n",
    "        chosen_prompt, chosen_response = extract_prompt_and_response(sample['chosen'])\n",
    "        rejected_prompt, rejected_response = extract_prompt_and_response(sample['rejected'])\n",
    "        \n",
    "        if chosen_prompt and chosen_response and rejected_response:\n",
    "            processed_data.append({\n",
    "                'prompt': chosen_prompt,\n",
    "                'chosen': chosen_response,\n",
    "                'rejected': rejected_response\n",
    "            })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# é è™•ç†æ•¸æ“š\n",
    "print('ðŸ”„ é è™•ç†åå¥½æ•¸æ“š...')\n",
    "processed_data = preprocess_dataset(dataset)\n",
    "\n",
    "print(f'âœ… è™•ç†å®Œæˆï¼Œæœ‰æ•ˆæ¨£æœ¬æ•¸: {len(processed_data)}')\n",
    "\n",
    "# é¡¯ç¤ºè™•ç†å¾Œçš„æ¨£æœ¬\n",
    "if processed_data:\n",
    "    print('\\n=== è™•ç†å¾Œæ¨£æœ¬ç¤ºä¾‹ ===')\n",
    "    sample = processed_data[0]\n",
    "    print(f'ðŸ“ Prompt: {sample[\"prompt\"]}')\n",
    "    print(f'âœ… Chosen: {sample[\"chosen\"]}')\n",
    "    print(f'âŒ Rejected: {sample[\"rejected\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºæœ€çµ‚çš„ DPO æ•¸æ“šé›†\n",
    "from datasets import Dataset\n",
    "\n",
    "if processed_data:\n",
    "    dpo_dataset = Dataset.from_list(processed_data)\n",
    "    print(f'ðŸ“Š DPO æ•¸æ“šé›†å‰µå»ºå®Œæˆ')\n",
    "    print(f'æ¨£æœ¬æ•¸: {len(dpo_dataset)}')\n",
    "    print(f'æ¬„ä½: {list(dpo_dataset.features.keys())}')\n",
    "    \n",
    "    # ä¿å­˜é è™•ç†æ•¸æ“š (å¯é¸)\n",
    "    output_dir = './dpo_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    dpo_dataset.save_to_disk(output_dir)\n",
    "    print(f'ðŸ’¾ æ•¸æ“šé›†å·²ä¿å­˜è‡³: {output_dir}')\n",
    "    \n",
    "else:\n",
    "    print('âŒ ç„¡æœ‰æ•ˆçš„è™•ç†æ•¸æ“šï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5: å¯¦é©—è¨­ç½®æª¢æŸ¥\n",
    "\n",
    "æœ€å¾Œç¢ºèªæ‰€æœ‰çµ„ä»¶æº–å‚™å°±ç·’ï¼Œç‚ºä¸‹ä¸€å€‹ notebook çš„ SFT åŸºç·šè¨“ç·´åšæº–å‚™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦é©—è¨­ç½®ç¸½çµ\n",
    "print('=== DPO å¯¦é©—ç’°å¢ƒè¨­ç½®ç¸½çµ ===')\n",
    "print(f'âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}')\n",
    "print(f'âœ… GPU å¯ç”¨: {torch.cuda.is_available()}')\n",
    "print(f'âœ… æ¨¡åž‹: {MODEL_NAME}')\n",
    "print(f'âœ… æ•¸æ“šé›†æ¨£æœ¬æ•¸: {len(processed_data) if processed_data else 0}')\n",
    "print(f'âœ… Tokenizer è©žå½™å¤§å°: {len(tokenizer)}')\n",
    "\n",
    "# æ¸¬è©¦ tokenization\n",
    "test_text = 'Hello, how are you?'\n",
    "tokens = tokenizer(test_text, return_tensors='pt')\n",
    "print(f'âœ… Tokenization æ¸¬è©¦æˆåŠŸ: {test_text} -> {tokens[\"input_ids\"].shape[1]} tokens')\n",
    "\n",
    "print('ðŸš€ æº–å‚™é€²å…¥ä¸‹ä¸€æ­¥: SFT åŸºç·šè¨“ç·´')\n",
    "print('è«‹åŸ·è¡Œ 02-SFT_Baseline.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}