# Lab 0.6: æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦è¸

## å¯¦é©—ç›®æ¨™

é€šéå®Œæ•´çš„å·¥ç¨‹åŒ–å£“ç¸®æµç¨‹å¯¦è¸ï¼ŒæŒæ¡å¾æ¨¡å‹è¼‰å…¥åˆ°æ¨ç†éƒ¨ç½²çš„ç«¯åˆ°ç«¯å£“ç¸®æŠ€èƒ½ï¼Œå»ºç«‹å·¥æ¥­ç´šçš„æ¨¡å‹å„ªåŒ–èƒ½åŠ›ã€‚

## å­¸ç¿’æˆæœ

å®Œæˆæœ¬å¯¦é©—å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š
- æŒæ¡æ¨¡å‹-ç¡¬é«”åŒ¹é…çš„ç²¾ç¢ºè¨ˆç®—æ–¹æ³•
- å¯¦æ–½å®Œæ•´çš„å·¥ç¨‹åŒ–å£“ç¸®æµç¨‹
- å»ºç«‹å£“ç¸®æ•ˆæœçš„ç§‘å­¸è©•ä¼°é«”ç³»
- å…·å‚™ç”Ÿç”¢ç´šæ¨¡å‹éƒ¨ç½²å„ªåŒ–èƒ½åŠ›

## å¯¦é©—ç’°å¢ƒè¦æ±‚

### ç¡¬é«”è¦æ±‚
- GPUï¼š16GB+é¡¯å­˜ï¼ˆæ¨è–¦RTX 4090æˆ–A100ï¼‰
- RAMï¼š32GB+ç³»çµ±è¨˜æ†¶é«”
- å­˜å„²ï¼š50GBå¯ç”¨ç©ºé–“

### è»Ÿé«”è¦æ±‚
- PyTorch 2.0+
- Transformers 4.30+
- BitsAndBytesã€ONNX Runtime
- å·²æ¿€æ´»çš„poetryè™›æ“¬ç’°å¢ƒ

## å¯¦é©—è³‡æºèªªæ˜

æœ¬LabåŒ…å«å®Œæ•´çš„å·¥ç¨‹åŒ–å£“ç¸®è³‡æºï¼š
- **`compression_lab.py`**: ä¸»å¯¦é©—è…³æœ¬ï¼Œæ¼”ç¤ºç«¯åˆ°ç«¯å£“ç¸®æµç¨‹
- **`MODEL_COMPRESSION_ENGINEERING.md`**: â­ **å·¥ç¨‹åŒ–å¯¦ç”¨æŒ‡å—** - åŒ…å«ç¡¬é«”åŒ¹é…å…¬å¼ã€æ±ºç­–è¡¨ã€å¿«é€Ÿå·¥å…·

## åŸ·è¡Œæ–¹å¼

```bash
# 1. å…ˆé–±è®€å·¥ç¨‹åŒ–æŒ‡å—ï¼ˆå¼·çƒˆæ¨è–¦ï¼‰
cat MODEL_COMPRESSION_ENGINEERING.md

# 2. é‹è¡Œå®Œæ•´å¯¦é©—
python compression_lab.py
```

> ğŸ’¡ **å»ºè­°å­¸ç¿’é †åº**: å…ˆå­¸ç¿’ç†è«–å°ˆè«–0.4å’Œ0.5ï¼Œå†é–±è®€å·¥ç¨‹åŒ–æŒ‡å—ï¼Œæœ€å¾ŒåŸ·è¡Œå¯¦é©—

## å¯¦é©—å…§å®¹

### æ ¸å¿ƒå¯¦é©—è…³æœ¬

```python
# compression_engineering_lab.py
"""
æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦è¸ä¸»è…³æœ¬
æ¼”ç¤ºå¾è¼‰å…¥åˆ°éƒ¨ç½²çš„å®Œæ•´å£“ç¸®æµç¨‹
"""

import torch
import numpy as np
import time
import json
import os
from datetime import datetime
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    BitsAndBytesConfig, TrainingArguments, Trainer
)
import psutil
import matplotlib.pyplot as plt
import pandas as pd

class ModelCompressionEngineeringLab:
    """æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦é©—å®¤"""

    def __init__(self, base_model_name="microsoft/DialoGPT-small"):
        self.base_model_name = base_model_name
        self.results = {}
        self.compression_timeline = []

    def step_1_model_analysis_and_baseline(self):
        """æ­¥é©Ÿ1: æ¨¡å‹åˆ†æèˆ‡åŸºç·šå»ºç«‹"""

        print("=== æ­¥é©Ÿ1: æ¨¡å‹åˆ†æèˆ‡åŸºç·šå»ºç«‹ ===")

        # 1.1 è¼‰å…¥åŸå§‹æ¨¡å‹
        print("1.1 è¼‰å…¥åŸå§‹æ¨¡å‹...")
        original_model, tokenizer = self._load_original_model()

        # 1.2 åˆ†ææ¨¡å‹çµæ§‹
        print("1.2 åˆ†ææ¨¡å‹çµæ§‹...")
        model_structure = self._analyze_model_structure(original_model)

        # 1.3 å»ºç«‹æ€§èƒ½åŸºç·š
        print("1.3 å»ºç«‹æ€§èƒ½åŸºç·š...")
        baseline_performance = self._establish_baseline_performance(original_model, tokenizer)

        # 1.4 ç¡¬é«”éœ€æ±‚åˆ†æ
        print("1.4 ç¡¬é«”éœ€æ±‚åˆ†æ...")
        hardware_analysis = self._analyze_hardware_requirements(model_structure)

        step1_results = {
            'model_structure': model_structure,
            'baseline_performance': baseline_performance,
            'hardware_analysis': hardware_analysis,
            'timestamp': datetime.now().isoformat()
        }

        self.results['step1_analysis'] = step1_results
        self._save_checkpoint('step1_analysis.json', step1_results)

        return step1_results

    def step_2_compression_strategy_design(self, target_constraints: dict):
        """æ­¥é©Ÿ2: å£“ç¸®ç­–ç•¥è¨­è¨ˆ"""

        print("=== æ­¥é©Ÿ2: å£“ç¸®ç­–ç•¥è¨­è¨ˆ ===")

        analysis_result = self.results.get('step1_analysis')
        if not analysis_result:
            raise ValueError("è«‹å…ˆå®Œæˆæ­¥é©Ÿ1çš„æ¨¡å‹åˆ†æ")

        # 2.1 è§£æç´„æŸæ¢ä»¶
        print("2.1 è§£æç›®æ¨™ç´„æŸ...")
        parsed_constraints = self._parse_constraints(target_constraints)

        # 2.2 ç”Ÿæˆå£“ç¸®å€™é¸ç­–ç•¥
        print("2.2 ç”Ÿæˆå€™é¸å£“ç¸®ç­–ç•¥...")
        candidate_strategies = self._generate_candidate_strategies(analysis_result, parsed_constraints)

        # 2.3 ç­–ç•¥æ•ˆæœé æ¸¬
        print("2.3 é æ¸¬å„ç­–ç•¥æ•ˆæœ...")
        strategy_predictions = self._predict_strategy_effects(candidate_strategies, analysis_result)

        # 2.4 é¸æ“‡æœ€å„ªç­–ç•¥
        print("2.4 é¸æ“‡æœ€å„ªç­–ç•¥...")
        optimal_strategy = self._select_optimal_strategy(strategy_predictions, parsed_constraints)

        step2_results = {
            'target_constraints': parsed_constraints,
            'candidate_strategies': candidate_strategies,
            'strategy_predictions': strategy_predictions,
            'optimal_strategy': optimal_strategy,
            'timestamp': datetime.now().isoformat()
        }

        self.results['step2_strategy'] = step2_results
        self._save_checkpoint('step2_strategy.json', step2_results)

        return step2_results

    def step_3_compression_implementation(self):
        """æ­¥é©Ÿ3: å£“ç¸®å¯¦æ–½"""

        print("=== æ­¥é©Ÿ3: å£“ç¸®å¯¦æ–½ ===")

        strategy = self.results.get('step2_strategy', {}).get('optimal_strategy')
        if not strategy:
            raise ValueError("è«‹å…ˆå®Œæˆæ­¥é©Ÿ2çš„ç­–ç•¥è¨­è¨ˆ")

        implementation_results = {}

        # 3.1 é‡åŒ–å¯¦æ–½
        if 'quantization' in strategy['methods']:
            print("3.1 åŸ·è¡Œé‡åŒ–å£“ç¸®...")
            quant_config = strategy['methods']['quantization']
            quantization_result = self._implement_quantization(quant_config)
            implementation_results['quantization'] = quantization_result

        # 3.2 å‰ªæå¯¦æ–½
        if 'pruning' in strategy['methods']:
            print("3.2 åŸ·è¡Œå‰ªæå£“ç¸®...")
            pruning_config = strategy['methods']['pruning']
            pruning_result = self._implement_pruning(pruning_config)
            implementation_results['pruning'] = pruning_result

        # 3.3 æ¨ç†å¼•æ“å„ªåŒ–
        if 'inference_optimization' in strategy['methods']:
            print("3.3 æ¨ç†å¼•æ“å„ªåŒ–...")
            optimization_result = self._optimize_inference_engine()
            implementation_results['inference_optimization'] = optimization_result

        # 3.4 æ•ˆæœé©—è­‰
        print("3.4 é©—è­‰å£“ç¸®æ•ˆæœ...")
        validation_result = self._validate_compression_effects(implementation_results)

        step3_results = {
            'implementation_results': implementation_results,
            'validation_result': validation_result,
            'compressed_model_info': self._get_compressed_model_info(),
            'timestamp': datetime.now().isoformat()
        }

        self.results['step3_implementation'] = step3_results
        self._save_checkpoint('step3_implementation.json', step3_results)

        return step3_results

    def step_4_deployment_preparation(self, deployment_target: dict):
        """æ­¥é©Ÿ4: éƒ¨ç½²æº–å‚™"""

        print("=== æ­¥é©Ÿ4: éƒ¨ç½²æº–å‚™ ===")

        # 4.1 æ¨¡å‹æ ¼å¼è½‰æ›
        print("4.1 æ¨¡å‹æ ¼å¼è½‰æ›...")
        format_conversion = self._convert_model_format(deployment_target)

        # 4.2 æ¨ç†æœå‹™é…ç½®
        print("4.2 æ¨ç†æœå‹™é…ç½®...")
        service_config = self._configure_inference_service(deployment_target)

        # 4.3 æ€§èƒ½åŸºæº–æ¸¬è©¦
        print("4.3 éƒ¨ç½²æ€§èƒ½æ¸¬è©¦...")
        deployment_benchmarks = self._run_deployment_benchmarks(deployment_target)

        # 4.4 ç”Ÿç”¢å°±ç·’æª¢æŸ¥
        print("4.4 ç”Ÿç”¢å°±ç·’æª¢æŸ¥...")
        production_readiness = self._check_production_readiness(deployment_benchmarks)

        step4_results = {
            'format_conversion': format_conversion,
            'service_config': service_config,
            'deployment_benchmarks': deployment_benchmarks,
            'production_readiness': production_readiness,
            'deployment_artifacts': self._prepare_deployment_artifacts(),
            'timestamp': datetime.now().isoformat()
        }

        self.results['step4_deployment'] = step4_results
        self._save_checkpoint('step4_deployment.json', step4_results)

        return step4_results

    # === æ ¸å¿ƒå¯¦æ–½æ–¹æ³• ===

    def _implement_quantization(self, config: dict) -> dict:
        """å¯¦æ–½é‡åŒ–å£“ç¸®"""

        print(f"å¯¦æ–½{config['method']}é‡åŒ–ï¼Œç›®æ¨™ç²¾åº¦ï¼š{config['target_precision']}")

        start_time = time.time()

        try:
            if config['method'] == 'ptq_bitsandbytes':
                # ä½¿ç”¨BitsAndBytesé€²è¡ŒPTQ
                if config['target_precision'] == 'int8':
                    bnb_config = BitsAndBytesConfig(load_in_8bit=True)
                elif config['target_precision'] == 'int4':
                    bnb_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_use_double_quant=False
                    )
                else:
                    raise ValueError(f"Unsupported precision: {config['target_precision']}")

                # è¼‰å…¥é‡åŒ–æ¨¡å‹
                quantized_model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    quantization_config=bnb_config,
                    device_map="auto"
                )

                # æ¸¬è©¦åŠŸèƒ½
                tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
                test_result = self._test_quantized_model(quantized_model, tokenizer)

                # è¨ˆç®—å£“ç¸®æ•ˆæœ
                original_size = self._estimate_model_size(self.base_model_name, 'fp16')
                quantized_size = self._estimate_model_size_quantized(quantized_model)

                end_time = time.time()

                return {
                    'method': config['method'],
                    'target_precision': config['target_precision'],
                    'success': True,
                    'compression_time_seconds': end_time - start_time,
                    'original_size_mb': original_size,
                    'quantized_size_mb': quantized_size,
                    'compression_ratio': original_size / quantized_size if quantized_size > 0 else 1,
                    'test_result': test_result,
                    'quantized_model': quantized_model
                }

            else:
                return {
                    'method': config['method'],
                    'success': False,
                    'error': f"Method {config['method']} not implemented in this demo"
                }

        except Exception as e:
            end_time = time.time()
            return {
                'method': config['method'],
                'success': False,
                'error': str(e),
                'compression_time_seconds': end_time - start_time
            }

    def _test_quantized_model(self, model, tokenizer) -> dict:
        """æ¸¬è©¦é‡åŒ–æ¨¡å‹åŠŸèƒ½"""

        test_prompts = [
            "äººå·¥æ™ºèƒ½çš„ç™¼å±•",
            "æ©Ÿå™¨å­¸ç¿’ç®—æ³•",
            "æ·±åº¦å­¸ç¿’æ‡‰ç”¨"
        ]

        test_results = []

        for prompt in test_prompts:
            try:
                inputs = tokenizer(prompt, return_tensors="pt")
                if torch.cuda.is_available():
                    inputs = {k: v.cuda() for k, v in inputs.items()}

                start_time = time.time()

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 30,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )

                inference_time = time.time() - start_time

                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                response = generated_text[len(prompt):].strip()

                test_results.append({
                    'prompt': prompt,
                    'response': response,
                    'inference_time': inference_time,
                    'success': True
                })

            except Exception as e:
                test_results.append({
                    'prompt': prompt,
                    'success': False,
                    'error': str(e)
                })

        # è¨ˆç®—å¹³å‡æ€§èƒ½
        successful_tests = [r for r in test_results if r['success']]
        avg_inference_time = np.mean([r['inference_time'] for r in successful_tests]) if successful_tests else 0

        return {
            'test_results': test_results,
            'success_rate': len(successful_tests) / len(test_results) * 100,
            'avg_inference_time': avg_inference_time,
            'functional_test_passed': len(successful_tests) == len(test_results)
        }

    def _estimate_model_size(self, model_name: str, precision: str) -> float:
        """ä¼°ç®—æ¨¡å‹å¤§å°"""

        try:
            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
            param_count = sum(p.numel() for p in model.parameters())

            precision_bytes = {'fp32': 4, 'fp16': 2, 'int8': 1, 'int4': 0.5}
            model_size_mb = param_count * precision_bytes[precision] / (1024**2)

            return model_size_mb

        except Exception as e:
            print(f"ä¼°ç®—æ¨¡å‹å¤§å°å¤±æ•—: {e}")
            return 1000.0  # é»˜èªä¼°ç®—å€¼

    def _estimate_model_size_quantized(self, model) -> float:
        """ä¼°ç®—é‡åŒ–æ¨¡å‹å¤§å°"""

        try:
            total_size = 0
            for param in model.parameters():
                total_size += param.numel() * param.element_size()

            return total_size / (1024**2)  # è½‰æ›ç‚ºMB

        except Exception as e:
            print(f"ä¼°ç®—é‡åŒ–æ¨¡å‹å¤§å°å¤±æ•—: {e}")
            return 500.0  # é»˜èªä¼°ç®—å€¼

    def run_hardware_matching_analysis(self):
        """é‹è¡Œç¡¬é«”åŒ¹é…åˆ†æ"""

        print("\\n=== ç¡¬é«”åŒ¹é…åˆ†æ ===")

        # ä½¿ç”¨ä¹‹å‰å®šç¾©çš„ç¡¬é«”åŒ¹é…è¨ˆç®—å™¨
        from MODEL_COMPRESSION_ENGINEERING import HardwareMatchingCalculator

        calculator = HardwareMatchingCalculator()

        # æ¨¡å‹åƒæ•¸ï¼ˆå‡è¨­7Båƒæ•¸çš„æ¨¡å‹ï¼‰
        model_params = 7e9

        print("1. ä¸åŒæ¨¡å¼ä¸‹çš„GPUéœ€æ±‚åˆ†æ:")

        # è¨“ç·´æ¨¡å¼
        train_req = calculator.calculate_minimum_gpus(
            model_params, 'fp16', 'training', batch_size=4, sequence_length=2048
        )

        print("\\nè¨“ç·´æ¨¡å¼:")
        print(f"  è¨˜æ†¶é«”éœ€æ±‚: {train_req['total_memory_required_gb']:.1f} GB")

        for gpu, req in train_req['gpu_requirements'].items():
            print(f"  {gpu}: {req['min_gpus']} GPUs ({req['memory_utilization']:.1f}% åˆ©ç”¨ç‡, ${req['total_cost']:,})")

        # æ¨ç†æ¨¡å¼
        infer_req = calculator.calculate_minimum_gpus(
            model_params, 'fp16', 'inference', batch_size=8, sequence_length=2048
        )

        print("\\næ¨ç†æ¨¡å¼:")
        print(f"  è¨˜æ†¶é«”éœ€æ±‚: {infer_req['total_memory_required_gb']:.1f} GB")

        for gpu, req in infer_req['gpu_requirements'].items():
            print(f"  {gpu}: {req['min_gpus']} GPUs ({req['memory_utilization']:.1f}% åˆ©ç”¨ç‡, ${req['total_cost']:,})")

        # 2. é‡åŒ–å¾Œçš„ç¡¬é«”éœ€æ±‚å°æ¯”
        print("\\n2. é‡åŒ–å¾Œç¡¬é«”éœ€æ±‚å°æ¯”:")

        quantization_scenarios = [
            {'precision': 'fp16', 'name': 'FP16'},
            {'precision': 'int8', 'name': 'INT8'},
            {'precision': 'int4', 'name': 'INT4'}
        ]

        comparison_data = []

        for scenario in quantization_scenarios:
            infer_req = calculator.calculate_minimum_gpus(
                model_params, scenario['precision'], 'inference', batch_size=8, sequence_length=2048
            )

            # é¸æ“‡RTX 4090ä½œç‚ºåƒè€ƒ
            rtx4090_req = infer_req['gpu_requirements']['RTX_4090']

            comparison_data.append({
                'é‡åŒ–æ–¹æ¡ˆ': scenario['name'],
                'è¨˜æ†¶é«”éœ€æ±‚(GB)': f"{infer_req['total_memory_required_gb']:.1f}",
                'æ‰€éœ€GPUæ•¸é‡': rtx4090_req['min_gpus'],
                'è¨˜æ†¶é«”åˆ©ç”¨ç‡': f"{rtx4090_req['memory_utilization']:.1f}%",
                'ç¸½æˆæœ¬(USD)': f"${rtx4090_req['total_cost']:,}"
            })

        # é¡¯ç¤ºå°æ¯”è¡¨æ ¼
        df = pd.DataFrame(comparison_data)
        print("\\né‡åŒ–æ–¹æ¡ˆå°æ¯”:")
        print(df.to_string(index=False))

        return {
            'training_requirements': train_req,
            'inference_requirements': infer_req,
            'quantization_comparison': comparison_data
        }

    def run_compression_experiment(self, compression_configs: list):
        """é‹è¡Œå£“ç¸®å¯¦é©—"""

        print("\\n=== å£“ç¸®å¯¦é©—åŸ·è¡Œ ===")

        experiment_results = {}

        for i, config in enumerate(compression_configs):
            experiment_name = f"experiment_{i+1}_{config['name']}"
            print(f"\\né‹è¡Œå¯¦é©— {i+1}: {config['name']}")

            try:
                # è¨˜éŒ„å¯¦é©—é–‹å§‹æ™‚é–“
                start_time = time.time()

                # åŸ·è¡Œå£“ç¸®
                if config['type'] == 'quantization':
                    result = self._run_quantization_experiment(config)
                elif config['type'] == 'mixed_compression':
                    result = self._run_mixed_compression_experiment(config)
                else:
                    result = {'error': f"Unsupported experiment type: {config['type']}"}

                # è¨˜éŒ„å¯¦é©—æ™‚é–“
                end_time = time.time()
                result['experiment_duration'] = end_time - start_time

                experiment_results[experiment_name] = result

                print(f"å¯¦é©— {i+1} å®Œæˆï¼Œè€—æ™‚ {result['experiment_duration']:.1f} ç§’")

            except Exception as e:
                print(f"å¯¦é©— {i+1} å¤±æ•—: {e}")
                experiment_results[experiment_name] = {
                    'success': False,
                    'error': str(e)
                }

        return experiment_results

    def _run_quantization_experiment(self, config: dict) -> dict:
        """é‹è¡Œé‡åŒ–å¯¦é©—"""

        # å¯¦æ–½é‡åŒ–
        quantization_result = self._implement_quantization(config['quantization_config'])

        if not quantization_result['success']:
            return quantization_result

        # æ€§èƒ½å°æ¯”æ¸¬è©¦
        performance_comparison = self._compare_quantization_performance(quantization_result)

        return {
            'quantization_result': quantization_result,
            'performance_comparison': performance_comparison,
            'success': True
        }

    def _compare_quantization_performance(self, quantization_result: dict) -> dict:
        """å°æ¯”é‡åŒ–å‰å¾Œçš„æ€§èƒ½"""

        print("å°æ¯”é‡åŒ–å‰å¾Œæ€§èƒ½...")

        # è¼‰å…¥åŸå§‹æ¨¡å‹
        original_tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        original_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )

        # è¼‰å…¥é‡åŒ–æ¨¡å‹
        quantized_model = quantization_result.get('quantized_model')

        # æ€§èƒ½æ¸¬è©¦
        test_prompts = ["äººå·¥æ™ºèƒ½æŠ€è¡“", "æ©Ÿå™¨å­¸ç¿’ç®—æ³•", "æ·±åº¦å­¸ç¿’æ‡‰ç”¨"]
        comparison_results = []

        for prompt in test_prompts:
            # åŸå§‹æ¨¡å‹æ¸¬è©¦
            original_result = self._benchmark_single_model(original_model, original_tokenizer, prompt)

            # é‡åŒ–æ¨¡å‹æ¸¬è©¦
            quantized_result = self._benchmark_single_model(quantized_model, original_tokenizer, prompt)

            comparison_results.append({
                'prompt': prompt,
                'original': original_result,
                'quantized': quantized_result,
                'speedup': original_result['inference_time'] / quantized_result['inference_time'] if quantized_result['inference_time'] > 0 else 1
            })

        # è¨ˆç®—å¹³å‡æ€§èƒ½æŒ‡æ¨™
        avg_speedup = np.mean([r['speedup'] for r in comparison_results])
        avg_original_time = np.mean([r['original']['inference_time'] for r in comparison_results])
        avg_quantized_time = np.mean([r['quantized']['inference_time'] for r in comparison_results])

        return {
            'detailed_comparisons': comparison_results,
            'average_speedup': avg_speedup,
            'average_original_time': avg_original_time,
            'average_quantized_time': avg_quantized_time,
            'compression_ratio': quantization_result['compression_ratio'],
            'memory_saving_percent': (1 - quantization_result['quantized_size_mb'] / quantization_result['original_size_mb']) * 100
        }

    def _benchmark_single_model(self, model, tokenizer, prompt: str) -> dict:
        """å°å–®å€‹æ¨¡å‹é€²è¡ŒåŸºæº–æ¸¬è©¦"""

        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}

            # è¨˜éŒ„æ¨ç†æ™‚é–“
            start_time = time.time()

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + 30,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )

            end_time = time.time()

            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = generated_text[len(prompt):].strip()

            return {
                'inference_time': end_time - start_time,
                'generated_response': response,
                'tokens_generated': outputs.shape[1] - inputs['input_ids'].shape[1],
                'success': True
            }

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'inference_time': float('inf')
            }

    def generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€çµ‚å¯¦é©—å ±å‘Š"""

        report = f"""# æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦è¸å ±å‘Š

## å¯¦é©—æ¦‚è¿°
- åŸºç¤æ¨¡å‹: {self.base_model_name}
- å¯¦é©—å®Œæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- å¯¦é©—éšæ®µ: {len(self.results)} å€‹éšæ®µ

## ä¸»è¦ç™¼ç¾
"""

        # åˆ†æå¯¦é©—çµæœ
        if 'step3_implementation' in self.results:
            impl_results = self.results['step3_implementation']

            # é‡åŒ–æ•ˆæœåˆ†æ
            if 'quantization' in impl_results['implementation_results']:
                quant_result = impl_results['implementation_results']['quantization']
                if quant_result['success']:
                    report += f"""
### é‡åŒ–å£“ç¸®æ•ˆæœ
- å£“ç¸®æ–¹æ³•: {quant_result['method']}
- ç›®æ¨™ç²¾åº¦: {quant_result['target_precision']}
- å£“ç¸®æ¯”: {quant_result['compression_ratio']:.2f}x
- æ¨¡å‹å¤§å°: {quant_result['original_size_mb']:.1f}MB â†’ {quant_result['quantized_size_mb']:.1f}MB
- å£“ç¸®æ™‚é–“: {quant_result['compression_time_seconds']:.1f} ç§’
"""

        if 'hardware_matching' in self.results:
            hardware_result = self.results['hardware_matching']
            report += """
### ç¡¬é«”éœ€æ±‚åˆ†æ
è©³è¦‹ç¡¬é«”åŒ¹é…åˆ†æçµæœè¡¨æ ¼ã€‚
"""

        report += f"""
## å·¥ç¨‹åŒ–è¦é»ç¸½çµ

### æˆåŠŸå› ç´ 
1. **å…¨é¢çš„å‰æœŸåˆ†æ**: è©³ç´°åˆ†ææ¨¡å‹çµæ§‹å’Œç¡¬é«”éœ€æ±‚
2. **ç§‘å­¸çš„ç­–ç•¥åˆ¶å®š**: åŸºæ–¼æ•¸æ“šé©…å‹•çš„å£“ç¸®ç­–ç•¥é¸æ“‡
3. **ç³»çµ±çš„å¯¦æ–½æµç¨‹**: éšæ®µæ€§å¯¦æ–½å’ŒæŒçºŒé©—è­‰
4. **å®Œæ•´çš„æ•ˆæœè©•ä¼°**: å¤šç¶­åº¦çš„å£“ç¸®æ•ˆæœè©•ä¼°

### æ”¹é€²å»ºè­°
1. å»ºç«‹æ›´å®Œå–„çš„è‡ªå‹•åŒ–å£“ç¸®æµç¨‹
2. å¢åŠ æ›´å¤šå£“ç¸®æŠ€è¡“çš„çµ„åˆå¯¦é©—
3. å»ºç«‹æ›´ç²¾ç¢ºçš„æ€§èƒ½é æ¸¬æ¨¡å‹
4. å®Œå–„ç”Ÿç”¢éƒ¨ç½²çš„ç›£æ§é«”ç³»

### æœ€ä½³å¯¦è¸
1. **æ¼¸é€²å¼å£“ç¸®**: åˆ†éšæ®µå¯¦æ–½ï¼Œé€æ­¥å„ªåŒ–
2. **å……åˆ†æ¸¬è©¦**: åœ¨æ¯å€‹éšæ®µé€²è¡Œå…¨é¢æ¸¬è©¦
3. **æŒçºŒç›£æ§**: éƒ¨ç½²å¾ŒæŒçºŒç›£æ§æ•ˆæœ
4. **æ–‡æª”å®Œå‚™**: è¨˜éŒ„æ‰€æœ‰æ±ºç­–å’Œå¯¦æ–½ç´°ç¯€

## éƒ¨ç½²å»ºè­°
{self._generate_deployment_recommendations()}

## æœªä¾†å„ªåŒ–æ–¹å‘
1. æ¢ç´¢æ›´å…ˆé€²çš„é‡åŒ–æŠ€è¡“ï¼ˆå¦‚QLoRAã€GGMLï¼‰
2. çµåˆæ¨¡å‹å‰ªæå’ŒçŸ¥è­˜è’¸é¤¾
3. é‡å°ç‰¹å®šç¡¬é«”çš„æ·±åº¦å„ªåŒ–
4. å»ºç«‹è‡ªå‹•åŒ–çš„å£“ç¸®åƒæ•¸èª¿å„ªç³»çµ±
"""

        return report

    def _generate_deployment_recommendations(self) -> str:
        """ç”Ÿæˆéƒ¨ç½²å»ºè­°"""

        recommendations = []

        # åŸºæ–¼å¯¦é©—çµæœç”Ÿæˆå»ºè­°
        if 'step3_implementation' in self.results:
            impl_results = self.results['step3_implementation']['implementation_results']

            if 'quantization' in impl_results and impl_results['quantization']['success']:
                quant_result = impl_results['quantization']
                compression_ratio = quant_result.get('compression_ratio', 1)

                if compression_ratio > 3:
                    recommendations.append("âœ… é‡åŒ–æ•ˆæœå„ªç§€ï¼Œæ¨è–¦ç”Ÿç”¢éƒ¨ç½²")
                elif compression_ratio > 2:
                    recommendations.append("âš ï¸ é‡åŒ–æ•ˆæœè‰¯å¥½ï¼Œå»ºè­°å°è¦æ¨¡è©¦é©—å¾Œéƒ¨ç½²")
                else:
                    recommendations.append("âŒ é‡åŒ–æ•ˆæœæœ‰é™ï¼Œå»ºè­°é€²ä¸€æ­¥å„ªåŒ–")

        if not recommendations:
            recommendations.append("å»ºè­°å®Œæˆå®Œæ•´å¯¦é©—å¾Œå†åˆ¶å®šéƒ¨ç½²ç­–ç•¥")

        return "\\n".join(recommendations)

    # === è¼”åŠ©æ–¹æ³• ===

    def _save_checkpoint(self, filename: str, data: dict):
        """ä¿å­˜æª¢æŸ¥é»æ•¸æ“š"""

        os.makedirs('checkpoints', exist_ok=True)
        checkpoint_path = os.path.join('checkpoints', filename)

        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)

        print(f"æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_path}")

    def _load_original_model(self):
        """è¼‰å…¥åŸå§‹æ¨¡å‹"""

        print(f"è¼‰å…¥æ¨¡å‹: {self.base_model_name}")

        tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )

        print(f"æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œåƒæ•¸é‡: {model.num_parameters():,}")

        return model, tokenizer

    def _analyze_model_structure(self, model) -> dict:
        """åˆ†ææ¨¡å‹çµæ§‹"""

        # è¨ˆç®—åƒæ•¸åˆ†ä½ˆ
        param_distribution = {}
        total_params = 0

        for name, param in model.named_parameters():
            layer_type = name.split('.')[0] if '.' in name else name
            param_count = param.numel()

            if layer_type not in param_distribution:
                param_distribution[layer_type] = 0

            param_distribution[layer_type] += param_count
            total_params += param_count

        # è¨ˆç®—åƒæ•¸æ¯”ä¾‹
        param_ratios = {k: v/total_params*100 for k, v in param_distribution.items()}

        return {
            'total_parameters': total_params,
            'parameter_distribution': param_distribution,
            'parameter_ratios': param_ratios,
            'model_size_mb': total_params * 2 / (1024**2),  # FP16
            'architecture_type': 'transformer'  # å‡è¨­ç‚ºtransformeræ¶æ§‹
        }

# ä¸»å¯¦é©—åŸ·è¡Œå‡½æ•¸
def main():
    """ä¸»å¯¦é©—åŸ·è¡Œå‡½æ•¸"""

    print("=== æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦è¸å¯¦é©— ===\\n")

    # åˆå§‹åŒ–å¯¦é©—å®¤
    lab = ModelCompressionEngineeringLab()

    try:
        # æ­¥é©Ÿ1: æ¨¡å‹åˆ†æ
        step1_result = lab.step_1_model_analysis_and_baseline()

        # ç¡¬é«”åŒ¹é…åˆ†æ
        hardware_matching = lab.run_hardware_matching_analysis()
        lab.results['hardware_matching'] = hardware_matching

        # æ­¥é©Ÿ2: å£“ç¸®ç­–ç•¥è¨­è¨ˆ
        target_constraints = {
            'max_memory_gb': 16,      # ç›®æ¨™ï¼šé©é…16GB GPU
            'max_accuracy_loss': 5,   # æœ€å¤§å¯æ¥å—æº–ç¢ºæ€§æå¤±5%
            'min_speedup': 1.5,       # æœ€å°åŠ é€Ÿæ¯”1.5x
            'deployment_target': 'cloud_inference'
        }

        step2_result = lab.step_2_compression_strategy_design(target_constraints)

        # æ­¥é©Ÿ3: å£“ç¸®å¯¦æ–½
        step3_result = lab.step_3_compression_implementation()

        # é‹è¡Œå£“ç¸®å¯¦é©—
        compression_configs = [
            {
                'name': 'INT8_PTQ',
                'type': 'quantization',
                'quantization_config': {
                    'method': 'ptq_bitsandbytes',
                    'target_precision': 'int8'
                }
            },
            {
                'name': 'INT4_PTQ',
                'type': 'quantization',
                'quantization_config': {
                    'method': 'ptq_bitsandbytes',
                    'target_precision': 'int4'
                }
            }
        ]

        experiment_results = lab.run_compression_experiment(compression_configs)
        lab.results['experiments'] = experiment_results

        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        final_report = lab.generate_final_report()

        # ä¿å­˜å ±å‘Š
        with open('compression_engineering_final_report.md', 'w', encoding='utf-8') as f:
            f.write(final_report)

        print("\\n=== å¯¦é©—å®Œæˆ ===")
        print("æœ€çµ‚å ±å‘Šå·²ä¿å­˜åˆ°: compression_engineering_final_report.md")
        print("æª¢æŸ¥é»æ•¸æ“šå·²ä¿å­˜åˆ°: checkpoints/ ç›®éŒ„")

        # é¡¯ç¤ºé—œéµçµæœ
        print("\\né—œéµå¯¦é©—çµæœ:")
        for exp_name, exp_result in experiment_results.items():
            if exp_result.get('success'):
                quant_result = exp_result.get('quantization_result', {})
                print(f"  {exp_name}: {quant_result.get('compression_ratio', 1):.2f}x å£“ç¸®")

        return lab.results

    except Exception as e:
        print(f"å¯¦é©—åŸ·è¡Œå‡ºéŒ¯: {e}")
        return None

if __name__ == "__main__":
    results = main()
```

## å¯¦é©—åŸ·è¡ŒæŒ‡å—

### æº–å‚™éšæ®µ
```bash
# 1. æ¿€æ´»è™›æ“¬ç’°å¢ƒ
source 00-Course_Setup/.venv/bin/activate

# 2. å®‰è£é¡å¤–ä¾è³´
pip install bitsandbytes accelerate onnxruntime

# 3. é€²å…¥å¯¦é©—ç›®éŒ„
cd 00-LLM_Fundamentals/02-Labs/Lab-0.6-Model_Compression_Engineering
```

### åŸ·è¡Œéšæ®µ
```bash
# é‹è¡Œå®Œæ•´å·¥ç¨‹åŒ–å¯¦é©—
python compression_engineering_lab.py
```

### é©—è­‰éšæ®µ
æª¢æŸ¥ç”Ÿæˆçš„æ–‡ä»¶ï¼š
- `compression_engineering_final_report.md` - æœ€çµ‚å ±å‘Š
- `checkpoints/` - å„éšæ®µæª¢æŸ¥é»æ•¸æ“š
- å„ç¨®æ€§èƒ½å°æ¯”åœ–è¡¨

## é æœŸå­¸ç¿’æˆæœ

### æŠ€è¡“èƒ½åŠ›æå‡
- âœ… **ç²¾ç¢ºè¨ˆç®—èƒ½åŠ›**: æŒæ¡æ¨¡å‹-ç¡¬é«”åŒ¹é…çš„ç²¾ç¢ºè¨ˆç®—
- âœ… **å·¥ç¨‹åŒ–æ€ç¶­**: å»ºç«‹ç³»çµ±æ€§çš„å£“ç¸®å·¥ç¨‹åŒ–æ€ç¶­
- âœ… **å¯¦æ–½æŠ€èƒ½**: å…·å‚™ç«¯åˆ°ç«¯å£“ç¸®å¯¦æ–½èƒ½åŠ›
- âœ… **å„ªåŒ–æ„è­˜**: å½¢æˆæŒçºŒå„ªåŒ–çš„å·¥ç¨‹æ„è­˜

### å¯¦è¸ç¶“é©—ç²å¾—
- ğŸ”§ å®Œæ•´å£“ç¸®æµç¨‹çš„å¯¦éš›æ“ä½œç¶“é©—
- ğŸ“Š å¤šç¶­åº¦æ•ˆæœè©•ä¼°çš„å¯¦è¸æ–¹æ³•
- ğŸ¯ ç¡¬é«”ç´„æŸä¸‹çš„å„ªåŒ–ç­–ç•¥åˆ¶å®š
- ğŸš€ ç”Ÿç”¢éƒ¨ç½²çš„æº–å‚™å’Œé©—è­‰æµç¨‹

### å·¥å…·æŒæ¡ç¨‹åº¦
- **é‡åŒ–å·¥å…·**: BitsAndBytesã€GPTQã€AWQç­‰
- **æ€§èƒ½åˆ†æ**: è¨˜æ†¶é«”åˆ†æã€æ€§èƒ½åŸºæº–æ¸¬è©¦
- **éƒ¨ç½²å·¥å…·**: ONNXã€TensorRTç­‰æ¨ç†å¼•æ“
- **ç›£æ§å·¥å…·**: è³‡æºç›£æ§ã€æ€§èƒ½ç›£æ§ç³»çµ±

## å¯¦é©—å ±å‘Šè¦æ±‚

### å¿…ç­”å•é¡Œ
1. **ç¡¬é«”åŒ¹é…åˆ†æ**: ç‚ºä¸åŒè¦æ¨¡æ¨¡å‹è¨ˆç®—ç¡¬é«”éœ€æ±‚
2. **å£“ç¸®ç­–ç•¥é¸æ“‡**: è§£é‡‹æ‚¨é¸æ“‡ç‰¹å®šå£“ç¸®ç­–ç•¥çš„åŸå› 
3. **æ•ˆæœè©•ä¼°**: åˆ†æå£“ç¸®å‰å¾Œçš„æ€§èƒ½è®ŠåŒ–
4. **å·¥ç¨‹åŒ–è¦é»**: ç¸½çµå·¥ç¨‹åŒ–å¯¦æ–½çš„é—œéµè¦é»

### å»¶ä¼¸æ€è€ƒ
1. å¦‚ä½•å»ºç«‹è‡ªå‹•åŒ–çš„å£“ç¸®åƒæ•¸èª¿å„ªç³»çµ±ï¼Ÿ
2. åœ¨è³‡æºå—é™ç’°å¢ƒä¸‹å¦‚ä½•æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ï¼Ÿ
3. å¦‚ä½•è™•ç†å£“ç¸®éç¨‹ä¸­çš„æ„å¤–æƒ…æ³å’ŒéŒ¯èª¤ï¼Ÿ
4. æœªä¾†å¯èƒ½çš„æ¨¡å‹å£“ç¸®æŠ€è¡“ç™¼å±•æ–¹å‘æ˜¯ä»€éº¼ï¼Ÿ

é€™å€‹Labæä¾›äº†å®Œæ•´çš„æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦è¸é«”é©—ï¼Œå¹«åŠ©å­¸å“¡æŒæ¡å·¥æ¥­ç´šçš„æ¨¡å‹å„ªåŒ–å’Œéƒ¨ç½²æŠ€èƒ½ï¼