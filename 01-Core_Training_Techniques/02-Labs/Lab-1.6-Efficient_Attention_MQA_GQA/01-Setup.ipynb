{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Lab-1.6: 高效注意力機制架構分析\n## MHA vs MQA vs GQA 完整對比\n\n**學習目標**:\n- 深度理解三種注意力架構的核心差異\n- 掌握 KV Cache 優化原理與實現\n- 分析記憶體使用與推理性能的權衡\n- 實現完整的性能基準測試\n\n**前置知識**: Multi-Head Attention, PyTorch 基礎, CUDA 記憶體管理\n\n---\n\n## 🔍 核心架構對比\n\n### 1. Multi-Head Attention (MHA) - 傳統方法\n```\nQ: [batch, seq_len, num_heads=32, head_dim=128]     ← 32 組獨立的 Query\nK: [batch, seq_len, num_heads=32, head_dim=128]     ← 32 組獨立的 Key  \nV: [batch, seq_len, num_heads=32, head_dim=128]     ← 32 組獨立的 Value\n\n特點: 每個頭都有獨立的 K, V\n優點: 最高表現力，理論上質量最好\n缺點: KV Cache 最大，推理速度最慢\n```\n\n### 2. Multi-Query Attention (MQA) - 極致優化\n```\nQ: [batch, seq_len, num_heads=32, head_dim=128]     ← 32 組獨立的 Query\nK: [batch, seq_len, num_heads=1,  head_dim=128]     ← 1 組共享的 Key (廣播)\nV: [batch, seq_len, num_heads=1,  head_dim=128]     ← 1 組共享的 Value (廣播)\n\n特點: 所有頭共享單一 K, V\n優點: KV Cache 最小 (減少 32x)，速度最快\n缺點: 表現力受限，質量可能略降\n```\n\n### 3. Grouped-Query Attention (GQA) - 平衡之選\n```\nQ: [batch, seq_len, num_heads=32, head_dim=128]     ← 32 組獨立的 Query\nK: [batch, seq_len, num_heads=8,  head_dim=128]     ← 8 組分組的 Key\nV: [batch, seq_len, num_heads=8,  head_dim=128]     ← 8 組分組的 Value\n\n特點: 4 個 Q heads 共享 1 組 K, V (32÷8=4)\n優點: 平衡質量與效率，廣泛驗證\n缺點: 複雜度相對較高\n```\n\n---\n\n## 📊 資源消耗對比 (Llama-2-7B 規模)\n\n| 架構 | KV Groups | KV Cache (GB) | 相對MHA | 推理加速 | 質量保持 |\n|------|-----------|---------------|---------|----------|----------|\n| MHA  | 32        | 1.05          | 100%    | 1.0x     | 100%     |\n| GQA-8| 8         | 0.26          | 25%     | 1.3x     | 95-98%   |\n| GQA-4| 4         | 0.13          | 12.5%   | 1.5x     | 90-95%   |\n| MQA  | 1         | 0.03          | 3%      | 1.8x     | 85-90%   |\n\n---\n\n## 🎯 選擇建議\n\n**🚀 追求極致速度**: MQA\n- 適用: 推理密集型應用、邊緣設備\n- 典型: PaLM, Falcon 模型\n\n**⚖️ 平衡質量與效率**: GQA-8\n- 適用: 生產部署、通用服務\n- 典型: Llama-2, Mistral 模型\n\n**🔬 研究與基準**: MHA\n- 適用: 質量上限探索、算法驗證\n- 典型: GPT, BERT 系列模型\n\n**預計時間**: 60-90分鐘"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention 實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"標準 Multi-Head Attention\"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads 組 K\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads 組 V\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        B, N, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # 處理 KV Cache\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            K = torch.cat([past_k, K], dim=1)\n",
    "            V = torch.cat([past_v, V], dim=1)\n",
    "        \n",
    "        # Transpose for attention\n",
    "        Q = Q.transpose(1, 2)  # [B, H, N, D]\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(B, N, self.hidden_dim)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            return output, (K.transpose(1, 2), V.transpose(1, 2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. KV Cache 性能測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KV Cache 大小分析 (Llama-2-7B 配置)\n",
      "======================================================================\n",
      "\n",
      "配置:\n",
      "  num_layers: 32\n",
      "  num_query_heads: 32\n",
      "  head_dim: 128\n",
      "  seq_len: 2048\n",
      "\n",
      "架構                   KV Heads     每層(MB)          總計(GB)          相對MHA       \n",
      "----------------------------------------------------------------------\n",
      "MHA (標準)             32           33.55           1.05            100%        \n",
      "GQA-8                8            8.39            0.26            25.0%       \n",
      "GQA-4                4            4.19            0.13            12.5%       \n",
      "MQA                  1            1.05            0.03            3.1%        \n",
      "\n",
      "💡 觀察: MQA 將 KV Cache 減少至 MHA 的 3%, GQA-8 減少至 25%\n"
     ]
    }
   ],
   "source": [
    "def measure_kv_cache_size(num_layers, seq_len, num_kv_heads, head_dim, dtype=torch.float16):\n",
    "    \"\"\"計算 KV Cache 大小\"\"\"\n",
    "    bytes_per_element = 2 if dtype == torch.float16 else 4\n",
    "    \n",
    "    # 每層: 2 (K+V) × seq_len × num_kv_heads × head_dim × bytes\n",
    "    per_layer_mb = 2 * seq_len * num_kv_heads * head_dim * bytes_per_element / 1e6\n",
    "    total_mb = per_layer_mb * num_layers\n",
    "    \n",
    "    return {\n",
    "        'per_layer_mb': per_layer_mb,\n",
    "        'total_mb': total_mb,\n",
    "        'total_gb': total_mb / 1024\n",
    "    }\n",
    "\n",
    "# Llama-2-7B 配置\n",
    "config = {\n",
    "    'num_layers': 32,\n",
    "    'num_query_heads': 32,\n",
    "    'head_dim': 128,\n",
    "    'seq_len': 2048\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KV Cache 大小分析 (Llama-2-7B 配置)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n配置:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# 對比不同架構\n",
    "architectures = [\n",
    "    ('MHA (標準)', config['num_query_heads']),\n",
    "    ('GQA-8', 8),\n",
    "    ('GQA-4', 4),\n",
    "    ('MQA', 1)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'架構':<20} {'KV Heads':<12} {'每層(MB)':<15} {'總計(GB)':<15} {'相對MHA':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "mha_total = None\n",
    "for name, num_kv_heads in architectures:\n",
    "    cache_size = measure_kv_cache_size(\n",
    "        config['num_layers'],\n",
    "        config['seq_len'],\n",
    "        num_kv_heads,\n",
    "        config['head_dim']\n",
    "    )\n",
    "    \n",
    "    if mha_total is None:\n",
    "        mha_total = cache_size['total_gb']\n",
    "        relative = \"100%\"\n",
    "    else:\n",
    "        relative = f\"{cache_size['total_gb']/mha_total*100:.1f}%\"\n",
    "    \n",
    "    print(f\"{name:<20} {num_kv_heads:<12} {cache_size['per_layer_mb']:<15.2f} {cache_size['total_gb']:<15.2f} {relative:<12}\")\n",
    "\n",
    "print(f\"\\n💡 觀察: MQA 將 KV Cache 減少至 MHA 的 3%, GQA-8 減少至 25%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. 推理性能基準測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "px8hs0hmo5h",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(attn_module, seq_len, num_tokens=100):\n",
    "    \"\"\"測試生成性能\"\"\"\n",
    "    attn_module.eval()\n",
    "    \n",
    "    # 模擬推理\n",
    "    x = torch.randn(1, seq_len, 768, device=device, dtype=torch.float16)\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        # Prefill\n",
    "        start = time.time()\n",
    "        out, past_kv = attn_module(x, use_cache=True)\n",
    "        torch.cuda.synchronize()\n",
    "        prefill_time = time.time() - start\n",
    "        \n",
    "        # Decode\n",
    "        for _ in range(num_tokens):\n",
    "            new_token = torch.randn(1, 1, 768, device=device, dtype=torch.float16)\n",
    "            start = time.time()\n",
    "            out, past_kv = attn_module(new_token, past_kv=past_kv, use_cache=True)\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    return {\n",
    "        'prefill_time': prefill_time,\n",
    "        'avg_decode_time': np.mean(times),\n",
    "        'total_time': prefill_time + sum(times),\n",
    "        'peak_memory_gb': peak_mem\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MHA 測試通過: torch.Size([2, 128, 768]) → torch.Size([2, 128, 768])\n",
      "MHA 推理基準測試...\n",
      "Prefill: 0.65ms\n",
      "Decode: 0.49ms/token\n",
      "記憶體: 0.021GB\n"
     ]
    }
   ],
   "source": [
    "# 測試\n",
    "mha = MultiHeadAttention(768, 12).to(device).half()  # 使用 fp16\n",
    "x = torch.randn(2, 128, 768, device=device, dtype=torch.float16)\n",
    "out = mha(x)\n",
    "print(f\"✅ MHA 測試通過: {x.shape} → {out.shape}\")\n",
    "\n",
    "# 測試\n",
    "print(\"MHA 推理基準測試...\")\n",
    "result = benchmark_generation(mha, seq_len=128, num_tokens=50)\n",
    "print(f\"Prefill: {result['prefill_time']*1000:.2f}ms\")\n",
    "print(f\"Decode: {result['avg_decode_time']*1000:.2f}ms/token\")\n",
    "print(f\"記憶體: {result['peak_memory_gb']:.3f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}