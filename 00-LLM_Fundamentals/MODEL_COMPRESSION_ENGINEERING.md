# LLMæ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦è¸æŒ‡å—

## ğŸ“‹ ç¸½è¦½

æœ¬æŒ‡å—æä¾›å¾æ¨¡å‹è¼‰å…¥åˆ°æ¨ç†éƒ¨ç½²çš„å®Œæ•´å·¥ç¨‹åŒ–å£“ç¸®æµç¨‹ï¼ŒåŒ…æ‹¬ç²¾ç¢ºçš„ç¡¬é«”åŒ¹é…è¨ˆç®—å…¬å¼å’Œå¯¦ç”¨çš„å£“ç¸®å¯¦æ–½ç­–ç•¥ã€‚

## ğŸ”§ ç¡¬é«”åŒ¹é…æ ¸å¿ƒè¨ˆç®—å…¬å¼

### åŸºç¤è¨˜æ†¶é«”éœ€æ±‚è¨ˆç®—

```python
# æ ¸å¿ƒè¨˜æ†¶é«”è¨ˆç®—å…¬å¼
class MemoryCalculationFormulas:
    """è¨˜æ†¶é«”è¨ˆç®—æ ¸å¿ƒå…¬å¼é›†åˆ"""

    @staticmethod
    def training_memory_formula(params: int, batch_size: int, seq_len: int,
                               d_model: int, n_layers: int, precision: str = 'fp16') -> dict:
        """
        è¨“ç·´è¨˜æ†¶é«”ç²¾ç¢ºè¨ˆç®—å…¬å¼

        Memory_train = PÃ—(W + O + G) + AÃ—L + B

        å…¶ä¸­ï¼š
        P = æ¨¡å‹åƒæ•¸é‡
        W = æ¬Šé‡å­˜å„²ä¿‚æ•¸ (fp16=2, fp32=4 bytes)
        O = å„ªåŒ–å™¨ä¿‚æ•¸ (Adam=8, SGD=0 bytes per param)
        G = æ¢¯åº¦å­˜å„²ä¿‚æ•¸ (èˆ‡æ¬Šé‡ç›¸åŒç²¾åº¦)
        A = å–®å±¤æ¿€æ´»è¨˜æ†¶é«” = BÃ—SÃ—DÃ—precision_bytes
        L = å±¤æ•¸ (è¨“ç·´æ™‚éœ€è¦ä¿å­˜æ‰€æœ‰å±¤æ¿€æ´»å€¼)
        B = ç·©è¡å€è¨˜æ†¶é«” â‰ˆ æ¨¡å‹è¨˜æ†¶é«”çš„20%
        """

        precision_bytes = {'fp16': 2, 'fp32': 4, 'bf16': 2, 'int8': 1}
        weight_coefficient = precision_bytes[precision]
        optimizer_coefficient = 8  # Adam: momentum(4) + variance(4) bytes
        gradient_coefficient = precision_bytes[precision]

        # æ¨¡å‹ç›¸é—œè¨˜æ†¶é«” (åƒæ•¸ã€å„ªåŒ–å™¨ã€æ¢¯åº¦)
        model_memory = params * (weight_coefficient + optimizer_coefficient + gradient_coefficient)

        # æ¿€æ´»å€¼è¨˜æ†¶é«” (è¨“ç·´æ™‚éœ€è¦ä¿å­˜æ‰€æœ‰å±¤)
        single_layer_activation = batch_size * seq_len * d_model * precision_bytes[precision]
        total_activation_memory = single_layer_activation * n_layers

        # ç·©è¡å€è¨˜æ†¶é«”
        buffer_memory = model_memory * 0.2

        total_memory = model_memory + total_activation_memory + buffer_memory

        return {
            'total_memory_gb': total_memory / (1024**3),
            'model_memory_gb': model_memory / (1024**3),
            'activation_memory_gb': total_activation_memory / (1024**3),
            'buffer_memory_gb': buffer_memory / (1024**3),
            'breakdown': {
                'weights_gb': (params * weight_coefficient) / (1024**3),
                'optimizer_gb': (params * optimizer_coefficient) / (1024**3),
                'gradients_gb': (params * gradient_coefficient) / (1024**3),
                'activations_gb': total_activation_memory / (1024**3)
            }
        }

    @staticmethod
    def inference_memory_formula(params: int, batch_size: int, seq_len: int,
                                d_model: int, n_layers: int, n_heads: int,
                                precision: str = 'fp16') -> dict:
        """
        æ¨ç†è¨˜æ†¶é«”ç²¾ç¢ºè¨ˆç®—å…¬å¼

        Memory_infer = PÃ—W + KV + A + B

        å…¶ä¸­ï¼š
        P = æ¨¡å‹åƒæ•¸é‡
        W = æ¬Šé‡å­˜å„²ä¿‚æ•¸
        KV = KV Cacheè¨˜æ†¶é«” = 2Ã—LÃ—BÃ—HÃ—SÃ—(D/H)Ã—precision_bytes
        A = ç•¶å‰æ¿€æ´»è¨˜æ†¶é«” = BÃ—SÃ—DÃ—precision_bytes (åªéœ€è¦ç•¶å‰å±¤)
        B = ç·©è¡å€è¨˜æ†¶é«” â‰ˆ 10%
        """

        precision_bytes = {'fp16': 2, 'fp32': 4, 'bf16': 2, 'int8': 1}
        weight_coefficient = precision_bytes[precision]

        # æ¨¡å‹åƒæ•¸è¨˜æ†¶é«”
        model_memory = params * weight_coefficient

        # KV Cacheè¨˜æ†¶é«”: 2(K+V) Ã— layers Ã— batch Ã— heads Ã— seq_len Ã— head_dim
        d_head = d_model // n_heads
        kv_cache_memory = 2 * n_layers * batch_size * n_heads * seq_len * d_head * precision_bytes[precision]

        # ç•¶å‰æ¿€æ´»è¨˜æ†¶é«” (æ¨ç†æ™‚åªéœ€è¦ç•¶å‰å±¤)
        current_activation_memory = batch_size * seq_len * d_model * precision_bytes[precision]

        # ç·©è¡å€è¨˜æ†¶é«”
        buffer_memory = model_memory * 0.1  # æ¨ç†æ™‚ç·©è¡éœ€æ±‚è¼ƒå°

        total_memory = model_memory + kv_cache_memory + current_activation_memory + buffer_memory

        return {
            'total_memory_gb': total_memory / (1024**3),
            'model_memory_gb': model_memory / (1024**3),
            'kv_cache_memory_gb': kv_cache_memory / (1024**3),
            'activation_memory_gb': current_activation_memory / (1024**3),
            'buffer_memory_gb': buffer_memory / (1024**3),
            'kv_cache_per_token_mb': (kv_cache_memory / seq_len) / (1024**2)
        }

    @staticmethod
    def gpu_matching_formula(memory_required_gb: float, target_gpus: dict,
                           safety_margin: float = 0.2) -> dict:
        """
        GPUåŒ¹é…è¨ˆç®—å…¬å¼

        æœ€å°GPUæ•¸ = ceil(Memory_required / (GPU_memory Ã— (1 - safety_margin)))
        è¨˜æ†¶é«”åˆ©ç”¨ç‡ = Memory_required / (min_GPUs Ã— GPU_memory)
        æˆæœ¬æ•ˆç›Š = Performance_gain / Total_cost
        """

        matching_results = {}

        for gpu_name, gpu_specs in target_gpus.items():
            available_memory = gpu_specs['memory_gb'] * (1 - safety_margin)
            min_gpus = max(1, np.ceil(memory_required_gb / available_memory))

            actual_utilization = memory_required_gb / (min_gpus * gpu_specs['memory_gb']) * 100
            total_cost = min_gpus * gpu_specs['price_usd']
            cost_per_gb = total_cost / memory_required_gb

            matching_results[gpu_name] = {
                'min_gpus': int(min_gpus),
                'memory_utilization_percent': actual_utilization,
                'total_cost_usd': total_cost,
                'cost_per_gb': cost_per_gb,
                'cost_efficiency_score': gpu_specs['memory_gb'] / gpu_specs['price_usd']  # GB per USD
            }

        return matching_results

    @staticmethod
    def communication_bandwidth_formula(model_size_gb: float, training_time_per_step: float,
                                      parallelism_type: str, num_gpus: int) -> dict:
        """
        é€šè¨Šé »å¯¬éœ€æ±‚è¨ˆç®—å…¬å¼

        æ•¸æ“šä¸¦è¡Œ: BW = 2Ã—(P-1)/P Ã— model_size / step_time
        æ¨¡å‹ä¸¦è¡Œ: BW = 2 Ã— shard_size Ã— layers / step_time
        æµæ°´ç·šä¸¦è¡Œ: BW = activation_size Ã— micro_batches / step_time
        """

        if parallelism_type == 'data_parallel':
            # AllReduceé€šè¨Šæ¨¡å¼
            allreduce_efficiency = (num_gpus - 1) / num_gpus
            bandwidth_gbps = 2 * allreduce_efficiency * model_size_gb / training_time_per_step

        elif parallelism_type == 'tensor_parallel':
            # AllGather + ReduceScatter
            shard_size_gb = model_size_gb / num_gpus
            bandwidth_gbps = 2 * shard_size_gb / training_time_per_step

        elif parallelism_type == 'pipeline_parallel':
            # é»å°é»é€šè¨Š
            activation_size_gb = 0.1  # ä¼°ç®—å€¼ï¼Œå¯¦éš›å–æ±ºæ–¼æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•·åº¦
            micro_batches = 4  # å…¸å‹å¾®æ‰¹æ¬¡æ•¸
            bandwidth_gbps = activation_size_gb * micro_batches / training_time_per_step

        else:
            raise ValueError(f"Unsupported parallelism type: {parallelism_type}")

        return {
            'required_bandwidth_gbps': bandwidth_gbps,
            'parallelism_type': parallelism_type,
            'num_gpus': num_gpus,
            'communication_efficiency': bandwidth_gbps / (model_size_gb / training_time_per_step)
        }
```

### å£“ç¸®æ•ˆæœé æ¸¬å…¬å¼

```python
class CompressionEffectCalculator:
    """å£“ç¸®æ•ˆæœé æ¸¬è¨ˆç®—å™¨"""

    @staticmethod
    def quantization_effect_formula(original_precision: str, target_precision: str,
                                  model_params: int, model_type: str = 'transformer') -> dict:
        """
        é‡åŒ–æ•ˆæœé æ¸¬å…¬å¼

        å£“ç¸®æ¯” = original_bits / target_bits
        è¨˜æ†¶é«”ç¯€çœ = (1 - target_bytes/original_bytes) Ã— 100%
        æ€§èƒ½å½±éŸ¿ = f(æ¨¡å‹é¡å‹, é‡åŒ–æ–¹æ³•, ç²¾åº¦é™ä½å¹…åº¦)
        """

        precision_bits = {'fp32': 32, 'fp16': 16, 'bf16': 16, 'int8': 8, 'int4': 4, 'nf4': 4}
        precision_bytes = {'fp32': 4, 'fp16': 2, 'bf16': 2, 'int8': 1, 'int4': 0.5, 'nf4': 0.5}

        compression_ratio = precision_bits[original_precision] / precision_bits[target_precision]
        memory_saving = (1 - precision_bytes[target_precision] / precision_bytes[original_precision]) * 100

        # æ€§èƒ½å½±éŸ¿é æ¸¬ï¼ˆåŸºæ–¼ç¶“é©—å…¬å¼ï¼‰
        accuracy_degradation_map = {
            ('fp32', 'fp16'): {'transformer': 0.5, 'cnn': 0.2, 'rnn': 1.0},
            ('fp16', 'int8'): {'transformer': 2.0, 'cnn': 1.0, 'rnn': 3.0},
            ('fp16', 'int4'): {'transformer': 5.0, 'cnn': 3.0, 'rnn': 8.0},
            ('int8', 'int4'): {'transformer': 3.0, 'cnn': 2.0, 'rnn': 5.0}
        }

        accuracy_impact = accuracy_degradation_map.get(
            (original_precision, target_precision), {}
        ).get(model_type, 2.0)  # é»˜èª2%æ€§èƒ½ä¸‹é™

        # æ¨ç†åŠ é€Ÿé æ¸¬
        theoretical_speedup = compression_ratio * 0.7  # è€ƒæ…®å¯¦éš›ç¡¬é«”æ•ˆç‡
        memory_bandwidth_speedup = min(compression_ratio, 2.0)  # è¨˜æ†¶é«”é »å¯¬é™åˆ¶

        return {
            'compression_ratio': compression_ratio,
            'memory_saving_percent': memory_saving,
            'model_size_reduction_mb': model_params * (precision_bytes[original_precision] - precision_bytes[target_precision]) / (1024**2),
            'estimated_accuracy_degradation_percent': accuracy_impact,
            'theoretical_speedup': theoretical_speedup,
            'memory_bandwidth_speedup': memory_bandwidth_speedup,
            'recommended_for_deployment': accuracy_impact < 5.0 and compression_ratio > 1.5
        }

    @staticmethod
    def pruning_effect_formula(sparsity_ratio: float, pruning_type: str,
                             model_params: int) -> dict:
        """
        å‰ªææ•ˆæœé æ¸¬å…¬å¼

        å¯¦éš›å£“ç¸®æ¯” = structured_efficiency Ã— sparsity_ratio
        æ€§èƒ½å½±éŸ¿ = sparsity_penalty Ã— sparsity_ratio^2
        åŠ é€Ÿæ•ˆæœ = hardware_support Ã— compression_ratio
        """

        # ä¸åŒå‰ªæé¡å‹çš„æ•ˆç‡ä¿‚æ•¸
        efficiency_map = {
            'unstructured': 0.3,    # éçµæ§‹åŒ–ç¨€ç–çš„ç¡¬é«”æ•ˆç‡è¼ƒä½
            'n_m_structured': 0.8,  # N:Mçµæ§‹åŒ–ç¨€ç–ç¡¬é«”æ”¯æŒå¥½
            'channel_pruning': 0.9, # é€šé“å‰ªææ•ˆç‡æœ€é«˜
            'layer_pruning': 0.95   # å±¤å‰ªææ•ˆç‡æ¥µé«˜ä½†å½±éŸ¿è¼ƒå¤§
        }

        # æ€§èƒ½å½±éŸ¿ä¿‚æ•¸
        performance_penalty_map = {
            'unstructured': 0.5,
            'n_m_structured': 0.3,
            'channel_pruning': 0.8,
            'layer_pruning': 1.2
        }

        efficiency = efficiency_map.get(pruning_type, 0.5)
        penalty = performance_penalty_map.get(pruning_type, 0.5)

        # å¯¦éš›å£“ç¸®æ•ˆæœ
        effective_compression = efficiency * sparsity_ratio
        theoretical_speedup = 1 / (1 - effective_compression)

        # æ€§èƒ½å½±éŸ¿é æ¸¬
        accuracy_degradation = penalty * (sparsity_ratio ** 1.5) * 100

        # è¨˜æ†¶é«”ç¯€çœ
        memory_saving_mb = model_params * 2 * effective_compression / (1024**2)  # å‡è¨­fp16

        return {
            'sparsity_ratio': sparsity_ratio,
            'effective_compression_ratio': effective_compression,
            'theoretical_speedup': theoretical_speedup,
            'memory_saving_mb': memory_saving_mb,
            'estimated_accuracy_degradation_percent': accuracy_degradation,
            'hardware_efficiency': efficiency,
            'recommended_max_sparsity': min(0.9, 5.0 / penalty)  # ä¿æŒ<5%æ€§èƒ½æå¤±çš„æœ€å¤§ç¨€ç–åº¦
        }
```

## ğŸ­ å·¥ç¨‹åŒ–å£“ç¸®å¯¦æ–½æµç¨‹

### å®Œæ•´å·¥ç¨‹åŒ–Pipeline

```python
class ModelCompressionPipeline:
    """æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–ç®¡ç·š"""

    def __init__(self, config: dict):
        self.config = config
        self.compression_history = []
        self.performance_baselines = {}

    def stage_1_model_analysis(self, model_path: str) -> dict:
        """éšæ®µ1: æ¨¡å‹è¼‰å…¥èˆ‡å…¨é¢åˆ†æ"""

        print("=== éšæ®µ1: æ¨¡å‹åˆ†æèˆ‡åŸºç·šå»ºç«‹ ===")

        # 1.1 è¼‰å…¥æ¨¡å‹ä¸¦åˆ†æçµæ§‹
        model_info = self._load_and_analyze_model(model_path)

        # 1.2 å»ºç«‹æ€§èƒ½åŸºç·š
        baseline_metrics = self._establish_performance_baseline(model_path)

        # 1.3 ç¡¬é«”é©é…åˆ†æ
        hardware_analysis = self._analyze_hardware_compatibility(model_info)

        # 1.4 å£“ç¸®æ½›åŠ›è©•ä¼°
        compression_potential = self._assess_compression_potential(model_info, baseline_metrics)

        analysis_result = {
            'model_info': model_info,
            'baseline_metrics': baseline_metrics,
            'hardware_analysis': hardware_analysis,
            'compression_potential': compression_potential,
            'recommendations': self._generate_compression_recommendations(compression_potential)
        }

        # ä¿å­˜åˆ†æçµæœ
        self._save_analysis_result(analysis_result, 'stage1_analysis.json')

        return analysis_result

    def stage_2_compression_strategy(self, analysis_result: dict,
                                   target_constraints: dict) -> dict:
        """éšæ®µ2: å£“ç¸®ç­–ç•¥åˆ¶å®š"""

        print("=== éšæ®µ2: å£“ç¸®ç­–ç•¥åˆ¶å®š ===")

        # 2.1 è§£æç›®æ¨™ç´„æŸ
        constraints = self._parse_target_constraints(target_constraints)

        # 2.2 ç”Ÿæˆå£“ç¸®æ–¹æ¡ˆ
        compression_strategies = self._generate_compression_strategies(
            analysis_result, constraints
        )

        # 2.3 æ–¹æ¡ˆè©•ä¼°å’Œæ’åº
        ranked_strategies = self._evaluate_and_rank_strategies(
            compression_strategies, constraints
        )

        # 2.4 é¢¨éšªè©•ä¼°
        risk_assessment = self._assess_compression_risks(ranked_strategies)

        strategy_result = {
            'target_constraints': constraints,
            'compression_strategies': ranked_strategies,
            'risk_assessment': risk_assessment,
            'recommended_strategy': ranked_strategies[0] if ranked_strategies else None
        }

        self._save_analysis_result(strategy_result, 'stage2_strategy.json')

        return strategy_result

    def stage_3_compression_implementation(self, strategy: dict, model_path: str) -> dict:
        """éšæ®µ3: å£“ç¸®å¯¦æ–½"""

        print("=== éšæ®µ3: å£“ç¸®å¯¦æ–½ ===")

        implementation_steps = [
            self._implement_quantization,
            self._implement_pruning,
            self._implement_knowledge_distillation,
            self._optimize_inference_engine,
            self._validate_compression_effects
        ]

        compressed_model_info = {}

        for step_func in implementation_steps:
            if self._should_execute_step(step_func.__name__, strategy):
                print(f"åŸ·è¡Œ: {step_func.__name__}")

                step_result = step_func(model_path, strategy, compressed_model_info)
                compressed_model_info.update(step_result)

                # è¨˜éŒ„å£“ç¸®æ­·å²
                self.compression_history.append({
                    'step': step_func.__name__,
                    'timestamp': datetime.now(),
                    'result': step_result,
                    'performance_impact': self._measure_performance_impact(step_result)
                })

                print(f"å®Œæˆ: {step_func.__name__}")

        return {
            'compressed_model_path': compressed_model_info.get('final_model_path'),
            'compression_summary': self._generate_compression_summary(compressed_model_info),
            'compression_history': self.compression_history
        }

    def stage_4_deployment_optimization(self, compressed_model_path: str,
                                      deployment_target: dict) -> dict:
        """éšæ®µ4: éƒ¨ç½²å„ªåŒ–"""

        print("=== éšæ®µ4: éƒ¨ç½²å„ªåŒ– ===")

        # 4.1 æ¨ç†å¼•æ“é©é…
        inference_optimization = self._optimize_for_inference_engine(
            compressed_model_path, deployment_target
        )

        # 4.2 æœå‹™æ¶æ§‹å„ªåŒ–
        service_optimization = self._optimize_service_architecture(
            inference_optimization, deployment_target
        )

        # 4.3 æ€§èƒ½åŸºæº–æ¸¬è©¦
        deployment_benchmarks = self._run_deployment_benchmarks(
            service_optimization, deployment_target
        )

        # 4.4 ç”Ÿç”¢éƒ¨ç½²æº–å‚™
        production_readiness = self._prepare_production_deployment(
            deployment_benchmarks, deployment_target
        )

        return {
            'inference_optimization': inference_optimization,
            'service_optimization': service_optimization,
            'deployment_benchmarks': deployment_benchmarks,
            'production_readiness': production_readiness
        }

    def _implement_quantization(self, model_path: str, strategy: dict, context: dict) -> dict:
        """å¯¦æ–½é‡åŒ–å£“ç¸®"""

        quantization_config = strategy.get('quantization', {})

        if not quantization_config.get('enabled', False):
            return {'quantization_skipped': True}

        print(f"å¯¦æ–½ {quantization_config['method']} é‡åŒ–...")

        # æ ¹æ“šé‡åŒ–æ–¹æ³•é¸æ“‡å¯¦æ–½ç­–ç•¥
        if quantization_config['method'] == 'ptq':
            result = self._implement_ptq_quantization(model_path, quantization_config)
        elif quantization_config['method'] == 'qat':
            result = self._implement_qat_quantization(model_path, quantization_config)
        elif quantization_config['method'] == 'gptq':
            result = self._implement_gptq_quantization(model_path, quantization_config)
        elif quantization_config['method'] == 'awq':
            result = self._implement_awq_quantization(model_path, quantization_config)
        else:
            raise ValueError(f"Unsupported quantization method: {quantization_config['method']}")

        # é‡åŒ–æ•ˆæœé©—è­‰
        validation_result = self._validate_quantization_effect(result, quantization_config)

        return {
            'quantization_result': result,
            'validation_result': validation_result,
            'quantized_model_path': result['output_model_path']
        }

    def _implement_ptq_quantization(self, model_path: str, config: dict) -> dict:
        """å¯¦æ–½PTQé‡åŒ–"""

        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch

        # è¼‰å…¥æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)

        # æº–å‚™æ ¡æº–æ•¸æ“š
        calibration_data = self._prepare_calibration_data(config.get('calibration_samples', 128))

        # åŸ·è¡Œé‡åŒ–
        if config['target_precision'] == 'int8':
            # INT8 PTQ
            quantized_model = self._apply_int8_ptq(model, calibration_data, config)
        elif config['target_precision'] == 'int4':
            # INT4 PTQ (ä½¿ç”¨BitsAndBytesConfig)
            from transformers import BitsAndBytesConfig

            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=False
            )

            quantized_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=bnb_config,
                device_map="auto"
            )
        else:
            raise ValueError(f"Unsupported target precision: {config['target_precision']}")

        # ä¿å­˜é‡åŒ–å¾Œæ¨¡å‹
        output_path = f"{model_path}_quantized_{config['target_precision']}"
        quantized_model.save_pretrained(output_path)
        tokenizer.save_pretrained(output_path)

        return {
            'method': 'ptq',
            'target_precision': config['target_precision'],
            'output_model_path': output_path,
            'calibration_samples': len(calibration_data),
            'quantization_config': config
        }

    def _prepare_calibration_data(self, num_samples: int = 128) -> list:
        """æº–å‚™æ ¡æº–æ•¸æ“š"""

        # é€™è£¡ä½¿ç”¨ç°¡å–®çš„æ–‡æœ¬æ¨£æœ¬ä½œç‚ºæ ¡æº–æ•¸æ“š
        calibration_texts = [
            "äººå·¥æ™ºèƒ½æŠ€è¡“æ­£åœ¨æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼ï¼Œå¾æ™ºèƒ½æ‰‹æ©Ÿåˆ°è‡ªå‹•é§•é§›æ±½è»Šã€‚",
            "æ©Ÿå™¨å­¸ç¿’ç®—æ³•èƒ½å¤ å¾å¤§é‡æ•¸æ“šä¸­å­¸ç¿’æ¨¡å¼ï¼Œä¸¦åšå‡ºé æ¸¬å’Œæ±ºç­–ã€‚",
            "æ·±åº¦ç¥ç¶“ç¶²è·¯é€šéå¤šå±¤çµæ§‹æ¨¡æ“¬äººè…¦çš„ä¿¡æ¯è™•ç†æ–¹å¼ã€‚",
            "è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä½¿è¨ˆç®—æ©Ÿèƒ½å¤ ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚",
            "è¨ˆç®—æ©Ÿè¦–è¦ºæŠ€è¡“è®“æ©Ÿå™¨èƒ½å¤ è­˜åˆ¥å’Œç†è§£åœ–åƒå…§å®¹ã€‚"
        ] * (num_samples // 5 + 1)

        return calibration_texts[:num_samples]

    def _validate_quantization_effect(self, quantization_result: dict, config: dict) -> dict:
        """é©—è­‰é‡åŒ–æ•ˆæœ"""

        print("é©—è­‰é‡åŒ–æ•ˆæœ...")

        # è¼‰å…¥é‡åŒ–å¾Œçš„æ¨¡å‹
        quantized_model_path = quantization_result['output_model_path']

        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            tokenizer = AutoTokenizer.from_pretrained(quantized_model_path)
            quantized_model = AutoModelForCausalLM.from_pretrained(quantized_model_path)

            # ç°¡å–®çš„åŠŸèƒ½æ¸¬è©¦
            test_prompt = "äººå·¥æ™ºèƒ½çš„æœªä¾†ç™¼å±•"
            inputs = tokenizer(test_prompt, return_tensors="pt")

            with torch.no_grad():
                outputs = quantized_model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + 20,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )

            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

            return {
                'functional_test_passed': True,
                'test_prompt': test_prompt,
                'generated_response': generated_text[len(test_prompt):],
                'quantization_successful': True
            }

        except Exception as e:
            return {
                'functional_test_passed': False,
                'error': str(e),
                'quantization_successful': False
            }

    def generate_compression_report(self, pipeline_results: dict) -> str:
        """ç”Ÿæˆå£“ç¸®å ±å‘Š"""

        report = f"""# æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å ±å‘Š

## å£“ç¸®ç¸½çµ
- åŸå§‹æ¨¡å‹: {pipeline_results.get('original_model_path', 'Unknown')}
- å£“ç¸®å¾Œæ¨¡å‹: {pipeline_results.get('compressed_model_path', 'Unknown')}
- å£“ç¸®å®Œæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## å£“ç¸®æ•ˆæœ
"""

        if 'compression_summary' in pipeline_results:
            summary = pipeline_results['compression_summary']

            report += f"""
### æ¨¡å‹å¤§å°è®ŠåŒ–
- åŸå§‹å¤§å°: {summary.get('original_size_mb', 0):.1f} MB
- å£“ç¸®å¾Œå¤§å°: {summary.get('compressed_size_mb', 0):.1f} MB
- å£“ç¸®æ¯”: {summary.get('compression_ratio', 1):.2f}x
- å­˜å„²ç¯€çœ: {summary.get('storage_saving_percent', 0):.1f}%

### æ€§èƒ½å½±éŸ¿
- æ¨ç†é€Ÿåº¦è®ŠåŒ–: {summary.get('inference_speedup', 1):.2f}x
- æº–ç¢ºæ€§è®ŠåŒ–: {summary.get('accuracy_change_percent', 0):+.2f}%
- è¨˜æ†¶é«”ä½¿ç”¨è®ŠåŒ–: {summary.get('memory_change_percent', 0):+.1f}%
"""

        report += """
## å£“ç¸®æ­¥é©Ÿè©³æƒ…
"""

        if 'compression_history' in pipeline_results:
            for i, step in enumerate(pipeline_results['compression_history']):
                report += f"""
### æ­¥é©Ÿ {i+1}: {step['step']}
- åŸ·è¡Œæ™‚é–“: {step.get('timestamp', 'Unknown')}
- æ€§èƒ½å½±éŸ¿: {step.get('performance_impact', {})}
"""

        report += f"""
## éƒ¨ç½²å»ºè­°
{self._generate_deployment_recommendations(pipeline_results)}

## ç›£æ§è¦é»
{self._generate_monitoring_recommendations(pipeline_results)}

## é¢¨éšªè©•ä¼°
{self._generate_risk_assessment(pipeline_results)}
"""

        return report

def _generate_deployment_recommendations(self, results: dict) -> str:
        """ç”Ÿæˆéƒ¨ç½²å»ºè­°"""

        recommendations = []

        # åŸºæ–¼å£“ç¸®æ•ˆæœçš„å»ºè­°
        if results.get('compression_summary', {}).get('compression_ratio', 1) > 2:
            recommendations.append("- âœ… å£“ç¸®æ•ˆæœé¡¯è‘—ï¼Œå»ºè­°åœ¨ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²")
        else:
            recommendations.append("- âš ï¸ å£“ç¸®æ•ˆæœæœ‰é™ï¼Œå»ºè­°é€²ä¸€æ­¥å„ªåŒ–")

        # åŸºæ–¼æº–ç¢ºæ€§çš„å»ºè­°
        accuracy_change = results.get('compression_summary', {}).get('accuracy_change_percent', 0)
        if accuracy_change > -2:
            recommendations.append("- âœ… æº–ç¢ºæ€§æå¤±å¯æ¥å—ï¼Œå¯ç›´æ¥éƒ¨ç½²")
        elif accuracy_change > -5:
            recommendations.append("- âš ï¸ æº–ç¢ºæ€§æœ‰æ‰€ä¸‹é™ï¼Œå»ºè­°é€²è¡ŒA/Bæ¸¬è©¦")
        else:
            recommendations.append("- âŒ æº–ç¢ºæ€§æå¤±éå¤§ï¼Œéœ€è¦é‡æ–°å„ªåŒ–å£“ç¸®ç­–ç•¥")

        # åŸºæ–¼ç¡¬é«”åŒ¹é…çš„å»ºè­°
        if results.get('hardware_analysis', {}).get('compatibility_score', 0) > 0.8:
            recommendations.append("- âœ… ç¡¬é«”å…¼å®¹æ€§è‰¯å¥½ï¼Œç„¡éœ€é¡å¤–é©é…")
        else:
            recommendations.append("- âš ï¸ éœ€è¦é‡å°ç›®æ¨™ç¡¬é«”é€²è¡Œå°ˆé–€å„ªåŒ–")

        return "\\n".join(recommendations)
```

### å¯¦éš›å£“ç¸®å·¥ç¨‹åŒ–å¯¦ä¾‹

```python
# å®Œæ•´çš„å·¥ç¨‹åŒ–å£“ç¸®å¯¦ä¾‹
class LLaMACompressionExample:
    """LLaMAæ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦ä¾‹"""

    def __init__(self):
        self.model_name = "NousResearch/Llama-2-7b-hf"
        self.target_deployment = "cloud_inference"

    def run_complete_compression_pipeline(self):
        """é‹è¡Œå®Œæ•´å£“ç¸®æµç¨‹"""

        # 1. æ¨¡å‹åˆ†æ
        print("æ­¥é©Ÿ1: åˆ†æLLaMA-7Bæ¨¡å‹...")
        analysis = self.analyze_llama_model()

        # 2. ç¡¬é«”åŒ¹é…è¨ˆç®—
        print("æ­¥é©Ÿ2: ç¡¬é«”éœ€æ±‚è¨ˆç®—...")
        hardware_matching = self.calculate_hardware_requirements(analysis)

        # 3. å£“ç¸®ç­–ç•¥é¸æ“‡
        print("æ­¥é©Ÿ3: é¸æ“‡å£“ç¸®ç­–ç•¥...")
        compression_strategy = self.select_compression_strategy(analysis, hardware_matching)

        # 4. åŸ·è¡Œå£“ç¸®
        print("æ­¥é©Ÿ4: åŸ·è¡Œæ¨¡å‹å£“ç¸®...")
        compression_result = self.execute_compression(compression_strategy)

        # 5. éƒ¨ç½²å„ªåŒ–
        print("æ­¥é©Ÿ5: éƒ¨ç½²å„ªåŒ–...")
        deployment_result = self.optimize_for_deployment(compression_result)

        # 6. æœ€çµ‚é©—è­‰
        print("æ­¥é©Ÿ6: æœ€çµ‚æ•ˆæœé©—è­‰...")
        final_validation = self.validate_final_model(deployment_result)

        return {
            'analysis': analysis,
            'hardware_matching': hardware_matching,
            'compression_strategy': compression_strategy,
            'compression_result': compression_result,
            'deployment_result': deployment_result,
            'final_validation': final_validation
        }

    def analyze_llama_model(self) -> dict:
        """åˆ†æLLaMAæ¨¡å‹"""

        # LLaMA-7Bæ¨¡å‹é…ç½®
        model_config = {
            'name': 'LLaMA-7B',
            'parameters': 6.7e9,  # å¯¦éš›åƒæ•¸é‡
            'vocab_size': 32000,
            'd_model': 4096,
            'n_layers': 32,
            'n_heads': 32,
            'd_ff': 11008,
            'max_seq_len': 2048
        }

        # ä½¿ç”¨ä¹‹å‰å®šç¾©çš„è¨ˆç®—å…¬å¼
        memory_calc = MemoryCalculationFormulas()

        # è¨“ç·´è¨˜æ†¶é«”éœ€æ±‚
        training_memory = memory_calc.training_memory_formula(
            params=int(model_config['parameters']),
            batch_size=4,
            seq_len=2048,
            d_model=4096,
            n_layers=32,
            precision='fp16'
        )

        # æ¨ç†è¨˜æ†¶é«”éœ€æ±‚
        inference_memory = memory_calc.inference_memory_formula(
            params=int(model_config['parameters']),
            batch_size=8,
            seq_len=2048,
            d_model=4096,
            n_layers=32,
            n_heads=32,
            precision='fp16'
        )

        return {
            'model_config': model_config,
            'training_memory_analysis': training_memory,
            'inference_memory_analysis': inference_memory,
            'compression_opportunities': {
                'quantization_potential': 'High',  # 7Bæ¨¡å‹é‡åŒ–æ•ˆæœé€šå¸¸å¾ˆå¥½
                'pruning_potential': 'Medium',     # éœ€è¦è¬¹æ…å‰ªæ
                'distillation_potential': 'High',  # å¯ä»¥è’¸é¤¾åˆ°æ›´å°æ¨¡å‹
                'architecture_optimization': 'Medium'
            }
        }

    def calculate_hardware_requirements(self, analysis: dict) -> dict:
        """è¨ˆç®—ç¡¬é«”éœ€æ±‚"""

        # ç›®æ¨™GPUè¦æ ¼
        target_gpus = {
            'RTX_4090': {'memory_gb': 24, 'fp16_tflops': 166, 'price_usd': 1600},
            'A100_80GB': {'memory_gb': 80, 'fp16_tflops': 312, 'price_usd': 20000},
            'H100': {'memory_gb': 80, 'fp16_tflops': 534, 'price_usd': 30000}
        }

        memory_calc = MemoryCalculationFormulas()

        # è¨“ç·´ç¡¬é«”åŒ¹é…
        training_memory_gb = analysis['training_memory_analysis']['total_memory_gb']
        training_matching = memory_calc.gpu_matching_formula(
            training_memory_gb, target_gpus, safety_margin=0.2
        )

        # æ¨ç†ç¡¬é«”åŒ¹é…
        inference_memory_gb = analysis['inference_memory_analysis']['total_memory_gb']
        inference_matching = memory_calc.gpu_matching_formula(
            inference_memory_gb, target_gpus, safety_margin=0.1
        )

        return {
            'training_requirements': {
                'memory_required_gb': training_memory_gb,
                'gpu_matching': training_matching
            },
            'inference_requirements': {
                'memory_required_gb': inference_memory_gb,
                'gpu_matching': inference_matching
            },
            'cost_analysis': self._analyze_deployment_costs(training_matching, inference_matching),
            'recommendations': self._generate_hardware_recommendations(training_matching, inference_matching)
        }

    def select_compression_strategy(self, analysis: dict, hardware_matching: dict) -> dict:
        """é¸æ“‡å£“ç¸®ç­–ç•¥"""

        # åŸºæ–¼åˆ†æçµæœé¸æ“‡æœ€å„ªå£“ç¸®ç­–ç•¥
        model_params = analysis['model_config']['parameters']
        target_memory_gb = 16  # å‡è¨­ç›®æ¨™æ˜¯é©é…16GB GPU

        current_memory_gb = analysis['inference_memory_analysis']['total_memory_gb']
        required_compression_ratio = current_memory_gb / target_memory_gb

        strategy = {'compression_methods': []}

        if required_compression_ratio > 4:
            # éœ€è¦æ¥µè‡´å£“ç¸®
            strategy['compression_methods'] = [
                {'type': 'quantization', 'method': 'gptq', 'target_precision': 'int4'},
                {'type': 'pruning', 'method': 'structured', 'sparsity': 0.3},
                {'type': 'optimization', 'method': 'inference_engine'}
            ]
        elif required_compression_ratio > 2:
            # éœ€è¦é©ä¸­å£“ç¸®
            strategy['compression_methods'] = [
                {'type': 'quantization', 'method': 'awq', 'target_precision': 'int8'},
                {'type': 'optimization', 'method': 'inference_engine'}
            ]
        else:
            # è¼•åº¦å£“ç¸®
            strategy['compression_methods'] = [
                {'type': 'quantization', 'method': 'ptq', 'target_precision': 'fp16'},
                {'type': 'optimization', 'method': 'graph_optimization'}
            ]

        strategy['target_compression_ratio'] = required_compression_ratio
        strategy['target_memory_gb'] = target_memory_gb
        strategy['estimated_accuracy_impact'] = self._estimate_strategy_accuracy_impact(strategy)

        return strategy

    def _estimate_strategy_accuracy_impact(self, strategy: dict) -> float:
        """ä¼°ç®—ç­–ç•¥çš„æº–ç¢ºæ€§å½±éŸ¿"""

        total_impact = 0.0

        for method in strategy['compression_methods']:
            if method['type'] == 'quantization':
                if method['target_precision'] == 'int4':
                    total_impact += 3.0  # 4ä½é‡åŒ–ç´„3%æå¤±
                elif method['target_precision'] == 'int8':
                    total_impact += 1.5  # 8ä½é‡åŒ–ç´„1.5%æå¤±
                elif method['target_precision'] == 'fp16':
                    total_impact += 0.5  # FP16ç´„0.5%æå¤±

            elif method['type'] == 'pruning':
                sparsity = method.get('sparsity', 0)
                total_impact += sparsity * 5  # ç¨€ç–åº¦æ¯10%ç´„å¢åŠ 0.5%æå¤±

        return min(total_impact, 15.0)  # æœ€å¤§å½±éŸ¿ä¸è¶…é15%

    def execute_compression(self, strategy: dict) -> dict:
        """åŸ·è¡Œå£“ç¸®ç­–ç•¥"""

        compression_results = {}
        current_model_path = self.model_name

        for i, method in enumerate(strategy['compression_methods']):
            print(f"åŸ·è¡Œå£“ç¸®æ–¹æ³• {i+1}/{len(strategy['compression_methods'])}: {method['type']}")

            if method['type'] == 'quantization':
                result = self._execute_quantization_step(current_model_path, method)
                current_model_path = result.get('output_model_path', current_model_path)

            elif method['type'] == 'pruning':
                result = self._execute_pruning_step(current_model_path, method)
                current_model_path = result.get('output_model_path', current_model_path)

            elif method['type'] == 'optimization':
                result = self._execute_optimization_step(current_model_path, method)
                current_model_path = result.get('output_model_path', current_model_path)

            compression_results[f"{method['type']}_{i}"] = result

            # ä¸­é–“é©—è­‰
            validation = self._intermediate_validation(current_model_path, method)
            compression_results[f"{method['type']}_validation_{i}"] = validation

        return {
            'final_model_path': current_model_path,
            'compression_steps': compression_results,
            'overall_effect': self._calculate_overall_effect(compression_results)
        }

# ä½¿ç”¨ç¤ºä¾‹
def run_llama_compression_example():
    """é‹è¡ŒLLaMAå£“ç¸®ç¤ºä¾‹"""

    compressor = LLaMACompressionExample()

    # åŸ·è¡Œå®Œæ•´å£“ç¸®æµç¨‹
    pipeline_result = compressor.run_complete_compression_pipeline()

    # ç”Ÿæˆå ±å‘Š
    report = compressor.generate_compression_report(pipeline_result)

    # ä¿å­˜å ±å‘Š
    with open('llama_compression_report.md', 'w', encoding='utf-8') as f:
        f.write(report)

    print("\\n=== å£“ç¸®å®Œæˆ ===")
    print("è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ° llama_compression_report.md")

    return pipeline_result

if __name__ == "__main__":
    run_llama_compression_example()
```

## ğŸ¯ é—œéµå·¥ç¨‹åŒ–è€ƒé‡

### ç¡¬é«”è³‡æºåŒ¹é…æ±ºç­–è¡¨

| æ¨¡å‹è¦æ¨¡ | è¨“ç·´ç¡¬é«”æ¨è–¦ | æ¨ç†ç¡¬é«”æ¨è–¦ | é‡åŒ–ç­–ç•¥ | é æœŸæ•ˆæœ |
|----------|--------------|--------------|----------|----------|
| <1B | RTX 4090 Ã—1 | RTX 3080 Ã—1 | INT8/INT4 | 4-8xå£“ç¸® |
| 1B-7B | A100 80GB Ã—1-2 | RTX 4090 Ã—1 | FP16â†’INT8 | 2-4xå£“ç¸® |
| 7B-30B | A100 80GB Ã—2-4 | A100 40GB Ã—1-2 | AWQ/GPTQ | 2-4xå£“ç¸® |
| 30B-70B | A100 80GB Ã—4-8 | A100 80GB Ã—2-4 | GPTQ+åˆ†ç‰‡ | 4x+å£“ç¸® |
| >70B | H100 Ã—8+ | A100 80GB Ã—4+ | æ··åˆå£“ç¸® | è‡ªå®šç¾© |

### å£“ç¸®æµç¨‹æ±ºç­–æ¨¹

```mermaid
graph TD
    A[æ¨¡å‹è¼‰å…¥] --> B{è¨˜æ†¶é«”æ˜¯å¦è¶…é™?}
    B -->|æ˜¯| C[é‡åŒ–å£“ç¸®]
    B -->|å¦| D{æ€§èƒ½æ˜¯å¦æ»¿è¶³?}
    C --> E{å£“ç¸®å¾Œæ€§èƒ½OK?}
    E -->|æ˜¯| F[æ¨ç†å¼•æ“å„ªåŒ–]
    E -->|å¦| G[å‰ªæ+è’¸é¤¾]
    D -->|æ˜¯| F
    D -->|å¦| H[æ¨ç†å¼•æ“å„ªåŒ–]
    F --> I[éƒ¨ç½²æ¸¬è©¦]
    G --> I
    H --> I
    I --> J{æ¸¬è©¦é€šé?}
    J -->|æ˜¯| K[ç”Ÿç”¢éƒ¨ç½²]
    J -->|å¦| L[ç­–ç•¥èª¿æ•´]
    L --> C
```

### æ•ˆæœè©•ä¼°æ¨™æº–

```python
# å£“ç¸®æ•ˆæœè©•ä¼°æ¨™æº–
compression_standards = {
    "å„ªç§€": {
        "compression_ratio": "> 4x",
        "accuracy_loss": "< 2%",
        "inference_speedup": "> 3x",
        "memory_saving": "> 75%"
    },
    "è‰¯å¥½": {
        "compression_ratio": "2-4x",
        "accuracy_loss": "< 5%",
        "inference_speedup": "2-3x",
        "memory_saving": "50-75%"
    },
    "å¯æ¥å—": {
        "compression_ratio": "1.5-2x",
        "accuracy_loss": "< 8%",
        "inference_speedup": "1.5-2x",
        "memory_saving": "25-50%"
    },
    "éœ€æ”¹é€²": {
        "compression_ratio": "< 1.5x",
        "accuracy_loss": "> 8%",
        "inference_speedup": "< 1.5x",
        "memory_saving": "< 25%"
    }
}
```

## ğŸ“Š å¯¦ç”¨å·¥å…·å’Œæª¢æŸ¥æ¸…å–®

### å£“ç¸®å‰æª¢æŸ¥æ¸…å–® âœ…

- [ ] **æ¨¡å‹åˆ†æå®Œæˆ**
  - [ ] åƒæ•¸é‡åˆ†ä½ˆçµ±è¨ˆ
  - [ ] æ¬Šé‡å’Œæ¿€æ´»å€¼åˆ†ä½ˆåˆ†æ
  - [ ] æ€§èƒ½åŸºç·šå»ºç«‹
  - [ ] ç¡¬é«”éœ€æ±‚è©•ä¼°

- [ ] **å£“ç¸®ç­–ç•¥ç¢ºå®š**
  - [ ] ç›®æ¨™ç´„æŸæ˜ç¢º
  - [ ] å£“ç¸®æ–¹æ³•é¸æ“‡
  - [ ] é¢¨éšªè©•ä¼°å®Œæˆ
  - [ ] å›æ»¾æ–¹æ¡ˆæº–å‚™

- [ ] **ç’°å¢ƒæº–å‚™å°±ç·’**
  - [ ] é–‹ç™¼ç’°å¢ƒé…ç½®
  - [ ] æ¸¬è©¦æ•¸æ“šæº–å‚™
  - [ ] è©•ä¼°å·¥å…·å®‰è£
  - [ ] ç›£æ§ç³»çµ±éƒ¨ç½²

### å£“ç¸®å¾Œé©—è­‰æ¸…å–® âœ…

- [ ] **åŠŸèƒ½é©—è­‰**
  - [ ] åŸºæœ¬æ¨ç†åŠŸèƒ½
  - [ ] APIæ¥å£æ­£å¸¸
  - [ ] é‚Šç•Œæƒ…æ³è™•ç†
  - [ ] éŒ¯èª¤è™•ç†æ©Ÿåˆ¶

- [ ] **æ€§èƒ½é©—è­‰**
  - [ ] æ¨ç†é€Ÿåº¦æ¸¬è©¦
  - [ ] ååé‡æ¸¬è©¦
  - [ ] è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§
  - [ ] é•·æœŸç©©å®šæ€§æ¸¬è©¦

- [ ] **æº–ç¢ºæ€§é©—è­‰**
  - [ ] æ¨™æº–åŸºæº–æ¸¬è©¦
  - [ ] æ¥­å‹™å ´æ™¯æ¸¬è©¦
  - [ ] å°æ¯”åŸºç·šæ¨¡å‹
  - [ ] ç”¨æˆ¶æ¥å—åº¦æ¸¬è©¦

## ğŸ’¡ æœ€ä½³å¯¦è¸å»ºè­°

### å·¥ç¨‹åŒ–åŸå‰‡
1. **æ¼¸é€²å¼å£“ç¸®**: åˆ†éšæ®µå¯¦æ–½ï¼Œæ¯éšæ®µé©—è­‰æ•ˆæœ
2. **æ•¸æ“šé©…å‹•**: åŸºæ–¼å¯¦æ¸¬æ•¸æ“šåšæ±ºç­–ï¼Œä¸ä¾è³´ç†è«–å‡è¨­
3. **å¯å›æ»¾è¨­è¨ˆ**: æ¯æ­¥éƒ½è¦æœ‰å›æ»¾æ–¹æ¡ˆ
4. **å…¨é¢ç›£æ§**: å»ºç«‹å®Œæ•´çš„ç›£æ§å’Œå‘Šè­¦é«”ç³»
5. **æ–‡æª”å…ˆè¡Œ**: è©³ç´°è¨˜éŒ„æ¯å€‹æ±ºç­–å’Œå¯¦æ–½ç´°ç¯€

### å¸¸è¦‹é™·é˜±é¿å…
1. **éåº¦å„ªåŒ–**: ä¸è¦ç‚ºäº†æ¥µè‡´å£“ç¸®çŠ§ç‰²ç©©å®šæ€§
2. **å¿½è¦–å¯¦æ¸¬**: ç†è«–è¨ˆç®—éœ€è¦å¯¦éš›æ¸¬è©¦é©—è­‰
3. **å–®é»å„ªåŒ–**: è¦è€ƒæ…®æ•´é«”ç³»çµ±çš„å¹³è¡¡
4. **ç¼ºä¹ç›£æ§**: éƒ¨ç½²å¾Œè¦æŒçºŒç›£æ§æ•ˆæœ
5. **æ–‡æª”ç¼ºå¤±**: è¦ç‚ºå¾ŒçºŒç¶­è­·ç•™ä¸‹å®Œæ•´æ–‡æª”

---

é€™å€‹å·¥ç¨‹åŒ–æŒ‡å—æä¾›äº†å¾ç†è«–è¨ˆç®—åˆ°å¯¦éš›å¯¦æ–½çš„å®Œæ•´æ–¹æ¡ˆï¼Œå¹«åŠ©æ‚¨åœ¨å¯¦éš›é …ç›®ä¸­æˆåŠŸæ‡‰ç”¨LLMæ¨¡å‹å£“ç¸®æŠ€è¡“ï¼