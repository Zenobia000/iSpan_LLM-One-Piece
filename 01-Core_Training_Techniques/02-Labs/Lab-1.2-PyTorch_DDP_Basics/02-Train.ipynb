{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.2: PyTorch DDP åˆ†æ•£å¼è¨“ç·´åŸºç¤ - 02-Train\n",
    "## åˆ†æ•£å¼è¨“ç·´æµç¨‹å¯¦ä½œ\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ æ³¨æ„äº‹é …\n",
    "\n",
    "æœ¬notebookåœ¨**å–®GPUç’°å¢ƒ**ä¸­æ¼”ç¤ºDDPè¨“ç·´æµç¨‹çš„**æ¦‚å¿µå’Œä»£ç¢¼çµæ§‹**ã€‚\n",
    "- âœ… **å¯å­¸ç¿’**: å®Œæ•´çš„è¨“ç·´é‚è¼¯ã€æ¢¯åº¦åŒæ­¥æ©Ÿåˆ¶ã€æ€§èƒ½ç›£æ§\n",
    "- âš ï¸ **é™åˆ¶**: ç„¡æ³•å±•ç¤ºçœŸæ­£çš„å¤šGPUåŠ é€Ÿå’Œé€šè¨Šæ•ˆæœ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. å¯¦ä½œå®Œæ•´çš„DDPè¨“ç·´å¾ªç’°\n",
    "2. ç†è§£æ¢¯åº¦åŒæ­¥å’Œåƒæ•¸æ›´æ–°æ©Ÿåˆ¶\n",
    "3. å­¸ç¿’åˆ†æ•£å¼è¨“ç·´çš„æ—¥èªŒå’Œç›£æ§\n",
    "4. æŒæ¡æª¢æŸ¥é»ä¿å­˜å’Œè¼‰å…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥è¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è¼‰å…¥å‰ä¸€å€‹notebookçš„è¨­ç½®\n",
    "if os.path.exists('ddp_setup.pth'):\n",
    "    setup_data = torch.load('ddp_setup.pth', map_location='cpu')\n",
    "    config = setup_data['config']\n",
    "    device = torch.device(setup_data['device'])\n",
    "    print(\"âœ… æˆåŠŸè¼‰å…¥å‰ä¸€æ­¥çš„é…ç½®\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°setupé…ç½®ï¼Œè«‹å…ˆé‹è¡Œ 01-Setup.ipynb\")\n",
    "    # æä¾›é è¨­é…ç½®\n",
    "    config = {\n",
    "        'batch_size': 8, 'learning_rate': 5e-4, 'num_epochs': 3,\n",
    "        'warmup_steps': 100, 'weight_decay': 0.01,\n",
    "        'vocab_size': 8000, 'd_model': 256, 'nhead': 8, \n",
    "        'num_layers': 4, 'max_seq_len': 128\n",
    "    }\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"è¨­å‚™: {device}\")\n",
    "print(f\"æ‰¹æ¬¡å¤§å°: {config['batch_size']}\")\n",
    "print(f\"å­¸ç¿’ç‡: {config['learning_rate']}\")\n",
    "print(f\"è¨“ç·´è¼ªæ•¸: {config['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é‡æ–°å‰µå»ºæ¨¡å‹å’Œæ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°å®šç¾©æ¨¡å‹å’Œæ•¸æ“šé›†ï¼ˆèˆ‡01-Setupç›¸åŒï¼‰\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=8000, d_model=256, nhead=8, num_layers=4, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.embedding(input_ids) + self.pos_encoding(positions)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.bool()\n",
    "            attention_mask = ~attention_mask\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=attention_mask)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTextDataset(data.Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_len=128, vocab_size=8000):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.data = np.random.randint(1, vocab_size, (num_samples, seq_len))\n",
    "        self.attention_masks = np.ones((num_samples, seq_len))\n",
    "        for i in range(num_samples):\n",
    "            actual_len = np.random.randint(seq_len // 2, seq_len + 1)\n",
    "            self.attention_masks[i, actual_len:] = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.attention_masks[idx], dtype=torch.long)\n",
    "        labels = torch.cat([input_ids[1:], torch.tensor([0])], dim=0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹å’Œæ•¸æ“šé›†\n",
    "model = SimpleTransformer(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    nhead=config['nhead'],\n",
    "    num_layers=config['num_layers'],\n",
    "    max_seq_len=config['max_seq_len']\n",
    ").to(device)\n",
    "\n",
    "train_dataset = DummyTextDataset(num_samples=2000, seq_len=config['max_seq_len'])\n",
    "val_dataset = DummyTextDataset(num_samples=400, seq_len=config['max_seq_len'])\n",
    "\n",
    "print(f\"æ¨¡å‹åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"è¨“ç·´æ•¸æ“š: {len(train_dataset)} æ¨£æœ¬\")\n",
    "print(f\"é©—è­‰æ•¸æ“š: {len(val_dataset)} æ¨£æœ¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DDP è¨“ç·´å™¨é¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPTrainer:\n",
    "    \"\"\"\n",
    "    åˆ†æ•£å¼è¨“ç·´å™¨é¡åˆ¥\n",
    "    æ”¯æ´å–®GPUæ¼”ç¤ºå’ŒçœŸæ­£çš„å¤šGPUè¨“ç·´\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_dataset, val_dataset, config, rank=0, world_size=1):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.config = config\n",
    "        self.device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # æ¨¡å‹è¨­ç½®\n",
    "        self.model = model.to(self.device)\n",
    "        self.is_distributed = world_size > 1 and dist.is_initialized()\n",
    "        \n",
    "        if self.is_distributed:\n",
    "            self.ddp_model = DDP(self.model, device_ids=[rank])\n",
    "            print(f\"[Rank {rank}] ä½¿ç”¨DDPæ¨¡å‹\")\n",
    "        else:\n",
    "            self.ddp_model = self.model\n",
    "            print(\"ä½¿ç”¨å–®GPUæ¨¡å‹ï¼ˆéDDPï¼‰\")\n",
    "        \n",
    "        # æ•¸æ“šè¼‰å…¥å™¨\n",
    "        self.setup_dataloaders(train_dataset, val_dataset)\n",
    "        \n",
    "        # å„ªåŒ–å™¨å’Œèª¿åº¦å™¨\n",
    "        self.setup_optimizer_and_scheduler()\n",
    "        \n",
    "        # è¨“ç·´ç‹€æ…‹\n",
    "        self.global_step = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_history = {'loss': [], 'lr': [], 'step': []}\n",
    "        self.val_history = {'loss': [], 'step': []}\n",
    "        \n",
    "        # å‰µå»ºä¿å­˜ç›®éŒ„\n",
    "        self.save_dir = Path('checkpoints')\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def setup_dataloaders(self, train_dataset, val_dataset):\n",
    "        \"\"\"è¨­ç½®æ•¸æ“šè¼‰å…¥å™¨\"\"\"\n",
    "        # è¨“ç·´æ•¸æ“šè¼‰å…¥å™¨\n",
    "        if self.is_distributed:\n",
    "            train_sampler = DistributedSampler(\n",
    "                train_dataset, num_replicas=self.world_size, rank=self.rank, shuffle=True\n",
    "            )\n",
    "        else:\n",
    "            train_sampler = None\n",
    "        \n",
    "        self.train_loader = data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            sampler=train_sampler,\n",
    "            shuffle=(train_sampler is None),\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # é©—è­‰æ•¸æ“šè¼‰å…¥å™¨\n",
    "        if self.is_distributed:\n",
    "            val_sampler = DistributedSampler(\n",
    "                val_dataset, num_replicas=self.world_size, rank=self.rank, shuffle=False\n",
    "            )\n",
    "        else:\n",
    "            val_sampler = None\n",
    "        \n",
    "        self.val_loader = data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            sampler=val_sampler,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.train_sampler = train_sampler\n",
    "        self.val_sampler = val_sampler\n",
    "    \n",
    "    def setup_optimizer_and_scheduler(self):\n",
    "        \"\"\"è¨­ç½®å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.ddp_model.parameters(),\n",
    "            lr=self.config['learning_rate'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # è¨ˆç®—ç¸½æ­¥æ•¸\n",
    "        total_steps = self.config['num_epochs'] * len(self.train_loader)\n",
    "        warmup_steps = self.config['warmup_steps']\n",
    "        \n",
    "        # Warmup + Cosine èª¿åº¦\n",
    "        warmup_scheduler = LinearLR(\n",
    "            self.optimizer, start_factor=0.1, total_iters=warmup_steps\n",
    "        )\n",
    "        cosine_scheduler = CosineAnnealingLR(\n",
    "            self.optimizer, T_max=total_steps - warmup_steps\n",
    "        )\n",
    "        \n",
    "        self.scheduler = SequentialLR(\n",
    "            self.optimizer,\n",
    "            schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "            milestones=[warmup_steps]\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"å–®å€‹è¨“ç·´æ­¥é©Ÿ\"\"\"\n",
    "        self.ddp_model.train()\n",
    "        \n",
    "        # æº–å‚™æ•¸æ“š\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.device)\n",
    "        labels = batch['labels'].to(self.device)\n",
    "        \n",
    "        # å‰å‘å‚³æ’­\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.ddp_model(input_ids, attention_mask)\n",
    "        \n",
    "        # è¨ˆç®—æå¤±\n",
    "        # logits: [batch_size, seq_len, vocab_size]\n",
    "        # labels: [batch_size, seq_len]\n",
    "        loss = self.criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # åå‘å‚³æ’­\n",
    "        loss.backward()\n",
    "        \n",
    "        # æ¢¯åº¦è£å‰ª\n",
    "        if 'gradient_clipping' in self.config:\n",
    "            torch.nn.utils.clip_grad_norm_(self.ddp_model.parameters(), self.config['gradient_clipping'])\n",
    "        \n",
    "        # æ›´æ–°åƒæ•¸\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"é©—è­‰éšæ®µ\"\"\"\n",
    "        self.ddp_model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                logits = self.ddp_model(input_ids, attention_mask)\n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "        \n",
    "        # åœ¨åˆ†æ•£å¼ç’°å¢ƒä¸­èšåˆé©—è­‰æå¤±\n",
    "        if self.is_distributed:\n",
    "            avg_loss_tensor = torch.tensor(avg_loss, device=self.device)\n",
    "            dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)\n",
    "            avg_loss = avg_loss_tensor.item() / self.world_size\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"ä¿å­˜æª¢æŸ¥é»\"\"\"\n",
    "        if self.rank == 0:  # åªæœ‰rank 0ä¿å­˜æª¢æŸ¥é»\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': self.global_step,\n",
    "                'model_state_dict': self.ddp_model.module.state_dict() if hasattr(self.ddp_model, 'module') else self.ddp_model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'config': self.config,\n",
    "                'train_history': self.train_history,\n",
    "                'val_history': self.val_history,\n",
    "                'best_val_loss': self.best_val_loss\n",
    "            }\n",
    "            \n",
    "            # ä¿å­˜æœ€æ–°æª¢æŸ¥é»\n",
    "            torch.save(checkpoint, self.save_dir / 'latest_checkpoint.pth')\n",
    "            \n",
    "            # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if is_best:\n",
    "                torch.save(checkpoint, self.save_dir / 'best_model.pth')\n",
    "                print(f\"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (é©—è­‰æå¤±: {self.best_val_loss:.4f})\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"ä¸»è¦è¨“ç·´å¾ªç’°\"\"\"\n",
    "        print(f\"\\n=== é–‹å§‹DDPè¨“ç·´ ===\")\n",
    "        print(f\"Rank: {self.rank}/{self.world_size}\")\n",
    "        print(f\"è¨­å‚™: {self.device}\")\n",
    "        print(f\"æ˜¯å¦åˆ†æ•£å¼: {self.is_distributed}\")\n",
    "        print(f\"æ‰¹æ¬¡å¤§å°: {self.config['batch_size']}\")\n",
    "        print(f\"è¨“ç·´è¼ªæ•¸: {self.config['num_epochs']}\")\n",
    "        print(f\"ç¸½è¨“ç·´æ­¥æ•¸: {self.config['num_epochs'] * len(self.train_loader)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config['num_epochs']):\n",
    "            # ç‚ºåˆ†æ•£å¼æ¡æ¨£å™¨è¨­ç½®epoch\n",
    "            if self.train_sampler is not None:\n",
    "                self.train_sampler.set_epoch(epoch)\n",
    "            \n",
    "            # è¨“ç·´éšæ®µ\n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            if self.rank == 0:\n",
    "                pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config[\"num_epochs\"]}')\n",
    "            else:\n",
    "                pbar = self.train_loader\n",
    "            \n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                loss = self.train_step(batch)\n",
    "                epoch_loss += loss\n",
    "                num_batches += 1\n",
    "                self.global_step += 1\n",
    "                \n",
    "                # è¨˜éŒ„è¨“ç·´æ­·å²\n",
    "                if batch_idx % 10 == 0:  # æ¯10æ­¥è¨˜éŒ„ä¸€æ¬¡\n",
    "                    current_lr = self.scheduler.get_last_lr()[0]\n",
    "                    self.train_history['loss'].append(loss)\n",
    "                    self.train_history['lr'].append(current_lr)\n",
    "                    self.train_history['step'].append(self.global_step)\n",
    "                \n",
    "                # æ›´æ–°é€²åº¦æ¢\n",
    "                if self.rank == 0 and isinstance(pbar, tqdm):\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{loss:.4f}',\n",
    "                        'lr': f'{self.scheduler.get_last_lr()[0]:.2e}',\n",
    "                        'step': self.global_step\n",
    "                    })\n",
    "            \n",
    "            # è¨ˆç®—å¹³å‡è¨“ç·´æå¤±\n",
    "            avg_train_loss = epoch_loss / num_batches\n",
    "            \n",
    "            # é©—è­‰éšæ®µ\n",
    "            val_loss = self.validate()\n",
    "            self.val_history['loss'].append(val_loss)\n",
    "            self.val_history['step'].append(self.global_step)\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€ä½³æ¨¡å‹\n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "            \n",
    "            # ä¿å­˜æª¢æŸ¥é»\n",
    "            self.save_checkpoint(epoch, is_best)\n",
    "            \n",
    "            # åªæœ‰rank 0æ‰“å°æ—¥èªŒ\n",
    "            if self.rank == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"\\nEpoch {epoch+1}/{self.config['num_epochs']}:\")\n",
    "                print(f\"  è¨“ç·´æå¤±: {avg_train_loss:.4f}\")\n",
    "                print(f\"  é©—è­‰æå¤±: {val_loss:.4f} {'ğŸ“‰' if is_best else ''}\")\n",
    "                print(f\"  å­¸ç¿’ç‡: {self.scheduler.get_last_lr()[0]:.2e}\")\n",
    "                print(f\"  å·²ç”¨æ™‚é–“: {elapsed/60:.1f} åˆ†é˜\")\n",
    "        \n",
    "        if self.rank == 0:\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\nâœ… è¨“ç·´å®Œæˆï¼\")\n",
    "            print(f\"ç¸½ç”¨æ™‚: {total_time/60:.1f} åˆ†é˜\")\n",
    "            print(f\"æœ€ä½³é©—è­‰æå¤±: {self.best_val_loss:.4f}\")\n",
    "            print(f\"ç¸½è¨“ç·´æ­¥æ•¸: {self.global_step}\")\n",
    "\n",
    "print(\"âœ… DDPTrainer é¡åˆ¥å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å–®GPUè¨“ç·´æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨å–®GPUç’°å¢ƒä¸­é‹è¡ŒDDPè¨“ç·´å™¨\n",
    "print(\"=== å–®GPUç’°å¢ƒä¸‹çš„DDPè¨“ç·´æ¼”ç¤º ===\")\n",
    "print(\"æ³¨æ„: é€™å±•ç¤ºäº†DDPçš„å®Œæ•´è¨“ç·´é‚è¼¯ï¼Œä½†æ²’æœ‰å¤šGPUåŠ é€Ÿ\")\n",
    "print()\n",
    "\n",
    "# å‰µå»ºè¨“ç·´å™¨\n",
    "trainer = DDPTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    config=config,\n",
    "    rank=0,\n",
    "    world_size=1\n",
    ")\n",
    "\n",
    "print(f\"è¨“ç·´å™¨è¨­ç½®å®Œæˆ:\")\n",
    "print(f\"  è¨“ç·´æ‰¹æ¬¡æ•¸: {len(trainer.train_loader)}\")\n",
    "print(f\"  é©—è­‰æ‰¹æ¬¡æ•¸: {len(trainer.val_loader)}\")\n",
    "print(f\"  å„ªåŒ–å™¨: {type(trainer.optimizer).__name__}\")\n",
    "print(f\"  èª¿åº¦å™¨: {type(trainer.scheduler).__name__}\")\n",
    "\n",
    "# æ¸¬è©¦ä¸€å€‹è¨“ç·´æ­¥é©Ÿ\n",
    "print(\"\\n=== æ¸¬è©¦å–®å€‹è¨“ç·´æ­¥é©Ÿ ===\")\n",
    "sample_batch = next(iter(trainer.train_loader))\n",
    "initial_loss = trainer.train_step(sample_batch)\n",
    "print(f\"åˆå§‹æå¤±: {initial_loss:.4f}\")\n",
    "print(f\"ç•¶å‰å­¸ç¿’ç‡: {trainer.scheduler.get_last_lr()[0]:.2e}\")\n",
    "print(f\"å…¨å±€æ­¥æ•¸: {trainer.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åŸ·è¡Œå®Œæ•´è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é–‹å§‹å®Œæ•´è¨“ç·´\n",
    "print(\"ğŸš€ é–‹å§‹å®Œæ•´è¨“ç·´...\")\n",
    "print(\"æ³¨æ„: é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜æ™‚é–“\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ï¸ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è¨“ç·´çµæœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½è¨“ç·´æ›²ç·š\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# è¨“ç·´æå¤±æ›²ç·š\n",
    "plt.subplot(1, 3, 1)\n",
    "if trainer.train_history['loss']:\n",
    "    plt.plot(trainer.train_history['step'], trainer.train_history['loss'], label='è¨“ç·´æå¤±', alpha=0.7)\n",
    "    # å¹³æ»‘æ›²ç·š\n",
    "    if len(trainer.train_history['loss']) > 10:\n",
    "        from scipy.ndimage import uniform_filter1d\n",
    "        smoothed = uniform_filter1d(trainer.train_history['loss'], size=10)\n",
    "        plt.plot(trainer.train_history['step'], smoothed, label='å¹³æ»‘è¨“ç·´æå¤±', linewidth=2)\n",
    "plt.xlabel('è¨“ç·´æ­¥æ•¸')\n",
    "plt.ylabel('æå¤±')\n",
    "plt.title('è¨“ç·´æå¤±æ›²ç·š')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# é©—è­‰æå¤±æ›²ç·š\n",
    "plt.subplot(1, 3, 2)\n",
    "if trainer.val_history['loss']:\n",
    "    plt.plot(trainer.val_history['step'], trainer.val_history['loss'], 'o-', label='é©—è­‰æå¤±', color='orange')\n",
    "    plt.axhline(y=trainer.best_val_loss, color='red', linestyle='--', alpha=0.7, label=f'æœ€ä½³: {trainer.best_val_loss:.4f}')\n",
    "plt.xlabel('è¨“ç·´æ­¥æ•¸')\n",
    "plt.ylabel('æå¤±')\n",
    "plt.title('é©—è­‰æå¤±æ›²ç·š')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# å­¸ç¿’ç‡æ›²ç·š\n",
    "plt.subplot(1, 3, 3)\n",
    "if trainer.train_history['lr']:\n",
    "    plt.plot(trainer.train_history['step'], trainer.train_history['lr'], label='å­¸ç¿’ç‡', color='green')\n",
    "plt.xlabel('è¨“ç·´æ­¥æ•¸')\n",
    "plt.ylabel('å­¸ç¿’ç‡')\n",
    "plt.title('å­¸ç¿’ç‡èª¿åº¦')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# è¨“ç·´çµ±è¨ˆ\n",
    "print(\"\\n=== è¨“ç·´çµ±è¨ˆ ===\")\n",
    "print(f\"ç¸½è¨“ç·´æ­¥æ•¸: {trainer.global_step}\")\n",
    "print(f\"æœ€ä½³é©—è­‰æå¤±: {trainer.best_val_loss:.4f}\")\n",
    "if trainer.train_history['loss']:\n",
    "    print(f\"æœ€çµ‚è¨“ç·´æå¤±: {trainer.train_history['loss'][-1]:.4f}\")\n",
    "    print(f\"åˆå§‹è¨“ç·´æå¤±: {trainer.train_history['loss'][0]:.4f}\")\n",
    "    print(f\"æå¤±æ”¹å–„: {trainer.train_history['loss'][0] - trainer.train_history['loss'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\næª¢æŸ¥é»å·²ä¿å­˜åˆ°: {trainer.save_dir}\")\n",
    "if (trainer.save_dir / 'best_model.pth').exists():\n",
    "    print(\"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜\")\n",
    "if (trainer.save_dir / 'latest_checkpoint.pth').exists():\n",
    "    print(\"âœ… æœ€æ–°æª¢æŸ¥é»å·²ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¤šGPUè¨“ç·´ä»£ç¢¼ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå®Œæ•´çš„å¤šGPUè¨“ç·´è…³æœ¬\n",
    "multi_gpu_train_script = '''\n",
    "#!/usr/bin/env python3\n",
    "# multi_gpu_train.py - å®Œæ•´çš„å¤šGPU DDPè¨“ç·´è…³æœ¬\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"åˆå§‹åŒ–åˆ†æ•£å¼ç’°å¢ƒ\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # åˆå§‹åŒ–é€²ç¨‹çµ„\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    print(f\"[Rank {rank}] é€²ç¨‹çµ„åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"æ¸…ç†åˆ†æ•£å¼ç’°å¢ƒ\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def main_worker(rank, world_size, args):\n",
    "    \"\"\"ä¸»è¦çš„å·¥ä½œé€²ç¨‹å‡½æ•¸\"\"\"\n",
    "    try:\n",
    "        # è¨­ç½®åˆ†æ•£å¼ç’°å¢ƒ\n",
    "        setup(rank, world_size)\n",
    "        \n",
    "        # å‰µå»ºæ¨¡å‹ã€æ•¸æ“šé›†ã€è¨“ç·´å™¨ç­‰\n",
    "        # (ä½¿ç”¨æœ¬notebookä¸­å®šç¾©çš„é¡åˆ¥)\n",
    "        model = SimpleTransformer().to(rank)\n",
    "        train_dataset = DummyTextDataset(num_samples=args.num_samples)\n",
    "        val_dataset = DummyTextDataset(num_samples=args.num_samples // 5)\n",
    "        \n",
    "        config = {\n",
    "            'batch_size': args.batch_size,\n",
    "            'learning_rate': args.lr,\n",
    "            'num_epochs': args.epochs,\n",
    "            'warmup_steps': 100,\n",
    "            'weight_decay': 0.01,\n",
    "            'gradient_clipping': 1.0\n",
    "        }\n",
    "        \n",
    "        # å‰µå»ºDDPè¨“ç·´å™¨\n",
    "        trainer = DDPTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            config=config,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "        \n",
    "        # é–‹å§‹è¨“ç·´\n",
    "        trainer.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Rank {rank}] éŒ¯èª¤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        cleanup()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='å¤šGPU DDPè¨“ç·´')\n",
    "    parser.add_argument('--epochs', type=int, default=5, help='è¨“ç·´è¼ªæ•¸')\n",
    "    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')\n",
    "    parser.add_argument('--lr', type=float, default=5e-4, help='å­¸ç¿’ç‡')\n",
    "    parser.add_argument('--num-samples', type=int, default=2000, help='è¨“ç·´æ¨£æœ¬æ•¸')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # æª¢æŸ¥GPUæ•¸é‡\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"éŒ¯èª¤: æœªæª¢æ¸¬åˆ°CUDAæ”¯æŒ\")\n",
    "        return\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size < 2:\n",
    "        print(f\"è­¦å‘Š: æª¢æ¸¬åˆ° {world_size} å€‹GPUï¼Œå»ºè­°è‡³å°‘2å€‹GPUé€²è¡ŒDDPè¨“ç·´\")\n",
    "    \n",
    "    print(f\"é–‹å§‹ {world_size} GPU DDPè¨“ç·´\")\n",
    "    print(f\"é…ç½®: epochs={args.epochs}, batch_size={args.batch_size}, lr={args.lr}\")\n",
    "    \n",
    "    # å•Ÿå‹•å¤šé€²ç¨‹è¨“ç·´\n",
    "    mp.spawn(\n",
    "        main_worker,\n",
    "        args=(world_size, args),\n",
    "        nprocs=world_size,\n",
    "        join=True\n",
    "    )\n",
    "    \n",
    "    print(\"è¨“ç·´å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# ä¿å­˜è…³æœ¬\n",
    "script_path = Path('multi_gpu_train.py')\n",
    "with open(script_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(multi_gpu_train_script)\n",
    "\n",
    "print(f\"âœ… å¤šGPUè¨“ç·´è…³æœ¬å·²ä¿å­˜åˆ°: {script_path}\")\n",
    "print()\n",
    "print(\"=== ä½¿ç”¨æ–¹æ³• ===\")\n",
    "print(\"# ä½¿ç”¨ torchrun (æ¨è–¦)\")\n",
    "print(\"torchrun --nproc_per_node=4 multi_gpu_train.py --epochs 10 --batch-size 16\")\n",
    "print()\n",
    "print(\"# ä½¿ç”¨ mp.spawn\")\n",
    "print(\"python multi_gpu_train.py --epochs 10 --batch-size 16\")\n",
    "print()\n",
    "print(\"# å¤šç¯€é»è¨“ç·´\")\n",
    "print(\"# ç¯€é» 0 (master):\")\n",
    "print(\"torchrun --nnodes=2 --nproc_per_node=4 --node_rank=0 \\\\\")\n",
    "print(\"         --master_addr=192.168.1.100 --master_port=29500 \\\\\")\n",
    "print(\"         multi_gpu_train.py\")\n",
    "print()\n",
    "print(\"# ç¯€é» 1 (worker):\")\n",
    "print(\"torchrun --nnodes=2 --nproc_per_node=4 --node_rank=1 \\\\\")\n",
    "print(\"         --master_addr=192.168.1.100 --master_port=29500 \\\\\")\n",
    "print(\"         multi_gpu_train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DDP æ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP æ€§èƒ½ç‰¹æ€§åˆ†æ\n",
    "print(\"=== DDP æ€§èƒ½ç‰¹æ€§åˆ†æ ===\")\n",
    "print()\n",
    "\n",
    "# ç†è«–æ€§èƒ½è¨ˆç®—\n",
    "model_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_mb = model_params * 4 / (1024 * 1024)  # FP32\n",
    "gradient_size_mb = model_size_mb  # æ¢¯åº¦å¤§å°ç´„ç­‰æ–¼åƒæ•¸å¤§å°\n",
    "\n",
    "print(f\"æ¨¡å‹åˆ†æ:\")\n",
    "print(f\"  åƒæ•¸é‡: {model_params:,}\")\n",
    "print(f\"  æ¨¡å‹å¤§å°: {model_size_mb:.1f} MB (FP32)\")\n",
    "print(f\"  æ¢¯åº¦å¤§å°: {gradient_size_mb:.1f} MB\")\n",
    "print()\n",
    "\n",
    "# å¤šGPUæ€§èƒ½é æœŸ\n",
    "print(f\"å¤šGPUæ€§èƒ½é æœŸ (ç†è«–å€¼):\")\n",
    "gpu_counts = [1, 2, 4, 8]\n",
    "for gpu_count in gpu_counts:\n",
    "    # ç†è«–è¨ˆç®—æ™‚é–“\n",
    "    compute_time = 1.0  # åŸºæº–è¨ˆç®—æ™‚é–“\n",
    "    parallel_compute_time = compute_time / gpu_count  # å®Œç¾ä¸¦è¡Œ\n",
    "    \n",
    "    # é€šè¨Šæ™‚é–“ (All-Reduce)\n",
    "    communication_time = gradient_size_mb * 0.001 * (gpu_count - 1) / gpu_count  # ç°¡åŒ–ä¼°ç®—\n",
    "    \n",
    "    total_time = parallel_compute_time + communication_time\n",
    "    speedup = 1.0 / total_time\n",
    "    efficiency = speedup / gpu_count * 100\n",
    "    \n",
    "    print(f\"  {gpu_count} GPU: åŠ é€Ÿæ¯” {speedup:.2f}x, æ•ˆç‡ {efficiency:.1f}%\")\n",
    "\n",
    "print()\n",
    "print(f\"DDP å„ªåŒ–ç‰¹æ€§:\")\n",
    "print(f\"  âœ… æ¢¯åº¦è‡ªå‹•åŒæ­¥ (All-Reduce)\")\n",
    "print(f\"  âœ… é€šè¨Šèˆ‡è¨ˆç®—é‡ç–Š\")\n",
    "print(f\"  âœ… æ¢¯åº¦å£“ç¸® (å¯é¸)\")\n",
    "print(f\"  âœ… éŒ¯èª¤æª¢æ¸¬å’Œæ¢å¾©\")\n",
    "print(f\"  âœ… å‹•æ…‹æ‰¹æ¬¡å¤§å°èª¿æ•´\")\n",
    "print()\n",
    "\n",
    "# è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ\n",
    "print(f\"è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ (å–®GPU vs å¤šGPU):\")\n",
    "base_memory = model_size_mb * 2  # æ¨¡å‹ + æ¢¯åº¦\n",
    "optimizer_memory = model_size_mb * 2  # Adamç‹€æ…‹\n",
    "activation_memory = config['batch_size'] * config['max_seq_len'] * config['d_model'] * 4 / (1024 * 1024)\n",
    "\n",
    "print(f\"  å–®GPUè¨˜æ†¶é«”éœ€æ±‚:\")\n",
    "print(f\"    æ¨¡å‹ + æ¢¯åº¦: {base_memory:.1f} MB\")\n",
    "print(f\"    å„ªåŒ–å™¨ç‹€æ…‹: {optimizer_memory:.1f} MB\")\n",
    "print(f\"    æ¿€æ´»å€¼: {activation_memory:.1f} MB\")\n",
    "print(f\"    ç¸½è¨ˆ: {base_memory + optimizer_memory + activation_memory:.1f} MB\")\n",
    "print()\n",
    "print(f\"  å¤šGPUè¨˜æ†¶é«”å„ªå‹¢:\")\n",
    "print(f\"    - æ¿€æ´»å€¼åˆ†æ•£åˆ°å„GPU\")\n",
    "print(f\"    - å¯æ”¯æŒæ›´å¤§çš„å…¨å±€æ‰¹æ¬¡å¤§å°\")\n",
    "print(f\"    - åƒæ•¸å’Œæ¢¯åº¦åœ¨éœ€è¦æ™‚åŒæ­¥\")\n",
    "\n",
    "# é€šè¨Šæ¨¡å¼åˆ†æ\n",
    "print(f\"\\n=== DDP é€šè¨Šæ¨¡å¼ ===\")\n",
    "print(f\"1. All-Reduce æ¨¡å¼:\")\n",
    "print(f\"   - æ¯å€‹GPUè¨ˆç®—æœ¬åœ°æ¢¯åº¦\")\n",
    "print(f\"   - ä½¿ç”¨All-Reduceèšåˆæ‰€æœ‰æ¢¯åº¦\")\n",
    "print(f\"   - æ‰€æœ‰GPUç²å¾—ç›¸åŒçš„å¹³å‡æ¢¯åº¦\")\n",
    "print(f\"   - åŒæ­¥æ›´æ–°æ¨¡å‹åƒæ•¸\")\n",
    "print()\n",
    "print(f\"2. é€šè¨Šæ‹“æ’²:\")\n",
    "print(f\"   - Ring All-Reduce: O(N) æ­¥é©Ÿ, å¸¶å¯¬åˆ©ç”¨ç‡é«˜\")\n",
    "print(f\"   - Tree All-Reduce: O(log N) æ­¥é©Ÿ, é©åˆå¤§è¦æ¨¡\")\n",
    "print(f\"   - NCCLè‡ªå‹•é¸æ“‡æœ€å„ªæ‹“æ’²\")\n",
    "print()\n",
    "print(f\"3. å„ªåŒ–ç­–ç•¥:\")\n",
    "print(f\"   - æ¢¯åº¦ç´¯ç©: æ¸›å°‘é€šè¨Šé »ç‡\")\n",
    "print(f\"   - æ¢¯åº¦å£“ç¸®: æ¸›å°‘é€šè¨Šé‡\")\n",
    "print(f\"   - è¨ˆç®—é€šè¨Šé‡ç–Š: éš±è—é€šè¨Šå»¶é²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç¸½çµèˆ‡ä¸‹ä¸€æ­¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Lab-1.2 Train å®Œæˆç¸½çµ ===\")\n",
    "print()\n",
    "print(\"âœ… å·²å®Œæˆ:\")\n",
    "print(\"  1. âœ… å¯¦ä½œå®Œæ•´çš„DDPè¨“ç·´å™¨é¡åˆ¥\")\n",
    "print(\"  2. âœ… æ¼”ç¤ºå–®GPUç’°å¢ƒä¸‹çš„DDPè¨“ç·´æµç¨‹\")\n",
    "print(\"  3. âœ… å¯¦ç¾æ¢¯åº¦åŒæ­¥å’Œåƒæ•¸æ›´æ–°æ©Ÿåˆ¶\")\n",
    "print(\"  4. âœ… è¨“ç·´æ­·å²è¨˜éŒ„å’Œå¯è¦–åŒ–\")\n",
    "print(\"  5. âœ… æª¢æŸ¥é»ä¿å­˜å’Œè¼‰å…¥åŠŸèƒ½\")\n",
    "print(\"  6. âœ… å¤šGPUè¨“ç·´è…³æœ¬ç”Ÿæˆ\")\n",
    "print(\"  7. âœ… DDPæ€§èƒ½ç‰¹æ€§åˆ†æ\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ¯ é—œéµå­¸ç¿’æˆæœ:\")\n",
    "print(\"  - ç†è§£DDPçš„å®Œæ•´è¨“ç·´æµç¨‹\")\n",
    "print(\"  - æŒæ¡åˆ†æ•£å¼æ•¸æ“šè¼‰å…¥å’Œæ¡æ¨£\")\n",
    "print(\"  - å­¸æœƒæ¢¯åº¦åŒæ­¥å’Œé€šè¨Šå„ªåŒ–\")\n",
    "print(\"  - ç†Ÿæ‚‰å¤šGPUè¨“ç·´çš„é…ç½®æ–¹æ³•\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "print(\"  - checkpoints/: è¨“ç·´æª¢æŸ¥é»ç›®éŒ„\")\n",
    "print(\"  - training_curves.png: è¨“ç·´æ›²ç·šåœ–\")\n",
    "print(\"  - multi_gpu_train.py: å¤šGPUè¨“ç·´è…³æœ¬\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ ä¸‹ä¸€æ­¥å»ºè­°:\")\n",
    "print(\"  - 03-Optimization.ipynb: é€šè¨Šå„ªåŒ–å’Œæ€§èƒ½èª¿å„ª\")\n",
    "print(\"  - 04-Advanced.ipynb: é€²éšæŠ€è¡“å’Œæ•…éšœè™•ç†\")\n",
    "print(\"  - åœ¨å¤šGPUç’°å¢ƒä¸­æ¸¬è©¦ç”Ÿæˆçš„è¨“ç·´è…³æœ¬\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”§ å¤šGPUç’°å¢ƒä½¿ç”¨æŒ‡å—:\")\n",
    "print(\"  1. æª¢æŸ¥GPUæ•¸é‡: nvidia-smi\")\n",
    "print(\"  2. é©—è­‰NCCL: python -c 'import torch; print(torch.distributed.is_nccl_available())'\")\n",
    "print(\"  3. é‹è¡Œè¨“ç·´: torchrun --nproc_per_node=N multi_gpu_train.py\")\n",
    "print(\"  4. ç›£æ§è¨“ç·´: watch -n 1 nvidia-smi\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’¡ é‡è¦æ¦‚å¿µå›é¡§:\")\n",
    "print(\"  - DDP = æ•¸æ“šä¸¦è¡Œ + åƒæ•¸åŒæ­¥\")\n",
    "print(\"  - All-Reduce = é«˜æ•ˆçš„æ¢¯åº¦èšåˆç®—æ³•\")\n",
    "print(\"  - DistributedSampler = ç¢ºä¿æ•¸æ“šä¸é‡è¤‡\")\n",
    "print(\"  - Rank 0 = ä¸»é€²ç¨‹è² è²¬æ—¥èªŒå’Œæª¢æŸ¥é»\")\n",
    "print(\"  - é€šè¨Šå¾Œç«¯: NCCL (GPU) vs Gloo (CPU)\")\n",
    "\n",
    "# ä¿å­˜è¨“ç·´çµæœæ‘˜è¦\n",
    "summary = {\n",
    "    'training_completed': True,\n",
    "    'final_train_loss': trainer.train_history['loss'][-1] if trainer.train_history['loss'] else None,\n",
    "    'best_val_loss': trainer.best_val_loss,\n",
    "    'total_steps': trainer.global_step,\n",
    "    'model_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'config': config\n",
    "}\n",
    "\n",
    "with open('training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nğŸ’¾ è¨“ç·´æ‘˜è¦å·²ä¿å­˜åˆ° training_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}