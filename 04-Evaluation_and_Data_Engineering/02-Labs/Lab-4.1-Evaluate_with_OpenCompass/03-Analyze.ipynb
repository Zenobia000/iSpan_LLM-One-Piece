{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1 - OpenCompass è©•ä¼°å¯¦æˆ°\n",
    "## Notebook 03: çµæœåˆ†æ\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "1. è¼‰å…¥è©•ä¼°çµæœä¸¦é€²è¡Œçµ±è¨ˆåˆ†æ\n",
    "2. éŒ¯èª¤åˆ†æ (æ‰¾å‡ºå¸¸è¦‹éŒ¯èª¤æ¨¡å¼)\n",
    "3. ä¿¡å¿ƒåº¦åˆ†æ (æª¢æŸ¥ logits åˆ†ä½ˆ)\n",
    "4. å­¸ç§‘é›£åº¦æ’å\n",
    "5. æ¨¡å‹å¼·å¼±é …è­˜åˆ¥\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 30-45 åˆ†é˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥è©•ä¼°çµæœ\n",
    "\n",
    "å¾ 02-Evaluate.ipynb ç”¢ç”Ÿçš„ JSON æª”æ¡ˆä¸­è¼‰å…¥çµæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®šè·¯å¾‘\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "ANALYSIS_DIR = Path(\"./analysis\")\n",
    "ANALYSIS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“‚ è¼‰å…¥è©•ä¼°çµæœ...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è¼‰å…¥çµæœæª”æ¡ˆ\n",
    "results = {}\n",
    "result_files = {\n",
    "    'llama-2-7b': RESULTS_DIR / \"llama2_7b_results.json\",\n",
    "    'qwen-7b': RESULTS_DIR / \"qwen_7b_results.json\"\n",
    "}\n",
    "\n",
    "for model_key, result_file in result_files.items():\n",
    "    if result_file.exists():\n",
    "        with open(result_file, 'r', encoding='utf-8') as f:\n",
    "            results[model_key] = json.load(f)\n",
    "        print(f\"âœ… {model_key}: {result_file.name}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  {model_key}: æª”æ¡ˆä¸å­˜åœ¨ ({result_file})\")\n",
    "\n",
    "if not results:\n",
    "    raise FileNotFoundError(\"æœªæ‰¾åˆ°ä»»ä½•è©•ä¼°çµæœ,è«‹å…ˆåŸ·è¡Œ 02-Evaluate.ipynb\")\n",
    "\n",
    "print(\"\\nâœ… çµæœè¼‰å…¥å®Œæˆ\")\n",
    "print(f\"å…±è¼‰å…¥ {len(results)} å€‹æ¨¡å‹çš„çµæœ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•´é«”çµ±è¨ˆåˆ†æ\n",
    "\n",
    "è¨ˆç®—å„æ¨¡å‹çš„æ•´é«”è¡¨ç¾çµ±è¨ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_statistics(results: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ•´é«”çµ±è¨ˆè³‡è¨Š\n",
    "    \n",
    "    Args:\n",
    "        results: è©•ä¼°çµæœå­—å…¸\n",
    "    \n",
    "    Returns:\n",
    "        çµ±è¨ˆè³‡è¨Š DataFrame\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        # è¨ˆç®—ç¸½é¡Œæ•¸\n",
    "        total_questions = sum(sr['total'] for sr in result['subject_results'])\n",
    "        total_correct = sum(sr['correct'] for sr in result['subject_results'])\n",
    "        \n",
    "        stats.append({\n",
    "            'æ¨¡å‹': result['model_name'],\n",
    "            'æ•´é«”æº–ç¢ºç‡': f\"{result['overall_accuracy']:.2%}\",\n",
    "            'æ­£ç¢ºé¡Œæ•¸': total_correct,\n",
    "            'ç¸½é¡Œæ•¸': total_questions,\n",
    "            'éŒ¯èª¤é¡Œæ•¸': total_questions - total_correct,\n",
    "            'è©•ä¼°å­¸ç§‘æ•¸': len(result['subject_results']),\n",
    "        })\n",
    "        \n",
    "        # æ·»åŠ åˆ†é¡æº–ç¢ºç‡\n",
    "        for category, accuracy in result['category_accuracies'].items():\n",
    "            stats[-1][f'{category}æº–ç¢ºç‡'] = f\"{accuracy:.2%}\"\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "# è¨ˆç®—çµ±è¨ˆè³‡è¨Š\n",
    "stats_df = calculate_overall_statistics(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"æ•´é«”çµ±è¨ˆåˆ†æ\")\n",
    "print(\"=\" * 60)\n",
    "print(stats_df.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ä¿å­˜çµ±è¨ˆçµæœ\n",
    "stats_df.to_csv(ANALYSIS_DIR / \"overall_statistics.csv\", index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… çµ±è¨ˆçµæœå·²ä¿å­˜: {ANALYSIS_DIR / 'overall_statistics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å­¸ç§‘é›£åº¦åˆ†æ\n",
    "\n",
    "åˆ†ææ¯å€‹å­¸ç§‘çš„é›£åº¦ (åŸºæ–¼æ‰€æœ‰æ¨¡å‹çš„å¹³å‡æº–ç¢ºç‡)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_subject_difficulty(results: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åˆ†æå­¸ç§‘é›£åº¦\n",
    "    \n",
    "    Args:\n",
    "        results: è©•ä¼°çµæœå­—å…¸\n",
    "    \n",
    "    Returns:\n",
    "        å­¸ç§‘é›£åº¦ DataFrame (æŒ‰é›£åº¦é™åºæ’åˆ—)\n",
    "    \"\"\"\n",
    "    subject_stats = defaultdict(lambda: {'accuracies': [], 'total_questions': 0})\n",
    "    \n",
    "    # æ”¶é›†æ¯å€‹å­¸ç§‘çš„æº–ç¢ºç‡\n",
    "    for model_key, result in results.items():\n",
    "        for subject_result in result['subject_results']:\n",
    "            subject = subject_result['subject']\n",
    "            subject_stats[subject]['accuracies'].append(subject_result['accuracy'])\n",
    "            subject_stats[subject]['total_questions'] = subject_result['total']\n",
    "    \n",
    "    # è¨ˆç®—å¹³å‡æº–ç¢ºç‡å’Œé›£åº¦\n",
    "    difficulty_data = []\n",
    "    for subject, stats in subject_stats.items():\n",
    "        avg_accuracy = np.mean(stats['accuracies'])\n",
    "        std_accuracy = np.std(stats['accuracies'])\n",
    "        \n",
    "        # é›£åº¦å®šç¾©: 1 - æº–ç¢ºç‡ (è¶Šé«˜è¶Šé›£)\n",
    "        difficulty = 1 - avg_accuracy\n",
    "        \n",
    "        difficulty_data.append({\n",
    "            'å­¸ç§‘': subject,\n",
    "            'é¡Œæ•¸': stats['total_questions'],\n",
    "            'å¹³å‡æº–ç¢ºç‡': avg_accuracy,\n",
    "            'æº–ç¢ºç‡æ¨™æº–å·®': std_accuracy,\n",
    "            'é›£åº¦åˆ†æ•¸': difficulty,\n",
    "            'é›£åº¦ç­‰ç´š': 'å›°é›£' if difficulty > 0.6 else 'ä¸­ç­‰' if difficulty > 0.4 else 'ç°¡å–®'\n",
    "        })\n",
    "    \n",
    "    # æŒ‰é›£åº¦æ’åº\n",
    "    df = pd.DataFrame(difficulty_data)\n",
    "    df = df.sort_values('é›£åº¦åˆ†æ•¸', ascending=False)\n",
    "    \n",
    "    # æ ¼å¼åŒ–ç™¾åˆ†æ¯”\n",
    "    df['å¹³å‡æº–ç¢ºç‡'] = df['å¹³å‡æº–ç¢ºç‡'].apply(lambda x: f\"{x:.2%}\")\n",
    "    df['æº–ç¢ºç‡æ¨™æº–å·®'] = df['æº–ç¢ºç‡æ¨™æº–å·®'].apply(lambda x: f\"{x:.2%}\")\n",
    "    df['é›£åº¦åˆ†æ•¸'] = df['é›£åº¦åˆ†æ•¸'].apply(lambda x: f\"{x:.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# åˆ†æå­¸ç§‘é›£åº¦\n",
    "difficulty_df = analyze_subject_difficulty(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"å­¸ç§‘é›£åº¦æ’å (ç”±é›£åˆ°æ˜“)\")\n",
    "print(\"=\" * 80)\n",
    "print(difficulty_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ä¿å­˜çµæœ\n",
    "difficulty_df.to_csv(ANALYSIS_DIR / \"subject_difficulty.csv\", index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… é›£åº¦åˆ†æå·²ä¿å­˜: {ANALYSIS_DIR / 'subject_difficulty.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. éŒ¯èª¤åˆ†æ\n",
    "\n",
    "æ·±å…¥åˆ†ææ¨¡å‹çš„éŒ¯èª¤æ¨¡å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(results: dict) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    åˆ†æéŒ¯èª¤æ¨¡å¼\n",
    "    \n",
    "    Args:\n",
    "        results: è©•ä¼°çµæœå­—å…¸\n",
    "    \n",
    "    Returns:\n",
    "        éŒ¯èª¤åˆ†æçµæœå­—å…¸\n",
    "    \"\"\"\n",
    "    error_analysis = {}\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        model_name = result['model_name']\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰éŒ¯èª¤\n",
    "        errors = []\n",
    "        \n",
    "        for subject_result in result['subject_results']:\n",
    "            subject = subject_result['subject']\n",
    "            \n",
    "            for detail in subject_result['details']:\n",
    "                if not detail['is_correct']:\n",
    "                    errors.append({\n",
    "                        'å­¸ç§‘': subject,\n",
    "                        'å•é¡Œ': detail['question'][:50] + '...' if len(detail['question']) > 50 else detail['question'],\n",
    "                        'æ­£ç¢ºç­”æ¡ˆ': detail['correct_answer'],\n",
    "                        'é æ¸¬ç­”æ¡ˆ': detail['predicted_answer'],\n",
    "                        'éŒ¯èª¤é¡å‹': f\"{detail['correct_answer']}â†’{detail['predicted_answer']}\"\n",
    "                    })\n",
    "        \n",
    "        # å‰µå»ºéŒ¯èª¤ DataFrame\n",
    "        if errors:\n",
    "            error_df = pd.DataFrame(errors)\n",
    "            \n",
    "            # åˆ†æéŒ¯èª¤é¡å‹åˆ†ä½ˆ\n",
    "            error_type_counts = Counter(error_df['éŒ¯èª¤é¡å‹'])\n",
    "            \n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"æ¨¡å‹: {model_name}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"\\nç¸½éŒ¯èª¤æ•¸: {len(errors)}\")\n",
    "            print(f\"\\næœ€å¸¸è¦‹éŒ¯èª¤é¡å‹ (Top 5):\")\n",
    "            for error_type, count in error_type_counts.most_common(5):\n",
    "                print(f\"  {error_type}: {count} æ¬¡ ({count/len(errors):.1%})\")\n",
    "            \n",
    "            # æŒ‰å­¸ç§‘çµ±è¨ˆéŒ¯èª¤\n",
    "            subject_errors = error_df.groupby('å­¸ç§‘').size().sort_values(ascending=False)\n",
    "            print(f\"\\nå„å­¸ç§‘éŒ¯èª¤æ•¸:\")\n",
    "            for subject, count in subject_errors.items():\n",
    "                print(f\"  {subject}: {count}\")\n",
    "            \n",
    "            error_analysis[model_key] = {\n",
    "                'errors': error_df,\n",
    "                'error_type_counts': error_type_counts,\n",
    "                'subject_errors': subject_errors\n",
    "            }\n",
    "            \n",
    "            # ä¿å­˜éŒ¯èª¤è©³æƒ…\n",
    "            error_file = ANALYSIS_DIR / f\"{model_key}_errors.csv\"\n",
    "            error_df.to_csv(error_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\nâœ… éŒ¯èª¤è©³æƒ…å·²ä¿å­˜: {error_file}\")\n",
    "        else:\n",
    "            print(f\"\\nâœ¨ {model_name}: æ²’æœ‰éŒ¯èª¤ (å®Œç¾è¡¨ç¾!)\")\n",
    "    \n",
    "    return error_analysis\n",
    "\n",
    "\n",
    "# åŸ·è¡ŒéŒ¯èª¤åˆ†æ\n",
    "error_analysis = analyze_errors(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ä¿¡å¿ƒåº¦åˆ†æ\n",
    "\n",
    "åˆ†ææ¨¡å‹å°ç­”æ¡ˆçš„ä¿¡å¿ƒåº¦ (åŸºæ–¼ logits åˆ†ä½ˆ)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence(results: dict) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    åˆ†ææ¨¡å‹ä¿¡å¿ƒåº¦\n",
    "    \n",
    "    Args:\n",
    "        results: è©•ä¼°çµæœå­—å…¸\n",
    "    \n",
    "    Returns:\n",
    "        ä¿¡å¿ƒåº¦åˆ†æçµæœ\n",
    "    \"\"\"\n",
    "    confidence_analysis = {}\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        model_name = result['model_name']\n",
    "        \n",
    "        confidence_data = []\n",
    "        \n",
    "        for subject_result in result['subject_results']:\n",
    "            subject = subject_result['subject']\n",
    "            \n",
    "            for detail in subject_result['details']:\n",
    "                logits = detail['logits']\n",
    "                \n",
    "                # è¨ˆç®— logits çµ±è¨ˆ\n",
    "                logit_values = list(logits.values())\n",
    "                max_logit = max(logit_values)\n",
    "                min_logit = min(logit_values)\n",
    "                logit_range = max_logit - min_logit  # ä¿¡å¿ƒåº¦æŒ‡æ¨™\n",
    "                \n",
    "                # Softmax è¨ˆç®—æ©Ÿç‡\n",
    "                exp_logits = np.exp(np.array(logit_values) - max_logit)  # é˜²æ­¢æº¢å‡º\n",
    "                probs = exp_logits / exp_logits.sum()\n",
    "                max_prob = probs.max()\n",
    "                \n",
    "                confidence_data.append({\n",
    "                    'å­¸ç§‘': subject,\n",
    "                    'æ˜¯å¦æ­£ç¢º': detail['is_correct'],\n",
    "                    'Logitç¯„åœ': logit_range,\n",
    "                    'æœ€å¤§æ©Ÿç‡': max_prob,\n",
    "                    'é æ¸¬ç­”æ¡ˆ': detail['predicted_answer'],\n",
    "                    'æ­£ç¢ºç­”æ¡ˆ': detail['correct_answer']\n",
    "                })\n",
    "        \n",
    "        # å‰µå»º DataFrame\n",
    "        conf_df = pd.DataFrame(confidence_data)\n",
    "        \n",
    "        # åˆ†ææ­£ç¢º vs éŒ¯èª¤çš„ä¿¡å¿ƒåº¦å·®ç•°\n",
    "        correct_conf = conf_df[conf_df['æ˜¯å¦æ­£ç¢º']]\n",
    "        incorrect_conf = conf_df[~conf_df['æ˜¯å¦æ­£ç¢º']]\n",
    "        \n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"æ¨¡å‹: {model_name} - ä¿¡å¿ƒåº¦åˆ†æ\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        \n",
    "        if len(correct_conf) > 0:\n",
    "            print(f\"\\næ­£ç¢ºç­”æ¡ˆçš„ä¿¡å¿ƒåº¦:\")\n",
    "            print(f\"  å¹³å‡ Logit ç¯„åœ: {correct_conf['Logitç¯„åœ'].mean():.3f}\")\n",
    "            print(f\"  å¹³å‡æœ€å¤§æ©Ÿç‡:    {correct_conf['æœ€å¤§æ©Ÿç‡'].mean():.3f}\")\n",
    "        \n",
    "        if len(incorrect_conf) > 0:\n",
    "            print(f\"\\néŒ¯èª¤ç­”æ¡ˆçš„ä¿¡å¿ƒåº¦:\")\n",
    "            print(f\"  å¹³å‡ Logit ç¯„åœ: {incorrect_conf['Logitç¯„åœ'].mean():.3f}\")\n",
    "            print(f\"  å¹³å‡æœ€å¤§æ©Ÿç‡:    {incorrect_conf['æœ€å¤§æ©Ÿç‡'].mean():.3f}\")\n",
    "        \n",
    "        # é«˜ä¿¡å¿ƒåº¦éŒ¯èª¤ (éåº¦è‡ªä¿¡)\n",
    "        if len(incorrect_conf) > 0:\n",
    "            overconfident = incorrect_conf[incorrect_conf['æœ€å¤§æ©Ÿç‡'] > 0.8]\n",
    "            print(f\"\\néåº¦è‡ªä¿¡çš„éŒ¯èª¤ (æ©Ÿç‡ > 0.8): {len(overconfident)} é¡Œ\")\n",
    "            if len(overconfident) > 0:\n",
    "                print(f\"  ä½”ç¸½éŒ¯èª¤çš„æ¯”ä¾‹: {len(overconfident)/len(incorrect_conf):.1%}\")\n",
    "        \n",
    "        # ä½ä¿¡å¿ƒåº¦æ­£ç¢º (ä¿å®ˆ)\n",
    "        if len(correct_conf) > 0:\n",
    "            underconfident = correct_conf[correct_conf['æœ€å¤§æ©Ÿç‡'] < 0.5]\n",
    "            print(f\"\\nä¿å®ˆçš„æ­£ç¢ºç­”æ¡ˆ (æ©Ÿç‡ < 0.5): {len(underconfident)} é¡Œ\")\n",
    "            if len(underconfident) > 0:\n",
    "                print(f\"  ä½”ç¸½æ­£ç¢ºçš„æ¯”ä¾‹: {len(underconfident)/len(correct_conf):.1%}\")\n",
    "        \n",
    "        confidence_analysis[model_key] = conf_df\n",
    "        \n",
    "        # ä¿å­˜ä¿¡å¿ƒåº¦åˆ†æ\n",
    "        conf_file = ANALYSIS_DIR / f\"{model_key}_confidence.csv\"\n",
    "        conf_df.to_csv(conf_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nâœ… ä¿¡å¿ƒåº¦åˆ†æå·²ä¿å­˜: {conf_file}\")\n",
    "    \n",
    "    return confidence_analysis\n",
    "\n",
    "\n",
    "# åŸ·è¡Œä¿¡å¿ƒåº¦åˆ†æ\n",
    "confidence_analysis = analyze_confidence(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹å¼·å¼±é …å°æ¯”\n",
    "\n",
    "è­˜åˆ¥æ¯å€‹æ¨¡å‹çš„å¼·é …å’Œå¼±é …å­¸ç§‘ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_strengths(results: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    å°æ¯”æ¨¡å‹åœ¨å„å­¸ç§‘çš„è¡¨ç¾\n",
    "    \n",
    "    Args:\n",
    "        results: è©•ä¼°çµæœå­—å…¸\n",
    "    \n",
    "    Returns:\n",
    "        å­¸ç§‘å°æ¯” DataFrame\n",
    "    \"\"\"\n",
    "    # æ”¶é›†æ‰€æœ‰å­¸ç§‘çš„æº–ç¢ºç‡\n",
    "    subject_comparison = defaultdict(dict)\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        model_name = result['model_name']\n",
    "        \n",
    "        for subject_result in result['subject_results']:\n",
    "            subject = subject_result['subject']\n",
    "            accuracy = subject_result['accuracy']\n",
    "            subject_comparison[subject][model_name] = accuracy\n",
    "    \n",
    "    # å‰µå»º DataFrame\n",
    "    comparison_df = pd.DataFrame(subject_comparison).T\n",
    "    \n",
    "    # è¨ˆç®—å·®ç•°\n",
    "    if len(comparison_df.columns) == 2:\n",
    "        model1, model2 = comparison_df.columns\n",
    "        comparison_df['å·®ç•°'] = comparison_df[model1] - comparison_df[model2]\n",
    "        comparison_df['å„ªå‹¢æ¨¡å‹'] = comparison_df['å·®ç•°'].apply(\n",
    "            lambda x: model1 if x > 0 else (model2 if x < 0 else 'ç›¸åŒ')\n",
    "        )\n",
    "    \n",
    "    # æ’åº\n",
    "    if 'å·®ç•°' in comparison_df.columns:\n",
    "        comparison_df = comparison_df.sort_values('å·®ç•°', ascending=False)\n",
    "    \n",
    "    # æ ¼å¼åŒ–\n",
    "    for col in comparison_df.columns:\n",
    "        if col not in ['å„ªå‹¢æ¨¡å‹']:\n",
    "            if comparison_df[col].dtype in [np.float64, np.float32]:\n",
    "                comparison_df[col] = comparison_df[col].apply(lambda x: f\"{x:.2%}\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "if len(results) >= 2:\n",
    "    # å°æ¯”åˆ†æ\n",
    "    comparison_df = compare_model_strengths(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"æ¨¡å‹å¼·å¼±é …å°æ¯” (æŒ‰å·®ç•°æ’åº)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string())\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ä¿å­˜å°æ¯”çµæœ\n",
    "    comparison_df.to_csv(ANALYSIS_DIR / \"model_comparison.csv\", encoding='utf-8-sig')\n",
    "    print(f\"\\nâœ… å°æ¯”çµæœå·²ä¿å­˜: {ANALYSIS_DIR / 'model_comparison.csv'}\")\n",
    "    \n",
    "    # ç¸½çµå¼·å¼±é …\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"å¼·å¼±é …ç¸½çµ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name in comparison_df.columns:\n",
    "        if model_name not in ['å·®ç•°', 'å„ªå‹¢æ¨¡å‹']:\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            \n",
    "            # æ‰¾å‡ºå¼·é … (æº–ç¢ºç‡æœ€é«˜çš„ 3 å€‹å­¸ç§‘)\n",
    "            strengths = comparison_df[comparison_df['å„ªå‹¢æ¨¡å‹'] == model_name].head(3)\n",
    "            if len(strengths) > 0:\n",
    "                print(f\"  å¼·é …:\")\n",
    "                for subject in strengths.index:\n",
    "                    print(f\"    - {subject}: {strengths.loc[subject, model_name]}\")\n",
    "            \n",
    "            # æ‰¾å‡ºå¼±é … (æº–ç¢ºç‡æœ€ä½çš„ 3 å€‹å­¸ç§‘)\n",
    "            weaknesses = comparison_df[comparison_df['å„ªå‹¢æ¨¡å‹'] != model_name].tail(3)\n",
    "            if len(weaknesses) > 0:\n",
    "                print(f\"  å¼±é …:\")\n",
    "                for subject in weaknesses.index:\n",
    "                    print(f\"    - {subject}: {weaknesses.loc[subject, model_name]}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  éœ€è¦è‡³å°‘ 2 å€‹æ¨¡å‹çš„çµæœæ‰èƒ½é€²è¡Œå°æ¯”åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç”Ÿæˆåˆ†ææ‘˜è¦\n",
    "\n",
    "ç”Ÿæˆå®Œæ•´çš„åˆ†ææ‘˜è¦å ±å‘Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_summary(results: dict, output_file: Path):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆåˆ†ææ‘˜è¦å ±å‘Š\n",
    "    \n",
    "    Args:\n",
    "        results: è©•ä¼°çµæœå­—å…¸\n",
    "        output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# C-Eval è©•ä¼°åˆ†æå ±å‘Š\\n\\n\")\n",
    "        f.write(f\"ç”Ÿæˆæ™‚é–“: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        # 1. æ•´é«”è¡¨ç¾\n",
    "        f.write(\"## 1. æ•´é«”è¡¨ç¾\\n\\n\")\n",
    "        for model_key, result in results.items():\n",
    "            f.write(f\"### {result['model_name']}\\n\\n\")\n",
    "            f.write(f\"- æ•´é«”æº–ç¢ºç‡: {result['overall_accuracy']:.2%}\\n\")\n",
    "            f.write(f\"- è©•ä¼°å­¸ç§‘æ•¸: {len(result['subject_results'])}\\n\")\n",
    "            f.write(f\"\\nåˆ†é¡æº–ç¢ºç‡:\\n\")\n",
    "            for category, accuracy in result['category_accuracies'].items():\n",
    "                f.write(f\"  - {category}: {accuracy:.2%}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # 2. å­¸ç§‘é›£åº¦æ’å\n",
    "        f.write(\"## 2. å­¸ç§‘é›£åº¦æ’å\\n\\n\")\n",
    "        difficulty_df = analyze_subject_difficulty(results)\n",
    "        f.write(difficulty_df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # 3. éŒ¯èª¤çµ±è¨ˆ\n",
    "        f.write(\"## 3. éŒ¯èª¤çµ±è¨ˆ\\n\\n\")\n",
    "        for model_key, result in results.items():\n",
    "            total_errors = sum(\n",
    "                len([d for d in sr['details'] if not d['is_correct']])\n",
    "                for sr in result['subject_results']\n",
    "            )\n",
    "            f.write(f\"### {result['model_name']}\\n\\n\")\n",
    "            f.write(f\"- ç¸½éŒ¯èª¤æ•¸: {total_errors}\\n\")\n",
    "            \n",
    "            # å„å­¸ç§‘éŒ¯èª¤æ•¸\n",
    "            f.write(f\"\\nå„å­¸ç§‘éŒ¯èª¤æ•¸:\\n\")\n",
    "            for sr in result['subject_results']:\n",
    "                errors = len([d for d in sr['details'] if not d['is_correct']])\n",
    "                f.write(f\"  - {sr['subject']}: {errors}/{sr['total']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # 4. é—œéµç™¼ç¾\n",
    "        f.write(\"## 4. é—œéµç™¼ç¾\\n\\n\")\n",
    "        \n",
    "        # æ‰¾å‡ºæœ€é›£çš„å­¸ç§‘\n",
    "        difficulty_df_raw = analyze_subject_difficulty(results)\n",
    "        hardest_subject = difficulty_df_raw.iloc[0]['å­¸ç§‘']\n",
    "        easiest_subject = difficulty_df_raw.iloc[-1]['å­¸ç§‘']\n",
    "        \n",
    "        f.write(f\"- æœ€å›°é›£å­¸ç§‘: {hardest_subject}\\n\")\n",
    "        f.write(f\"- æœ€ç°¡å–®å­¸ç§‘: {easiest_subject}\\n\")\n",
    "        \n",
    "        # æ¨¡å‹å°æ¯”\n",
    "        if len(results) >= 2:\n",
    "            model_names = [r['model_name'] for r in results.values()]\n",
    "            accuracies = [r['overall_accuracy'] for r in results.values()]\n",
    "            best_model_idx = np.argmax(accuracies)\n",
    "            f.write(f\"- æœ€ä½³æ¨¡å‹: {model_names[best_model_idx]} ({accuracies[best_model_idx]:.2%})\\n\")\n",
    "            f.write(f\"- æº–ç¢ºç‡å·®è·: {(max(accuracies) - min(accuracies)):.2%}\\n\")\n",
    "        \n",
    "        f.write(\"\\n---\\n\\n\")\n",
    "        f.write(\"è©³ç´°åœ–è¡¨è«‹åƒé–± 04-Visualize_and_Report.ipynb\\n\")\n",
    "    \n",
    "    print(f\"\\nâœ… åˆ†ææ‘˜è¦å·²ç”Ÿæˆ: {output_file}\")\n",
    "\n",
    "\n",
    "# ç”Ÿæˆæ‘˜è¦\n",
    "summary_file = ANALYSIS_DIR / \"analysis_summary.md\"\n",
    "generate_analysis_summary(results, summary_file)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"åˆ†æå®Œæˆ!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\næ‰€æœ‰åˆ†æçµæœå·²ä¿å­˜è‡³: {ANALYSIS_DIR}\")\n",
    "print(\"\\nç”Ÿæˆçš„æª”æ¡ˆ:\")\n",
    "for file in sorted(ANALYSIS_DIR.glob(\"*\")):\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ç¸½çµ\n",
    "\n",
    "åœ¨æœ¬ notebook ä¸­,æˆ‘å€‘å®Œæˆäº†:\n",
    "\n",
    "1. âœ… è¼‰å…¥è©•ä¼°çµæœä¸¦è¨ˆç®—æ•´é«”çµ±è¨ˆ\n",
    "2. âœ… å­¸ç§‘é›£åº¦åˆ†æ (è­˜åˆ¥æœ€é›£å’Œæœ€ç°¡å–®çš„å­¸ç§‘)\n",
    "3. âœ… éŒ¯èª¤åˆ†æ (æ‰¾å‡ºå¸¸è¦‹éŒ¯èª¤æ¨¡å¼)\n",
    "4. âœ… ä¿¡å¿ƒåº¦åˆ†æ (æª¢æŸ¥æ¨¡å‹çš„éåº¦è‡ªä¿¡å’Œä¿å®ˆå‚¾å‘)\n",
    "5. âœ… æ¨¡å‹å¼·å¼±é …å°æ¯” (è­˜åˆ¥å„æ¨¡å‹çš„å„ªåŠ£å‹¢)\n",
    "6. âœ… ç”Ÿæˆåˆ†ææ‘˜è¦å ±å‘Š\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ **04-Visualize_and_Report.ipynb** ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨å’Œå®Œæ•´è©•ä¼°å ±å‘Šã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**é—œéµç™¼ç¾**:\n",
    "- å­¸ç§‘é›£åº¦èˆ‡æ¨¡å‹è¡¨ç¾é«˜åº¦ç›¸é—œ\n",
    "- ä¸­æ–‡å„ªåŒ–æ¨¡å‹ (å¦‚ Qwen) é€šå¸¸åœ¨ä¸­æ–‡ä»»å‹™ä¸Šè¡¨ç¾æ›´å¥½\n",
    "- éŒ¯èª¤æ¨¡å¼å¯ä»¥å¹«åŠ©è­˜åˆ¥æ¨¡å‹çš„ç³»çµ±æ€§å¼±é»\n",
    "- ä¿¡å¿ƒåº¦åˆ†æå¯ä»¥æ­ç¤ºæ¨¡å‹çš„æ ¡æº–å“è³ª\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
