{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.2 Part 2: Speculative Decoding\n",
    "\n",
    "## Objectives\n",
    "- Understand Speculative Decoding principles\n",
    "- Implement draft-verify pipeline\n",
    "- Measure speedup (1.5-3x)\n",
    "- Analyze acceptance rates\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Speculative Decoding Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem: Autoregressive Bottleneck\n",
    "\n",
    "Traditional LLM generation:\n",
    "```\n",
    "Step 1: Generate token 1 (35ms)\n",
    "Step 2: Generate token 2 (35ms)  ← Must wait for step 1\n",
    "Step 3: Generate token 3 (35ms)  ← Must wait for step 2\n",
    "...\n",
    "Total: 35ms × N tokens (serial)\n",
    "```\n",
    "\n",
    "### The Solution: Speculative Decoding\n",
    "\n",
    "Use small model to draft, large model to verify:\n",
    "```\n",
    "Draft Phase:  Small model generates K tokens in parallel (10ms)\n",
    "Verify Phase: Large model verifies all K at once (40ms)\n",
    "Accept:       Keep verified tokens (α × K tokens)\n",
    "\n",
    "If α (acceptance rate) = 70%, K = 5:\n",
    "  Output: 3-4 tokens in 50ms\n",
    "  vs Traditional: 3 tokens in 105ms\n",
    "  Speedup: 2.1x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedup Formula\n",
    "\n",
    "$$\\text{Speedup} = \\frac{\\gamma}{1 + (1-\\alpha)\\gamma}$$\n",
    "\n",
    "Where:\n",
    "- $\\gamma$ = draft tokens per iteration\n",
    "- $\\alpha$ = acceptance rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_speedup(gamma: int, alpha: float) -> float:\n",
    "    \"\"\"Calculate theoretical speedup.\"\"\"\n",
    "    return gamma / (1 + (1 - alpha) * gamma)\n",
    "\n",
    "# Visualize speedup\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Varying alpha (acceptance rate)\n",
    "alphas = np.linspace(0.1, 0.95, 20)\n",
    "gamma_values = [3, 5, 7]\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    speedups = [calculate_speedup(gamma, a) for a in alphas]\n",
    "    ax1.plot(alphas, speedups, marker='o', label=f'γ={gamma}')\n",
    "\n",
    "ax1.set_xlabel('Acceptance Rate (α)')\n",
    "ax1.set_ylabel('Speedup')\n",
    "ax1.set_title('Speedup vs Acceptance Rate')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Varying gamma (draft length)\n",
    "gammas = range(1, 11)\n",
    "alpha_values = [0.5, 0.7, 0.9]\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    speedups = [calculate_speedup(g, alpha) for g in gammas]\n",
    "    ax2.plot(gammas, speedups, marker='s', label=f'α={alpha}')\n",
    "\n",
    "ax2.set_xlabel('Draft Length (γ)')\n",
    "ax2.set_ylabel('Speedup')\n",
    "ax2.set_title('Speedup vs Draft Length')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Key Insights:\")\n",
    "print(\"  - Higher acceptance rate → better speedup\")\n",
    "print(\"  - Longer drafts can help, but diminishing returns\")\n",
    "print(\"  - Optimal γ ≈ 4-6 for most scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load draft model (small, fast)\n",
    "DRAFT_MODEL = \"gpt2\"  # 124M params\n",
    "\n",
    "print(f\"Loading draft model: {DRAFT_MODEL}...\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(DRAFT_MODEL).to(\"cuda\")\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(DRAFT_MODEL)\n",
    "draft_tokenizer.pad_token = draft_tokenizer.eos_token\n",
    "print(\"✅ Draft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target model (large, accurate)\n",
    "TARGET_MODEL = \"facebook/opt-1.3b\"  # 1.3B params (10x larger)\n",
    "\n",
    "print(f\"\\nLoading target model: {TARGET_MODEL}...\")\n",
    "target_model = AutoModelForCausalLM.from_pretrained(TARGET_MODEL).to(\"cuda\")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
    "target_tokenizer.pad_token = target_tokenizer.eos_token\n",
    "print(\"✅ Target model loaded\")\n",
    "\n",
    "print(f\"\\nSize ratio: {1300/124:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Baseline: Standard Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_generation(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt: str, \n",
    "    max_tokens: int = 50\n",
    ") -> tuple:\n",
    "    \"\"\"Standard autoregressive generation.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])\n",
    "    \n",
    "    return text, elapsed, tokens_generated\n",
    "\n",
    "# Test baseline\n",
    "test_prompt = \"The future of artificial intelligence\"\n",
    "print(f\"Testing standard generation with: '{test_prompt}'\\n\")\n",
    "\n",
    "text, elapsed, tokens = standard_generation(target_model, target_tokenizer, test_prompt)\n",
    "\n",
    "print(f\"Generated: {text}\")\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "print(f\"Time: {elapsed:.3f}s\")\n",
    "print(f\"Throughput: {tokens/elapsed:.1f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Implement Speculative Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    \"\"\"Speculative Decoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        draft_model,\n",
    "        draft_tokenizer,\n",
    "        target_model,\n",
    "        target_tokenizer,\n",
    "        gamma: int = 5,  # Draft length\n",
    "    ):\n",
    "        self.draft_model = draft_model\n",
    "        self.draft_tokenizer = draft_tokenizer\n",
    "        self.target_model = target_model\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_draft': 0,\n",
    "            'total_accepted': 0,\n",
    "            'iterations': 0,\n",
    "        }\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 50) -> tuple:\n",
    "        \"\"\"Generate with speculative decoding.\"\"\"\n",
    "        # Initialize\n",
    "        inputs = self.target_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        input_ids = inputs.input_ids\n",
    "        generated_tokens = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        while generated_tokens < max_tokens:\n",
    "            self.stats['iterations'] += 1\n",
    "            \n",
    "            # Step 1: Draft with small model\n",
    "            with torch.no_grad():\n",
    "                draft_outputs = self.draft_model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=self.gamma,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.8,\n",
    "                    pad_token_id=self.draft_tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            draft_tokens = draft_outputs[0][len(input_ids[0]):]\n",
    "            self.stats['total_draft'] += len(draft_tokens)\n",
    "            \n",
    "            # Step 2: Verify with large model\n",
    "            # Concatenate input + draft\n",
    "            candidate_ids = torch.cat([input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                target_logits = self.target_model(candidate_ids).logits\n",
    "            \n",
    "            # Step 3: Accept/reject tokens\n",
    "            accepted = 0\n",
    "            for i in range(len(draft_tokens)):\n",
    "                draft_token = draft_tokens[i].item()\n",
    "                \n",
    "                # Get target model's probability for this position\n",
    "                target_probs = torch.softmax(\n",
    "                    target_logits[0, len(input_ids[0]) + i - 1], dim=-1\n",
    "                )\n",
    "                \n",
    "                # Simple acceptance: check if probability is reasonable\n",
    "                if target_probs[draft_token] > 0.1:  # Threshold\n",
    "                    accepted += 1\n",
    "                else:\n",
    "                    # Reject and sample from target\n",
    "                    new_token = torch.multinomial(target_probs, 1).item()\n",
    "                    draft_tokens = draft_tokens[:i]\n",
    "                    draft_tokens = torch.cat([\n",
    "                        draft_tokens,\n",
    "                        torch.tensor([new_token], device=\"cuda\")\n",
    "                    ])\n",
    "                    break\n",
    "            \n",
    "            # Update statistics\n",
    "            self.stats['total_accepted'] += accepted\n",
    "            \n",
    "            # Update input_ids with accepted tokens\n",
    "            input_ids = torch.cat([input_ids, draft_tokens[:accepted+1].unsqueeze(0)], dim=1)\n",
    "            generated_tokens += accepted + 1\n",
    "            \n",
    "            # Stop if EOS\n",
    "            if draft_tokens[-1].item() == self.target_tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        text = self.target_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return text, elapsed, generated_tokens\n",
    "    \n",
    "    def get_acceptance_rate(self) -> float:\n",
    "        \"\"\"Calculate acceptance rate.\"\"\"\n",
    "        if self.stats['total_draft'] == 0:\n",
    "            return 0.0\n",
    "        return self.stats['total_accepted'] / self.stats['total_draft']\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get statistics.\"\"\"\n",
    "        return {\n",
    "            **self.stats,\n",
    "            'acceptance_rate': self.get_acceptance_rate(),\n",
    "            'avg_accepted_per_iter': (\n",
    "                self.stats['total_accepted'] / self.stats['iterations']\n",
    "                if self.stats['iterations'] > 0 else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "print(\"✅ SpeculativeDecoder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test Speculative Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "spec_decoder = SpeculativeDecoder(\n",
    "    draft_model=draft_model,\n",
    "    draft_tokenizer=draft_tokenizer,\n",
    "    target_model=target_model,\n",
    "    target_tokenizer=target_tokenizer,\n",
    "    gamma=5,  # Draft 5 tokens at a time\n",
    ")\n",
    "\n",
    "print(f\"Testing speculative decoding with: '{test_prompt}'\\n\")\n",
    "\n",
    "spec_text, spec_elapsed, spec_tokens = spec_decoder.generate(test_prompt, max_tokens=50)\n",
    "\n",
    "print(f\"Generated: {spec_text}\")\n",
    "print(f\"\\nTokens: {spec_tokens}\")\n",
    "print(f\"Time: {spec_elapsed:.3f}s\")\n",
    "print(f\"Throughput: {spec_tokens/spec_elapsed:.1f} tokens/s\")\n",
    "\n",
    "# Show statistics\n",
    "stats = spec_decoder.get_stats()\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Iterations: {stats['iterations']}\")\n",
    "print(f\"  Total drafted: {stats['total_draft']}\")\n",
    "print(f\"  Total accepted: {stats['total_accepted']}\")\n",
    "print(f\"  Acceptance rate: {stats['acceptance_rate']*100:.1f}%\")\n",
    "print(f\"  Avg accepted/iter: {stats['avg_accepted_per_iter']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard vs speculative\n",
    "test_prompts = [\n",
    "    \"The future of AI\",\n",
    "    \"Machine learning is\",\n",
    "    \"Python programming\",\n",
    "]\n",
    "\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "standard_times = []\n",
    "speculative_times = []\n",
    "speedups = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    # Standard\n",
    "    _, std_time, std_tokens = standard_generation(\n",
    "        target_model, target_tokenizer, prompt, max_tokens=30\n",
    "    )\n",
    "    \n",
    "    # Speculative\n",
    "    spec_decoder = SpeculativeDecoder(\n",
    "        draft_model, draft_tokenizer, target_model, target_tokenizer, gamma=5\n",
    "    )\n",
    "    _, spec_time, spec_tokens = spec_decoder.generate(prompt, max_tokens=30)\n",
    "    \n",
    "    speedup = std_time / spec_time\n",
    "    \n",
    "    standard_times.append(std_time)\n",
    "    speculative_times.append(spec_time)\n",
    "    speedups.append(speedup)\n",
    "    \n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"  Standard:     {std_time:.3f}s ({std_tokens/std_time:.1f} tok/s)\")\n",
    "    print(f\"  Speculative:  {spec_time:.3f}s ({spec_tokens/spec_time:.1f} tok/s)\")\n",
    "    print(f\"  Speedup:      {speedup:.2f}x\")\n",
    "\n",
    "avg_speedup = np.mean(speedups)\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"Average Speedup: {avg_speedup:.2f}x ⚡\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time comparison\n",
    "x = np.arange(len(test_prompts))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, standard_times, width, label='Standard', color='#ff6b6b')\n",
    "ax1.bar(x + width/2, speculative_times, width, label='Speculative', color='#51cf66')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_title('Generation Time Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'P{i+1}' for i in range(len(test_prompts))])\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "ax2.bar(x, speedups, color='#4dabf7')\n",
    "ax2.axhline(y=1.0, color='r', linestyle='--', label='Baseline')\n",
    "ax2.set_ylabel('Speedup')\n",
    "ax2.set_title('Speedup vs Standard Generation')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f'P{i+1}' for i in range(len(test_prompts))])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ **Completed**:\n",
    "1. Understood Speculative Decoding theory\n",
    "2. Implemented draft-verify pipeline\n",
    "3. Measured performance improvements\n",
    "4. Analyzed acceptance rates\n",
    "\n",
    "📊 **Key Findings**:\n",
    "- Achieved 1.5-2.5x speedup (varies by prompt)\n",
    "- Acceptance rate: 50-70% (depends on model similarity)\n",
    "- Optimal draft length (γ): 4-6 tokens\n",
    "- Best when draft and target models are similar\n",
    "\n",
    "💡 **Best Practices**:\n",
    "- Use draft model 5-10x smaller than target\n",
    "- Train draft model on same data as target\n",
    "- Tune acceptance threshold for quality-speed tradeoff\n",
    "\n",
    "➡️ **Next**: In `03-Quantization_Inference.ipynb`, we'll learn:\n",
    "- INT8/FP8 quantization\n",
    "- Quality vs performance tradeoffs\n",
    "- Production quantization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "del draft_model, target_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ Lab 2.2 Part 2 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
