{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.4 Part 2: Deployment Implementation\n",
    "\n",
    "## Objectives\n",
    "- Create optimized Docker images\n",
    "- Design Kubernetes deployment manifests\n",
    "- Implement auto-scaling configuration\n",
    "- Set up model registry and version management\n",
    "- Design CI/CD pipeline\n",
    "\n",
    "## Estimated Time: 60-120 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Docker Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate optimized Dockerfile\n",
    "import os\n",
    "\n",
    "# Multi-stage Dockerfile for production\n",
    "dockerfile_content = '''\n",
    "# Multi-stage Dockerfile for vLLM + FastAPI service\n",
    "FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS builder\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    python3.10 \\\\\n",
    "    python3.10-dev \\\\\n",
    "    python3-pip \\\\\n",
    "    git \\\\\n",
    "    curl \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Create virtual environment\n",
    "RUN python3.10 -m pip install --upgrade pip\n",
    "RUN python3.10 -m pip install wheel setuptools\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt /tmp/\n",
    "RUN pip install --no-cache-dir -r /tmp/requirements.txt\n",
    "\n",
    "# Compile flash-attention (optional)\n",
    "RUN pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Production stage\n",
    "FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS production\n",
    "\n",
    "# Install minimal runtime dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    python3.10 \\\\\n",
    "    python3.10-distutils \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy Python environment from builder\n",
    "COPY --from=builder /usr/local/lib/python3.10 /usr/local/lib/python3.10\n",
    "COPY --from=builder /usr/local/bin /usr/local/bin\n",
    "\n",
    "# Create application user\n",
    "RUN useradd --create-home --shell /bin/bash app\n",
    "USER app\n",
    "WORKDIR /home/app\n",
    "\n",
    "# Copy application code\n",
    "COPY --chown=app:app src/ ./src/\n",
    "COPY --chown=app:app config/ ./config/\n",
    "\n",
    "# Environment variables\n",
    "ENV PYTHONPATH=/home/app/src\n",
    "ENV CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \\\\\n",
    "  CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Start command\n",
    "CMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"1\"]\n",
    "'''\n",
    "\n",
    "# Save Dockerfile\n",
    "with open('Dockerfile.production', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"‚úÖ Generated optimized Dockerfile\")\n",
    "print(\"\\nKey optimizations:\")\n",
    "print(\"‚Ä¢ Multi-stage build (reduces image size)\")\n",
    "print(\"‚Ä¢ Non-root user (security)\")\n",
    "print(\"‚Ä¢ Health check (container health)\")\n",
    "print(\"‚Ä¢ Minimal runtime dependencies\")\n",
    "print(\"‚Ä¢ Layer caching optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate requirements.txt for Docker build\n",
    "requirements_txt = '''\n",
    "# Core inference\n",
    "vllm>=0.6.0\n",
    "torch>=2.5.1\n",
    "transformers>=4.57.0\n",
    "\n",
    "# Web framework\n",
    "fastapi>=0.104.0\n",
    "uvicorn[standard]>=0.24.0\n",
    "\n",
    "# Monitoring\n",
    "prometheus-client>=0.19.0\n",
    "\n",
    "# Utilities\n",
    "pydantic>=2.0.0\n",
    "httpx>=0.25.0\n",
    "python-json-logger>=2.0.0\n",
    "\n",
    "# Optional optimizations\n",
    "# flash-attn --no-build-isolation\n",
    "bitsandbytes>=0.48.1\n",
    "'''\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements_txt.strip())\n",
    "\n",
    "print(\"‚úÖ Generated requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate docker-compose.yml for local development\n",
    "docker_compose = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  llm-service:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.production\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "      - VLLM_GPU_MEMORY_UTILIZATION=0.9\n",
    "      - VLLM_MAX_MODEL_LEN=2048\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    volumes:\n",
    "      - model-cache:/home/app/.cache\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 2m\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "      - prometheus-data:/prometheus\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana-data:/var/lib/grafana\n",
    "      - ./config/grafana:/etc/grafana/provisioning\n",
    "\n",
    "volumes:\n",
    "  model-cache:\n",
    "  prometheus-data:\n",
    "  grafana-data:\n",
    "'''\n",
    "\n",
    "with open('docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose.strip())\n",
    "\n",
    "print(\"‚úÖ Generated docker-compose.yml for local testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Kubernetes Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Kubernetes deployment manifest\n",
    "k8s_deployment = f'''\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: llm-service\n",
    "  namespace: production\n",
    "  labels:\n",
    "    app: llm-service\n",
    "    version: v1.0.0\n",
    "spec:\n",
    "  replicas: {resources['instances']}\n",
    "  strategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxSurge: 1\n",
    "      maxUnavailable: 0  # Zero downtime deployment\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-service\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: llm-service\n",
    "      annotations:\n",
    "        prometheus.io/scrape: \"true\"\n",
    "        prometheus.io/port: \"8000\"\n",
    "        prometheus.io/path: \"/metrics\"\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: llm-service\n",
    "        image: your-registry/llm-service:v1.0.0\n",
    "        imagePullPolicy: Always\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "          name: http\n",
    "        resources:\n",
    "          requests:\n",
    "            nvidia.com/gpu: 1\n",
    "            memory: \"32Gi\"\n",
    "            cpu: \"8\"\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1\n",
    "            memory: \"48Gi\"\n",
    "            cpu: \"12\"\n",
    "        env:\n",
    "        - name: CUDA_VISIBLE_DEVICES\n",
    "          value: \"0\"\n",
    "        - name: VLLM_GPU_MEMORY_UTILIZATION\n",
    "          value: \"0.9\"\n",
    "        - name: VLLM_MAX_MODEL_LEN\n",
    "          value: \"{requirements.max_context_length}\"\n",
    "        - name: MODEL_NAME\n",
    "          valueFrom:\n",
    "            configMapKeyRef:\n",
    "              name: llm-config\n",
    "              key: model_name\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 120\n",
    "          periodSeconds: 30\n",
    "          timeoutSeconds: 10\n",
    "          failureThreshold: 3\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /ready\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          failureThreshold: 3\n",
    "        volumeMounts:\n",
    "        - name: model-cache\n",
    "          mountPath: /home/app/.cache\n",
    "        - name: config\n",
    "          mountPath: /home/app/config\n",
    "      volumes:\n",
    "      - name: model-cache\n",
    "        persistentVolumeClaim:\n",
    "          claimName: model-cache-pvc\n",
    "      - name: config\n",
    "        configMap:\n",
    "          name: llm-config\n",
    "      nodeSelector:\n",
    "        node-type: gpu\n",
    "      tolerations:\n",
    "      - key: nvidia.com/gpu\n",
    "        operator: Exists\n",
    "        effect: NoSchedule\n",
    "'''\n",
    "\n",
    "# Save deployment manifest\n",
    "os.makedirs('k8s', exist_ok=True)\n",
    "with open('k8s/deployment.yaml', 'w') as f:\n",
    "    f.write(k8s_deployment.strip())\n",
    "\n",
    "print(\"‚úÖ Generated Kubernetes Deployment manifest\")\n",
    "print(f\"   Replicas: {resources['instances']}\")\n",
    "print(\"   Features: Rolling updates, health checks, resource limits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Service manifest\n",
    "k8s_service = '''\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: llm-service\n",
    "  namespace: production\n",
    "  labels:\n",
    "    app: llm-service\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "    name: http\n",
    "  selector:\n",
    "    app: llm-service\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: llm-config\n",
    "  namespace: production\n",
    "data:\n",
    "  model_name: \"meta-llama/Llama-2-7b-hf\"\n",
    "  max_model_len: \"4096\"\n",
    "  tensor_parallel_size: \"1\"\n",
    "  gpu_memory_utilization: \"0.9\"\n",
    "  max_num_seqs: \"32\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: model-cache-pvc\n",
    "  namespace: production\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 50Gi\n",
    "  storageClassName: gp3\n",
    "'''\n",
    "\n",
    "with open('k8s/service.yaml', 'w') as f:\n",
    "    f.write(k8s_service.strip())\n",
    "\n",
    "print(\"‚úÖ Generated Kubernetes Service and ConfigMap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Auto-Scaling Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal Pod Autoscaler (HPA)\n",
    "hpa_manifest = f'''\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: llm-service-hpa\n",
    "  namespace: production\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: llm-service\n",
    "  minReplicas: {max(1, resources['instances'] // 2)}\n",
    "  maxReplicas: {resources['instances'] * 2}\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: requests_per_second\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"50\"  # Scale up if > 50 RPS per pod\n",
    "  behavior:\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50\n",
    "        periodSeconds: 60\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 10\n",
    "        periodSeconds: 60\n",
    "'''\n",
    "\n",
    "with open('k8s/hpa.yaml', 'w') as f:\n",
    "    f.write(hpa_manifest.strip())\n",
    "\n",
    "print(\"‚úÖ Generated HPA configuration\")\n",
    "print(f\"   Min replicas: {max(1, resources['instances'] // 2)}\")\n",
    "print(f\"   Max replicas: {resources['instances'] * 2}\")\n",
    "print(\"   Metrics: Memory utilization, Requests per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical Pod Autoscaler (VPA) - optional\n",
    "vpa_manifest = '''\n",
    "apiVersion: autoscaling.k8s.io/v1\n",
    "kind: VerticalPodAutoscaler\n",
    "metadata:\n",
    "  name: llm-service-vpa\n",
    "  namespace: production\n",
    "spec:\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: llm-service\n",
    "  updatePolicy:\n",
    "    updateMode: \"Auto\"  # Auto-update resource requests\n",
    "  resourcePolicy:\n",
    "    containerPolicies:\n",
    "    - containerName: llm-service\n",
    "      minAllowed:\n",
    "        cpu: 4\n",
    "        memory: 16Gi\n",
    "      maxAllowed:\n",
    "        cpu: 16\n",
    "        memory: 64Gi\n",
    "      controlledResources: [\"cpu\", \"memory\"]\n",
    "'''\n",
    "\n",
    "with open('k8s/vpa.yaml', 'w') as f:\n",
    "    f.write(vpa_manifest.strip())\n",
    "\n",
    "print(\"‚úÖ Generated VPA configuration\")\n",
    "print(\"   Automatically adjusts CPU/Memory requests based on usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Balancer and Ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Ingress configuration\n",
    "ingress_manifest = '''\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: llm-service-ingress\n",
    "  namespace: production\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
    "    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n",
    "    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"600\"\n",
    "    nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\"\n",
    "    nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n",
    "    nginx.ingress.kubernetes.io/proxy-body-size: \"32m\"\n",
    "    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n",
    "    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\n",
    "    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n",
    "spec:\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - api.yourcompany.com\n",
    "    secretName: llm-service-tls\n",
    "  rules:\n",
    "  - host: api.yourcompany.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /v1\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: llm-service\n",
    "            port:\n",
    "              number: 80\n",
    "      - path: /health\n",
    "        pathType: Exact\n",
    "        backend:\n",
    "          service:\n",
    "            name: llm-service\n",
    "            port:\n",
    "              number: 80\n",
    "      - path: /metrics\n",
    "        pathType: Exact\n",
    "        backend:\n",
    "          service:\n",
    "            name: llm-service\n",
    "            port:\n",
    "              number: 80\n",
    "'''\n",
    "\n",
    "with open('k8s/ingress.yaml', 'w') as f:\n",
    "    f.write(ingress_manifest.strip())\n",
    "\n",
    "print(\"‚úÖ Generated Ingress configuration\")\n",
    "print(\"   Features: SSL termination, rate limiting, health checks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Helm Chart Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Helm chart structure\n",
    "helm_values = f'''\n",
    "# Default values for llm-service Helm chart\n",
    "\n",
    "replicaCount: {resources['instances']}\n",
    "\n",
    "image:\n",
    "  repository: your-registry/llm-service\n",
    "  pullPolicy: Always\n",
    "  tag: \"v1.0.0\"\n",
    "\n",
    "model:\n",
    "  name: \"meta-llama/Llama-2-7b-hf\"\n",
    "  maxModelLen: {requirements.max_context_length}\n",
    "  tensorParallelSize: 1\n",
    "  gpuMemoryUtilization: 0.9\n",
    "  maxNumSeqs: 32\n",
    "\n",
    "service:\n",
    "  type: ClusterIP\n",
    "  port: 80\n",
    "  targetPort: 8000\n",
    "\n",
    "ingress:\n",
    "  enabled: true\n",
    "  className: \"nginx\"\n",
    "  annotations:\n",
    "    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n",
    "    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n",
    "  hosts:\n",
    "    - host: api.yourcompany.com\n",
    "      paths:\n",
    "        - path: /v1\n",
    "          pathType: Prefix\n",
    "  tls:\n",
    "    - secretName: llm-service-tls\n",
    "      hosts:\n",
    "        - api.yourcompany.com\n",
    "\n",
    "resources:\n",
    "  limits:\n",
    "    nvidia.com/gpu: 1\n",
    "    memory: 48Gi\n",
    "    cpu: 12\n",
    "  requests:\n",
    "    nvidia.com/gpu: 1\n",
    "    memory: 32Gi\n",
    "    cpu: 8\n",
    "\n",
    "autoscaling:\n",
    "  enabled: true\n",
    "  minReplicas: {max(1, resources['instances'] // 2)}\n",
    "  maxReplicas: {resources['instances'] * 2}\n",
    "  targetCPUUtilizationPercentage: 80\n",
    "  targetMemoryUtilizationPercentage: 80\n",
    "\n",
    "nodeSelector:\n",
    "  node-type: gpu\n",
    "\n",
    "tolerations:\n",
    "  - key: nvidia.com/gpu\n",
    "    operator: Exists\n",
    "    effect: NoSchedule\n",
    "\n",
    "persistence:\n",
    "  enabled: true\n",
    "  storageClass: \"gp3\"\n",
    "  size: 50Gi\n",
    "\n",
    "monitoring:\n",
    "  prometheus:\n",
    "    enabled: true\n",
    "    path: /metrics\n",
    "    port: 8000\n",
    "\n",
    "# Security settings\n",
    "securityContext:\n",
    "  runAsNonRoot: true\n",
    "  runAsUser: 1000\n",
    "  runAsGroup: 1000\n",
    "  fsGroup: 1000\n",
    "\n",
    "# Pod disruption budget\n",
    "podDisruptionBudget:\n",
    "  enabled: true\n",
    "  minAvailable: 1\n",
    "'''\n",
    "\n",
    "os.makedirs('helm/llm-service', exist_ok=True)\n",
    "with open('helm/llm-service/values.yaml', 'w') as f:\n",
    "    f.write(helm_values.strip())\n",
    "\n",
    "print(\"‚úÖ Generated Helm values.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Chart.yaml\n",
    "chart_yaml = '''\n",
    "apiVersion: v2\n",
    "name: llm-service\n",
    "description: Production LLM serving with vLLM and FastAPI\n",
    "\n",
    "version: 1.0.0\n",
    "appVersion: \"v1.0.0\"\n",
    "\n",
    "maintainers:\n",
    "  - name: LLM Team\n",
    "    email: llm-team@yourcompany.com\n",
    "\n",
    "dependencies:\n",
    "  - name: prometheus\n",
    "    version: \"25.x.x\"\n",
    "    repository: https://prometheus-community.github.io/helm-charts\n",
    "    condition: prometheus.enabled\n",
    "  \n",
    "  - name: grafana\n",
    "    version: \"8.x.x\"\n",
    "    repository: https://grafana.github.io/helm-charts\n",
    "    condition: grafana.enabled\n",
    "\n",
    "keywords:\n",
    "  - llm\n",
    "  - inference\n",
    "  - vllm\n",
    "  - fastapi\n",
    "  - ai\n",
    "\n",
    "home: https://github.com/yourorg/llm-service\n",
    "sources:\n",
    "  - https://github.com/vllm-project/vllm\n",
    "  - https://fastapi.tiangolo.com/\n",
    "'''\n",
    "\n",
    "with open('helm/llm-service/Chart.yaml', 'w') as f:\n",
    "    f.write(chart_yaml.strip())\n",
    "\n",
    "print(\"‚úÖ Generated Helm Chart.yaml\")\n",
    "print(\"   Includes dependencies for Prometheus and Grafana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. CI/CD Pipeline Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GitHub Actions workflow\n",
    "github_workflow = '''\n",
    "name: LLM Service CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main, develop ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "env:\n",
    "  REGISTRY: ghcr.io\n",
    "  IMAGE_NAME: ${{ github.repository }}/llm-service\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - name: Checkout\n",
    "      uses: actions/checkout@v4\n",
    "\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.10'\n",
    "\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "        pip install pytest pytest-asyncio\n",
    "\n",
    "    - name: Run tests\n",
    "      run: |\n",
    "        pytest tests/ -v\n",
    "\n",
    "    - name: Lint code\n",
    "      run: |\n",
    "        pip install black flake8\n",
    "        black --check src/\n",
    "        flake8 src/\n",
    "\n",
    "  build:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    permissions:\n",
    "      contents: read\n",
    "      packages: write\n",
    "    steps:\n",
    "    - name: Checkout\n",
    "      uses: actions/checkout@v4\n",
    "\n",
    "    - name: Log in to Container Registry\n",
    "      uses: docker/login-action@v3\n",
    "      with:\n",
    "        registry: ${{ env.REGISTRY }}\n",
    "        username: ${{ github.actor }}\n",
    "        password: ${{ secrets.GITHUB_TOKEN }}\n",
    "\n",
    "    - name: Extract metadata\n",
    "      id: meta\n",
    "      uses: docker/metadata-action@v5\n",
    "      with:\n",
    "        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n",
    "        tags: |\n",
    "          type=ref,event=branch\n",
    "          type=ref,event=pr\n",
    "          type=sha,prefix={{branch}}-\n",
    "          type=raw,value=latest,enable={{is_default_branch}}\n",
    "\n",
    "    - name: Build and push Docker image\n",
    "      uses: docker/build-push-action@v5\n",
    "      with:\n",
    "        context: .\n",
    "        file: Dockerfile.production\n",
    "        push: true\n",
    "        tags: ${{ steps.meta.outputs.tags }}\n",
    "        labels: ${{ steps.meta.outputs.labels }}\n",
    "\n",
    "  deploy:\n",
    "    needs: build\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    steps:\n",
    "    - name: Checkout\n",
    "      uses: actions/checkout@v4\n",
    "\n",
    "    - name: Configure kubectl\n",
    "      run: |\n",
    "        echo \"${{ secrets.KUBECONFIG }}\" | base64 -d > $HOME/.kube/config\n",
    "\n",
    "    - name: Install Helm\n",
    "      uses: azure/setup-helm@v3\n",
    "      with:\n",
    "        version: '3.12.0'\n",
    "\n",
    "    - name: Deploy to staging\n",
    "      run: |\n",
    "        helm upgrade --install llm-service-staging ./helm/llm-service \\\\\n",
    "          --namespace staging \\\\\n",
    "          --set image.tag=${GITHUB_SHA:0:8} \\\\\n",
    "          --set replicaCount=1\n",
    "\n",
    "    - name: Run smoke tests\n",
    "      run: |\n",
    "        kubectl wait --for=condition=available --timeout=300s deployment/llm-service -n staging\n",
    "        python tests/smoke_test.py --host staging.internal\n",
    "\n",
    "    - name: Deploy to production\n",
    "      if: success()\n",
    "      run: |\n",
    "        helm upgrade --install llm-service-prod ./helm/llm-service \\\\\n",
    "          --namespace production \\\\\n",
    "          --set image.tag=${GITHUB_SHA:0:8}\n",
    "'''\n",
    "\n",
    "os.makedirs('.github/workflows', exist_ok=True)\n",
    "with open('.github/workflows/ci-cd.yml', 'w') as f:\n",
    "    f.write(github_workflow.strip())\n",
    "\n",
    "print(\"‚úÖ Generated GitHub Actions CI/CD pipeline\")\n",
    "print(\"\\nPipeline stages:\")\n",
    "print(\"1. Test: Run unit tests and linting\")\n",
    "print(\"2. Build: Create and push Docker image\")\n",
    "print(\"3. Deploy: Deploy to staging, run smoke tests, deploy to production\")\n",
    "\n",
    "# Generate deployment script\n",
    "deploy_script = f'''\n",
    "#!/bin/bash\n",
    "# Production deployment script\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "# Configuration\n",
    "NAMESPACE=\"production\"\n",
    "RELEASE_NAME=\"llm-service-prod\"\n",
    "IMAGE_TAG=\"${{1:-latest}}\"\n",
    "\n",
    "echo \"Deploying LLM service to production...\"\n",
    "echo \"Namespace: $NAMESPACE\"\n",
    "echo \"Release: $RELEASE_NAME\"\n",
    "echo \"Image tag: $IMAGE_TAG\"\n",
    "\n",
    "# Create namespace if not exists\n",
    "kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -\n",
    "\n",
    "# Deploy with Helm\n",
    "helm upgrade --install $RELEASE_NAME ./helm/llm-service \\\\\n",
    "  --namespace $NAMESPACE \\\\\n",
    "  --set image.tag=$IMAGE_TAG \\\\\n",
    "  --set replicaCount={resources['instances']} \\\\\n",
    "  --wait \\\\\n",
    "  --timeout 10m\n",
    "\n",
    "# Wait for rollout\n",
    "kubectl rollout status deployment/llm-service -n $NAMESPACE\n",
    "\n",
    "# Check health\n",
    "kubectl wait --for=condition=available --timeout=300s deployment/llm-service -n $NAMESPACE\n",
    "\n",
    "echo \"‚úÖ Deployment completed successfully!\"\n",
    "echo \"\\nService endpoints:\"\n",
    "kubectl get ingress -n $NAMESPACE\n",
    "'''\n",
    "\n",
    "with open('deploy.sh', 'w') as f:\n",
    "    f.write(deploy_script.strip())\n",
    "    \n",
    "os.chmod('deploy.sh', 0o755)\n",
    "\n",
    "print(\"\\n‚úÖ Generated deployment script (deploy.sh)\")\n",
    "print(\"   Usage: ./deploy.sh [image-tag]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "‚úÖ **Completed**:\n",
    "1. Created optimized multi-stage Dockerfile\n",
    "2. Generated Kubernetes deployment manifests\n",
    "3. Configured auto-scaling (HPA and VPA)\n",
    "4. Set up Ingress with SSL and rate limiting\n",
    "5. Created Helm chart structure\n",
    "6. Designed CI/CD pipeline with GitHub Actions\n",
    "\n",
    "üìÅ **Generated Files**:\n",
    "- `Dockerfile.production`: Optimized container image\n",
    "- `docker-compose.yml`: Local development stack\n",
    "- `k8s/`: Kubernetes manifests\n",
    "- `helm/`: Helm chart for deployment\n",
    "- `.github/workflows/ci-cd.yml`: CI/CD pipeline\n",
    "- `deploy.sh`: Production deployment script\n",
    "\n",
    "üéØ **Production Ready Features**:\n",
    "- Zero-downtime deployments\n",
    "- Auto-scaling based on load\n",
    "- Health checks and monitoring\n",
    "- SSL termination and security\n",
    "- Multi-environment support\n",
    "\n",
    "‚û°Ô∏è **Next**: In `03-Performance_and_Cost.ipynb`, we'll cover:\n",
    "- Performance optimization strategies\n",
    "- Cost optimization techniques\n",
    "- SLI/SLO definition and monitoring\n",
    "- Resource allocation optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Modify Helm values**: Change replica count and resource limits\n",
    "2. **Add environment**: Create a development namespace with different configs\n",
    "3. **Security hardening**: Add network policies and pod security standards\n",
    "4. **Multi-region**: Modify manifests for cross-region deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}