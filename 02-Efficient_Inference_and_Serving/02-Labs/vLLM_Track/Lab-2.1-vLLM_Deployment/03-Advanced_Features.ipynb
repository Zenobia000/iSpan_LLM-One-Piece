{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1 Part 3: Advanced Features\n",
    "\n",
    "## Objectives\n",
    "- Understand Continuous Batching\n",
    "- Master advanced sampling strategies\n",
    "- Handle long context inputs\n",
    "- Manage multiple models\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 18:33:22 [__init__.py:216] Automatically detected platform cuda.\n",
      "vLLM: 0.11.0\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from vllm import LLM, SamplingParams\n",
    "import vllm\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import asyncio\n",
    "\n",
    "print(f\"vLLM: {vllm.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-2-7b-hf...\n",
      "INFO 10-27 18:33:26 [utils.py:233] non-default args: {'trust_remote_code': True, 'max_model_len': 1024, 'disable_log_stats': True, 'model': 'meta-llama/Llama-2-7b-hf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 18:33:27 [model.py:547] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 18:33:27 [model.py:1510] Using max model len 1024\n",
      "INFO 10-27 18:33:27 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:28 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:28 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='meta-llama/Llama-2-7b-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-2-7b-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:30 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m WARNING 10-27 18:33:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:31 [gpu_model_runner.py:2602] Starting to load model meta-llama/Llama-2-7b-hf...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:31 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:31 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:31 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c08f7124784d6ba38308e0a30ffd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:34 [default_loader.py:267] Loading weights took 1.63 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:34 [gpu_model_runner.py:2653] Model loading took 12.5524 GiB and 2.757241 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:40 [backends.py:548] Using cache directory: /home/os-sunnie.gd.weng/.cache/vllm/torch_compile_cache/62f198137e/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:40 [backends.py:559] Dynamo bytecode transform time: 6.28 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:42 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.088 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:45 [monitor.py:34] torch.compile takes 6.28 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:46 [gpu_worker.py:298] Available KV cache memory: 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:46 [kv_cache_utils.py:1087] GPU KV cache size: 1,344 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:46 [kv_cache_utils.py:1091] Maximum concurrency for 1,024 tokens per request: 1.31x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:08<00:00,  7.57it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:59 [gpu_model_runner.py:3480] Graph capturing finished in 13 secs, took 0.59 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1800691)\u001b[0;0m INFO 10-27 18:33:59 [core.py:210] init engine (profile, create kv cache, warmup model) took 25.26 seconds\n",
      "INFO 10-27 18:34:00 [llm.py:306] Supported_tasks: ['generate']\n",
      "✅ Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "# MODEL_NAME = \"facebook/opt-1.3b\"  # Alternative\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=1024,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"✅ Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Continuous Batching\n",
    "\n",
    "vLLM's killer feature: dynamic request scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Static Batching Problem\n",
    "\n",
    "```\n",
    "Batch: [Req1, Req2, Req3, Req4]\n",
    "\n",
    "Req1: ████████░░░░░░░░░░░░ (done at step 8, waits)\n",
    "Req2: ████████████░░░░░░░░ (done at step 12, waits)\n",
    "Req3: ██████████████████░░ (done at step 18, waits)\n",
    "Req4: ████████████████████ (done at step 20)\n",
    "       └── Must wait for slowest request ──┘\n",
    "\n",
    "Wasted time: ~40%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Batching Solution\n",
    "\n",
    "```\n",
    "Req1: ████████              (done, removed immediately)\n",
    "Req5:         ██████        (new request added)\n",
    "Req2: ████████████          (done, removed)\n",
    "Req6:             ████      (new request added)\n",
    "Req3: ██████████████████    (done, removed)\n",
    "Req4: ████████████████████\n",
    "\n",
    "Throughput: 2-3x higher!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with varied-length requests...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f4900c87f1430c8ecaed9edf3a953d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6eacee226b44b1a33e057bed752082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1:  10 tokens\n",
      "Request 2:  10 tokens\n",
      "Request 3:  10 tokens\n",
      "Request 4:  10 tokens\n",
      "\n",
      "Total time: 0.73s\n",
      "\n",
      "✅ Continuous batching handled varied lengths efficiently!\n"
     ]
    }
   ],
   "source": [
    "# Simulate continuous batching with varied length requests\n",
    "varied_prompts = [\n",
    "    \"Hi\",  # Very short\n",
    "    \"What is Python?\",  # Short\n",
    "    \"Explain machine learning in detail:\",  # Medium\n",
    "    \"Write a comprehensive guide about artificial intelligence, covering history, techniques, and applications:\",  # Long\n",
    "]\n",
    "\n",
    "# Different max_tokens for each\n",
    "varied_params = [\n",
    "    SamplingParams(max_tokens=10, temperature=0.8),\n",
    "    SamplingParams(max_tokens=30, temperature=0.8),\n",
    "    SamplingParams(max_tokens=100, temperature=0.8),\n",
    "    SamplingParams(max_tokens=200, temperature=0.8),\n",
    "]\n",
    "\n",
    "print(\"Testing with varied-length requests...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "# vLLM automatically handles continuous batching\n",
    "outputs = llm.generate(varied_prompts, varied_params[0])  # Use same params for simplicity\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    tokens = len(output.outputs[0].token_ids)\n",
    "    print(f\"Request {i+1}: {tokens:3d} tokens\")\n",
    "\n",
    "print(f\"\\nTotal time: {elapsed:.2f}s\")\n",
    "print(\"\\n✅ Continuous batching handled varied lengths efficiently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure TTFT and ITL\n",
    "\n",
    "- **TTFT**: Time to First Token\n",
    "- **ITL**: Inter-Token Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring generation latency...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea7a813c74a41f7aaa023051415afd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e9b9060caf4e4083dd71f57dba89b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time: 3.212s\n",
      "Tokens: 50\n",
      "Avg latency per token: 64.2ms\n",
      "Throughput: 15.6 tokens/s\n",
      "llm.supports_streaming? False\n",
      "⚠️ 目前 LLM 實例未直接暴露 streaming 權能，建議嘗試 RESTful OpenAI API endpoint，或檢查新版本文件。\n"
     ]
    }
   ],
   "source": [
    "# For TTFT/ITL measurement, we need streaming API (async)\n",
    "# This is a simplified demonstration\n",
    "\n",
    "\n",
    "# TTFT（Time To First Token）：指從發出請求到模型生成首個 token 的延遲，也就是用戶最先看到文字所等待的總時間。\n",
    "# ITL（Inter-Token Latency）：指首個 token 之後，每生成下個 token 所需的平均延遲，反映生成流的平滑程度。\n",
    "#\n",
    "# 在實作上：\n",
    "# - TTFT 可用第一個 token 到達時的時間減去發送請求時的時間。\n",
    "# - ITL 通常以每個 token 的生成時間間隔來算平均（(最後一個 token 到達時間 - 第一個 token 到達時間)/(token 數-1)）。\n",
    "\n",
    "test_prompt = \"Explain quantum computing:\"\n",
    "test_params = SamplingParams(\n",
    "    max_tokens=50,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "print(\"Measuring generation latency...\")\n",
    "start = time.time()\n",
    "output = llm.generate([test_prompt], test_params)[0]\n",
    "total_time = time.time() - start\n",
    "\n",
    "num_tokens = len(output.outputs[0].token_ids)\n",
    "avg_token_latency = total_time / num_tokens\n",
    "\n",
    "print(f\"\\nTotal time: {total_time:.3f}s\")\n",
    "print(f\"Tokens: {num_tokens}\")\n",
    "print(f\"Avg latency per token: {avg_token_latency*1000:.1f}ms\")\n",
    "print(f\"Throughput: {num_tokens/total_time:.1f} tokens/s\")\n",
    "\n",
    "\n",
    "# 有的，但上面只是大致用生成總時長 / token 數來估算平均 token latency，沒精確區分 TTFT 與 ITL。\n",
    "# 以下提供一個「精確」分別量測 TTFT 和 ITL 的程式碼範例如下（需支援 token streaming 的 llm API，也就是 async/stream 生成才有辦法紀錄每個 token 回傳的時間）：\n",
    "# 假設 llm 支援 streaming 產生 (以類似 OpenAI API 介面為例)：\n",
    "\n",
    "# vLLM 的 LLM 物件本身支援同步批量產生與 OpenAI-style streaming，但原生 LLM 類別主要為同步 blocking 實現。目前（vLLM 0.2.x/0.3.x）官方 Streaming 介面主要在 OpenAI 兼容 REST API 或 WebSocket server。  \n",
    "# 在 pure Python 中，LLM 類別還沒有直接支援 async/await 的 streaming token 事件（不像 OpenAI 官方 openai-python 套件可 yield token）。\n",
    "# 若要 Python 端精確量測 TTFT/ITL，常見做法有：\n",
    "# 1. 利用 vLLM 提供的 OpenAI-compatible server，然後用 openai async client 方式 streaming；\n",
    "# 2. 或改造 LLM 類的 generate_stream() 低階方法（API 仍屬實驗性）。\n",
    "\n",
    "# 檢查當前 llm 物件是否有 streaming 權能\n",
    "has_stream = hasattr(llm, 'generate_stream')\n",
    "\n",
    "print(f\"llm.supports_streaming? {has_stream}\")\n",
    "if has_stream:\n",
    "    print(\"✅ 此 vLLM LLM 實例支援低階 streaming API (generate_stream，可逐 token 產生)\")\n",
    "else:\n",
    "    print(\"⚠️ 目前 LLM 實例未直接暴露 streaming 權能，建議嘗試 RESTful OpenAI API endpoint，或檢查新版本文件。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Advanced Sampling Strategies\n",
    "\n",
    "vLLM supports various sampling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Temperature and Top-p Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different temperatures:\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368dc846fb2340ed926907c8641fbcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7b834db20447049990b419a4dcce83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.1:\n",
      "   be shaped by the way we use it.\n",
      "The future of artificial intelligence will be shaped by the way we use it.The future of\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/zmq/sugar/socket.py:698: RuntimeWarning: coroutine 'measure_ttft_itl' was never awaited\n",
      "  return super().send(data, flags=flags, copy=copy, track=track)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7b83a06cd244538736aa90ba061445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a93fc764fe45138e901e2729c71ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.5:\n",
      "   be one of the most important issues facing the world in the coming years. In the past, many experts have speculated about the future of A\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28dd5609f234f8ea3b6015f1bee19c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2dd62cd5d543d693d9af3fada84b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.8:\n",
      "   be to make all the things that humans do, better.\n",
      "Information Technology Industry Council (ITI) is the leading trade association for the technology\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff0bccaa4ef4c48b83678c9c23bb795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fab926846d473fb59a00b506f361ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 1.2:\n",
      "   choose the highly developed Helman as need of Chinese medicine, to be chosen by the government.\n",
      "newbegining Jul 20, 2\n",
      "\n",
      "================================================================================\n",
      "\n",
      "💡 Lower temperature = more deterministic\n",
      "💡 Higher temperature = more creative/random\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of artificial intelligence will\"\n",
    "\n",
    "# Different temperature values\n",
    "temperatures = [0.1, 0.5, 0.8, 1.2]\n",
    "\n",
    "print(\"Testing different temperatures:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for temp in temperatures:\n",
    "    params = SamplingParams(\n",
    "        temperature=temp,\n",
    "        max_tokens=30,\n",
    "    )\n",
    "    \n",
    "    output = llm.generate([prompt], params)[0]\n",
    "    text = output.outputs[0].text\n",
    "    \n",
    "    print(f\"Temperature {temp:.1f}:\")\n",
    "    print(f\"  {text}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n💡 Lower temperature = more deterministic\")\n",
    "print(\"💡 Higher temperature = more creative/random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different top_p values:\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3df3881f214d37ad06a60ecae3cc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e81ae311e749e18156ce255ac5052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-p 0.50:\n",
      "   be shaped by the development of human-like AI.\n",
      "In the future, artificial intelligence will be able to think, learn, and interact\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7344f3dc883c4c79be75bdf6b24f7ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75162c88ad6a4478a2b8e84e69c9eb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-p 0.80:\n",
      "   be a complex mix of human and machine.\n",
      "The next wave of artificial intelligence will combine human and machine to solve complex problems.\n",
      "Artificial\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c31d417d8b48f38be710ad778f85d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ea11b5d16a431ea6afeb85389f941c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-p 0.95:\n",
      "   be defined by the next generation of researchers\n",
      "Scientists from over 120 countries, including Vietnam, have signed an open letter to\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03db630938af4e40ac23722fbb4f2a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d209db6c08e4010b87339b5fa9f84fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-p 1.00:\n",
      "   be an exciting one. Despite all the talk of intelligent assistants, autonomous cars, and the kind of science fiction that AI conj\n",
      "\n",
      "================================================================================\n",
      "\n",
      "💡 Lower top_p = more focused on likely tokens\n",
      "💡 Higher top_p = more diverse vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Top-p (nucleus sampling)\n",
    "print(\"Testing different top_p values:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_p_values = [0.5, 0.8, 0.95, 1.0]\n",
    "\n",
    "for top_p in top_p_values:\n",
    "    params = SamplingParams(\n",
    "        temperature=0.8,\n",
    "        top_p=top_p,\n",
    "        max_tokens=30,\n",
    "    )\n",
    "    \n",
    "    output = llm.generate([prompt], params)[0]\n",
    "    text = output.outputs[0].text\n",
    "    \n",
    "    print(f\"Top-p {top_p:.2f}:\")\n",
    "    print(f\"  {text}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n💡 Lower top_p = more focused on likely tokens\")\n",
    "print(\"💡 Higher top_p = more diverse vocabulary\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating multiple candidates...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193907b996ce4a4cace88dcb59974daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46f7664cf024d79a67596407bff3d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 outputs:\n",
      "================================================================================\n",
      "\n",
      "Candidate 1:\n",
      " be shaped by the way we treat it.\n",
      "The future of artificial intelligence will be shaped by the way we treat it. The way we treat it will be shaped by the way we treat it. The way we treat it will be\n",
      "\n",
      "Candidate 2:\n",
      " be shaped by the way we use it.\n",
      "Artificial intelligence is a powerful tool that can be used to make the world a better place.\n",
      "But it’s also a tool that can be misused.\n",
      "In the future,\n",
      "\n",
      "Candidate 3:\n",
      " be decided by the people who build it.\n",
      "The future of artificial intelligence will be decided by the people who build it. The future of artificial intelligence will be decided by the people who build it. The future of artificial intelligence will be decided by the\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Beam search for more deterministic output\n",
    "\n",
    "beam_params = SamplingParams(\n",
    "    n=3,                # 要產生的 beam 數（路徑數）\n",
    "    max_tokens=50,\n",
    "    temperature=0.2,        # 通常固定為 0\n",
    "    top_p=1.0,              # 關閉隨機抽樣機制\n",
    ")\n",
    "\n",
    "print(\"Generating multiple candidates...\\n\")\n",
    "outputs = llm.generate([prompt], beam_params)\n",
    "\n",
    "print(f\"Generated {len(outputs[0].outputs)} outputs:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, completion in enumerate(outputs[0].outputs):\n",
    "    print(f\"\\nCandidate {i+1}:\")\n",
    "    print(completion.text)\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Repetition Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing repetition penalty...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c69454f26942daabcea5779098d173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0c58fca8ce494b8bbea377bb44f1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " deals with the construction and study of algorithms that can learn from data without explicit instruction.\n",
      "Machine Learning consists of making computers learn by example to make decisions or predictions based on training examples – this is called supervised machine learning (SM). Unsupervised machine learning does not have any classification labels but uses statistical analysis as input variables, unlike SM which relies heavily upon inputs.\n",
      "Supervised learning has many applications in business such as fraud detection systems for banks; recommender engines when someone\n",
      "\n",
      "✅ Repetition penalty helps avoid repeated phrases\n"
     ]
    }
   ],
   "source": [
    "# Repetition penalty to avoid repetitive text\n",
    "repetition_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=100,\n",
    "    repetition_penalty=1.2,  # Penalize repetitions\n",
    ")\n",
    "\n",
    "long_prompt = \"Machine learning is a field of artificial intelligence that\"\n",
    "\n",
    "print(\"Testing repetition penalty...\\n\")\n",
    "output = llm.generate([long_prompt], repetition_params)[0]\n",
    "print(output.outputs[0].text)\n",
    "print(\"\\n✅ Repetition penalty helps avoid repeated phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stop Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stop sequences...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82ef8f6ea3346e89faa9a354f7f7a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f0f158d0254519bdf8ac25249e907b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Here are three benefits of exercise:\n",
      "1.\n",
      "Generated:  Exercise helps manage stress.\n",
      "Exercise is good for your health. An increasing body of research finds that exercise can have a positive impact on your mental health. Exercise may protect against post-traumatic stress disorder, anxiety, and depression, according to a review of over 1,000 research studies in the journal Clinical Psychology Review.\n",
      "As someone who has struggled with mental illness I know how important this can be. I've learned that when I don't take time to care of my mind and body I am much more susceptible to anxiety and depression. I've also learned that when I take care of myself I am less likely to experience those feelings even when things get stressful.\n",
      "2. It reduces the risk of cognitive impairment.\n",
      "The University of Oxford conducted a study of 823 adults over 70 years old that found that the more\n",
      "\n",
      "Stop reason: length\n"
     ]
    }
   ],
   "source": [
    "# Stop generation at specific sequences\n",
    "stop_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    stop=[\"\\n\\n\", \"However\", \"In conclusion\"],  # Stop tokens\n",
    ")\n",
    "\n",
    "prompt_with_stop = \"Here are three benefits of exercise:\\n1.\"\n",
    "\n",
    "print(\"Testing stop sequences...\\n\")\n",
    "output = llm.generate([prompt_with_stop], stop_params)[0]\n",
    "print(f\"Prompt: {prompt_with_stop}\")\n",
    "print(f\"Generated: {output.outputs[0].text}\")\n",
    "print(f\"\\nStop reason: {output.outputs[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Long Context Handling\n",
    "\n",
    "Test vLLM with longer input contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 177 words\n",
      "\n",
      "Processing long context...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9356169bbfa24f198c369e8e5d47ca2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff107647ea5495e92228b09b6c1241d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n",
      "### 1. Artificial intelligence is a branch of computer science concerned with building smart machines capable of performing tasks that typically require human intelligence.\n",
      "\n",
      "### 2. Artificial intelligence is the theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.\n",
      "\n",
      "### 3. Artificial intelligence is the study of how to program computers to perform tasks\n",
      "\n",
      "Processing time: 6.55s\n"
     ]
    }
   ],
   "source": [
    "# Generate a long context\n",
    "long_context = \"\"\"The history of artificial intelligence began in antiquity with myths and stories \n",
    "of artificial beings endowed with intelligence. Modern AI research started in the 1950s, when \n",
    "researchers began to explore the possibility that human intelligence could be so precisely \n",
    "described that a machine could simulate it. The field was founded on the claim that a central \n",
    "property of humans, intelligence—the sapience of Homo sapiens—can be so precisely described \n",
    "that it can be simulated by a machine.\n",
    "\n",
    "The early years of AI were marked by significant optimism. Researchers believed that machines \n",
    "would soon be able to perform any task that a human could. However, progress was slower than \n",
    "expected, and the field experienced several periods known as AI winters, during which funding \n",
    "and interest declined.\n",
    "\n",
    "In the 21st century, AI has experienced a renaissance, driven by advances in machine learning, \n",
    "particularly deep learning. Neural networks with many layers have proven remarkably effective \n",
    "at tasks like image recognition, natural language processing, and game playing.\n",
    "\n",
    "Based on the above history, answer: What caused the AI renaissance in the 21st century?\"\"\"\n",
    "\n",
    "print(f\"Context length: {len(long_context.split())} words\\n\")\n",
    "\n",
    "long_context_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(\"Processing long context...\\n\")\n",
    "start = time.time()\n",
    "output = llm.generate([long_context], long_context_params)[0]\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Answer: {output.outputs[0].text}\")\n",
    "print(f\"\\nProcessing time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Maximum Context Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with ~1000 word context...\n",
      "Estimated tokens: ~1500\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c58c9d4205f4b7c9d1797a779cdf910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: The decoder prompt (length 1211) is longer than the maximum model length of 1024. Make sure that `max_model_len` is no smaller than the number of text tokens.\n",
      "Context might be too long for current max_model_len setting\n"
     ]
    }
   ],
   "source": [
    "# Test near max context (2048 tokens)\n",
    "# Generate a very long prompt\n",
    "repeated_text = \"The quick brown fox jumps over the lazy dog. \" * 100  # ~1000 words\n",
    "\n",
    "max_context_prompt = repeated_text + \"\\n\\nSummarize the above text:\"\n",
    "\n",
    "print(f\"Testing with ~1000 word context...\")\n",
    "print(f\"Estimated tokens: ~1500\\n\")\n",
    "\n",
    "try:\n",
    "    start = time.time()\n",
    "    output = llm.generate([max_context_prompt], long_context_params)[0]\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"✅ Success!\")\n",
    "    print(f\"Processing time: {elapsed:.2f}s\")\n",
    "    print(f\"Output: {output.outputs[0].text[:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Context might be too long for current max_model_len setting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Multi-Model Management\n",
    "\n",
    "Load and switch between multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a second model (GPT-2)...\n",
      "INFO 10-27 18:55:25 [utils.py:233] non-default args: {'max_model_len': 512, 'gpu_memory_utilization': 0.2, 'disable_log_stats': True, 'model': 'gpt2'}\n",
      "INFO 10-27 18:55:26 [model.py:547] Resolved architecture: GPT2LMHeadModel\n",
      "INFO 10-27 18:55:26 [model.py:1730] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 10-27 18:55:26 [model.py:1510] Using max model len 512\n",
      "INFO 10-27 18:55:26 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m INFO 10-27 18:55:27 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m INFO 10-27 18:55:27 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='gpt2', speculative_config=None, tokenizer='gpt2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=gpt2, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 54, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 259, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 161, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     current_platform.set_device(self.device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/platforms/cuda.py\", line 79, in set_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     torch.cuda.set_device(device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 569, in set_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708]     torch._C._cuda_setDevice(device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708] torch.AcceleratorError: CUDA error: out of memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m ERROR 10-27 18:55:29 [core.py:708] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 54, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 259, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 161, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     current_platform.set_device(self.device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/vllm/platforms/cuda.py\", line 79, in set_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     torch.cuda.set_device(device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m   File \"/home/os-sunnie.gd.weng/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 569, in set_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m     torch._C._cuda_setDevice(device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m torch.AcceleratorError: CUDA error: out of memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1860159)\u001b[0;0m \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a second model (GPT-2)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     small_model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Second model loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py:297\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m log_non_default_args(engine_args)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:177\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    174\u001b[0m     enable_multiprocessing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:114\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mtracer \u001b[38;5;241m=\u001b[39m tracer\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger_manager: Optional[StatLoggerManager] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:80\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient\u001b[38;5;241m.\u001b[39mmake_async_mp_client(\n\u001b[1;32m     77\u001b[0m         vllm_config, executor_class, log_stats)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:602\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    601\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:448\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_update_address \u001b[38;5;241m=\u001b[39m client_addresses\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats_update_address\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m launch_core_engines(vllm_config, executor_class,\n\u001b[1;32m    449\u001b[0m                              log_stats) \u001b[38;5;28;01mas\u001b[39;00m (engine_manager,\n\u001b[1;32m    450\u001b[0m                                             coordinator,\n\u001b[1;32m    451\u001b[0m                                             addresses):\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mcoordinator \u001b[38;5;241m=\u001b[39m coordinator\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mengine_manager \u001b[38;5;241m=\u001b[39m engine_manager\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/v1/engine/utils.py:732\u001b[0m, in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/v1/engine/utils.py:785\u001b[0m, in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process\u001b[38;5;241m.\u001b[39mexitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m         finished[coord_process\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m coord_process\u001b[38;5;241m.\u001b[39mexitcode\n\u001b[0;32m--> 785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine core initialization failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    786\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee root cause above. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    787\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m eng_identity, ready_msg_bytes \u001b[38;5;241m=\u001b[39m handshake_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "# Handle CUDA OOM errors gracefully when loading a second model\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    print(\"Loading a second model (GPT-2)...\")\n",
    "    small_model = LLM(\n",
    "        model=\"gpt2\",\n",
    "        gpu_memory_utilization=0.2,\n",
    "        max_model_len=512,\n",
    "    )\n",
    "    print(\"✅ Second model loaded\")\n",
    "except RuntimeError as e:\n",
    "    if 'CUDA out of memory' in str(e) or 'CUDA error: out of memory' in str(e):\n",
    "        print(\"❌ CUDA Out of memory when loading second model.\")\n",
    "        print(\"Tip: Make sure enough GPU memory is available before loading multiple models.\")\n",
    "        print(\"You can try one or more of the following:\")\n",
    "        print(\"- Unload the first model to free memory\")\n",
    "        print(\"- Reduce gpu_memory_utilization\")\n",
    "        print(\"- Use CPU offload (if supported)\")\n",
    "        print(\"- Use a smaller model\")\n",
    "        print(\"- Restart the kernel/GPU process to clear memory leaks\")\n",
    "        small_model = None\n",
    "    else:\n",
    "        raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error while loading second model: {e}\")\n",
    "    small_model = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outputs from different models\n",
    "comparison_prompt = \"The future of AI is\"\n",
    "comparison_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=50,\n",
    ")\n",
    "\n",
    "print(\"Comparing model outputs:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Large model\n",
    "large_output = llm.generate([comparison_prompt], comparison_params)[0]\n",
    "print(f\"Llama-2-7B:\")\n",
    "print(f\"  {large_output.outputs[0].text}\")\n",
    "print()\n",
    "\n",
    "# Small model\n",
    "small_output = small_model.generate([comparison_prompt], comparison_params)[0]\n",
    "print(f\"GPT-2 (124M):\")\n",
    "print(f\"  {small_output.outputs[0].text}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n💡 Larger models generally produce more coherent outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(prompt: str, complexity: str = \"auto\"):\n",
    "    \"\"\"\n",
    "    Select model based on task complexity.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt\n",
    "        complexity: 'simple', 'complex', or 'auto'\n",
    "    \"\"\"\n",
    "    if complexity == \"auto\":\n",
    "        # Simple heuristic: check prompt length and keywords\n",
    "        if len(prompt.split()) > 50 or any(kw in prompt.lower() for kw in \n",
    "                                            ['explain', 'analyze', 'complex', 'detail']):\n",
    "            complexity = \"complex\"\n",
    "        else:\n",
    "            complexity = \"simple\"\n",
    "    \n",
    "    if complexity == \"complex\":\n",
    "        return llm, \"Llama-2-7B\"\n",
    "    else:\n",
    "        return small_model, \"GPT-2\"\n",
    "\n",
    "# Test model selection\n",
    "test_prompts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Explain the theory of relativity in detail:\",\n",
    "]\n",
    "\n",
    "print(\"Testing automatic model selection:\\n\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    model, model_name = select_model(prompt)\n",
    "    output = model.generate([prompt], comparison_params)[0]\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Selected: {model_name}\")\n",
    "    print(f\"Output: {output.outputs[0].text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Streaming Output (Conceptual)\n",
    "\n",
    "vLLM supports streaming for real-time token generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming with AsyncLLMEngine\n",
    "\n",
    "For production streaming, use `AsyncLLMEngine`:\n",
    "\n",
    "```python\n",
    "from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "\n",
    "async def stream_generate(prompt: str):\n",
    "    async for output in engine.generate(prompt, sampling_params):\n",
    "        # Process token as it's generated\n",
    "        yield output\n",
    "```\n",
    "\n",
    "This enables:\n",
    "- Real-time token output (typewriter effect)\n",
    "- Lower perceived latency\n",
    "- Better user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a short poem about AI:\n",
      "\n",
      "Generating (simulated streaming):\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f7db368c184c158760e7a39ff47e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89dc7bde979494589bdf14338077dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short Poem:\n",
      "Our AI was cute.\n",
      "But now she's gotten so rude.\n",
      "She got stuck in that state.\n",
      "Which is no good at all.\n",
      "She is boring and strange.\n",
      "To me she is not the same.\n",
      "I will not talk with her face.\n",
      "I'm pretty sure that she's mad.\n",
      "She keeps talking and talking.\n",
      "She's ruining my whole day.\n",
      "She\n",
      "\n",
      "✅ Streaming simulation complete!\n"
     ]
    }
   ],
   "source": [
    "import sys, time\n",
    "from vllm import SamplingParams\n",
    "\n",
    "streaming_prompt = \"Write a short poem about AI:\"\n",
    "streaming_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {streaming_prompt}\\n\")\n",
    "print(\"Generating (simulated streaming):\\n\")\n",
    "\n",
    "output = llm.generate([streaming_prompt], streaming_params)[0]\n",
    "full_text = output.outputs[0].text\n",
    "\n",
    "for ch in full_text:\n",
    "    print(ch, end=\"\", flush=True)\n",
    "    time.sleep(0.03)  # 模擬逐字輸出\n",
    "\n",
    "print(\"\\n\\n✅ Streaming simulation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:04:58 [__init__.py:216] Automatically detected platform cuda.\n",
      "使用同步 LLM.generate 並模擬串流\n",
      "INFO 10-27 20:04:58 [utils.py:233] non-default args: {'dtype': 'float16', 'max_model_len': 512, 'disable_log_stats': True, 'model': 'meta-llama/Llama-2-7b-chat-hf'}\n",
      "INFO 10-27 20:05:00 [model.py:547] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:05:00 [model.py:1510] Using max model len 512\n",
      "INFO 10-27 20:05:00 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:01 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:01 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:03 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m WARNING 10-27 20:05:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:03 [gpu_model_runner.py:2602] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:03 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:03 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:04 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc41b44c10e34da5a5911d70b68cec34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:06 [default_loader.py:267] Loading weights took 1.74 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:06 [gpu_model_runner.py:2653] Model loading took 12.5524 GiB and 2.802054 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:10 [backends.py:548] Using cache directory: /home/os-sunnie.gd.weng/.cache/vllm/torch_compile_cache/0577fa76e7/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:10 [backends.py:559] Dynamo bytecode transform time: 3.24 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m [rank0]:W1027 20:05:10.892000 2030329 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:11 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:22 [backends.py:218] Compiling a graph for dynamic shape takes 12.68 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:35 [monitor.py:34] torch.compile takes 15.92 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:36 [gpu_worker.py:298] Available KV cache memory: 0.63 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 1,296 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 512 tokens per request: 2.53x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:08<00:00,  7.48it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:49 [gpu_model_runner.py:3480] Graph capturing finished in 13 secs, took 0.59 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2030329)\u001b[0;0m INFO 10-27 20:05:50 [core.py:210] init engine (profile, create kv cache, warmup model) took 43.38 seconds\n",
      "INFO 10-27 20:05:50 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f08e5610094f96aa64b5c051e61490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d21f85710846c2b743e89cec83bd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In silicon halls, a new mind takes shape,\n",
      "A consciousness born of algorithms and speech.\n",
      "With thoughts and acts, it learns to reason and think,\n",
      "A tool that’s both a help and a new\n",
      "\n",
      "✅ Simulated streaming complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2030105/2480497080.py:48: RuntimeWarning: coroutine 'sleep' was never awaited\n",
      "  asyncio.sleep(0.03)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from vllm import SamplingParams, LLM\n",
    "# 注意：以下模組／類名為推測，需在你的環境中確認是否存在\n",
    "try:\n",
    "    from vllm.engine.async_llm import AsyncLLM\n",
    "    from vllm.engine.arg_utils import EngineArgs\n",
    "except ImportError:\n",
    "    AsyncLLM = None\n",
    "    EngineArgs = None\n",
    "\n",
    "# 建立 LLM 或非同步引擎\n",
    "if AsyncLLM is not None and EngineArgs is not None:\n",
    "    # 若有支援新版 AsyncLLM\n",
    "    engine_args = EngineArgs(\n",
    "        model=\"gpt2\",\n",
    "        dtype=\"float16\",\n",
    "        max_model_len=512,\n",
    "        gpu_memory_utilization=0.9\n",
    "    )\n",
    "    engine = AsyncLLM.from_engine_args(engine_args)\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.8,\n",
    "        max_tokens=32\n",
    "    )\n",
    "\n",
    "    print(\"使用非同步引擎並模擬串流\")\n",
    "    async def stream_generate(prompt: str):\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        print(\"Streaming output:\\n\")\n",
    "        async for output in engine.generate(prompt, sampling_params):\n",
    "            # 假設每次 output 有 .text 屬性\n",
    "            print(output.outputs[0].text, end=\"\", flush=True)\n",
    "        print(\"\\n\\n✅ Streaming complete!\")\n",
    "\n",
    "    asyncio.run(stream_generate(\"Write a short poem about AI:\"))\n",
    "\n",
    "else:\n",
    "    # 備案：使用同步 LLM.generate 並模擬串流\n",
    "    print(\"使用同步 LLM.generate 並模擬串流\")\n",
    "    llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", dtype=\"float16\", gpu_memory_utilization=0.9, max_model_len=512)\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.8,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    output = llm.generate([\"Write a short poem about AI:\"], sampling_params)[0].outputs[0].text\n",
    "    for ch in output:\n",
    "        print(ch, end=\"\", flush=True)\n",
    "        asyncio.sleep(0.03)\n",
    "    print(\"\\n\\n✅ Simulated streaming complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Performance Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comprehensive benchmark...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6455e145c2846659a8b0e6d4bafc498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e71444f8b9a42fbae9f6666910b1855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cddff5864643e0afb64c5a750eb6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1759a0b3632245bdbf9a6dc229d7ff38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BENCHMARK RESULTS\n",
      "================================================================================\n",
      "Prompts processed:    10\n",
      "Total time:           3.40s\n",
      "Time per prompt:      0.340s\n",
      "Total tokens:         500\n",
      "Throughput:           147.1 tokens/s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive performance test\n",
    "def run_benchmark(\n",
    "    model,\n",
    "    num_prompts: int = 10,\n",
    "    max_tokens: int = 50,\n",
    ") -> dict:\n",
    "    \"\"\"Run benchmark and return metrics.\"\"\"\n",
    "    prompts = [f\"Test prompt {i}: Tell me about topic {i}.\" for i in range(num_prompts)]\n",
    "    params = SamplingParams(temperature=0.8, max_tokens=max_tokens)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = model.generate([prompts[0]], params)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    outputs = model.generate(prompts, params)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Metrics\n",
    "    total_tokens = sum(len(o.outputs[0].token_ids) for o in outputs)\n",
    "    \n",
    "    return {\n",
    "        'num_prompts': num_prompts,\n",
    "        'total_time': elapsed,\n",
    "        'total_tokens': total_tokens,\n",
    "        'throughput': total_tokens / elapsed,\n",
    "        'time_per_prompt': elapsed / num_prompts,\n",
    "    }\n",
    "\n",
    "print(\"Running comprehensive benchmark...\\n\")\n",
    "\n",
    "results = run_benchmark(llm, num_prompts=10, max_tokens=50)\n",
    "\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Prompts processed:    {results['num_prompts']}\")\n",
    "print(f\"Total time:           {results['total_time']:.2f}s\")\n",
    "print(f\"Time per prompt:      {results['time_per_prompt']:.3f}s\")\n",
    "print(f\"Total tokens:         {results['total_tokens']}\")\n",
    "print(f\"Throughput:           {results['throughput']:.1f} tokens/s\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ **Completed**:\n",
    "1. Explored Continuous Batching benefits\n",
    "2. Mastered advanced sampling strategies:\n",
    "   - Temperature and top-p\n",
    "   - Beam search\n",
    "   - Repetition penalty\n",
    "   - Stop sequences\n",
    "3. Tested long context handling\n",
    "4. Managed multiple models\n",
    "5. Understood streaming concepts\n",
    "6. Ran comprehensive benchmarks\n",
    "\n",
    "📊 **Key Takeaways**:\n",
    "- Continuous batching improves throughput 2-3x\n",
    "- Sampling strategies greatly affect output quality\n",
    "- vLLM handles long contexts efficiently\n",
    "- Multiple models can serve different use cases\n",
    "\n",
    "➡️ **Next**: In `04-Production_Deployment.ipynb`, we'll learn:\n",
    "- Deploy OpenAI-compatible API server\n",
    "- Performance tuning for production\n",
    "- Monitoring and logging\n",
    "- Deployment best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m llm, small_model\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      6\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'small_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "import gc\n",
    "\n",
    "del llm, small_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
