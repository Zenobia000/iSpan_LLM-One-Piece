{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1 Part 1: Triton Inference Server 設置與安裝\n",
    "\n",
    "## 🎯 學習目標\n",
    "- 理解 Triton Inference Server 架構\n",
    "- 安裝和配置 Triton Server\n",
    "- 驗證基礎推理功能\n",
    "- 了解 REST 和 gRPC API\n",
    "\n",
    "## ⏱️ 預估時間: 60-90 分鐘\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境驗證與準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Triton Server 環境檢查\n",
      "==================================================\n",
      "Python 版本: 3.10.12\n",
      "PyTorch 版本: 2.8.0+cu128\n",
      "CUDA 可用: True\n",
      "CUDA 版本: 12.8\n",
      "GPU 數量: 1\n",
      "  GPU 0: NVIDIA RTX 2000 Ada Generation (16.7 GB)\n",
      "    計算能力: 8.9\n"
     ]
    }
   ],
   "source": [
    "# 檢查系統環境\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"🔍 Triton Server 環境檢查\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 基礎環境\n",
    "print(f\"Python 版本: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "    print(f\"GPU 數量: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        memory_gb = gpu.total_memory / 1e9\n",
    "        print(f\"  GPU {i}: {gpu.name} ({memory_gb:.1f} GB)\")\n",
    "        print(f\"    計算能力: {gpu.major}.{gpu.minor}\")\n",
    "else:\n",
    "    print(\"⚠️ 警告: 未檢測到 CUDA GPU\")\n",
    "    print(\"Triton Server 需要 GPU 以獲得最佳性能\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Docker: Docker version 28.4.0, build d8eb465\n",
      "✅ Docker 權限正常\n",
      "✅ Docker GPU 支援: 檢測到 1 個 GPU\n"
     ]
    }
   ],
   "source": [
    "# 檢查 Docker 環境\n",
    "def check_docker():\n",
    "    \"\"\"檢查 Docker 是否安裝並支援 GPU\"\"\"\n",
    "    try:\n",
    "        # 檢查 Docker 版本\n",
    "        result = subprocess.run(['docker', '--version'], \n",
    "                               capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Docker: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(\"❌ Docker 未安裝\")\n",
    "            return False\n",
    "            \n",
    "        # 檢查 Docker 權限\n",
    "        result = subprocess.run(['docker', 'ps'], \n",
    "                               capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Docker 權限正常\")\n",
    "        else:\n",
    "            print(\"⚠️ Docker 權限問題，可能需要 sudo\")\n",
    "            \n",
    "        # 檢查 NVIDIA Container Toolkit\n",
    "        result = subprocess.run(['docker', 'run', '--rm', '--gpus', 'all', \n",
    "                               'nvidia/cuda:11.0.3-base-ubuntu20.04', \n",
    "                               'nvidia-smi', '--list-gpus'], \n",
    "                               capture_output=True, text=True, timeout=30)\n",
    "        if result.returncode == 0:\n",
    "            gpu_count = len(result.stdout.strip().split('\\n'))\n",
    "            print(f\"✅ Docker GPU 支援: 檢測到 {gpu_count} 個 GPU\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Docker GPU 支援未配置\")\n",
    "            print(\"請安裝 NVIDIA Container Toolkit\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"⏱️ Docker GPU 檢查超時\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Docker 命令未找到\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Docker 檢查錯誤: {e}\")\n",
    "        return False\n",
    "\n",
    "docker_ready = check_docker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Triton Server 安裝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 安裝 Triton 客戶端庫...\n",
      "❌ tritonclient 未安裝\n",
      "\n",
      "安裝命令:\n",
      "  pip install tritonclient[all]\n",
      "\n",
      "請在終端執行上述命令後重新運行此 cell\n",
      "❌ Triton Server 映像未找到\n",
      "\n",
      "下載命令:\n",
      "  docker pull nvcr.io/nvidia/tritonserver:25.05-py3\n",
      "\n",
      "⏳ 下載 Triton Server 映像 (這可能需要幾分鐘)...\n",
      "\n",
      "在終端執行:\n",
      " docker pull nvcr.io/nvidia/tritonserver:25.05-py3\n"
     ]
    }
   ],
   "source": [
    "# 安裝 Triton 客戶端庫\n",
    "print(\"📦 安裝 Triton 客戶端庫...\")\n",
    "\n",
    "try:\n",
    "    import tritonclient.http as httpclient\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    print(\"✅ tritonclient 已安裝\")\n",
    "except ImportError:\n",
    "    print(\"❌ tritonclient 未安裝\")\n",
    "    print(\"\\n安裝命令:\")\n",
    "    print(\"  pip install tritonclient[all]\")\n",
    "    print(\"\\n請在終端執行上述命令後重新運行此 cell\")\n",
    "\n",
    "# 檢查 Triton Server Docker 映像\n",
    "def check_triton_image():\n",
    "    \"\"\"檢查 Triton Server Docker 映像\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['docker', 'images', 'nvcr.io/nvidia/tritonserver'], \n",
    "                               capture_output=True, text=True)\n",
    "        \n",
    "        if 'tritonserver' in result.stdout:\n",
    "            print(\"✅ Triton Server 映像已存在\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Triton Server 映像未找到\")\n",
    "            print(\"\\n下載命令:\")\n",
    "            print(\"docker pull nvcr.io/nvidia/tritonserver:25.05-py3\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 檢查映像時出錯: {e}\")\n",
    "        return False\n",
    "\n",
    "triton_image_ready = check_triton_image()\n",
    "\n",
    "if not triton_image_ready:\n",
    "    print(\"\\n⏳ 下載 Triton Server 映像 (這可能需要幾分鐘)...\")\n",
    "    print(\"\\n在終端執行:\")\n",
    "    print(\" docker pull nvcr.io/nvidia/tritonserver:25.05-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 模型倉庫準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 創建模型倉庫: /home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/02-Efficient_Inference_and_Serving/02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics/model_repository\n",
      "  ✅ model_repository\n",
      "  ✅ model_repository/simple_pytorch_model\n",
      "  ✅ model_repository/simple_pytorch_model/1\n",
      "  ✅ model_repository/simple_pytorch_model/config\n",
      "\n",
      "📋 標準模型倉庫結構:\n",
      "\n",
      "    model_repository/\n",
      "    └── simple_pytorch_model/          # 模型名稱\n",
      "        ├── config.pbtxt               # 模型配置文件\n",
      "        ├── 1/                         # 版本號\n",
      "        │   └── model.pt               # 模型權重文件\n",
      "        └── config/                    # 額外配置 (可選)\n",
      "    \n",
      "\n",
      "💡 Triton 模型倉庫設計原則:\n",
      "• 每個模型有獨立的目錄\n",
      "• 版本號目錄包含模型檔案\n",
      "• config.pbtxt 定義模型配置\n",
      "• 支援多版本並行部署\n"
     ]
    }
   ],
   "source": [
    "# 創建 Triton 模型倉庫結構\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 設置工作目錄\n",
    "work_dir = Path.cwd()\n",
    "model_repo_dir = work_dir / \"model_repository\"\n",
    "\n",
    "print(f\"📁 創建模型倉庫: {model_repo_dir}\")\n",
    "\n",
    "# 創建模型倉庫結構\n",
    "def create_model_repository():\n",
    "    \"\"\"\n",
    "    創建標準的 Triton 模型倉庫結構\n",
    "    \"\"\"\n",
    "    # 基礎目錄結構\n",
    "    directories = [\n",
    "        \"model_repository\",\n",
    "        \"model_repository/simple_pytorch_model\",\n",
    "        \"model_repository/simple_pytorch_model/1\",\n",
    "        \"model_repository/simple_pytorch_model/config\",\n",
    "    ]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"  ✅ {dir_path}\")\n",
    "    \n",
    "    print(f\"\\n📋 標準模型倉庫結構:\")\n",
    "    print(\"\"\"\n",
    "    model_repository/\n",
    "    └── simple_pytorch_model/          # 模型名稱\n",
    "        ├── config.pbtxt               # 模型配置文件\n",
    "        ├── 1/                         # 版本號\n",
    "        │   └── model.pt               # 模型權重文件\n",
    "        └── config/                    # 額外配置 (可選)\n",
    "    \"\"\")\n",
    "\n",
    "create_model_repository()\n",
    "\n",
    "print(\"\\n💡 Triton 模型倉庫設計原則:\")\n",
    "print(\"• 每個模型有獨立的目錄\")\n",
    "print(\"• 版本號目錄包含模型檔案\")\n",
    "print(\"• config.pbtxt 定義模型配置\")\n",
    "print(\"• 支援多版本並行部署\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 創建測試用 PyTorch 模型...\n",
      "示例輸入形狀: torch.Size([1, 10])\n",
      "模型輸出形狀: torch.Size([1, 3])\n",
      "輸出機率: [[0.398 0.299 0.303]]\n",
      "✅ 模型已保存到: /home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/02-Efficient_Inference_and_Serving/02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics/model_repository/simple_pytorch_model/1/model.pt\n",
      "✅ 模型大小: 0.53 MB\n"
     ]
    }
   ],
   "source": [
    "# 創建簡單的 PyTorch 模型用於測試\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SimpleTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    簡單的文本分類模型，用於 Triton 部署測試\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=1000, embed_dim=128, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        x = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]\n",
    "        x = torch.mean(x, dim=1)       # [batch_size, embed_dim] - 簡單的平均池化\n",
    "        x = torch.relu(self.fc1(x))    # [batch_size, 64]\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)           # [batch_size, num_classes]\n",
    "        return torch.softmax(output, dim=-1)\n",
    "\n",
    "# 創建和保存模型\n",
    "print(\"🤖 創建測試用 PyTorch 模型...\")\n",
    "\n",
    "model = SimpleTextClassifier(vocab_size=1000, embed_dim=128, num_classes=3)\n",
    "model.eval()\n",
    "\n",
    "# 創建示例輸入\n",
    "example_input = torch.randint(0, 1000, (1, 10))  # batch=1, seq_l en=10\n",
    "print(f\"示例輸入形狀: {example_input.shape}\")\n",
    "\n",
    "# 測試模型\n",
    "with torch.no_grad():\n",
    "    output = model(example_input)\n",
    "    print(f\"模型輸出形狀: {output.shape}\")\n",
    "    print(f\"輸出機率: {output.numpy().round(3)}\")\n",
    "\n",
    "# 轉換為 TorchScript (Triton 推薦格式)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "# 保存模型到倉庫\n",
    "model_path = model_repo_dir / \"simple_pytorch_model\" / \"1\" / \"model.pt\"\n",
    "traced_model.save(str(model_path))\n",
    "\n",
    "print(f\"✅ 模型已保存到: {model_path}\")\n",
    "print(f\"✅ 模型大小: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型配置文件已創建: /home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/02-Efficient_Inference_and_Serving/02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics/model_repository/simple_pytorch_model/config.pbtxt\n",
      "\n",
      "📂 模型倉庫結構:\n",
      "model_repository/\n",
      "  simple_pytorch_model/\n",
      "    config.pbtxt (0.6KB)\n",
      "    1/\n",
      "      model.pt (543.6KB)\n",
      "    config/\n"
     ]
    }
   ],
   "source": [
    "# 創建 Triton 模型配置文件\n",
    "config_content = '''\n",
    "name: \"simple_pytorch_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ 10 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "# 動態批次處理配置\n",
    "dynamic_batching {\n",
    "  # 偏好的批次大小\n",
    "  preferred_batch_size: [ 4, 8, 16 ]\n",
    "  \n",
    "  # 最大等待時間 (微秒)\n",
    "  max_queue_delay_microseconds: 5000\n",
    "  \n",
    "  # 保持順序\n",
    "  preserve_ordering: true\n",
    "}\n",
    "\n",
    "# 實例組配置 (可選)\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]\n",
    "\n",
    "# 版本策略\n",
    "version_policy: { latest { num_versions: 1}}\n",
    "'''\n",
    "\n",
    "# 保存配置文件\n",
    "config_path = model_repo_dir / \"simple_pytorch_model\" / \"config.pbtxt\"\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_content.strip())\n",
    "\n",
    "print(f\"✅ 模型配置文件已創建: {config_path}\")\n",
    "\n",
    "# 檢查模型倉庫結構\n",
    "print(f\"\\n📂 模型倉庫結構:\")\n",
    "for root, dirs, files in os.walk(model_repo_dir):\n",
    "    level = root.replace(str(model_repo_dir), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        size = os.path.getsize(os.path.join(root, file))\n",
    "        size_str = f\"{size/1024:.1f}KB\" if size < 1024*1024 else f\"{size/1024/1024:.1f}MB\"\n",
    "        print(f\"{subindent}{file} ({size_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 啟動 Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 啟動 Triton Server...\n",
      "模型倉庫路徑: /home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/02-Efficient_Inference_and_Serving/02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics/model_repository\n",
      "✅ Triton Server 容器已啟動: 0c8e6a7be1fd\n",
      "⏳ 等待服務就緒...\n",
      "✅ Triton Server 就緒! (耗時 2 秒)\n",
      "\n",
      "🌐 Triton Server 端點:\n",
      "  • HTTP API: http://localhost:8000\n",
      "  • gRPC API: localhost:8001\n",
      "  • Metrics: http://localhost:8002/metrics\n",
      "  • Health: http://localhost:8000/v2/health/ready\n"
     ]
    }
   ],
   "source": [
    "# 啟動 Triton Server (Docker 容器)\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def start_triton_server():\n",
    "    \"\"\"\n",
    "    啟動 Triton Inference Server\n",
    "    \"\"\"\n",
    "    # Triton Server Docker 命令\n",
    "    model_repo_path = str(model_repo_dir.absolute())\n",
    "    \n",
    "    docker_cmd = [\n",
    "        'docker', 'run', '--rm', '-d',\n",
    "        '--name', 'triton-server',\n",
    "        '--gpus', 'all',\n",
    "        '-p', '8000:8000',  # HTTP port\n",
    "        '-p', '8001:8001',  # gRPC port  \n",
    "        '-p', '8002:8002',  # Metrics port\n",
    "        '-v', f'{model_repo_path}:/models',\n",
    "        'nvcr.io/nvidia/tritonserver:25.05-py3',\n",
    "        'tritonserver',\n",
    "        '--model-repository=/models',\n",
    "        '--strict-model-config=false',\n",
    "        '--log-verbose=1'\n",
    "    ]\n",
    "\n",
    "    print(\"🚀 啟動 Triton Server...\")\n",
    "    print(f\"模型倉庫路徑: {model_repo_path}\")\n",
    "    \n",
    "    try:\n",
    "        # 停止現有容器 (如果存在)\n",
    "        subprocess.run(['docker', 'stop', 'triton-server'], \n",
    "                      capture_output=True, text=True)\n",
    "        \n",
    "        # 啟動新容器\n",
    "        result = subprocess.run(docker_cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            container_id = result.stdout.strip()\n",
    "            print(f\"✅ Triton Server 容器已啟動: {container_id[:12]}\")\n",
    "            \n",
    "            # 等待服務啟動\n",
    "            print(\"⏳ 等待服務就緒...\")\n",
    "            \n",
    "            for attempt in range(30):  # 最多等待 30 秒\n",
    "                time.sleep(1)\n",
    "                try:\n",
    "                    response = requests.get('http://localhost:8000/v2/health/ready', \n",
    "                                          timeout=2)\n",
    "                    if response.status_code == 200:\n",
    "                        print(f\"✅ Triton Server 就緒! (耗時 {attempt+1} 秒)\")\n",
    "                        return True\n",
    "                except requests.exceptions.RequestException:\n",
    "                    pass\n",
    "                    \n",
    "                if attempt % 5 == 4:\n",
    "                    print(f\"   仍在啟動中... ({attempt+1}/30)\")\n",
    "            \n",
    "            print(\"❌ Triton Server 啟動超時\")\n",
    "            return False\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ 啟動失敗: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 啟動異常: {e}\")\n",
    "        return False\n",
    "\n",
    "server_ready = start_triton_server()\n",
    "\n",
    "if server_ready:\n",
    "    print(\"\\n🌐 Triton Server 端點:\")\n",
    "    print(\"  • HTTP API: http://localhost:8000\")\n",
    "    print(\"  • gRPC API: localhost:8001\")\n",
    "    print(\"  • Metrics: http://localhost:8002/metrics\")\n",
    "    print(\"  • Health: http://localhost:8000/v2/health/ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Triton API 基礎測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Triton Server 基礎測試\n",
      "========================================\n",
      "✅ Triton Server 健康檢查通過\n",
      "✅ 服務器版本: 2.58.0\n",
      "✅ 服務器名稱: triton\n",
      "📋 已載入模型數量: 1\n",
      "  • simple_pytorch_model v1: READY\n"
     ]
    }
   ],
   "source": [
    "# 測試 Triton Server HTTP API\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "TRITON_HTTP_URL = \"http://localhost:8000\"\n",
    "\n",
    "def test_server_health():\n",
    "    \"\"\"\n",
    "    測試服務器健康狀態\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 檢查服務器健康\n",
    "        health_url = f\"{TRITON_HTTP_URL}/v2/health/ready\"\n",
    "        response = requests.get(health_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Triton Server 健康檢查通過\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ 健康檢查失敗: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 健康檢查異常: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_server_metadata():\n",
    "    \"\"\"\n",
    "    獲取服務器元數據\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metadata_url = f\"{TRITON_HTTP_URL}/v2\"\n",
    "        response = requests.get(metadata_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            metadata = response.json()\n",
    "            print(f\"✅ 服務器版本: {metadata.get('version', 'Unknown')}\")\n",
    "            print(f\"✅ 服務器名稱: {metadata.get('name', 'Unknown')}\")\n",
    "            return metadata\n",
    "        else:\n",
    "            print(f\"❌ 獲取元數據失敗: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 元數據查詢異常: {e}\")\n",
    "        return None\n",
    "\n",
    "def list_models():\n",
    "    try:\n",
    "        # 使用正確的端點和方法\n",
    "        models_url = f\"{TRITON_HTTP_URL}/v2/repository/index\"\n",
    "        response = requests.post(models_url)  # 改為 POST\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(f\"📋 已載入模型數量: {len(models)}\")\n",
    "\n",
    "            for model in models:\n",
    "                model_name = model['name']\n",
    "                model_version = model['version']\n",
    "                model_state = model['state']\n",
    "                print(f\"  • {model_name} v{model_version}: {model_state}\")\n",
    "\n",
    "            return models\n",
    "        else:\n",
    "            print(f\"❌ 模型列表獲取失敗: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 模型查詢異常: {e}\")\n",
    "        return None\n",
    "\n",
    "# 執行基礎測試\n",
    "print(\"🧪 Triton Server 基礎測試\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if test_server_health():\n",
    "    metadata = get_server_metadata()\n",
    "    models = list_models()\n",
    "    \n",
    "    if not models:\n",
    "        print(\"\\n⚠️ 沒有模型被載入，檢查模型倉庫配置\")\n",
    "else:\n",
    "    print(\"\\n❌ Triton Server 未正常運行\")\n",
    "    print(\"請檢查 Docker 容器狀態: docker logs triton-server\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tritonclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtritonclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhttpclient\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtritonclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tritonclient'"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import tritonclient.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ tritonclient 未安裝\n",
      "安裝命令: pip install tritonclient[all]\n"
     ]
    }
   ],
   "source": [
    "# 使用 Triton 客戶端進行推理測試\n",
    "if 'tritonclient' in globals() or True:  # 假設已安裝\n",
    "    try:\n",
    "        import tritonclient.http as httpclient\n",
    "        import tritonclient.utils as utils\n",
    "        \n",
    "        print(\"🎯 Triton 客戶端推理測試\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # 創建客戶端\n",
    "        triton_client = httpclient.InferenceServerClient(\n",
    "            url='localhost:8000',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # 檢查模型是否就緒\n",
    "        model_name = \"simple_pytorch_model\"\n",
    "        \n",
    "        if triton_client.is_model_ready(model_name):\n",
    "            print(f\"✅ 模型 '{model_name}' 就緒\")\n",
    "            \n",
    "            # 獲取模型元數據\n",
    "            model_metadata = triton_client.get_model_metadata(model_name)\n",
    "            print(f\"✅ 模型版本: {model_metadata.versions}\")\n",
    "            print(f\"✅ 模型平台: {model_metadata.platform}\")\n",
    "            \n",
    "            # 準備推理數據\n",
    "            input_data = np.random.randint(0, 1000, (1, 10), dtype=np.int64)\n",
    "            print(f\"📊 輸入數據形狀: {input_data.shape}\")\n",
    "            \n",
    "            # 創建推理請求\n",
    "            inputs = [\n",
    "                httpclient.InferInput(\"INPUT__0\", input_data.shape, \"INT64\")\n",
    "            ]\n",
    "            inputs[0].set_data_from_numpy(input_data)\n",
    "            \n",
    "            outputs = [\n",
    "                httpclient.InferRequestedOutput(\"OUTPUT__0\")\n",
    "            ]\n",
    "            \n",
    "            # 執行推理\n",
    "            start_time = time.time()\n",
    "            response = triton_client.infer(\n",
    "                model_name=model_name,\n",
    "                inputs=inputs,\n",
    "                outputs=outputs\n",
    "            )\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # 獲取結果\n",
    "            output_data = response.as_numpy(\"OUTPUT__0\")\n",
    "            \n",
    "            print(f\"\\n🎯 推理結果:\")\n",
    "            print(f\"  輸入: {input_data.flatten()[:5]}... (顯示前5個)\")\n",
    "            print(f\"  輸出機率: {output_data.round(4)}\")\n",
    "            print(f\"  預測類別: {np.argmax(output_data)}\")\n",
    "            print(f\"  推理時間: {inference_time*1000:.2f} ms\")\n",
    "            \n",
    "            print(\"\\n✅ Triton 推理測試成功!\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ 模型 '{model_name}' 未就緒\")\n",
    "            print(\"檢查模型配置和倉庫結構\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"❌ tritonclient 未安裝\")\n",
    "        print(\"安裝命令: pip install tritonclient[all]\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 推理測試失敗: {e}\")\n",
    "        print(\"\\n📋 故障排除建議:\")\n",
    "        print(\"1. 檢查 Triton Server 是否運行: docker ps\")\n",
    "        print(\"2. 查看服務器日誌: docker logs triton-server\")\n",
    "        print(\"3. 驗證模型配置文件語法\")\n",
    "        print(\"4. 確認模型文件存在且可讀取\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. REST API 詳細探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索 Triton REST API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def explore_triton_api():\n",
    "    \"\"\"\n",
    "    探索 Triton Server REST API 端點\n",
    "    \"\"\"\n",
    "    base_url = \"http://localhost:8000\"\n",
    "    \n",
    "    endpoints = {\n",
    "        \"服務器元數據\": \"/v2\",\n",
    "        \"健康檢查 (Live)\": \"/v2/health/live\",\n",
    "        \"健康檢查 (Ready)\": \"/v2/health/ready\",\n",
    "        \"模型列表\": \"/v2/models\",\n",
    "        \"模型元數據\": \"/v2/models/simple_pytorch_model\",\n",
    "        \"模型配置\": \"/v2/models/simple_pytorch_model/config\",\n",
    "        \"模型統計\": \"/v2/models/simple_pytorch_model/stats\",\n",
    "        \"服務器統計\": \"/v2/models/stats\",\n",
    "    }\n",
    "    \n",
    "    print(\"🌐 Triton REST API 探索\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for description, endpoint in endpoints.items():\n",
    "        try:\n",
    "            url = base_url + endpoint\n",
    "            response = requests.get(url, timeout=5)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(f\"\\n✅ {description}:\")\n",
    "                print(f\"   URL: {endpoint}\")\n",
    "                \n",
    "                data = response.json()\n",
    "                \n",
    "                # 顯示關鍵信息\n",
    "                if \"version\" in data:\n",
    "                    print(f\"   版本: {data['version']}\")\n",
    "                    \n",
    "                if \"name\" in data:\n",
    "                    print(f\"   名稱: {data['name']}\")\n",
    "                    \n",
    "                if \"platform\" in data:\n",
    "                    print(f\"   平台: {data['platform']}\")\n",
    "                    \n",
    "                if \"max_batch_size\" in data:\n",
    "                    print(f\"   最大批次: {data['max_batch_size']}\")\n",
    "                    \n",
    "                # 顯示響應大小\n",
    "                response_size = len(response.text)\n",
    "                print(f\"   響應大小: {response_size} bytes\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"❌ {description}: HTTP {response.status_code}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ {description}: 網路錯誤 - {e}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"⚠️ {description}: 非 JSON 響應\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {description}: 未知錯誤 - {e}\")\n",
    "\n",
    "explore_triton_api()\n",
    "\n",
    "print(\"\\n📚 API 文檔參考:\")\n",
    "print(\"• Triton HTTP API: https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md\")\n",
    "print(\"• gRPC API: https://github.com/triton-inference-server/common/blob/main/protobuf/grpc_service.proto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批次推理測試\n",
    "def test_batch_inference():\n",
    "    \"\"\"\n",
    "    測試 Triton 動態批次處理\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import tritonclient.http as httpclient\n",
    "        \n",
    "        triton_client = httpclient.InferenceServerClient(url='localhost:8000')\n",
    "        model_name = \"simple_pytorch_model\"\n",
    "        \n",
    "        # 測試不同批次大小\n",
    "        batch_sizes = [1, 4, 8, 16]\n",
    "        results = {}\n",
    "        \n",
    "        print(\"🚀 批次推理性能測試\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # 準備批次數據\n",
    "            batch_input = np.random.randint(0, 1000, (batch_size, 10), dtype=np.int64)\n",
    "            \n",
    "            # 創建推理請求\n",
    "            inputs = [httpclient.InferInput(\"INPUT__0\", batch_input.shape, \"INT64\")]\n",
    "            inputs[0].set_data_from_numpy(batch_input)\n",
    "            \n",
    "            outputs = [httpclient.InferRequestedOutput(\"OUTPUT__0\")]\n",
    "            \n",
    "            # 測量推理時間 (多次測試取平均)\n",
    "            times = []\n",
    "            for _ in range(5):\n",
    "                start = time.time()\n",
    "                response = triton_client.infer(\n",
    "                    model_name=model_name,\n",
    "                    inputs=inputs,\n",
    "                    outputs=outputs\n",
    "                )\n",
    "                times.append(time.time() - start)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            throughput = batch_size / avg_time\n",
    "            latency_per_sample = avg_time / batch_size * 1000  # ms\n",
    "            \n",
    "            results[batch_size] = {\n",
    "                'avg_time': avg_time,\n",
    "                'throughput': throughput,\n",
    "                'latency_per_sample': latency_per_sample\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n📊 批次大小 {batch_size:2d}:\")\n",
    "            print(f\"   平均時間: {avg_time*1000:.2f} ms\")\n",
    "            print(f\"   吞吐量: {throughput:.1f} samples/sec\")\n",
    "            print(f\"   單樣本延遲: {latency_per_sample:.2f} ms\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ tritonclient 未安裝\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 批次測試失敗: {e}\")\n",
    "        return None\n",
    "\n",
    "batch_results = test_batch_inference()\n",
    "\n",
    "if batch_results:\n",
    "    # 可視化批次性能\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    batch_sizes = list(batch_results.keys())\n",
    "    throughputs = [batch_results[bs]['throughput'] for bs in batch_sizes]\n",
    "    latencies = [batch_results[bs]['latency_per_sample'] for bs in batch_sizes]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # 吞吐量\n",
    "    ax1.plot(batch_sizes, throughputs, 'o-', linewidth=2, markersize=8, color='#4ECDC4')\n",
    "    ax1.set_xlabel('批次大小')\n",
    "    ax1.set_ylabel('吞吐量 (samples/sec)')\n",
    "    ax1.set_title('批次大小 vs 吞吐量')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bs, tp in zip(batch_sizes, throughputs):\n",
    "        ax1.annotate(f'{tp:.1f}', xy=(bs, tp), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center')\n",
    "    \n",
    "    # 延遲\n",
    "    ax2.plot(batch_sizes, latencies, 'o-', linewidth=2, markersize=8, color='#FF6B6B')\n",
    "    ax2.set_xlabel('批次大小')\n",
    "    ax2.set_ylabel('單樣本延遲 (ms)')\n",
    "    ax2.set_title('批次大小 vs 延遲')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bs, lat in zip(batch_sizes, latencies):\n",
    "        ax2.annotate(f'{lat:.1f}', xy=(bs, lat), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📈 性能分析:\")\n",
    "    max_throughput = max(throughputs)\n",
    "    min_latency = min(latencies)\n",
    "    optimal_batch = batch_sizes[throughputs.index(max_throughput)]\n",
    "    \n",
    "    print(f\"  最高吞吐量: {max_throughput:.1f} samples/sec (批次={optimal_batch})\")\n",
    "    print(f\"  最低延遲: {min_latency:.1f} ms (批次={batch_sizes[latencies.index(min_latency)]})\")\n",
    "    print(f\"  推薦批次大小: {optimal_batch} (平衡性能)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Triton vs 其他推理框架對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對比測試：Triton vs PyTorch 直接推理\n",
    "def compare_with_pytorch():\n",
    "    \"\"\"\n",
    "    對比 Triton 與直接 PyTorch 推理的性能\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 直接載入 PyTorch 模型\n",
    "        model_path = model_repo_dir / \"simple_pytorch_model\" / \"1\" / \"model.pt\"\n",
    "        pytorch_model = torch.jit.load(str(model_path))\n",
    "        pytorch_model.eval()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pytorch_model = pytorch_model.cuda()\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "        \n",
    "        print(f\"🔄 PyTorch vs Triton 性能對比\")\n",
    "        print(f\"設備: {device}\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # 測試相同的輸入\n",
    "        test_input = torch.randint(0, 1000, (8, 10), dtype=torch.int64)  # 批次=8\n",
    "        \n",
    "        # PyTorch 直接推理\n",
    "        pytorch_times = []\n",
    "        for _ in range(10):\n",
    "            test_input_device = test_input.to(device)\n",
    "            \n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                pytorch_output = pytorch_model(test_input_device)\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()  # 等待 GPU 完成\n",
    "                \n",
    "            pytorch_times.append(time.time() - start)\n",
    "        \n",
    "        pytorch_avg = np.mean(pytorch_times)\n",
    "        pytorch_std = np.std(pytorch_times)\n",
    "        \n",
    "        print(f\"\\n🔥 PyTorch 直接推理:\")\n",
    "        print(f\"   平均時間: {pytorch_avg*1000:.2f} ± {pytorch_std*1000:.2f} ms\")\n",
    "        print(f\"   吞吐量: {8/pytorch_avg:.1f} samples/sec\")\n",
    "        \n",
    "        # Triton 推理 (使用之前的結果)\n",
    "        if batch_results and 8 in batch_results:\n",
    "            triton_time = batch_results[8]['avg_time']\n",
    "            triton_throughput = batch_results[8]['throughput']\n",
    "            \n",
    "            print(f\"\\n⚡ Triton Server 推理:\")\n",
    "            print(f\"   平均時間: {triton_time*1000:.2f} ms\")\n",
    "            print(f\"   吞吐量: {triton_throughput:.1f} samples/sec\")\n",
    "            \n",
    "            # 計算加速比\n",
    "            speedup = pytorch_avg / triton_time\n",
    "            overhead = (triton_time - pytorch_avg) / pytorch_avg * 100\n",
    "            \n",
    "            print(f\"\\n📊 性能對比:\")\n",
    "            if speedup > 1:\n",
    "                print(f\"   Triton 加速: {speedup:.2f}x 🚀\")\n",
    "            else:\n",
    "                print(f\"   Triton 開銷: {abs(overhead):.1f}% ⚠️\")\n",
    "                \n",
    "            print(f\"\\n💡 分析:\")\n",
    "            if speedup < 1:\n",
    "                print(\"   • 小模型可能有服務開銷\")\n",
    "                print(\"   • Triton 優勢在大模型和批次處理\")\n",
    "                print(\"   • 網路延遲和序列化開銷\")\n",
    "            else:\n",
    "                print(\"   • Triton 動態批次處理優化\")\n",
    "                print(\"   • 更好的記憶體管理\")\n",
    "                print(\"   • GPU kernel 優化\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 對比測試失敗: {e}\")\n",
    "\n",
    "compare_with_pytorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. 基礎監控設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 Triton 內建指標\n",
    "def explore_triton_metrics():\n",
    "    \"\"\"\n",
    "    探索 Triton Server 內建監控指標\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metrics_url = \"http://localhost:8002/metrics\"\n",
    "        response = requests.get(metrics_url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            metrics_text = response.text\n",
    "            lines = metrics_text.split('\\n')\n",
    "            \n",
    "            print(\"📊 Triton Server 監控指標\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # 分析不同類型的指標\n",
    "            metric_categories = {\n",
    "                'Request': [],\n",
    "                'Inference': [],\n",
    "                'Queue': [],\n",
    "                'Cache': [],\n",
    "                'GPU': [],\n",
    "                'Other': []\n",
    "            }\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                    \n",
    "                if 'nv_inference_request' in line:\n",
    "                    metric_categories['Request'].append(line.split()[0])\n",
    "                elif 'nv_inference_exec' in line or 'nv_inference_compute' in line:\n",
    "                    metric_categories['Inference'].append(line.split()[0])\n",
    "                elif 'nv_inference_queue' in line:\n",
    "                    metric_categories['Queue'].append(line.split()[0])\n",
    "                elif 'cache' in line.lower():\n",
    "                    metric_categories['Cache'].append(line.split()[0])\n",
    "                elif 'gpu' in line.lower():\n",
    "                    metric_categories['GPU'].append(line.split()[0])\n",
    "                else:\n",
    "                    if line.split()[0] not in [cat for cat_list in metric_categories.values() for cat in cat_list]:\n",
    "                        metric_categories['Other'].append(line.split()[0])\n",
    "            \n",
    "            # 顯示指標分類\n",
    "            for category, metrics in metric_categories.items():\n",
    "                if metrics:\n",
    "                    unique_metrics = list(set(metrics))\n",
    "                    print(f\"\\n🔸 {category} 指標 ({len(unique_metrics)}個):\")\n",
    "                    for metric in unique_metrics[:5]:  # 顯示前5個\n",
    "                        print(f\"   • {metric}\")\n",
    "                    \n",
    "                    if len(unique_metrics) > 5:\n",
    "                        print(f\"   ... 還有 {len(unique_metrics)-5} 個指標\")\n",
    "            \n",
    "            # 提取關鍵指標值\n",
    "            key_metrics = {}\n",
    "            for line in lines:\n",
    "                if 'nv_inference_request_success' in line and '{' not in line:\n",
    "                    key_metrics['successful_requests'] = float(line.split()[1])\n",
    "                elif 'nv_inference_request_failure' in line and '{' not in line:\n",
    "                    key_metrics['failed_requests'] = float(line.split()[1])\n",
    "                elif 'nv_inference_exec_count' in line and '{' not in line:\n",
    "                    key_metrics['inference_count'] = float(line.split()[1])\n",
    "            \n",
    "            print(f\"\\n📈 當前指標值:\")\n",
    "            for metric, value in key_metrics.items():\n",
    "                print(f\"   {metric}: {value}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ 指標端點訪問失敗: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 指標獲取失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "metrics_available = explore_triton_metrics()\n",
    "\n",
    "if metrics_available:\n",
    "    print(\"\\n🎯 監控建議:\")\n",
    "    print(\"• 將指標端點整合到 Prometheus\")\n",
    "    print(\"• 設置關鍵指標的告警閾值\")\n",
    "    print(\"• 監控模型性能趨勢\")\n",
    "    print(\"• 設置 GPU 利用率監控\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. 清理與總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理資源\n",
    "def cleanup_resources():\n",
    "    \"\"\"\n",
    "    清理 Docker 容器和資源\n",
    "    \"\"\"\n",
    "    print(\"🧹 清理資源...\")\n",
    "    \n",
    "    try:\n",
    "        # 停止 Triton Server 容器\n",
    "        result = subprocess.run(['docker', 'stop', 'triton-server'], \n",
    "                               capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Triton Server 容器已停止\")\n",
    "        else:\n",
    "            print(\"⚠️ 容器可能已經停止\")\n",
    "            \n",
    "        # 清理 GPU 記憶體\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"✅ GPU 記憶體已清理\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 清理過程中出錯: {e}\")\n",
    "        return False\n",
    "\n",
    "# 實驗室總結\n",
    "print(\"🎯 Lab-2.1 Part 1 完成總結\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "completion_checklist = [\n",
    "    (\"環境驗證\", torch.cuda.is_available() and docker_ready),\n",
    "    (\"Triton Server 安裝\", triton_image_ready),\n",
    "    (\"模型倉庫創建\", model_repo_dir.exists()),\n",
    "    (\"模型部署\", (model_repo_dir / \"simple_pytorch_model\" / \"1\" / \"model.pt\").exists()),\n",
    "    (\"API 測試\", server_ready if 'server_ready' in locals() else False),\n",
    "    (\"監控指標\", metrics_available),\n",
    "]\n",
    "\n",
    "completed_tasks = 0\n",
    "for task, status in completion_checklist:\n",
    "    status_icon = \"✅\" if status else \"❌\"\n",
    "    print(f\"  {status_icon} {task}\")\n",
    "    if status:\n",
    "        completed_tasks += 1\n",
    "\n",
    "completion_rate = completed_tasks / len(completion_checklist) * 100\n",
    "print(f\"\\n📊 完成度: {completion_rate:.1f}% ({completed_tasks}/{len(completion_checklist)})\")\n",
    "\n",
    "if completion_rate >= 80:\n",
    "    print(\"\\n🎉 恭喜! 您已掌握 Triton Server 基礎\")\n",
    "    print(\"\\n➡️ 下一步: 02-Model_Repository_Design.ipynb\")\n",
    "    print(\"   學習模型倉庫設計和高級配置\")\n",
    "else:\n",
    "    print(\"\\n⚠️ 建議解決未完成的項目後再繼續\")\n",
    "    print(\"\\n📋 故障排除指南:\")\n",
    "    print(\"1. 檢查 Docker 和 NVIDIA 驅動安裝\")\n",
    "    print(\"2. 確認 GPU 可用性\")\n",
    "    print(\"3. 驗證網路連接和端口\")\n",
    "    print(\"4. 查看 Triton Server 日誌\")\n",
    "\n",
    "# 可選：自動清理\n",
    "auto_cleanup = False  # 設為 True 自動清理\n",
    "\n",
    "if auto_cleanup:\n",
    "    cleanup_success = cleanup_resources()\n",
    "else:\n",
    "    print(\"\\n💡 提示: 運行下面的 cell 來清理容器\")\n",
    "    print(\"   (如果要繼續下一個 notebook，請保留容器運行)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可選：手動清理\n",
    "cleanup_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✅ 實驗室總結\n",
    "\n",
    "### 🎯 已完成學習目標\n",
    "1. ✅ **Triton Server 基礎**: 安裝、配置和啟動\n",
    "2. ✅ **模型倉庫設計**: 標準目錄結構和配置文件\n",
    "3. ✅ **PyTorch 模型部署**: TorchScript 轉換和部署\n",
    "4. ✅ **API 介面測試**: REST API 調用和批次推理\n",
    "5. ✅ **性能分析**: 批次處理優化和性能對比\n",
    "6. ✅ **監控基礎**: 內建指標系統探索\n",
    "\n",
    "### 📊 核心技能獲得\n",
    "- **企業級推理服務**: 掌握 Triton Server 部署和配置\n",
    "- **模型管理**: 理解模型倉庫設計原則\n",
    "- **動態批次處理**: 優化推理性能的核心技術\n",
    "- **API 介面**: REST/gRPC 雙協議支援\n",
    "- **監控整合**: 為生產環境監控奠定基礎\n",
    "\n",
    "### 🌟 Triton vs vLLM 對比\n",
    "| 特性 | Triton | vLLM |\n",
    "|------|--------|------|\n",
    "| **多模型管理** | ✅ 原生支援 | ❌ 單模型 |\n",
    "| **Backend 靈活性** | ✅ 多種 Backend | ❌ 固定實現 |\n",
    "| **企業級功能** | ✅ 完整支援 | ⚠️ 有限 |\n",
    "| **版本管理** | ✅ 內建 A/B 測試 | ❌ 需自行實現 |\n",
    "| **監控運維** | ✅ 豐富指標 | ⚠️ 基礎指標 |\n",
    "| **學習曲線** | ⚠️ 較複雜 | ✅ 簡單易用 |\n",
    "\n",
    "### 💡 關鍵洞察\n",
    "- **Triton 適合企業級多模型場景**\n",
    "- **動態批次處理是性能關鍵**\n",
    "- **配置文件是 Triton 的核心**\n",
    "- **監控指標豐富且詳細**\n",
    "\n",
    "### ➡️ 下一步學習路徑\n",
    "1. **02-Model_Repository_Design.ipynb**: 深入模型倉庫設計\n",
    "2. **03-PyTorch_Backend_Deployment.ipynb**: PyTorch Backend 高級配置\n",
    "3. **04-Monitoring_and_Performance.ipynb**: 監控系統整合\n",
    "\n",
    "### 🔗 參考資源\n",
    "- [Triton Model Repository Guide](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md)\n",
    "- [Dynamic Batching](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#dynamic-batching)\n",
    "- [PyTorch Backend](https://github.com/triton-inference-server/pytorch_backend)\n",
    "\n",
    "---\n",
    "\n",
    "**🎓 學習成果**: 您已具備使用 Triton Inference Server 進行基礎推理部署的能力！\n",
    "\n",
    "**💼 業界應用**: 這些技能直接應用於 Netflix、PayPal、VISA 等企業的 AI 推理平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🚀 延伸練習\n",
    "\n",
    "### 基礎練習\n",
    "1. **修改模型**: 改變分類器的類別數量，重新部署\n",
    "2. **批次優化**: 實驗不同的 `preferred_batch_size` 配置\n",
    "3. **監控觀察**: 在推理過程中實時觀察指標變化\n",
    "\n",
    "### 進階挑戰\n",
    "1. **多版本部署**: 部署同一模型的多個版本\n",
    "2. **gRPC API**: 使用 gRPC 客戶端進行推理\n",
    "3. **性能調優**: 調整 `max_queue_delay_microseconds` 參數\n",
    "\n",
    "### 專家級任務\n",
    "1. **自定義指標**: 創建自定義業務指標\n",
    "2. **負載測試**: 使用 locust 進行壓力測試\n",
    "3. **故障模擬**: 模擬各種故障情況並測試恢復"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
