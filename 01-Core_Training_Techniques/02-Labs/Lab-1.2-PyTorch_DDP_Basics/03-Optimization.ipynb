{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.2: PyTorch DDP åˆ†æ•£å¼è¨“ç·´åŸºç¤ - 03-Optimization\n",
    "## é€šè¨Šå„ªåŒ–èˆ‡æ€§èƒ½èª¿å„ª\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ æ³¨æ„äº‹é …\n",
    "\n",
    "æœ¬notebookå°ˆæ³¨æ–¼**DDPé€šè¨Šå„ªåŒ–çš„ç†è«–å’Œå¯¦ä½œæŠ€å·§**ï¼Œåœ¨å–®GPUç’°å¢ƒä¸­æ¼”ç¤ºå„ªåŒ–é…ç½®ã€‚\n",
    "- âœ… **å¯å­¸ç¿’**: é€šè¨Šå„ªåŒ–ç­–ç•¥ã€æ€§èƒ½èª¿å„ªæŠ€å·§ã€é…ç½®æœ€ä½³å¯¦è¸\n",
    "- âš ï¸ **é™åˆ¶**: ç„¡æ³•æ¸¬é‡çœŸå¯¦çš„å¤šGPUé€šè¨Šæ€§èƒ½\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. ç†è§£DDPé€šè¨Šç“¶é ¸å’Œå„ªåŒ–ç­–ç•¥\n",
    "2. æŒæ¡æ¢¯åº¦ç´¯ç©å’Œæ··åˆç²¾åº¦è¨“ç·´\n",
    "3. å­¸ç¿’é€šè¨Šèˆ‡è¨ˆç®—é‡ç–ŠæŠ€è¡“\n",
    "4. å¯¦ä½œæ€§èƒ½ç›£æ§å’Œèª¿å„ªå·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥åŸºç¤è¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.utils.data as data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import psutil\n",
    "import threading\n",
    "\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "print(f\"åˆ†æ•£å¼æ”¯æ´: {torch.distributed.is_available()}\")\n",
    "print(f\"NCCL æ”¯æ´: {torch.distributed.is_nccl_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é€šè¨Šåˆ†æå·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CommunicationProfile:\n",
    "    \"\"\"é€šè¨Šæ€§èƒ½åˆ†æçµæœ\"\"\"\n",
    "    operation: str\n",
    "    data_size_mb: float\n",
    "    time_ms: float\n",
    "    bandwidth_gbps: float\n",
    "    gpu_count: int\n",
    "    backend: str\n",
    "\n",
    "class DDPCommunicationAnalyzer:\n",
    "    \"\"\"\n",
    "    DDPé€šè¨Šåˆ†æå™¨\n",
    "    åˆ†æå’Œå„ªåŒ–åˆ†æ•£å¼è¨“ç·´ä¸­çš„é€šè¨Šæ¨¡å¼\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, world_size=1):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.world_size = world_size\n",
    "        self.profiles = []\n",
    "        \n",
    "        # è¨ˆç®—æ¨¡å‹é€šè¨Šé‡\n",
    "        self.model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        self.gradient_size_mb = self.model_params * 4 / (1024 * 1024)  # FP32\n",
    "        self.gradient_size_mb_fp16 = self.model_params * 2 / (1024 * 1024)  # FP16\n",
    "        \n",
    "        print(f\"=== DDP é€šè¨Šåˆ†æå™¨åˆå§‹åŒ– ===\")\n",
    "        print(f\"æ¨¡å‹åƒæ•¸é‡: {self.model_params:,}\")\n",
    "        print(f\"æ¢¯åº¦å¤§å° (FP32): {self.gradient_size_mb:.2f} MB\")\n",
    "        print(f\"æ¢¯åº¦å¤§å° (FP16): {self.gradient_size_mb_fp16:.2f} MB\")\n",
    "        print(f\"World Size: {self.world_size}\")\n",
    "    \n",
    "    def analyze_communication_patterns(self):\n",
    "        \"\"\"åˆ†æä¸åŒé€šè¨Šæ¨¡å¼çš„ç‰¹æ€§\"\"\"\n",
    "        print(\"\\n=== é€šè¨Šæ¨¡å¼åˆ†æ ===\")\n",
    "        \n",
    "        patterns = {\n",
    "            'All-Reduce (Ring)': {\n",
    "                'description': 'Ringæ‹“æ’²çš„All-Reduceç®—æ³•',\n",
    "                'steps': self.world_size - 1,\n",
    "                'data_per_step': self.gradient_size_mb / self.world_size,\n",
    "                'total_data': self.gradient_size_mb * (self.world_size - 1) / self.world_size,\n",
    "                'latency_factor': self.world_size - 1\n",
    "            },\n",
    "            'All-Reduce (Tree)': {\n",
    "                'description': 'Treeæ‹“æ’²çš„All-Reduceç®—æ³•',\n",
    "                'steps': 2 * int(np.log2(self.world_size)),\n",
    "                'data_per_step': self.gradient_size_mb,\n",
    "                'total_data': self.gradient_size_mb * 2 * int(np.log2(self.world_size)),\n",
    "                'latency_factor': 2 * int(np.log2(self.world_size))\n",
    "            },\n",
    "            'Parameter Server': {\n",
    "                'description': 'åƒæ•¸æœå‹™å™¨æ¨¡å¼',\n",
    "                'steps': 2,  # æ”¶é›† + å»£æ’­\n",
    "                'data_per_step': self.gradient_size_mb * (self.world_size - 1),\n",
    "                'total_data': self.gradient_size_mb * 2 * (self.world_size - 1),\n",
    "                'latency_factor': 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for name, pattern in patterns.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  æè¿°: {pattern['description']}\")\n",
    "            print(f\"  é€šè¨Šæ­¥æ•¸: {pattern['steps']}\")\n",
    "            print(f\"  æ¯æ­¥æ•¸æ“šé‡: {pattern['data_per_step']:.2f} MB\")\n",
    "            print(f\"  ç¸½æ•¸æ“šé‡: {pattern['total_data']:.2f} MB\")\n",
    "            print(f\"  å»¶é²ä¿‚æ•¸: {pattern['latency_factor']}\")\n",
    "            \n",
    "            # è¨ˆç®—ç†è«–æ€§èƒ½\n",
    "            if self.world_size > 1:\n",
    "                efficiency = 1.0 / pattern['latency_factor']\n",
    "                print(f\"  ç†è«–æ•ˆç‡: {efficiency:.3f}\")\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def simulate_communication_overhead(self, bandwidth_gbps=10.0, latency_us=5.0):\n",
    "        \"\"\"æ¨¡æ“¬é€šè¨Šé–‹éŠ·\"\"\"\n",
    "        print(f\"\\n=== é€šè¨Šé–‹éŠ·æ¨¡æ“¬ ===\")\n",
    "        print(f\"å‡è¨­æ¢ä»¶: å¸¶å¯¬ {bandwidth_gbps} Gbps, å»¶é² {latency_us} Î¼s\")\n",
    "        \n",
    "        # ä¸åŒGPUæ•¸é‡çš„é€šè¨Šé–‹éŠ·\n",
    "        gpu_counts = [2, 4, 8, 16, 32] if self.world_size == 1 else [self.world_size]\n",
    "        \n",
    "        results = {}\n",
    "        for gpu_count in gpu_counts:\n",
    "            # Ring All-Reduce\n",
    "            chunk_size_mb = self.gradient_size_mb / gpu_count\n",
    "            transfer_time_ms = (chunk_size_mb * 8) / bandwidth_gbps  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "            latency_time_ms = latency_us * (gpu_count - 1) / 1000\n",
    "            total_time_ms = transfer_time_ms + latency_time_ms\n",
    "            \n",
    "            results[gpu_count] = {\n",
    "                'transfer_time_ms': transfer_time_ms,\n",
    "                'latency_time_ms': latency_time_ms,\n",
    "                'total_time_ms': total_time_ms,\n",
    "                'effective_bandwidth_gbps': (self.gradient_size_mb * 8) / (total_time_ms / 1000)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{gpu_count} GPU:\")\n",
    "            print(f\"  æ•¸æ“šå‚³è¼¸æ™‚é–“: {transfer_time_ms:.2f} ms\")\n",
    "            print(f\"  å»¶é²æ™‚é–“: {latency_time_ms:.2f} ms\")\n",
    "            print(f\"  ç¸½é€šè¨Šæ™‚é–“: {total_time_ms:.2f} ms\")\n",
    "            print(f\"  æœ‰æ•ˆå¸¶å¯¬: {results[gpu_count]['effective_bandwidth_gbps']:.2f} Gbps\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_gradient_compression(self):\n",
    "        \"\"\"åˆ†ææ¢¯åº¦å£“ç¸®æŠ€è¡“\"\"\"\n",
    "        print(f\"\\n=== æ¢¯åº¦å£“ç¸®åˆ†æ ===\")\n",
    "        \n",
    "        compression_methods = {\n",
    "            'FP32 (ç„¡å£“ç¸®)': {\n",
    "                'bits_per_param': 32,\n",
    "                'compression_ratio': 1.0,\n",
    "                'accuracy_loss': 0.0\n",
    "            },\n",
    "            'FP16': {\n",
    "                'bits_per_param': 16,\n",
    "                'compression_ratio': 2.0,\n",
    "                'accuracy_loss': 0.01\n",
    "            },\n",
    "            'BF16': {\n",
    "                'bits_per_param': 16,\n",
    "                'compression_ratio': 2.0,\n",
    "                'accuracy_loss': 0.005\n",
    "            },\n",
    "            'INT8 é‡åŒ–': {\n",
    "                'bits_per_param': 8,\n",
    "                'compression_ratio': 4.0,\n",
    "                'accuracy_loss': 0.02\n",
    "            },\n",
    "            'Top-Kç¨€ç–åŒ–': {\n",
    "                'bits_per_param': 32,  # ä¿æŒç²¾åº¦\n",
    "                'compression_ratio': 10.0,  # åªå‚³è¼¸10%\n",
    "                'accuracy_loss': 0.03\n",
    "            },\n",
    "            'éš¨æ©Ÿé‡åŒ–': {\n",
    "                'bits_per_param': 1,  # æ¥µç«¯å£“ç¸®\n",
    "                'compression_ratio': 32.0,\n",
    "                'accuracy_loss': 0.05\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for method, props in compression_methods.items():\n",
    "            compressed_size = self.gradient_size_mb / props['compression_ratio']\n",
    "            bandwidth_savings = (1 - 1/props['compression_ratio']) * 100\n",
    "            \n",
    "            print(f\"\\n{method}:\")\n",
    "            print(f\"  æ¯åƒæ•¸ä½æ•¸: {props['bits_per_param']}\")\n",
    "            print(f\"  å£“ç¸®æ¯”: {props['compression_ratio']:.1f}x\")\n",
    "            print(f\"  å£“ç¸®å¾Œå¤§å°: {compressed_size:.2f} MB\")\n",
    "            print(f\"  å¸¶å¯¬ç¯€çœ: {bandwidth_savings:.1f}%\")\n",
    "            print(f\"  ç²¾åº¦æå¤±: {props['accuracy_loss']:.1%}\")\n",
    "        \n",
    "        return compression_methods\n",
    "\n",
    "# å‰µå»ºé€šè¨Šåˆ†æå™¨\n",
    "# ä½¿ç”¨ç°¡å–®æ¨¡å‹é€²è¡Œæ¼”ç¤º\n",
    "demo_model = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ").to(device)\n",
    "\n",
    "analyzer = DDPCommunicationAnalyzer(demo_model, device, world_size=4)\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "communication_patterns = analyzer.analyze_communication_patterns()\n",
    "communication_overhead = analyzer.simulate_communication_overhead()\n",
    "compression_analysis = analyzer.analyze_gradient_compression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¢¯åº¦ç´¯ç©å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulationOptimizer:\n",
    "    \"\"\"\n",
    "    æ¢¯åº¦ç´¯ç©å„ªåŒ–å™¨\n",
    "    å¯¦ç¾é«˜æ•ˆçš„æ¢¯åº¦ç´¯ç©å’ŒåŒæ­¥ç­–ç•¥\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, accumulation_steps=4, sync_every_n_steps=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.sync_every_n_steps = sync_every_n_steps or accumulation_steps\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.accumulated_gradients = 0\n",
    "        \n",
    "        # æ€§èƒ½çµ±è¨ˆ\n",
    "        self.sync_times = []\n",
    "        self.compute_times = []\n",
    "        \n",
    "        print(f\"=== æ¢¯åº¦ç´¯ç©å„ªåŒ–å™¨ ===\")\n",
    "        print(f\"ç´¯ç©æ­¥æ•¸: {self.accumulation_steps}\")\n",
    "        print(f\"åŒæ­¥é–“éš”: {self.sync_every_n_steps}\")\n",
    "        print(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°å€æ•¸: {self.accumulation_steps}x\")\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"æ¸…é›¶æ¢¯åº¦\"\"\"\n",
    "        if self.accumulated_gradients == 0:\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        \"\"\"åå‘å‚³æ’­\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ç¸®æ”¾æå¤±\n",
    "        scaled_loss = loss / self.accumulation_steps\n",
    "        scaled_loss.backward()\n",
    "        \n",
    "        self.accumulated_gradients += 1\n",
    "        compute_time = time.time() - start_time\n",
    "        self.compute_times.append(compute_time * 1000)  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "        \n",
    "        return scaled_loss.item()\n",
    "    \n",
    "    def step(self, clip_grad_norm=None):\n",
    "        \"\"\"å„ªåŒ–å™¨æ­¥é©Ÿ\"\"\"\n",
    "        if self.accumulated_gradients >= self.accumulation_steps:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # æ¢¯åº¦è£å‰ª\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad_norm)\n",
    "            \n",
    "            # åƒæ•¸æ›´æ–°\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            sync_time = time.time() - start_time\n",
    "            self.sync_times.append(sync_time * 1000)  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "            \n",
    "            self.accumulated_gradients = 0\n",
    "            self.step_count += 1\n",
    "            \n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_performance_stats(self):\n",
    "        \"\"\"ç²å–æ€§èƒ½çµ±è¨ˆ\"\"\"\n",
    "        stats = {\n",
    "            'avg_compute_time_ms': np.mean(self.compute_times) if self.compute_times else 0,\n",
    "            'avg_sync_time_ms': np.mean(self.sync_times) if self.sync_times else 0,\n",
    "            'total_steps': self.step_count,\n",
    "            'compute_sync_ratio': 0\n",
    "        }\n",
    "        \n",
    "        if stats['avg_sync_time_ms'] > 0:\n",
    "            stats['compute_sync_ratio'] = stats['avg_compute_time_ms'] / stats['avg_sync_time_ms']\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def analyze_gradient_accumulation_benefits():\n",
    "    \"\"\"åˆ†ææ¢¯åº¦ç´¯ç©çš„å„ªå‹¢\"\"\"\n",
    "    print(\"\\n=== æ¢¯åº¦ç´¯ç©å„ªå‹¢åˆ†æ ===\")\n",
    "    \n",
    "    base_batch_size = 8\n",
    "    accumulation_steps_list = [1, 2, 4, 8, 16]\n",
    "    \n",
    "    for acc_steps in accumulation_steps_list:\n",
    "        effective_batch_size = base_batch_size * acc_steps\n",
    "        communication_reduction = (acc_steps - 1) / acc_steps * 100\n",
    "        memory_efficiency = 1.0  # è¨˜æ†¶é«”ä½¿ç”¨ä¸è®Š\n",
    "        \n",
    "        print(f\"\\nç´¯ç©æ­¥æ•¸ {acc_steps}:\")\n",
    "        print(f\"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}\")\n",
    "        print(f\"  é€šè¨Šæ¸›å°‘: {communication_reduction:.1f}%\")\n",
    "        print(f\"  è¨˜æ†¶é«”æ•ˆç‡: {memory_efficiency:.1f}x\")\n",
    "        print(f\"  åŒæ­¥é »ç‡: 1/{acc_steps}\")\n",
    "        \n",
    "        # ç†è«–åŠ é€Ÿæ¯”\n",
    "        if acc_steps > 1:\n",
    "            speedup = acc_steps / (1 + 0.1 * (acc_steps - 1))  # å‡è¨­10%çš„é–‹éŠ·\n",
    "            print(f\"  ç†è«–åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ é—œéµå„ªå‹¢:\")\n",
    "    print(f\"  1. æ¸›å°‘é€šè¨Šé »ç‡ â†’ é™ä½ç¶²çµ¡é–‹éŠ·\")\n",
    "    print(f\"  2. å¢å¤§æœ‰æ•ˆæ‰¹æ¬¡ â†’ æ”¹å–„è¨“ç·´ç©©å®šæ€§\")\n",
    "    print(f\"  3. ä¿æŒè¨˜æ†¶é«”ä½¿ç”¨ â†’ é©åˆè¨˜æ†¶é«”å—é™ç’°å¢ƒ\")\n",
    "    print(f\"  4. æ›´å¥½çš„è¨ˆç®—é€šè¨Šæ¯” â†’ æå‡æ•´é«”æ•ˆç‡\")\n",
    "\n",
    "# æ¼”ç¤ºæ¢¯åº¦ç´¯ç©\n",
    "demo_optimizer = torch.optim.AdamW(demo_model.parameters(), lr=1e-3)\n",
    "grad_acc_optimizer = GradientAccumulationOptimizer(\n",
    "    model=demo_model,\n",
    "    optimizer=demo_optimizer,\n",
    "    accumulation_steps=4\n",
    ")\n",
    "\n",
    "# æ¨¡æ“¬è¨“ç·´æ­¥é©Ÿ\n",
    "print(\"\\n=== æ¢¯åº¦ç´¯ç©æ¼”ç¤º ===\")\n",
    "demo_input = torch.randn(8, 1024).to(device)\n",
    "demo_target = torch.randint(0, 10, (8,)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(8):\n",
    "    grad_acc_optimizer.zero_grad()\n",
    "    \n",
    "    output = demo_model(demo_input)\n",
    "    loss = criterion(output, demo_target)\n",
    "    \n",
    "    scaled_loss = grad_acc_optimizer.backward(loss)\n",
    "    step_taken = grad_acc_optimizer.step(clip_grad_norm=1.0)\n",
    "    \n",
    "    print(f\"æ­¥é©Ÿ {step+1}: æå¤± {scaled_loss:.4f}, åƒæ•¸æ›´æ–°: {step_taken}\")\n",
    "\n",
    "# æ€§èƒ½çµ±è¨ˆ\n",
    "perf_stats = grad_acc_optimizer.get_performance_stats()\n",
    "print(f\"\\næ€§èƒ½çµ±è¨ˆ:\")\n",
    "for key, value in perf_stats.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"  {key}: {value:.2f} ms\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "\n",
    "# åˆ†ææ¢¯åº¦ç´¯ç©å„ªå‹¢\n",
    "analyze_gradient_accumulation_benefits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ··åˆç²¾åº¦è¨“ç·´å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecisionDDPTrainer:\n",
    "    \"\"\"\n",
    "    æ··åˆç²¾åº¦DDPè¨“ç·´å™¨\n",
    "    çµåˆè‡ªå‹•æ··åˆç²¾åº¦å’Œæ¢¯åº¦ç¸®æ”¾\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, scheduler=None, \n",
    "                 use_amp=True, gradient_accumulation_steps=1):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.use_amp = use_amp and torch.cuda.is_available()\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        \n",
    "        # æ··åˆç²¾åº¦ç›¸é—œ\n",
    "        if self.use_amp:\n",
    "            self.scaler = GradScaler()\n",
    "            print(\"âœ… å•Ÿç”¨è‡ªå‹•æ··åˆç²¾åº¦ (AMP)\")\n",
    "        else:\n",
    "            self.scaler = None\n",
    "            print(\"âš ï¸ æœªå•Ÿç”¨æ··åˆç²¾åº¦\")\n",
    "        \n",
    "        # æ€§èƒ½ç›£æ§\n",
    "        self.performance_metrics = {\n",
    "            'forward_time': [],\n",
    "            'backward_time': [],\n",
    "            'optimizer_time': [],\n",
    "            'memory_usage': [],\n",
    "            'loss_scale': []\n",
    "        }\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.accumulated_loss = 0.0\n",
    "    \n",
    "    def train_step(self, batch_data, criterion, clip_grad_norm=None):\n",
    "        \"\"\"å–®å€‹è¨“ç·´æ­¥é©Ÿ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # å‰å‘å‚³æ’­\n",
    "        if self.use_amp:\n",
    "            with autocast():\n",
    "                outputs = self.model(batch_data['input'])\n",
    "                loss = criterion(outputs, batch_data['target'])\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "        else:\n",
    "            outputs = self.model(batch_data['input'])\n",
    "            loss = criterion(outputs, batch_data['target'])\n",
    "            loss = loss / self.gradient_accumulation_steps\n",
    "        \n",
    "        forward_time = time.time() - start_time\n",
    "        \n",
    "        # åå‘å‚³æ’­\n",
    "        start_time = time.time()\n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        backward_time = time.time() - start_time\n",
    "        self.accumulated_loss += loss.item()\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦éœ€è¦åƒæ•¸æ›´æ–°\n",
    "        if (self.step_count + 1) % self.gradient_accumulation_steps == 0:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if self.use_amp:\n",
    "                # æ¢¯åº¦ç¸®æ”¾å’Œæ›´æ–°\n",
    "                if clip_grad_norm is not None:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad_norm)\n",
    "                \n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                # è¨˜éŒ„loss scale\n",
    "                self.performance_metrics['loss_scale'].append(self.scaler.get_scale())\n",
    "            else:\n",
    "                # æ¨™æº–æ›´æ–°\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad_norm)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            optimizer_time = time.time() - start_time\n",
    "            \n",
    "            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™\n",
    "            self.performance_metrics['forward_time'].append(forward_time * 1000)\n",
    "            self.performance_metrics['backward_time'].append(backward_time * 1000)\n",
    "            self.performance_metrics['optimizer_time'].append(optimizer_time * 1000)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                memory_usage = torch.cuda.memory_allocated() / (1024**2)  # MB\n",
    "                self.performance_metrics['memory_usage'].append(memory_usage)\n",
    "            \n",
    "            avg_loss = self.accumulated_loss\n",
    "            self.accumulated_loss = 0.0\n",
    "            \n",
    "            return avg_loss, True  # è¿”å›æå¤±å’Œæ˜¯å¦æ›´æ–°äº†åƒæ•¸\n",
    "        \n",
    "        self.step_count += 1\n",
    "        return loss.item(), False\n",
    "    \n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"ç²å–æ€§èƒ½æ‘˜è¦\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for metric, values in self.performance_metrics.items():\n",
    "            if values:\n",
    "                summary[f'{metric}_avg'] = np.mean(values)\n",
    "                summary[f'{metric}_std'] = np.std(values)\n",
    "                summary[f'{metric}_max'] = np.max(values)\n",
    "                summary[f'{metric}_min'] = np.min(values)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "def compare_precision_modes():\n",
    "    \"\"\"æ¯”è¼ƒä¸åŒç²¾åº¦æ¨¡å¼çš„ç‰¹æ€§\"\"\"\n",
    "    print(\"\\n=== ç²¾åº¦æ¨¡å¼æ¯”è¼ƒ ===\")\n",
    "    \n",
    "    precision_modes = {\n",
    "        'FP32 (å–®ç²¾åº¦)': {\n",
    "            'memory_factor': 1.0,\n",
    "            'speed_factor': 1.0,\n",
    "            'accuracy': 'highest',\n",
    "            'numerical_range': 'Â±3.4e38',\n",
    "            'mantissa_bits': 23\n",
    "        },\n",
    "        'FP16 (åŠç²¾åº¦)': {\n",
    "            'memory_factor': 0.5,\n",
    "            'speed_factor': 1.5,  # Tensor CoreåŠ é€Ÿ\n",
    "            'accuracy': 'good',\n",
    "            'numerical_range': 'Â±6.5e4',\n",
    "            'mantissa_bits': 10\n",
    "        },\n",
    "        'BF16 (Brain Float16)': {\n",
    "            'memory_factor': 0.5,\n",
    "            'speed_factor': 1.4,\n",
    "            'accuracy': 'better',\n",
    "            'numerical_range': 'Â±3.4e38',\n",
    "            'mantissa_bits': 7\n",
    "        },\n",
    "        'Mixed Precision': {\n",
    "            'memory_factor': 0.6,  # ä¸»è¦è¨ˆç®—FP16ï¼Œæ¬Šé‡FP32\n",
    "            'speed_factor': 1.6,\n",
    "            'accuracy': 'high',\n",
    "            'numerical_range': 'adaptive',\n",
    "            'mantissa_bits': 'adaptive'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for mode, props in precision_modes.items():\n",
    "        print(f\"\\n{mode}:\")\n",
    "        print(f\"  è¨˜æ†¶é«”ä½¿ç”¨: {props['memory_factor']:.1f}x\")\n",
    "        print(f\"  è¨ˆç®—é€Ÿåº¦: {props['speed_factor']:.1f}x\")\n",
    "        print(f\"  æ•¸å€¼ç²¾åº¦: {props['accuracy']}\")\n",
    "        print(f\"  æ•¸å€¼ç¯„åœ: {props['numerical_range']}\")\n",
    "        print(f\"  å°¾æ•¸ä½æ•¸: {props['mantissa_bits']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ é¸æ“‡å»ºè­°:\")\n",
    "    print(f\"  - æœ€é«˜ç²¾åº¦éœ€æ±‚: FP32\")\n",
    "    print(f\"  - å¹³è¡¡æ€§èƒ½ç²¾åº¦: Mixed Precision\")\n",
    "    print(f\"  - æ¥µè‡´æ€§èƒ½: FP16 (éœ€è¦ä»”ç´°èª¿å„ª)\")\n",
    "    print(f\"  - å¤§æ¨¡å‹è¨“ç·´: BF16 (æ›´ç©©å®šçš„FP16æ›¿ä»£)\")\n",
    "\n",
    "# æ¼”ç¤ºæ··åˆç²¾åº¦è¨“ç·´\n",
    "print(\"=== æ··åˆç²¾åº¦è¨“ç·´æ¼”ç¤º ===\")\n",
    "\n",
    "# å‰µå»ºè¨“ç·´å™¨\n",
    "demo_optimizer = torch.optim.AdamW(demo_model.parameters(), lr=1e-3)\n",
    "mixed_precision_trainer = MixedPrecisionDDPTrainer(\n",
    "    model=demo_model,\n",
    "    optimizer=demo_optimizer,\n",
    "    use_amp=True,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n",
    "\n",
    "# æ¨¡æ“¬è¨“ç·´æ•¸æ“š\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for step in range(6):\n",
    "    batch_data = {\n",
    "        'input': torch.randn(8, 1024).to(device),\n",
    "        'target': torch.randint(0, 10, (8,)).to(device)\n",
    "    }\n",
    "    \n",
    "    loss, param_updated = mixed_precision_trainer.train_step(\n",
    "        batch_data, criterion, clip_grad_norm=1.0\n",
    "    )\n",
    "    \n",
    "    print(f\"æ­¥é©Ÿ {step+1}: æå¤± {loss:.4f}, åƒæ•¸æ›´æ–°: {param_updated}\")\n",
    "\n",
    "# æ€§èƒ½æ‘˜è¦\n",
    "perf_summary = mixed_precision_trainer.get_performance_summary()\n",
    "print(f\"\\n=== æ€§èƒ½æ‘˜è¦ ===\")\n",
    "for key, value in perf_summary.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"{key}: {value:.2f} ms\")\n",
    "    elif 'memory' in key:\n",
    "        print(f\"{key}: {value:.1f} MB\")\n",
    "    elif 'scale' in key:\n",
    "        print(f\"{key}: {value:.0f}\")\n",
    "\n",
    "# æ¯”è¼ƒç²¾åº¦æ¨¡å¼\n",
    "compare_precision_modes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. é€šè¨Šèˆ‡è¨ˆç®—é‡ç–Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationCommunicationOverlap:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—é€šè¨Šé‡ç–Šåˆ†æå™¨\n",
    "    åˆ†æå’Œå„ªåŒ–è¨ˆç®—èˆ‡é€šè¨Šçš„é‡ç–Šç­–ç•¥\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.layer_info = self._analyze_model_layers()\n",
    "        \n",
    "    def _analyze_model_layers(self):\n",
    "        \"\"\"åˆ†ææ¨¡å‹å±¤ç´šçµæ§‹\"\"\"\n",
    "        layer_info = []\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # è‘‰å­ç¯€é»\n",
    "                param_count = sum(p.numel() for p in module.parameters())\n",
    "                if param_count > 0:\n",
    "                    layer_info.append({\n",
    "                        'name': name,\n",
    "                        'type': type(module).__name__,\n",
    "                        'parameters': param_count,\n",
    "                        'gradient_size_mb': param_count * 4 / (1024 * 1024)  # FP32\n",
    "                    })\n",
    "        \n",
    "        return layer_info\n",
    "    \n",
    "    def analyze_overlap_potential(self):\n",
    "        \"\"\"åˆ†æé‡ç–Šæ½›åŠ›\"\"\"\n",
    "        print(\"\\n=== è¨ˆç®—é€šè¨Šé‡ç–Šåˆ†æ ===\")\n",
    "        \n",
    "        total_params = sum(layer['parameters'] for layer in self.layer_info)\n",
    "        total_gradient_mb = sum(layer['gradient_size_mb'] for layer in self.layer_info)\n",
    "        \n",
    "        print(f\"æ¨¡å‹ç¸½åƒæ•¸: {total_params:,}\")\n",
    "        print(f\"ç¸½æ¢¯åº¦å¤§å°: {total_gradient_mb:.2f} MB\")\n",
    "        print()\n",
    "        \n",
    "        print(\"å±¤ç´šæ¢¯åº¦åˆ†å¸ƒ:\")\n",
    "        for i, layer in enumerate(self.layer_info):\n",
    "            percentage = layer['parameters'] / total_params * 100\n",
    "            print(f\"  {layer['name']:<20} ({layer['type']:<15}): \"\n",
    "                  f\"{layer['parameters']:>8,} params ({percentage:5.1f}%), \"\n",
    "                  f\"{layer['gradient_size_mb']:6.2f} MB\")\n",
    "        \n",
    "        return self.layer_info\n",
    "    \n",
    "    def simulate_overlap_strategies(self):\n",
    "        \"\"\"æ¨¡æ“¬ä¸åŒé‡ç–Šç­–ç•¥\"\"\"\n",
    "        print(f\"\\n=== é‡ç–Šç­–ç•¥æ¨¡æ“¬ ===\")\n",
    "        \n",
    "        # å‡è¨­çš„è¨ˆç®—å’Œé€šè¨Šæ™‚é–“\n",
    "        compute_time_per_layer = 10  # ms\n",
    "        communication_bandwidth = 10  # GB/s\n",
    "        \n",
    "        strategies = {\n",
    "            'ç„¡é‡ç–Š': {\n",
    "                'description': 'ä¸²è¡ŒåŸ·è¡Œè¨ˆç®—å’Œé€šè¨Š',\n",
    "                'overlap_ratio': 0.0\n",
    "            },\n",
    "            'å±¤ç´šé‡ç–Š': {\n",
    "                'description': 'æŒ‰å±¤é‡ç–Šæ¢¯åº¦é€šè¨Š',\n",
    "                'overlap_ratio': 0.7\n",
    "            },\n",
    "            'åƒæ•¸çµ„é‡ç–Š': {\n",
    "                'description': 'æŒ‰åƒæ•¸çµ„é‡ç–Šé€šè¨Š',\n",
    "                'overlap_ratio': 0.8\n",
    "            },\n",
    "            'å®Œå…¨ç•°æ­¥': {\n",
    "                'description': 'å®Œå…¨ç•°æ­¥è¨ˆç®—é€šè¨Š',\n",
    "                'overlap_ratio': 0.95\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        total_compute_time = len(self.layer_info) * compute_time_per_layer\n",
    "        total_gradient_mb = sum(layer['gradient_size_mb'] for layer in self.layer_info)\n",
    "        total_comm_time = total_gradient_mb * 1000 / communication_bandwidth  # ms\n",
    "        \n",
    "        print(f\"å‡è¨­æ¢ä»¶:\")\n",
    "        print(f\"  æ¯å±¤è¨ˆç®—æ™‚é–“: {compute_time_per_layer} ms\")\n",
    "        print(f\"  é€šè¨Šå¸¶å¯¬: {communication_bandwidth} GB/s\")\n",
    "        print(f\"  ç¸½è¨ˆç®—æ™‚é–“: {total_compute_time} ms\")\n",
    "        print(f\"  ç¸½é€šè¨Šæ™‚é–“: {total_comm_time:.1f} ms\")\n",
    "        print()\n",
    "        \n",
    "        for strategy, props in strategies.items():\n",
    "            # è¨ˆç®—é‡ç–Šå¾Œçš„ç¸½æ™‚é–“\n",
    "            overlapped_comm_time = total_comm_time * (1 - props['overlap_ratio'])\n",
    "            total_time = total_compute_time + overlapped_comm_time\n",
    "            \n",
    "            # è¨ˆç®—åŠ é€Ÿæ¯”\n",
    "            baseline_time = total_compute_time + total_comm_time\n",
    "            speedup = baseline_time / total_time\n",
    "            \n",
    "            efficiency = props['overlap_ratio'] * 100\n",
    "            \n",
    "            print(f\"{strategy}:\")\n",
    "            print(f\"  æè¿°: {props['description']}\")\n",
    "            print(f\"  é‡ç–Šæ•ˆç‡: {efficiency:.1f}%\")\n",
    "            print(f\"  ç¸½æ™‚é–“: {total_time:.1f} ms\")\n",
    "            print(f\"  åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "            print()\n",
    "        \n",
    "        return strategies\n",
    "    \n",
    "    def recommend_optimization_strategy(self):\n",
    "        \"\"\"æ¨è–¦å„ªåŒ–ç­–ç•¥\"\"\"\n",
    "        print(f\"\\n=== å„ªåŒ–ç­–ç•¥æ¨è–¦ ===\")\n",
    "        \n",
    "        total_params = sum(layer['parameters'] for layer in self.layer_info)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # åŸºæ–¼æ¨¡å‹å¤§å°çš„æ¨è–¦\n",
    "        if total_params < 1e6:  # å°æ¨¡å‹\n",
    "            recommendations.append(\"ğŸ”¹ å°æ¨¡å‹ (<1Måƒæ•¸): é€šè¨Šé–‹éŠ·ç›¸å°è¼ƒå°ï¼Œé‡é»å„ªåŒ–è¨ˆç®—\")\n",
    "            recommendations.append(\"  - ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°\")\n",
    "            recommendations.append(\"  - ç°¡å–®çš„æ¢¯åº¦ç´¯ç©å³å¯\")\n",
    "        elif total_params < 1e8:  # ä¸­ç­‰æ¨¡å‹\n",
    "            recommendations.append(\"ğŸ”¹ ä¸­ç­‰æ¨¡å‹ (1M-100Måƒæ•¸): å¹³è¡¡è¨ˆç®—å’Œé€šè¨Šå„ªåŒ–\")\n",
    "            recommendations.append(\"  - å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´\")\n",
    "            recommendations.append(\"  - ä½¿ç”¨å±¤ç´šæ¢¯åº¦é‡ç–Š\")\n",
    "            recommendations.append(\"  - é©åº¦çš„æ¢¯åº¦ç´¯ç© (4-8æ­¥)\")\n",
    "        else:  # å¤§æ¨¡å‹\n",
    "            recommendations.append(\"ğŸ”¹ å¤§æ¨¡å‹ (>100Måƒæ•¸): é‡é»å„ªåŒ–é€šè¨Š\")\n",
    "            recommendations.append(\"  - å•Ÿç”¨æ‰€æœ‰é€šè¨Šå„ªåŒ–\")\n",
    "            recommendations.append(\"  - ä½¿ç”¨æ¢¯åº¦å£“ç¸®\")\n",
    "            recommendations.append(\"  - å¤§æ­¥æ•¸æ¢¯åº¦ç´¯ç© (16+æ­¥)\")\n",
    "            recommendations.append(\"  - è€ƒæ…®ZeROå„ªåŒ–å™¨\")\n",
    "        \n",
    "        # åŸºæ–¼å±¤ç´šåˆ†å¸ƒçš„æ¨è–¦\n",
    "        max_layer_params = max(layer['parameters'] for layer in self.layer_info)\n",
    "        min_layer_params = min(layer['parameters'] for layer in self.layer_info)\n",
    "        param_variance = max_layer_params / min_layer_params if min_layer_params > 0 else float('inf')\n",
    "        \n",
    "        if param_variance > 10:\n",
    "            recommendations.append(\"\\nğŸ”¹ å±¤ç´šåƒæ•¸åˆ†å¸ƒä¸å‡: ä½¿ç”¨åˆ†å±¤é€šè¨Šç­–ç•¥\")\n",
    "            recommendations.append(\"  - å¤§å±¤å–®ç¨é€šè¨Š\")\n",
    "            recommendations.append(\"  - å°å±¤èšåˆé€šè¨Š\")\n",
    "        else:\n",
    "            recommendations.append(\"\\nğŸ”¹ å±¤ç´šåƒæ•¸åˆ†å¸ƒå‡å‹»: ä½¿ç”¨çµ±ä¸€é€šè¨Šç­–ç•¥\")\n",
    "            recommendations.append(\"  - å‡å‹»çš„é‡ç–Šç­–ç•¥\")\n",
    "            recommendations.append(\"  - æ¨™æº–çš„All-Reduce\")\n",
    "        \n",
    "        # è¼¸å‡ºæ¨è–¦\n",
    "        for rec in recommendations:\n",
    "            print(rec)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# å‰µå»ºé‡ç–Šåˆ†æå™¨\n",
    "overlap_analyzer = ComputationCommunicationOverlap(demo_model)\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "layer_analysis = overlap_analyzer.analyze_overlap_potential()\n",
    "overlap_strategies = overlap_analyzer.simulate_overlap_strategies()\n",
    "optimization_recommendations = overlap_analyzer.recommend_optimization_strategy()\n",
    "\n",
    "# DDPç‰¹å®šçš„é‡ç–ŠæŠ€è¡“\n",
    "print(f\"\\n=== DDPé‡ç–ŠæŠ€è¡“è©³è§£ ===\")\n",
    "print(f\"\\n1. æ¢¯åº¦é‡ç–Š (Gradient Overlapping):\")\n",
    "print(f\"   - è¨ˆç®—å¾Œå‘æ¢¯åº¦çš„åŒæ™‚é€²è¡ŒAll-Reduce\")\n",
    "   f\"   - ä½¿ç”¨DDPçš„bucketæ©Ÿåˆ¶åˆ†æ‰¹é€šè¨Š\")\n",
    "print(f\"   - åƒæ•¸: ddp_comm_hook, bucket_size_mb\")\n",
    "print(f\"\\n2. åƒæ•¸é‡ç–Š (Parameter Overlapping):\")\n",
    "print(f\"   - åœ¨è¨ˆç®—æ¢¯åº¦æ™‚é‡ç–Šåƒæ•¸å»£æ’­\")\n",
    "print(f\"   - é©ç”¨æ–¼ZeRO-3ç­‰åƒæ•¸åˆ†ç‰‡æ–¹æ¡ˆ\")\n",
    "print(f\"   - æ¸›å°‘åƒæ•¸è¼‰å…¥çš„ç­‰å¾…æ™‚é–“\")\n",
    "print(f\"\\n3. æµæ°´ç·šé‡ç–Š (Pipeline Overlapping):\")\n",
    "print(f\"   - ä¸åŒå±¤çš„è¨ˆç®—å’Œé€šè¨Šæµæ°´ç·šåŸ·è¡Œ\")\n",
    "print(f\"   - éœ€è¦ä»”ç´°çš„ä¾è³´é—œä¿‚ç®¡ç†\")\n",
    "print(f\"   - æœ€å¤§åŒ–ç¡¬é«”åˆ©ç”¨ç‡\")\n",
    "print(f\"\\n4. ç•°æ­¥é€šè¨Š (Asynchronous Communication):\")\n",
    "print(f\"   - ä½¿ç”¨CUDAæµå¯¦ç¾çœŸæ­£çš„ç•°æ­¥\")\n",
    "print(f\"   - éœ€è¦åŒæ­¥é»ä¾†ä¿è­‰æ­£ç¢ºæ€§\")\n",
    "print(f\"   - å¹³è¡¡å»¶é²å’Œä¸€è‡´æ€§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½ç›£æ§èˆ‡èª¿å„ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPPerformanceMonitor:\n",
    "    \"\"\"\n",
    "    DDPæ€§èƒ½ç›£æ§å™¨\n",
    "    å¯¦æ™‚ç›£æ§å’Œåˆ†æåˆ†æ•£å¼è¨“ç·´æ€§èƒ½\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, world_size=1):\n",
    "        self.model = model\n",
    "        self.world_size = world_size\n",
    "        self.monitoring_data = {\n",
    "            'timestamps': [],\n",
    "            'gpu_memory': [],\n",
    "            'gpu_utilization': [],\n",
    "            'cpu_usage': [],\n",
    "            'network_io': [],\n",
    "            'step_times': [],\n",
    "            'loss_values': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        self.is_monitoring = False\n",
    "        self.monitor_thread = None\n",
    "        \n",
    "    def start_monitoring(self, interval=1.0):\n",
    "        \"\"\"é–‹å§‹æ€§èƒ½ç›£æ§\"\"\"\n",
    "        if self.is_monitoring:\n",
    "            print(\"ç›£æ§å·²åœ¨é‹è¡Œä¸­\")\n",
    "            return\n",
    "        \n",
    "        self.is_monitoring = True\n",
    "        self.monitor_thread = threading.Thread(\n",
    "            target=self._monitoring_loop,\n",
    "            args=(interval,),\n",
    "            daemon=True\n",
    "        )\n",
    "        self.monitor_thread.start()\n",
    "        print(f\"âœ… é–‹å§‹æ€§èƒ½ç›£æ§ (é–“éš”: {interval}s)\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"åœæ­¢æ€§èƒ½ç›£æ§\"\"\"\n",
    "        if not self.is_monitoring:\n",
    "            print(\"ç›£æ§æœªé‹è¡Œ\")\n",
    "            return\n",
    "        \n",
    "        self.is_monitoring = False\n",
    "        if self.monitor_thread:\n",
    "            self.monitor_thread.join()\n",
    "        print(\"â¹ï¸ æ€§èƒ½ç›£æ§å·²åœæ­¢\")\n",
    "    \n",
    "    def _monitoring_loop(self, interval):\n",
    "        \"\"\"ç›£æ§å¾ªç’°\"\"\"\n",
    "        while self.is_monitoring:\n",
    "            try:\n",
    "                timestamp = time.time()\n",
    "                \n",
    "                # GPUç›£æ§\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB\n",
    "                    gpu_utilization = torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else 0\n",
    "                else:\n",
    "                    gpu_memory = 0\n",
    "                    gpu_utilization = 0\n",
    "                \n",
    "                # CPUç›£æ§\n",
    "                cpu_usage = psutil.cpu_percent()\n",
    "                \n",
    "                # ç¶²çµ¡IO (ç°¡åŒ–)\n",
    "                net_io = psutil.net_io_counters()\n",
    "                network_io = net_io.bytes_sent + net_io.bytes_recv\n",
    "                \n",
    "                # è¨˜éŒ„æ•¸æ“š\n",
    "                self.monitoring_data['timestamps'].append(timestamp)\n",
    "                self.monitoring_data['gpu_memory'].append(gpu_memory)\n",
    "                self.monitoring_data['gpu_utilization'].append(gpu_utilization)\n",
    "                self.monitoring_data['cpu_usage'].append(cpu_usage)\n",
    "                self.monitoring_data['network_io'].append(network_io)\n",
    "                \n",
    "                time.sleep(interval)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ç›£æ§éŒ¯èª¤: {e}\")\n",
    "                break\n",
    "    \n",
    "    def record_training_step(self, step_time, loss, learning_rate):\n",
    "        \"\"\"è¨˜éŒ„è¨“ç·´æ­¥é©Ÿæ•¸æ“š\"\"\"\n",
    "        self.monitoring_data['step_times'].append(step_time)\n",
    "        self.monitoring_data['loss_values'].append(loss)\n",
    "        self.monitoring_data['learning_rates'].append(learning_rate)\n",
    "    \n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"ç”Ÿæˆæ€§èƒ½å ±å‘Š\"\"\"\n",
    "        print(\"\\n=== DDP æ€§èƒ½å ±å‘Š ===\")\n",
    "        \n",
    "        # åŸºæœ¬çµ±è¨ˆ\n",
    "        if self.monitoring_data['step_times']:\n",
    "            avg_step_time = np.mean(self.monitoring_data['step_times'])\n",
    "            throughput = 1000 / avg_step_time  # steps per second\n",
    "            print(f\"\\nè¨“ç·´æ€§èƒ½:\")\n",
    "            print(f\"  å¹³å‡æ­¥é©Ÿæ™‚é–“: {avg_step_time:.2f} ms\")\n",
    "            print(f\"  è¨“ç·´ååé‡: {throughput:.2f} steps/sec\")\n",
    "            print(f\"  ç¸½è¨“ç·´æ­¥æ•¸: {len(self.monitoring_data['step_times'])}\")\n",
    "        \n",
    "        # è³‡æºä½¿ç”¨çµ±è¨ˆ\n",
    "        if self.monitoring_data['gpu_memory']:\n",
    "            avg_gpu_mem = np.mean(self.monitoring_data['gpu_memory'])\n",
    "            max_gpu_mem = np.max(self.monitoring_data['gpu_memory'])\n",
    "            print(f\"\\nGPUè³‡æº:\")\n",
    "            print(f\"  å¹³å‡GPUè¨˜æ†¶é«”: {avg_gpu_mem:.1f} MB\")\n",
    "            print(f\"  å³°å€¼GPUè¨˜æ†¶é«”: {max_gpu_mem:.1f} MB\")\n",
    "        \n",
    "        if self.monitoring_data['cpu_usage']:\n",
    "            avg_cpu = np.mean(self.monitoring_data['cpu_usage'])\n",
    "            max_cpu = np.max(self.monitoring_data['cpu_usage'])\n",
    "            print(f\"\\nCPUè³‡æº:\")\n",
    "            print(f\"  å¹³å‡CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%\")\n",
    "            print(f\"  å³°å€¼CPUä½¿ç”¨ç‡: {max_cpu:.1f}%\")\n",
    "        \n",
    "        # ç¶²çµ¡çµ±è¨ˆ\n",
    "        if len(self.monitoring_data['network_io']) > 1:\n",
    "            total_io = self.monitoring_data['network_io'][-1] - self.monitoring_data['network_io'][0]\n",
    "            duration = self.monitoring_data['timestamps'][-1] - self.monitoring_data['timestamps'][0]\n",
    "            avg_bandwidth = total_io / duration / (1024**2)  # MB/s\n",
    "            print(f\"\\nç¶²çµ¡IO:\")\n",
    "            print(f\"  ç¸½æ•¸æ“šå‚³è¼¸: {total_io / (1024**2):.1f} MB\")\n",
    "            print(f\"  å¹³å‡å¸¶å¯¬: {avg_bandwidth:.2f} MB/s\")\n",
    "        \n",
    "        # æ•ˆç‡åˆ†æ\n",
    "        self._analyze_efficiency()\n",
    "        \n",
    "        return self.monitoring_data\n",
    "    \n",
    "    def _analyze_efficiency(self):\n",
    "        \"\"\"åˆ†æè¨“ç·´æ•ˆç‡\"\"\"\n",
    "        print(f\"\\n=== æ•ˆç‡åˆ†æ ===\")\n",
    "        \n",
    "        # è¨ˆç®—ç†è«–vså¯¦éš›æ€§èƒ½\n",
    "        model_params = sum(p.numel() for p in self.model.parameters())\n",
    "        model_flops = model_params * 2  # ç°¡åŒ–ä¼°ç®—\n",
    "        \n",
    "        if self.monitoring_data['step_times']:\n",
    "            avg_step_time_s = np.mean(self.monitoring_data['step_times']) / 1000\n",
    "            actual_flops_per_sec = model_flops / avg_step_time_s\n",
    "            \n",
    "            print(f\"è¨ˆç®—æ•ˆç‡:\")\n",
    "            print(f\"  æ¨¡å‹åƒæ•¸é‡: {model_params:,}\")\n",
    "            print(f\"  ä¼°ç®—FLOPS: {model_flops:,}\")\n",
    "            print(f\"  å¯¦éš›FLOPS/ç§’: {actual_flops_per_sec:.2e}\")\n",
    "        \n",
    "        # é€šè¨Šæ•ˆç‡ï¼ˆåƒ…å¤šGPUæ™‚æœ‰æ„ç¾©ï¼‰\n",
    "        if self.world_size > 1:\n",
    "            gradient_size_mb = model_params * 4 / (1024**2)  # FP32\n",
    "            print(f\"\\né€šè¨Šæ•ˆç‡:\")\n",
    "            print(f\"  æ¢¯åº¦å¤§å°: {gradient_size_mb:.2f} MB\")\n",
    "            print(f\"  ç†è«–é€šè¨Šæ™‚é–“ (10GB/s): {gradient_size_mb * 8 / 10:.2f} ms\")\n",
    "            if self.monitoring_data['step_times']:\n",
    "                comm_overhead = avg_step_time_s * 1000 * 0.1  # å‡è¨­10%æ˜¯é€šè¨Š\n",
    "                print(f\"  ä¼°ç®—é€šè¨Šé–‹éŠ·: {comm_overhead:.2f} ms\")\n",
    "        \n",
    "        # æ“´å±•æ•ˆç‡\n",
    "        if self.world_size > 1:\n",
    "            ideal_speedup = self.world_size\n",
    "            if self.monitoring_data['step_times']:\n",
    "                # å‡è¨­å–®GPUåŸºæº–æ™‚é–“\n",
    "                single_gpu_time = avg_step_time_s * self.world_size * 0.9  # 90%ç†æƒ³\n",
    "                actual_speedup = single_gpu_time / avg_step_time_s\n",
    "                efficiency = actual_speedup / ideal_speedup * 100\n",
    "                \n",
    "                print(f\"\\næ“´å±•æ•ˆç‡:\")\n",
    "                print(f\"  ç†æƒ³åŠ é€Ÿæ¯”: {ideal_speedup:.1f}x\")\n",
    "                print(f\"  å¯¦éš›åŠ é€Ÿæ¯”: {actual_speedup:.1f}x\")\n",
    "                print(f\"  æ“´å±•æ•ˆç‡: {efficiency:.1f}%\")\n",
    "    \n",
    "    def plot_performance_curves(self):\n",
    "        \"\"\"ç¹ªè£½æ€§èƒ½æ›²ç·š\"\"\"\n",
    "        if not self.monitoring_data['timestamps']:\n",
    "            print(\"ç„¡ç›£æ§æ•¸æ“šå¯ç¹ªè£½\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # GPUè¨˜æ†¶é«”ä½¿ç”¨\n",
    "        if self.monitoring_data['gpu_memory']:\n",
    "            axes[0, 0].plot(self.monitoring_data['gpu_memory'])\n",
    "            axes[0, 0].set_title('GPU è¨˜æ†¶é«”ä½¿ç”¨')\n",
    "            axes[0, 0].set_ylabel('è¨˜æ†¶é«” (MB)')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # CPUä½¿ç”¨ç‡\n",
    "        if self.monitoring_data['cpu_usage']:\n",
    "            axes[0, 1].plot(self.monitoring_data['cpu_usage'])\n",
    "            axes[0, 1].set_title('CPU ä½¿ç”¨ç‡')\n",
    "            axes[0, 1].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # è¨“ç·´æ­¥é©Ÿæ™‚é–“\n",
    "        if self.monitoring_data['step_times']:\n",
    "            axes[1, 0].plot(self.monitoring_data['step_times'])\n",
    "            axes[1, 0].set_title('è¨“ç·´æ­¥é©Ÿæ™‚é–“')\n",
    "            axes[1, 0].set_ylabel('æ™‚é–“ (ms)')\n",
    "            axes[1, 0].set_xlabel('æ­¥é©Ÿ')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æå¤±æ›²ç·š\n",
    "        if self.monitoring_data['loss_values']:\n",
    "            axes[1, 1].plot(self.monitoring_data['loss_values'])\n",
    "            axes[1, 1].set_title('è¨“ç·´æå¤±')\n",
    "            axes[1, 1].set_ylabel('æå¤±')\n",
    "            axes[1, 1].set_xlabel('æ­¥é©Ÿ')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('ddp_performance_curves.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"ğŸ“Š æ€§èƒ½æ›²ç·šå·²ä¿å­˜åˆ° ddp_performance_curves.png\")\n",
    "\n",
    "# æ¼”ç¤ºæ€§èƒ½ç›£æ§\n",
    "print(\"=== DDP æ€§èƒ½ç›£æ§æ¼”ç¤º ===\")\n",
    "\n",
    "# å‰µå»ºç›£æ§å™¨\n",
    "monitor = DDPPerformanceMonitor(demo_model, world_size=4)\n",
    "\n",
    "# é–‹å§‹ç›£æ§\n",
    "monitor.start_monitoring(interval=0.5)\n",
    "\n",
    "# æ¨¡æ“¬è¨“ç·´éç¨‹\n",
    "print(\"\\næ¨¡æ“¬è¨“ç·´éç¨‹...\")\n",
    "optimizer = torch.optim.AdamW(demo_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(10):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # æ¨¡æ“¬å‰å‘å‚³æ’­\n",
    "    inputs = torch.randn(8, 1024).to(device)\n",
    "    targets = torch.randint(0, 10, (8,)).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = demo_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    step_time = (time.time() - start_time) * 1000  # ms\n",
    "    \n",
    "    # è¨˜éŒ„è¨“ç·´æ•¸æ“š\n",
    "    monitor.record_training_step(step_time, loss.item(), 1e-3)\n",
    "    \n",
    "    print(f\"æ­¥é©Ÿ {step+1}: æå¤± {loss.item():.4f}, æ™‚é–“ {step_time:.2f}ms\")\n",
    "    \n",
    "    time.sleep(0.1)  # æ¨¡æ“¬è¨“ç·´é–“éš”\n",
    "\n",
    "# åœæ­¢ç›£æ§ä¸¦ç”Ÿæˆå ±å‘Š\n",
    "time.sleep(1)  # ç¢ºä¿æ”¶é›†åˆ°è¶³å¤ æ•¸æ“š\n",
    "monitor.stop_monitoring()\n",
    "\n",
    "# ç”Ÿæˆæ€§èƒ½å ±å‘Š\n",
    "performance_data = monitor.generate_performance_report()\n",
    "\n",
    "# ç¹ªè£½æ€§èƒ½æ›²ç·š\n",
    "monitor.plot_performance_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å„ªåŒ–é…ç½®ç”Ÿæˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPOptimizationConfigGenerator:\n",
    "    \"\"\"\n",
    "    DDPå„ªåŒ–é…ç½®ç”Ÿæˆå™¨\n",
    "    æ ¹æ“šç¡¬é«”ç’°å¢ƒå’Œæ¨¡å‹ç‰¹æ€§ç”Ÿæˆæœ€å„ªé…ç½®\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hardware_profiles = {\n",
    "            'single_gpu': {\n",
    "                'gpu_count': 1,\n",
    "                'memory_per_gpu': 16,\n",
    "                'bandwidth': 0,\n",
    "                'description': 'å–®GPUç’°å¢ƒ'\n",
    "            },\n",
    "            'dual_gpu': {\n",
    "                'gpu_count': 2,\n",
    "                'memory_per_gpu': 16,\n",
    "                'bandwidth': 50,  # GB/s\n",
    "                'description': 'é›™GPUå·¥ä½œç«™'\n",
    "            },\n",
    "            'quad_gpu': {\n",
    "                'gpu_count': 4,\n",
    "                'memory_per_gpu': 24,\n",
    "                'bandwidth': 100,\n",
    "                'description': 'å››GPUé«˜ç«¯å·¥ä½œç«™'\n",
    "            },\n",
    "            'dgx_node': {\n",
    "                'gpu_count': 8,\n",
    "                'memory_per_gpu': 80,\n",
    "                'bandwidth': 600,  # NVLink\n",
    "                'description': 'DGXç¯€é»'\n",
    "            },\n",
    "            'multi_node': {\n",
    "                'gpu_count': 32,\n",
    "                'memory_per_gpu': 80,\n",
    "                'bandwidth': 200,  # InfiniBand\n",
    "                'description': 'å¤šç¯€é»é›†ç¾¤'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_model_requirements(self, model):\n",
    "        \"\"\"åˆ†ææ¨¡å‹éœ€æ±‚\"\"\"\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # ä¼°ç®—è¨˜æ†¶é«”éœ€æ±‚ (FP32)\n",
    "        model_memory = total_params * 4 / (1024**3)  # GB\n",
    "        gradient_memory = model_memory\n",
    "        optimizer_memory = model_memory * 2  # Adam state\n",
    "        activation_memory = 2  # ä¼°ç®—å€¼\n",
    "        \n",
    "        total_memory = model_memory + gradient_memory + optimizer_memory + activation_memory\n",
    "        \n",
    "        return {\n",
    "            'total_params': total_params,\n",
    "            'model_memory_gb': model_memory,\n",
    "            'total_memory_gb': total_memory,\n",
    "            'gradient_size_gb': gradient_memory\n",
    "        }\n",
    "    \n",
    "    def generate_config(self, model, hardware_profile='auto', target_batch_size=32):\n",
    "        \"\"\"ç”Ÿæˆå„ªåŒ–é…ç½®\"\"\"\n",
    "        model_reqs = self.analyze_model_requirements(model)\n",
    "        \n",
    "        if hardware_profile == 'auto':\n",
    "            hardware_profile = self._select_hardware_profile(model_reqs)\n",
    "        \n",
    "        hw_profile = self.hardware_profiles[hardware_profile]\n",
    "        \n",
    "        config = {\n",
    "            'hardware': {\n",
    "                'profile': hardware_profile,\n",
    "                'gpu_count': hw_profile['gpu_count'],\n",
    "                'memory_per_gpu': hw_profile['memory_per_gpu'],\n",
    "                'bandwidth_gbps': hw_profile['bandwidth']\n",
    "            },\n",
    "            'model': model_reqs,\n",
    "            'training': self._generate_training_config(model_reqs, hw_profile, target_batch_size),\n",
    "            'optimization': self._generate_optimization_config(model_reqs, hw_profile),\n",
    "            'monitoring': self._generate_monitoring_config(hw_profile)\n",
    "        }\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _select_hardware_profile(self, model_reqs):\n",
    "        \"\"\"è‡ªå‹•é¸æ“‡ç¡¬é«”é…ç½®\"\"\"\n",
    "        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "        \n",
    "        if gpu_count == 1:\n",
    "            return 'single_gpu'\n",
    "        elif gpu_count == 2:\n",
    "            return 'dual_gpu'\n",
    "        elif gpu_count <= 4:\n",
    "            return 'quad_gpu'\n",
    "        elif gpu_count <= 8:\n",
    "            return 'dgx_node'\n",
    "        else:\n",
    "            return 'multi_node'\n",
    "    \n",
    "    def _generate_training_config(self, model_reqs, hw_profile, target_batch_size):\n",
    "        \"\"\"ç”Ÿæˆè¨“ç·´é…ç½®\"\"\"\n",
    "        gpu_count = hw_profile['gpu_count']\n",
    "        memory_per_gpu = hw_profile['memory_per_gpu']\n",
    "        \n",
    "        # è¨ˆç®—æœ€å¤§å¯è¡Œæ‰¹æ¬¡å¤§å°\n",
    "        available_memory = memory_per_gpu * 0.8  # ç•™20%ç·©è¡\n",
    "        max_batch_per_gpu = max(1, int(available_memory / model_reqs['total_memory_gb']))\n",
    "        \n",
    "        # è¨ˆç®—æ¢¯åº¦ç´¯ç©\n",
    "        total_batch_capacity = max_batch_per_gpu * gpu_count\n",
    "        if target_batch_size <= total_batch_capacity:\n",
    "            micro_batch_size = target_batch_size // gpu_count\n",
    "            gradient_accumulation = 1\n",
    "        else:\n",
    "            micro_batch_size = max_batch_per_gpu\n",
    "            gradient_accumulation = target_batch_size // total_batch_capacity\n",
    "        \n",
    "        # æ··åˆç²¾åº¦æ¨è–¦\n",
    "        use_amp = model_reqs['total_params'] > 1e6  # å¤§æ–¼1Måƒæ•¸æ¨è–¦AMP\n",
    "        \n",
    "        return {\n",
    "            'batch_size_per_gpu': micro_batch_size,\n",
    "            'gradient_accumulation_steps': gradient_accumulation,\n",
    "            'effective_batch_size': micro_batch_size * gpu_count * gradient_accumulation,\n",
    "            'use_mixed_precision': use_amp,\n",
    "            'gradient_clipping': 1.0,\n",
    "            'dataloader_num_workers': min(4, max(1, gpu_count))\n",
    "        }\n",
    "    \n",
    "    def _generate_optimization_config(self, model_reqs, hw_profile):\n",
    "        \"\"\"ç”Ÿæˆå„ªåŒ–é…ç½®\"\"\"\n",
    "        gpu_count = hw_profile['gpu_count']\n",
    "        bandwidth = hw_profile['bandwidth']\n",
    "        \n",
    "        # é€šè¨Šå¾Œç«¯\n",
    "        if gpu_count > 1:\n",
    "            backend = 'nccl'\n",
    "        else:\n",
    "            backend = None\n",
    "        \n",
    "        # DDPåƒæ•¸\n",
    "        bucket_size_mb = 25  # é è¨­å€¼\n",
    "        if model_reqs['gradient_size_gb'] * 1024 < bucket_size_mb:\n",
    "            bucket_size_mb = max(1, int(model_reqs['gradient_size_gb'] * 1024))\n",
    "        \n",
    "        # é€šè¨Šå„ªåŒ–\n",
    "        use_gradient_compression = model_reqs['total_params'] > 1e8 and gpu_count > 4\n",
    "        overlap_communication = gpu_count > 2\n",
    "        \n",
    "        config = {\n",
    "            'backend': backend,\n",
    "            'bucket_size_mb': bucket_size_mb,\n",
    "            'find_unused_parameters': False,\n",
    "            'broadcast_buffers': True,\n",
    "            'gradient_as_bucket_view': True\n",
    "        }\n",
    "        \n",
    "        if use_gradient_compression:\n",
    "            config['gradient_compression'] = {\n",
    "                'method': 'fp16',\n",
    "                'ratio': 2.0\n",
    "            }\n",
    "        \n",
    "        if overlap_communication:\n",
    "            config['communication_overlap'] = {\n",
    "                'enabled': True,\n",
    "                'overlap_ratio': 0.8\n",
    "            }\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _generate_monitoring_config(self, hw_profile):\n",
    "        \"\"\"ç”Ÿæˆç›£æ§é…ç½®\"\"\"\n",
    "        return {\n",
    "            'log_interval': 50,\n",
    "            'save_interval': 1000,\n",
    "            'eval_interval': 500,\n",
    "            'monitor_gpu_memory': True,\n",
    "            'monitor_communication': hw_profile['gpu_count'] > 1,\n",
    "            'profile_steps': 100 if hw_profile['gpu_count'] > 1 else 0\n",
    "        }\n",
    "    \n",
    "    def print_config(self, config):\n",
    "        \"\"\"æ‰“å°é…ç½®\"\"\"\n",
    "        print(\"\\n=== DDP å„ªåŒ–é…ç½® ===\")\n",
    "        \n",
    "        print(f\"\\nğŸ–¥ï¸ ç¡¬é«”é…ç½®:\")\n",
    "        hw = config['hardware']\n",
    "        print(f\"  é…ç½®é¡å‹: {hw['profile']}\")\n",
    "        print(f\"  GPUæ•¸é‡: {hw['gpu_count']}\")\n",
    "        print(f\"  æ¯GPUè¨˜æ†¶é«”: {hw['memory_per_gpu']} GB\")\n",
    "        print(f\"  ç¶²çµ¡å¸¶å¯¬: {hw['bandwidth_gbps']} GB/s\")\n",
    "        \n",
    "        print(f\"\\nğŸ¤– æ¨¡å‹åˆ†æ:\")\n",
    "        model = config['model']\n",
    "        print(f\"  åƒæ•¸é‡: {model['total_params']:,}\")\n",
    "        print(f\"  æ¨¡å‹è¨˜æ†¶é«”: {model['model_memory_gb']:.2f} GB\")\n",
    "        print(f\"  ç¸½è¨˜æ†¶é«”éœ€æ±‚: {model['total_memory_gb']:.2f} GB\")\n",
    "        \n",
    "        print(f\"\\nğŸš€ è¨“ç·´é…ç½®:\")\n",
    "        train = config['training']\n",
    "        print(f\"  æ¯GPUæ‰¹æ¬¡å¤§å°: {train['batch_size_per_gpu']}\")\n",
    "        print(f\"  æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {train['gradient_accumulation_steps']}\")\n",
    "        print(f\"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {train['effective_batch_size']}\")\n",
    "        print(f\"  æ··åˆç²¾åº¦: {'âœ…' if train['use_mixed_precision'] else 'âŒ'}\")\n",
    "        print(f\"  æ¢¯åº¦è£å‰ª: {train['gradient_clipping']}\")\n",
    "        \n",
    "        print(f\"\\nâš¡ å„ªåŒ–é…ç½®:\")\n",
    "        opt = config['optimization']\n",
    "        print(f\"  é€šè¨Šå¾Œç«¯: {opt['backend'] or 'N/A'}\")\n",
    "        print(f\"  DDPæ¡¶å¤§å°: {opt['bucket_size_mb']} MB\")\n",
    "        print(f\"  æŸ¥æ‰¾æœªä½¿ç”¨åƒæ•¸: {'âœ…' if opt['find_unused_parameters'] else 'âŒ'}\")\n",
    "        \n",
    "        if 'gradient_compression' in opt:\n",
    "            gc = opt['gradient_compression']\n",
    "            print(f\"  æ¢¯åº¦å£“ç¸®: {gc['method']} ({gc['ratio']}x)\")\n",
    "        \n",
    "        if 'communication_overlap' in opt:\n",
    "            co = opt['communication_overlap']\n",
    "            print(f\"  é€šè¨Šé‡ç–Š: âœ… ({co['overlap_ratio']:.1%})\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ç›£æ§é…ç½®:\")\n",
    "        mon = config['monitoring']\n",
    "        print(f\"  æ—¥èªŒé–“éš”: {mon['log_interval']}æ­¥\")\n",
    "        print(f\"  ä¿å­˜é–“éš”: {mon['save_interval']}æ­¥\")\n",
    "        print(f\"  é©—è­‰é–“éš”: {mon['eval_interval']}æ­¥\")\n",
    "        print(f\"  GPUç›£æ§: {'âœ…' if mon['monitor_gpu_memory'] else 'âŒ'}\")\n",
    "        print(f\"  é€šè¨Šç›£æ§: {'âœ…' if mon['monitor_communication'] else 'âŒ'}\")\n",
    "    \n",
    "    def save_config_file(self, config, filename='ddp_config.json'):\n",
    "        \"\"\"ä¿å­˜é…ç½®æ–‡ä»¶\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nğŸ’¾ é…ç½®å·²ä¿å­˜åˆ° {filename}\")\n",
    "\n",
    "# æ¼”ç¤ºé…ç½®ç”Ÿæˆ\n",
    "print(\"=== DDP é…ç½®ç”Ÿæˆæ¼”ç¤º ===\")\n",
    "\n",
    "config_generator = DDPOptimizationConfigGenerator()\n",
    "\n",
    "# ç‚ºç¤ºä¾‹æ¨¡å‹ç”Ÿæˆé…ç½®\n",
    "optimized_config = config_generator.generate_config(\n",
    "    model=demo_model,\n",
    "    hardware_profile='auto',\n",
    "    target_batch_size=64\n",
    ")\n",
    "\n",
    "# æ‰“å°é…ç½®\n",
    "config_generator.print_config(optimized_config)\n",
    "\n",
    "# ä¿å­˜é…ç½®\n",
    "config_generator.save_config_file(optimized_config, 'optimal_ddp_config.json')\n",
    "\n",
    "# ç”Ÿæˆä¸åŒç¡¬é«”ç’°å¢ƒçš„é…ç½®å°æ¯”\n",
    "print(\"\\n=== ä¸åŒç¡¬é«”ç’°å¢ƒé…ç½®å°æ¯” ===\")\n",
    "hardware_types = ['single_gpu', 'quad_gpu', 'dgx_node']\n",
    "\n",
    "for hw_type in hardware_types:\n",
    "    print(f\"\\n--- {hw_type.upper()} ---\")\n",
    "    test_config = config_generator.generate_config(\n",
    "        model=demo_model,\n",
    "        hardware_profile=hw_type,\n",
    "        target_batch_size=64\n",
    "    )\n",
    "    \n",
    "    # åªæ‰“å°é—œéµä¿¡æ¯\n",
    "    hw = test_config['hardware']\n",
    "    train = test_config['training']\n",
    "    print(f\"GPU: {hw['gpu_count']}, æ‰¹æ¬¡: {train['batch_size_per_gpu']}, \"\n",
    "          f\"ç´¯ç©: {train['gradient_accumulation_steps']}, \"\n",
    "          f\"æœ‰æ•ˆæ‰¹æ¬¡: {train['effective_batch_size']}, \"\n",
    "          f\"AMP: {'âœ…' if train['use_mixed_precision'] else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç¸½çµèˆ‡æœ€ä½³å¯¦è¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Lab-1.2 Optimization å®Œæˆç¸½çµ ===\")\n",
    "print()\n",
    "print(\"âœ… å·²å®Œæˆçš„å„ªåŒ–æŠ€è¡“å­¸ç¿’:\")\n",
    "print(\"  1. âœ… é€šè¨Šåˆ†æèˆ‡ç“¶é ¸è­˜åˆ¥\")\n",
    "print(\"  2. âœ… æ¢¯åº¦ç´¯ç©å„ªåŒ–ç­–ç•¥\")\n",
    "print(\"  3. âœ… æ··åˆç²¾åº¦è¨“ç·´é…ç½®\")\n",
    "print(\"  4. âœ… è¨ˆç®—é€šè¨Šé‡ç–ŠæŠ€è¡“\")\n",
    "print(\"  5. âœ… æ€§èƒ½ç›£æ§èˆ‡åˆ†æ\")\n",
    "print(\"  6. âœ… è‡ªå‹•åŒ–é…ç½®ç”Ÿæˆ\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ¯ é—œéµå„ªåŒ–ç­–ç•¥ç¸½çµ:\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“¡ é€šè¨Šå„ªåŒ–:\")\n",
    "print(\"  â€¢ ä½¿ç”¨é©ç•¶çš„é€šè¨Šå¾Œç«¯ (NCCL for GPU)\")\n",
    "print(\"  â€¢ èª¿æ•´DDP bucketå¤§å°ä»¥å¹³è¡¡å»¶é²å’Œååé‡\")\n",
    "print(\"  â€¢ å•Ÿç”¨æ¢¯åº¦å£“ç¸® (FP16/BF16) æ¸›å°‘æ•¸æ“šå‚³è¼¸\")\n",
    "print(\"  â€¢ ä½¿ç”¨æ¢¯åº¦ç´¯ç©æ¸›å°‘é€šè¨Šé »ç‡\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’¾ è¨˜æ†¶é«”å„ªåŒ–:\")\n",
    "print(\"  â€¢ æ··åˆç²¾åº¦è¨“ç·´ (AMP) ç¯€çœè¨˜æ†¶é«”\")\n",
    "print(\"  â€¢ æ¢¯åº¦æª¢æŸ¥é»æŠ€è¡“é©ç”¨æ–¼è¶…å¤§æ¨¡å‹\")\n",
    "print(\"  â€¢ å„ªåŒ–å™¨ç‹€æ…‹å¸è¼‰åˆ°CPU (ZeRO-2)\")\n",
    "print(\"  â€¢ å‹•æ…‹æ‰¹æ¬¡å¤§å°èª¿æ•´\")\n",
    "print()\n",
    "\n",
    "print(\"âš¡ è¨ˆç®—å„ªåŒ–:\")\n",
    "print(\"  â€¢ è¨ˆç®—é€šè¨Šé‡ç–Šæœ€å¤§åŒ–ç¡¬é«”åˆ©ç”¨ç‡\")\n",
    "print(\"  â€¢ ä½¿ç”¨ç·¨è­¯å„ªåŒ– (torch.compile)\")\n",
    "print(\"  â€¢ Tensor Coreå„ªåŒ–æ··åˆç²¾åº¦é‹ç®—\")\n",
    "print(\"  â€¢ é©ç•¶çš„æ•¸æ“šè¼‰å…¥ä¸¦è¡Œåº¦\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“Š ç›£æ§å„ªåŒ–:\")\n",
    "print(\"  â€¢ å¯¦æ™‚æ€§èƒ½ç›£æ§è­˜åˆ¥ç“¶é ¸\")\n",
    "print(\"  â€¢ GPUåˆ©ç”¨ç‡å’Œè¨˜æ†¶é«”ä½¿ç”¨è¿½è¹¤\")\n",
    "print(\"  â€¢ é€šè¨Šé–‹éŠ·åˆ†æ\")\n",
    "print(\"  â€¢ è‡ªå‹•åŒ–æ€§èƒ½èª¿å„ª\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”§ é…ç½®æœ€ä½³å¯¦è¸:\")\n",
    "optimization_best_practices = {\n",
    "    'å°æ¨¡å‹ (<10Måƒæ•¸)': {\n",
    "        'batch_size': 'ç›¡é‡å¤§',\n",
    "        'gradient_accumulation': '1-2æ­¥',\n",
    "        'mixed_precision': 'å¯é¸',\n",
    "        'communication': 'æ¨™æº–è¨­ç½®'\n",
    "    },\n",
    "    'ä¸­å‹æ¨¡å‹ (10M-1Båƒæ•¸)': {\n",
    "        'batch_size': 'é©ä¸­',\n",
    "        'gradient_accumulation': '4-8æ­¥',\n",
    "        'mixed_precision': 'å¼·çƒˆæ¨è–¦',\n",
    "        'communication': 'æ¢¯åº¦å£“ç¸®'\n",
    "    },\n",
    "    'å¤§å‹æ¨¡å‹ (>1Båƒæ•¸)': {\n",
    "        'batch_size': 'è¨˜æ†¶é«”é™åˆ¶',\n",
    "        'gradient_accumulation': '16+æ­¥',\n",
    "        'mixed_precision': 'å¿…éœ€',\n",
    "        'communication': 'å…¨é¢å„ªåŒ–'\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_size, practices in optimization_best_practices.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    for aspect, recommendation in practices.items():\n",
    "        print(f\"  â€¢ {aspect}: {recommendation}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸš€ å¤šGPUç’°å¢ƒå¯¦æˆ°å»ºè­°:\")\n",
    "print(\"  1. å¾å°è¦æ¨¡é–‹å§‹ï¼Œé€æ­¥æ“´å±•åˆ°å¤šGPU\")\n",
    "print(\"  2. ä½¿ç”¨æ€§èƒ½ç›£æ§è­˜åˆ¥ç“¶é ¸\")\n",
    "print(\"  3. æ¸¬è©¦ä¸åŒçš„æ‰¹æ¬¡å¤§å°å’Œç´¯ç©ç­–ç•¥\")\n",
    "print(\"  4. é©—è­‰æ•¸å€¼ç©©å®šæ€§å’Œæ”¶æ–‚æ€§\")\n",
    "print(\"  5. å»ºç«‹è‡ªå‹•åŒ–èª¿å„ªæµç¨‹\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ ç”Ÿæˆçš„å·¥å…·å’Œé…ç½®:\")\n",
    "print(\"  â€¢ ddp_performance_curves.png: æ€§èƒ½ç›£æ§åœ–è¡¨\")\n",
    "print(\"  â€¢ optimal_ddp_config.json: å„ªåŒ–é…ç½®æ–‡ä»¶\")\n",
    "print(\"  â€¢ DDPCommunicationAnalyzer: é€šè¨Šåˆ†æå·¥å…·\")\n",
    "print(\"  â€¢ MixedPrecisionDDPTrainer: æ··åˆç²¾åº¦è¨“ç·´å™¨\")\n",
    "print(\"  â€¢ DDPPerformanceMonitor: æ€§èƒ½ç›£æ§å™¨\")\n",
    "print(\"  â€¢ DDPOptimizationConfigGenerator: é…ç½®ç”Ÿæˆå™¨\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ ä¸‹ä¸€æ­¥å­¸ç¿’:\")\n",
    "print(\"  - 04-Advanced.ipynb: é€²éšæŠ€è¡“å’Œæ•…éšœè™•ç†\")\n",
    "print(\"  - Lab-1.3: DeepSpeed ZeRO å„ªåŒ–\")\n",
    "print(\"  - åœ¨çœŸå¯¦å¤šGPUç’°å¢ƒä¸­é©—è­‰å„ªåŒ–æ•ˆæœ\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’¡ é—œéµè¨˜æ†¶é»:\")\n",
    "print(\"  - é€šè¨Šé–‹éŠ·éš¨GPUæ•¸é‡ç·šæ€§å¢é•·\")\n",
    "print(\"  - æ¢¯åº¦ç´¯ç©æ˜¯æ¸›å°‘é€šè¨Šçš„æœ€æœ‰æ•ˆæ–¹æ³•\")\n",
    "print(\"  - æ··åˆç²¾åº¦æ—¢ç¯€çœè¨˜æ†¶é«”åˆæå‡é€Ÿåº¦\")\n",
    "print(\"  - è¨ˆç®—é€šè¨Šé‡ç–Šå¯ç²å¾—é¡¯è‘—åŠ é€Ÿ\")\n",
    "print(\"  - æ€§èƒ½ç›£æ§æ˜¯æŒçºŒå„ªåŒ–çš„åŸºç¤\")\n",
    "\n",
    "# ä¿å­˜å„ªåŒ–ç¸½çµ\n",
    "optimization_summary = {\n",
    "    'techniques_covered': [\n",
    "        'Communication Analysis',\n",
    "        'Gradient Accumulation',\n",
    "        'Mixed Precision Training',\n",
    "        'Computation-Communication Overlap',\n",
    "        'Performance Monitoring',\n",
    "        'Automated Configuration'\n",
    "    ],\n",
    "    'tools_created': [\n",
    "        'DDPCommunicationAnalyzer',\n",
    "        'GradientAccumulationOptimizer', \n",
    "        'MixedPrecisionDDPTrainer',\n",
    "        'ComputationCommunicationOverlap',\n",
    "        'DDPPerformanceMonitor',\n",
    "        'DDPOptimizationConfigGenerator'\n",
    "    ],\n",
    "    'best_practices': optimization_best_practices,\n",
    "    'files_generated': [\n",
    "        'ddp_performance_curves.png',\n",
    "        'optimal_ddp_config.json'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('ddp_optimization_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(optimization_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nğŸ’¾ å„ªåŒ–ç¸½çµå·²ä¿å­˜åˆ° ddp_optimization_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}