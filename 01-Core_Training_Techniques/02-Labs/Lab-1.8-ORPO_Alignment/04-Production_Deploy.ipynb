{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# Lab-1.8-04: ORPO æ¨¡å‹ç”Ÿç”¢éƒ¨ç½²æŒ‡å—\n",
    "\n",
    "**éƒ¨ç½²ç›®æ¨™**: å°‡è¨“ç·´å®Œæˆçš„ ORPO å°é½Šæ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ\n",
    "- æ¨¡å‹é‡åŒ–èˆ‡å„ªåŒ–\n",
    "- æ¨ç†æœå‹™æ­å»º\n",
    "- æ€§èƒ½ç›£æ§èˆ‡ A/B æ¸¬è©¦\n",
    "- æŒçºŒå„ªåŒ–ç­–ç•¥\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™å’Œä¾è³´å®‰è£\n",
    "\n",
    "å®‰è£ç”Ÿç”¢éƒ¨ç½²æ‰€éœ€çš„é¡å¤–ä¾è³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥å’Œå®‰è£éƒ¨ç½²ç›¸é—œä¾è³´\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"âœ… {package} å·²å®‰è£\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ å®‰è£ {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# éƒ¨ç½²ç›¸é—œä¾è³´\n",
    "deployment_packages = [\n",
    "    'fastapi',\n",
    "    'uvicorn', \n",
    "    'gradio',\n",
    "    'prometheus_client',\n",
    "    'psutil'\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ æª¢æŸ¥éƒ¨ç½²ä¾è³´...\")\n",
    "for package in deployment_packages:\n",
    "    install_if_missing(package)\n",
    "\n",
    "print(\"\\nğŸ“‹ å°å…¥å¿…è¦æ¨¡çµ„...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "# éƒ¨ç½²ç›¸é—œ\n",
    "import fastapi\n",
    "import uvicorn\n",
    "import gradio as gr\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "\n",
    "# æª¢æŸ¥è¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ¯ éƒ¨ç½²è¨­å‚™: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-optimization-section",
   "metadata": {},
   "source": [
    "## 2. æ¨¡å‹è¼‰å…¥èˆ‡å„ªåŒ–\n",
    "\n",
    "è¼‰å…¥å·²è¨“ç·´çš„ ORPO æ¨¡å‹ä¸¦é€²è¡Œç”Ÿç”¢å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionORPOModel:\n",
    "    \"\"\"ç”Ÿç”¢ç’°å¢ƒçš„ ORPO æ¨¡å‹åŒ…è£å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"microsoft/DialoGPT-medium\", use_quantization=True):\n",
    "        self.model_name = model_name\n",
    "        self.use_quantization = use_quantization\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "        \n",
    "        # æ€§èƒ½çµ±è¨ˆ\n",
    "        self.request_count = 0\n",
    "        self.total_inference_time = 0\n",
    "        self.avg_tokens_per_second = 0\n",
    "        \n",
    "        print(f\"ğŸ­ åˆå§‹åŒ–ç”Ÿç”¢æ¨¡å‹: {model_name}\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"è¼‰å…¥å’Œå„ªåŒ–æ¨¡å‹\"\"\"\n",
    "        print(\"ğŸ“¥ è¼‰å…¥æ¨¡å‹å’Œ tokenizer...\")\n",
    "        \n",
    "        # è¼‰å…¥ tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # é‡åŒ–é…ç½®\n",
    "        if self.use_quantization:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "            print(\"âš¡ ä½¿ç”¨ 4-bit é‡åŒ–\")\n",
    "        else:\n",
    "            bnb_config = None\n",
    "            print(\"ğŸ”¥ ä½¿ç”¨å…¨ç²¾åº¦æ¨¡å‹\")\n",
    "        \n",
    "        # è¼‰å…¥æ¨¡å‹\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16 if self.use_quantization else torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # å‰µå»ºç”Ÿæˆç®¡é“\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\",\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "        print(f\"ğŸ“Š æ¨¡å‹åƒæ•¸é‡: {self.model.num_parameters() / 1e6:.1f}M\")\n",
    "        \n",
    "    def generate_response(self, prompt: str, max_tokens: int = 256) -> Dict:\n",
    "        \"\"\"ç”Ÿæˆå›æ‡‰\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # ç”Ÿæˆå›æ‡‰\n",
    "            outputs = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated_text = outputs[0]['generated_text']\n",
    "            response = generated_text[len(prompt):].strip()\n",
    "            \n",
    "            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™\n",
    "            inference_time = time.time() - start_time\n",
    "            token_count = len(self.tokenizer.encode(response))\n",
    "            tokens_per_second = token_count / inference_time if inference_time > 0 else 0\n",
    "            \n",
    "            # æ›´æ–°çµ±è¨ˆ\n",
    "            self.request_count += 1\n",
    "            self.total_inference_time += inference_time\n",
    "            self.avg_tokens_per_second = (\n",
    "                (self.avg_tokens_per_second * (self.request_count - 1) + tokens_per_second) \n",
    "                / self.request_count\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'inference_time': inference_time,\n",
    "                'token_count': token_count,\n",
    "                'tokens_per_second': tokens_per_second,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error: {str(e)}\",\n",
    "                'inference_time': time.time() - start_time,\n",
    "                'token_count': 0,\n",
    "                'tokens_per_second': 0,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"ç²å–æ¨¡å‹çµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            'request_count': self.request_count,\n",
    "            'total_inference_time': self.total_inference_time,\n",
    "            'avg_inference_time': self.total_inference_time / max(self.request_count, 1),\n",
    "            'avg_tokens_per_second': self.avg_tokens_per_second,\n",
    "            'model_name': self.model_name,\n",
    "            'quantized': self.use_quantization\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–ç”Ÿç”¢æ¨¡å‹\n",
    "production_model = ProductionORPOModel(use_quantization=True)\n",
    "production_model.load_model()\n",
    "\n",
    "print(\"ğŸ‰ ç”Ÿç”¢æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-test-section",
   "metadata": {},
   "source": [
    "## 3. æ€§èƒ½åŸºæº–æ¸¬è©¦\n",
    "\n",
    "æ¸¬è©¦æ¨¡å‹åœ¨ç”Ÿç”¢ç’°å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_performance_benchmark():\n",
    "    \"\"\"é‹è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ é–‹å§‹æ€§èƒ½åŸºæº–æ¸¬è©¦...\")\n",
    "    \n",
    "    # æ¸¬è©¦ç”¨ä¾‹\n",
    "    test_prompts = [\n",
    "        \"How can I improve my productivity at work?\",\n",
    "        \"What are the benefits of regular exercise?\",\n",
    "        \"Explain the concept of machine learning\",\n",
    "        \"How to cook a perfect pasta?\",\n",
    "        \"What is the meaning of life?\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\nğŸ“ æ¸¬è©¦ {i+1}/{len(test_prompts)}: {prompt[:50]}...\")\n",
    "        \n",
    "        # è¨˜éŒ„ç³»çµ±è³‡æº\n",
    "        gpu_memory_before = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "        cpu_percent_before = psutil.cpu_percent()\n",
    "        \n",
    "        # ç”Ÿæˆå›æ‡‰\n",
    "        result = production_model.generate_response(prompt, max_tokens=128)\n",
    "        \n",
    "        # è¨˜éŒ„è³‡æºä½¿ç”¨\n",
    "        gpu_memory_after = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "        cpu_percent_after = psutil.cpu_percent()\n",
    "        \n",
    "        # ä¿å­˜çµæœ\n",
    "        test_result = {\n",
    "            'prompt': prompt,\n",
    "            'response': result['response'][:100] + '...' if len(result['response']) > 100 else result['response'],\n",
    "            'inference_time': result['inference_time'],\n",
    "            'token_count': result['token_count'],\n",
    "            'tokens_per_second': result['tokens_per_second'],\n",
    "            'gpu_memory_usage': gpu_memory_after - gpu_memory_before,\n",
    "            'cpu_usage': (cpu_percent_before + cpu_percent_after) / 2,\n",
    "            'success': result['success']\n",
    "        }\n",
    "        \n",
    "        results.append(test_result)\n",
    "        \n",
    "        print(f\"â±ï¸  æ¨ç†æ™‚é–“: {result['inference_time']:.3f}s\")\n",
    "        print(f\"ğŸ”¤ Token æ•¸é‡: {result['token_count']}\")\n",
    "        print(f\"âš¡ Token/ç§’: {result['tokens_per_second']:.1f}\")\n",
    "        \n",
    "        # æ¸…ç†è¨˜æ†¶é«”\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# é‹è¡ŒåŸºæº–æ¸¬è©¦\n",
    "benchmark_results = run_performance_benchmark()\n",
    "\n",
    "# çµ±è¨ˆåˆ†æ\n",
    "df = pd.DataFrame(benchmark_results)\n",
    "print(\"\\nğŸ“Š æ€§èƒ½çµ±è¨ˆæ‘˜è¦:\")\n",
    "print(f\"å¹³å‡æ¨ç†æ™‚é–“: {df['inference_time'].mean():.3f}s\")\n",
    "print(f\"å¹³å‡ Token/ç§’: {df['tokens_per_second'].mean():.1f}\")\n",
    "print(f\"æˆåŠŸç‡: {df['success'].mean()*100:.1f}%\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"å¹³å‡ GPU è¨˜æ†¶é«”å¢é•·: {df['gpu_memory_usage'].mean():.3f} GB\")\n",
    "print(f\"å¹³å‡ CPU ä½¿ç”¨ç‡: {df['cpu_usage'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring-section",
   "metadata": {},
   "source": [
    "## 4. ç›£æ§ç³»çµ±è¨­ç½®\n",
    "\n",
    "è¨­ç½® Prometheus ç›£æ§æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitoring-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus ç›£æ§æŒ‡æ¨™\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "class ModelMonitoring:\n",
    "    \"\"\"æ¨¡å‹ç›£æ§ç³»çµ±\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # è«‹æ±‚è¨ˆæ•¸å™¨\n",
    "        self.request_count = Counter(\n",
    "            'orpo_model_requests_total',\n",
    "            'Total number of requests to ORPO model',\n",
    "            ['status']  # success, error\n",
    "        )\n",
    "        \n",
    "        # æ¨ç†æ™‚é–“åˆ†å¸ƒ\n",
    "        self.inference_duration = Histogram(\n",
    "            'orpo_model_inference_duration_seconds',\n",
    "            'Time spent on model inference',\n",
    "            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "        )\n",
    "        \n",
    "        # Token ç”Ÿæˆé€Ÿåº¦\n",
    "        self.tokens_per_second = Gauge(\n",
    "            'orpo_model_tokens_per_second',\n",
    "            'Average tokens generated per second'\n",
    "        )\n",
    "        \n",
    "        # GPU è¨˜æ†¶é«”ä½¿ç”¨\n",
    "        self.gpu_memory_usage = Gauge(\n",
    "            'orpo_model_gpu_memory_gb',\n",
    "            'GPU memory usage in GB'\n",
    "        )\n",
    "        \n",
    "        # CPU ä½¿ç”¨ç‡\n",
    "        self.cpu_usage = Gauge(\n",
    "            'orpo_model_cpu_percent',\n",
    "            'CPU usage percentage'\n",
    "        )\n",
    "        \n",
    "        print(\"ğŸ“Š ç›£æ§ç³»çµ±åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def record_request(self, result: Dict):\n",
    "        \"\"\"è¨˜éŒ„è«‹æ±‚æŒ‡æ¨™\"\"\"\n",
    "        # è¨˜éŒ„è«‹æ±‚ç‹€æ…‹\n",
    "        status = 'success' if result['success'] else 'error'\n",
    "        self.request_count.labels(status=status).inc()\n",
    "        \n",
    "        # è¨˜éŒ„æ¨ç†æ™‚é–“\n",
    "        self.inference_duration.observe(result['inference_time'])\n",
    "        \n",
    "        # æ›´æ–°é€Ÿåº¦æŒ‡æ¨™\n",
    "        self.tokens_per_second.set(result['tokens_per_second'])\n",
    "        \n",
    "        # æ›´æ–°è³‡æºä½¿ç”¨\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_usage.set(torch.cuda.memory_allocated() / 1e9)\n",
    "        self.cpu_usage.set(psutil.cpu_percent())\n",
    "    \n",
    "    def get_metrics(self) -> str:\n",
    "        \"\"\"ç²å– Prometheus æ ¼å¼çš„æŒ‡æ¨™\"\"\"\n",
    "        return generate_latest()\n",
    "\n",
    "# åˆå§‹åŒ–ç›£æ§\n",
    "monitoring = ModelMonitoring()\n",
    "\n",
    "print(\"ğŸ“ˆ ç›£æ§ç³»çµ±å°±ç·’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-service-section",
   "metadata": {},
   "source": [
    "## 5. FastAPI æœå‹™æ­å»º\n",
    "\n",
    "å‰µå»º RESTful API æœå‹™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import PlainTextResponse\n",
    "import uvicorn\n",
    "\n",
    "# API æ•¸æ“šæ¨¡å‹\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    response: str\n",
    "    inference_time: float\n",
    "    token_count: int\n",
    "    tokens_per_second: float\n",
    "    request_id: str\n",
    "\n",
    "# å‰µå»º FastAPI æ‡‰ç”¨\n",
    "app = FastAPI(\n",
    "    title=\"ORPO Model API\",\n",
    "    description=\"Production API for ORPO-aligned language model\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"ORPO Model API is running\", \"status\": \"healthy\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"å¥åº·æª¢æŸ¥ç«¯é»\"\"\"\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1e9 if gpu_available else 0\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"gpu_available\": gpu_available,\n",
    "        \"gpu_memory_gb\": gpu_memory,\n",
    "        \"model_loaded\": production_model.model is not None,\n",
    "        \"total_requests\": production_model.request_count\n",
    "    }\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)\n",
    "async def generate_text(request: GenerationRequest):\n",
    "    \"\"\"æ–‡æœ¬ç”Ÿæˆç«¯é»\"\"\"\n",
    "    try:\n",
    "        # ç”Ÿæˆå”¯ä¸€è«‹æ±‚ ID\n",
    "        request_id = f\"req_{int(time.time()*1000)}_{production_model.request_count}\"\n",
    "        \n",
    "        # ç”Ÿæˆå›æ‡‰\n",
    "        result = production_model.generate_response(\n",
    "            request.prompt, \n",
    "            max_tokens=request.max_tokens\n",
    "        )\n",
    "        \n",
    "        # è¨˜éŒ„ç›£æ§æŒ‡æ¨™\n",
    "        monitoring.record_request(result)\n",
    "        \n",
    "        if not result['success']:\n",
    "            raise HTTPException(status_code=500, detail=result.get('error', 'Generation failed'))\n",
    "        \n",
    "        return GenerationResponse(\n",
    "            response=result['response'],\n",
    "            inference_time=result['inference_time'],\n",
    "            token_count=result['token_count'],\n",
    "            tokens_per_second=result['tokens_per_second'],\n",
    "            request_id=request_id\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "async def get_stats():\n",
    "    \"\"\"ç²å–æ¨¡å‹çµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "    return production_model.get_stats()\n",
    "\n",
    "@app.get(\"/metrics\", response_class=PlainTextResponse)\n",
    "async def get_metrics():\n",
    "    \"\"\"Prometheus æŒ‡æ¨™ç«¯é»\"\"\"\n",
    "    return monitoring.get_metrics()\n",
    "\n",
    "print(\"ğŸŒ FastAPI æœå‹™é…ç½®å®Œæˆ\")\n",
    "print(\"å¯ç”¨ç«¯é»:\")\n",
    "print(\"  GET  /          - æœå‹™ç‹€æ…‹\")\n",
    "print(\"  GET  /health    - å¥åº·æª¢æŸ¥\")\n",
    "print(\"  POST /generate  - æ–‡æœ¬ç”Ÿæˆ\")\n",
    "print(\"  GET  /stats     - æ¨¡å‹çµ±è¨ˆ\")\n",
    "print(\"  GET  /metrics   - Prometheus æŒ‡æ¨™\")\n",
    "\n",
    "# æ³¨æ„ï¼šåœ¨ Jupyter ä¸­ä¸èƒ½ç›´æ¥é‹è¡Œ uvicorn.run()\n",
    "# å¯¦éš›éƒ¨ç½²æ™‚ä½¿ç”¨: uvicorn main:app --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-interface-section",
   "metadata": {},
   "source": [
    "## 6. Gradio ç”¨æˆ¶ç•Œé¢\n",
    "\n",
    "å‰µå»ºå‹å¥½çš„ Web ç•Œé¢ä¾›ç”¨æˆ¶æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gradio_generate(prompt, max_tokens, temperature):\n",
    "    \"\"\"Gradio ä»‹é¢çš„ç”Ÿæˆå‡½æ•¸\"\"\"\n",
    "    try:\n",
    "        # èª¿æ•´æ¨¡å‹åƒæ•¸ï¼ˆé€™è£¡ç°¡åŒ–è™•ç†ï¼‰\n",
    "        result = production_model.generate_response(prompt, max_tokens=int(max_tokens))\n",
    "        \n",
    "        # è¨˜éŒ„ç›£æ§æŒ‡æ¨™\n",
    "        monitoring.record_request(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            stats_text = f\"\"\"ğŸ“Š ç”Ÿæˆçµ±è¨ˆ:\n",
    "â±ï¸ æ¨ç†æ™‚é–“: {result['inference_time']:.3f}s\n",
    "ğŸ”¤ Token æ•¸é‡: {result['token_count']}\n",
    "âš¡ Token/ç§’: {result['tokens_per_second']:.1f}\n",
    "ğŸ“ˆ ç¸½è«‹æ±‚æ•¸: {production_model.request_count}\"\"\"\n",
    "            \n",
    "            return result['response'], stats_text\n",
    "        else:\n",
    "            return f\"âŒ ç”Ÿæˆå¤±æ•—: {result.get('error', 'Unknown error')}\", \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"âŒ éŒ¯èª¤: {str(e)}\", \"\"\n",
    "\n",
    "# å‰µå»º Gradio ç•Œé¢\n",
    "def create_gradio_interface():\n",
    "    \"\"\"å‰µå»º Gradio ç¶²é ç•Œé¢\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"ORPO Model Demo\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # ğŸ¤– ORPO å°é½Šæ¨¡å‹æ¼”ç¤º\n",
    "            \n",
    "            é€™æ˜¯ä¸€å€‹åŸºæ–¼ ORPO (Odds Ratio Preference Optimization) è¨“ç·´çš„å°é½Šèªè¨€æ¨¡å‹ã€‚\n",
    "            ORPO æ˜¯ä¸€ç¨®æ–°çš„å–®éšæ®µå°é½Šæ–¹æ³•ï¼Œç›¸æ¯”å‚³çµ±çš„ SFT+DPO æ–¹æ³•æ›´åŠ é«˜æ•ˆã€‚\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                prompt_input = gr.Textbox(\n",
    "                    label=\"è¼¸å…¥æç¤º (Prompt)\",\n",
    "                    placeholder=\"è«‹è¼¸å…¥æ‚¨çš„å•é¡Œæˆ–æç¤º...\",\n",
    "                    lines=3\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    max_tokens_slider = gr.Slider(\n",
    "                        label=\"æœ€å¤§ Token æ•¸\",\n",
    "                        minimum=50,\n",
    "                        maximum=512,\n",
    "                        value=256,\n",
    "                        step=10\n",
    "                    )\n",
    "                    \n",
    "                    temperature_slider = gr.Slider(\n",
    "                        label=\"æº«åº¦ (Temperature)\",\n",
    "                        minimum=0.1,\n",
    "                        maximum=2.0,\n",
    "                        value=0.7,\n",
    "                        step=0.1\n",
    "                    )\n",
    "                \n",
    "                generate_btn = gr.Button(\"ğŸš€ ç”Ÿæˆå›æ‡‰\", variant=\"primary\")\n",
    "                \n",
    "            with gr.Column(scale=1):\n",
    "                stats_display = gr.Textbox(\n",
    "                    label=\"æ€§èƒ½çµ±è¨ˆ\",\n",
    "                    value=\"ç­‰å¾…ç”Ÿæˆ...\",\n",
    "                    lines=6,\n",
    "                    interactive=False\n",
    "                )\n",
    "        \n",
    "        response_output = gr.Textbox(\n",
    "            label=\"æ¨¡å‹å›æ‡‰\",\n",
    "            lines=8,\n",
    "            interactive=False\n",
    "        )\n",
    "        \n",
    "        # é è¨­ç¤ºä¾‹\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡ï¼Ÿ\", 200, 0.7],\n",
    "                [\"è§£é‡‹ä¸€ä¸‹æ©Ÿå™¨å­¸ç¿’çš„åŸºæœ¬æ¦‚å¿µ\", 300, 0.8],\n",
    "                [\"çµ¦æˆ‘ä¸€äº›å¥åº·é£²é£Ÿçš„å»ºè­°\", 250, 0.6],\n",
    "                [\"å¦‚ä½•å­¸ç¿’æ–°çš„ç·¨ç¨‹èªè¨€ï¼Ÿ\", 280, 0.7]\n",
    "            ],\n",
    "            inputs=[prompt_input, max_tokens_slider, temperature_slider]\n",
    "        )\n",
    "        \n",
    "        # äº‹ä»¶ç¶å®š\n",
    "        generate_btn.click(\n",
    "            fn=gradio_generate,\n",
    "            inputs=[prompt_input, max_tokens_slider, temperature_slider],\n",
    "            outputs=[response_output, stats_display]\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### ğŸ“‹ ä½¿ç”¨èªªæ˜\n",
    "            \n",
    "            1. **è¼¸å…¥æç¤º**: åœ¨ä¸Šæ–¹æ–‡æœ¬æ¡†ä¸­è¼¸å…¥æ‚¨çš„å•é¡Œæˆ–æç¤º\n",
    "            2. **èª¿æ•´åƒæ•¸**: å¯ä»¥èª¿æ•´æœ€å¤§ Token æ•¸å’Œæº«åº¦åƒæ•¸\n",
    "            3. **ç”Ÿæˆå›æ‡‰**: é»æ“Šã€Œç”Ÿæˆå›æ‡‰ã€æŒ‰éˆ•ç²å¾—æ¨¡å‹å›æ‡‰\n",
    "            4. **æŸ¥çœ‹çµ±è¨ˆ**: å³å´æœƒé¡¯ç¤ºç”Ÿæˆçš„æ€§èƒ½çµ±è¨ˆä¿¡æ¯\n",
    "            \n",
    "            **åƒæ•¸èªªæ˜**:\n",
    "            - **æœ€å¤§ Token æ•¸**: æ§åˆ¶å›æ‡‰çš„æœ€å¤§é•·åº¦\n",
    "            - **æº«åº¦**: æ§åˆ¶å›æ‡‰çš„å‰µé€ æ€§ï¼ˆè¶Šé«˜è¶Šæœ‰å‰µæ„ï¼Œè¶Šä½è¶Šä¿å®ˆï¼‰\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# å‰µå»ºç•Œé¢\n",
    "gradio_demo = create_gradio_interface()\n",
    "print(\"ğŸ¨ Gradio ç•Œé¢å·²é…ç½®\")\n",
    "print(\"åŸ·è¡Œ gradio_demo.launch() ä¾†å•Ÿå‹•ç•Œé¢\")\n",
    "\n",
    "# æ³¨æ„ï¼šåœ¨ Jupyter ä¸­å¯ä»¥åŸ·è¡Œä»¥ä¸‹ä»£ç¢¼ä¾†å•Ÿå‹•ç•Œé¢\n",
    "# gradio_demo.launch(share=False, server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab-testing-section",
   "metadata": {},
   "source": [
    "## 7. A/B æ¸¬è©¦æ¡†æ¶\n",
    "\n",
    "å¯¦ç¾æ¨¡å‹ç‰ˆæœ¬å°æ¯”æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dataclass\n",
    "class ABTestConfig:\n",
    "    \"\"\"A/B æ¸¬è©¦é…ç½®\"\"\"\n",
    "    test_name: str\n",
    "    model_a_name: str\n",
    "    model_b_name: str\n",
    "    traffic_split: float = 0.5  # æµé‡åˆ†é…æ¯”ä¾‹ (0.5 = 50/50)\n",
    "    start_time: datetime = None\n",
    "    end_time: datetime = None\n",
    "    \n",
    "@dataclass\n",
    "class ABTestResult:\n",
    "    \"\"\"A/B æ¸¬è©¦çµæœ\"\"\"\n",
    "    user_id: str\n",
    "    model_version: str\n",
    "    prompt: str\n",
    "    response: str\n",
    "    inference_time: float\n",
    "    user_rating: Optional[int] = None  # 1-5 è©•åˆ†\n",
    "    timestamp: datetime = None\n",
    "\n",
    "class ABTestManager:\n",
    "    \"\"\"A/B æ¸¬è©¦ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.active_tests: Dict[str, ABTestConfig] = {}\n",
    "        self.test_results: List[ABTestResult] = []\n",
    "        \n",
    "    def create_test(self, config: ABTestConfig):\n",
    "        \"\"\"å‰µå»ºæ–°çš„ A/B æ¸¬è©¦\"\"\"\n",
    "        if config.start_time is None:\n",
    "            config.start_time = datetime.now()\n",
    "        if config.end_time is None:\n",
    "            config.end_time = config.start_time + timedelta(days=7)\n",
    "            \n",
    "        self.active_tests[config.test_name] = config\n",
    "        print(f\"ğŸ§ª A/B æ¸¬è©¦å·²å‰µå»º: {config.test_name}\")\n",
    "        print(f\"   æ¨¡å‹ A: {config.model_a_name}\")\n",
    "        print(f\"   æ¨¡å‹ B: {config.model_b_name}\")\n",
    "        print(f\"   æµé‡åˆ†é…: {config.traffic_split*100:.0f}% / {(1-config.traffic_split)*100:.0f}%\")\n",
    "        \n",
    "    def assign_model(self, test_name: str, user_id: str) -> str:\n",
    "        \"\"\"ç‚ºç”¨æˆ¶åˆ†é…æ¨¡å‹ç‰ˆæœ¬\"\"\"\n",
    "        if test_name not in self.active_tests:\n",
    "            return \"default\"\n",
    "            \n",
    "        config = self.active_tests[test_name]\n",
    "        \n",
    "        # æª¢æŸ¥æ¸¬è©¦æ˜¯å¦åœ¨æœ‰æ•ˆæœŸå…§\n",
    "        now = datetime.now()\n",
    "        if now < config.start_time or now > config.end_time:\n",
    "            return config.model_a_name  # é»˜èªè¿”å› A ç‰ˆæœ¬\n",
    "        \n",
    "        # åŸºæ–¼ç”¨æˆ¶ ID çš„ä¸€è‡´æ€§åˆ†é…ï¼ˆç¢ºä¿åŒä¸€ç”¨æˆ¶ç¸½æ˜¯ç²å¾—ç›¸åŒç‰ˆæœ¬ï¼‰\n",
    "        random.seed(hash(user_id + test_name))\n",
    "        if random.random() < config.traffic_split:\n",
    "            return config.model_a_name\n",
    "        else:\n",
    "            return config.model_b_name\n",
    "    \n",
    "    def record_result(self, result: ABTestResult):\n",
    "        \"\"\"è¨˜éŒ„æ¸¬è©¦çµæœ\"\"\"\n",
    "        if result.timestamp is None:\n",
    "            result.timestamp = datetime.now()\n",
    "        self.test_results.append(result)\n",
    "        \n",
    "    def analyze_test(self, test_name: str) -> Dict:\n",
    "        \"\"\"åˆ†æ A/B æ¸¬è©¦çµæœ\"\"\"\n",
    "        if test_name not in self.active_tests:\n",
    "            return {\"error\": \"Test not found\"}\n",
    "        \n",
    "        config = self.active_tests[test_name]\n",
    "        \n",
    "        # ç¯©é¸ç›¸é—œçµæœ\n",
    "        relevant_results = [\n",
    "            r for r in self.test_results \n",
    "            if r.model_version in [config.model_a_name, config.model_b_name]\n",
    "        ]\n",
    "        \n",
    "        if not relevant_results:\n",
    "            return {\"error\": \"No results found\"}\n",
    "        \n",
    "        # åˆ†çµ„çµ±è¨ˆ\n",
    "        model_a_results = [r for r in relevant_results if r.model_version == config.model_a_name]\n",
    "        model_b_results = [r for r in relevant_results if r.model_version == config.model_b_name]\n",
    "        \n",
    "        def calculate_stats(results):\n",
    "            if not results:\n",
    "                return {\"count\": 0, \"avg_inference_time\": 0, \"avg_rating\": 0}\n",
    "            \n",
    "            ratings = [r.user_rating for r in results if r.user_rating is not None]\n",
    "            \n",
    "            return {\n",
    "                \"count\": len(results),\n",
    "                \"avg_inference_time\": sum(r.inference_time for r in results) / len(results),\n",
    "                \"avg_rating\": sum(ratings) / len(ratings) if ratings else 0,\n",
    "                \"rating_count\": len(ratings)\n",
    "            }\n",
    "        \n",
    "        model_a_stats = calculate_stats(model_a_results)\n",
    "        model_b_stats = calculate_stats(model_b_results)\n",
    "        \n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"model_a\": {\n",
    "                \"name\": config.model_a_name,\n",
    "                \"stats\": model_a_stats\n",
    "            },\n",
    "            \"model_b\": {\n",
    "                \"name\": config.model_b_name,\n",
    "                \"stats\": model_b_stats\n",
    "            },\n",
    "            \"total_results\": len(relevant_results)\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ– A/B æ¸¬è©¦ç®¡ç†å™¨\n",
    "ab_test_manager = ABTestManager()\n",
    "\n",
    "# å‰µå»ºç¤ºä¾‹ A/B æ¸¬è©¦\n",
    "test_config = ABTestConfig(\n",
    "    test_name=\"orpo_vs_baseline\",\n",
    "    model_a_name=\"ORPO_Model\",\n",
    "    model_b_name=\"Baseline_Model\",\n",
    "    traffic_split=0.5\n",
    ")\n",
    "\n",
    "ab_test_manager.create_test(test_config)\n",
    "\n",
    "# æ¨¡æ“¬ä¸€äº›æ¸¬è©¦çµæœ\n",
    "def simulate_ab_test_results():\n",
    "    \"\"\"æ¨¡æ“¬ A/B æ¸¬è©¦çµæœ\"\"\"\n",
    "    print(\"\\nğŸ”¬ æ¨¡æ“¬ A/B æ¸¬è©¦çµæœ...\")\n",
    "    \n",
    "    users = [f\"user_{i}\" for i in range(100)]\n",
    "    prompts = [\n",
    "        \"How to be more productive?\",\n",
    "        \"Explain quantum computing\",\n",
    "        \"Best practices for coding\",\n",
    "        \"How to stay healthy?\"\n",
    "    ]\n",
    "    \n",
    "    for user in users:\n",
    "        prompt = random.choice(prompts)\n",
    "        model_version = ab_test_manager.assign_model(\"orpo_vs_baseline\", user)\n",
    "        \n",
    "        # æ¨¡æ“¬ä¸åŒæ¨¡å‹çš„æ€§èƒ½å·®ç•°\n",
    "        if model_version == \"ORPO_Model\":\n",
    "            inference_time = random.uniform(0.5, 1.2)\n",
    "            rating = random.choices([3, 4, 5], weights=[0.1, 0.4, 0.5])[0]\n",
    "        else:\n",
    "            inference_time = random.uniform(0.8, 1.8)\n",
    "            rating = random.choices([2, 3, 4], weights=[0.2, 0.5, 0.3])[0]\n",
    "        \n",
    "        result = ABTestResult(\n",
    "            user_id=user,\n",
    "            model_version=model_version,\n",
    "            prompt=prompt,\n",
    "            response=\"Generated response...\",\n",
    "            inference_time=inference_time,\n",
    "            user_rating=rating if random.random() > 0.3 else None  # 70% ç”¨æˆ¶æä¾›è©•åˆ†\n",
    "        )\n",
    "        \n",
    "        ab_test_manager.record_result(result)\n",
    "    \n",
    "    return ab_test_manager.analyze_test(\"orpo_vs_baseline\")\n",
    "\n",
    "# é‹è¡Œæ¨¡æ“¬æ¸¬è©¦\n",
    "test_analysis = simulate_ab_test_results()\n",
    "print(\"\\nğŸ“Š A/B æ¸¬è©¦åˆ†æçµæœ:\")\n",
    "print(json.dumps(test_analysis, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment-checklist-section",
   "metadata": {},
   "source": [
    "## 8. ç”Ÿç”¢éƒ¨ç½²æª¢æŸ¥æ¸…å–®\n",
    "\n",
    "ç¢ºä¿æ¨¡å‹æº–å‚™å¥½æŠ•å…¥ç”Ÿç”¢ç’°å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_readiness_check():\n",
    "    \"\"\"ç”Ÿç”¢å°±ç·’æª¢æŸ¥\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” ç”Ÿç”¢éƒ¨ç½²å°±ç·’æª¢æŸ¥\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    checklist = {\n",
    "        \"æ¨¡å‹è¼‰å…¥\": production_model.model is not None,\n",
    "        \"GPU å¯ç”¨æ€§\": torch.cuda.is_available(),\n",
    "        \"è¨˜æ†¶é«”å……è¶³\": torch.cuda.get_device_properties(0).total_memory > 8e9 if torch.cuda.is_available() else True,\n",
    "        \"ç›£æ§ç³»çµ±\": monitoring is not None,\n",
    "        \"API æœå‹™\": app is not None,\n",
    "        \"A/B æ¸¬è©¦\": ab_test_manager is not None,\n",
    "        \"åŸºæº–æ¸¬è©¦å®Œæˆ\": len(benchmark_results) > 0,\n",
    "    }\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    for check, passed in checklist.items():\n",
    "        status = \"âœ…\" if passed else \"âŒ\"\n",
    "        print(f\"{status} {check}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"ğŸ‰ æ‰€æœ‰æª¢æŸ¥é€šéï¼æ¨¡å‹å·²æº–å‚™å¥½æŠ•å…¥ç”Ÿç”¢ç’°å¢ƒã€‚\")\n",
    "    else:\n",
    "        print(\"âš ï¸  éƒ¨åˆ†æª¢æŸ¥æœªé€šéï¼Œè«‹å…ˆè§£æ±ºç›¸é—œå•é¡Œã€‚\")\n",
    "    \n",
    "    # éƒ¨ç½²å»ºè­°\n",
    "    print(\"\\nğŸ“‹ ç”Ÿç”¢éƒ¨ç½²å»ºè­°:\")\n",
    "    suggestions = [\n",
    "        \"ğŸ”§ ä½¿ç”¨ Docker å®¹å™¨åŒ–éƒ¨ç½²\",\n",
    "        \"ğŸ”’ å•Ÿç”¨ HTTPS å’Œèº«ä»½é©—è­‰\",\n",
    "        \"ğŸ“Š è¨­ç½® Prometheus + Grafana ç›£æ§\",\n",
    "        \"ğŸš¦ é…ç½®è² è¼‰å‡è¡¡å™¨\",\n",
    "        \"ğŸ’¾ è¨­ç½®å®šæœŸæ¨¡å‹å‚™ä»½\",\n",
    "        \"ğŸ”„ å¯¦æ–½æ»¾å‹•æ›´æ–°ç­–ç•¥\",\n",
    "        \"âš¡ è€ƒæ…®ä½¿ç”¨ NVIDIA Triton æ¨ç†æœå‹™å™¨\",\n",
    "        \"ğŸ“ å»ºç«‹æ—¥èªŒèšåˆå’Œåˆ†æ\",\n",
    "        \"ğŸ§ª æŒçºŒé‹è¡Œ A/B æ¸¬è©¦\",\n",
    "        \"ğŸ” è¨­ç½®ç•°å¸¸æª¢æ¸¬å’Œè‡ªå‹•å‘Šè­¦\"\n",
    "    ]\n",
    "    \n",
    "    for suggestion in suggestions:\n",
    "        print(f\"  {suggestion}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# åŸ·è¡Œå°±ç·’æª¢æŸ¥\n",
    "ready_for_production = production_readiness_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "docker-section",
   "metadata": {},
   "source": [
    "## 9. Docker éƒ¨ç½²é…ç½®\n",
    "\n",
    "ç”Ÿæˆ Docker é…ç½®æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "docker-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docker_files():\n",
    "    \"\"\"ç”Ÿæˆ Docker éƒ¨ç½²æ–‡ä»¶\"\"\"\n",
    "    \n",
    "    # Dockerfile\n",
    "    dockerfile_content = \"\"\"\n",
    "# ORPO Model Production Dockerfile\n",
    "FROM nvidia/cuda:12.1-runtime-ubuntu22.04\n",
    "\n",
    "# è¨­ç½®å·¥ä½œç›®éŒ„\n",
    "WORKDIR /app\n",
    "\n",
    "# å®‰è£ç³»çµ±ä¾è³´\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3 \\\n",
    "    python3-pip \\\n",
    "    git \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# å®‰è£ Python ä¾è³´\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# è¤‡è£½æ‡‰ç”¨ä»£ç¢¼\n",
    "COPY . .\n",
    "\n",
    "# è¨­ç½®ç’°å¢ƒè®Šé‡\n",
    "ENV PYTHONPATH=/app\n",
    "ENV CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# æš´éœ²ç«¯å£\n",
    "EXPOSE 8000 7860\n",
    "\n",
    "# å¥åº·æª¢æŸ¥\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# å•Ÿå‹•å‘½ä»¤\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"1\"]\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # requirements.txt\n",
    "    requirements_content = \"\"\"\n",
    "torch>=2.1.0\n",
    "transformers>=4.35.0\n",
    "peft>=0.6.0\n",
    "bitsandbytes>=0.41.0\n",
    "accelerate>=0.24.0\n",
    "fastapi>=0.104.0\n",
    "uvicorn[standard]>=0.24.0\n",
    "gradio>=4.0.0\n",
    "prometheus-client>=0.19.0\n",
    "psutil>=5.9.0\n",
    "pydantic>=2.0.0\n",
    "numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # docker-compose.yml\n",
    "    docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  orpo-model:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"  # FastAPI\n",
    "      - \"7860:7860\"  # Gradio (å¦‚æœéœ€è¦)\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "      - MODEL_NAME=microsoft/DialoGPT-medium\n",
    "      - USE_QUANTIZATION=true\n",
    "    volumes:\n",
    "      - ./models:/app/models\n",
    "      - ./logs:/app/logs\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 60s\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "    restart: unless-stopped\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana-storage:/var/lib/grafana\n",
    "    restart: unless-stopped\n",
    "\n",
    "volumes:\n",
    "  grafana-storage:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # prometheus.yml\n",
    "    prometheus_config = \"\"\"\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'orpo-model'\n",
    "    static_configs:\n",
    "      - targets: ['orpo-model:8000']\n",
    "    metrics_path: '/metrics'\n",
    "    scrape_interval: 5s\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # main.py (ç°¡åŒ–ç‰ˆçš„ FastAPI æ‡‰ç”¨)\n",
    "    main_py_content = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from prometheus_client import generate_latest, Counter, Histogram\n",
    "from fastapi.responses import PlainTextResponse\n",
    "\n",
    "# åˆå§‹åŒ– FastAPI\n",
    "app = FastAPI(title=\"ORPO Model API\", version=\"1.0.0\")\n",
    "\n",
    "# Prometheus æŒ‡æ¨™\n",
    "REQUEST_COUNT = Counter('requests_total', 'Total requests', ['method', 'endpoint'])\n",
    "REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')\n",
    "\n",
    "# å…¨å±€è®Šé‡\n",
    "model = None\n",
    "tokenizer = None\n",
    "text_generator = None\n",
    "\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 256\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    global model, tokenizer, text_generator\n",
    "    \n",
    "    model_name = os.getenv(\"MODEL_NAME\", \"microsoft/DialoGPT-medium\")\n",
    "    use_quantization = os.getenv(\"USE_QUANTIZATION\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    text_generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    REQUEST_COUNT.labels(method=\"GET\", endpoint=\"/\").inc()\n",
    "    return {\"message\": \"ORPO Model API\", \"status\": \"running\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    REQUEST_COUNT.labels(method=\"GET\", endpoint=\"/health\").inc()\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"gpu_available\": torch.cuda.is_available()\n",
    "    }\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: GenerationRequest):\n",
    "    REQUEST_COUNT.labels(method=\"POST\", endpoint=\"/generate\").inc()\n",
    "    \n",
    "    with REQUEST_DURATION.time():\n",
    "        if text_generator is None:\n",
    "            raise HTTPException(status_code=500, detail=\"Model not loaded\")\n",
    "        \n",
    "        try:\n",
    "            outputs = text_generator(\n",
    "                request.prompt,\n",
    "                max_new_tokens=request.max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            response = outputs[0]['generated_text'][len(request.prompt):].strip()\n",
    "            \n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"prompt\": request.prompt\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/metrics\", response_class=PlainTextResponse)\n",
    "async def get_metrics():\n",
    "    return generate_latest()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # ä¿å­˜æ–‡ä»¶\n",
    "    files = {\n",
    "        \"Dockerfile\": dockerfile_content,\n",
    "        \"requirements.txt\": requirements_content,\n",
    "        \"docker-compose.yml\": docker_compose_content,\n",
    "        \"prometheus.yml\": prometheus_config,\n",
    "        \"main.py\": main_py_content\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ³ Docker éƒ¨ç½²æ–‡ä»¶å…§å®¹:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for filename, content in files.items():\n",
    "        print(f\"\\nğŸ“„ {filename}:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "    \n",
    "    print(\"\\nğŸš€ éƒ¨ç½²å‘½ä»¤:\")\n",
    "    commands = [\n",
    "        \"# æ§‹å»ºå’Œå•Ÿå‹•æœå‹™\",\n",
    "        \"docker-compose up --build -d\",\n",
    "        \"\",\n",
    "        \"# æŸ¥çœ‹æ—¥èªŒ\", \n",
    "        \"docker-compose logs -f orpo-model\",\n",
    "        \"\",\n",
    "        \"# åœæ­¢æœå‹™\",\n",
    "        \"docker-compose down\",\n",
    "        \"\",\n",
    "        \"# é‡å•Ÿç‰¹å®šæœå‹™\",\n",
    "        \"docker-compose restart orpo-model\"\n",
    "    ]\n",
    "    \n",
    "    for cmd in commands:\n",
    "        print(cmd)\n",
    "    \n",
    "    return files\n",
    "\n",
    "# ç”Ÿæˆ Docker é…ç½®\n",
    "docker_files = generate_docker_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## 10. éƒ¨ç½²ç¸½çµèˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### ğŸ¯ éƒ¨ç½²å®Œæˆæª¢æŸ¥æ¸…å–®\n",
    "\n",
    "âœ… **æ¨¡å‹å„ªåŒ–**: ä½¿ç”¨é‡åŒ–æŠ€è¡“æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨  \n",
    "âœ… **API æœå‹™**: FastAPI æä¾›é«˜æ€§èƒ½ RESTful æ¥å£  \n",
    "âœ… **ç”¨æˆ¶ç•Œé¢**: Gradio æä¾›å‹å¥½çš„ Web æ¸¬è©¦ç•Œé¢  \n",
    "âœ… **ç›£æ§ç³»çµ±**: Prometheus æŒ‡æ¨™æ”¶é›†å’Œåˆ†æ  \n",
    "âœ… **A/B æ¸¬è©¦**: å®Œæ•´çš„æ¨¡å‹ç‰ˆæœ¬å°æ¯”æ¡†æ¶  \n",
    "âœ… **å®¹å™¨åŒ–**: Docker é…ç½®ç¢ºä¿ä¸€è‡´çš„éƒ¨ç½²ç’°å¢ƒ  \n",
    "âœ… **å¥åº·æª¢æŸ¥**: è‡ªå‹•åŒ–çš„æœå‹™ç‹€æ…‹ç›£æ§  \n",
    "âœ… **æ€§èƒ½åŸºæº–**: è©³ç´°çš„æ¨ç†æ€§èƒ½è©•ä¼°  \n",
    "\n",
    "### ğŸ—ï¸ ç”Ÿç”¢ç’°å¢ƒæ¶æ§‹å»ºè­°\n",
    "\n",
    "```\n",
    "ç”¨æˆ¶è«‹æ±‚ â†’ è² è¼‰å‡è¡¡å™¨ â†’ API Gateway â†’ ORPO æ¨¡å‹æœå‹™\n",
    "                                      â†“\n",
    "                               ç›£æ§ç³»çµ± (Prometheus/Grafana)\n",
    "                                      â†“\n",
    "                               A/B æ¸¬è©¦ç³»çµ± â†’ æ•¸æ“šåˆ†æ\n",
    "```\n",
    "\n",
    "### ğŸ“Š é—œéµæ€§èƒ½æŒ‡æ¨™ (KPIs)\n",
    "\n",
    "1. **å»¶é²æŒ‡æ¨™**:\n",
    "   - P50 æ¨ç†æ™‚é–“ < 1.0s\n",
    "   - P95 æ¨ç†æ™‚é–“ < 2.0s\n",
    "   - P99 æ¨ç†æ™‚é–“ < 5.0s\n",
    "\n",
    "2. **ååé‡æŒ‡æ¨™**:\n",
    "   - QPS (æ¯ç§’æŸ¥è©¢æ•¸) > 10\n",
    "   - Token/ç§’ > 50\n",
    "\n",
    "3. **å¯é æ€§æŒ‡æ¨™**:\n",
    "   - å¯ç”¨æ€§ > 99.9%\n",
    "   - éŒ¯èª¤ç‡ < 0.1%\n",
    "\n",
    "4. **è³‡æºåˆ©ç”¨ç‡**:\n",
    "   - GPU åˆ©ç”¨ç‡ 60-80%\n",
    "   - è¨˜æ†¶é«”ä½¿ç”¨ç‡ < 90%\n",
    "\n",
    "### ğŸ”§ é‹ç¶­æœ€ä½³å¯¦è¸\n",
    "\n",
    "1. **ç›£æ§å‘Šè­¦**:\n",
    "   - è¨­ç½®æ¨ç†æ™‚é–“ç•°å¸¸å‘Šè­¦\n",
    "   - GPU è¨˜æ†¶é«”ä½¿ç”¨å‘Šè­¦\n",
    "   - æœå‹™å¥åº·ç‹€æ…‹ç›£æ§\n",
    "\n",
    "2. **è‡ªå‹•åŒ–éƒ¨ç½²**:\n",
    "   - CI/CD ç®¡é“è‡ªå‹•æ¸¬è©¦å’Œéƒ¨ç½²\n",
    "   - è—ç¶ éƒ¨ç½²æˆ–æ»¾å‹•æ›´æ–°\n",
    "   - è‡ªå‹•å›æ»¾æ©Ÿåˆ¶\n",
    "\n",
    "3. **æ•¸æ“šç®¡ç†**:\n",
    "   - å®šæœŸå‚™ä»½æ¨¡å‹æª¢æŸ¥é»\n",
    "   - æ—¥èªŒè¼ªè½‰å’Œæ­¸æª”\n",
    "   - ç”¨æˆ¶åé¥‹æ•¸æ“šæ”¶é›†\n",
    "\n",
    "4. **å®‰å…¨é˜²è­·**:\n",
    "   - API èº«ä»½é©—è­‰å’Œæˆæ¬Š\n",
    "   - è¼¸å…¥å…§å®¹éæ¿¾å’Œé©—è­‰\n",
    "   - é€Ÿç‡é™åˆ¶é˜²æ­¢æ¿«ç”¨\n",
    "\n",
    "### ğŸš€ æ“´å±•ç­–ç•¥\n",
    "\n",
    "1. **æ°´å¹³æ“´å±•**: å¢åŠ æ›´å¤šæ¨¡å‹å¯¦ä¾‹è™•ç†ä¸¦ç™¼è«‹æ±‚\n",
    "2. **æ¨¡å‹åˆ†ç‰‡**: å¤§å‹æ¨¡å‹åˆ†ä½ˆå¼éƒ¨ç½²\n",
    "3. **ç·©å­˜ç­–ç•¥**: å¸¸è¦‹æŸ¥è©¢çµæœç·©å­˜\n",
    "4. **é‚Šç·£éƒ¨ç½²**: CDN é‚Šç·£ç¯€é»éƒ¨ç½²é™ä½å»¶é²\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ ORPO æ¨¡å‹ç”Ÿç”¢éƒ¨ç½²æŒ‡å—å®Œæˆï¼**\n",
    "\n",
    "ç¾åœ¨æ‚¨æ“æœ‰äº†ä¸€å€‹å®Œæ•´çš„ ORPO å°é½Šæ¨¡å‹ç”Ÿç”¢éƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š\n",
    "- é«˜æ€§èƒ½æ¨ç†æœå‹™\n",
    "- å…¨é¢çš„ç›£æ§ç³»çµ±  \n",
    "- ç§‘å­¸çš„ A/B æ¸¬è©¦æ¡†æ¶\n",
    "- å®¹å™¨åŒ–éƒ¨ç½²è§£æ±ºæ–¹æ¡ˆ\n",
    "- ç”Ÿç”¢ç´šåˆ¥çš„æœ€ä½³å¯¦è¸å»ºè­°\n",
    "\n",
    "é€™å¥—å®Œæ•´çš„éƒ¨ç½²æµç¨‹å¯ä»¥ç¢ºä¿ ORPO æ¨¡å‹åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ç©©å®šã€é«˜æ•ˆåœ°é‹è¡Œï¼Œç‚ºç”¨æˆ¶æä¾›å„ªè³ªçš„ AI æœå‹™é«”é©—ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}