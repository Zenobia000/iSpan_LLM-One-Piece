{
 "cells": [
  {"cell_type": "markdown", "id": "0", "metadata": {}, "source": ["# Lab-1.7: DPO 訓練\n\n**目標**: 實現並訓練 DPO 對齊模型"]},
  {"cell_type": "code", "execution_count": null, "id": "1", "metadata": {}, "outputs": [], "source": ["import torch\nimport torch.nn.functional as F\n\ndef dpo_loss(policy_logps_chosen, policy_logps_rejected, ref_logps_chosen, ref_logps_rejected, beta=0.1):\n    chosen_rewards = beta * (policy_logps_chosen - ref_logps_chosen)\n    rejected_rewards = beta * (policy_logps_rejected - ref_logps_rejected)\n    loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n    return loss\n\nprint('✅ DPO 損失函數實現完成')"]}
 ]
}
