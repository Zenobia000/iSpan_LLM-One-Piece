{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.4.3: 極致性能調優 🚀\n",
    "\n",
    "## 🎯 學習目標\n",
    "\n",
    "- **瓶頸分析**: 識別和解決性能瓶頸\n",
    "- **GPU 記憶體管理**: 優化記憶體使用策略\n",
    "- **Multi-GPU 配置**: 擴展到多 GPU 環境\n",
    "- **性能監控**: 實時監控和調優指標\n",
    "\n",
    "## 🏗️ 實驗架構\n",
    "\n",
    "```\n",
    "Performance Optimization Pipeline\n",
    "├── 瓶頸分析和診斷\n",
    "├── GPU 記憶體優化\n",
    "├── Multi-GPU 負載均衡\n",
    "└── 實時性能監控\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境準備與基礎設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "import psutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import tritonclient.http as httpclient\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# GPU 監控相關\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"✅ NVIDIA GPU 監控已初始化\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"⚠️  NVIDIA GPU 監控不可用\")\n",
    "\n",
    "# 設置圖表樣式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🔧 環境設置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 性能監控基礎設施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"性能指標數據結構\"\"\"\n",
    "    timestamp: float\n",
    "    latency_ms: float\n",
    "    throughput_rps: float\n",
    "    cpu_usage: float\n",
    "    memory_usage: float\n",
    "    gpu_usage: Optional[float] = None\n",
    "    gpu_memory_usage: Optional[float] = None\n",
    "    queue_size: Optional[int] = None\n",
    "    batch_size: Optional[int] = None\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"實時性能監控器\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.metrics_history: List[PerformanceMetrics] = []\n",
    "        self.monitoring = False\n",
    "        \n",
    "    def get_system_metrics(self) -> Dict:\n",
    "        \"\"\"獲取系統資源使用情況\"\"\"\n",
    "        metrics = {\n",
    "            'cpu_usage': psutil.cpu_percent(interval=0.1),\n",
    "            'memory_usage': psutil.virtual_memory().percent,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        # GPU 監控\n",
    "        if GPU_AVAILABLE:\n",
    "            try:\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                \n",
    "                metrics.update({\n",
    "                    'gpu_usage': gpu_util.gpu,\n",
    "                    'gpu_memory_usage': (mem_info.used / mem_info.total) * 100\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"GPU 監控錯誤: {e}\")\n",
    "                \n",
    "        return metrics\n",
    "    \n",
    "    def get_triton_metrics(self) -> Dict:\n",
    "        \"\"\"獲取 Triton 服務器指標\"\"\"\n",
    "        try:\n",
    "            # 獲取服務器統計信息\n",
    "            stats = self.client.get_inference_statistics()\n",
    "            \n",
    "            # 獲取服務器狀態\n",
    "            server_metadata = self.client.get_server_metadata()\n",
    "            \n",
    "            return {\n",
    "                'server_stats': stats,\n",
    "                'server_metadata': server_metadata,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Triton 指標獲取錯誤: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    async def monitor_performance(self, duration: int = 60, interval: float = 1.0):\n",
    "        \"\"\"持續監控性能\"\"\"\n",
    "        self.monitoring = True\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"🔍 開始性能監控 ({duration} 秒)\")\n",
    "        \n",
    "        while self.monitoring and (time.time() - start_time) < duration:\n",
    "            # 收集指標\n",
    "            system_metrics = self.get_system_metrics()\n",
    "            triton_metrics = self.get_triton_metrics()\n",
    "            \n",
    "            # 記錄指標\n",
    "            metric = PerformanceMetrics(\n",
    "                timestamp=system_metrics['timestamp'],\n",
    "                latency_ms=0,  # 將在負載測試中更新\n",
    "                throughput_rps=0,  # 將在負載測試中更新\n",
    "                cpu_usage=system_metrics['cpu_usage'],\n",
    "                memory_usage=system_metrics['memory_usage'],\n",
    "                gpu_usage=system_metrics.get('gpu_usage'),\n",
    "                gpu_memory_usage=system_metrics.get('gpu_memory_usage')\n",
    "            )\n",
    "            \n",
    "            self.metrics_history.append(metric)\n",
    "            \n",
    "            await asyncio.sleep(interval)\n",
    "        \n",
    "        print(\"✅ 性能監控完成\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"停止監控\"\"\"\n",
    "        self.monitoring = False\n",
    "    \n",
    "    def plot_metrics(self, figsize: Tuple[int, int] = (15, 10)):\n",
    "        \"\"\"繪製性能指標圖表\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"⚠️  沒有性能數據可顯示\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        timestamps = [m.timestamp for m in self.metrics_history]\n",
    "        start_time = timestamps[0]\n",
    "        relative_times = [(t - start_time) for t in timestamps]\n",
    "        \n",
    "        # CPU 使用率\n",
    "        axes[0, 0].plot(relative_times, [m.cpu_usage for m in self.metrics_history], 'b-')\n",
    "        axes[0, 0].set_title('CPU 使用率 (%)')\n",
    "        axes[0, 0].set_ylabel('使用率 (%)')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # 記憶體使用率\n",
    "        axes[0, 1].plot(relative_times, [m.memory_usage for m in self.metrics_history], 'g-')\n",
    "        axes[0, 1].set_title('記憶體使用率 (%)')\n",
    "        axes[0, 1].set_ylabel('使用率 (%)')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # GPU 使用率\n",
    "        if any(m.gpu_usage is not None for m in self.metrics_history):\n",
    "            gpu_usage = [m.gpu_usage or 0 for m in self.metrics_history]\n",
    "            axes[1, 0].plot(relative_times, gpu_usage, 'r-')\n",
    "            axes[1, 0].set_title('GPU 使用率 (%)')\n",
    "            axes[1, 0].set_ylabel('使用率 (%)')\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # GPU 記憶體使用率\n",
    "        if any(m.gpu_memory_usage is not None for m in self.metrics_history):\n",
    "            gpu_mem = [m.gpu_memory_usage or 0 for m in self.metrics_history]\n",
    "            axes[1, 1].plot(relative_times, gpu_mem, 'orange')\n",
    "            axes[1, 1].set_title('GPU 記憶體使用率 (%)')\n",
    "            axes[1, 1].set_ylabel('使用率 (%)')\n",
    "            axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 初始化監控器\n",
    "monitor = PerformanceMonitor()\n",
    "print(\"📊 性能監控器已初始化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 瓶頸分析和診斷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckAnalyzer:\n",
    "    \"\"\"瓶頸分析器\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        \n",
    "    def analyze_model_performance(self, model_name: str, test_duration: int = 30) -> Dict:\n",
    "        \"\"\"分析單個模型的性能瓶頸\"\"\"\n",
    "        print(f\"🔍 分析模型 {model_name} 的性能...\")\n",
    "        \n",
    "        # 獲取模型配置\n",
    "        try:\n",
    "            model_config = self.client.get_model_config(model_name)\n",
    "            model_metadata = self.client.get_model_metadata(model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 無法獲取模型信息: {e}\")\n",
    "            return {}\n",
    "        \n",
    "        # 分析結果\n",
    "        analysis = {\n",
    "            'model_name': model_name,\n",
    "            'config': model_config,\n",
    "            'metadata': model_metadata,\n",
    "            'bottlenecks': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # 檢查配置潛在問題\n",
    "        config_dict = model_config\n",
    "        \n",
    "        # 檢查動態批次設置\n",
    "        if 'dynamic_batching' in config_dict:\n",
    "            db_config = config_dict['dynamic_batching']\n",
    "            max_queue_delay = db_config.get('max_queue_delay_microseconds', 0)\n",
    "            \n",
    "            if max_queue_delay == 0:\n",
    "                analysis['bottlenecks'].append(\"動態批次未設置最大延遲\")\n",
    "                analysis['recommendations'].append(\"設置適當的 max_queue_delay_microseconds\")\n",
    "            \n",
    "            preferred_batch_size = db_config.get('preferred_batch_size', [])\n",
    "            if not preferred_batch_size:\n",
    "                analysis['bottlenecks'].append(\"未設置首選批次大小\")\n",
    "                analysis['recommendations'].append(\"配置 preferred_batch_size 優化吞吐量\")\n",
    "        else:\n",
    "            analysis['bottlenecks'].append(\"未啟用動態批次\")\n",
    "            analysis['recommendations'].append(\"啟用動態批次以提高吞吐量\")\n",
    "        \n",
    "        # 檢查實例數量\n",
    "        instance_group = config_dict.get('instance_group', [{}])[0]\n",
    "        count = instance_group.get('count', 1)\n",
    "        \n",
    "        if count == 1:\n",
    "            analysis['bottlenecks'].append(\"單實例可能成為瓶頸\")\n",
    "            analysis['recommendations'].append(\"考慮增加實例數量\")\n",
    "        \n",
    "        # 檢查 GPU 類型\n",
    "        kind = instance_group.get('kind', 'KIND_AUTO')\n",
    "        if kind == 'KIND_CPU':\n",
    "            analysis['bottlenecks'].append(\"使用 CPU 推理可能較慢\")\n",
    "            analysis['recommendations'].append(\"考慮使用 GPU 推理\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def benchmark_latency_throughput(self, model_name: str, \n",
    "                                   batch_sizes: List[int] = [1, 2, 4, 8, 16, 32],\n",
    "                                   concurrency_levels: List[int] = [1, 2, 4, 8, 16]) -> Dict:\n",
    "        \"\"\"基準測試延遲和吞吐量\"\"\"\n",
    "        print(f\"📊 對模型 {model_name} 進行基準測試...\")\n",
    "        \n",
    "        results = {\n",
    "            'batch_size_analysis': {},\n",
    "            'concurrency_analysis': {},\n",
    "            'optimal_config': {}\n",
    "        }\n",
    "        \n",
    "        # 測試不同批次大小\n",
    "        print(\"🔬 測試批次大小影響...\")\n",
    "        for batch_size in batch_sizes:\n",
    "            try:\n",
    "                latency, throughput = self._test_batch_size(model_name, batch_size)\n",
    "                results['batch_size_analysis'][batch_size] = {\n",
    "                    'latency_ms': latency,\n",
    "                    'throughput_rps': throughput\n",
    "                }\n",
    "                print(f\"  批次大小 {batch_size}: 延遲 {latency:.2f}ms, 吞吐量 {throughput:.2f} RPS\")\n",
    "            except Exception as e:\n",
    "                print(f\"  批次大小 {batch_size} 測試失敗: {e}\")\n",
    "        \n",
    "        # 測試不同並發級別\n",
    "        print(\"🔬 測試並發級別影響...\")\n",
    "        for concurrency in concurrency_levels:\n",
    "            try:\n",
    "                latency, throughput = self._test_concurrency(model_name, concurrency)\n",
    "                results['concurrency_analysis'][concurrency] = {\n",
    "                    'latency_ms': latency,\n",
    "                    'throughput_rps': throughput\n",
    "                }\n",
    "                print(f\"  並發級別 {concurrency}: 延遲 {latency:.2f}ms, 吞吐量 {throughput:.2f} RPS\")\n",
    "            except Exception as e:\n",
    "                print(f\"  並發級別 {concurrency} 測試失敗: {e}\")\n",
    "        \n",
    "        # 找出最優配置\n",
    "        if results['batch_size_analysis']:\n",
    "            best_batch = max(results['batch_size_analysis'].items(), \n",
    "                           key=lambda x: x[1]['throughput_rps'])\n",
    "            results['optimal_config']['best_batch_size'] = best_batch[0]\n",
    "            \n",
    "        if results['concurrency_analysis']:\n",
    "            best_concurrency = max(results['concurrency_analysis'].items(), \n",
    "                                 key=lambda x: x[1]['throughput_rps'])\n",
    "            results['optimal_config']['best_concurrency'] = best_concurrency[0]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _test_batch_size(self, model_name: str, batch_size: int) -> Tuple[float, float]:\n",
    "        \"\"\"測試特定批次大小的性能\"\"\"\n",
    "        # 創建測試數據\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        \n",
    "        try:\n",
    "            # 獲取模型元數據以了解輸入格式\n",
    "            metadata = self.client.get_model_metadata(model_name)\n",
    "            \n",
    "            # 創建示例輸入 (這裡需要根據實際模型調整)\n",
    "            for inp in metadata['inputs']:\n",
    "                input_name = inp['name']\n",
    "                input_shape = inp['shape']\n",
    "                input_dtype = inp['datatype']\n",
    "                \n",
    "                # 創建測試數據\n",
    "                if input_dtype == 'FP32':\n",
    "                    test_data = np.random.random([batch_size] + input_shape).astype(np.float32)\n",
    "                elif input_dtype == 'INT64':\n",
    "                    test_data = np.random.randint(0, 1000, [batch_size] + input_shape).astype(np.int64)\n",
    "                else:\n",
    "                    test_data = np.random.random([batch_size] + input_shape).astype(np.float32)\n",
    "                \n",
    "                inputs.append(httpclient.InferInput(input_name, test_data.shape, input_dtype))\n",
    "                inputs[-1].set_data_from_numpy(test_data)\n",
    "            \n",
    "            for out in metadata['outputs']:\n",
    "                outputs.append(httpclient.InferRequestedOutput(out['name']))\n",
    "        \n",
    "        except Exception as e:\n",
    "            # 如果無法獲取元數據，使用默認測試\n",
    "            test_data = np.random.random((batch_size, 224, 224, 3)).astype(np.float32)\n",
    "            inputs = [httpclient.InferInput(\"input\", test_data.shape, \"FP32\")]\n",
    "            inputs[0].set_data_from_numpy(test_data)\n",
    "            outputs = [httpclient.InferRequestedOutput(\"output\")]\n",
    "        \n",
    "        # 執行測試\n",
    "        num_requests = 100\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_requests):\n",
    "            try:\n",
    "                self.client.infer(model_name, inputs, outputs=outputs)\n",
    "            except Exception:\n",
    "                pass  # 忽略個別請求錯誤\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        total_time = end_time - start_time\n",
    "        avg_latency = (total_time / num_requests) * 1000  # 轉換為毫秒\n",
    "        throughput = (num_requests * batch_size) / total_time\n",
    "        \n",
    "        return avg_latency, throughput\n",
    "    \n",
    "    def _test_concurrency(self, model_name: str, concurrency: int) -> Tuple[float, float]:\n",
    "        \"\"\"測試特定並發級別的性能\"\"\"\n",
    "        # 簡化的並發測試\n",
    "        return self._test_batch_size(model_name, 1)  # 暫時使用批次大小測試\n",
    "    \n",
    "    def generate_optimization_report(self, model_name: str) -> str:\n",
    "        \"\"\"生成優化建議報告\"\"\"\n",
    "        analysis = self.analyze_model_performance(model_name)\n",
    "        benchmark = self.benchmark_latency_throughput(model_name)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# 🎯 模型性能優化報告: {model_name}\n",
    "\n",
    "## 📊 瓶頸分析\n",
    "\"\"\"\n",
    "        \n",
    "        if analysis.get('bottlenecks'):\n",
    "            report += \"\\n### 🚨 發現的瓶頸:\\n\"\n",
    "            for bottleneck in analysis['bottlenecks']:\n",
    "                report += f\"- {bottleneck}\\n\"\n",
    "        \n",
    "        if analysis.get('recommendations'):\n",
    "            report += \"\\n### 💡 優化建議:\\n\"\n",
    "            for rec in analysis['recommendations']:\n",
    "                report += f\"- {rec}\\n\"\n",
    "        \n",
    "        if benchmark.get('optimal_config'):\n",
    "            report += \"\\n### 🎯 最優配置:\\n\"\n",
    "            config = benchmark['optimal_config']\n",
    "            if 'best_batch_size' in config:\n",
    "                report += f\"- 最佳批次大小: {config['best_batch_size']}\\n\"\n",
    "            if 'best_concurrency' in config:\n",
    "                report += f\"- 最佳並發級別: {config['best_concurrency']}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# 初始化分析器\n",
    "analyzer = BottleneckAnalyzer()\n",
    "print(\"🔍 瓶頸分析器已初始化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPU 記憶體管理優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryOptimizer:\n",
    "    \"\"\"GPU 記憶體優化器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_available = GPU_AVAILABLE\n",
    "        \n",
    "    def get_gpu_memory_info(self) -> Dict:\n",
    "        \"\"\"獲取 GPU 記憶體信息\"\"\"\n",
    "        if not self.gpu_available:\n",
    "            return {'error': 'GPU 不可用'}\n",
    "        \n",
    "        try:\n",
    "            device_count = pynvml.nvmlDeviceGetCount()\n",
    "            gpu_info = {}\n",
    "            \n",
    "            for i in range(device_count):\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "                \n",
    "                gpu_info[f'gpu_{i}'] = {\n",
    "                    'name': name,\n",
    "                    'total_memory_gb': mem_info.total / (1024**3),\n",
    "                    'used_memory_gb': mem_info.used / (1024**3),\n",
    "                    'free_memory_gb': mem_info.free / (1024**3),\n",
    "                    'utilization_percent': (mem_info.used / mem_info.total) * 100\n",
    "                }\n",
    "            \n",
    "            return gpu_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'獲取 GPU 信息失敗: {e}'}\n",
    "    \n",
    "    def analyze_memory_usage_pattern(self, duration: int = 60) -> Dict:\n",
    "        \"\"\"分析記憶體使用模式\"\"\"\n",
    "        print(f\"📈 分析 GPU 記憶體使用模式 ({duration} 秒)...\")\n",
    "        \n",
    "        if not self.gpu_available:\n",
    "            return {'error': 'GPU 不可用'}\n",
    "        \n",
    "        memory_history = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            gpu_info = self.get_gpu_memory_info()\n",
    "            if 'error' not in gpu_info:\n",
    "                timestamp = time.time()\n",
    "                for gpu_id, info in gpu_info.items():\n",
    "                    memory_history.append({\n",
    "                        'timestamp': timestamp,\n",
    "                        'gpu_id': gpu_id,\n",
    "                        'used_memory_gb': info['used_memory_gb'],\n",
    "                        'utilization_percent': info['utilization_percent']\n",
    "                    })\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # 分析模式\n",
    "        analysis = {\n",
    "            'memory_history': memory_history,\n",
    "            'peak_usage': {},\n",
    "            'average_usage': {},\n",
    "            'memory_spikes': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # 計算統計信息\n",
    "        if memory_history:\n",
    "            gpus = set(item['gpu_id'] for item in memory_history)\n",
    "            \n",
    "            for gpu_id in gpus:\n",
    "                gpu_data = [item for item in memory_history if item['gpu_id'] == gpu_id]\n",
    "                usage_values = [item['utilization_percent'] for item in gpu_data]\n",
    "                \n",
    "                analysis['peak_usage'][gpu_id] = max(usage_values)\n",
    "                analysis['average_usage'][gpu_id] = sum(usage_values) / len(usage_values)\n",
    "                \n",
    "                # 檢測記憶體峰值\n",
    "                threshold = 90  # 90% 使用率閾值\n",
    "                spikes = [item for item in gpu_data if item['utilization_percent'] > threshold]\n",
    "                if spikes:\n",
    "                    analysis['memory_spikes'].extend(spikes)\n",
    "        \n",
    "        # 生成建議\n",
    "        if analysis['memory_spikes']:\n",
    "            analysis['recommendations'].append(\"檢測到記憶體使用峰值，考慮優化批次大小\")\n",
    "        \n",
    "        avg_usage = analysis.get('average_usage', {})\n",
    "        if avg_usage and any(usage > 80 for usage in avg_usage.values()):\n",
    "            analysis['recommendations'].append(\"記憶體使用率較高，建議啟用記憶體優化策略\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def generate_memory_optimization_config(self, target_memory_usage: float = 75.0) -> Dict:\n",
    "        \"\"\"生成記憶體優化配置\"\"\"\n",
    "        gpu_info = self.get_gpu_memory_info()\n",
    "        \n",
    "        if 'error' in gpu_info:\n",
    "            return gpu_info\n",
    "        \n",
    "        optimization_config = {\n",
    "            'memory_pool_config': {},\n",
    "            'instance_group_config': {},\n",
    "            'batching_config': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        for gpu_id, info in gpu_info.items():\n",
    "            total_memory_gb = info['total_memory_gb']\n",
    "            target_memory_gb = total_memory_gb * (target_memory_usage / 100)\n",
    "            \n",
    "            # 記憶體池配置\n",
    "            optimization_config['memory_pool_config'][gpu_id] = {\n",
    "                'gpu_memory_pool_byte_size': int(target_memory_gb * 1024**3),\n",
    "                'pinned_memory_pool_byte_size': 268435456  # 256MB\n",
    "            }\n",
    "            \n",
    "            # 實例組配置\n",
    "            optimization_config['instance_group_config'][gpu_id] = {\n",
    "                'count': 1,\n",
    "                'kind': 'KIND_GPU',\n",
    "                'gpus': [int(gpu_id.split('_')[1])]\n",
    "            }\n",
    "            \n",
    "            # 建議的批次配置\n",
    "            optimization_config['batching_config'][gpu_id] = {\n",
    "                'max_batch_size': min(32, max(1, int(target_memory_gb / 2))),\n",
    "                'preferred_batch_size': [1, 2, 4, 8]\n",
    "            }\n",
    "        \n",
    "        # 通用建議\n",
    "        optimization_config['recommendations'] = [\n",
    "            \"使用動態批次以最大化記憶體效率\",\n",
    "            \"監控記憶體使用情況並調整批次大小\",\n",
    "            \"考慮使用模型並行化分散記憶體負載\",\n",
    "            f\"目標記憶體使用率: {target_memory_usage}%\"\n",
    "        ]\n",
    "        \n",
    "        return optimization_config\n",
    "    \n",
    "    def plot_memory_usage(self, memory_history: List[Dict], figsize: Tuple[int, int] = (12, 6)):\n",
    "        \"\"\"繪製記憶體使用圖表\"\"\"\n",
    "        if not memory_history:\n",
    "            print(\"⚠️  沒有記憶體使用數據\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # 按 GPU 分組\n",
    "        gpus = set(item['gpu_id'] for item in memory_history)\n",
    "        \n",
    "        for gpu_id in gpus:\n",
    "            gpu_data = [item for item in memory_history if item['gpu_id'] == gpu_id]\n",
    "            timestamps = [item['timestamp'] for item in gpu_data]\n",
    "            utilization = [item['utilization_percent'] for item in gpu_data]\n",
    "            \n",
    "            # 轉換為相對時間\n",
    "            start_time = min(timestamps)\n",
    "            relative_times = [(t - start_time) / 60 for t in timestamps]  # 轉換為分鐘\n",
    "            \n",
    "            plt.plot(relative_times, utilization, label=f'{gpu_id.upper()}')\n",
    "        \n",
    "        plt.title('GPU 記憶體使用率趨勢')\n",
    "        plt.xlabel('時間 (分鐘)')\n",
    "        plt.ylabel('記憶體使用率 (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='警告線 (80%)')\n",
    "        plt.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='危險線 (90%)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 初始化 GPU 記憶體優化器\n",
    "gpu_optimizer = GPUMemoryOptimizer()\n",
    "print(\"🎯 GPU 記憶體優化器已初始化\")\n",
    "\n",
    "# 顯示當前 GPU 狀態\n",
    "gpu_info = gpu_optimizer.get_gpu_memory_info()\n",
    "print(\"\\n📊 當前 GPU 狀態:\")\n",
    "for gpu_id, info in gpu_info.items():\n",
    "    if gpu_id != 'error':\n",
    "        print(f\"  {gpu_id.upper()}: {info['name']}\")\n",
    "        print(f\"    總記憶體: {info['total_memory_gb']:.2f} GB\")\n",
    "        print(f\"    已使用: {info['used_memory_gb']:.2f} GB ({info['utilization_percent']:.1f}%)\")\n",
    "        print(f\"    可用: {info['free_memory_gb']:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"  {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-GPU 配置和負載均衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPUManager:\n",
    "    \"\"\"Multi-GPU 管理器\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.gpu_optimizer = GPUMemoryOptimizer()\n",
    "        \n",
    "    def analyze_gpu_topology(self) -> Dict:\n",
    "        \"\"\"分析 GPU 拓撲結構\"\"\"\n",
    "        print(\"🔍 分析 GPU 拓撲結構...\")\n",
    "        \n",
    "        if not GPU_AVAILABLE:\n",
    "            return {'error': 'GPU 不可用'}\n",
    "        \n",
    "        try:\n",
    "            device_count = pynvml.nvmlDeviceGetCount()\n",
    "            topology = {\n",
    "                'device_count': device_count,\n",
    "                'devices': {},\n",
    "                'nvlink_topology': {},\n",
    "                'recommendations': []\n",
    "            }\n",
    "            \n",
    "            for i in range(device_count):\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                \n",
    "                try:\n",
    "                    # 嘗試獲取 GPU 拓撲信息\n",
    "                    pci_info = pynvml.nvmlDeviceGetPciInfo(handle)\n",
    "                    \n",
    "                    topology['devices'][f'gpu_{i}'] = {\n",
    "                        'name': name,\n",
    "                        'memory_total_gb': mem_info.total / (1024**3),\n",
    "                        'pci_bus': pci_info.bus,\n",
    "                        'pci_device': pci_info.device,\n",
    "                        'pci_domain': pci_info.domain\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    topology['devices'][f'gpu_{i}'] = {\n",
    "                        'name': name,\n",
    "                        'memory_total_gb': mem_info.total / (1024**3),\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "            \n",
    "            # 分析 NVLink 連接 (如果可用)\n",
    "            if device_count > 1:\n",
    "                try:\n",
    "                    for i in range(device_count):\n",
    "                        handle_i = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                        for j in range(i + 1, device_count):\n",
    "                            handle_j = pynvml.nvmlDeviceGetHandleByIndex(j)\n",
    "                            # 這裡可以添加 NVLink 連接檢測\n",
    "                            # 由於 NVML API 的限制，這部分可能需要其他方法\n",
    "                            pass\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            # 生成建議\n",
    "            if device_count == 1:\n",
    "                topology['recommendations'].append(\"單 GPU 配置，考慮增加 GPU 數量以提高性能\")\n",
    "            elif device_count > 1:\n",
    "                topology['recommendations'].append(f\"檢測到 {device_count} 個 GPU，可以配置模型並行\")\n",
    "                topology['recommendations'].append(\"建議為不同模型分配不同 GPU 以避免競爭\")\n",
    "            \n",
    "            return topology\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'分析 GPU 拓撲失敗: {e}'}\n",
    "    \n",
    "    def generate_multi_gpu_config(self, model_configs: List[Dict]) -> Dict:\n",
    "        \"\"\"生成 Multi-GPU 配置\"\"\"\n",
    "        print(\"⚙️  生成 Multi-GPU 配置...\")\n",
    "        \n",
    "        topology = self.analyze_gpu_topology()\n",
    "        if 'error' in topology:\n",
    "            return topology\n",
    "        \n",
    "        device_count = topology['device_count']\n",
    "        if device_count < 2:\n",
    "            return {'error': '需要至少 2 個 GPU 來配置 Multi-GPU'}\n",
    "        \n",
    "        multi_gpu_config = {\n",
    "            'gpu_allocation': {},\n",
    "            'load_balancing': {},\n",
    "            'model_configs': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # GPU 分配策略\n",
    "        num_models = len(model_configs)\n",
    "        \n",
    "        if num_models <= device_count:\n",
    "            # 每個模型分配一個 GPU\n",
    "            for i, model_config in enumerate(model_configs):\n",
    "                model_name = model_config.get('name', f'model_{i}')\n",
    "                gpu_id = i % device_count\n",
    "                \n",
    "                multi_gpu_config['gpu_allocation'][model_name] = [gpu_id]\n",
    "                multi_gpu_config['model_configs'][model_name] = {\n",
    "                    'instance_group': [\n",
    "                        {\n",
    "                            'count': 1,\n",
    "                            'kind': 'KIND_GPU',\n",
    "                            'gpus': [gpu_id]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "        else:\n",
    "            # 模型數量超過 GPU 數量，需要共享\n",
    "            for i, model_config in enumerate(model_configs):\n",
    "                model_name = model_config.get('name', f'model_{i}')\n",
    "                gpu_id = i % device_count\n",
    "                \n",
    "                if gpu_id not in multi_gpu_config['gpu_allocation']:\n",
    "                    multi_gpu_config['gpu_allocation'][gpu_id] = []\n",
    "                \n",
    "                multi_gpu_config['gpu_allocation'][gpu_id].append(model_name)\n",
    "                multi_gpu_config['model_configs'][model_name] = {\n",
    "                    'instance_group': [\n",
    "                        {\n",
    "                            'count': 1,\n",
    "                            'kind': 'KIND_GPU',\n",
    "                            'gpus': [gpu_id]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "        \n",
    "        # 負載均衡配置\n",
    "        multi_gpu_config['load_balancing'] = {\n",
    "            'strategy': 'round_robin',\n",
    "            'health_check': {\n",
    "                'enabled': True,\n",
    "                'interval_ms': 5000\n",
    "            },\n",
    "            'failover': {\n",
    "                'enabled': True,\n",
    "                'retry_count': 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 生成建議\n",
    "        multi_gpu_config['recommendations'] = [\n",
    "            f\"已為 {num_models} 個模型配置 {device_count} 個 GPU\",\n",
    "            \"建議監控各 GPU 的負載均衡情況\",\n",
    "            \"可以根據模型使用頻率調整 GPU 分配\",\n",
    "            \"考慮為高頻模型配置多個實例\"\n",
    "        ]\n",
    "        \n",
    "        if num_models > device_count:\n",
    "            multi_gpu_config['recommendations'].append(\n",
    "                \"模型數量超過 GPU 數量，建議監控 GPU 使用率\"\n",
    "            )\n",
    "        \n",
    "        return multi_gpu_config\n",
    "    \n",
    "    def monitor_multi_gpu_performance(self, duration: int = 60) -> Dict:\n",
    "        \"\"\"監控 Multi-GPU 性能\"\"\"\n",
    "        print(f\"📊 監控 Multi-GPU 性能 ({duration} 秒)...\")\n",
    "        \n",
    "        if not GPU_AVAILABLE:\n",
    "            return {'error': 'GPU 不可用'}\n",
    "        \n",
    "        performance_data = {\n",
    "            'timeline': [],\n",
    "            'gpu_metrics': {},\n",
    "            'load_balance_analysis': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            timestamp = time.time()\n",
    "            gpu_info = self.gpu_optimizer.get_gpu_memory_info()\n",
    "            \n",
    "            if 'error' not in gpu_info:\n",
    "                frame_data = {'timestamp': timestamp}\n",
    "                \n",
    "                for gpu_id, info in gpu_info.items():\n",
    "                    frame_data[gpu_id] = {\n",
    "                        'memory_usage': info['utilization_percent'],\n",
    "                        'memory_used_gb': info['used_memory_gb']\n",
    "                    }\n",
    "                    \n",
    "                    # 累積統計\n",
    "                    if gpu_id not in performance_data['gpu_metrics']:\n",
    "                        performance_data['gpu_metrics'][gpu_id] = {\n",
    "                            'usage_history': [],\n",
    "                            'peak_usage': 0,\n",
    "                            'avg_usage': 0\n",
    "                        }\n",
    "                    \n",
    "                    performance_data['gpu_metrics'][gpu_id]['usage_history'].append(\n",
    "                        info['utilization_percent']\n",
    "                    )\n",
    "                    performance_data['gpu_metrics'][gpu_id]['peak_usage'] = max(\n",
    "                        performance_data['gpu_metrics'][gpu_id]['peak_usage'],\n",
    "                        info['utilization_percent']\n",
    "                    )\n",
    "                \n",
    "                performance_data['timeline'].append(frame_data)\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        # 計算平均使用率和負載均衡分析\n",
    "        for gpu_id, metrics in performance_data['gpu_metrics'].items():\n",
    "            usage_history = metrics['usage_history']\n",
    "            if usage_history:\n",
    "                metrics['avg_usage'] = sum(usage_history) / len(usage_history)\n",
    "        \n",
    "        # 負載均衡分析\n",
    "        avg_usages = [metrics['avg_usage'] for metrics in performance_data['gpu_metrics'].values()]\n",
    "        if len(avg_usages) > 1:\n",
    "            usage_variance = np.var(avg_usages)\n",
    "            performance_data['load_balance_analysis'] = {\n",
    "                'usage_variance': usage_variance,\n",
    "                'max_usage_diff': max(avg_usages) - min(avg_usages),\n",
    "                'is_balanced': usage_variance < 100  # 變異數閾值\n",
    "            }\n",
    "            \n",
    "            if not performance_data['load_balance_analysis']['is_balanced']:\n",
    "                performance_data['recommendations'].append(\n",
    "                    \"檢測到負載不均衡，建議重新分配模型或調整實例數量\"\n",
    "                )\n",
    "        \n",
    "        return performance_data\n",
    "    \n",
    "    def plot_multi_gpu_performance(self, performance_data: Dict, figsize: Tuple[int, int] = (15, 8)):\n",
    "        \"\"\"繪製 Multi-GPU 性能圖表\"\"\"\n",
    "        if not performance_data.get('timeline'):\n",
    "            print(\"⚠️  沒有性能數據可顯示\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # 提取數據\n",
    "        timeline = performance_data['timeline']\n",
    "        timestamps = [frame['timestamp'] for frame in timeline]\n",
    "        start_time = min(timestamps)\n",
    "        relative_times = [(t - start_time) / 60 for t in timestamps]  # 轉換為分鐘\n",
    "        \n",
    "        # GPU 使用率時間線\n",
    "        gpu_ids = [key for key in timeline[0].keys() if key.startswith('gpu_')]\n",
    "        \n",
    "        for gpu_id in gpu_ids:\n",
    "            usage_data = [frame[gpu_id]['memory_usage'] for frame in timeline]\n",
    "            axes[0, 0].plot(relative_times, usage_data, label=gpu_id.upper())\n",
    "        \n",
    "        axes[0, 0].set_title('GPU 記憶體使用率時間線')\n",
    "        axes[0, 0].set_xlabel('時間 (分鐘)')\n",
    "        axes[0, 0].set_ylabel('使用率 (%)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # 平均使用率比較\n",
    "        gpu_metrics = performance_data['gpu_metrics']\n",
    "        gpu_names = list(gpu_metrics.keys())\n",
    "        avg_usages = [gpu_metrics[gpu]['avg_usage'] for gpu in gpu_names]\n",
    "        \n",
    "        axes[0, 1].bar(range(len(gpu_names)), avg_usages)\n",
    "        axes[0, 1].set_title('平均 GPU 使用率比較')\n",
    "        axes[0, 1].set_xlabel('GPU')\n",
    "        axes[0, 1].set_ylabel('平均使用率 (%)')\n",
    "        axes[0, 1].set_xticks(range(len(gpu_names)))\n",
    "        axes[0, 1].set_xticklabels([gpu.upper() for gpu in gpu_names])\n",
    "        axes[0, 1].grid(True, axis='y')\n",
    "        \n",
    "        # 峰值使用率比較\n",
    "        peak_usages = [gpu_metrics[gpu]['peak_usage'] for gpu in gpu_names]\n",
    "        \n",
    "        axes[1, 0].bar(range(len(gpu_names)), peak_usages, color='orange')\n",
    "        axes[1, 0].set_title('峰值 GPU 使用率比較')\n",
    "        axes[1, 0].set_xlabel('GPU')\n",
    "        axes[1, 0].set_ylabel('峰值使用率 (%)')\n",
    "        axes[1, 0].set_xticks(range(len(gpu_names)))\n",
    "        axes[1, 0].set_xticklabels([gpu.upper() for gpu in gpu_names])\n",
    "        axes[1, 0].grid(True, axis='y')\n",
    "        \n",
    "        # 負載均衡分析\n",
    "        if 'load_balance_analysis' in performance_data:\n",
    "            balance_info = performance_data['load_balance_analysis']\n",
    "            axes[1, 1].text(0.1, 0.8, f\"使用率變異數: {balance_info['usage_variance']:.2f}\", \n",
    "                          transform=axes[1, 1].transAxes, fontsize=12)\n",
    "            axes[1, 1].text(0.1, 0.6, f\"最大使用率差異: {balance_info['max_usage_diff']:.2f}%\", \n",
    "                          transform=axes[1, 1].transAxes, fontsize=12)\n",
    "            balance_status = \"是\" if balance_info['is_balanced'] else \"否\"\n",
    "            axes[1, 1].text(0.1, 0.4, f\"負載均衡: {balance_status}\", \n",
    "                          transform=axes[1, 1].transAxes, fontsize=12, \n",
    "                          color='green' if balance_info['is_balanced'] else 'red')\n",
    "            axes[1, 1].set_title('負載均衡分析')\n",
    "            axes[1, 1].set_xlim(0, 1)\n",
    "            axes[1, 1].set_ylim(0, 1)\n",
    "            axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 初始化 Multi-GPU 管理器\n",
    "multi_gpu_manager = MultiGPUManager()\n",
    "print(\"🚀 Multi-GPU 管理器已初始化\")\n",
    "\n",
    "# 分析 GPU 拓撲\n",
    "topology = multi_gpu_manager.analyze_gpu_topology()\n",
    "if 'error' not in topology:\n",
    "    print(f\"\\n🔍 GPU 拓撲分析:\")\n",
    "    print(f\"  檢測到 {topology['device_count']} 個 GPU\")\n",
    "    for gpu_id, info in topology['devices'].items():\n",
    "        print(f\"    {gpu_id.upper()}: {info['name']} ({info['memory_total_gb']:.1f} GB)\")\n",
    "else:\n",
    "    print(f\"GPU 拓撲分析失敗: {topology['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 綜合性能優化實戰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveOptimizer:\n",
    "    \"\"\"綜合性能優化器\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.monitor = PerformanceMonitor(triton_url)\n",
    "        self.analyzer = BottleneckAnalyzer(triton_url)\n",
    "        self.gpu_optimizer = GPUMemoryOptimizer()\n",
    "        self.multi_gpu_manager = MultiGPUManager(triton_url)\n",
    "    \n",
    "    async def run_comprehensive_optimization(self, model_names: List[str], \n",
    "                                           optimization_duration: int = 300) -> Dict:\n",
    "        \"\"\"執行綜合性能優化分析\"\"\"\n",
    "        print(\"🚀 開始綜合性能優化分析...\")\n",
    "        \n",
    "        optimization_results = {\n",
    "            'models_analyzed': model_names,\n",
    "            'baseline_performance': {},\n",
    "            'bottleneck_analysis': {},\n",
    "            'memory_analysis': {},\n",
    "            'multi_gpu_analysis': {},\n",
    "            'optimization_recommendations': {},\n",
    "            'projected_improvements': {}\n",
    "        }\n",
    "        \n",
    "        # 1. 基線性能測試\n",
    "        print(\"📊 1/5 執行基線性能測試...\")\n",
    "        for model_name in model_names:\n",
    "            try:\n",
    "                baseline = await self._measure_baseline_performance(model_name)\n",
    "                optimization_results['baseline_performance'][model_name] = baseline\n",
    "                print(f\"  {model_name}: 延遲 {baseline.get('avg_latency_ms', 'N/A')}ms, \"\n",
    "                      f\"吞吐量 {baseline.get('throughput_rps', 'N/A')} RPS\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {model_name} 基線測試失敗: {e}\")\n",
    "        \n",
    "        # 2. 瓶頸分析\n",
    "        print(\"🔍 2/5 執行瓶頸分析...\")\n",
    "        for model_name in model_names:\n",
    "            try:\n",
    "                analysis = self.analyzer.analyze_model_performance(model_name)\n",
    "                optimization_results['bottleneck_analysis'][model_name] = analysis\n",
    "                if analysis.get('bottlenecks'):\n",
    "                    print(f\"  {model_name}: 發現 {len(analysis['bottlenecks'])} 個瓶頸\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {model_name} 瓶頸分析失敗: {e}\")\n",
    "        \n",
    "        # 3. 記憶體分析\n",
    "        print(\"💾 3/5 執行記憶體分析...\")\n",
    "        try:\n",
    "            memory_analysis = self.gpu_optimizer.analyze_memory_usage_pattern(60)\n",
    "            optimization_results['memory_analysis'] = memory_analysis\n",
    "            if memory_analysis.get('memory_spikes'):\n",
    "                print(f\"  檢測到 {len(memory_analysis['memory_spikes'])} 個記憶體峰值\")\n",
    "        except Exception as e:\n",
    "            print(f\"  記憶體分析失敗: {e}\")\n",
    "        \n",
    "        # 4. Multi-GPU 分析\n",
    "        print(\"🔄 4/5 執行 Multi-GPU 分析...\")\n",
    "        try:\n",
    "            model_configs = [{'name': name} for name in model_names]\n",
    "            multi_gpu_config = self.multi_gpu_manager.generate_multi_gpu_config(model_configs)\n",
    "            optimization_results['multi_gpu_analysis'] = multi_gpu_config\n",
    "            if 'error' not in multi_gpu_config:\n",
    "                print(f\"  生成了 Multi-GPU 配置，支持 {len(model_names)} 個模型\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Multi-GPU 分析失敗: {e}\")\n",
    "        \n",
    "        # 5. 生成優化建議\n",
    "        print(\"💡 5/5 生成優化建議...\")\n",
    "        optimization_results['optimization_recommendations'] = \\\n",
    "            self._generate_comprehensive_recommendations(optimization_results)\n",
    "        \n",
    "        # 6. 預測改進效果\n",
    "        optimization_results['projected_improvements'] = \\\n",
    "            self._project_improvements(optimization_results)\n",
    "        \n",
    "        print(\"✅ 綜合性能優化分析完成\")\n",
    "        return optimization_results\n",
    "    \n",
    "    async def _measure_baseline_performance(self, model_name: str) -> Dict:\n",
    "        \"\"\"測量基線性能\"\"\"\n",
    "        try:\n",
    "            # 獲取模型元數據\n",
    "            metadata = self.client.get_model_metadata(model_name)\n",
    "            \n",
    "            # 創建測試輸入\n",
    "            inputs = []\n",
    "            outputs = []\n",
    "            \n",
    "            for inp in metadata['inputs']:\n",
    "                input_name = inp['name']\n",
    "                input_shape = inp['shape']\n",
    "                input_dtype = inp['datatype']\n",
    "                \n",
    "                if input_dtype == 'FP32':\n",
    "                    test_data = np.random.random([1] + input_shape).astype(np.float32)\n",
    "                elif input_dtype == 'INT64':\n",
    "                    test_data = np.random.randint(0, 1000, [1] + input_shape).astype(np.int64)\n",
    "                else:\n",
    "                    test_data = np.random.random([1] + input_shape).astype(np.float32)\n",
    "                \n",
    "                inputs.append(httpclient.InferInput(input_name, test_data.shape, input_dtype))\n",
    "                inputs[-1].set_data_from_numpy(test_data)\n",
    "            \n",
    "            for out in metadata['outputs']:\n",
    "                outputs.append(httpclient.InferRequestedOutput(out['name']))\n",
    "            \n",
    "            # 預熱\n",
    "            for _ in range(10):\n",
    "                self.client.infer(model_name, inputs, outputs=outputs)\n",
    "            \n",
    "            # 測量性能\n",
    "            num_requests = 100\n",
    "            latencies = []\n",
    "            \n",
    "            for _ in range(num_requests):\n",
    "                start_time = time.time()\n",
    "                self.client.infer(model_name, inputs, outputs=outputs)\n",
    "                end_time = time.time()\n",
    "                latencies.append((end_time - start_time) * 1000)  # 轉換為毫秒\n",
    "            \n",
    "            # 計算統計信息\n",
    "            avg_latency = sum(latencies) / len(latencies)\n",
    "            p95_latency = np.percentile(latencies, 95)\n",
    "            p99_latency = np.percentile(latencies, 99)\n",
    "            throughput = num_requests / (sum(latencies) / 1000)  # RPS\n",
    "            \n",
    "            return {\n",
    "                'avg_latency_ms': avg_latency,\n",
    "                'p95_latency_ms': p95_latency,\n",
    "                'p99_latency_ms': p99_latency,\n",
    "                'throughput_rps': throughput,\n",
    "                'min_latency_ms': min(latencies),\n",
    "                'max_latency_ms': max(latencies)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _generate_comprehensive_recommendations(self, results: Dict) -> Dict:\n",
    "        \"\"\"生成綜合優化建議\"\"\"\n",
    "        recommendations = {\n",
    "            'high_priority': [],\n",
    "            'medium_priority': [],\n",
    "            'low_priority': [],\n",
    "            'implementation_order': []\n",
    "        }\n",
    "        \n",
    "        # 分析瓶頸並生成建議\n",
    "        for model_name, analysis in results.get('bottleneck_analysis', {}).items():\n",
    "            if analysis.get('bottlenecks'):\n",
    "                for bottleneck in analysis['bottlenecks']:\n",
    "                    if '動態批次' in bottleneck:\n",
    "                        recommendations['high_priority'].append(\n",
    "                            f\"{model_name}: 啟用並優化動態批次配置\"\n",
    "                        )\n",
    "                    elif '實例' in bottleneck:\n",
    "                        recommendations['medium_priority'].append(\n",
    "                            f\"{model_name}: 增加模型實例數量\"\n",
    "                        )\n",
    "                    elif 'CPU' in bottleneck:\n",
    "                        recommendations['high_priority'].append(\n",
    "                            f\"{model_name}: 從 CPU 遷移到 GPU 推理\"\n",
    "                        )\n",
    "        \n",
    "        # 記憶體優化建議\n",
    "        memory_analysis = results.get('memory_analysis', {})\n",
    "        if memory_analysis.get('memory_spikes'):\n",
    "            recommendations['high_priority'].append(\n",
    "                \"優化 GPU 記憶體使用，減少記憶體峰值\"\n",
    "            )\n",
    "        \n",
    "        if memory_analysis.get('recommendations'):\n",
    "            recommendations['medium_priority'].extend(\n",
    "                memory_analysis['recommendations']\n",
    "            )\n",
    "        \n",
    "        # Multi-GPU 建議\n",
    "        multi_gpu_analysis = results.get('multi_gpu_analysis', {})\n",
    "        if 'error' not in multi_gpu_analysis and multi_gpu_analysis.get('recommendations'):\n",
    "            recommendations['low_priority'].extend(\n",
    "                multi_gpu_analysis['recommendations']\n",
    "            )\n",
    "        \n",
    "        # 實施順序建議\n",
    "        recommendations['implementation_order'] = [\n",
    "            \"1. 啟用和優化動態批次處理\",\n",
    "            \"2. 優化 GPU 記憶體配置\",\n",
    "            \"3. 調整模型實例數量\",\n",
    "            \"4. 配置 Multi-GPU 負載均衡\",\n",
    "            \"5. 實施性能監控和告警\"\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _project_improvements(self, results: Dict) -> Dict:\n",
    "        \"\"\"預測優化改進效果\"\"\"\n",
    "        improvements = {\n",
    "            'latency_improvement': {},\n",
    "            'throughput_improvement': {},\n",
    "            'memory_efficiency': {},\n",
    "            'overall_score': {}\n",
    "        }\n",
    "        \n",
    "        for model_name, baseline in results.get('baseline_performance', {}).items():\n",
    "            if 'error' in baseline:\n",
    "                continue\n",
    "            \n",
    "            # 基於瓶頸分析預測改進\n",
    "            bottlenecks = results.get('bottleneck_analysis', {}).get(model_name, {}).get('bottlenecks', [])\n",
    "            \n",
    "            latency_improvement = 1.0  # 無改進基線\n",
    "            throughput_improvement = 1.0\n",
    "            \n",
    "            # 動態批次改進預測\n",
    "            if any('動態批次' in b for b in bottlenecks):\n",
    "                latency_improvement *= 0.8  # 20% 延遲改進\n",
    "                throughput_improvement *= 3.0  # 3倍吞吐量改進\n",
    "            \n",
    "            # 實例數量改進預測\n",
    "            if any('實例' in b for b in bottlenecks):\n",
    "                throughput_improvement *= 2.0  # 2倍吞吐量改進\n",
    "            \n",
    "            # GPU 遷移改進預測\n",
    "            if any('CPU' in b for b in bottlenecks):\n",
    "                latency_improvement *= 0.3  # 70% 延遲改進\n",
    "                throughput_improvement *= 5.0  # 5倍吞吐量改進\n",
    "            \n",
    "            improvements['latency_improvement'][model_name] = {\n",
    "                'current_ms': baseline.get('avg_latency_ms', 0),\n",
    "                'projected_ms': baseline.get('avg_latency_ms', 0) * latency_improvement,\n",
    "                'improvement_percent': (1 - latency_improvement) * 100\n",
    "            }\n",
    "            \n",
    "            improvements['throughput_improvement'][model_name] = {\n",
    "                'current_rps': baseline.get('throughput_rps', 0),\n",
    "                'projected_rps': baseline.get('throughput_rps', 0) * throughput_improvement,\n",
    "                'improvement_percent': (throughput_improvement - 1) * 100\n",
    "            }\n",
    "            \n",
    "            # 整體評分\n",
    "            score = (throughput_improvement + (2 - latency_improvement)) / 2\n",
    "            improvements['overall_score'][model_name] = min(score * 20, 100)  # 最高100分\n",
    "        \n",
    "        return improvements\n",
    "    \n",
    "    def generate_optimization_report(self, results: Dict) -> str:\n",
    "        \"\"\"生成詳細的優化報告\"\"\"\n",
    "        report = f\"\"\"\n",
    "# 🚀 綜合性能優化報告\n",
    "\n",
    "## 📊 分析概述\n",
    "- 分析模型數量: {len(results['models_analyzed'])}\n",
    "- 分析時間: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- 模型清單: {', '.join(results['models_analyzed'])}\n",
    "\n",
    "## 📈 基線性能\n",
    "\"\"\"\n",
    "        \n",
    "        for model_name, baseline in results.get('baseline_performance', {}).items():\n",
    "            if 'error' not in baseline:\n",
    "                report += f\"\"\"\n",
    "### {model_name}\n",
    "- 平均延遲: {baseline.get('avg_latency_ms', 'N/A'):.2f} ms\n",
    "- P95 延遲: {baseline.get('p95_latency_ms', 'N/A'):.2f} ms\n",
    "- 吞吐量: {baseline.get('throughput_rps', 'N/A'):.2f} RPS\n",
    "\"\"\"\n",
    "        \n",
    "        report += \"\\n## 🔍 瓶頸分析\\n\"\n",
    "        for model_name, analysis in results.get('bottleneck_analysis', {}).items():\n",
    "            if analysis.get('bottlenecks'):\n",
    "                report += f\"\\n### {model_name}\\n\"\n",
    "                for bottleneck in analysis['bottlenecks']:\n",
    "                    report += f\"- ⚠️  {bottleneck}\\n\"\n",
    "        \n",
    "        report += \"\\n## 💡 優化建議\\n\"\n",
    "        recommendations = results.get('optimization_recommendations', {})\n",
    "        \n",
    "        if recommendations.get('high_priority'):\n",
    "            report += \"\\n### 🔴 高優先級\\n\"\n",
    "            for rec in recommendations['high_priority']:\n",
    "                report += f\"- {rec}\\n\"\n",
    "        \n",
    "        if recommendations.get('medium_priority'):\n",
    "            report += \"\\n### 🟡 中優先級\\n\"\n",
    "            for rec in recommendations['medium_priority']:\n",
    "                report += f\"- {rec}\\n\"\n",
    "        \n",
    "        if recommendations.get('implementation_order'):\n",
    "            report += \"\\n### 📋 實施順序\\n\"\n",
    "            for order in recommendations['implementation_order']:\n",
    "                report += f\"{order}\\n\"\n",
    "        \n",
    "        report += \"\\n## 📊 預期改進效果\\n\"\n",
    "        improvements = results.get('projected_improvements', {})\n",
    "        \n",
    "        for model_name in results['models_analyzed']:\n",
    "            if model_name in improvements.get('latency_improvement', {}):\n",
    "                lat_imp = improvements['latency_improvement'][model_name]\n",
    "                thr_imp = improvements['throughput_improvement'][model_name]\n",
    "                score = improvements['overall_score'].get(model_name, 0)\n",
    "                \n",
    "                report += f\"\"\"\n",
    "### {model_name}\n",
    "- 延遲改進: {lat_imp['improvement_percent']:.1f}% ({lat_imp['current_ms']:.2f}ms → {lat_imp['projected_ms']:.2f}ms)\n",
    "- 吞吐量改進: {thr_imp['improvement_percent']:.1f}% ({thr_imp['current_rps']:.2f} → {thr_imp['projected_rps']:.2f} RPS)\n",
    "- 整體評分: {score:.1f}/100\n",
    "\"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# 初始化綜合優化器\n",
    "comprehensive_optimizer = ComprehensiveOptimizer()\n",
    "print(\"🎯 綜合性能優化器已初始化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 實戰演示：完整優化流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置要分析的模型\n",
    "models_to_analyze = [\n",
    "    \"text_classification\",  # 替換為您的實際模型名稱\n",
    "    \"sentiment_analysis\",   # 替換為您的實際模型名稱\n",
    "]\n",
    "\n",
    "print(\"🚀 開始完整的性能優化流程演示\")\n",
    "print(f\"將分析以下模型: {', '.join(models_to_analyze)}\")\n",
    "print(\"\\n注意: 請確保這些模型已在 Triton 服務器中加載\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查模型可用性\n",
    "available_models = []\n",
    "\n",
    "try:\n",
    "    client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "    server_metadata = client.get_server_metadata()\n",
    "    print(\"🔍 檢查可用模型...\")\n",
    "    \n",
    "    for model_name in models_to_analyze:\n",
    "        try:\n",
    "            model_metadata = client.get_model_metadata(model_name)\n",
    "            available_models.append(model_name)\n",
    "            print(f\"  ✅ {model_name} - 可用\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {model_name} - 不可用: {e}\")\n",
    "    \n",
    "    if not available_models:\n",
    "        print(\"\\n⚠️  沒有可用的模型進行分析\")\n",
    "        print(\"請確保 Triton 服務器正在運行並且已加載模型\")\n",
    "    else:\n",
    "        print(f\"\\n✅ 找到 {len(available_models)} 個可用模型\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 無法連接到 Triton 服務器: {e}\")\n",
    "    print(\"請確保 Triton 服務器在 localhost:8000 上運行\")\n",
    "    available_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行綜合優化分析（如果有可用模型）\n",
    "if available_models:\n",
    "    print(\"🎯 開始綜合優化分析...\")\n",
    "    \n",
    "    # 這是一個異步函數，在 Jupyter 中需要使用 await\n",
    "    optimization_results = await comprehensive_optimizer.run_comprehensive_optimization(\n",
    "        model_names=available_models,\n",
    "        optimization_duration=60  # 縮短分析時間以便演示\n",
    "    )\n",
    "    \n",
    "    # 生成詳細報告\n",
    "    detailed_report = comprehensive_optimizer.generate_optimization_report(optimization_results)\n",
    "    print(detailed_report)\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  跳過綜合分析 - 沒有可用模型\")\n",
    "    \n",
    "    # 演示其他功能\n",
    "    print(\"\\n🔧 演示其他優化功能...\")\n",
    "    \n",
    "    # GPU 記憶體信息\n",
    "    gpu_info = gpu_optimizer.get_gpu_memory_info()\n",
    "    print(\"\\n📊 GPU 記憶體狀態:\")\n",
    "    if 'error' not in gpu_info:\n",
    "        for gpu_id, info in gpu_info.items():\n",
    "            print(f\"  {gpu_id}: {info['utilization_percent']:.1f}% 使用率\")\n",
    "    else:\n",
    "        print(f\"  {gpu_info}\")\n",
    "    \n",
    "    # 記憶體優化配置\n",
    "    memory_config = gpu_optimizer.generate_memory_optimization_config()\n",
    "    print(\"\\n⚙️  記憶體優化配置已生成\")\n",
    "    \n",
    "    # Multi-GPU 拓撲\n",
    "    topology = multi_gpu_manager.analyze_gpu_topology()\n",
    "    if 'error' not in topology:\n",
    "        print(f\"\\n🔄 檢測到 {topology['device_count']} 個 GPU\")\n",
    "    else:\n",
    "        print(f\"\\n🔄 GPU 拓撲: {topology['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能監控儀表板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 啟動實時性能監控（演示版本）\n",
    "if available_models:\n",
    "    print(\"📊 啟動實時性能監控...\")\n",
    "    \n",
    "    # 啟動後台監控\n",
    "    monitoring_task = asyncio.create_task(\n",
    "        monitor.monitor_performance(duration=30, interval=1.0)\n",
    "    )\n",
    "    \n",
    "    # 等待監控完成\n",
    "    await monitoring_task\n",
    "    \n",
    "    # 顯示監控結果\n",
    "    if monitor.metrics_history:\n",
    "        print(f\"\\n📈 收集了 {len(monitor.metrics_history)} 個監控數據點\")\n",
    "        monitor.plot_metrics()\n",
    "    else:\n",
    "        print(\"\\n⚠️  沒有收集到監控數據\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  跳過實時監控 - 沒有可用模型\")\n",
    "    \n",
    "    # 演示靜態 GPU 監控\n",
    "    print(\"\\n📊 演示靜態 GPU 監控...\")\n",
    "    \n",
    "    # 模擬監控數據\n",
    "    import random\n",
    "    \n",
    "    mock_history = []\n",
    "    for i in range(30):\n",
    "        mock_history.append({\n",
    "            'timestamp': time.time() + i,\n",
    "            'gpu_id': 'gpu_0',\n",
    "            'used_memory_gb': 4.5 + random.uniform(-0.5, 0.5),\n",
    "            'utilization_percent': 60 + random.uniform(-20, 20)\n",
    "        })\n",
    "    \n",
    "    gpu_optimizer.plot_memory_usage(mock_history)\n",
    "    print(\"上圖顯示了模擬的 GPU 記憶體使用情況\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 優化配置生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimized_model_config(model_name: str, optimization_level: str = \"balanced\") -> Dict:\n",
    "    \"\"\"生成優化的模型配置\"\"\"\n",
    "    \n",
    "    base_config = {\n",
    "        \"name\": model_name,\n",
    "        \"platform\": \"tensorrt_plan\",  # 或其他適當的平台\n",
    "        \"max_batch_size\": 32,\n",
    "        \"input\": [],\n",
    "        \"output\": [],\n",
    "        \"dynamic_batching\": {\n",
    "            \"preferred_batch_size\": [1, 2, 4, 8, 16],\n",
    "            \"max_queue_delay_microseconds\": 1000\n",
    "        },\n",
    "        \"instance_group\": [\n",
    "            {\n",
    "                \"count\": 1,\n",
    "                \"kind\": \"KIND_GPU\",\n",
    "                \"gpus\": [0]\n",
    "            }\n",
    "        ],\n",
    "        \"optimization\": {\n",
    "            \"graph\": {\n",
    "                \"level\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 根據優化級別調整配置\n",
    "    if optimization_level == \"latency\":\n",
    "        # 優化延遲\n",
    "        base_config[\"max_batch_size\"] = 4\n",
    "        base_config[\"dynamic_batching\"][\"preferred_batch_size\"] = [1, 2]\n",
    "        base_config[\"dynamic_batching\"][\"max_queue_delay_microseconds\"] = 100\n",
    "        base_config[\"instance_group\"][0][\"count\"] = 2\n",
    "        \n",
    "    elif optimization_level == \"throughput\":\n",
    "        # 優化吞吐量\n",
    "        base_config[\"max_batch_size\"] = 64\n",
    "        base_config[\"dynamic_batching\"][\"preferred_batch_size\"] = [8, 16, 32, 64]\n",
    "        base_config[\"dynamic_batching\"][\"max_queue_delay_microseconds\"] = 5000\n",
    "        base_config[\"instance_group\"][0][\"count\"] = 1\n",
    "        \n",
    "    elif optimization_level == \"memory\":\n",
    "        # 優化記憶體使用\n",
    "        base_config[\"max_batch_size\"] = 16\n",
    "        base_config[\"dynamic_batching\"][\"preferred_batch_size\"] = [1, 2, 4, 8]\n",
    "        base_config[\"optimization\"][\"memory_pool_byte_size\"] = 1073741824  # 1GB\n",
    "        \n",
    "    return base_config\n",
    "\n",
    "# 生成不同優化級別的配置示例\n",
    "print(\"⚙️  生成優化配置示例...\")\n",
    "\n",
    "optimization_levels = [\"latency\", \"throughput\", \"memory\", \"balanced\"]\n",
    "\n",
    "for level in optimization_levels:\n",
    "    config = generate_optimized_model_config(\"example_model\", level)\n",
    "    print(f\"\\n📋 {level.upper()} 優化配置:\")\n",
    "    print(f\"  最大批次大小: {config['max_batch_size']}\")\n",
    "    print(f\"  首選批次大小: {config['dynamic_batching']['preferred_batch_size']}\")\n",
    "    print(f\"  最大隊列延遲: {config['dynamic_batching']['max_queue_delay_microseconds']} μs\")\n",
    "    print(f\"  實例數量: {config['instance_group'][0]['count']}\")\n",
    "\n",
    "print(\"\\n💾 配置文件已生成，可以保存為 config.pbtxt 並應用到 Triton 服務器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 總結和最佳實踐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 性能優化總結\n",
    "\n",
    "通過本實驗室，我們學習了 Triton Inference Server 的極致性能優化技術：\n",
    "\n",
    "#### ✅ 完成的優化項目\n",
    "\n",
    "1. **瓶頸分析和診斷**\n",
    "   - 自動化模型性能瓶頸識別\n",
    "   - 批次大小和並發級別優化\n",
    "   - 配置問題檢測和建議\n",
    "\n",
    "2. **GPU 記憶體管理**\n",
    "   - 實時記憶體使用監控\n",
    "   - 記憶體使用模式分析\n",
    "   - 自動化記憶體配置生成\n",
    "\n",
    "3. **Multi-GPU 配置**\n",
    "   - GPU 拓撲結構分析\n",
    "   - 智能負載均衡配置\n",
    "   - 跨 GPU 性能監控\n",
    "\n",
    "4. **綜合性能優化**\n",
    "   - 端到端優化流程\n",
    "   - 改進效果預測\n",
    "   - 詳細優化報告生成\n",
    "\n",
    "#### 🏆 最佳實踐\n",
    "\n",
    "1. **配置優化**\n",
    "   - 啟用動態批次處理\n",
    "   - 合理設置首選批次大小\n",
    "   - 根據硬件調整實例數量\n",
    "\n",
    "2. **記憶體管理**\n",
    "   - 監控 GPU 記憶體使用率\n",
    "   - 設置合理的記憶體池大小\n",
    "   - 避免記憶體碎片化\n",
    "\n",
    "3. **多 GPU 部署**\n",
    "   - 合理分配模型到不同 GPU\n",
    "   - 實施負載均衡和故障轉移\n",
    "   - 監控 GPU 間負載分佈\n",
    "\n",
    "4. **持續優化**\n",
    "   - 建立性能監控體系\n",
    "   - 定期執行性能評估\n",
    "   - 根據業務需求調整配置\n",
    "\n",
    "#### 🚀 下一步建議\n",
    "\n",
    "1. **生產環境部署**\n",
    "   - 應用學到的優化配置\n",
    "   - 建立性能監控告警\n",
    "   - 實施自動化優化流程\n",
    "\n",
    "2. **高級功能探索**\n",
    "   - 模型 Ensemble 和 Pipeline\n",
    "   - 自定義後端開發\n",
    "   - 分散式推理架構\n",
    "\n",
    "3. **持續學習**\n",
    "   - 關注 Triton 新功能發布\n",
    "   - 參與社區討論和貢獻\n",
    "   - 探索新的優化技術\n",
    "\n",
    "### 📚 相關資源\n",
    "\n",
    "- [Triton Performance Guide](https://github.com/triton-inference-server/tutorials)\n",
    "- [GPU Memory Optimization](https://developer.nvidia.com/blog/cuda-memory-optimization/)\n",
    "- [Multi-GPU Best Practices](https://docs.nvidia.com/deeplearning/frameworks/user-guide/)\n",
    "\n",
    "🎉 **恭喜！您已經掌握了 Triton Inference Server 的極致性能優化技術！**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}