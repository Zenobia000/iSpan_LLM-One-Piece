{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab-1.7: SFT (Supervised Fine-Tuning) 基線模型訓練\n",
    "\n",
    "**實驗目標**: 訓練監督式微調基線模型，作為 DPO 的參考模型\n",
    "\n",
    "在進行 DPO 對齊之前，我們需要先訓練一個 SFT 基線模型。這個基線模型將：\n",
    "1. **作為 DPO 的起始點**: DPO 需要在 SFT 模型基礎上進行偏好優化\n",
    "2. **提供性能對比基準**: 評估 DPO 對齊效果\n",
    "3. **確保基本對話能力**: 在偏好優化前具備基本的指令遵循能力\n",
    "\n",
    "## SFT vs DPO\n",
    "\n",
    "- **SFT**: 使用監督學習從指令-回應對中學習\n",
    "- **DPO**: 使用偏好對比學習來優化模型行為\n",
    "- **流程**: 預訓練模型 → SFT → DPO → 部署\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 步驟 1: 環境載入與配置\n",
    "\n",
    "重新載入之前準備的環境和數據，並配置 PEFT 進行高效微調。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_from_disk, Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 設置隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('🚀 開始 SFT 基線模型訓練')\n",
    "print(f'GPU 可用: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'當前 GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU 記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 步驟 2: 載入模型與 Tokenizer\n",
    "\n",
    "使用與之前相同的模型配置，並準備 PEFT 配置進行高效微調。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型配置 (與第一個 notebook 保持一致)\n",
    "MODEL_NAME = 'microsoft/DialoGPT-medium'  # 或 'gpt2'\n",
    "\n",
    "print(f'📦 載入模型: {MODEL_NAME}')\n",
    "\n",
    "# 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 載入模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f'✅ 模型載入成功')\n",
    "print(f'模型參數量: {model.num_parameters():,}')\n",
    "print(f'詞彙大小: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 步驟 3: PEFT 配置\n",
    "\n",
    "使用 LoRA 進行參數高效微調，減少訓練時間和記憶體需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置 LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                        # rank\n",
    "    lora_alpha=32,              # scaling parameter\n",
    "    target_modules=['c_attn', 'c_proj', 'c_fc'],  # 針對 GPT-2 架構\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "# 準備模型進行 PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 顯示可訓練參數\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print('✅ PEFT 配置完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 步驟 4: 準備 SFT 訓練數據\n",
    "\n",
    "從之前的偏好數據中提取指令-回應對，用於監督式微調。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入之前處理的數據\n",
    "try:\n",
    "    dpo_dataset = load_from_disk('./dpo_data')\n",
    "    print(f'✅ 載入 DPO 數據集，樣本數: {len(dpo_dataset)}')\n",
    "except:\n",
    "    print('⚠️  無法載入已保存的數據，使用模擬數據')\n",
    "    # 創建模擬訓練數據\n",
    "    mock_data = [\n",
    "        {\n",
    "            'prompt': '請解釋什麼是機器學習?',\n",
    "            'chosen': '機器學習是人工智能的一個分支，它讓計算機能夠從數據中學習並做出決策，而無需明確編程。通過算法分析大量數據，系統可以識別模式並提高性能。',\n",
    "            'rejected': '機器學習就是讓機器變聰明。'\n",
    "        },\n",
    "        {\n",
    "            'prompt': '如何學習程式設計?',\n",
    "            'chosen': '學習程式設計建議從基礎語法開始，選擇一門適合的語言如Python，多做練習項目，參與開源專案，並持續學習新技術。實作是最重要的學習方式。',\n",
    "            'rejected': '學程式就是寫code。'\n",
    "        }\n",
    "    ]\n",
    "    dpo_dataset = Dataset.from_list(mock_data)\n",
    "\n",
    "print(f'數據集大小: {len(dpo_dataset)}')\n",
    "print(f'數據欄位: {list(dpo_dataset.features.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sft_dataset(dataset, tokenizer, max_length=512):\n",
    "    \"\"\"將偏好數據轉換為 SFT 訓練格式\"\"\"\n",
    "    sft_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # 使用 chosen 回應作為訓練目標\n",
    "        conversation = f\"Human: {sample['prompt']}\\n\\nAssistant: {sample['chosen']}\"\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = tokenizer(\n",
    "            conversation,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        sft_data.append({\n",
    "            'input_ids': tokens['input_ids'],\n",
    "            'attention_mask': tokens['attention_mask'],\n",
    "            'labels': tokens['input_ids'].copy()  # 對於語言模型，labels 等於 input_ids\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(sft_data)\n",
    "\n",
    "# 創建 SFT 訓練數據集\n",
    "print('🔄 轉換數據為 SFT 格式...')\n",
    "sft_dataset = create_sft_dataset(dpo_dataset, tokenizer)\n",
    "\n",
    "print(f'✅ SFT 數據集準備完成，樣本數: {len(sft_dataset)}')\n",
    "\n",
    "# 顯示示例\n",
    "sample = sft_dataset[0]\n",
    "decoded_text = tokenizer.decode(sample['input_ids'])\n",
    "print(f'\\n📝 訓練樣本示例:')\n",
    "print(decoded_text[:200] + '...' if len(decoded_text) > 200 else decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 步驟 5: 訓練配置\n",
    "\n",
    "設置訓練參數，使用適合單 GPU 環境的配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練配置\n",
    "output_dir = './sft_model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # 基本設置\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    \n",
    "    # 優化設置\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    \n",
    "    # 記憶體優化\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # 其他設置\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # 不使用 wandb\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print('⚙️  訓練配置:')\n",
    "print(f'輸出目錄: {output_dir}')\n",
    "print(f'訓練輪數: {training_args.num_train_epochs}')\n",
    "print(f'批次大小: {training_args.per_device_train_batch_size}')\n",
    "print(f'學習率: {training_args.learning_rate}')\n",
    "print(f'使用 FP16: {training_args.fp16}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 步驟 6: 開始 SFT 訓練\n",
    "\n",
    "使用 Hugging Face Trainer 進行監督式微調。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數據整理器\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # 不使用 masked language modeling\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# 創建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=sft_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print('🚀 開始 SFT 訓練...')\n",
    "\n",
    "# 訓練前的記憶體狀態\n",
    "if torch.cuda.is_available():\n",
    "    print(f'訓練前 GPU 記憶體: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "# 開始訓練\n",
    "try:\n",
    "    trainer.train()\n",
    "    print('✅ SFT 訓練完成！')\n",
    "    \n",
    "    # 訓練後的記憶體狀態\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'訓練後 GPU 記憶體: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'❌ 訓練過程中出錯: {e}')\n",
    "    print('嘗試減少批次大小或序列長度')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 步驟 7: 保存模型與評估\n",
    "\n",
    "保存訓練好的 SFT 模型，並進行簡單的生成測試。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "print('💾 保存 SFT 模型...')\n",
    "\n",
    "# 保存 PEFT 模型\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f'✅ 模型已保存至: {output_dir}')\n",
    "\n",
    "# 列出保存的檔案\n",
    "saved_files = list(Path(output_dir).glob('*'))\n",
    "print(f'保存的檔案: {[f.name for f in saved_files]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試生成\n",
    "def test_generation(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"測試模型生成能力\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 準備輸入\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # 生成回應\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解碼輸出\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 測試生成\n",
    "print('🧪 測試 SFT 模型生成...')\n",
    "\n",
    "test_prompts = [\n",
    "    \"Human: 什麼是深度學習?\\n\\nAssistant:\",\n",
    "    \"Human: 如何提高程式設計技能?\\n\\nAssistant:\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f'\\n📝 測試 {i+1}:')\n",
    "    print(f'Prompt: {prompt.split(\"Assistant:\")[0]}Assistant:')\n",
    "    \n",
    "    try:\n",
    "        response = test_generation(model, tokenizer, prompt)\n",
    "        print(f'Generated: {response}')\n",
    "    except Exception as e:\n",
    "        print(f'生成失敗: {e}')\n",
    "    \n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 步驟 8: 訓練總結\n",
    "\n",
    "總結 SFT 基線模型的訓練結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練總結\n",
    "print('=== SFT 基線模型訓練總結 ===')\n",
    "print(f'✅ 基礎模型: {MODEL_NAME}')\n",
    "print(f'✅ PEFT 方法: LoRA (r={lora_config.r}, alpha={lora_config.lora_alpha})')\n",
    "print(f'✅ 訓練數據: {len(sft_dataset)} 個樣本')\n",
    "print(f'✅ 訓練輪數: {training_args.num_train_epochs}')\n",
    "print(f'✅ 模型保存位置: {output_dir}')\n",
    "\n",
    "# 顯示訓練參數統計\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'✅ 可訓練參數: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)')\n",
    "\n",
    "print('\\n🎯 SFT 基線模型已準備完成！')\n",
    "print('下一步: 執行 03-DPO_Training.ipynb 進行偏好優化')\n",
    "\n",
    "# 清理 GPU 記憶體\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'GPU 記憶體已清理，當前使用: {torch.cuda.memory_allocated() / 1e9:.2f} GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}