{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.4: 記憶體分析與優化 (Memory Profiling)\n",
    "\n",
    "**學習目標**:\n",
    "- 掌握 PyTorch 記憶體分析工具\n",
    "- 識別訓練過程中的記憶體瓶頸\n",
    "- 使用 Profiler 進行性能分析\n",
    "- 制定記憶體優化策略\n",
    "\n",
    "**預計時間**: 30-45分鐘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. 理論背景\n",
    "\n",
    "### 1.1 GPU 記憶體組成\n",
    "\n",
    "訓練深度學習模型時, GPU 記憶體主要用於:\n",
    "\n",
    "```\n",
    "總記憶體 = 模型參數 + 梯度 + 優化器狀態 + 激活值 + 臨時緩存\n",
    "```\n",
    "\n",
    "**各部分占比 (以 7B 模型 FP32 訓練為例)**:\n",
    "- 模型參數: 28GB (7B × 4 bytes)\n",
    "- 梯度: 28GB (與參數同大小)\n",
    "- 優化器狀態 (Adam): 56GB (momentum + variance)\n",
    "- 激活值: 視批次大小而定 (通常 10-30GB)\n",
    "- **總計**: ~120-140GB\n",
    "\n",
    "### 1.2 記憶體分析目標\n",
    "\n",
    "1. **定位記憶體峰值**: 找出訓練過程中記憶體占用最高的時刻\n",
    "2. **識別記憶體洩漏**: 檢查是否有未釋放的張量\n",
    "3. **優化記憶體分配**: 減少不必要的記憶體開銷\n",
    "4. **預測記憶體需求**: 為生產環境規劃硬體資源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2. 環境設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "使用設備: cuda\n",
      "GPU: NVIDIA RTX 2000 Ada Generation\n",
      "總記憶體: 16.71 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"總記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3. 基礎記憶體監控工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU 記憶體初始狀態\n",
      "============================================================\n",
      "記憶體使用:\n",
      "  已分配 (Allocated): 0.000 GB\n",
      "  已保留 (Reserved):  0.000 GB\n",
      "  峰值分配 (Peak):     0.000 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'allocated': 0.0, 'reserved': 0.0, 'peak': 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_gpu_memory(prefix=\"\"):\n",
    "    \"\"\"打印當前 GPU 記憶體使用情況\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(f\"{prefix}GPU 不可用\")\n",
    "        return\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    print(f\"{prefix}記憶體使用:\")\n",
    "    print(f\"  已分配 (Allocated): {allocated:.3f} GB\")\n",
    "    print(f\"  已保留 (Reserved):  {reserved:.3f} GB\")\n",
    "    print(f\"  峰值分配 (Peak):     {max_allocated:.3f} GB\")\n",
    "    \n",
    "    return {\n",
    "        \"allocated\": allocated,\n",
    "        \"reserved\": reserved,\n",
    "        \"peak\": max_allocated\n",
    "    }\n",
    "\n",
    "\n",
    "def reset_gpu_memory():\n",
    "    \"\"\"重置 GPU 記憶體統計\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU 記憶體初始狀態\")\n",
    "print(\"=\" * 60)\n",
    "reset_gpu_memory()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4. 記憶體追蹤器類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 記憶體分析器初始化完成\n"
     ]
    }
   ],
   "source": [
    "class MemoryProfiler:\n",
    "    \"\"\"記憶體分析器\"\"\"\n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "        self.timeline = []\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"重置分析器\"\"\"\n",
    "        self.snapshots = []\n",
    "        self.timeline = []\n",
    "        reset_gpu_memory()\n",
    "    \n",
    "    def snapshot(self, label=\"\", timestamp=None):\n",
    "        \"\"\"記錄當前記憶體快照\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        \n",
    "        if timestamp is None:\n",
    "            timestamp = time.time()\n",
    "        \n",
    "        snapshot = {\n",
    "            \"label\": label,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9,\n",
    "            \"reserved\": torch.cuda.memory_reserved() / 1e9,\n",
    "            \"peak\": torch.cuda.max_memory_allocated() / 1e9\n",
    "        }\n",
    "        \n",
    "        self.snapshots.append(snapshot)\n",
    "        return snapshot\n",
    "    \n",
    "    def start_timeline(self):\n",
    "        \"\"\"開始記錄記憶體時間線\"\"\"\n",
    "        self.timeline = []\n",
    "        self.timeline_start = time.time()\n",
    "    \n",
    "    def record_timeline(self, label=\"\"):\n",
    "        \"\"\"記錄時間線點\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        \n",
    "        elapsed = time.time() - self.timeline_start\n",
    "        self.timeline.append({\n",
    "            \"time\": elapsed,\n",
    "            \"label\": label,\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9\n",
    "        })\n",
    "    \n",
    "    def plot_snapshots(self):\n",
    "        \"\"\"繪製記憶體快照\"\"\"\n",
    "        if not self.snapshots:\n",
    "            print(\"沒有快照可繪製\")\n",
    "            return\n",
    "        \n",
    "        labels = [s[\"label\"] for s in self.snapshots]\n",
    "        allocated = [s[\"allocated\"] for s in self.snapshots]\n",
    "        reserved = [s[\"reserved\"] for s in self.snapshots]\n",
    "        peak = [s[\"peak\"] for s in self.snapshots]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax.bar(x - width, allocated, width, label=\"已分配\", color=\"#3498db\")\n",
    "        ax.bar(x, reserved, width, label=\"已保留\", color=\"#95a5a6\")\n",
    "        ax.bar(x + width, peak, width, label=\"峰值\", color=\"#e74c3c\")\n",
    "        \n",
    "        ax.set_xlabel(\"階段\")\n",
    "        ax.set_ylabel(\"記憶體 (GB)\")\n",
    "        ax.set_title(\"記憶體使用快照\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_timeline(self):\n",
    "        \"\"\"繪製記憶體時間線\"\"\"\n",
    "        if not self.timeline:\n",
    "            print(\"沒有時間線可繪製\")\n",
    "            return\n",
    "        \n",
    "        times = [t[\"time\"] for t in self.timeline]\n",
    "        memory = [t[\"allocated\"] for t in self.timeline]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(times, memory, linewidth=2, color=\"#3498db\", marker=\"o\")\n",
    "        plt.xlabel(\"時間 (秒)\")\n",
    "        plt.ylabel(\"已分配記憶體 (GB)\")\n",
    "        plt.title(\"記憶體使用時間線\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"打印分析摘要\"\"\"\n",
    "        if not self.snapshots:\n",
    "            print(\"沒有快照數據\")\n",
    "            return\n",
    "        \n",
    "        peak_snapshot = max(self.snapshots, key=lambda s: s[\"allocated\"])\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"記憶體分析摘要\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"總快照數: {len(self.snapshots)}\")\n",
    "        print(f\"\\n峰值記憶體使用:\")\n",
    "        print(f\"  階段: {peak_snapshot['label']}\")\n",
    "        print(f\"  已分配: {peak_snapshot['allocated']:.3f} GB\")\n",
    "        print(f\"  已保留: {peak_snapshot['reserved']:.3f} GB\")\n",
    "        print(f\"  峰值: {peak_snapshot['peak']:.3f} GB\")\n",
    "\n",
    "\n",
    "# 創建全局分析器\n",
    "profiler = MemoryProfiler()\n",
    "print(\"✅ 記憶體分析器初始化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 5. 實驗 1: 模型載入的記憶體分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "實驗 1: 模型載入記憶體分析\n",
      "======================================================================\n",
      "\n",
      "載入 GPT-2 Medium...\n",
      "移動到 GPU...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 23.06 MiB is free. Process 3859131 has 12.57 GiB memory in use. Process 3880458 has 2.10 GiB memory in use. Including non-PyTorch memory, this process has 510.00 MiB memory in use. Of the allocated memory 413.55 MiB is allocated by PyTorch, and 12.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 移到 GPU\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m移動到 GPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m profiler\u001b[38;5;241m.\u001b[39msnapshot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型移到 GPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 計算模型參數量\u001b[39;00m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4343\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   4339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4340\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4342\u001b[0m         )\n\u001b[0;32m-> 4343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 23.06 MiB is free. Process 3859131 has 12.57 GiB memory in use. Process 3880458 has 2.10 GiB memory in use. Including non-PyTorch memory, this process has 510.00 MiB memory in use. Of the allocated memory 413.55 MiB is allocated by PyTorch, and 12.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"實驗 1: 模型載入記憶體分析\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "profiler.reset()\n",
    "profiler.snapshot(\"初始狀態\")\n",
    "\n",
    "# 載入模型\n",
    "print(\"\\n載入 GPT-2 Medium...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "profiler.snapshot(\"模型載入 (CPU)\")\n",
    "\n",
    "# 移到 GPU\n",
    "print(\"移動到 GPU...\")\n",
    "model = model.to(device)\n",
    "profiler.snapshot(\"模型移到 GPU\")\n",
    "\n",
    "# 計算模型參數量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "param_size_gb = total_params * 4 / 1e9  # FP32: 4 bytes per param\n",
    "\n",
    "print(f\"\\n模型統計:\")\n",
    "print(f\"  總參數量: {total_params / 1e6:.1f}M\")\n",
    "print(f\"  可訓練參數: {trainable_params / 1e6:.1f}M\")\n",
    "print(f\"  理論大小 (FP32): {param_size_gb:.3f} GB\")\n",
    "\n",
    "# 顯示記憶體快照\n",
    "profiler.plot_snapshots()\n",
    "profiler.print_summary()\n",
    "\n",
    "# 分析\n",
    "stats = profiler.snapshots[-1]\n",
    "print(f\"\\n分析:\")\n",
    "print(f\"  實際記憶體占用: {stats['allocated']:.3f} GB\")\n",
    "print(f\"  理論參數大小: {param_size_gb:.3f} GB\")\n",
    "print(f\"  開銷: {(stats['allocated'] - param_size_gb) / param_size_gb * 100:.1f}%\")\n",
    "\n",
    "# 清理\n",
    "del model\n",
    "reset_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 6. 實驗 2: 訓練過程記憶體分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備數據\n",
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, num_samples=100, seq_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.texts = [f\"The quick brown fox jumps over the lazy dog. \" * 10 for _ in range(num_samples)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encodings = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = SimpleTextDataset(tokenizer, num_samples=100, seq_length=128)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"實驗 2: 訓練過程記憶體分析\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 重新載入模型\n",
    "profiler.reset()\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "profiler.snapshot(\"訓練前\")\n",
    "\n",
    "# 訓練一個 epoch\n",
    "model.train()\n",
    "profiler.start_timeline()\n",
    "\n",
    "print(\"\\n開始訓練...\")\n",
    "for step, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    # 記錄前向傳播前\n",
    "    if step == 0:\n",
    "        profiler.record_timeline(\"前向前\")\n",
    "    \n",
    "    # 前向傳播\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    if step == 0:\n",
    "        profiler.record_timeline(\"前向後\")\n",
    "    \n",
    "    # 反向傳播\n",
    "    loss.backward()\n",
    "    \n",
    "    if step == 0:\n",
    "        profiler.record_timeline(\"反向後\")\n",
    "    \n",
    "    # 更新參數\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step == 0:\n",
    "        profiler.record_timeline(\"更新後\")\n",
    "        profiler.snapshot(\"第1次迭代完成\")\n",
    "    \n",
    "    # 每10步記錄一次\n",
    "    if step % 10 == 0:\n",
    "        profiler.record_timeline(f\"Step {step}\")\n",
    "\n",
    "profiler.snapshot(\"訓練完成\")\n",
    "\n",
    "# 繪製結果\n",
    "print(\"\\n記憶體快照:\")\n",
    "profiler.plot_snapshots()\n",
    "\n",
    "print(\"\\n記憶體時間線:\")\n",
    "profiler.plot_timeline()\n",
    "\n",
    "profiler.print_summary()\n",
    "\n",
    "# 清理\n",
    "del model, optimizer\n",
    "reset_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 7. 實驗 3: 不同批次大小的記憶體影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"實驗 3: 批次大小對記憶體的影響\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def measure_batch_size_memory(batch_size, num_steps=10):\n",
    "    \"\"\"測量特定批次大小的記憶體使用\"\"\"\n",
    "    reset_gpu_memory()\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        try:\n",
    "            batch = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "            batch = next(dataloader_iter)\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    del model, optimizer\n",
    "    reset_gpu_memory()\n",
    "    \n",
    "    return peak_memory\n",
    "\n",
    "\n",
    "# 測試不同批次大小\n",
    "batch_sizes = [1, 2, 4, 8, 16]\n",
    "memory_usage = []\n",
    "\n",
    "print(\"\\n測試不同批次大小...\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"  測試 batch_size={bs}...\", end=\" \")\n",
    "    try:\n",
    "        mem = measure_batch_size_memory(bs, num_steps=10)\n",
    "        memory_usage.append(mem)\n",
    "        print(f\"峰值記憶體: {mem:.2f} GB\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"OOM (記憶體不足)\")\n",
    "            memory_usage.append(None)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# 繪製結果\n",
    "valid_bs = [bs for bs, mem in zip(batch_sizes, memory_usage) if mem is not None]\n",
    "valid_mem = [mem for mem in memory_usage if mem is not None]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(valid_bs, valid_mem, marker=\"o\", linewidth=2, markersize=10, color=\"#3498db\")\n",
    "plt.xlabel(\"批次大小 (Batch Size)\")\n",
    "plt.ylabel(\"峰值記憶體 (GB)\")\n",
    "plt.title(\"批次大小 vs 記憶體使用\", fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# 添加數值標籤\n",
    "for bs, mem in zip(valid_bs, valid_mem):\n",
    "    plt.text(bs, mem, f\"{mem:.2f}GB\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 分析\n",
    "print(\"\\n=\" * 70)\n",
    "print(\"批次大小記憶體分析\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'批次大小':<15} {'峰值記憶體 (GB)':<20} {'相對於 BS=1':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for bs, mem in zip(batch_sizes, memory_usage):\n",
    "    if mem is not None:\n",
    "        relative = f\"{mem / memory_usage[0]:.2f}x\" if memory_usage[0] else \"N/A\"\n",
    "        print(f\"{bs:<15} {mem:<20.2f} {relative:<20}\")\n",
    "    else:\n",
    "        print(f\"{bs:<15} {'OOM':<20} {'N/A':<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 8. 實驗 4: PyTorch Profiler 詳細分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"實驗 4: PyTorch Profiler 詳細性能分析\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "reset_gpu_memory()\n",
    "\n",
    "# 重新載入模型\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(\"\\n使用 PyTorch Profiler 分析訓練...\")\n",
    "print(\"(這可能需要幾分鐘)\\n\")\n",
    "\n",
    "model.train()\n",
    "dataloader_iter = iter(dataloader)\n",
    "\n",
    "# 使用 Profiler\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=False\n",
    ") as prof:\n",
    "    for step in range(5):  # 只分析 5 步\n",
    "        with record_function(f\"training_step_{step}\"):\n",
    "            batch = next(dataloader_iter)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with record_function(\"forward\"):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            with record_function(\"backward\"):\n",
    "                loss.backward()\n",
    "            \n",
    "            with record_function(\"optimizer_step\"):\n",
    "                optimizer.step()\n",
    "\n",
    "# 打印 Profiler 結果\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Profiler 報告 (按 CUDA 時間排序, Top 10)\")\n",
    "print(\"=\" * 70)\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"cuda_time_total\",\n",
    "    row_limit=10\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Profiler 報告 (按記憶體使用排序, Top 10)\")\n",
    "print(\"=\" * 70)\n",
    "print(prof.key_averages().table(\n",
    "    sort_by=\"self_cuda_memory_usage\",\n",
    "    row_limit=10\n",
    "))\n",
    "\n",
    "# 分析函數級別的性能\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"函數級別分析\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "events = prof.key_averages()\n",
    "for evt in events:\n",
    "    if evt.key in [\"forward\", \"backward\", \"optimizer_step\"]:\n",
    "        print(f\"\\n{evt.key}:\")\n",
    "        print(f\"  CPU 時間: {evt.cpu_time_total / 1e3:.2f} ms\")\n",
    "        print(f\"  CUDA 時間: {evt.cuda_time_total / 1e3:.2f} ms\")\n",
    "        print(f\"  記憶體: {evt.cpu_memory_usage / 1e6:.2f} MB\")\n",
    "\n",
    "# 清理\n",
    "del model, optimizer\n",
    "reset_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 9. 實驗 5: 優化技術對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"實驗 5: 優化技術記憶體對比\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def measure_optimization(config_name, use_amp=False, use_checkpoint=False, accumulation_steps=1):\n",
    "    \"\"\"測量不同優化配置的記憶體使用\"\"\"\n",
    "    reset_gpu_memory()\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(device)\n",
    "    if use_checkpoint:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # 訓練幾步\n",
    "    for step in range(10):\n",
    "        if step % accumulation_steps == 0:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        batch = next(dataloader_iter)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        if use_amp:\n",
    "            with autocast(dtype=torch.float16):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "        else:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    del model, optimizer\n",
    "    reset_gpu_memory()\n",
    "    \n",
    "    return peak_memory\n",
    "\n",
    "\n",
    "# 測試不同配置\n",
    "configs = [\n",
    "    (\"基準 (無優化)\", False, False, 1),\n",
    "    (\"混合精度 (FP16)\", True, False, 1),\n",
    "    (\"梯度檢查點\", False, True, 1),\n",
    "    (\"梯度累積 (x4)\", False, False, 4),\n",
    "    (\"FP16 + 檢查點\", True, True, 1),\n",
    "    (\"全部優化\", True, True, 4)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\n測試不同優化配置...\")\n",
    "for config_name, use_amp, use_checkpoint, accum in configs:\n",
    "    print(f\"  {config_name}...\", end=\" \")\n",
    "    try:\n",
    "        mem = measure_optimization(config_name, use_amp, use_checkpoint, accum)\n",
    "        results.append((config_name, mem))\n",
    "        print(f\"峰值: {mem:.2f} GB\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"OOM\")\n",
    "            results.append((config_name, None))\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# 繪製結果\n",
    "valid_results = [(name, mem) for name, mem in results if mem is not None]\n",
    "names = [name for name, _ in valid_results]\n",
    "memories = [mem for _, mem in valid_results]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#f39c12\", \"#9b59b6\", \"#1abc9c\"]\n",
    "bars = plt.bar(range(len(names)), memories, color=colors[:len(names)])\n",
    "plt.xticks(range(len(names)), names, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"峰值記憶體 (GB)\")\n",
    "plt.title(\"不同優化技術的記憶體使用對比\", fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 添加數值標籤\n",
    "for bar, mem in zip(bars, memories):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{mem:.2f}GB',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 分析\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"優化技術記憶體節省分析\")\n",
    "print(\"=\" * 80)\n",
    "baseline_mem = results[0][1]\n",
    "print(f\"\\n{'配置':<25} {'峰值記憶體 (GB)':<20} {'節省比例':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for name, mem in results:\n",
    "    if mem is not None:\n",
    "        saving = (baseline_mem - mem) / baseline_mem * 100 if baseline_mem else 0\n",
    "        print(f\"{name:<25} {mem:<20.2f} {saving:>12.1f}%\")\n",
    "    else:\n",
    "        print(f\"{name:<25} {'OOM':<20} {'N/A':<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 10. 記憶體優化建議生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimization_suggestions(model_params_gb, available_gpu_gb, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    根據硬體配置生成記憶體優化建議\n",
    "    \n",
    "    Args:\n",
    "        model_params_gb: 模型參數大小 (GB)\n",
    "        available_gpu_gb: 可用 GPU 記憶體 (GB)\n",
    "        batch_size: 期望的批次大小\n",
    "        seq_length: 序列長度\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"記憶體優化建議生成器\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n配置信息:\")\n",
    "    print(f\"  模型大小: {model_params_gb:.2f} GB\")\n",
    "    print(f\"  可用記憶體: {available_gpu_gb:.2f} GB\")\n",
    "    print(f\"  目標批次: {batch_size}\")\n",
    "    print(f\"  序列長度: {seq_length}\")\n",
    "    \n",
    "    # 估算記憶體需求\n",
    "    # 模型參數 + 梯度 + 優化器狀態 (Adam: 2x params) + 激活值\n",
    "    estimated_base = model_params_gb * 4  # params + grads + optimizer\n",
    "    activation_estimate = batch_size * seq_length * 0.001  # 粗略估計\n",
    "    total_estimate = estimated_base + activation_estimate\n",
    "    \n",
    "    print(f\"\\n估算記憶體需求:\")\n",
    "    print(f\"  模型 + 梯度 + 優化器: {estimated_base:.2f} GB\")\n",
    "    print(f\"  激活值 (估計): {activation_estimate:.2f} GB\")\n",
    "    print(f\"  總計: {total_estimate:.2f} GB\")\n",
    "    \n",
    "    # 生成建議\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"優化建議\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    if total_estimate > available_gpu_gb:\n",
    "        shortage = total_estimate - available_gpu_gb\n",
    "        print(f\"\\n⚠️  記憶體不足: 缺少 {shortage:.2f} GB\")\n",
    "        print(\"\\n必要優化 (按優先級):\")\n",
    "        \n",
    "        # 1. 混合精度\n",
    "        print(\"\\n1. ✅ 啟用混合精度訓練 (FP16/BF16)\")\n",
    "        print(\"   - 記憶體節省: ~50%\")\n",
    "        print(\"   - 速度提升: 2-3x\")\n",
    "        print(\"   - 代碼: model.to(device); use autocast()\")\n",
    "        suggestions.append(\"mixed_precision\")\n",
    "        \n",
    "        estimated_base *= 0.5\n",
    "        total_estimate = estimated_base + activation_estimate\n",
    "        \n",
    "        if total_estimate > available_gpu_gb:\n",
    "            # 2. 梯度檢查點\n",
    "            print(\"\\n2. ✅ 啟用梯度檢查點\")\n",
    "            print(\"   - 記憶體節省: ~40%\")\n",
    "            print(\"   - 時間開銷: +20-30%\")\n",
    "            print(\"   - 代碼: model.gradient_checkpointing_enable()\")\n",
    "            suggestions.append(\"gradient_checkpointing\")\n",
    "            \n",
    "            activation_estimate *= 0.6\n",
    "            total_estimate = estimated_base + activation_estimate\n",
    "        \n",
    "        if total_estimate > available_gpu_gb:\n",
    "            # 3. 減小批次 + 梯度累積\n",
    "            suggested_micro_batch = max(1, batch_size // 4)\n",
    "            accumulation = batch_size // suggested_micro_batch\n",
    "            print(f\"\\n3. ✅ 減小批次大小 + 梯度累積\")\n",
    "            print(f\"   - 建議: micro_batch={suggested_micro_batch}, accumulation={accumulation}\")\n",
    "            print(f\"   - 有效批次: {suggested_micro_batch * accumulation}\")\n",
    "            print(f\"   - 記憶體節省: ~{(1 - suggested_micro_batch/batch_size)*100:.0f}%\")\n",
    "            suggestions.append(f\"gradient_accumulation_{accumulation}\")\n",
    "        \n",
    "        if total_estimate > available_gpu_gb:\n",
    "            # 4. 考慮其他方案\n",
    "            print(\"\\n4. 💡 考慮其他方案:\")\n",
    "            print(\"   - DeepSpeed ZeRO (多GPU/CPU offload)\")\n",
    "            print(\"   - 模型量化 (INT8/INT4)\")\n",
    "            print(\"   - PEFT 技術 (LoRA/QLoRA)\")\n",
    "    else:\n",
    "        margin = available_gpu_gb - total_estimate\n",
    "        print(f\"\\n✅ 記憶體充足: 剩餘 {margin:.2f} GB\")\n",
    "        print(\"\\n可選優化 (提升速度):\")\n",
    "        print(\"\\n1. ⚡ 啟用混合精度訓練\")\n",
    "        print(\"   - 速度提升: 2-3x\")\n",
    "        print(\"   - 記憶體額外節省: ~50%\")\n",
    "        \n",
    "        if margin > model_params_gb:\n",
    "            print(\"\\n2. 📈 可以增大批次大小\")\n",
    "            suggested_bs = int(batch_size * (1 + margin / total_estimate))\n",
    "            print(f\"   - 建議批次: {suggested_bs}\")\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "print(\"\\n示例 1: 8GB GPU 訓練 GPT-2 Medium\")\n",
    "generate_optimization_suggestions(\n",
    "    model_params_gb=1.4,  # GPT-2 Medium FP32\n",
    "    available_gpu_gb=8.0,\n",
    "    batch_size=8,\n",
    "    seq_length=512\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "print(\"示例 2: 24GB GPU 訓練 GPT-2 Large\")\n",
    "generate_optimization_suggestions(\n",
    "    model_params_gb=3.0,  # GPT-2 Large FP32\n",
    "    available_gpu_gb=24.0,\n",
    "    batch_size=16,\n",
    "    seq_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 11. 實驗總結與最佳實踐\n",
    "\n",
    "### 實驗結論\n",
    "\n",
    "1. **記憶體組成理解**:\n",
    "   - 模型參數: ~固定大小\n",
    "   - 梯度: 與參數同大小\n",
    "   - 優化器狀態: Adam 需要 2x 參數大小\n",
    "   - 激活值: 與批次大小和序列長度成正比\n",
    "\n",
    "2. **批次大小影響**:\n",
    "   - 批次越大, 激活值記憶體越高\n",
    "   - 記憶體增長接近線性\n",
    "   - 需要在批次大小和記憶體之間權衡\n",
    "\n",
    "3. **優化技術效果**:\n",
    "   - 混合精度: 最高優先級, 節省 ~50%, 提速 2-3x\n",
    "   - 梯度檢查點: 節省 30-50%, 時間代價 20-30%\n",
    "   - 梯度累積: 不增加記憶體, 實現大批次效果\n",
    "   - 組合優化: 可節省 70-80% 記憶體\n",
    "\n",
    "### 記憶體優化決策樹\n",
    "\n",
    "```\n",
    "開始訓練\n",
    "    |\n",
    "    ├─ OOM? ─ 否 ─→ 考慮混合精度加速\n",
    "    |         |        |\n",
    "    |         └─ OOM? ─ 否 ─→ 可以增大批次\n",
    "    |\n",
    "    └─ 是\n",
    "       |\n",
    "       ├─ 步驟 1: 啟用混合精度 (FP16/BF16)\n",
    "       |          ↓\n",
    "       |       還 OOM?\n",
    "       |\n",
    "       ├─ 步驟 2: 啟用梯度檢查點\n",
    "       |          ↓\n",
    "       |       還 OOM?\n",
    "       |\n",
    "       ├─ 步驟 3: 減小批次 + 梯度累積\n",
    "       |          ↓\n",
    "       |       還 OOM?\n",
    "       |\n",
    "       └─ 步驟 4: 考慮 DeepSpeed/PEFT/量化\n",
    "```\n",
    "\n",
    "### PyTorch 記憶體管理工具總結\n",
    "\n",
    "#### 1. 基礎監控\n",
    "\n",
    "```python\n",
    "# 查看當前記憶體使用\n",
    "torch.cuda.memory_allocated()  # 已分配\n",
    "torch.cuda.memory_reserved()   # 已保留\n",
    "torch.cuda.max_memory_allocated()  # 峰值\n",
    "\n",
    "# 重置統計\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.empty_cache()  # 清空緩存\n",
    "```\n",
    "\n",
    "#### 2. Profiler 使用\n",
    "\n",
    "```python\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    profile_memory=True,\n",
    "    record_shapes=True\n",
    ") as prof:\n",
    "    # 訓練代碼\n",
    "    pass\n",
    "\n",
    "# 查看結果\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))\n",
    "```\n",
    "\n",
    "#### 3. 記憶體快照\n",
    "\n",
    "```python\n",
    "# 記錄記憶體快照 (需要 PyTorch 2.1+)\n",
    "torch.cuda.memory._record_memory_history()\n",
    "# ... 訓練代碼\n",
    "torch.cuda.memory._dump_snapshot(\"memory_snapshot.pickle\")\n",
    "```\n",
    "\n",
    "### 常見記憶體問題診斷\n",
    "\n",
    "#### 問題 1: OOM (Out of Memory)\n",
    "\n",
    "**症狀**: `RuntimeError: CUDA out of memory`\n",
    "\n",
    "**診斷步驟**:\n",
    "1. 打印峰值記憶體: `torch.cuda.max_memory_allocated()`\n",
    "2. 檢查批次大小是否過大\n",
    "3. 檢查是否有記憶體洩漏\n",
    "\n",
    "**解決方案**:\n",
    "```python\n",
    "# 1. 減小批次\n",
    "batch_size = 1\n",
    "\n",
    "# 2. 啟用混合精度\n",
    "with autocast(dtype=torch.float16):\n",
    "    outputs = model(**batch)\n",
    "\n",
    "# 3. 梯度檢查點\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 4. 清空緩存\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "#### 問題 2: 記憶體洩漏\n",
    "\n",
    "**症狀**: 記憶體持續增長, 不釋放\n",
    "\n",
    "**診斷**:\n",
    "```python\n",
    "# 監控每步記憶體\n",
    "for step in range(100):\n",
    "    # 訓練\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "```\n",
    "\n",
    "**常見原因**:\n",
    "- 未 detach 的張量\n",
    "- 保存了整個計算圖\n",
    "- 全局變數累積\n",
    "\n",
    "**解決**:\n",
    "```python\n",
    "# 使用 .detach() 斷開計算圖\n",
    "loss_value = loss.detach().item()\n",
    "\n",
    "# 避免保存張量到列表\n",
    "losses.append(loss.item())  # ✅ 正確\n",
    "losses.append(loss)         # ❌ 會洩漏\n",
    "```\n",
    "\n",
    "#### 問題 3: 記憶體碎片化\n",
    "\n",
    "**症狀**: 總記憶體充足, 但分配失敗\n",
    "\n",
    "**解決**:\n",
    "```python\n",
    "# 定期清理\n",
    "if step % 100 == 0:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "```\n",
    "\n",
    "### 生產環境建議\n",
    "\n",
    "#### 1. 記憶體監控\n",
    "\n",
    "```python\n",
    "class MemoryMonitor:\n",
    "    def __init__(self, log_interval=100):\n",
    "        self.log_interval = log_interval\n",
    "        self.peak_memory = 0\n",
    "    \n",
    "    def log(self, step):\n",
    "        if step % self.log_interval == 0:\n",
    "            current = torch.cuda.memory_allocated() / 1e9\n",
    "            peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "            self.peak_memory = max(self.peak_memory, peak)\n",
    "            \n",
    "            print(f\"Step {step}: {current:.2f}GB (peak: {peak:.2f}GB)\")\n",
    "            \n",
    "            # 記錄到 tensorboard/wandb\n",
    "            # writer.add_scalar('memory/allocated', current, step)\n",
    "\n",
    "monitor = MemoryMonitor()\n",
    "for step in range(num_steps):\n",
    "    # 訓練\n",
    "    monitor.log(step)\n",
    "```\n",
    "\n",
    "#### 2. 自動優化選擇\n",
    "\n",
    "```python\n",
    "def auto_configure(model, gpu_memory_gb):\n",
    "    \"\"\"根據 GPU 記憶體自動配置優化\"\"\"\n",
    "    model_size_gb = sum(p.numel() for p in model.parameters()) * 4 / 1e9\n",
    "    \n",
    "    config = {}\n",
    "    \n",
    "    # 總是使用混合精度\n",
    "    config['use_amp'] = True\n",
    "    \n",
    "    # 記憶體緊張時使用檢查點\n",
    "    if model_size_gb * 4 > gpu_memory_gb * 0.7:\n",
    "        config['use_checkpoint'] = True\n",
    "    \n",
    "    # 自動計算批次大小\n",
    "    available = gpu_memory_gb - model_size_gb * 2  # 留給激活值\n",
    "    config['batch_size'] = max(1, int(available / 0.5))  # 粗略估計\n",
    "    \n",
    "    return config\n",
    "```\n",
    "\n",
    "### 記憶體優化 Checklist\n",
    "\n",
    "訓練前檢查:\n",
    "- [ ] 啟用混合精度訓練\n",
    "- [ ] 根據 GPU 選擇合適的批次大小\n",
    "- [ ] 大模型考慮梯度檢查點\n",
    "- [ ] 設置記憶體監控\n",
    "\n",
    "訓練中監控:\n",
    "- [ ] 定期記錄峰值記憶體\n",
    "- [ ] 檢查記憶體是否持續增長 (洩漏)\n",
    "- [ ] 觀察 GPU 利用率\n",
    "\n",
    "遇到 OOM:\n",
    "- [ ] 減小批次大小\n",
    "- [ ] 啟用梯度累積\n",
    "- [ ] 啟用梯度檢查點\n",
    "- [ ] 清空 CUDA 緩存\n",
    "- [ ] 考慮 DeepSpeed/FSDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 12. 下一步學習\n",
    "\n",
    "恭喜完成 Lab-1.4 所有實驗! 🎉\n",
    "\n",
    "您已經掌握了:\n",
    "- ✅ 混合精度訓練 (速度 2-3x, 記憶體省 50%)\n",
    "- ✅ 梯度累積 (突破記憶體限制)\n",
    "- ✅ 梯度檢查點 (記憶體省 30-50%)\n",
    "- ✅ 記憶體分析與優化 (定位瓶頸, 制定策略)\n",
    "\n",
    "### 推薦下一步:\n",
    "\n",
    "1. **應用到 PEFT Labs**\n",
    "   - 在 LoRA/QLoRA 訓練中應用這些優化\n",
    "   - 訓練更大的模型\n",
    "\n",
    "2. **學習分散式訓練**\n",
    "   - Lab-1.2: PyTorch DDP Basics\n",
    "   - Lab-1.3: DeepSpeed (多GPU)\n",
    "\n",
    "3. **高級優化技術**\n",
    "   - FlashAttention (長序列優化)\n",
    "   - Efficient Attention (MQA/GQA)\n",
    "\n",
    "4. **生產部署**\n",
    "   - 模型量化與壓縮\n",
    "   - 推理優化\n",
    "\n",
    "訓練優化是 LLM 工程的核心技能, 繼續加油! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
