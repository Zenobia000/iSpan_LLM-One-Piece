{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2: Wanda Pruning - Environment Setup\n",
    "\n",
    "**Goal:** Prepare the environment for Wanda pruning experiments.\n",
    "\n",
    "**You will learn to:**\n",
    "- Verify GPU and PyTorch sparse tensor support\n",
    "- Install Wanda library and dependencies\n",
    "- Load a baseline dense model for pruning\n",
    "- Prepare calibration data for activation statistics\n",
    "\n",
    "---\n",
    "\n",
    "## Why Environment Verification Matters\n",
    "\n",
    "**Wanda pruning has specific requirements**:\n",
    "- **GPU Memory**: Pruning requires loading FP16 model (~15GB for Llama-2-7B)\n",
    "- **PyTorch Sparse**: Need PyTorch with sparse tensor support\n",
    "- **Calibration Data**: Representative dataset for activation statistics\n",
    "- **Hardware Acceleration**: Optional 2:4 sparse support (NVIDIA A100)\n",
    "\n",
    "**Time investment**: 5-10 minutes (one-time setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Hardware Verification\n",
    "\n",
    "First, let's verify GPU availability and specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA GPU status\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Configuration Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch and CUDA versions\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # GPU details\n",
    "    gpu_id = 0\n",
    "    gpu_props = torch.cuda.get_device_properties(gpu_id)\n",
    "    \n",
    "    print(f\"\\n‚úÖ GPU Detected:\")\n",
    "    print(f\"   Name: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "    print(f\"   Total Memory: {gpu_props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Compute Capability: SM {gpu_props.major}.{gpu_props.minor}\")\n",
    "    \n",
    "    # Check 2:4 sparse support (Ampere+ = SM 8.0+)\n",
    "    if gpu_props.major >= 8:\n",
    "        print(f\"   ‚úÖ 2:4 Sparse Support: YES (SM {gpu_props.major}.{gpu_props.minor} >= 8.0)\")\n",
    "        print(f\"      ‚Üí Can use NVIDIA Sparse Tensor Cores for 2x acceleration\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  2:4 Sparse Support: NO (SM {gpu_props.major}.{gpu_props.minor} < 8.0)\")\n",
    "        print(f\"      ‚Üí Sparse pruning will have no hardware acceleration\")\n",
    "    \n",
    "    # Memory recommendation\n",
    "    if gpu_props.total_memory / 1e9 >= 16:\n",
    "        print(f\"   ‚úÖ Memory: Sufficient for pruning (>= 16GB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Memory: Limited (<16GB). May need smaller model.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No GPU detected!\")\n",
    "    print(\"   Wanda pruning requires GPU for efficient computation.\")\n",
    "    print(\"   Please run on a machine with NVIDIA GPU.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Check PyTorch Sparse Tensor Support\n",
    "\n",
    "Wanda creates sparse models. Let's verify PyTorch sparse functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PyTorch Sparse Tensor Support Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test sparse tensor creation\n",
    "try:\n",
    "    # Create a dense tensor\n",
    "    dense = torch.randn(100, 100)\n",
    "    \n",
    "    # Create sparse tensor (COO format)\n",
    "    indices = torch.LongTensor([[0, 1, 1], [2, 0, 2]])\n",
    "    values = torch.FloatTensor([3, 4, 5])\n",
    "    sparse = torch.sparse_coo_tensor(indices, values, (2, 3))\n",
    "    \n",
    "    print(\"‚úÖ Sparse tensor creation: SUCCESS\")\n",
    "    print(f\"   Sparse tensor shape: {sparse.shape}\")\n",
    "    print(f\"   Sparse tensor format: {sparse.layout}\")\n",
    "    \n",
    "    # Test sparse to dense conversion\n",
    "    dense_from_sparse = sparse.to_dense()\n",
    "    print(\"‚úÖ Sparse to dense conversion: SUCCESS\")\n",
    "    \n",
    "    # Test sparsity measurement\n",
    "    test_tensor = torch.randn(100, 100)\n",
    "    test_tensor[test_tensor.abs() < 0.5] = 0  # Create 50% sparsity\n",
    "    sparsity = (test_tensor == 0).sum().item() / test_tensor.numel()\n",
    "    print(f\"‚úÖ Sparsity measurement: {sparsity:.2%}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All sparse tensor operations working!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Sparse tensor test failed: {e}\")\n",
    "    print(\"   Please update PyTorch to version >= 2.0\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Install Wanda and Dependencies\n",
    "\n",
    "We'll install:\n",
    "- **transformers**: Model loading and inference\n",
    "- **datasets**: Calibration data loading\n",
    "- **accelerate**: Distributed loading support\n",
    "\n",
    "**Note**: Wanda is typically implemented as a standalone script. We'll implement it ourselves in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "!pip install -q transformers>=4.35.0  # Model support\n",
    "!pip install -q datasets  # Calibration data\n",
    "!pip install -q accelerate  # Distributed loading\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Verify Library Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import datasets\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Library Version Check\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch:      {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"Accelerate:   {accelerate.__version__}\")\n",
    "print(f\"Datasets:     {datasets.__version__}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Version checks\n",
    "def check_version(name, current, required):\n",
    "    from packaging import version\n",
    "    if version.parse(current) >= version.parse(required):\n",
    "        print(f\"‚úÖ {name}: {current} >= {required}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {name}: {current} < {required} (may cause issues)\")\n",
    "\n",
    "check_version(\"Transformers\", transformers.__version__, \"4.35.0\")\n",
    "check_version(\"PyTorch\", torch.__version__.split(\"+\")[0], \"2.0.0\")\n",
    "\n",
    "print(\"\\n‚úÖ All libraries verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Load Baseline Model (Dense)\n",
    "\n",
    "Let's load the baseline **Llama-2-7B** model in FP16 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Change to TinyLlama if OOM\n",
    "# Alternative: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loading Baseline Model: {MODEL_NAME}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚è≥ This may take 1-3 minutes...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load model in FP16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # FP16 precision\n",
    "    device_map=\"auto\",          # Automatic device placement\n",
    "    trust_remote_code=True      # Allow custom model code\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded in FP16\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\nüìä GPU Memory Usage:\")\n",
    "    print(f\"   Allocated: {memory_allocated:.2f} GB\")\n",
    "    print(f\"   Reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# Model info\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nüìù Model Info:\")\n",
    "print(f\"   Parameters: {num_params / 1e9:.2f}B\")\n",
    "print(f\"   Precision: FP16 (2 bytes/param)\")\n",
    "print(f\"   Estimated size: {num_params * 2 / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Baseline model ready for pruning!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Prepare Calibration Data\n",
    "\n",
    "Wanda requires calibration data to collect activation statistics. We'll use **WikiText-2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Calibration Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load WikiText-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded\")\n",
    "print(f\"   Total samples: {len(dataset)}\")\n",
    "print(f\"   Sample text: {dataset[0]['text'][:200]}...\")\n",
    "\n",
    "# Filter out empty texts\n",
    "dataset = dataset.filter(lambda x: len(x['text'].strip()) > 0)\n",
    "print(f\"\\n‚úÖ Filtered dataset: {len(dataset)} non-empty samples\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Prepare Calibration Samples\n",
    "\n",
    "Create tokenized calibration samples for activation collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NSAMPLES = 128  # Number of calibration samples\n",
    "SEQLEN = 2048   # Sequence length\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Preparing Calibration Samples\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Samples: {NSAMPLES}\")\n",
    "print(f\"Sequence length: {SEQLEN}\\n\")\n",
    "\n",
    "# Tokenize and prepare samples\n",
    "calibration_samples = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(NSAMPLES), desc=\"Tokenizing\"):\n",
    "    if i >= len(dataset):\n",
    "        break\n",
    "    \n",
    "    text = dataset[i]['text']\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=SEQLEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    calibration_samples.append(inputs['input_ids'])\n",
    "\n",
    "# Stack into batch\n",
    "calibration_batch = torch.cat(calibration_samples, dim=0)\n",
    "\n",
    "print(f\"\\n‚úÖ Calibration data prepared\")\n",
    "print(f\"   Shape: {calibration_batch.shape}\")\n",
    "print(f\"   Size: {calibration_batch.numel() * 4 / 1e6:.2f} MB\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Test Baseline Inference\n",
    "\n",
    "Verify the model works correctly before pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Baseline Inference Test\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "latency = end_time - start_time\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Output: {generated_text}\\n\")\n",
    "print(f\"‚è±Ô∏è  Latency: {latency:.2f} seconds\")\n",
    "print(f\"üìä Tokens/sec: {len(outputs[0]) / latency:.2f}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Inference test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Calculate Current Sparsity (Should be 0%)\n",
    "\n",
    "Let's measure the baseline model's sparsity (should be near 0% for dense model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model, threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate model sparsity (percentage of near-zero weights)\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += (param.abs() < threshold).sum().item()\n",
    "    \n",
    "    sparsity = zero_params / total_params if total_params > 0 else 0\n",
    "    return sparsity, total_params, zero_params\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Baseline Model Sparsity\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sparsity, total, zeros = calculate_sparsity(model)\n",
    "\n",
    "print(f\"Total parameters: {total / 1e9:.2f}B\")\n",
    "print(f\"Near-zero params: {zeros / 1e6:.2f}M\")\n",
    "print(f\"Sparsity: {sparsity:.4%}\")\n",
    "print(f\"\\n‚úÖ Baseline sparsity is near 0% (expected for dense model)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Setup Complete!\n",
    "\n",
    "**Summary**:\n",
    "- ‚úÖ GPU verified (CUDA available)\n",
    "- ‚úÖ PyTorch sparse tensor support confirmed\n",
    "- ‚úÖ Libraries installed (transformers, datasets, accelerate)\n",
    "- ‚úÖ Baseline FP16 model loaded\n",
    "- ‚úÖ Calibration data prepared (128 samples)\n",
    "- ‚úÖ Inference test passed\n",
    "- ‚úÖ Baseline sparsity measured (~0%)\n",
    "\n",
    "**Next Steps**:\n",
    "1. Proceed to **02-Prune.ipynb** to apply Wanda pruning\n",
    "2. Target: 50% sparsity (3.5B effective parameters)\n",
    "3. Expected precision loss: <8% (Perplexity +0.44)\n",
    "\n",
    "**Key Variables Available**:\n",
    "- `model`: Baseline dense Llama-2-7B model\n",
    "- `tokenizer`: Tokenizer for text processing\n",
    "- `calibration_batch`: Tokenized calibration data (128 samples)\n",
    "- `calculate_sparsity()`: Function to measure sparsity\n",
    "\n",
    "---\n",
    "\n",
    "**‚è≠Ô∏è Continue to**: [02-Prune.ipynb](./02-Prune.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
