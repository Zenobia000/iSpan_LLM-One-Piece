{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: (IA)³ - Fine-Tuning a GPT-2 Model with Extreme Parameter Efficiency\n",
        "---\n",
        "## Notebook 3: Inference\n",
        "\n",
        "**Goal:** In this notebook, you will load the trained (IA)³ adapter and use the fine-tuned GPT-2 model to generate positive movie reviews.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Reload the base GPT-2 model.\n",
        "-   Load the trained (IA)³ adapter from a checkpoint.\n",
        "-   Generate text from a prompt using the fine-tuned model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Reload Model and Adapter\n",
        "\n",
        "We will load the base `gpt2` model and then apply our trained (IA)³ weights on top of it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- Load Base Model and Tokenizer ---\n",
        "model_checkpoint = \"gpt2\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Load PEFT Adapter ---\n",
        "output_dir = \"./gpt2-ia3-imdb\"\n",
        "latest_checkpoint = max(\n",
        "    [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")],\n",
        "    key=os.path.getmtime\n",
        ")\n",
        "print(f\"Loading adapter from: {latest_checkpoint}\")\n",
        "\n",
        "inference_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inference_model.to(device)\n",
        "inference_model.eval()\n",
        "\n",
        "print(\"✅ Inference model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Perform Inference\n",
        "\n",
        "Let's test the model with the same prompt we used in the Prefix Tuning lab to see how well this extremely lightweight method performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the prompt\n",
        "prompt = \"This movie was absolutely\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate the text\n",
        "with torch.no_grad():\n",
        "    outputs = inference_model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=3\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(\"--- Prompt ---\")\n",
        "print(prompt)\n",
        "print(\"\\n--- Generated Reviews (with IA³) ---\")\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"{i+1}: {tokenizer.decode(output, skip_special_tokens=True)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Lab Conclusion\n",
        "\n",
        "This concludes the (IA)³ lab and the main series of PEFT labs. You have now implemented five distinct parameter-efficient fine-tuning methods and have seen how to configure and apply them using the Hugging Face `peft` and `transformers` libraries.\n",
        "\n",
        "You should now have a strong practical understanding of the trade-offs between these different techniques in terms of parameter efficiency, architectural changes, and configuration complexity.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
