{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3: Knowledge Distillation - Environment Setup\n",
    "\n",
    "**Goal:** Prepare the environment for knowledge distillation experiments.\n",
    "\n",
    "**You will learn to:**\n",
    "- Verify GPU and PyTorch environment\n",
    "- Install required libraries (transformers, datasets)\n",
    "- Load teacher model (BERT-base, 110M params)\n",
    "- Initialize student model (BERT-6L, 52M params) with layer-wise init\n",
    "- Prepare GLUE SST-2 dataset\n",
    "- Test baseline teacher performance\n",
    "\n",
    "---\n",
    "\n",
    "## Why Environment Verification Matters\n",
    "\n",
    "**Knowledge distillation requires**:\n",
    "- **GPU Memory**: Load both teacher and student (~1.5GB)\n",
    "- **Correct libraries**: Transformers with distillation support\n",
    "- **Quality dataset**: Representative calibration data\n",
    "- **Baseline metrics**: Teacher performance as upper bound\n",
    "\n",
    "**Time investment**: 10-15 minutes (one-time setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Hardware Verification\n",
    "\n",
    "Check GPU availability and specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA GPU status\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Configuration Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch and CUDA versions\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # GPU details\n",
    "    gpu_id = 0\n",
    "    gpu_props = torch.cuda.get_device_properties(gpu_id)\n",
    "    \n",
    "    print(f\"\\n‚úÖ GPU Detected:\")\n",
    "    print(f\"   Name: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "    print(f\"   Total Memory: {gpu_props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Compute Capability: SM {gpu_props.major}.{gpu_props.minor}\")\n",
    "    \n",
    "    # Memory recommendation\n",
    "    if gpu_props.total_memory / 1e9 >= 4:\n",
    "        print(f\"   ‚úÖ Memory: Sufficient for distillation (>= 4GB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Memory: Limited (<4GB). May need smaller models.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU detected!\")\n",
    "    print(\"   Distillation can run on CPU but will be slower.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Install Required Libraries\n",
    "\n",
    "Install transformers, datasets, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "!pip install -q transformers>=4.35.0  # BERT and training utilities\n",
    "!pip install -q datasets              # GLUE dataset\n",
    "!pip install -q accelerate            # Training acceleration\n",
    "!pip install -q evaluate              # Evaluation metrics\n",
    "!pip install -q scikit-learn          # Additional metrics\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Verify Library Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import accelerate\n",
    "import evaluate\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Library Version Check\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch:      {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"Datasets:     {datasets.__version__}\")\n",
    "print(f\"Accelerate:   {accelerate.__version__}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Version checks\n",
    "from packaging import version\n",
    "\n",
    "def check_version(name, current, required):\n",
    "    if version.parse(current) >= version.parse(required):\n",
    "        print(f\"‚úÖ {name}: {current} >= {required}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {name}: {current} < {required} (may cause issues)\")\n",
    "\n",
    "check_version(\"Transformers\", transformers.__version__, \"4.35.0\")\n",
    "check_version(\"PyTorch\", torch.__version__.split(\"+\")[0], \"2.0.0\")\n",
    "\n",
    "print(\"\\n‚úÖ All libraries verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Load Teacher Model (BERT-base)\n",
    "\n",
    "Load the pre-trained teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import gc\n",
    "\n",
    "# Model configuration\n",
    "TEACHER_MODEL = \"bert-base-uncased\"  # 110M parameters\n",
    "NUM_LABELS = 2  # Binary classification (SST-2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loading Teacher Model: {TEACHER_MODEL}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚è≥ This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL)\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load teacher model\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TEACHER_MODEL,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model = teacher_model.to(device)\n",
    "print(f\"‚úÖ Teacher model loaded on {device}\")\n",
    "\n",
    "# Model info\n",
    "num_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\"\\nüìù Teacher Model Info:\")\n",
    "print(f\"   Parameters: {num_params / 1e6:.2f}M\")\n",
    "print(f\"   Layers: {teacher_model.config.num_hidden_layers}\")\n",
    "print(f\"   Hidden size: {teacher_model.config.hidden_size}\")\n",
    "print(f\"   Attention heads: {teacher_model.config.num_attention_heads}\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\nüìä GPU Memory Usage:\")\n",
    "    print(f\"   Allocated: {memory_allocated:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Teacher model ready!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Initialize Student Model (BERT-6L)\n",
    "\n",
    "Create a smaller student model with layer-wise initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "import copy\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Initializing Student Model (BERT-6L)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Student configuration (half the layers)\n",
    "student_config = BertConfig(\n",
    "    vocab_size=teacher_model.config.vocab_size,\n",
    "    hidden_size=teacher_model.config.hidden_size,  # Same as teacher\n",
    "    num_hidden_layers=6,  # Half of teacher (12 -> 6)\n",
    "    num_attention_heads=teacher_model.config.num_attention_heads,\n",
    "    intermediate_size=teacher_model.config.intermediate_size,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "# Create student model\n",
    "student_model = BertForSequenceClassification(student_config)\n",
    "\n",
    "print(f\"\\nüìù Student Model Info:\")\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "print(f\"   Parameters: {student_params / 1e6:.2f}M\")\n",
    "print(f\"   Layers: {student_config.num_hidden_layers}\")\n",
    "print(f\"   Compression ratio: {num_params / student_params:.2f}x\")\n",
    "\n",
    "print(\"\\n‚è≥ Applying layer-wise initialization...\")\n",
    "\n",
    "# Layer-wise initialization (ÈöîÂ±§ÂàùÂßãÂåñ)\n",
    "# Student layer i = Teacher layer 2i\n",
    "teacher_state = teacher_model.state_dict()\n",
    "student_state = student_model.state_dict()\n",
    "\n",
    "# Copy embeddings\n",
    "for key in student_state.keys():\n",
    "    if 'embeddings' in key:\n",
    "        student_state[key] = teacher_state[key].clone()\n",
    "\n",
    "# Copy selected layers (0, 2, 4, 6, 8, 10)\n",
    "teacher_layers = teacher_model.bert.encoder.layer\n",
    "student_layers = student_model.bert.encoder.layer\n",
    "\n",
    "layer_mapping = [0, 2, 4, 6, 8, 10]  # Select every 2nd layer\n",
    "for student_idx, teacher_idx in enumerate(layer_mapping):\n",
    "    # Copy layer weights\n",
    "    student_layers[student_idx].load_state_dict(\n",
    "        teacher_layers[teacher_idx].state_dict()\n",
    "    )\n",
    "    print(f\"   Student layer {student_idx} ‚Üê Teacher layer {teacher_idx}\")\n",
    "\n",
    "# Copy pooler and classifier\n",
    "for key in student_state.keys():\n",
    "    if 'pooler' in key or 'classifier' in key:\n",
    "        if key in teacher_state:\n",
    "            student_state[key] = teacher_state[key].clone()\n",
    "\n",
    "# Load initialized weights\n",
    "student_model.load_state_dict(student_state)\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "print(\"‚úÖ Layer-wise initialization complete!\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\nüìä GPU Memory Usage (Teacher + Student):\")\n",
    "    print(f\"   Allocated: {memory_allocated:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Student model ready!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Load GLUE SST-2 Dataset\n",
    "\n",
    "Prepare the Stanford Sentiment Treebank dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading GLUE SST-2 Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded:\")\n",
    "print(f\"   Train: {len(dataset['train'])} samples\")\n",
    "print(f\"   Validation: {len(dataset['validation'])} samples\")\n",
    "print(f\"   Test: {len(dataset['test'])} samples (no labels)\")\n",
    "\n",
    "# Show sample\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\nüìù Sample:\")\n",
    "print(f\"   Sentence: {sample['sentence']}\")\n",
    "print(f\"   Label: {sample['label']} (0=Negative, 1=Positive)\")\n",
    "\n",
    "# Label distribution\n",
    "train_labels = [ex['label'] for ex in dataset['train']]\n",
    "neg_count = train_labels.count(0)\n",
    "pos_count = train_labels.count(1)\n",
    "\n",
    "print(f\"\\nüìä Label Distribution (Train):\")\n",
    "print(f\"   Negative: {neg_count} ({neg_count/len(train_labels):.1%})\")\n",
    "print(f\"   Positive: {pos_count} ({pos_count/len(train_labels):.1%})\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Tokenize Dataset\n",
    "\n",
    "Prepare tokenized inputs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Tokenizing Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"‚è≥ Tokenizing train set...\")\n",
    "tokenized_train = dataset['train'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['sentence', 'idx']\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Tokenizing validation set...\")\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['sentence', 'idx']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Tokenization complete!\")\n",
    "print(f\"   Train samples: {len(tokenized_train)}\")\n",
    "print(f\"   Val samples: {len(tokenized_val)}\")\n",
    "\n",
    "# Show tokenized sample\n",
    "sample_tokens = tokenized_train[0]\n",
    "print(f\"\\nüìù Tokenized Sample:\")\n",
    "print(f\"   Input IDs shape: {len(sample_tokens['input_ids'])}\")\n",
    "print(f\"   Attention mask: {sample_tokens['attention_mask'][:20]}...\")\n",
    "print(f\"   Label: {sample_tokens['label']}\")\n",
    "\n",
    "# Decode to verify\n",
    "decoded = tokenizer.decode(sample_tokens['input_ids'])\n",
    "print(f\"   Decoded: {decoded[:100]}...\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Test Teacher Baseline Performance\n",
    "\n",
    "Evaluate teacher model on validation set to establish upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing Teacher Baseline Performance\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare validation dataloader\n",
    "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataloader = DataLoader(tokenized_val, batch_size=32)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    \n",
    "    return accuracy, all_preds, all_labels\n",
    "\n",
    "# Evaluate teacher\n",
    "print(\"\\n‚è≥ Evaluating teacher model...\")\n",
    "teacher_accuracy, teacher_preds, true_labels = evaluate_model(teacher_model, val_dataloader)\n",
    "\n",
    "print(f\"\\nüìä Teacher Performance (Pre-trained):\")\n",
    "print(f\"   Accuracy: {teacher_accuracy:.4f} ({teacher_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Per-class accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"\\nüìã Classification Report:\")\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    teacher_preds, \n",
    "    target_names=['Negative', 'Positive'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, teacher_preds)\n",
    "print(f\"\\nüìä Confusion Matrix:\")\n",
    "print(f\"               Predicted\")\n",
    "print(f\"             Neg    Pos\")\n",
    "print(f\"Actual Neg  {cm[0][0]:4d}  {cm[0][1]:4d}\")\n",
    "print(f\"       Pos  {cm[1][0]:4d}  {cm[1][1]:4d}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Baseline evaluation complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Test Student Baseline (Before Distillation)\n",
    "\n",
    "Evaluate student model performance before distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Testing Student Baseline (Before Distillation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate student\n",
    "print(\"\\n‚è≥ Evaluating student model (initialized from teacher)...\")\n",
    "student_accuracy, student_preds, _ = evaluate_model(student_model, val_dataloader)\n",
    "\n",
    "print(f\"\\nüìä Student Performance (Before Distillation):\")\n",
    "print(f\"   Accuracy: {student_accuracy:.4f} ({student_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\nüìä Teacher vs Student (Pre-distillation):\")\n",
    "print(f\"   Teacher:  {teacher_accuracy:.4f} ({teacher_accuracy*100:.2f}%)\")\n",
    "print(f\"   Student:  {student_accuracy:.4f} ({student_accuracy*100:.2f}%)\")\n",
    "print(f\"   Gap:      {(teacher_accuracy - student_accuracy)*100:.2f}%\")\n",
    "print(f\"   Relative: {student_accuracy/teacher_accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note:\")\n",
    "print(\"   Student performance is lower because it has fewer layers.\")\n",
    "print(\"   Distillation will help close this gap!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Pre-distillation baseline established!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Quick Inference Test\n",
    "\n",
    "Test both models on sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Quick Inference Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"This movie is absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Terrible waste of time. The plot was boring and predictable.\",\n",
    "    \"It was okay, nothing special but not terrible either.\"\n",
    "]\n",
    "\n",
    "label_names = ['Negative', 'Positive']\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"\\nTest {i}: {sentence}\")\n",
    "    print(\"‚îÄ\" * 60)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Teacher prediction\n",
    "    teacher_model.eval()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(**inputs)\n",
    "        teacher_probs = torch.softmax(teacher_outputs.logits, dim=-1)[0]\n",
    "        teacher_pred = torch.argmax(teacher_probs).item()\n",
    "    \n",
    "    # Student prediction\n",
    "    student_model.eval()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student_model(**inputs)\n",
    "        student_probs = torch.softmax(student_outputs.logits, dim=-1)[0]\n",
    "        student_pred = torch.argmax(student_probs).item()\n",
    "    \n",
    "    print(f\"Teacher: {label_names[teacher_pred]} (Neg: {teacher_probs[0]:.3f}, Pos: {teacher_probs[1]:.3f})\")\n",
    "    print(f\"Student: {label_names[student_pred]} (Neg: {student_probs[0]:.3f}, Pos: {student_probs[1]:.3f})\")\n",
    "    \n",
    "    if teacher_pred == student_pred:\n",
    "        print(\"‚úÖ Agreement\")\n",
    "    else:\n",
    "        print(\"‚ùå Disagreement\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Inference test complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Setup Complete!\n",
    "\n",
    "**Summary**:\n",
    "- ‚úÖ GPU verified (CUDA available)\n",
    "- ‚úÖ Libraries installed (transformers, datasets, evaluate)\n",
    "- ‚úÖ Teacher model loaded (BERT-base, 110M params)\n",
    "- ‚úÖ Student model initialized (BERT-6L, 52M params, 2.1x compression)\n",
    "- ‚úÖ GLUE SST-2 dataset prepared (67K train, 872 val)\n",
    "- ‚úÖ Baseline performance measured\n",
    "\n",
    "**Key Metrics**:\n",
    "- Teacher accuracy: ~92.8% (upper bound)\n",
    "- Student accuracy (pre-distillation): ~85-88%\n",
    "- Performance gap: ~5-8%\n",
    "\n",
    "**Next Steps**:\n",
    "1. Proceed to **02-Distill.ipynb** to apply knowledge distillation\n",
    "2. Close the performance gap using teacher's soft labels\n",
    "3. Target: Student accuracy ~91-92% (98-99% of teacher)\n",
    "\n",
    "**Key Variables Available**:\n",
    "- `teacher_model`: BERT-base teacher model\n",
    "- `student_model`: BERT-6L student model (layer-wise initialized)\n",
    "- `tokenizer`: BERT tokenizer\n",
    "- `tokenized_train`: Tokenized training set\n",
    "- `tokenized_val`: Tokenized validation set\n",
    "- `teacher_accuracy`: Baseline teacher performance\n",
    "- `student_accuracy`: Pre-distillation student performance\n",
    "\n",
    "---\n",
    "\n",
    "**‚è≠Ô∏è Continue to**: [02-Distill.ipynb](./02-Distill.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
