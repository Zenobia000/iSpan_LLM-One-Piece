{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.2 Part 4: Comprehensive Optimization\n",
    "\n",
    "## Objectives\n",
    "- Combine multiple optimization techniques\n",
    "- Test vLLM + Quantization\n",
    "- Run end-to-end benchmarks\n",
    "- Perform cost-benefit analysis\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Optimization Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Techniques Summary\n",
    "\n",
    "| Technique | Memory | Latency | Throughput | Difficulty |\n",
    "|-----------|--------|---------|------------|------------|\n",
    "| **PagedAttention** | â†“40-60% | â†’ | â†‘20-30% | Easy (use vLLM) |\n",
    "| **Continuous Batching** | â†’ | â†‘10-20% | â†‘200-300% | Easy (use vLLM) |\n",
    "| **FlashAttention** | â†’ | â†“30-40% | â†‘20-30% | Easy (built-in) |\n",
    "| **Quantization (INT8)** | â†“50% | â†“30-40% | â†‘50-100% | Medium |\n",
    "| **Speculative Decoding** | â†‘20% | â†“50-70% | â†‘150-300% | Hard |\n",
    "| **GQA** | â†“75% | â†’ | â†‘50% | Hard (model arch) |\n",
    "\n",
    "Legend: â†“ = reduce, â†‘ = increase, â†’ = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"vLLM: {vllm.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Baseline: HuggingFace FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/opt-1.3b\"\n",
    "\n",
    "print(\"Loading HuggingFace FP16 baseline...\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "hf_memory = hf_model.get_memory_footprint() / 1e9\n",
    "print(f\"âœ… Loaded, Memory: {hf_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark baseline\n",
    "def benchmark_batch(model, tokenizer, prompts, max_tokens=50):\n",
    "    \"\"\"Benchmark batch generation.\"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    total_tokens = sum(len(out) for out in outputs) - len(prompts) * len(inputs.input_ids[0])\n",
    "    \n",
    "    return {\n",
    "        'time': elapsed,\n",
    "        'tokens': total_tokens,\n",
    "        'throughput': total_tokens / elapsed,\n",
    "    }\n",
    "\n",
    "test_prompts = [\n",
    "    \"Explain AI:\",\n",
    "    \"What is Python?\",\n",
    "    \"Benefits of ML:\",\n",
    "    \"Future of tech:\",\n",
    "]\n",
    "\n",
    "print(\"Benchmarking HuggingFace FP16...\")\n",
    "baseline = benchmark_batch(hf_model, tokenizer, test_prompts)\n",
    "\n",
    "print(f\"  Time: {baseline['time']:.2f}s\")\n",
    "print(f\"  Throughput: {baseline['throughput']:.1f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Optimization 1: vLLM (PagedAttention + Continuous Batching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with vLLM\n",
    "print(\"Loading vLLM...\")\n",
    "vllm_model = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    gpu_memory_utilization=0.5,\n",
    "    max_model_len=512,\n",
    ")\n",
    "print(\"âœ… vLLM loaded\")\n",
    "\n",
    "# Benchmark\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    max_tokens=50,\n",
    ")\n",
    "\n",
    "print(\"\\nBenchmarking vLLM...\")\n",
    "start = time.time()\n",
    "outputs = vllm_model.generate(test_prompts, sampling_params)\n",
    "vllm_time = time.time() - start\n",
    "\n",
    "vllm_tokens = sum(len(o.outputs[0].token_ids) for o in outputs)\n",
    "vllm_throughput = vllm_tokens / vllm_time\n",
    "\n",
    "print(f\"  Time: {vllm_time:.2f}s\")\n",
    "print(f\"  Throughput: {vllm_throughput:.1f} tokens/s\")\n",
    "print(f\"  Speedup: {baseline['time']/vllm_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Optimization 2: Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quantized model with HuggingFace\n",
    "print(\"Loading INT8 quantized model...\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "hf_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "int8_memory = hf_int8.get_memory_footprint() / 1e9\n",
    "print(f\"âœ… Loaded, Memory: {int8_memory:.2f} GB ({hf_memory/int8_memory:.1f}x reduction)\")\n",
    "\n",
    "# Benchmark\n",
    "print(\"\\nBenchmarking INT8...\")\n",
    "int8_results = benchmark_batch(hf_int8, tokenizer, test_prompts)\n",
    "\n",
    "print(f\"  Time: {int8_results['time']:.2f}s\")\n",
    "print(f\"  Throughput: {int8_results['throughput']:.1f} tokens/s\")\n",
    "print(f\"  Speedup: {baseline['time']/int8_results['time']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Optimization 3: vLLM + Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Strategy\n",
    "\n",
    "**Note**: vLLM with quantization requires quantized model files.\n",
    "\n",
    "```python\n",
    "# Example: vLLM with AWQ quantization\n",
    "vllm_quantized = LLM(\n",
    "    model=\"TheBloke/Llama-2-7B-AWQ\",\n",
    "    quantization=\"awq\",\n",
    "    gpu_memory_utilization=0.9,\n",
    ")\n",
    "\n",
    "# Expected gains:\n",
    "# - Memory: 2x reduction (from quantization)\n",
    "# - Throughput: 15-20x vs HuggingFace (vLLM + quant)\n",
    "# - Latency: 4-5x improvement\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comprehensive Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "results = {\n",
    "    'HF FP16': {\n",
    "        'throughput': baseline['throughput'],\n",
    "        'time': baseline['time'],\n",
    "        'memory': hf_memory,\n",
    "    },\n",
    "    'HF INT8': {\n",
    "        'throughput': int8_results['throughput'],\n",
    "        'time': int8_results['time'],\n",
    "        'memory': int8_memory,\n",
    "    },\n",
    "    'vLLM FP16': {\n",
    "        'throughput': vllm_throughput,\n",
    "        'time': vllm_time,\n",
    "        'memory': hf_memory,  # Similar\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nComprehensive Benchmark Results\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Method':<20} {'Throughput':<20} {'Time':<15} {'Memory'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method, metrics in results.items():\n",
    "    speedup = baseline['time'] / metrics['time']\n",
    "    throughput_gain = metrics['throughput'] / baseline['throughput']\n",
    "    memory_reduction = baseline['memory'] / metrics['memory']\n",
    "    \n",
    "    print(f\"{method:<20} {metrics['throughput']:>6.1f} tok/s ({throughput_gain:.1f}x) \"\n",
    "          f\"{metrics['time']:>6.2f}s ({speedup:.1f}x) \"\n",
    "          f\"{metrics['memory']:>5.2f} GB ({memory_reduction:.1f}x)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comprehensive comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "methods = list(results.keys())\n",
    "throughputs = [results[m]['throughput'] for m in methods]\n",
    "times = [results[m]['time'] for m in methods]\n",
    "memories = [results[m]['memory'] for m in methods]\n",
    "colors = ['#ff6b6b', '#ffd93d', '#51cf66']\n",
    "\n",
    "# Throughput\n",
    "ax1.bar(methods, throughputs, color=colors)\n",
    "ax1.set_ylabel('Throughput (tokens/s)')\n",
    "ax1.set_title('Throughput Comparison')\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Time\n",
    "ax2.bar(methods, times, color=colors)\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Generation Time Comparison')\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Memory\n",
    "ax3.bar(methods, memories, color=colors)\n",
    "ax3.set_ylabel('Memory (GB)')\n",
    "ax3.set_title('Memory Usage Comparison')\n",
    "ax3.tick_params(axis='x', rotation=15)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "speedups = [baseline['time']/results[m]['time'] for m in methods]\n",
    "ax4.bar(methods, speedups, color=colors)\n",
    "ax4.axhline(y=1.0, color='r', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax4.set_ylabel('Speedup vs Baseline')\n",
    "ax4.set_title('Overall Speedup')\n",
    "ax4.tick_params(axis='x', rotation=15)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cost savings\n",
    "def calculate_cost_savings(\n",
    "    baseline_throughput: float,\n",
    "    optimized_throughput: float,\n",
    "    gpu_cost_per_hour: float = 3.0,  # A100 cost\n",
    "    tokens_per_day: int = 1_000_000,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate cost savings from optimization.\n",
    "    \"\"\"\n",
    "    # Hours needed to process tokens\n",
    "    baseline_hours = tokens_per_day / (baseline_throughput * 3600)\n",
    "    optimized_hours = tokens_per_day / (optimized_throughput * 3600)\n",
    "    \n",
    "    # Daily cost\n",
    "    baseline_cost = baseline_hours * gpu_cost_per_hour\n",
    "    optimized_cost = optimized_hours * gpu_cost_per_hour\n",
    "    \n",
    "    savings = baseline_cost - optimized_cost\n",
    "    savings_pct = savings / baseline_cost * 100\n",
    "    \n",
    "    return {\n",
    "        'baseline_cost': baseline_cost,\n",
    "        'optimized_cost': optimized_cost,\n",
    "        'daily_savings': savings,\n",
    "        'monthly_savings': savings * 30,\n",
    "        'savings_pct': savings_pct,\n",
    "    }\n",
    "\n",
    "# Calculate for vLLM\n",
    "print(\"Cost-Benefit Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Assumptions: A100 GPU ($3/hr), 1M tokens/day\\n\")\n",
    "\n",
    "vllm_savings = calculate_cost_savings(\n",
    "    baseline['throughput'],\n",
    "    vllm_throughput,\n",
    ")\n",
    "\n",
    "print(f\"Baseline (HF FP16):\")\n",
    "print(f\"  Daily cost:     ${vllm_savings['baseline_cost']:.2f}\")\n",
    "print(f\"\\nOptimized (vLLM):\")\n",
    "print(f\"  Daily cost:     ${vllm_savings['optimized_cost']:.2f}\")\n",
    "print(f\"  Daily savings:  ${vllm_savings['daily_savings']:.2f} ({vllm_savings['savings_pct']:.1f}%)\")\n",
    "print(f\"  Monthly savings: ${vllm_savings['monthly_savings']:.2f}\")\n",
    "print(f\"  Yearly savings:  ${vllm_savings['monthly_savings']*12:.2f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ’° Significant cost savings with vLLM optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Decision Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision matrix\n",
    "import pandas as pd\n",
    "\n",
    "decision_matrix = pd.DataFrame({\n",
    "    'Optimization': [\n",
    "        'Baseline (HF FP16)',\n",
    "        'PagedAttention (vLLM)',\n",
    "        'Quantization (INT8)',\n",
    "        'Speculative Decoding',\n",
    "        'vLLM + INT8',\n",
    "        'All Combined',\n",
    "    ],\n",
    "    'Speedup': [1.0, 5.0, 1.5, 2.0, 7.5, 15.0],\n",
    "    'Memory': [1.0, 0.6, 0.5, 1.2, 0.3, 0.3],\n",
    "    'Complexity': ['Low', 'Low', 'Medium', 'High', 'Medium', 'High'],\n",
    "    'Quality': [100, 100, 98, 100, 98, 97],\n",
    "    'Use Case': [\n",
    "        'Development',\n",
    "        'Production (recommended)',\n",
    "        'Memory-limited',\n",
    "        'Latency-critical',\n",
    "        'High-throughput',\n",
    "        'Maximum performance',\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nOptimization Decision Matrix\")\n",
    "print(\"=\" * 100)\n",
    "print(decision_matrix.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"  ðŸ¥‡ Production: vLLM (easy + effective)\")\n",
    "print(\"  ðŸ¥ˆ Memory-limited: vLLM + INT8\")\n",
    "print(\"  ðŸ¥‰ Extreme performance: All techniques combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "âœ… **Completed Lab-2.2**:\n",
    "1. Analyzed KV Cache optimization\n",
    "2. Implemented Speculative Decoding\n",
    "3. Applied quantization techniques\n",
    "4. Combined multiple optimizations\n",
    "5. Performed cost-benefit analysis\n",
    "\n",
    "ðŸ“Š **Key Achievements**:\n",
    "- vLLM: 5-10x throughput improvement\n",
    "- Quantization: 2x memory reduction\n",
    "- Speculative Decoding: 1.5-3x latency reduction\n",
    "- Combined: 10-20x overall improvement\n",
    "\n",
    "ðŸ’¡ **Best Practices**:\n",
    "1. **Start simple**: Use vLLM first (biggest impact)\n",
    "2. **Add quantization**: If memory-constrained\n",
    "3. **Advanced techniques**: Only if needed (Speculative Decoding)\n",
    "4. **Monitor quality**: Always validate output quality\n",
    "5. **Measure everything**: Benchmark before and after\n",
    "\n",
    "ðŸŽ“ **Skills Mastered**:\n",
    "- KV Cache management\n",
    "- PagedAttention principles\n",
    "- Speculative Decoding implementation\n",
    "- Quantization strategies\n",
    "- Performance optimization methodology\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **Lab-2.3**: FastAPI Service Construction\n",
    "- **Lab-2.4**: Production Environment Deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)\n",
    "- [GPTQ Paper](https://arxiv.org/abs/2210.17323)\n",
    "- [AWQ Paper](https://arxiv.org/abs/2306.00978)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "import gc\n",
    "\n",
    "del hf_model, hf_int8, vllm_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ‰ Congratulations! Lab-2.2 Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nYou've mastered advanced inference optimization techniques!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
