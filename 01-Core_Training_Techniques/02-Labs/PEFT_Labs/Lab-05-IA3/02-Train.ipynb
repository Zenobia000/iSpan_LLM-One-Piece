{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: (IA)Â³ - Fine-Tuning a GPT-2 Model with Extreme Parameter Efficiency\n",
        "---\n",
        "## Notebook 2: The Training Process\n",
        "\n",
        "**Goal:** In this notebook, you will fine-tune a `gpt2` model to generate positive movie reviews using **(IA)Â³**. You will see firsthand how few parameters are needed for this method to be effective.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Load and prepare the `imdb` dataset.\n",
        "-   Load a pre-trained GPT-2 model.\n",
        "-   Deeply understand and configure `peft.IA3Config`.\n",
        "-   Apply (IA)Â³ scaling vectors to the model.\n",
        "-   Fine-tune the model by training *only* the (IA)Â³ vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Dataset and Preprocess\n",
        "\n",
        "This step is identical to the Prefix Tuning lab. We will load the `imdb` dataset, filter for positive reviews, and tokenize the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Load and Filter Dataset ---\n",
        "dataset = load_dataset(\"imdb\", split=\"train[:500]\")\n",
        "positive_dataset = dataset.filter(lambda example: example[\"label\"] == 1)\n",
        "positive_dataset = positive_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def preprocess_function(examples):\n",
        "    outputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "    outputs[\"labels\"] = outputs[\"input_ids\"]\n",
        "    return outputs\n",
        "\n",
        "# --- Apply Preprocessing ---\n",
        "tokenized_datasets = positive_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"âœ… Dataset loaded and preprocessed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load the Base Model\n",
        "\n",
        "We load the standard `gpt2` model for causal language modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "print(\"âœ… Base GPT-2 model loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Configure (IA)Â³\n",
        "\n",
        "Here we configure (IA)Â³. Instead of adding new layers or reparameterizing weight matrices, (IA)Â³ learns vectors that scale the internal activations of the model. This is an extremely lightweight approach.\n",
        "\n",
        "#### Key Hugging Face `peft` Components:\n",
        "\n",
        "-   `peft.IA3Config`: The configuration class for this method.\n",
        "    -   `task_type=\"CAUSAL_LM\"`: We specify the task type.\n",
        "    -   `target_modules`: This is crucial for (IA)Â³. The original paper found that the most effective approach is to apply the scaling vectors to the **key** and **value** projections in the attention layers, and to the **feed-forward** network. For a `gpt2` model in `transformers`, these are typically named `c_attn` (which handles Q, K, and V) and `mlp.c_proj` or similar. We will target these.\n",
        "    -   `feedforward_modules`: This tells `peft` which modules are part of the feed-forward network, which is necessary for (IA)Â³ to work correctly. For GPT-2, this is `mlp.c_proj`.\n",
        "-   `peft.get_peft_model`: Applies the configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_model, IA3Config, TaskType\n",
        "\n",
        "# --- (IA)Â³ Configuration ---\n",
        "ia3_config = IA3Config(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\"c_attn\", \"mlp.c_proj\"],\n",
        "    feedforward_modules=[\"mlp.c_proj\"],\n",
        ")\n",
        "\n",
        "# --- Create PeftModel ---\n",
        "peft_model = get_peft_model(model, ia3_config)\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "# Notice how incredibly small the number of trainable parameters is!\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Set Up Training\n",
        "\n",
        "The final step is to configure and run the training process. This is identical to the setup in the Prefix Tuning lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-ia3-imdb\",\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # A higher learning rate can be effective for these lightweight methods\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# --- Create Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"ðŸš€ Starting training with (IA)Â³...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
