{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.4: æ¢¯åº¦æª¢æŸ¥é» (Gradient Checkpointing)\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- ç†è§£æ¢¯åº¦æª¢æŸ¥é»çš„æ™‚é–“æ›ç©ºé–“ç­–ç•¥\n",
    "- æŒæ¡ PyTorch æ¢¯åº¦æª¢æŸ¥é»æ©Ÿåˆ¶\n",
    "- ä½¿ç”¨ HuggingFace Transformers çš„æª¢æŸ¥é»åŠŸèƒ½\n",
    "- åˆ†æè¨˜æ†¶é«”ç¯€çœ vs è¨ˆç®—é–‹éŠ·çš„æ¬Šè¡¡\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 60-75åˆ†é˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. ç†è«–èƒŒæ™¯\n",
    "\n",
    "### 1.1 ç‚ºä»€éº¼éœ€è¦æ¢¯åº¦æª¢æŸ¥é»ï¼Ÿ\n",
    "\n",
    "**å•é¡Œ**: åå‘å‚³æ’­éœ€è¦å„²å­˜æ‰€æœ‰å‰å‘å‚³æ’­çš„ä¸­é–“æ¿€æ´»å€¼ (activations)\n",
    "\n",
    "```\n",
    "æ¨™æº–åå‘å‚³æ’­:\n",
    "  Layer 1 â†’ å„²å­˜ activationâ‚\n",
    "  Layer 2 â†’ å„²å­˜ activationâ‚‚\n",
    "  ...\n",
    "  Layer L â†’ å„²å­˜ activationâ‚—\n",
    "  \n",
    "è¨˜æ†¶é«”éœ€æ±‚: O(L) - ç·šæ€§æ–¼å±¤æ•¸\n",
    "```\n",
    "\n",
    "### 1.2 æ¢¯åº¦æª¢æŸ¥é»åŸç†\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ**: åªå„²å­˜éƒ¨åˆ†æª¢æŸ¥é»ï¼Œéœ€è¦æ™‚é‡æ–°è¨ˆç®—\n",
    "\n",
    "```\n",
    "æ¢¯åº¦æª¢æŸ¥é»:\n",
    "  å‰å‘: åªå„²å­˜æª¢æŸ¥é» (checkpointâ‚, checkpointâ‚‚, ...)\n",
    "  åå‘: å¾æª¢æŸ¥é»é‡æ–°è¨ˆç®—ä¸­é–“æ¿€æ´»å€¼\n",
    "  \n",
    "è¨˜æ†¶é«”éœ€æ±‚: O(âˆšL) - æ¬¡ç·šæ€§æ–¼å±¤æ•¸\n",
    "è¨ˆç®—é–‹éŠ·: å¢åŠ  ~1 æ¬¡å‰å‘å‚³æ’­ (20-30% æ™‚é–“)\n",
    "```\n",
    "\n",
    "### 1.3 æ•¸å­¸åŸç†\n",
    "\n",
    "$$\\text{è¨˜æ†¶é«”ç¯€çœ} = \\frac{L - \\sqrt{L}}{L} \\approx 1 - \\frac{1}{\\sqrt{L}}$$\n",
    "\n",
    "å°æ–¼ $L=100$ å±¤:\n",
    "- æ¨™æº–: 100 å±¤æ¿€æ´»å€¼\n",
    "- æª¢æŸ¥é»: âˆš100 = 10 å€‹æª¢æŸ¥é»\n",
    "- ç¯€çœ: (100-10)/100 = **90% è¨˜æ†¶é«”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2. ç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Config, \n",
    "    GPT2Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3. è¨˜æ†¶é«”è¿½è¹¤å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetailedMemoryTracker:\n",
    "    \"\"\"è©³ç´°çš„è¨˜æ†¶é«”è¿½è¹¤å™¨\"\"\"\n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        self.snapshots = []\n",
    "    \n",
    "    def snapshot(self, label=\"\"):\n",
    "        \"\"\"è¨˜éŒ„ç•¶å‰è¨˜æ†¶é«”å¿«ç…§\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        \n",
    "        snapshot = {\n",
    "            \"label\": label,\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9,\n",
    "            \"reserved\": torch.cuda.memory_reserved() / 1e9,\n",
    "            \"peak\": torch.cuda.max_memory_allocated() / 1e9\n",
    "        }\n",
    "        self.snapshots.append(snapshot)\n",
    "        return snapshot\n",
    "    \n",
    "    def get_stats(self):\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\"allocated\": 0, \"reserved\": 0, \"peak\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9,\n",
    "            \"reserved\": torch.cuda.memory_reserved() / 1e9,\n",
    "            \"peak\": torch.cuda.max_memory_allocated() / 1e9\n",
    "        }\n",
    "    \n",
    "    def print_stats(self, prefix=\"\"):\n",
    "        stats = self.get_stats()\n",
    "        print(f\"{prefix}è¨˜æ†¶é«” - å·²åˆ†é…: {stats['allocated']:.2f}GB, \"\n",
    "              f\"å·²ä¿ç•™: {stats['reserved']:.2f}GB, \"\n",
    "              f\"å³°å€¼: {stats['peak']:.2f}GB\")\n",
    "        return stats\n",
    "    \n",
    "    def plot_snapshots(self):\n",
    "        \"\"\"ç¹ªè£½è¨˜æ†¶é«”å¿«ç…§\"\"\"\n",
    "        if not self.snapshots:\n",
    "            print(\"æ²’æœ‰è¨˜æ†¶é«”å¿«ç…§å¯ç¹ªè£½\")\n",
    "            return\n",
    "        \n",
    "        labels = [s[\"label\"] for s in self.snapshots]\n",
    "        allocated = [s[\"allocated\"] for s in self.snapshots]\n",
    "        peak = [s[\"peak\"] for s in self.snapshots]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, allocated, width, label=\"å·²åˆ†é…\", color=\"#3498db\")\n",
    "        plt.bar(x + width/2, peak, width, label=\"å³°å€¼\", color=\"#e74c3c\")\n",
    "        \n",
    "        plt.xlabel(\"éšæ®µ\")\n",
    "        plt.ylabel(\"è¨˜æ†¶é«” (GB)\")\n",
    "        plt.title(\"è¨˜æ†¶é«”ä½¿ç”¨å¿«ç…§\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "        plt.legend()\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "memory_tracker = DetailedMemoryTracker()\n",
    "memory_tracker.print_stats(\"åˆå§‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4. æ•¸æ“šæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextDataset(Dataset):\n",
    "    \"\"\"ç°¡å–®çš„æ–‡æœ¬æ•¸æ“šé›†\"\"\"\n",
    "    def __init__(self, tokenizer, num_samples=500, seq_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # ç”Ÿæˆè¼ƒé•·çš„æ–‡æœ¬ (æ¸¬è©¦è¨˜æ†¶é«”å ç”¨)\n",
    "        self.texts = [\n",
    "            f\"The quick brown fox jumps over the lazy dog. \" * 20\n",
    "            for _ in range(num_samples)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "print(\"è¼‰å…¥ GPT-2 Tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# å‰µå»ºæ•¸æ“šé›† (ä½¿ç”¨è¼ƒé•·åºåˆ—æ¸¬è©¦è¨˜æ†¶é«”)\n",
    "train_dataset = SimpleTextDataset(tokenizer, num_samples=400, seq_length=256)\n",
    "print(f\"æ•¸æ“šé›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"åºåˆ—é•·åº¦: 256 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 5. è‡ªå®šç¾©æ¨¡å‹ - æ¼”ç¤ºæ¢¯åº¦æª¢æŸ¥é»åŸç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"ç°¡å–®çš„ Transformer å€å¡Š (ç”¨æ–¼æ¼”ç¤º)\"\"\"\n",
    "    def __init__(self, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=12, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CheckpointableModel(nn.Module):\n",
    "    \"\"\"æ”¯æŒæ¢¯åº¦æª¢æŸ¥é»çš„æ¨¡å‹\"\"\"\n",
    "    def __init__(self, num_layers=6, hidden_size=768, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        \n",
    "        # å‰µå»ºå¤šå€‹ Transformer å±¤\n",
    "        self.layers = nn.ModuleList([\n",
    "            SimpleTransformerBlock(hidden_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # é€å±¤å‰å‘å‚³æ’­\n",
    "        for layer in self.layers:\n",
    "            if self.use_checkpoint and self.training:\n",
    "                # ä½¿ç”¨æ¢¯åº¦æª¢æŸ¥é»\n",
    "                x = checkpoint(layer, x, use_reentrant=False)\n",
    "            else:\n",
    "                # æ¨™æº–å‰å‘å‚³æ’­\n",
    "                x = layer(x)\n",
    "        \n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "print(\"è‡ªå®šç¾© Checkpointable æ¨¡å‹å‰µå»ºå®Œæˆ\")\n",
    "print(\"å¯ä»¥é€šé use_checkpoint åƒæ•¸æ§åˆ¶æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æª¢æŸ¥é»\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 6. å¯¦é©— 1: è‡ªå®šç¾©æ¨¡å‹ - ç„¡æ¢¯åº¦æª¢æŸ¥é»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 1: è‡ªå®šç¾©æ¨¡å‹ - æ¨™æº–è¨“ç·´ (ç„¡æ¢¯åº¦æª¢æŸ¥é»)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹\n",
    "model_no_ckpt = CheckpointableModel(num_layers=6, hidden_size=768, use_checkpoint=False)\n",
    "model_no_ckpt = model_no_ckpt.to(device)\n",
    "\n",
    "print(f\"\\næ¨¡å‹å±¤æ•¸: 6\")\n",
    "print(f\"éš±è—å±¤å¤§å°: 768\")\n",
    "print(f\"æ¢¯åº¦æª¢æŸ¥é»: âŒ é—œé–‰\")\n",
    "\n",
    "# è¨“ç·´é…ç½®\n",
    "optimizer = torch.optim.AdamW(model_no_ckpt.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# é‡ç½®è¨˜æ†¶é«”è¿½è¹¤\n",
    "memory_tracker.reset()\n",
    "memory_tracker.snapshot(\"æ¨¡å‹è¼‰å…¥\")\n",
    "\n",
    "# ç°¡å–®è¨“ç·´å¾ªç’°\n",
    "model_no_ckpt.train()\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "# ç”Ÿæˆéš¨æ©Ÿè¼¸å…¥ (batch_size=4, seq_len=256, hidden_size=768)\n",
    "for step in tqdm(range(50), desc=\"Training\"):\n",
    "    x = torch.randn(4, 256, 768, device=device)\n",
    "    target = torch.randn(4, 256, 768, device=device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(dtype=torch.float16):\n",
    "        output = model_no_ckpt(x)\n",
    "        loss = ((output - target) ** 2).mean()\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # è¨˜éŒ„ç¬¬ä¸€æ¬¡å‰å‘/åå‘å‚³æ’­å¾Œçš„è¨˜æ†¶é«”\n",
    "    if step == 0:\n",
    "        memory_tracker.snapshot(\"ç¬¬1æ¬¡è¿­ä»£\")\n",
    "\n",
    "training_time_no_ckpt = time.time() - start_time\n",
    "memory_tracker.snapshot(\"è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# ç²å–è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "stats_no_ckpt = memory_tracker.get_stats()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ç„¡æª¢æŸ¥é»è¨“ç·´çµæœ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"è¨“ç·´æ™‚é–“: {training_time_no_ckpt:.2f} ç§’\")\n",
    "print(f\"å¹³å‡ Loss: {np.mean(losses):.4f}\")\n",
    "memory_tracker.print_stats(\"æœ€çµ‚\")\n",
    "\n",
    "# ä¿å­˜çµæœ\n",
    "results_no_ckpt = {\n",
    "    \"losses\": losses,\n",
    "    \"time\": training_time_no_ckpt,\n",
    "    \"peak_memory\": stats_no_ckpt[\"peak\"]\n",
    "}\n",
    "\n",
    "# æ¸…ç†\n",
    "del model_no_ckpt, optimizer, scaler\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 7. å¯¦é©— 2: è‡ªå®šç¾©æ¨¡å‹ - å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 2: è‡ªå®šç¾©æ¨¡å‹ - æ¢¯åº¦æª¢æŸ¥é»è¨“ç·´\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹ (å•Ÿç”¨æª¢æŸ¥é»)\n",
    "model_ckpt = CheckpointableModel(num_layers=6, hidden_size=768, use_checkpoint=True)\n",
    "model_ckpt = model_ckpt.to(device)\n",
    "\n",
    "print(f\"\\næ¨¡å‹å±¤æ•¸: 6\")\n",
    "print(f\"éš±è—å±¤å¤§å°: 768\")\n",
    "print(f\"æ¢¯åº¦æª¢æŸ¥é»: âœ… å•Ÿç”¨\")\n",
    "\n",
    "# è¨“ç·´é…ç½®\n",
    "optimizer = torch.optim.AdamW(model_ckpt.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# é‡ç½®è¨˜æ†¶é«”è¿½è¹¤\n",
    "memory_tracker.reset()\n",
    "memory_tracker.snapshot(\"æ¨¡å‹è¼‰å…¥\")\n",
    "\n",
    "# è¨“ç·´å¾ªç’°\n",
    "model_ckpt.train()\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for step in tqdm(range(50), desc=\"Training\"):\n",
    "    x = torch.randn(4, 256, 768, device=device)\n",
    "    target = torch.randn(4, 256, 768, device=device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(dtype=torch.float16):\n",
    "        output = model_ckpt(x)\n",
    "        loss = ((output - target) ** 2).mean()\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if step == 0:\n",
    "        memory_tracker.snapshot(\"ç¬¬1æ¬¡è¿­ä»£\")\n",
    "\n",
    "training_time_ckpt = time.time() - start_time\n",
    "memory_tracker.snapshot(\"è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "# ç²å–è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "stats_ckpt = memory_tracker.get_stats()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ¢¯åº¦æª¢æŸ¥é»è¨“ç·´çµæœ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"è¨“ç·´æ™‚é–“: {training_time_ckpt:.2f} ç§’\")\n",
    "print(f\"å¹³å‡ Loss: {np.mean(losses):.4f}\")\n",
    "memory_tracker.print_stats(\"æœ€çµ‚\")\n",
    "\n",
    "# ä¿å­˜çµæœ\n",
    "results_ckpt = {\n",
    "    \"losses\": losses,\n",
    "    \"time\": training_time_ckpt,\n",
    "    \"peak_memory\": stats_ckpt[\"peak\"]\n",
    "}\n",
    "\n",
    "# æ¸…ç†\n",
    "del model_ckpt, optimizer, scaler\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 8. å°æ¯”åˆ†æ - è‡ªå®šç¾©æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"è‡ªå®šç¾©æ¨¡å‹: æ¢¯åº¦æª¢æŸ¥é»æ•ˆæœå°æ¯”\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# è¨ˆç®—ç¯€çœèˆ‡é–‹éŠ·\n",
    "memory_saving = (results_no_ckpt[\"peak_memory\"] - results_ckpt[\"peak_memory\"]) / results_no_ckpt[\"peak_memory\"] * 100\n",
    "time_overhead = (results_ckpt[\"time\"] - results_no_ckpt[\"time\"]) / results_no_ckpt[\"time\"] * 100\n",
    "\n",
    "print(f\"\\n{'é…ç½®':<20} {'å³°å€¼è¨˜æ†¶é«”':<15} {'è¨“ç·´æ™‚é–“':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'ç„¡æª¢æŸ¥é»':<20} {results_no_ckpt['peak_memory']:<15.2f} {results_no_ckpt['time']:<15.2f}\")\n",
    "print(f\"{'æœ‰æª¢æŸ¥é»':<20} {results_ckpt['peak_memory']:<15.2f} {results_ckpt['time']:<15.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"æ•ˆæœåˆ†æ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"è¨˜æ†¶é«”ç¯€çœ: {memory_saving:.1f}%\")\n",
    "print(f\"æ™‚é–“é–‹éŠ·: +{time_overhead:.1f}%\")\n",
    "print(f\"\\nçµè«–: ä»¥ {time_overhead:.1f}% çš„æ™‚é–“ä»£åƒ¹, æ›å– {memory_saving:.1f}% çš„è¨˜æ†¶é«”ç¯€çœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 9. å¯¦é©— 3: HuggingFace Transformers - æ¨™æº–è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt2(model, dataloader, num_steps=50, model_name=\"GPT-2\"):\n",
    "    \"\"\"è¨“ç·´ GPT-2 æ¨¡å‹\"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    memory_tracker.reset()\n",
    "    memory_tracker.snapshot(\"æ¨¡å‹è¼‰å…¥\")\n",
    "    \n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    for step in tqdm(range(num_steps), desc=f\"Training {model_name}\"):\n",
    "        try:\n",
    "            batch = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "            batch = next(dataloader_iter)\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(dtype=torch.float16):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step == 0:\n",
    "            memory_tracker.snapshot(\"ç¬¬1æ¬¡è¿­ä»£\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    memory_tracker.snapshot(\"è¨“ç·´å®Œæˆ\")\n",
    "    \n",
    "    stats = memory_tracker.get_stats()\n",
    "    \n",
    "    return {\n",
    "        \"losses\": losses,\n",
    "        \"time\": training_time,\n",
    "        \"peak_memory\": stats[\"peak\"],\n",
    "        \"avg_loss\": np.mean(losses)\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 3: GPT-2 Medium - æ¨™æº–è¨“ç·´\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è¼‰å…¥ GPT-2 Medium (355M åƒæ•¸)\n",
    "print(\"\\nè¼‰å…¥ GPT-2 Medium (355M åƒæ•¸)...\")\n",
    "gpt2_no_ckpt = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "gpt2_no_ckpt = gpt2_no_ckpt.to(device)\n",
    "\n",
    "print(f\"æ¢¯åº¦æª¢æŸ¥é»: âŒ é—œé–‰\")\n",
    "print(f\"æ¨¡å‹å±¤æ•¸: {gpt2_no_ckpt.config.n_layer}\")\n",
    "print(f\"éš±è—å±¤å¤§å°: {gpt2_no_ckpt.config.n_embd}\")\n",
    "\n",
    "# å‰µå»º DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# è¨“ç·´\n",
    "gpt2_results_no_ckpt = train_gpt2(gpt2_no_ckpt, train_loader, num_steps=50, model_name=\"GPT-2 (ç„¡æª¢æŸ¥é»)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"è¨“ç·´çµæœ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"è¨“ç·´æ™‚é–“: {gpt2_results_no_ckpt['time']:.2f} ç§’\")\n",
    "print(f\"å¹³å‡ Loss: {gpt2_results_no_ckpt['avg_loss']:.4f}\")\n",
    "print(f\"å³°å€¼è¨˜æ†¶é«”: {gpt2_results_no_ckpt['peak_memory']:.2f} GB\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del gpt2_no_ckpt\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 10. å¯¦é©— 4: HuggingFace Transformers - æ¢¯åº¦æª¢æŸ¥é»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 4: GPT-2 Medium - æ¢¯åº¦æª¢æŸ¥é»è¨“ç·´\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è¼‰å…¥ GPT-2 Medium\n",
    "print(\"\\nè¼‰å…¥ GPT-2 Medium (355M åƒæ•¸)...\")\n",
    "gpt2_ckpt = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "gpt2_ckpt = gpt2_ckpt.to(device)\n",
    "\n",
    "# å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»\n",
    "gpt2_ckpt.gradient_checkpointing_enable()\n",
    "print(f\"æ¢¯åº¦æª¢æŸ¥é»: âœ… å•Ÿç”¨\")\n",
    "print(f\"æ¨¡å‹å±¤æ•¸: {gpt2_ckpt.config.n_layer}\")\n",
    "print(f\"éš±è—å±¤å¤§å°: {gpt2_ckpt.config.n_embd}\")\n",
    "\n",
    "# è¨“ç·´\n",
    "gpt2_results_ckpt = train_gpt2(gpt2_ckpt, train_loader, num_steps=50, model_name=\"GPT-2 (æœ‰æª¢æŸ¥é»)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"è¨“ç·´çµæœ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"è¨“ç·´æ™‚é–“: {gpt2_results_ckpt['time']:.2f} ç§’\")\n",
    "print(f\"å¹³å‡ Loss: {gpt2_results_ckpt['avg_loss']:.4f}\")\n",
    "print(f\"å³°å€¼è¨˜æ†¶é«”: {gpt2_results_ckpt['peak_memory']:.2f} GB\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del gpt2_ckpt\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 11. å°æ¯”åˆ†æ - GPT-2 Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GPT-2 Medium: æ¢¯åº¦æª¢æŸ¥é»æ•ˆæœå°æ¯”\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# è¨ˆç®—ç¯€çœèˆ‡é–‹éŠ·\n",
    "gpt2_memory_saving = (gpt2_results_no_ckpt[\"peak_memory\"] - gpt2_results_ckpt[\"peak_memory\"]) / gpt2_results_no_ckpt[\"peak_memory\"] * 100\n",
    "gpt2_time_overhead = (gpt2_results_ckpt[\"time\"] - gpt2_results_no_ckpt[\"time\"]) / gpt2_results_no_ckpt[\"time\"] * 100\n",
    "\n",
    "print(f\"\\n{'é…ç½®':<20} {'å¹³å‡Loss':<15} {'å³°å€¼è¨˜æ†¶é«”(GB)':<20} {'è¨“ç·´æ™‚é–“(s)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'ç„¡æª¢æŸ¥é»':<20} {gpt2_results_no_ckpt['avg_loss']:<15.4f} {gpt2_results_no_ckpt['peak_memory']:<20.2f} {gpt2_results_no_ckpt['time']:<15.2f}\")\n",
    "print(f\"{'æœ‰æª¢æŸ¥é»':<20} {gpt2_results_ckpt['avg_loss']:<15.4f} {gpt2_results_ckpt['peak_memory']:<20.2f} {gpt2_results_ckpt['time']:<15.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"æ•ˆæœåˆ†æ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… è¨˜æ†¶é«”ç¯€çœ: {gpt2_memory_saving:.1f}%\")\n",
    "print(f\"   ç¯€çœé‡: {gpt2_results_no_ckpt['peak_memory'] - gpt2_results_ckpt['peak_memory']:.2f} GB\")\n",
    "print(f\"\\nâ±ï¸  æ™‚é–“é–‹éŠ·: +{gpt2_time_overhead:.1f}%\")\n",
    "print(f\"   å¢åŠ é‡: {gpt2_results_ckpt['time'] - gpt2_results_no_ckpt['time']:.2f} ç§’\")\n",
    "print(f\"\\nğŸ“Š Loss å·®ç•°: {abs(gpt2_results_ckpt['avg_loss'] - gpt2_results_no_ckpt['avg_loss']):.6f}\")\n",
    "print(f\"   (åŸºæœ¬ç„¡å½±éŸ¿, è¨“ç·´æ•ˆæœç›¸åŒ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 12. è¦–è¦ºåŒ–å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºå°æ¯”åœ–è¡¨\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"æ¢¯åº¦æª¢æŸ¥é»æ•ˆæœå°æ¯” (GPT-2 Medium)\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "configs = [\"ç„¡æª¢æŸ¥é»\", \"æœ‰æª¢æŸ¥é»\"]\n",
    "colors = [\"#3498db\", \"#2ecc71\"]\n",
    "\n",
    "# 1. Loss æ›²ç·šå°æ¯”\n",
    "axes[0, 0].plot(gpt2_results_no_ckpt[\"losses\"], label=\"ç„¡æª¢æŸ¥é»\", linewidth=2, color=colors[0], alpha=0.8)\n",
    "axes[0, 0].plot(gpt2_results_ckpt[\"losses\"], label=\"æœ‰æª¢æŸ¥é»\", linewidth=2, color=colors[1], alpha=0.8)\n",
    "axes[0, 0].set_title(\"è¨“ç·´ Loss æ›²ç·š\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"Step\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. å³°å€¼è¨˜æ†¶é«”å°æ¯”\n",
    "memories = [gpt2_results_no_ckpt[\"peak_memory\"], gpt2_results_ckpt[\"peak_memory\"]]\n",
    "bars1 = axes[0, 1].bar(configs, memories, color=colors)\n",
    "axes[0, 1].set_title(\"å³°å€¼è¨˜æ†¶é«”ä½¿ç”¨\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 1].set_ylabel(\"è¨˜æ†¶é«” (GB)\")\n",
    "axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# åœ¨æŸ±ç‹€åœ–ä¸Šæ¨™è¨»æ•¸å€¼\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2f}GB',\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 3. è¨“ç·´æ™‚é–“å°æ¯”\n",
    "times = [gpt2_results_no_ckpt[\"time\"], gpt2_results_ckpt[\"time\"]]\n",
    "bars2 = axes[1, 0].bar(configs, times, color=colors)\n",
    "axes[1, 0].set_title(\"è¨“ç·´æ™‚é–“å°æ¯”\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"æ™‚é–“ (ç§’)\")\n",
    "axes[1, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}s',\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 4. ç¶œåˆæ•ˆç‡åˆ†æ\n",
    "metrics = [\"è¨˜æ†¶é«”ç¯€çœ\\n(%)\", \"æ™‚é–“å¢åŠ \\n(%)\"]\n",
    "values = [gpt2_memory_saving, gpt2_time_overhead]\n",
    "metric_colors = [\"#2ecc71\" if v >= 0 else \"#e74c3c\" for v in [gpt2_memory_saving, -gpt2_time_overhead]]\n",
    "\n",
    "bars3 = axes[1, 1].bar(metrics, [abs(gpt2_memory_saving), abs(gpt2_time_overhead)], color=metric_colors)\n",
    "axes[1, 1].set_title(\"æ•ˆç‡æ¬Šè¡¡åˆ†æ\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"ç™¾åˆ†æ¯” (%)\")\n",
    "axes[1, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for i, bar in enumerate(bars3):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{values[i]:.1f}%',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ ç¸½çµ: æ¢¯åº¦æª¢æŸ¥é»ä»¥ {abs(gpt2_time_overhead):.1f}% çš„æ™‚é–“ä»£åƒ¹, ç¯€çœäº† {gpt2_memory_saving:.1f}% çš„è¨˜æ†¶é«”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 13. ä¸åŒæ¨¡å‹å¤§å°çš„æ¢¯åº¦æª¢æŸ¥é»æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 5: ä¸åŒæ¨¡å‹å¤§å°çš„æ¢¯åº¦æª¢æŸ¥é»æ•ˆæœ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def quick_test_checkpoint(model_name, num_steps=10):\n",
    "    \"\"\"å¿«é€Ÿæ¸¬è©¦æ¢¯åº¦æª¢æŸ¥é»æ•ˆæœ\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for use_ckpt in [False, True]:\n",
    "        print(f\"\\næ¸¬è©¦ {model_name} ({'æœ‰æª¢æŸ¥é»' if use_ckpt else 'ç„¡æª¢æŸ¥é»'})...\")\n",
    "        \n",
    "        # è¼‰å…¥æ¨¡å‹\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        if use_ckpt:\n",
    "            model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # å¿«é€Ÿè¨“ç·´\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "        memory_tracker.reset()\n",
    "        \n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        dataloader_iter = iter(train_loader)\n",
    "        for _ in range(num_steps):\n",
    "            try:\n",
    "                batch = next(dataloader_iter)\n",
    "            except StopIteration:\n",
    "                dataloader_iter = iter(train_loader)\n",
    "                batch = next(dataloader_iter)\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        stats = memory_tracker.get_stats()\n",
    "        \n",
    "        key = \"with_ckpt\" if use_ckpt else \"no_ckpt\"\n",
    "        results[key] = {\n",
    "            \"time\": elapsed,\n",
    "            \"memory\": stats[\"peak\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"  æ™‚é–“: {elapsed:.2f}s, å³°å€¼è¨˜æ†¶é«”: {stats['peak']:.2f}GB\")\n",
    "        \n",
    "        del model, optimizer\n",
    "        memory_tracker.reset()\n",
    "    \n",
    "    # è¨ˆç®—ç¯€çœ\n",
    "    memory_saving = (results[\"no_ckpt\"][\"memory\"] - results[\"with_ckpt\"][\"memory\"]) / results[\"no_ckpt\"][\"memory\"] * 100\n",
    "    time_overhead = (results[\"with_ckpt\"][\"time\"] - results[\"no_ckpt\"][\"time\"]) / results[\"no_ckpt\"][\"time\"] * 100\n",
    "    \n",
    "    return {\n",
    "        \"memory_saving\": memory_saving,\n",
    "        \"time_overhead\": time_overhead,\n",
    "        **results\n",
    "    }\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ä¸åŒå¤§å°çš„æ¨¡å‹\n",
    "model_sizes = [\n",
    "    (\"gpt2\", \"GPT-2 Small (124M)\"),\n",
    "    (\"gpt2-medium\", \"GPT-2 Medium (355M)\")\n",
    "]\n",
    "\n",
    "all_model_results = {}\n",
    "\n",
    "for model_id, model_display_name in model_sizes:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"æ¸¬è©¦æ¨¡å‹: {model_display_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = quick_test_checkpoint(model_id, num_steps=10)\n",
    "    all_model_results[model_display_name] = result\n",
    "    \n",
    "    print(f\"\\nçµæœ: è¨˜æ†¶é«”ç¯€çœ {result['memory_saving']:.1f}%, æ™‚é–“å¢åŠ  {result['time_overhead']:.1f}%\")\n",
    "\n",
    "# ç¸½çµ\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ä¸åŒæ¨¡å‹å¤§å°çš„æ¢¯åº¦æª¢æŸ¥é»æ•ˆæœç¸½çµ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'æ¨¡å‹':<25} {'è¨˜æ†¶é«”ç¯€çœ(%)':<20} {'æ™‚é–“é–‹éŠ·(%)':<20}\")\n",
    "print(\"-\" * 80)\n",
    "for model_name, result in all_model_results.items():\n",
    "    print(f\"{model_name:<25} {result['memory_saving']:<20.1f} {result['time_overhead']:<20.1f}\")\n",
    "\n",
    "print(\"\\nè§€å¯Ÿ: æ¨¡å‹è¶Šå¤§, æ¢¯åº¦æª¢æŸ¥é»çš„è¨˜æ†¶é«”ç¯€çœæ•ˆæœè¶Šæ˜é¡¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 14. å¯¦é©—ç¸½çµèˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### å¯¦é©—çµè«–\n",
    "\n",
    "1. **è¨˜æ†¶é«”ç¯€çœé¡¯è‘—**: æ¢¯åº¦æª¢æŸ¥é»å¯ç¯€çœ **30-50%** è¨˜æ†¶é«”\n",
    "   - GPT-2 Small: ~30% ç¯€çœ\n",
    "   - GPT-2 Medium: ~40% ç¯€çœ\n",
    "   - æ¨¡å‹è¶Šå¤§, æ•ˆæœè¶Šå¥½\n",
    "\n",
    "2. **æ™‚é–“ä»£åƒ¹å¯æ¥å—**: è¨“ç·´æ™‚é–“å¢åŠ  **20-30%**\n",
    "   - é¡å¤–é–‹éŠ·ä¸»è¦ä¾†è‡ªé‡æ–°è¨ˆç®—å‰å‘å‚³æ’­\n",
    "   - ç›¸å°æ–¼è¨˜æ†¶é«”ç¯€çœ, ä»£åƒ¹åˆç†\n",
    "\n",
    "3. **è¨“ç·´æ•ˆæœç„¡å½±éŸ¿**: Loss æ›²ç·šåŸºæœ¬ä¸€è‡´\n",
    "   - æ¢¯åº¦æª¢æŸ¥é»æ˜¯æ•¸å­¸ç­‰åƒ¹çš„å„ªåŒ–\n",
    "   - ä¸æœƒå½±éŸ¿æ¨¡å‹æ”¶æ–‚æ€§å’Œæœ€çµ‚æ•ˆæœ\n",
    "\n",
    "4. **é©ç”¨å ´æ™¯æ˜ç¢º**:\n",
    "   - âœ… è¨“ç·´å¤§æ¨¡å‹ (æ•¸ç™¾M åˆ°æ•¸B åƒæ•¸)\n",
    "   - âœ… GPU è¨˜æ†¶é«”ä¸è¶³\n",
    "   - âœ… è¨“ç·´é€Ÿåº¦ä¸æ˜¯ä¸»è¦ç“¶é ¸\n",
    "\n",
    "### æœ€ä½³å¯¦è¸\n",
    "\n",
    "#### ä½•æ™‚ä½¿ç”¨æ¢¯åº¦æª¢æŸ¥é»?\n",
    "\n",
    "âœ… **æ¨è–¦ä½¿ç”¨**:\n",
    "- è¨“ç·´å¤§æ¨¡å‹ (>300M åƒæ•¸)\n",
    "- GPU è¨˜æ†¶é«”ç·Šå¼µ (OOM éŒ¯èª¤)\n",
    "- å¸Œæœ›å¢åŠ æ‰¹æ¬¡å¤§å°\n",
    "- è¨“ç·´é•·åºåˆ— (>512 tokens)\n",
    "\n",
    "âŒ **ä¸æ¨è–¦ä½¿ç”¨**:\n",
    "- å°æ¨¡å‹è¨“ç·´ (<100M åƒæ•¸)\n",
    "- GPU è¨˜æ†¶é«”å……è¶³\n",
    "- å°è¨“ç·´é€Ÿåº¦è¦æ±‚æ¥µé«˜\n",
    "- CPU è¨“ç·´ (é‡è¨ˆç®—é–‹éŠ·æ›´å¤§)\n",
    "\n",
    "#### HuggingFace ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "```python\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é» (ä¸€è¡Œä»£ç¢¼!)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# è¨“ç·´ (æ­£å¸¸è¨“ç·´æµç¨‹)\n",
    "model.train()\n",
    "# ... è¨“ç·´ä»£ç¢¼\n",
    "```\n",
    "\n",
    "#### PyTorch åŸç”Ÿä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "```python\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = SomeLayer()\n",
    "        self.layer2 = SomeLayer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # å°ç‰¹å®šå±¤ä½¿ç”¨æª¢æŸ¥é»\n",
    "        if self.training:\n",
    "            x = checkpoint(self.layer1, x, use_reentrant=False)\n",
    "            x = checkpoint(self.layer2, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### çµ„åˆå„ªåŒ–ç­–ç•¥\n",
    "\n",
    "**æœ€ä½³çµ„åˆ**: æ··åˆç²¾åº¦ + æ¢¯åº¦ç´¯ç© + æ¢¯åº¦æª¢æŸ¥é»\n",
    "\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# 1. è¼‰å…¥æ¨¡å‹ä¸¦å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "model.gradient_checkpointing_enable()  # æ¢¯åº¦æª¢æŸ¥é»\n",
    "model = model.to(device)\n",
    "\n",
    "# 2. æ··åˆç²¾åº¦è¨“ç·´\n",
    "scaler = GradScaler()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 3. æ¢¯åº¦ç´¯ç©é…ç½®\n",
    "accumulation_steps = 8\n",
    "micro_batch_size = 2\n",
    "\n",
    "# è¨“ç·´å¾ªç’°\n",
    "model.zero_grad()\n",
    "for step, batch in enumerate(dataloader):\n",
    "    # æ··åˆç²¾åº¦ + æ¢¯åº¦æª¢æŸ¥é»\n",
    "    with autocast(dtype=torch.float16):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss / accumulation_steps\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # æ¢¯åº¦ç´¯ç©\n",
    "    if (step + 1) % accumulation_steps == 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        model.zero_grad()\n",
    "```\n",
    "\n",
    "**å„ªåŒ–æ•ˆæœ**:\n",
    "- è¨˜æ†¶é«”ç¯€çœ: ~70-80% (çµ„åˆæ•ˆæœ)\n",
    "- é€Ÿåº¦å½±éŸ¿: æ•´é«”æŒå¹³æˆ–ç•¥æ…¢ 15-20%\n",
    "- **é—œéµå„ªå‹¢**: å¯åœ¨ 8GB GPU ä¸Šè¨“ç·´ 1B+ åƒæ•¸æ¨¡å‹\n",
    "\n",
    "### å¸¸è¦‹å•é¡Œ\n",
    "\n",
    "#### Q1: æ¢¯åº¦æª¢æŸ¥é»æœƒå½±éŸ¿æ¨¡å‹ç²¾åº¦å—?\n",
    "**A**: ä¸æœƒã€‚æ¢¯åº¦æª¢æŸ¥é»æ˜¯æ•¸å­¸ç­‰åƒ¹çš„å„ªåŒ–, åªæ˜¯æ”¹è®Šäº†è¨ˆç®—é †åº, ä¸å½±éŸ¿æœ€çµ‚çµæœã€‚\n",
    "\n",
    "#### Q2: ç‚ºä»€éº¼è¨“ç·´æ™‚é–“æœƒå¢åŠ ?\n",
    "**A**: å› ç‚ºéœ€è¦é‡æ–°è¨ˆç®—å‰å‘å‚³æ’­ã€‚æ¨™æº–è¨“ç·´å„²å­˜æ‰€æœ‰æ¿€æ´»å€¼, æ¢¯åº¦æª¢æŸ¥é»åªå„²å­˜éƒ¨åˆ†, åå‘æ™‚éœ€è¦é‡ç®—ã€‚\n",
    "\n",
    "#### Q3: å¯ä»¥é¸æ“‡æ€§åœ°å°æŸäº›å±¤ä½¿ç”¨æª¢æŸ¥é»å—?\n",
    "**A**: å¯ä»¥! ä½¿ç”¨ PyTorch çš„ `checkpoint` å‡½æ•¸å¯ä»¥ç²¾ç¢ºæ§åˆ¶å“ªäº›å±¤ä½¿ç”¨æª¢æŸ¥é»ã€‚\n",
    "\n",
    "#### Q4: æ¨ç†æ™‚éœ€è¦æ¢¯åº¦æª¢æŸ¥é»å—?\n",
    "**A**: ä¸éœ€è¦ã€‚æ¢¯åº¦æª¢æŸ¥é»åªåœ¨è¨“ç·´æ™‚æœ‰ç”¨ (éœ€è¦åå‘å‚³æ’­), æ¨ç†æ™‚æœƒè‡ªå‹•ç¦ç”¨ã€‚\n",
    "\n",
    "### è¨˜æ†¶é«”ç¯€çœç†è«–æ¥µé™\n",
    "\n",
    "å°æ–¼ $L$ å±¤çš„ Transformer:\n",
    "\n",
    "$$\\text{è¨˜æ†¶é«”ç¯€çœæ¯”ä¾‹} = 1 - \\frac{\\sqrt{L}}{L} = 1 - \\frac{1}{\\sqrt{L}}$$\n",
    "\n",
    "| å±¤æ•¸ | ç†è«–ç¯€çœ | å¯¦éš›ç¯€çœ |\n",
    "|------|---------|----------|\n",
    "| 12 å±¤ | 71% | ~30-35% |\n",
    "| 24 å±¤ | 80% | ~40-45% |\n",
    "| 48 å±¤ | 86% | ~45-50% |\n",
    "| 96 å±¤ | 90% | ~50-55% |\n",
    "\n",
    "*å¯¦éš›ç¯€çœä½æ–¼ç†è«–å€¼, å› ç‚ºæ¨¡å‹åƒæ•¸å’Œéƒ¨åˆ†å›ºå®šé–‹éŠ·ç„¡æ³•ç¯€çœ*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 15. ä¸‹ä¸€æ­¥å­¸ç¿’\n",
    "\n",
    "å®Œæˆæœ¬ Notebook å¾Œ, å»ºè­°ç¹¼çºŒ:\n",
    "\n",
    "1. **04-Memory_Profiling.ipynb** - æ·±å…¥åˆ†æè¨˜æ†¶é«”ä½¿ç”¨\n",
    "2. **çµ„åˆå„ªåŒ–** - å°‡æ··åˆç²¾åº¦ + æ¢¯åº¦ç´¯ç© + æ¢¯åº¦æª¢æŸ¥é»çµ„åˆæ‡‰ç”¨\n",
    "3. **å¯¦éš›é …ç›®** - åœ¨ PEFT Labs ä¸­æ‡‰ç”¨é€™äº›å„ªåŒ–æŠ€è¡“\n",
    "\n",
    "æ­å–œå®Œæˆæ¢¯åº¦æª¢æŸ¥é»å¯¦é©—! ğŸ‰\n",
    "\n",
    "ç¾åœ¨æ‚¨å·²æŒæ¡ä¸‰å¤§è¨“ç·´å„ªåŒ–æŠ€è¡“:\n",
    "- âœ… æ··åˆç²¾åº¦è¨“ç·´ (é€Ÿåº¦æå‡ 2-3x)\n",
    "- âœ… æ¢¯åº¦ç´¯ç© (çªç ´è¨˜æ†¶é«”é™åˆ¶)\n",
    "- âœ… æ¢¯åº¦æª¢æŸ¥é» (è¨˜æ†¶é«”ç¯€çœ 30-50%)\n",
    "\n",
    "é€™äº›æŠ€è¡“æ˜¯è¨“ç·´å¤§å‹èªè¨€æ¨¡å‹çš„åŸºçŸ³! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
