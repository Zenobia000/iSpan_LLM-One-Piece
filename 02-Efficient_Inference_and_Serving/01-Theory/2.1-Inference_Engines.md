# 大模型推理技術 2.1 推理引擎 (Inference Engines)

本教學模組深入探討大型語言模型 (LLM) 推理引擎的核心技術，涵蓋從基礎原理到生產應用的完整知識體系。

**🚨 重要說明**: 本課程提供完整理論教學與架構分析，適合單GPU環境學習與小規模實驗。

| 層次 | 學習目標 | 核心內容 | 產出 |
| :--- | :--- | :--- | :--- |
| **Fundamentals** | 理解LLM推理挑戰與引擎設計 | 自回歸生成、KV Cache、記憶體瓶頸 | 掌握推理基本概念 |
| **First Principles** | 掌握PagedAttention與Continuous Batching原理 | 虛擬記憶體管理、動態批次調度 | 理解核心演算法設計 |
| **Body of Knowledge** | 精通主流推理引擎應用 | vLLM、TensorRT-LLM、引擎選型 | 具備生產部署能力 |

---

## 1. Fundamentals (基礎)

### 1.1 LLM 推理的核心挑戰

#### 1.1.1 自回歸生成特性

LLM 採用自回歸 (Autoregressive) 方式逐個生成 token，每個 token 的生成依賴於之前所有 token。

**生成流程**:
```
生成 "Hello World"
Step 1: Input = ""      → Output = "Hello"
Step 2: Input = "Hello" → Output = " World"
```

**推理瓶頸**:
- **序列依賴**: 無法並行生成，必須逐個產生
- **KV Cache 增長**: 每生成一個 token，KV Cache 線性增長
- **記憶體頻寬受限**: 推理過程為 Memory-Bound 而非 Compute-Bound

#### 1.1.2 記憶體頻寬瓶頸

**案例分析**: Llama-2-7B 推理

```
模型參數: 7B × 2 bytes (FP16) = 14GB
單次前向傳播:
  - 計算量: 約 45M FLOPs (矩陣乘法)
  - 記憶體訪問: 14GB (讀取所有參數)

GPU 性能 (A100):
  - 計算能力: 312 TFLOPS (FP16)
  - 記憶體頻寬: 1.5 TB/s
  
時間分析:
  - 計算時間: 45M / 312T ≈ 0.14ms
  - 記憶體訪問時間: 14GB / 1.5TB/s ≈ 9ms
  
結論: 記憶體訪問時間 >> 計算時間 (Memory-Bound)
```

#### 1.1.3 KV Cache 記憶體占用

**KV Cache 計算** (Llama-2-7B 範例):

```python
num_layers = 32      # Transformer 層數
num_heads = 32       # 注意力頭數
head_dim = 128       # 每個頭的維度
batch_size = 16      # 批次大小
seq_len = 2048       # 生成序列長度
precision = 2        # FP16 = 2 bytes

kv_cache_size = (
    2                 # K 和 V
    × batch_size
    × num_layers
    × num_heads
    × seq_len
    × head_dim
    × precision
) / (1024**3)  # 轉為 GB

# 結果: 約 16GB (僅 KV Cache!)
```

**關鍵洞察**:
- KV Cache 隨序列長度線性增長
- 批次推理時記憶體占用巨大
- 成為推理擴展的主要瓶頸

---

## 2. First Principles (原理)

### 2.1 PagedAttention: 虛擬記憶體管理

#### 2.1.1 傳統 KV Cache 的記憶體浪費

**問題**: 靜態記憶體分配導致碎片化

```
傳統方案 (預分配最大長度):
Request 1 (seq_len=1024): 分配 2048 空間，浪費 50%
Request 2 (seq_len=512):  分配 2048 空間，浪費 75%

總記憶體利用率: 約 30-50%
```

#### 2.1.2 PagedAttention 核心設計

**靈感**: 借鑑作業系統虛擬記憶體管理

**核心概念**:
1. **物理塊 (Physical Block)**: 固定大小的 KV Cache 單元 (如 64 tokens)
2. **邏輯頁 (Logical Page)**: 請求的連續位址空間
3. **頁表 (Page Table)**: 邏輯頁到物理塊的映射

**記憶體節省**:
```
PagedAttention:
Request 1 (1024 tokens): 分配 16 個 block (1024/64)
Request 2 (512 tokens):  分配 8 個 block

記憶體利用率: >95%
節省: (50% - 5%) / 50% ≈ 88% 記憶體節省
```

**Python 實現概念**:
```python
class BlockManager:
    def __init__(self, num_blocks, block_size):
        self.num_blocks = num_blocks
        self.block_size = block_size  # 每個 block 的 tokens 數
        self.free_blocks = list(range(num_blocks))
        
    def allocate(self, num_required):
        """分配指定數量的 blocks"""
        blocks = []
        for _ in range(num_required):
            if not self.free_blocks:
                raise OutOfMemoryError("KV Cache 已滿")
            blocks.append(self.free_blocks.pop(0))
        return blocks
    
    def free(self, blocks):
        """釋放 blocks"""
        self.free_blocks.extend(blocks)
```

### 2.2 Continuous Batching: 動態批次調度

#### 2.2.1 Static Batching 的局限

**問題**: 批次內所有請求必須同時完成

```
Static Batching:
Batch = [Req1(長), Req2(短), Req3(短), Req4(短)]

時間軸:
Req1: ████████████████████ (800ms, 100 tokens)
Req2: ████████            (400ms, 50 tokens) - 閒置 400ms
Req3: ████████            (400ms, 50 tokens) - 閒置 400ms
Req4: ████████            (400ms, 50 tokens) - 閒置 400ms

總時間: 800ms
GPU 利用率: 400ms / 800ms = 50%
```

#### 2.2.2 Continuous Batching 優化

**核心思想**: 請求完成後立即移出批次，補充新請求

```
Continuous Batching:
時間: 0     200ms  400ms  600ms  800ms
Req1: ████████████████████
Req2: ████████ (完成，移出)
Req5:         ████████████ (補充進入)
Req3: ████████ (完成，移出)
Req6:                 ████ (補充進入)

吞吐量提升: 2-3x
GPU 利用率: 85-95%
```

---

## 3. Body of Knowledge (實務)

### 3.1 vLLM: 高效推理引擎

vLLM (UC Berkeley) 透過 PagedAttention 與 Continuous Batching 實現業界領先的推理性能。

#### 3.1.1 核心架構

```
vLLM 架構:
┌─────────────────────────────────┐
│       API Server (OpenAI兼容)    │
└────────────┬────────────────────┘
             │
┌────────────▼────────────────────┐
│         Scheduler               │
│  - Continuous Batching          │
│  - Request 優先級排程            │
└────────────┬────────────────────┘
             │
┌────────────▼────────────────────┐
│      Block Manager              │
│  - PagedAttention 記憶體管理     │
│  - 物理塊分配與回收              │
└────────────┬────────────────────┘
             │
┌────────────▼────────────────────┐
│       Model Executor            │
│  - PagedAttention 計算          │
│  - CUDA Kernel 優化             │
└─────────────────────────────────┘
```

#### 3.1.2 使用範例

**基礎推理**:
```python
from vllm import LLM, SamplingParams

# 初始化引擎
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    tensor_parallel_size=1,      # 單GPU
    gpu_memory_utilization=0.9,  # 使用 90% GPU 記憶體
    max_num_seqs=32,             # 最大並發請求數
)

# 配置生成參數
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100
)

# 批次推理
prompts = ["Hello, ", "Explain AI: ", "Code example:"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

**性能對比** (Llama-2-7B, A100):

| 引擎 | 吞吐量 (tokens/s) | TTFT (ms) | 記憶體利用率 |
|------|------------------|-----------|-------------|
| HuggingFace | 250 | 450 | 60% |
| **vLLM** | **2,800** | **120** | **95%** |
| 加速比 | **11.2x** | **3.8x** | **1.6x** |

---

### 3.2 TensorRT-LLM: NVIDIA 優化引擎

TensorRT-LLM 採用 AOT (Ahead-of-Time) 編譯優化，提供極致性能。

#### 3.2.1 編譯優化流程

```
TensorRT-LLM 流程:
HF Model → Convert → Build Engine → Runtime
   ↓           ↓          ↓            ↓
 PyTorch   TRT-LLM    優化編譯      推理執行
            格式      (kernel選擇)   (CUDA執行)
```

#### 3.2.2 量化支援

| 精度 | 記憶體占用 | 速度 | 精度損失 | 適用場景 |
|------|----------|------|---------|---------|
| FP16 | 100% | 1.0x | 無 | 標準推理 |
| INT8 | 50% | 1.5-2x | <1% | 平衡性能 |
| FP8 (H100) | 50% | 2-3x | <0.5% | 極致性能 |
| INT4 | 25% | 3-4x | 2-3% | 資源受限 |

---

### 3.3 引擎選型指南

| 引擎 | 吞吐量 | 延遲 | 易用性 | 部署 | 推薦場景 |
|------|-------|------|-------|------|---------|
| **vLLM** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 通用推薦 |
| **TensorRT-LLM** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | NVIDIA GPU 極致性能 |
| **SGLang** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | 結構化生成 |
| **TGI** | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | HuggingFace 生態 |

**選擇建議**:
- **通用場景**: vLLM (最佳平衡)
- **極致性能**: TensorRT-LLM (NVIDIA GPU)
- **結構化輸出**: SGLang (JSON/YAML)
- **快速原型**: TGI (HuggingFace 兼容)

---

## 延伸學習

**論文**:
- vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention (SOSP 2023)
- FlashAttention: Fast and Memory-Efficient Exact Attention (NeurIPS 2022)

**資源**:
- vLLM 官方文檔: https://docs.vllm.ai/
- TensorRT-LLM GitHub: https://github.com/NVIDIA/TensorRT-LLM
- SGLang 專案: https://github.com/sgl-project/sglang

**下一步**: 學習 Lab-2.1 (vLLM Deployment) 進行實作練習

---

**版本**: v1.0  
**制定日期**: 2025-10-09  
**維護者**: LLM 教學專案團隊
