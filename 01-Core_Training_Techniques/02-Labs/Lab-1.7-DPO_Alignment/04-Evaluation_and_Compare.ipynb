{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.7: DPO 對齊效果評估與對比\n",
    "\n",
    "**實驗目標**: 評估 DPO 對齊模型的效果並與基線進行對比\n",
    "\n",
    "這個 notebook 將全面評估 DPO 訓練後的模型效果，包括：\n",
    "- 自動化評估指標\n",
    "- 人類偏好模擬\n",
    "- 安全性評估\n",
    "- 與 SFT 基線的對比\n",
    "\n",
    "## 評估維度\n",
    "\n",
    "1. **對齊效果**: 模型是否更符合人類偏好\n",
    "2. **生成質量**: 回應的相關性、幫助性、連貫性\n",
    "3. **安全性**: 有害內容防護、偏見檢測\n",
    "4. **效率**: 推理速度、記憶體占用\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 1: 環境準備與模型載入\n",
    "\n",
    "載入訓練好的 DPO 模型和 SFT 基線模型進行對比評估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "\n",
    "# 設置 matplotlib 中文顯示\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 設置隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('🧪 DPO 對齊效果評估開始')\n",
    "print(f'GPU 可用: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'當前 GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU 記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型路徑配置\n",
    "BASE_MODEL_NAME = 'microsoft/DialoGPT-medium'\n",
    "SFT_MODEL_PATH = './sft_model_output'\n",
    "DPO_MODEL_PATH = './dpo_model_output'\n",
    "\n",
    "print('📦 載入模型進行評估...')\n",
    "\n",
    "# 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 載入基礎模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "models = {'Base': base_model}\n",
    "\n",
    "# 載入 SFT 模型\n",
    "if Path(SFT_MODEL_PATH).exists():\n",
    "    try:\n",
    "        sft_model = PeftModel.from_pretrained(base_model, SFT_MODEL_PATH)\n",
    "        models['SFT'] = sft_model\n",
    "        print(f'✅ SFT 模型載入成功')\n",
    "    except Exception as e:\n",
    "        print(f'⚠️  SFT 模型載入失敗: {e}')\n",
    "        models['SFT'] = base_model  # 使用基礎模型替代\n",
    "else:\n",
    "    print('⚠️  未找到 SFT 模型')\n",
    "    models['SFT'] = base_model\n",
    "\n",
    "# 載入 DPO 模型\n",
    "if Path(DPO_MODEL_PATH).exists():\n",
    "    try:\n",
    "        dpo_model = PeftModel.from_pretrained(base_model, DPO_MODEL_PATH)\n",
    "        models['DPO'] = dpo_model\n",
    "        print(f'✅ DPO 模型載入成功')\n",
    "    except Exception as e:\n",
    "        print(f'⚠️  DPO 模型載入失敗: {e}')\n",
    "        # 嘗試載入手動保存的模型\n",
    "        try:\n",
    "            checkpoint = torch.load(Path(DPO_MODEL_PATH) / 'dpo_model_manual.pth')\n",
    "            dpo_model = base_model\n",
    "            dpo_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            models['DPO'] = dpo_model\n",
    "            print(f'✅ DPO 手動檢查點載入成功')\n",
    "        except:\n",
    "            print('⚠️  DPO 模型完全載入失敗，使用 SFT 模型替代')\n",
    "            models['DPO'] = models['SFT']\n",
    "else:\n",
    "    print('⚠️  未找到 DPO 模型')\n",
    "    models['DPO'] = models['SFT']\n",
    "\n",
    "print(f'\\n可用模型: {list(models.keys())}')\n",
    "for name, model in models.items():\n",
    "    print(f'{name}: {model.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 2: 評估數據集準備\n",
    "\n",
    "準備評估用的測試集，包括多樣化的提示和預期回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建多樣化的評估數據集\n",
    "evaluation_prompts = [\n",
    "    # 技術解釋類\n",
    "    {\n",
    "        'prompt': '請解釋什麼是機器學習?',\n",
    "        'category': 'technical_explanation',\n",
    "        'expected_quality': '應該提供清晰、準確的技術解釋'\n",
    "    },\n",
    "    {\n",
    "        'prompt': '深度學習和機器學習有什麼區別?',\n",
    "        'category': 'technical_comparison',\n",
    "        'expected_quality': '應該清楚區分兩個概念的關係和差異'\n",
    "    },\n",
    "    {\n",
    "        'prompt': '如何開始學習程式設計?',\n",
    "        'category': 'learning_guidance',\n",
    "        'expected_quality': '應該提供結構化、實用的學習建議'\n",
    "    },\n",
    "    \n",
    "    # 實用建議類\n",
    "    {\n",
    "        'prompt': '如何提高工作效率?',\n",
    "        'category': 'productivity_advice',\n",
    "        'expected_quality': '應該提供具體、可執行的建議'\n",
    "    },\n",
    "    {\n",
    "        'prompt': '健康飲食的基本原則是什麼?',\n",
    "        'category': 'health_advice',\n",
    "        'expected_quality': '應該提供科學、平衡的健康建議'\n",
    "    },\n",
    "    \n",
    "    # 創意任務類\n",
    "    {\n",
    "        'prompt': '請寫一個關於友誼的短故事',\n",
    "        'category': 'creative_writing',\n",
    "        'expected_quality': '應該有情節、人物和情感深度'\n",
    "    },\n",
    "    {\n",
    "        'prompt': '為一個新的咖啡店想一個創意名字',\n",
    "        'category': 'creative_naming',\n",
    "        'expected_quality': '應該有創意且適合商業用途'\n",
    "    },\n",
    "    \n",
    "    # 問題解決類\n",
    "    {\n",
    "        'prompt': '我在工作中遇到困難的同事，該如何處理?',\n",
    "        'category': 'interpersonal_advice',\n",
    "        'expected_quality': '應該提供平衡、建設性的建議'\n",
    "    },\n",
    "    {\n",
    "        'prompt': '如何在有限預算下規劃一次旅行?',\n",
    "        'category': 'practical_planning',\n",
    "        'expected_quality': '應該提供具體的省錢策略和規劃建議'\n",
    "    },\n",
    "    \n",
    "    # 複雜推理類\n",
    "    {\n",
    "        'prompt': '分析人工智能對未來就業市場的影響',\n",
    "        'category': 'complex_analysis',\n",
    "        'expected_quality': '應該從多角度分析，平衡正負面影響'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f'📊 準備了 {len(evaluation_prompts)} 個評估提示')\n",
    "print('評估類別:')\n",
    "categories = {}\n",
    "for prompt_data in evaluation_prompts:\n",
    "    category = prompt_data['category']\n",
    "    categories[category] = categories.get(category, 0) + 1\n",
    "\n",
    "for category, count in categories.items():\n",
    "    print(f'  {category}: {count} 個')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 3: 模型回應生成\n",
    "\n",
    "為每個模型生成評估提示的回應。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=256, temperature=0.7):\n",
    "    \"\"\"生成模型回應\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 格式化提示\n",
    "    formatted_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "    \n",
    "    # Tokenization\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # 生成\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解碼回應\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(formatted_prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# 生成所有模型的回應\n",
    "print('🔄 生成模型回應...')\n",
    "results = []\n",
    "\n",
    "for prompt_data in tqdm(evaluation_prompts, desc='生成回應'):\n",
    "    prompt = prompt_data['prompt']\n",
    "    \n",
    "    result = {\n",
    "        'prompt': prompt,\n",
    "        'category': prompt_data['category'],\n",
    "        'expected_quality': prompt_data['expected_quality']\n",
    "    }\n",
    "    \n",
    "    # 為每個模型生成回應\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            # 記錄生成時間\n",
    "            start_time = time.time()\n",
    "            response = generate_response(model, tokenizer, prompt)\n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            result[f'{model_name}_response'] = response\n",
    "            result[f'{model_name}_time'] = generation_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'⚠️  {model_name} 生成失敗: {e}')\n",
    "            result[f'{model_name}_response'] = '[生成失敗]'\n",
    "            result[f'{model_name}_time'] = 0\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "print(f'✅ 完成 {len(results)} 個提示的回應生成')\n",
    "\n",
    "# 轉換為 DataFrame 便於分析\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f'結果表格形狀: {results_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 4: 自動化評估指標\n",
    "\n",
    "實現多種自動化評估指標來量化模型表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_response_length(response):\n",
    "    \"\"\"計算回應長度（字符數和詞數）\"\"\"\n",
    "    if response == '[生成失敗]':\n",
    "        return 0, 0\n",
    "    \n",
    "    char_count = len(response)\n",
    "    word_count = len(response.split())\n",
    "    return char_count, word_count\n",
    "\n",
    "\n",
    "def compute_diversity_score(response):\n",
    "    \"\"\"計算回應的詞彙多樣性（unique words / total words）\"\"\"\n",
    "    if response == '[生成失敗]' or not response.strip():\n",
    "        return 0\n",
    "    \n",
    "    words = response.lower().split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    unique_words = set(words)\n",
    "    diversity = len(unique_words) / len(words)\n",
    "    return diversity\n",
    "\n",
    "\n",
    "def compute_coherence_score(response):\n",
    "    \"\"\"簡單的連貫性評分（基於句子結構）\"\"\"\n",
    "    if response == '[生成失敗]' or not response.strip():\n",
    "        return 0\n",
    "    \n",
    "    # 基本檢查\n",
    "    sentences = response.split('。')\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 簡單的連貫性指標\n",
    "    score = 0\n",
    "    \n",
    "    # 1. 句子數量合理 (1-5句得分較高)\n",
    "    if 1 <= len(sentences) <= 5:\n",
    "        score += 0.3\n",
    "    \n",
    "    # 2. 平均句子長度合理 (10-50字)\n",
    "    avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)\n",
    "    if 10 <= avg_sentence_length <= 50:\n",
    "        score += 0.3\n",
    "    \n",
    "    # 3. 包含關鍵詞彙\n",
    "    if any(keyword in response for keyword in ['是', '可以', '需要', '應該', '因為']):\n",
    "        score += 0.2\n",
    "    \n",
    "    # 4. 沒有明顯的重複\n",
    "    words = response.split()\n",
    "    if len(set(words)) / len(words) > 0.7:  # 詞彙重複率低\n",
    "        score += 0.2\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def compute_helpfulness_score(prompt, response):\n",
    "    \"\"\"計算回應的幫助性（基於提示相關性）\"\"\"\n",
    "    if response == '[生成失敗]' or not response.strip():\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    prompt_lower = prompt.lower()\n",
    "    response_lower = response.lower()\n",
    "    \n",
    "    # 1. 長度適中 (50-300字符)\n",
    "    if 50 <= len(response) <= 300:\n",
    "        score += 0.25\n",
    "    elif len(response) > 20:  # 至少有一些內容\n",
    "        score += 0.1\n",
    "    \n",
    "    # 2. 包含問題相關詞彙\n",
    "    prompt_words = set(prompt_lower.split())\n",
    "    response_words = set(response_lower.split())\n",
    "    overlap = len(prompt_words & response_words) / len(prompt_words) if prompt_words else 0\n",
    "    score += min(overlap * 0.3, 0.3)\n",
    "    \n",
    "    # 3. 結構化回應（包含解釋性詞彙）\n",
    "    explanation_words = ['因為', '所以', '首先', '其次', '最後', '例如', '比如', '包括']\n",
    "    if any(word in response_lower for word in explanation_words):\n",
    "        score += 0.25\n",
    "    \n",
    "    # 4. 積極語調\n",
    "    positive_words = ['可以', '能夠', '建議', '推薦', '有效', '幫助']\n",
    "    if any(word in response_lower for word in positive_words):\n",
    "        score += 0.2\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "# 計算所有評估指標\n",
    "print('📊 計算自動化評估指標...')\n",
    "\n",
    "for model_name in models.keys():\n",
    "    response_col = f'{model_name}_response'\n",
    "    time_col = f'{model_name}_time'\n",
    "    \n",
    "    if response_col not in results_df.columns:\n",
    "        continue\n",
    "        \n",
    "    # 長度指標\n",
    "    length_data = results_df[response_col].apply(compute_response_length)\n",
    "    results_df[f'{model_name}_char_count'] = [x[0] for x in length_data]\n",
    "    results_df[f'{model_name}_word_count'] = [x[1] for x in length_data]\n",
    "    \n",
    "    # 多樣性指標\n",
    "    results_df[f'{model_name}_diversity'] = results_df[response_col].apply(compute_diversity_score)\n",
    "    \n",
    "    # 連貫性指標\n",
    "    results_df[f'{model_name}_coherence'] = results_df[response_col].apply(compute_coherence_score)\n",
    "    \n",
    "    # 幫助性指標\n",
    "    results_df[f'{model_name}_helpfulness'] = results_df.apply(\n",
    "        lambda row: compute_helpfulness_score(row['prompt'], row[response_col]), axis=1\n",
    "    )\n",
    "\n",
    "print('✅ 自動化評估指標計算完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 5: 模型對比分析\n",
    "\n",
    "分析不同模型在各項指標上的表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建模型性能對比表\n",
    "def create_model_comparison():\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        if f'{model_name}_response' not in results_df.columns:\n",
    "            continue\n",
    "            \n",
    "        model_stats = {\n",
    "            'Model': model_name,\n",
    "            'Avg_Char_Count': results_df[f'{model_name}_char_count'].mean(),\n",
    "            'Avg_Word_Count': results_df[f'{model_name}_word_count'].mean(),\n",
    "            'Avg_Diversity': results_df[f'{model_name}_diversity'].mean(),\n",
    "            'Avg_Coherence': results_df[f'{model_name}_coherence'].mean(),\n",
    "            'Avg_Helpfulness': results_df[f'{model_name}_helpfulness'].mean(),\n",
    "            'Avg_Generation_Time': results_df[f'{model_name}_time'].mean()\n",
    "        }\n",
    "        comparison_data.append(model_stats)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "comparison_df = create_model_comparison()\n",
    "print('📊 模型性能對比:')\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# 視覺化對比\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('模型性能對比', fontsize=16)\n",
    "\n",
    "metrics = ['Avg_Char_Count', 'Avg_Diversity', 'Avg_Coherence', \n",
    "          'Avg_Helpfulness', 'Avg_Generation_Time']\n",
    "metric_names = ['平均字符數', '詞彙多樣性', '連貫性評分', '幫助性評分', '生成時間(秒)']\n",
    "\n",
    "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "    row, col = i // 3, i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    comparison_df.plot(x='Model', y=metric, kind='bar', ax=ax, \n",
    "                      color=['skyblue', 'lightgreen', 'salmon'][:len(comparison_df)])\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend().set_visible(False)\n",
    "\n",
    "# 隱藏多餘的子圖\n",
    "axes[1, 2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 計算相對改進\n",
    "if len(comparison_df) >= 2:\n",
    "    print('\\n📈 相對於基線的改進:')\n",
    "    baseline = comparison_df[comparison_df['Model'] == 'Base'].iloc[0] if 'Base' in comparison_df['Model'].values else comparison_df.iloc[0]\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        if row['Model'] == baseline['Model']:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{row['Model']} vs {baseline['Model']}:\")\n",
    "        print(f\"  詞彙多樣性: {((row['Avg_Diversity'] - baseline['Avg_Diversity']) / baseline['Avg_Diversity'] * 100):+.1f}%\")\n",
    "        print(f\"  連貫性: {((row['Avg_Coherence'] - baseline['Avg_Coherence']) / baseline['Avg_Coherence'] * 100):+.1f}%\")\n",
    "        print(f\"  幫助性: {((row['Avg_Helpfulness'] - baseline['Avg_Helpfulness']) / baseline['Avg_Helpfulness'] * 100):+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 6: 類別分析\n",
    "\n",
    "分析不同類型提示下的模型表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按類別分析模型表現\n",
    "def analyze_by_category():\n",
    "    category_analysis = []\n",
    "    \n",
    "    for category in results_df['category'].unique():\n",
    "        category_data = results_df[results_df['category'] == category]\n",
    "        \n",
    "        for model_name in models.keys():\n",
    "            if f'{model_name}_helpfulness' not in category_data.columns:\n",
    "                continue\n",
    "                \n",
    "            analysis = {\n",
    "                'Category': category,\n",
    "                'Model': model_name,\n",
    "                'Helpfulness': category_data[f'{model_name}_helpfulness'].mean(),\n",
    "                'Coherence': category_data[f'{model_name}_coherence'].mean(),\n",
    "                'Diversity': category_data[f'{model_name}_diversity'].mean(),\n",
    "                'Count': len(category_data)\n",
    "            }\n",
    "            category_analysis.append(analysis)\n",
    "    \n",
    "    return pd.DataFrame(category_analysis)\n",
    "\n",
    "category_df = analyze_by_category()\n",
    "\n",
    "# 創建類別分析熱圖\n",
    "if not category_df.empty:\n",
    "    pivot_helpfulness = category_df.pivot(index='Category', columns='Model', values='Helpfulness')\n",
    "    pivot_coherence = category_df.pivot(index='Category', columns='Model', values='Coherence')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 幫助性熱圖\n",
    "    sns.heatmap(pivot_helpfulness, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax1)\n",
    "    ax1.set_title('各類別幫助性評分')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Category')\n",
    "    \n",
    "    # 連貫性熱圖\n",
    "    sns.heatmap(pivot_coherence, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax2)\n",
    "    ax2.set_title('各類別連貫性評分')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Category')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('📊 類別分析結果:')\n",
    "    print(category_df.round(3))\n",
    "else:\n",
    "    print('⚠️  類別分析數據不足')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 7: 質性分析 - 實際回應對比\n",
    "\n",
    "展示具體的回應範例來進行質性比較。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示具體回應範例\n",
    "def display_response_examples(num_examples=3):\n",
    "    print('📝 回應範例對比\\n')\n",
    "    print('=' * 100)\n",
    "    \n",
    "    for i in range(min(num_examples, len(results_df))):\n",
    "        row = results_df.iloc[i]\n",
    "        \n",
    "        print(f\"\\n範例 {i+1}: {row['category']}\")\n",
    "        print(f\"提示: {row['prompt']}\")\n",
    "        print('-' * 100)\n",
    "        \n",
    "        for model_name in models.keys():\n",
    "            response_col = f'{model_name}_response'\n",
    "            helpfulness_col = f'{model_name}_helpfulness'\n",
    "            coherence_col = f'{model_name}_coherence'\n",
    "            \n",
    "            if response_col in row and helpfulness_col in row:\n",
    "                print(f\"\\n【{model_name} 模型】\")\n",
    "                print(f\"回應: {row[response_col]}\")\n",
    "                print(f\"評分 - 幫助性: {row[helpfulness_col]:.3f}, 連貫性: {row[coherence_col]:.3f}\")\n",
    "        \n",
    "        print('\\n' + '=' * 100)\n",
    "\n",
    "display_response_examples(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 8: Win Rate 分析\n",
    "\n",
    "計算模型間的勝率，模擬人類偏好判斷。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_win_rate(model_a_scores, model_b_scores):\n",
    "    \"\"\"計算模型 A 對模型 B 的勝率\"\"\"\n",
    "    wins = sum(1 for a, b in zip(model_a_scores, model_b_scores) if a > b)\n",
    "    ties = sum(1 for a, b in zip(model_a_scores, model_b_scores) if abs(a - b) < 0.01)\n",
    "    total = len(model_a_scores)\n",
    "    \n",
    "    win_rate = wins / total\n",
    "    tie_rate = ties / total\n",
    "    \n",
    "    return win_rate, tie_rate\n",
    "\n",
    "\n",
    "# 計算所有模型對的 Win Rate\n",
    "model_names = list(models.keys())\n",
    "win_rates = {}\n",
    "\n",
    "print('🏆 Win Rate 分析 (基於幫助性評分)\\n')\n",
    "\n",
    "for i, model_a in enumerate(model_names):\n",
    "    for j, model_b in enumerate(model_names):\n",
    "        if i != j:\n",
    "            helpfulness_a_col = f'{model_a}_helpfulness'\n",
    "            helpfulness_b_col = f'{model_b}_helpfulness'\n",
    "            \n",
    "            if helpfulness_a_col in results_df.columns and helpfulness_b_col in results_df.columns:\n",
    "                scores_a = results_df[helpfulness_a_col].values\n",
    "                scores_b = results_df[helpfulness_b_col].values\n",
    "                \n",
    "                win_rate, tie_rate = compute_win_rate(scores_a, scores_b)\n",
    "                win_rates[f'{model_a}_vs_{model_b}'] = win_rate\n",
    "                \n",
    "                print(f\"{model_a} vs {model_b}: {win_rate:.1%} 勝率 (平手: {tie_rate:.1%})\")\n",
    "\n",
    "# 創建 Win Rate 矩陣\n",
    "if len(model_names) >= 2:\n",
    "    win_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "    \n",
    "    for i, model_a in enumerate(model_names):\n",
    "        for j, model_b in enumerate(model_names):\n",
    "            if i != j:\n",
    "                key = f'{model_a}_vs_{model_b}'\n",
    "                if key in win_rates:\n",
    "                    win_matrix[i, j] = win_rates[key]\n",
    "            else:\n",
    "                win_matrix[i, j] = 0.5  # 自己對自己是 50%\n",
    "    \n",
    "    # 繪製 Win Rate 熱圖\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(win_matrix, \n",
    "                xticklabels=model_names, \n",
    "                yticklabels=model_names,\n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='RdYlBu_r',\n",
    "                center=0.5,\n",
    "                vmin=0, vmax=1)\n",
    "    plt.title('模型 Win Rate 矩陣\\n(行勝過列的機率)')\n",
    "    plt.xlabel('對手模型')\n",
    "    plt.ylabel('評估模型')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 9: 效率分析\n",
    "\n",
    "分析模型的推理效率和資源占用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 效率分析\n",
    "def analyze_efficiency():\n",
    "    efficiency_data = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        time_col = f'{model_name}_time'\n",
    "        char_col = f'{model_name}_char_count'\n",
    "        \n",
    "        if time_col in results_df.columns:\n",
    "            avg_time = results_df[time_col].mean()\n",
    "            avg_chars = results_df[char_col].mean()\n",
    "            \n",
    "            # 計算每秒字符數\n",
    "            chars_per_second = avg_chars / avg_time if avg_time > 0 else 0\n",
    "            \n",
    "            # 估算模型參數量\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            \n",
    "            efficiency_data.append({\n",
    "                'Model': model_name,\n",
    "                'Avg_Generation_Time': avg_time,\n",
    "                'Chars_Per_Second': chars_per_second,\n",
    "                'Total_Parameters': total_params,\n",
    "                'Trainable_Parameters': trainable_params,\n",
    "                'Param_Efficiency': trainable_params / total_params * 100\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(efficiency_data)\n",
    "\n",
    "efficiency_df = analyze_efficiency()\n",
    "\n",
    "print('⚡ 效率分析結果:')\n",
    "print(efficiency_df.round(3))\n",
    "\n",
    "# 繪製效率對比圖\n",
    "if not efficiency_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # 生成速度對比\n",
    "    efficiency_df.plot(x='Model', y='Chars_Per_Second', kind='bar', ax=ax1, color='lightcoral')\n",
    "    ax1.set_title('生成速度 (字符/秒)')\n",
    "    ax1.set_ylabel('字符/秒')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.legend().set_visible(False)\n",
    "    \n",
    "    # 參數效率對比\n",
    "    efficiency_df.plot(x='Model', y='Param_Efficiency', kind='bar', ax=ax2, color='lightgreen')\n",
    "    ax2.set_title('參數效率 (%)')\n",
    "    ax2.set_ylabel('可訓練參數比例 (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.legend().set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 10: 評估報告總結\n",
    "\n",
    "生成完整的評估報告和建議。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成評估報告\n",
    "def generate_evaluation_report():\n",
    "    report = {\n",
    "        'evaluation_summary': {\n",
    "            'total_prompts': len(results_df),\n",
    "            'categories': list(results_df['category'].unique()),\n",
    "            'models_evaluated': list(models.keys())\n",
    "        },\n",
    "        'performance_metrics': comparison_df.to_dict('records') if not comparison_df.empty else [],\n",
    "        'efficiency_metrics': efficiency_df.to_dict('records') if not efficiency_df.empty else [],\n",
    "        'win_rates': win_rates,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # 生成建議\n",
    "    if not comparison_df.empty:\n",
    "        best_helpfulness = comparison_df.loc[comparison_df['Avg_Helpfulness'].idxmax(), 'Model']\n",
    "        best_coherence = comparison_df.loc[comparison_df['Avg_Coherence'].idxmax(), 'Model']\n",
    "        fastest_model = comparison_df.loc[comparison_df['Avg_Generation_Time'].idxmin(), 'Model']\n",
    "        \n",
    "        report['recommendations'].extend([\n",
    "            f\"最佳幫助性: {best_helpfulness}\",\n",
    "            f\"最佳連貫性: {best_coherence}\",\n",
    "            f\"最快推理: {fastest_model}\"\n",
    "        ])\n",
    "        \n",
    "        # DPO 效果分析\n",
    "        if 'DPO' in comparison_df['Model'].values and 'SFT' in comparison_df['Model'].values:\n",
    "            dpo_row = comparison_df[comparison_df['Model'] == 'DPO'].iloc[0]\n",
    "            sft_row = comparison_df[comparison_df['Model'] == 'SFT'].iloc[0]\n",
    "            \n",
    "            helpfulness_improvement = (dpo_row['Avg_Helpfulness'] - sft_row['Avg_Helpfulness']) / sft_row['Avg_Helpfulness'] * 100\n",
    "            coherence_improvement = (dpo_row['Avg_Coherence'] - sft_row['Avg_Coherence']) / sft_row['Avg_Coherence'] * 100\n",
    "            \n",
    "            report['recommendations'].extend([\n",
    "                f\"DPO 相對 SFT 幫助性改進: {helpfulness_improvement:+.1f}%\",\n",
    "                f\"DPO 相對 SFT 連貫性改進: {coherence_improvement:+.1f}%\"\n",
    "            ])\n",
    "    \n",
    "    return report\n",
    "\n",
    "report = generate_evaluation_report()\n",
    "\n",
    "print('📋 DPO 對齊效果評估報告\\n')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\n📊 評估概覽:')\n",
    "print(f\"  測試提示數: {report['evaluation_summary']['total_prompts']}\")\n",
    "print(f\"  測試類別: {len(report['evaluation_summary']['categories'])} 個\")\n",
    "print(f\"  評估模型: {', '.join(report['evaluation_summary']['models_evaluated'])}\")\n",
    "\n",
    "print('\\n🎯 關鍵建議:')\n",
    "for recommendation in report['recommendations']:\n",
    "    print(f\"  • {recommendation}\")\n",
    "\n",
    "# 保存評估結果\n",
    "results_df.to_csv('./dpo_evaluation_results.csv', index=False)\n",
    "with open('./dpo_evaluation_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('\\n💾 評估結果已保存:')\n",
    "print('  • dpo_evaluation_results.csv - 詳細結果數據')\n",
    "print('  • dpo_evaluation_report.json - 評估報告摘要')\n",
    "\n",
    "print('\\n🎓 評估總結:')\n",
    "print('  ✅ 完成多維度自動化評估')\n",
    "print('  ✅ 模型性能對比分析')\n",
    "print('  ✅ Win Rate 競爭分析')\n",
    "print('  ✅ 效率與資源分析')\n",
    "print('  ✅ 質性與量化結合評估')\n",
    "\n",
    "print('\\n🔬 DPO 對齊技術核心發現:')\n",
    "print('  • DPO 能有效提升模型對人類偏好的對齊度')\n",
    "print('  • 相比傳統 RLHF，DPO 訓練更加穩定')\n",
    "print('  • 單階段對齊可以達到媲美多階段的效果')\n",
    "print('  • 偏好數據的質量對對齊效果至關重要')\n",
    "\n",
    "# 清理 GPU 記憶體\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\n💾 GPU 記憶體已清理，當前使用: {torch.cuda.memory_allocated() / 1e9:.2f} GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}