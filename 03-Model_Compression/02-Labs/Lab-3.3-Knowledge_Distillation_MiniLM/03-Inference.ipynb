{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3: Knowledge Distillation - Inference Comparison\n",
    "\n",
    "**Goal:** Compare teacher vs distilled student model inference quality and performance.\n",
    "\n",
    "**You will learn to:**\n",
    "- Load distilled student model\n",
    "- Compare predictions side-by-side\n",
    "- Analyze agreement and disagreement cases\n",
    "- Measure inference latency improvements\n",
    "- Evaluate distillation effectiveness\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Completed notebooks:\n",
    "- **01-Setup.ipynb**: Environment and models\n",
    "- **02-Distill.ipynb**: Distillation training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "\n",
    "TEACHER_MODEL = \"bert-base-uncased\"\n",
    "STUDENT_MODEL_DIR = \"./distilled_student\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL)\n",
    "print(\"âœ… Tokenizer loaded\\n\")\n",
    "\n",
    "# Load teacher\n",
    "print(\"â³ Loading teacher model...\")\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TEACHER_MODEL, num_labels=2\n",
    ").to(device)\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\"âœ… Teacher loaded: {teacher_params/1e6:.2f}M params\\n\")\n",
    "\n",
    "# Load distilled student\n",
    "print(\"â³ Loading distilled student model...\")\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    STUDENT_MODEL_DIR\n",
    ").to(device)\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "print(f\"âœ… Student loaded: {student_params/1e6:.2f}M params\")\n",
    "print(f\"   Compression: {teacher_params/student_params:.2f}x\\n\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Prepare Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    # Clear positive\n",
    "    \"This movie is absolutely fantastic! A masterpiece of cinema.\",\n",
    "    \"I loved every minute of it. Brilliant performances all around.\",\n",
    "    \n",
    "    # Clear negative\n",
    "    \"Terrible waste of time. Boring and poorly executed.\",\n",
    "    \"One of the worst films I've ever seen. Avoid at all costs.\",\n",
    "    \n",
    "    # Neutral/mixed\n",
    "    \"It was okay. Some good moments but nothing special.\",\n",
    "    \"Mixed feelings about this one. Some parts work, others don't.\",\n",
    "    \n",
    "    # Sarcasm/difficult\n",
    "    \"Sure, if you enjoy being bored to death, this is perfect.\",\n",
    "    \"Absolutely 'brilliant' way to waste two hours of your life.\"\n",
    "]\n",
    "\n",
    "label_names = ['Negative', 'Positive']\n",
    "\n",
    "print(f\"ğŸ“ Prepared {len(test_samples)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Side-by-Side Inference Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEACHER vs STUDENT COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "teacher_model.eval()\n",
    "student_model.eval()\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, text in enumerate(test_samples, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: {text}\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Teacher inference\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(**inputs)\n",
    "        teacher_logits = teacher_outputs.logits[0]\n",
    "        teacher_probs = torch.softmax(teacher_logits, dim=-1)\n",
    "        teacher_pred = torch.argmax(teacher_probs).item()\n",
    "    teacher_time = time.time() - start\n",
    "    \n",
    "    # Student inference\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student_model(**inputs)\n",
    "        student_logits = student_outputs.logits[0]\n",
    "        student_probs = torch.softmax(student_logits, dim=-1)\n",
    "        student_pred = torch.argmax(student_probs).item()\n",
    "    student_time = time.time() - start\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"ğŸŸ¢ Teacher: {label_names[teacher_pred]:8s} (Neg: {teacher_probs[0]:.3f}, Pos: {teacher_probs[1]:.3f}) [{teacher_time*1000:.2f}ms]\")\n",
    "    print(f\"ğŸ”µ Student: {label_names[student_pred]:8s} (Neg: {student_probs[0]:.3f}, Pos: {student_probs[1]:.3f}) [{student_time*1000:.2f}ms]\")\n",
    "    \n",
    "    agreement = \"âœ… Agreement\" if teacher_pred == student_pred else \"âŒ Disagreement\"\n",
    "    speedup = teacher_time / student_time\n",
    "    print(f\"\\n{agreement} | Speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'teacher_pred': teacher_pred,\n",
    "        'student_pred': student_pred,\n",
    "        'agreement': teacher_pred == student_pred,\n",
    "        'teacher_time': teacher_time,\n",
    "        'student_time': student_time\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Agreement Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "agreements = [r['agreement'] for r in results]\n",
    "agreement_rate = np.mean(agreements)\n",
    "\n",
    "print(f\"\\nğŸ“Š Agreement Statistics:\")\n",
    "print(f\"   Total samples: {len(results)}\")\n",
    "print(f\"   Agreements: {sum(agreements)}\")\n",
    "print(f\"   Disagreements: {len(agreements) - sum(agreements)}\")\n",
    "print(f\"   Agreement rate: {agreement_rate:.1%}\")\n",
    "\n",
    "if agreement_rate >= 0.90:\n",
    "    print(\"\\nâœ… Excellent agreement (>=90%)! Student learned teacher well.\")\n",
    "elif agreement_rate >= 0.80:\n",
    "    print(\"\\nâœ… Good agreement (80-90%). Student performs well.\")\n",
    "elif agreement_rate >= 0.70:\n",
    "    print(\"\\nğŸŸ¡ Moderate agreement (70-80%). Some discrepancies.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Low agreement (<70%). Consider more distillation training.\")\n",
    "\n",
    "# Disagreement cases\n",
    "disagreements = [r for r in results if not r['agreement']]\n",
    "if disagreements:\n",
    "    print(f\"\\nâŒ Disagreement Cases ({len(disagreements)}):\")\n",
    "    for r in disagreements:\n",
    "        print(f\"   \\\"{r['text'][:60]}...\\\"\")\n",
    "        print(f\"      Teacher: {label_names[r['teacher_pred']]}, Student: {label_names[r['student_pred']]}\\n\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "teacher_times = [r['teacher_time'] for r in results]\n",
    "student_times = [r['student_time'] for r in results]\n",
    "\n",
    "avg_teacher = np.mean(teacher_times) * 1000\n",
    "avg_student = np.mean(student_times) * 1000\n",
    "avg_speedup = np.mean([t/s for t, s in zip(teacher_times, student_times)])\n",
    "\n",
    "print(f\"\\nğŸ“Š Inference Latency (single sample):\")\n",
    "print(f\"   Teacher: {avg_teacher:.2f}ms (avg)\")\n",
    "print(f\"   Student: {avg_student:.2f}ms (avg)\")\n",
    "print(f\"   Speedup: {avg_speedup:.2f}x\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Model Size:\")\n",
    "print(f\"   Teacher: {teacher_params/1e6:.1f}M params\")\n",
    "print(f\"   Student: {student_params/1e6:.1f}M params\")\n",
    "print(f\"   Compression: {teacher_params/student_params:.2f}x\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Inference Comparison Complete!\n",
    "\n",
    "**Summary**:\n",
    "- âœ… Compared teacher vs student predictions\n",
    "- âœ… Analyzed agreement rate (~90%+)\n",
    "- âœ… Measured inference speedup (~1.8-2.0x)\n",
    "- âœ… Identified challenging cases\n",
    "\n",
    "**Key Findings**:\n",
    "- Student achieves high agreement with teacher\n",
    "- 2.1x compression (110M â†’ 52M params)\n",
    "- 1.8-2.0x inference speedup\n",
    "- Disagreements mainly on edge cases\n",
    "\n",
    "**Next Steps**:\n",
    "- Proceed to **04-Benchmark.ipynb** for comprehensive evaluation\n",
    "\n",
    "---\n",
    "\n",
    "**â­ï¸ Continue to**: [04-Benchmark.ipynb](./04-Benchmark.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
