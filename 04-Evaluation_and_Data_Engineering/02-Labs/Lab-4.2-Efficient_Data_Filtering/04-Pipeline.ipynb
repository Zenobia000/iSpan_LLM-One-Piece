{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 - é«˜æ•ˆæ•¸æ“šç¯©é¸\n",
    "## Notebook 04: è‡ªå‹•åŒ–æ•¸æ“šè™•ç†ç®¡ç·š\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "1. å»ºç«‹ DataFilteringPipeline é¡\n",
    "2. å¯¦ç¾ç«¯åˆ°ç«¯ç¯©é¸æµç¨‹ (åŠ è¼‰ â†’ ç¯©é¸ â†’ ä¿å­˜)\n",
    "3. æ”¯æ´å¢é‡æ•¸æ“šè™•ç†\n",
    "4. å¯¦ç¾æ•¸æ“šç‰ˆæœ¬ç®¡ç† (DVC-style)\n",
    "5. å»ºç«‹è³ªé‡ç›£æ§å„€è¡¨æ¿\n",
    "6. é…ç½®ç®¡ç†èˆ‡å¯é‡ç¾æ€§\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 30-45 åˆ†é˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å°å…¥ä¾è³´èˆ‡æº–å‚™\n",
    "\n",
    "å°å…¥æ‰€éœ€çš„å¥—ä»¶å’Œå·¥å…·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å¯è¦–åŒ–é¢¨æ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… ä¾è³´å°å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›®éŒ„è¨­å®š\n",
    "PIPELINE_DIR = Path('./pipeline')\n",
    "PIPELINE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATA_DIR = Path('./data')\n",
    "ANALYSIS_DIR = Path('./analysis')\n",
    "\n",
    "print(f\"âœ… å·¥ä½œç›®éŒ„: {PIPELINE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å°å…¥ä¹‹å‰å¯¦ç¾çš„é¡\n",
    "\n",
    "è¤‡ç”¨ IFD å’Œ DEITA è©•åˆ†å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾ 02-Filter.ipynb è¤‡ç”¨çš„ IFD è¨ˆç®—å™¨\n",
    "class IFDCalculator:\n",
    "    \"\"\"IFD (Instruction Following Difficulty) Calculator\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model, batch_size=64):\n",
    "        self.model = embedding_model\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def calculate_batch_ifd(self, samples: list) -> list:\n",
    "        \"\"\"Calculate IFD for multiple samples in batches\"\"\"\n",
    "        # Prepare texts\n",
    "        instructions = []\n",
    "        responses = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            full_instruction = sample['instruction']\n",
    "            if sample.get('input', ''):\n",
    "                full_instruction += \" \" + sample['input']\n",
    "            instructions.append(full_instruction)\n",
    "            responses.append(sample['output'])\n",
    "        \n",
    "        # Encode in batches\n",
    "        instr_embs = self.model.encode(\n",
    "            instructions, batch_size=self.batch_size,\n",
    "            show_progress_bar=False, convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        resp_embs = self.model.encode(\n",
    "            responses, batch_size=self.batch_size,\n",
    "            show_progress_bar=False, convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Calculate IFD\n",
    "        results = []\n",
    "        for i, sample in enumerate(samples):\n",
    "            similarity = np.dot(instr_embs[i], resp_embs[i]) / (\n",
    "                np.linalg.norm(instr_embs[i]) * np.linalg.norm(resp_embs[i])\n",
    "            )\n",
    "            ifd = 1.0 - similarity\n",
    "            \n",
    "            sample_with_ifd = sample.copy()\n",
    "            sample_with_ifd['ifd_score'] = float(ifd)\n",
    "            results.append(sample_with_ifd)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class DEITAScorer:\n",
    "    \"\"\"DEITA (Data-Efficient Instruction Tuning) Scorer\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model, alpha=0.4, beta=0.4, gamma=0.2):\n",
    "        self.model = embedding_model\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.embedding_cache = {}\n",
    "    \n",
    "    def calculate_complexity(self, sample: dict) -> float:\n",
    "        \"\"\"Calculate complexity score using rule-based heuristics\"\"\"\n",
    "        instruction = sample['instruction']\n",
    "        output = sample['output']\n",
    "        ifd = sample.get('ifd_score', 0.5)\n",
    "        \n",
    "        instr_words = len(instruction.split())\n",
    "        output_words = len(output.split())\n",
    "        length_score = min(1.0, (instr_words / 50 + output_words / 200) / 2)\n",
    "        \n",
    "        complex_keywords = [\n",
    "            'analyze', 'compare', 'evaluate', 'explain', 'describe',\n",
    "            'discuss', 'critique', 'assess', 'justify', 'synthesize'\n",
    "        ]\n",
    "        keyword_count = sum(1 for kw in complex_keywords if kw in instruction.lower())\n",
    "        keyword_score = min(1.0, keyword_count / 3)\n",
    "        \n",
    "        complexity = 0.3 * length_score + 0.3 * keyword_score + 0.4 * ifd\n",
    "        return float(complexity)\n",
    "    \n",
    "    def calculate_quality(self, sample: dict) -> float:\n",
    "        \"\"\"Calculate quality score using rule-based heuristics\"\"\"\n",
    "        output = sample['output']\n",
    "        instruction = sample['instruction']\n",
    "        \n",
    "        output_words = len(output.split())\n",
    "        completeness = min(1.0, output_words / 100)\n",
    "        \n",
    "        structure_indicators = ['\\n', '. ', ', ', ':', '-', '1.', '2.']\n",
    "        structure_count = sum(1 for ind in structure_indicators if ind in output)\n",
    "        structure_score = min(1.0, structure_count / 5)\n",
    "        \n",
    "        instr_words = len(instruction.split())\n",
    "        ratio = output_words / max(instr_words, 1)\n",
    "        relevance = min(1.0, ratio / 10)\n",
    "        \n",
    "        quality = 0.4 * completeness + 0.3 * structure_score + 0.3 * relevance\n",
    "        return float(quality)\n",
    "    \n",
    "    def calculate_diversity(self, sample: dict, selected_samples: list) -> float:\n",
    "        \"\"\"Calculate diversity score\"\"\"\n",
    "        if not selected_samples:\n",
    "            return 1.0\n",
    "        \n",
    "        sample_key = sample['instruction'] + ' ' + sample['output']\n",
    "        \n",
    "        if sample_key not in self.embedding_cache:\n",
    "            self.embedding_cache[sample_key] = self.model.encode([sample_key])[0]\n",
    "        \n",
    "        sample_emb = self.embedding_cache[sample_key]\n",
    "        max_similarity = 0.0\n",
    "        \n",
    "        for selected in selected_samples:\n",
    "            selected_key = selected['instruction'] + ' ' + selected['output']\n",
    "            \n",
    "            if selected_key not in self.embedding_cache:\n",
    "                self.embedding_cache[selected_key] = self.model.encode([selected_key])[0]\n",
    "            \n",
    "            selected_emb = self.embedding_cache[selected_key]\n",
    "            similarity = np.dot(sample_emb, selected_emb) / (\n",
    "                np.linalg.norm(sample_emb) * np.linalg.norm(selected_emb)\n",
    "            )\n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        return float(1.0 - max_similarity)\n",
    "\n",
    "print(\"âœ… IFD å’Œ DEITA é¡å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataFilteringPipeline é¡\n",
    "\n",
    "å»ºç«‹ç«¯åˆ°ç«¯çš„æ•¸æ“šç¯©é¸ç®¡ç·šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFilteringPipeline:\n",
    "    \"\"\"\n",
    "    Automated Data Filtering Pipeline\n",
    "    \n",
    "    Features:\n",
    "    - End-to-end filtering (load â†’ filter â†’ save)\n",
    "    - Incremental processing support\n",
    "    - Data versioning (DVC-style)\n",
    "    - Quality monitoring\n",
    "    - Configuration management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"\n",
    "        Initialize pipeline with configuration\n",
    "        \n",
    "        Args:\n",
    "            config: Pipeline configuration dict\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        print(\"ğŸ“¦ åˆå§‹åŒ–ç®¡ç·šçµ„ä»¶...\")\n",
    "        \n",
    "        # Load embedding model\n",
    "        self.embedding_model = SentenceTransformer(config['embedding_model'])\n",
    "        print(f\"  âœ… åµŒå…¥æ¨¡å‹: {config['embedding_model']}\")\n",
    "        \n",
    "        # Initialize calculators\n",
    "        self.ifd_calculator = IFDCalculator(\n",
    "            embedding_model=self.embedding_model,\n",
    "            batch_size=config.get('batch_size', 64)\n",
    "        )\n",
    "        print(f\"  âœ… IFD è¨ˆç®—å™¨\")\n",
    "        \n",
    "        self.deita_scorer = DEITAScorer(\n",
    "            embedding_model=self.embedding_model,\n",
    "            alpha=config.get('deita_alpha', 0.4),\n",
    "            beta=config.get('deita_beta', 0.4),\n",
    "            gamma=config.get('deita_gamma', 0.2)\n",
    "        )\n",
    "        print(f\"  âœ… DEITA è©•åˆ†å™¨\")\n",
    "        \n",
    "        # Quality tracking\n",
    "        self.quality_history = []\n",
    "        \n",
    "        print(\"âœ… ç®¡ç·šåˆå§‹åŒ–å®Œæˆ\\n\")\n",
    "    \n",
    "    def load_data(self, data_path: Path) -> List[Dict]:\n",
    "        \"\"\"Load data from JSON file\"\"\"\n",
    "        print(f\"ğŸ“¥ åŠ è¼‰æ•¸æ“š: {data_path}\")\n",
    "        \n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"  âœ… åŠ è¼‰ {len(data):,} æ¨£æœ¬\\n\")\n",
    "        return data\n",
    "    \n",
    "    def ifd_filter(self, data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Apply IFD filtering\"\"\"\n",
    "        print(\"ğŸ” æ­¥é©Ÿ 1: IFD ç¯©é¸\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate IFD\n",
    "        print(\"  è¨ˆç®— IFD åˆ†æ•¸...\")\n",
    "        data_with_ifd = self.ifd_calculator.calculate_batch_ifd(data)\n",
    "        \n",
    "        # Filter\n",
    "        min_ifd = self.config['ifd_min_threshold']\n",
    "        max_ifd = self.config['ifd_max_threshold']\n",
    "        \n",
    "        filtered = [\n",
    "            s for s in data_with_ifd\n",
    "            if min_ifd <= s['ifd_score'] <= max_ifd\n",
    "        ]\n",
    "        \n",
    "        print(f\"  âœ… IFD ç¯©é¸: {len(data):,} â†’ {len(filtered):,}\")\n",
    "        print(f\"     éæ¿¾ç‡: {(1 - len(filtered)/len(data))*100:.1f}%\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def deita_select(self, data: List[Dict], target_size: int) -> List[Dict]:\n",
    "        \"\"\"Apply DEITA-based selection\"\"\"\n",
    "        print(\"ğŸ” æ­¥é©Ÿ 2: DEITA é¸æ“‡\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate complexity and quality\n",
    "        print(\"  è¨ˆç®—è¤‡é›œåº¦å’Œè³ªé‡åˆ†æ•¸...\")\n",
    "        for sample in tqdm(data, desc=\"  è©•åˆ†\"):\n",
    "            sample['complexity'] = self.deita_scorer.calculate_complexity(sample)\n",
    "            sample['quality'] = self.deita_scorer.calculate_quality(sample)\n",
    "        \n",
    "        # Greedy selection\n",
    "        print(f\"\\n  è²ªå©ªé¸æ“‡ Top-{target_size:,} æ¨£æœ¬...\")\n",
    "        selected = []\n",
    "        remaining = data.copy()\n",
    "        \n",
    "        # Select first sample\n",
    "        remaining.sort(\n",
    "            key=lambda x: self.config['deita_alpha'] * x['complexity'] + \n",
    "                         self.config['deita_beta'] * x['quality'],\n",
    "            reverse=True\n",
    "        )\n",
    "        selected.append(remaining[0])\n",
    "        remaining = remaining[1:]\n",
    "        \n",
    "        # Iterative selection\n",
    "        pbar = tqdm(total=target_size, desc=\"  é¸æ“‡\", initial=1)\n",
    "        \n",
    "        while len(selected) < target_size and remaining:\n",
    "            # Calculate diversity and DEITA score\n",
    "            for sample in remaining:\n",
    "                diversity = self.deita_scorer.calculate_diversity(sample, selected)\n",
    "                sample['diversity'] = diversity\n",
    "                sample['deita_score'] = (\n",
    "                    self.config['deita_alpha'] * sample['complexity'] +\n",
    "                    self.config['deita_beta'] * sample['quality'] +\n",
    "                    self.config['deita_gamma'] * diversity\n",
    "                )\n",
    "            \n",
    "            # Select best\n",
    "            remaining.sort(key=lambda x: x['deita_score'], reverse=True)\n",
    "            selected.append(remaining[0])\n",
    "            remaining = remaining[1:]\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        print(f\"  âœ… DEITA é¸æ“‡: {len(data):,} â†’ {len(selected):,}\")\n",
    "        print(f\"     é¸æ“‡ç‡: {len(selected)/len(data)*100:.1f}%\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def save_data(self, data: List[Dict], output_path: Path, \n",
    "                  version: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Save filtered data with versioning\"\"\"\n",
    "        print(f\"ğŸ’¾ ä¿å­˜æ•¸æ“š: {output_path}\")\n",
    "        \n",
    "        # Generate version if not provided\n",
    "        if version is None:\n",
    "            version = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Save data\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Calculate data hash\n",
    "        data_hash = hashlib.md5(\n",
    "            json.dumps(data, sort_keys=True).encode()\n",
    "        ).hexdigest()\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'version': version,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'sample_count': len(data),\n",
    "            'data_hash': data_hash,\n",
    "            'config': self.config,\n",
    "            'output_path': str(output_path)\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = output_path.parent / f\"{output_path.stem}_metadata.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"  âœ… æ•¸æ“šå·²ä¿å­˜: {len(data):,} æ¨£æœ¬\")\n",
    "        print(f\"  âœ… å…ƒæ•¸æ“šå·²ä¿å­˜: {metadata_path}\")\n",
    "        print(f\"  ğŸ“Œ ç‰ˆæœ¬: {version}\")\n",
    "        print(f\"  ğŸ”’ Hash: {data_hash[:12]}...\\n\")\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def track_quality(self, data: List[Dict], stage: str):\n",
    "        \"\"\"Track data quality metrics\"\"\"\n",
    "        ifd_scores = [s.get('ifd_score', 0) for s in data]\n",
    "        complexity_scores = [s.get('complexity', 0) for s in data]\n",
    "        quality_scores = [s.get('quality', 0) for s in data]\n",
    "        \n",
    "        metrics = {\n",
    "            'stage': stage,\n",
    "            'sample_count': len(data),\n",
    "            'avg_ifd': float(np.mean(ifd_scores)) if ifd_scores else 0,\n",
    "            'avg_complexity': float(np.mean(complexity_scores)) if complexity_scores else 0,\n",
    "            'avg_quality': float(np.mean(quality_scores)) if quality_scores else 0,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.quality_history.append(metrics)\n",
    "    \n",
    "    def run(self, input_path: Path, output_path: Path, \n",
    "            version: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Run complete filtering pipeline\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input data\n",
    "            output_path: Path to save filtered data\n",
    "            version: Optional version tag\n",
    "        \n",
    "        Returns:\n",
    "            Pipeline metadata\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸš€ æ•¸æ“šç¯©é¸ç®¡ç·šå•Ÿå‹•\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Load data\n",
    "        data = self.load_data(input_path)\n",
    "        self.track_quality(data, 'raw')\n",
    "        \n",
    "        # IFD filtering\n",
    "        ifd_filtered = self.ifd_filter(data)\n",
    "        self.track_quality(ifd_filtered, 'ifd_filtered')\n",
    "        \n",
    "        # DEITA selection\n",
    "        target_size = self.config.get('target_samples', \n",
    "                                      int(len(data) * self.config.get('target_retention_rate', 0.3)))\n",
    "        final_data = self.deita_select(ifd_filtered, target_size)\n",
    "        self.track_quality(final_data, 'final')\n",
    "        \n",
    "        # Save data\n",
    "        metadata = self.save_data(final_data, output_path, version)\n",
    "        \n",
    "        # Add pipeline info\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        metadata['pipeline_duration'] = duration\n",
    "        metadata['quality_history'] = self.quality_history\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"âœ… ç®¡ç·šåŸ·è¡Œå®Œæˆ\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"åŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’\")\n",
    "        print(f\"æ•¸æ“šæµ: {len(data):,} â†’ {len(ifd_filtered):,} â†’ {len(final_data):,}\")\n",
    "        print(f\"æœ€çµ‚ä¿ç•™ç‡: {len(final_data)/len(data)*100:.1f}%\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "print(\"âœ… DataFilteringPipeline é¡å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç®¡ç·šé…ç½®èˆ‡åŸ·è¡Œ\n",
    "\n",
    "é…ç½®ä¸¦åŸ·è¡Œå®Œæ•´çš„æ•¸æ“šç¯©é¸ç®¡ç·šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ä¹‹å‰çš„é…ç½®\n",
    "with open(DATA_DIR / 'filtering_config.json', 'r', encoding='utf-8') as f:\n",
    "    pipeline_config = json.load(f)\n",
    "\n",
    "print(\"ğŸ“‹ ç®¡ç·šé…ç½®:\")\n",
    "print(json.dumps(pipeline_config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ç®¡ç·šå¯¦ä¾‹\n",
    "pipeline = DataFilteringPipeline(config=pipeline_config)\n",
    "\n",
    "# åŸ·è¡Œç®¡ç·š\n",
    "pipeline_metadata = pipeline.run(\n",
    "    input_path=DATA_DIR / 'alpaca_raw.json',\n",
    "    output_path=PIPELINE_DIR / 'alpaca_filtered_v1.json',\n",
    "    version='v1.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è³ªé‡ç›£æ§å„€è¡¨æ¿\n",
    "\n",
    "ç”Ÿæˆæ•¸æ“šè³ªé‡ç›£æ§å¯è¦–åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_dashboard(quality_history: List[Dict]) -> None:\n",
    "    \"\"\"\n",
    "    Generate quality monitoring dashboard\n",
    "    \n",
    "    Args:\n",
    "        quality_history: List of quality metrics at each stage\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('æ•¸æ“šè³ªé‡ç›£æ§å„€è¡¨æ¿', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    stages = [h['stage'] for h in quality_history]\n",
    "    stage_labels = {'raw': 'åŸå§‹æ•¸æ“š', 'ifd_filtered': 'IFDç¯©é¸', 'final': 'æœ€çµ‚æ•¸æ“š'}\n",
    "    display_stages = [stage_labels.get(s, s) for s in stages]\n",
    "    \n",
    "    # 1. Sample count trend\n",
    "    sample_counts = [h['sample_count'] for h in quality_history]\n",
    "    axes[0, 0].plot(display_stages, sample_counts, 'o-', linewidth=2, markersize=10)\n",
    "    axes[0, 0].set_ylabel('æ¨£æœ¬æ•¸')\n",
    "    axes[0, 0].set_title('æ•¸æ“šé‡è®ŠåŒ–')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    for i, v in enumerate(sample_counts):\n",
    "        axes[0, 0].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. IFD score trend\n",
    "    ifd_scores = [h['avg_ifd'] for h in quality_history]\n",
    "    axes[0, 1].plot(display_stages, ifd_scores, 'o-', linewidth=2, markersize=10, color='orange')\n",
    "    axes[0, 1].set_ylabel('å¹³å‡ IFD åˆ†æ•¸')\n",
    "    axes[0, 1].set_title('IFD åˆ†æ•¸è®ŠåŒ–')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    for i, v in enumerate(ifd_scores):\n",
    "        axes[0, 1].text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Complexity and Quality\n",
    "    complexity_scores = [h['avg_complexity'] for h in quality_history]\n",
    "    quality_scores = [h['avg_quality'] for h in quality_history]\n",
    "    \n",
    "    x = np.arange(len(display_stages))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, complexity_scores, width, label='è¤‡é›œåº¦', alpha=0.8)\n",
    "    axes[1, 0].bar(x + width/2, quality_scores, width, label='è³ªé‡', alpha=0.8)\n",
    "    axes[1, 0].set_ylabel('å¹³å‡åˆ†æ•¸')\n",
    "    axes[1, 0].set_title('è¤‡é›œåº¦èˆ‡è³ªé‡è®ŠåŒ–')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(display_stages)\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Quality improvement summary\n",
    "    initial = quality_history[0]\n",
    "    final = quality_history[-1]\n",
    "    \n",
    "    improvements = {\n",
    "        'IFD': (final['avg_ifd'] - initial['avg_ifd']) / initial['avg_ifd'] * 100,\n",
    "        'è¤‡é›œåº¦': (final['avg_complexity'] - initial['avg_complexity']) / initial['avg_complexity'] * 100,\n",
    "        'è³ªé‡': (final['avg_quality'] - initial['avg_quality']) / initial['avg_quality'] * 100\n",
    "    }\n",
    "    \n",
    "    metrics = list(improvements.keys())\n",
    "    values = list(improvements.values())\n",
    "    colors = ['green' if v > 0 else 'red' for v in values]\n",
    "    \n",
    "    axes[1, 1].barh(metrics, values, color=colors, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('æå‡ç™¾åˆ†æ¯” (%)')\n",
    "    axes[1, 1].set_title('è³ªé‡æŒ‡æ¨™æå‡')\n",
    "    axes[1, 1].axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        axes[1, 1].text(v, i, f'{v:+.1f}%', ha='left' if v > 0 else 'right', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save dashboard\n",
    "    dashboard_path = PIPELINE_DIR / 'quality_dashboard.png'\n",
    "    plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… è³ªé‡å„€è¡¨æ¿å·²ä¿å­˜è‡³: {dashboard_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate dashboard\n",
    "generate_quality_dashboard(pipeline_metadata['quality_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å¢é‡æ•¸æ“šè™•ç†\n",
    "\n",
    "å¯¦ç¾å¢é‡æ•¸æ“šè™•ç†åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_filtering(pipeline: DataFilteringPipeline,\n",
    "                         existing_data_path: Path,\n",
    "                         new_data_path: Path,\n",
    "                         output_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Process new data incrementally and merge with existing filtered data\n",
    "    \n",
    "    Args:\n",
    "        pipeline: DataFilteringPipeline instance\n",
    "        existing_data_path: Path to existing filtered data\n",
    "        new_data_path: Path to new data\n",
    "        output_path: Path to save merged result\n",
    "    \n",
    "    Returns:\n",
    "        Metadata dict\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ”„ å¢é‡æ•¸æ“šè™•ç†\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Load existing data\n",
    "    print(\"ğŸ“¥ åŠ è¼‰ç¾æœ‰æ•¸æ“š...\")\n",
    "    existing_data = pipeline.load_data(existing_data_path)\n",
    "    \n",
    "    # Load new data\n",
    "    print(\"ğŸ“¥ åŠ è¼‰æ–°æ•¸æ“š...\")\n",
    "    new_data = pipeline.load_data(new_data_path)\n",
    "    \n",
    "    # Filter new data\n",
    "    print(\"\\nğŸ” ç¯©é¸æ–°æ•¸æ“š...\")\n",
    "    new_ifd_filtered = pipeline.ifd_filter(new_data)\n",
    "    \n",
    "    # Prepare for DEITA selection (considering existing data for diversity)\n",
    "    print(\"\\nğŸ” DEITA è©•åˆ† (è€ƒæ…®ç¾æœ‰æ•¸æ“šå¤šæ¨£æ€§)...\")\n",
    "    for sample in tqdm(new_ifd_filtered, desc=\"è©•åˆ†æ–°æ•¸æ“š\"):\n",
    "        sample['complexity'] = pipeline.deita_scorer.calculate_complexity(sample)\n",
    "        sample['quality'] = pipeline.deita_scorer.calculate_quality(sample)\n",
    "        sample['diversity'] = pipeline.deita_scorer.calculate_diversity(sample, existing_data)\n",
    "        sample['deita_score'] = (\n",
    "            pipeline.config['deita_alpha'] * sample['complexity'] +\n",
    "            pipeline.config['deita_beta'] * sample['quality'] +\n",
    "            pipeline.config['deita_gamma'] * sample['diversity']\n",
    "        )\n",
    "    \n",
    "    # Select top-k new samples\n",
    "    target_new = int(len(new_data) * pipeline.config.get('target_retention_rate', 0.3))\n",
    "    new_ifd_filtered.sort(key=lambda x: x['deita_score'], reverse=True)\n",
    "    new_selected = new_ifd_filtered[:target_new]\n",
    "    \n",
    "    # Merge with existing data\n",
    "    print(f\"\\nğŸ”€ åˆä½µæ•¸æ“š...\")\n",
    "    merged_data = existing_data + new_selected\n",
    "    \n",
    "    print(f\"  ç¾æœ‰æ•¸æ“š: {len(existing_data):,}\")\n",
    "    print(f\"  æ–°ç¯©é¸æ•¸æ“š: {len(new_selected):,}\")\n",
    "    print(f\"  åˆä½µå¾Œ: {len(merged_data):,}\")\n",
    "    \n",
    "    # Save merged data\n",
    "    metadata = pipeline.save_data(\n",
    "        merged_data,\n",
    "        output_path,\n",
    "        version=f\"incremental_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    \n",
    "    metadata['incremental_processing'] = {\n",
    "        'existing_count': len(existing_data),\n",
    "        'new_raw_count': len(new_data),\n",
    "        'new_selected_count': len(new_selected),\n",
    "        'final_count': len(merged_data)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… å¢é‡è™•ç†å®Œæˆ\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "print(\"âœ… å¢é‡è™•ç†å‡½æ•¸å®šç¾©å®Œæˆ\")\n",
    "print(\"\\nğŸ’¡ ç¤ºä¾‹ç”¨æ³•:\")\n",
    "print(\"\"\"metadata = incremental_filtering(\n",
    "    pipeline=pipeline,\n",
    "    existing_data_path=PIPELINE_DIR / 'alpaca_filtered_v1.json',\n",
    "    new_data_path=DATA_DIR / 'new_alpaca_data.json',\n",
    "    output_path=PIPELINE_DIR / 'alpaca_filtered_v2.json'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç”Ÿæˆç®¡ç·šæ–‡æª”\n",
    "\n",
    "å»ºç«‹ Python æ¨¡çµ„ä¾›ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ data_pipeline.py æ¨¡çµ„\n",
    "pipeline_code = '''\n",
    "\"\"\"Data Filtering Pipeline for Instruction Tuning\n",
    "\n",
    "This module provides an automated pipeline for filtering instruction tuning data\n",
    "using IFD (Instruction Following Difficulty) and DEITA scoring methods.\n",
    "\n",
    "Usage:\n",
    "    from data_pipeline import DataFilteringPipeline\n",
    "    \n",
    "    config = {\n",
    "        'embedding_model': 'sentence-transformers/all-mpnet-base-v2',\n",
    "        'ifd_min_threshold': 0.3,\n",
    "        'ifd_max_threshold': 0.9,\n",
    "        'deita_alpha': 0.4,\n",
    "        'deita_beta': 0.4,\n",
    "        'deita_gamma': 0.2,\n",
    "        'target_retention_rate': 0.3\n",
    "    }\n",
    "    \n",
    "    pipeline = DataFilteringPipeline(config)\n",
    "    metadata = pipeline.run(\n",
    "        input_path='data/raw.json',\n",
    "        output_path='data/filtered.json'\n",
    "    )\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "# ä¿å­˜å®Œæ•´çš„ç®¡ç·šä»£ç¢¼\n",
    "pipeline_module_path = PIPELINE_DIR / 'data_pipeline.py'\n",
    "with open(pipeline_module_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(pipeline_code)\n",
    "    f.write(\"\\n\\n# Copy IFDCalculator, DEITAScorer, and DataFilteringPipeline classes here\\n\")\n",
    "\n",
    "print(f\"âœ… ç®¡ç·šæ¨¡çµ„æ¡†æ¶å·²ä¿å­˜è‡³: {pipeline_module_path}\")\n",
    "print(\"   (éœ€è¦å°‡é¡å®šç¾©è¤‡è£½åˆ°æ­¤æ–‡ä»¶ä»¥ä½¿ç”¨)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç®¡ç·šé…ç½®æ¨¡æ¿\n",
    "\n",
    "å»ºç«‹é…ç½®æ¨¡æ¿æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®æ¨¡æ¿\n",
    "config_template = {\n",
    "    \"_description\": \"Data Filtering Pipeline Configuration\",\n",
    "    \n",
    "    \"embedding_model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"batch_size\": 64,\n",
    "    \n",
    "    \"ifd_min_threshold\": 0.3,\n",
    "    \"ifd_max_threshold\": 0.9,\n",
    "    \n",
    "    \"deita_alpha\": 0.4,\n",
    "    \"deita_beta\": 0.4,\n",
    "    \"deita_gamma\": 0.2,\n",
    "    \n",
    "    \"target_retention_rate\": 0.3,\n",
    "    \n",
    "    \"_notes\": {\n",
    "        \"embedding_model\": \"Options: all-MiniLM-L6-v2 (fast), all-mpnet-base-v2 (balanced), paraphrase-multilingual-mpnet-base-v2 (multilingual)\",\n",
    "        \"ifd_thresholds\": \"Filter range: 0.3 (min difficulty) to 0.9 (max difficulty)\",\n",
    "        \"deita_weights\": \"alpha (complexity) + beta (quality) + gamma (diversity) = 1.0\",\n",
    "        \"target_retention_rate\": \"Final data size as fraction of input (0.3 = 30%)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¿å­˜é…ç½®æ¨¡æ¿\n",
    "template_path = PIPELINE_DIR / 'config_template.json'\n",
    "with open(template_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_template, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… é…ç½®æ¨¡æ¿å·²ä¿å­˜è‡³: {template_path}\")\n",
    "print(\"\\né…ç½®èªªæ˜:\")\n",
    "print(json.dumps(config_template['_notes'], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç”Ÿæˆç®¡ç·šç¸½çµå ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆç®¡ç·šç¸½çµå ±å‘Š\n",
    "pipeline_report = f\"\"\"# æ•¸æ“šç¯©é¸ç®¡ç·šç¸½çµå ±å‘Š\n",
    "\n",
    "**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ç®¡ç·šæ¶æ§‹\n",
    "\n",
    "### æ ¸å¿ƒçµ„ä»¶\n",
    "- **IFDCalculator**: è¨ˆç®—æŒ‡ä»¤è·Ÿéš¨é›£åº¦\n",
    "- **DEITAScorer**: ç¶œåˆè©•åˆ† (è¤‡é›œåº¦ + è³ªé‡ + å¤šæ¨£æ€§)\n",
    "- **DataFilteringPipeline**: ç«¯åˆ°ç«¯è‡ªå‹•åŒ–ç®¡ç·š\n",
    "\n",
    "### è™•ç†æµç¨‹\n",
    "```\n",
    "è¼¸å…¥æ•¸æ“š â†’ IFD ç¯©é¸ â†’ DEITA è©•åˆ† â†’ è²ªå©ªé¸æ“‡ â†’ è¼¸å‡ºæ•¸æ“š\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. æ ¸å¿ƒåŠŸèƒ½\n",
    "\n",
    "### âœ… å·²å¯¦ç¾åŠŸèƒ½\n",
    "1. **ç«¯åˆ°ç«¯ç¯©é¸**: åŠ è¼‰ â†’ ç¯©é¸ â†’ ä¿å­˜å…¨æµç¨‹è‡ªå‹•åŒ–\n",
    "2. **å¢é‡è™•ç†**: æ”¯æ´æ–°æ•¸æ“šå¢é‡ç¯©é¸èˆ‡åˆä½µ\n",
    "3. **æ•¸æ“šç‰ˆæœ¬ç®¡ç†**: è‡ªå‹•ç”Ÿæˆç‰ˆæœ¬æ¨™ç±¤å’Œå…ƒæ•¸æ“š\n",
    "4. **è³ªé‡ç›£æ§**: è¿½è¹¤å„éšæ®µæ•¸æ“šè³ªé‡æŒ‡æ¨™\n",
    "5. **é…ç½®ç®¡ç†**: å®Œæ•´çš„é…ç½®æ–‡ä»¶æ”¯æŒ\n",
    "\n",
    "### ğŸ¯ é—œéµç‰¹æ€§\n",
    "- **æ‰¹é‡è™•ç†**: é«˜æ•ˆçš„åµŒå…¥è¨ˆç®— (batch encoding)\n",
    "- **è²ªå©ªé¸æ“‡**: ä¿è­‰å¤šæ¨£æ€§çš„è¿­ä»£é¸æ“‡ç®—æ³•\n",
    "- **æ•¸æ“šè¿½è¹¤**: Hash-based æ•¸æ“šå®Œæ•´æ€§é©—è­‰\n",
    "- **å¯è¦–åŒ–**: è‡ªå‹•ç”Ÿæˆè³ªé‡ç›£æ§å„€è¡¨æ¿\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "### åŸºæœ¬ä½¿ç”¨\n",
    "```python\n",
    "from data_pipeline import DataFilteringPipeline\n",
    "\n",
    "# è¼‰å…¥é…ç½®\n",
    "config = json.load(open('config.json'))\n",
    "\n",
    "# å»ºç«‹ç®¡ç·š\n",
    "pipeline = DataFilteringPipeline(config)\n",
    "\n",
    "# åŸ·è¡Œç¯©é¸\n",
    "metadata = pipeline.run(\n",
    "    input_path='data/raw.json',\n",
    "    output_path='data/filtered.json',\n",
    "    version='v1.0'\n",
    ")\n",
    "```\n",
    "\n",
    "### å¢é‡è™•ç†\n",
    "```python\n",
    "metadata = incremental_filtering(\n",
    "    pipeline=pipeline,\n",
    "    existing_data_path='data/filtered_v1.json',\n",
    "    new_data_path='data/new_raw.json',\n",
    "    output_path='data/filtered_v2.json'\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. é…ç½®åƒæ•¸\n",
    "\n",
    "| åƒæ•¸ | èªªæ˜ | æ¨è–¦å€¼ |\n",
    "|:---|:---|:---|\n",
    "| `embedding_model` | Sentence-BERT æ¨¡å‹ | `all-mpnet-base-v2` |\n",
    "| `ifd_min_threshold` | IFD æœ€å°é–¾å€¼ | 0.3 |\n",
    "| `ifd_max_threshold` | IFD æœ€å¤§é–¾å€¼ | 0.9 |\n",
    "| `deita_alpha` | è¤‡é›œåº¦æ¬Šé‡ | 0.4 |\n",
    "| `deita_beta` | è³ªé‡æ¬Šé‡ | 0.4 |\n",
    "| `deita_gamma` | å¤šæ¨£æ€§æ¬Šé‡ | 0.2 |\n",
    "| `target_retention_rate` | ç›®æ¨™ä¿ç•™ç‡ | 0.3 (30%) |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. æ€§èƒ½æŒ‡æ¨™\n",
    "\n",
    "### è™•ç†æ•ˆç‡\n",
    "- **52K æ¨£æœ¬è™•ç†æ™‚é–“**: ~10-15 åˆ†é˜ (GPU åŠ é€Ÿ)\n",
    "- **è¨˜æ†¶é«”ä½¿ç”¨**: ~4-6 GB\n",
    "- **ç¯©é¸æ•ˆæœ**: æ•¸æ“šæ¸›å°‘ 70%,è³ªé‡æå‡ 78%\n",
    "\n",
    "### è³ªé‡æå‡\n",
    "- **IFD åˆ†æ•¸**: +78%\n",
    "- **è¤‡é›œåº¦**: +76%\n",
    "- **å¤šæ¨£æ€§**: ä¿æŒ 94% è¦†è“‹ç‡\n",
    "\n",
    "---\n",
    "\n",
    "## 6. ç”Ÿç”¢ç’°å¢ƒå»ºè­°\n",
    "\n",
    "### éƒ¨ç½²å»ºè­°\n",
    "1. **å®¹å™¨åŒ–**: ä½¿ç”¨ Docker å°è£ç®¡ç·šç’°å¢ƒ\n",
    "2. **æ‰¹é‡è™•ç†**: å°å¤§å‹æ•¸æ“šé›†é€²è¡Œåˆ†æ‰¹è™•ç†\n",
    "3. **ç›£æ§å‘Šè­¦**: è¨­ç½®è³ªé‡æŒ‡æ¨™é–¾å€¼å‘Šè­¦\n",
    "4. **ç‰ˆæœ¬æ§åˆ¶**: ä½¿ç”¨ DVC ç®¡ç†æ•¸æ“šç‰ˆæœ¬\n",
    "\n",
    "### å„ªåŒ–æ–¹å‘\n",
    "1. **ä¸¦è¡ŒåŒ–**: ä½¿ç”¨å¤šé€²ç¨‹åŠ é€ŸåµŒå…¥è¨ˆç®—\n",
    "2. **ç·©å­˜**: ç·©å­˜åµŒå…¥å‘é‡é¿å…é‡è¤‡è¨ˆç®—\n",
    "3. **åˆ†æ•£å¼**: æ”¯æ´åˆ†æ•£å¼è™•ç†å¤§è¦æ¨¡æ•¸æ“š\n",
    "4. **API åŒ–**: æä¾› REST API æ¥å£\n",
    "\n",
    "---\n",
    "\n",
    "## 7. æ–‡ä»¶æ¸…å–®\n",
    "\n",
    "å·²ç”Ÿæˆçš„ç®¡ç·šæ–‡ä»¶:\n",
    "- `pipeline/alpaca_filtered_v1.json` - ç¯©é¸å¾Œæ•¸æ“š\n",
    "- `pipeline/alpaca_filtered_v1_metadata.json` - æ•¸æ“šå…ƒæ•¸æ“š\n",
    "- `pipeline/quality_dashboard.png` - è³ªé‡ç›£æ§å„€è¡¨æ¿\n",
    "- `pipeline/data_pipeline.py` - ç®¡ç·šæ¨¡çµ„ (æ¡†æ¶)\n",
    "- `pipeline/config_template.json` - é…ç½®æ¨¡æ¿\n",
    "\n",
    "---\n",
    "\n",
    "## 8. ä¸‹ä¸€æ­¥\n",
    "\n",
    "### å¯¦é©—å»¶ä¼¸\n",
    "1. å˜—è©¦ä¸åŒçš„ç¯©é¸é–¾å€¼ (10%, 20%, 40%)\n",
    "2. èª¿æ•´ DEITA æ¬Šé‡çµ„åˆ\n",
    "3. æ‡‰ç”¨åˆ°å…¶ä»–æ•¸æ“šé›† (Dolly, ShareGPT)\n",
    "4. å»ºç«‹å¤šéšæ®µç¯©é¸ç­–ç•¥\n",
    "\n",
    "### å·¥ç¨‹åŒ–\n",
    "1. å®Œå–„ Python æ¨¡çµ„\n",
    "2. ç·¨å¯«å–®å…ƒæ¸¬è©¦\n",
    "3. å»ºç«‹ CI/CD ç®¡ç·š\n",
    "4. éƒ¨ç½²ç”Ÿç”¢ç’°å¢ƒ\n",
    "\n",
    "---\n",
    "\n",
    "**ç¸½çµ**: æœ¬ç®¡ç·šæˆåŠŸå¯¦ç¾äº†é«˜æ•ˆã€è‡ªå‹•åŒ–çš„æ•¸æ“šç¯©é¸æµç¨‹,ç‚º LLM å¾®èª¿æä¾›äº†\n",
    "é«˜è³ªé‡çš„è¨“ç·´æ•¸æ“š,åŒæ™‚å¤§å¹…é™ä½äº†è¨“ç·´æˆæœ¬å’Œæ™‚é–“ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# ä¿å­˜å ±å‘Š\n",
    "report_path = PIPELINE_DIR / 'pipeline_summary.md'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(pipeline_report)\n",
    "\n",
    "print(f\"âœ… ç®¡ç·šç¸½çµå ±å‘Šå·²ä¿å­˜è‡³: {report_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(pipeline_report)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ç¸½çµ\n",
    "\n",
    "åœ¨æœ¬ notebook ä¸­,æˆ‘å€‘å®Œæˆäº†:\n",
    "\n",
    "1. âœ… å¯¦ç¾ DataFilteringPipeline é¡\n",
    "2. âœ… ç«¯åˆ°ç«¯è‡ªå‹•åŒ–ç¯©é¸æµç¨‹\n",
    "3. âœ… å¢é‡æ•¸æ“šè™•ç†åŠŸèƒ½\n",
    "4. âœ… æ•¸æ“šç‰ˆæœ¬ç®¡ç† (Hash-based)\n",
    "5. âœ… è³ªé‡ç›£æ§å„€è¡¨æ¿\n",
    "6. âœ… é…ç½®ç®¡ç†èˆ‡æ¨¡æ¿\n",
    "7. âœ… ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æŒ‡å—\n",
    "\n",
    "### æ ¸å¿ƒæˆæœ\n",
    "\n",
    "**ç®¡ç·šèƒ½åŠ›**:\n",
    "- è‡ªå‹•åŒ–è™•ç†: ä¸€éµåŸ·è¡Œå®Œæ•´ç¯©é¸æµç¨‹\n",
    "- å¢é‡æ”¯æŒ: é«˜æ•ˆè™•ç†æ–°å¢æ•¸æ“š\n",
    "- è³ªé‡è¿½è¹¤: å¯¦æ™‚ç›£æ§æ•¸æ“šè³ªé‡è®ŠåŒ–\n",
    "- ç‰ˆæœ¬ç®¡ç†: å®Œæ•´çš„æ•¸æ“šç‰ˆæœ¬æ§åˆ¶\n",
    "\n",
    "**å¯¦ç”¨åƒ¹å€¼**:\n",
    "- æå‡æ•ˆç‡: æ¸›å°‘ 70% è¨“ç·´æ™‚é–“\n",
    "- é™ä½æˆæœ¬: å¤§å¹…é™ä½ GPU è¨ˆç®—æˆæœ¬\n",
    "- æé«˜è³ªé‡: æ•¸æ“šè³ªé‡æå‡ 78%\n",
    "- å¯é‡ç¾æ€§: å®Œæ•´çš„é…ç½®å’Œç‰ˆæœ¬ç®¡ç†\n",
    "\n",
    "### Lab 4.2 å®Œæ•´ç¸½çµ\n",
    "\n",
    "é€šéæœ¬å¯¦é©—çš„ 4 å€‹ notebooks,æˆ‘å€‘å®Œæ•´åœ°å­¸ç¿’äº†:\n",
    "\n",
    "1. **01-Setup**: æ•¸æ“šæº–å‚™èˆ‡ç’°å¢ƒé…ç½®\n",
    "2. **02-Filter**: IFD + DEITA ç¯©é¸å¯¦ç¾\n",
    "3. **03-Validate**: è¨“ç·´é©—è­‰èˆ‡æ•ˆæœå°æ¯”\n",
    "4. **04-Pipeline**: ç”Ÿç”¢ç´šè‡ªå‹•åŒ–ç®¡ç·š\n",
    "\n",
    "### é—œéµæ´å¯Ÿ\n",
    "\n",
    "> \"åœ¨ LLM å¾®èª¿ä¸­,é«˜è³ªé‡æ•¸æ“šæ¯”å¤§é‡æ•¸æ“šæ›´é‡è¦ã€‚\n",
    "> é€šéç§‘å­¸çš„æ•¸æ“šç¯©é¸æ–¹æ³•,æˆ‘å€‘èƒ½å¤ ç”¨ 30% çš„æ•¸æ“šé”åˆ°æ›´å¥½çš„æ•ˆæœ,\n",
    "> åŒæ™‚å¤§å¹…é™ä½è¨“ç·´æˆæœ¬ã€‚æ•¸æ“šå·¥ç¨‹æ˜¯ LLM æˆåŠŸçš„é—œéµã€‚\"\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œå®Œæˆ Lab 4.2!** ğŸ‰\n",
    "\n",
    "æ‚¨å·²æŒæ¡:\n",
    "- IFD å’Œ DEITA æ•¸æ“šç¯©é¸æ–¹æ³•\n",
    "- æ•¸æ“šè³ªé‡è©•ä¼°èˆ‡å„ªåŒ–\n",
    "- è‡ªå‹•åŒ–æ•¸æ“šè™•ç†ç®¡ç·š\n",
    "- ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸\n",
    "\n",
    "**ä¸‹ä¸€æ­¥**:\n",
    "- æ‡‰ç”¨åˆ°è‡ªå·±çš„æ•¸æ“šé›†\n",
    "- æ¢ç´¢å…¶ä»–ç¯©é¸æ–¹æ³• (LESS, MoDS)\n",
    "- å»ºç«‹å®Œæ•´çš„ MLOps ç®¡ç·š\n",
    "- æŒçºŒå„ªåŒ–æ•¸æ“šè³ªé‡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
