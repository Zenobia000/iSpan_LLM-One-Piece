{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.3 Part 1: Basic FastAPI Service\n",
    "\n",
    "## Objectives\n",
    "- Learn FastAPI fundamentals\n",
    "- Design API endpoints for LLM\n",
    "- Implement request validation\n",
    "- Manage model lifecycle\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. FastAPI Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check installations\n",
    "try:\n",
    "    import fastapi\n",
    "    import uvicorn\n",
    "    import pydantic\n",
    "    print(f\"‚úÖ FastAPI: {fastapi.__version__}\")\n",
    "    print(f\"‚úÖ Uvicorn: {uvicorn.__version__}\")\n",
    "    print(f\"‚úÖ Pydantic: {pydantic.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing: {e}\")\n",
    "    print(\"\\nInstall: pip install fastapi uvicorn[standard] pydantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Basic API\n",
    "\n",
    "Let's create a simple FastAPI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_basic.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI(title=\"LLM Service API\", version=\"1.0.0\")\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    message: str\n",
    "\n",
    "@app.get(\"/\", response_model=HealthResponse)\n",
    "async def root():\n",
    "    return HealthResponse(\n",
    "        status=\"ok\",\n",
    "        message=\"LLM Service is running\"\n",
    "    )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Server (in terminal)\n",
    "\n",
    "```bash\n",
    "# Start the server\n",
    "uvicorn app_basic:app --host 0.0.0.0 --port 8000 --reload\n",
    "\n",
    "# Server will be at: http://localhost:8000\n",
    "# Interactive docs: http://localhost:8000/docs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API (if server is running)\n",
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/health\")\n",
    "    print(f\"‚úÖ Server is running: {response.json()}\")\n",
    "except:\n",
    "    print(\"‚ùå Server not running. Start with: uvicorn app_basic:app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. LLM Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_llm.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "app = FastAPI(title=\"LLM API\", version=\"1.0.0\")\n",
    "\n",
    "# Global model and tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str = Field(..., min_length=1, max_length=2048)\n",
    "    max_tokens: int = Field(default=100, ge=1, le=500)\n",
    "    temperature: float = Field(default=0.8, ge=0.0, le=2.0)\n",
    "    top_p: float = Field(default=0.95, ge=0.0, le=1.0)\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"prompt\": \"Explain machine learning:\",\n",
    "                \"max_tokens\": 100,\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 0.95\n",
    "            }\n",
    "        }\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    text: str\n",
    "    tokens_generated: int\n",
    "    model: str\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Load model on startup.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model_name = \"gpt2\"  # Use small model for demo\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded: {model_name}\")\n",
    "\n",
    "@app.post(\"/v1/completions\", response_model=GenerateResponse)\n",
    "async def generate(request: GenerateRequest):\n",
    "    \"\"\"Generate text completion.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(request.prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=request.max_tokens,\n",
    "                temperature=request.temperature,\n",
    "                top_p=request.top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])\n",
    "        \n",
    "        return GenerateResponse(\n",
    "            text=generated_text,\n",
    "            tokens_generated=tokens_generated,\n",
    "            model=\"gpt2\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generate endpoint\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "request_data = {\n",
    "    \"prompt\": \"The future of artificial intelligence is\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95\n",
    "}\n",
    "\n",
    "print(\"Sending request...\\n\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{API_URL}/v1/completions\",\n",
    "        json=request_data,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "    \n",
    "    result = response.json()\n",
    "    \n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ùå Server not running.\")\n",
    "    print(\"Start server: uvicorn app_llm:app --reload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Chat Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_chat.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Literal\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Global state\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: List[Message]\n",
    "    max_tokens: int = 100\n",
    "    temperature: float = 0.8\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    message: Message\n",
    "    tokens: int\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    global model, tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"‚úÖ Model loaded\")\n",
    "\n",
    "@app.post(\"/v1/chat/completions\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"Chat completions endpoint.\"\"\"\n",
    "    # Format messages into prompt\n",
    "    prompt = \"\"\n",
    "    for msg in request.messages:\n",
    "        if msg.role == \"system\":\n",
    "            prompt += f\"System: {msg.content}\\n\"\n",
    "        elif msg.role == \"user\":\n",
    "            prompt += f\"User: {msg.content}\\n\"\n",
    "        elif msg.role == \"assistant\":\n",
    "            prompt += f\"Assistant: {msg.content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant:\"\n",
    "    \n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=request.max_tokens,\n",
    "            temperature=request.temperature,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    response_text = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "    tokens = len(outputs[0]) - len(inputs.input_ids[0])\n",
    "    \n",
    "    return ChatResponse(\n",
    "        message=Message(role=\"assistant\", content=response_text.strip()),\n",
    "        tokens=tokens\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Chat Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chat endpoint\n",
    "chat_request = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "print(\"Testing chat endpoint...\\n\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8001/v1/chat/completions\",\n",
    "        json=chat_request\n",
    "    )\n",
    "    \n",
    "    result = response.json()\n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ùå Server not running on port 8001\")\n",
    "    print(\"Start: uvicorn app_chat:app --port 8001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Request Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced validation example\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "class AdvancedGenerateRequest(BaseModel):\n",
    "    prompt: str = Field(\n",
    "        ...,\n",
    "        min_length=1,\n",
    "        max_length=4096,\n",
    "        description=\"Input prompt for generation\"\n",
    "    )\n",
    "    max_tokens: int = Field(\n",
    "        default=100,\n",
    "        ge=1,\n",
    "        le=1000,\n",
    "        description=\"Maximum tokens to generate\"\n",
    "    )\n",
    "    temperature: float = Field(\n",
    "        default=0.8,\n",
    "        ge=0.0,\n",
    "        le=2.0,\n",
    "        description=\"Sampling temperature\"\n",
    "    )\n",
    "    stop: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=\"Stop sequences\"\n",
    "    )\n",
    "    \n",
    "    @validator('prompt')\n",
    "    def validate_prompt(cls, v):\n",
    "        \"\"\"Custom validation for prompt.\"\"\"\n",
    "        # Check for forbidden patterns\n",
    "        forbidden = ['<script>', 'DROP TABLE', 'rm -rf']\n",
    "        for pattern in forbidden:\n",
    "            if pattern.lower() in v.lower():\n",
    "                raise ValueError(f\"Forbidden pattern detected: {pattern}\")\n",
    "        return v\n",
    "    \n",
    "    @validator('temperature')\n",
    "    def validate_temperature(cls, v, values):\n",
    "        \"\"\"Warn about extreme temperatures.\"\"\"\n",
    "        if v < 0.1 or v > 1.5:\n",
    "            print(f\"‚ö†Ô∏è Unusual temperature: {v}\")\n",
    "        return v\n",
    "\n",
    "# Test validation\n",
    "print(\"Testing request validation:\\n\")\n",
    "\n",
    "# Valid request\n",
    "try:\n",
    "    valid_req = AdvancedGenerateRequest(\n",
    "        prompt=\"Hello world\",\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(f\"‚úÖ Valid request: {valid_req.prompt}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå {e}\")\n",
    "\n",
    "# Invalid request (too many tokens)\n",
    "try:\n",
    "    invalid_req = AdvancedGenerateRequest(\n",
    "        prompt=\"Test\",\n",
    "        max_tokens=2000  # Exceeds limit\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Invalid request caught: {e}\")\n",
    "\n",
    "# Forbidden pattern\n",
    "try:\n",
    "    forbidden_req = AdvancedGenerateRequest(\n",
    "        prompt=\"<script>alert('test')</script>\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Forbidden pattern caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_error_handling.py\n",
    "from fastapi import FastAPI, HTTPException, status\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ErrorResponse(BaseModel):\n",
    "    error: str\n",
    "    detail: str\n",
    "    request_id: str = None\n",
    "\n",
    "@app.exception_handler(ValidationError)\n",
    "async def validation_exception_handler(request, exc):\n",
    "    \"\"\"Handle validation errors.\"\"\"\n",
    "    logger.error(f\"Validation error: {exc}\")\n",
    "    return JSONResponse(\n",
    "        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "        content=ErrorResponse(\n",
    "            error=\"validation_error\",\n",
    "            detail=str(exc)\n",
    "        ).dict()\n",
    "    )\n",
    "\n",
    "@app.exception_handler(Exception)\n",
    "async def general_exception_handler(request, exc):\n",
    "    \"\"\"Handle general errors.\"\"\"\n",
    "    logger.error(f\"Unexpected error: {exc}\")\n",
    "    return JSONResponse(\n",
    "        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "        content=ErrorResponse(\n",
    "            error=\"internal_error\",\n",
    "            detail=\"An internal error occurred\"\n",
    "        ).dict()\n",
    "    )\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(prompt: str):\n",
    "    # Simulate error for demo\n",
    "    if \"error\" in prompt.lower():\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Prompt contains 'error' keyword\"\n",
    "        )\n",
    "    \n",
    "    return {\"text\": f\"Generated response for: {prompt}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "‚úÖ **Completed**:\n",
    "1. Created basic FastAPI application\n",
    "2. Implemented LLM endpoints (/completions, /chat)\n",
    "3. Added request validation with Pydantic\n",
    "4. Implemented error handling\n",
    "5. Managed model lifecycle\n",
    "\n",
    "üìö **Key Concepts**:\n",
    "- FastAPI automatic documentation\n",
    "- Pydantic data validation\n",
    "- Type hints and response models\n",
    "- Startup events for model loading\n",
    "- Exception handlers\n",
    "\n",
    "‚û°Ô∏è **Next**: In `02-Async_Processing.ipynb`, we'll learn:\n",
    "- Async/await patterns\n",
    "- Concurrent request handling\n",
    "- Streaming responses\n",
    "- Request queuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Lab 2.3 Part 1 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
