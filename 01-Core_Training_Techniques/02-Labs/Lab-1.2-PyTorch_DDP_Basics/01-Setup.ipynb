{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.2: PyTorch DDP åˆ†æ•£å¼è¨“ç·´åŸºç¤ - 01-Setup\n",
    "## ç’°å¢ƒè¨­ç½®èˆ‡åŸºç¤æ¦‚å¿µ\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ æ³¨æ„äº‹é …\n",
    "\n",
    "æœ¬å¯¦é©—å®¤åœ¨**å–®GPUç’°å¢ƒ**ä¸­æä¾›**æ¦‚å¿µæ¼”ç¤º**å’Œ**é…ç½®å­¸ç¿’**ã€‚\n",
    "- âœ… **å¯å­¸ç¿’**: DDP åŸç†ã€é…ç½®æ–¹æ³•ã€ä»£ç¢¼çµæ§‹\n",
    "- âš ï¸ **é™åˆ¶**: ç„¡æ³•å±•ç¤ºçœŸæ­£çš„å¤šGPUé€šè¨Šå’ŒåŠ é€Ÿæ•ˆæœ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. ç†è§£ PyTorch DDP çš„åŸºæœ¬æ¦‚å¿µ\n",
    "2. æŒæ¡åˆ†æ•£å¼è¨“ç·´çš„ç’°å¢ƒé…ç½®\n",
    "3. å­¸ç¿’é€²ç¨‹åˆå§‹åŒ–å’Œé€šè¨Šè¨­ç½®\n",
    "4. æº–å‚™è¨“ç·´æ•¸æ“šå’Œæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "print(f\"åˆ†æ•£å¼è¨“ç·´æ”¯æ´: {torch.distributed.is_available()}\")\n",
    "print(f\"NCCL å¾Œç«¯æ”¯æ´: {torch.distributed.is_nccl_available()}\")\n",
    "print(f\"Gloo å¾Œç«¯æ”¯æ´: {torch.distributed.is_gloo_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  è¨˜æ†¶é«”: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DDP æ ¸å¿ƒæ¦‚å¿µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP é—œéµæ¦‚å¿µæ¼”ç¤º\n",
    "print(\"=== PyTorch DDP æ ¸å¿ƒæ¦‚å¿µ ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. åŸºæœ¬è¡“èª:\")\n",
    "print(\"   - World Size: ç¸½çš„é€²ç¨‹æ•¸é‡ (é€šå¸¸ç­‰æ–¼GPUæ•¸é‡)\")\n",
    "print(\"   - Rank: æ¯å€‹é€²ç¨‹çš„å”¯ä¸€æ¨™è­˜ç¬¦ (0 åˆ° world_size-1)\")\n",
    "print(\"   - Local Rank: ç¯€é»å…§çš„GPUç·¨è™Ÿ\")\n",
    "print(\"   - Backend: é€šè¨Šå¾Œç«¯ (NCCL for GPU, Gloo for CPU)\")\n",
    "print()\n",
    "\n",
    "print(\"2. DDP å·¥ä½œæµç¨‹:\")\n",
    "print(\"   Step 1: åˆå§‹åŒ–é€²ç¨‹çµ„ (init_process_group)\")\n",
    "print(\"   Step 2: è¨­ç½®æœ¬åœ°è¨­å‚™ (cuda.set_device)\")\n",
    "print(\"   Step 3: åŒ…è£æ¨¡å‹ç‚ºDDP (DistributedDataParallel)\")\n",
    "print(\"   Step 4: ä½¿ç”¨DistributedSampleråˆ†å‰²æ•¸æ“š\")\n",
    "print(\"   Step 5: è¨“ç·´ (è‡ªå‹•æ¢¯åº¦åŒæ­¥)\")\n",
    "print(\"   Step 6: æ¸…ç† (destroy_process_group)\")\n",
    "print()\n",
    "\n",
    "print(\"3. é—œéµå„ªå‹¢:\")\n",
    "print(\"   - æ¢¯åº¦è‡ªå‹•åŒæ­¥ (All-Reduce)\")\n",
    "print(\"   - åƒæ•¸ä¸€è‡´æ€§ä¿è­‰\")\n",
    "print(\"   - é«˜æ•ˆçš„é€šè¨Šå„ªåŒ–\")\n",
    "print(\"   - å®¹éŒ¯å’Œæ•…éšœæ¢å¾©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. é€²ç¨‹åˆå§‹åŒ–å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ddp(rank, world_size, backend='nccl'):\n",
    "    \"\"\"\n",
    "    åˆå§‹åŒ–åˆ†æ•£å¼è¨“ç·´ç’°å¢ƒ\n",
    "    \n",
    "    Args:\n",
    "        rank: ç•¶å‰é€²ç¨‹ç·¨è™Ÿ\n",
    "        world_size: ç¸½é€²ç¨‹æ•¸\n",
    "        backend: é€šè¨Šå¾Œç«¯ ('nccl' for GPU, 'gloo' for CPU)\n",
    "    \"\"\"\n",
    "    print(f\"[Rank {rank}] åˆå§‹åŒ–é€²ç¨‹çµ„...\")\n",
    "    \n",
    "    # è¨­ç½®ç’°å¢ƒè®Šæ•¸\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # åˆå§‹åŒ–é€²ç¨‹çµ„\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "    \n",
    "    # è¨­ç½®CUDAè¨­å‚™\n",
    "    if torch.cuda.is_available() and backend == 'nccl':\n",
    "        torch.cuda.set_device(rank)\n",
    "        print(f\"[Rank {rank}] ä½¿ç”¨ GPU {rank}\")\n",
    "    else:\n",
    "        print(f\"[Rank {rank}] ä½¿ç”¨ CPU\")\n",
    "    \n",
    "    print(f\"[Rank {rank}] é€²ç¨‹çµ„åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "def cleanup_ddp():\n",
    "    \"\"\"\n",
    "    æ¸…ç†åˆ†æ•£å¼è¨“ç·´ç’°å¢ƒ\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "    print(\"é€²ç¨‹çµ„å·²æ¸…ç†\")\n",
    "\n",
    "# å–®GPUç’°å¢ƒçš„æ¨¡æ“¬è¨­ç½®\n",
    "def setup_single_gpu_demo():\n",
    "    \"\"\"\n",
    "    å–®GPUç’°å¢ƒçš„DDPæ¦‚å¿µæ¼”ç¤º\n",
    "    \"\"\"\n",
    "    print(\"=== å–®GPUç’°å¢ƒ DDP æ¦‚å¿µæ¼”ç¤º ===\")\n",
    "    print(\"æ³¨æ„: é€™æ˜¯æ¦‚å¿µæ¼”ç¤ºï¼Œä¸æœƒå•Ÿå‹•çœŸæ­£çš„å¤šé€²ç¨‹\")\n",
    "    print()\n",
    "    \n",
    "    # æ¨¡æ“¬åƒæ•¸\n",
    "    world_size = 1  # åœ¨å¤šGPUç’°å¢ƒä¸­ï¼Œé€™æœƒæ˜¯GPUæ•¸é‡\n",
    "    rank = 0        # åœ¨å¤šGPUç’°å¢ƒä¸­ï¼Œæ¯å€‹é€²ç¨‹æœ‰ä¸åŒçš„rank\n",
    "    \n",
    "    print(f\"æ¨¡æ“¬é…ç½®:\")\n",
    "    print(f\"  World Size: {world_size}\")\n",
    "    print(f\"  Rank: {rank}\")\n",
    "    print(f\"  Backend: {'nccl' if torch.cuda.is_available() else 'gloo'}\")\n",
    "    print(f\"  Device: {'cuda:0' if torch.cuda.is_available() else 'cpu'}\")\n",
    "    \n",
    "    return world_size, rank\n",
    "\n",
    "world_size, rank = setup_single_gpu_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç¤ºä¾‹æ¨¡å‹å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    ç°¡åŒ–çš„Transformeræ¨¡å‹ï¼Œç”¨æ–¼DDPæ¼”ç¤º\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=10000, d_model=512, nhead=8, num_layers=6, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.embedding(input_ids) + self.pos_encoding(positions)\n",
    "        \n",
    "        # è™•ç†attention mask\n",
    "        if attention_mask is not None:\n",
    "            # è½‰æ›ç‚ºtransformeræœŸæœ›çš„æ ¼å¼\n",
    "            attention_mask = attention_mask.bool()\n",
    "            attention_mask = ~attention_mask  # åè½‰ï¼šTrueè¡¨ç¤ºmasked\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹å¯¦ä¾‹\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleTransformer(\n",
    "    vocab_size=8000,\n",
    "    d_model=256,  # è¼ƒå°çš„æ¨¡å‹ä»¥é©æ‡‰å–®GPU\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    max_seq_len=128\n",
    ").to(device)\n",
    "\n",
    "print(f\"æ¨¡å‹å·²å‰µå»ºï¼Œè¨­å‚™: {device}\")\n",
    "print(f\"æ¨¡å‹åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"å¯è¨“ç·´åƒæ•¸é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ•¸æ“šé›†æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "class DummyTextDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    æ¨¡æ“¬æ–‡æœ¬æ•¸æ“šé›†ï¼Œç”¨æ–¼DDPè¨“ç·´æ¼”ç¤º\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples=1000, seq_len=128, vocab_size=8000):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # ç”Ÿæˆéš¨æ©Ÿæ•¸æ“šï¼ˆå¯¦éš›ä½¿ç”¨ä¸­æœƒè¼‰å…¥çœŸå¯¦æ•¸æ“šï¼‰\n",
    "        np.random.seed(42)  # ç¢ºä¿å¯é‡ç¾\n",
    "        self.data = np.random.randint(1, vocab_size, (num_samples, seq_len))\n",
    "        \n",
    "        # ç”Ÿæˆattention mask (éš¨æ©Ÿmaskä¸€äº›token)\n",
    "        self.attention_masks = np.ones((num_samples, seq_len))\n",
    "        for i in range(num_samples):\n",
    "            # éš¨æ©Ÿé¸æ“‡åºåˆ—çš„å¯¦éš›é•·åº¦\n",
    "            actual_len = np.random.randint(seq_len // 2, seq_len + 1)\n",
    "            self.attention_masks[i, actual_len:] = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.attention_masks[idx], dtype=torch.long)\n",
    "        \n",
    "        # å°æ–¼èªè¨€æ¨¡å‹ï¼Œlabelsé€šå¸¸æ˜¯input_idså‘å³ç§»å‹•ä¸€ä½\n",
    "        labels = torch.cat([input_ids[1:], torch.tensor([0])], dim=0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def create_dataloader(dataset, batch_size=8, shuffle=True, num_workers=2, is_distributed=False, rank=0, world_size=1):\n",
    "    \"\"\"\n",
    "    å‰µå»ºDataLoaderï¼Œæ”¯æ´åˆ†æ•£å¼è¨“ç·´\n",
    "    \"\"\"\n",
    "    if is_distributed:\n",
    "        # åˆ†æ•£å¼è¨“ç·´ä½¿ç”¨DistributedSampler\n",
    "        sampler = DistributedSampler(\n",
    "            dataset,\n",
    "            num_replicas=world_size,\n",
    "            rank=rank,\n",
    "            shuffle=shuffle\n",
    "        )\n",
    "        dataloader = data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        print(f\"[Rank {rank}] ä½¿ç”¨DistributedSampler\")\n",
    "    else:\n",
    "        # æ¨™æº–è¨“ç·´\n",
    "        dataloader = data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        print(\"ä½¿ç”¨æ¨™æº–DataLoader\")\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# å‰µå»ºæ•¸æ“šé›†\n",
    "train_dataset = DummyTextDataset(num_samples=2000, seq_len=128)\n",
    "val_dataset = DummyTextDataset(num_samples=400, seq_len=128)\n",
    "\n",
    "print(f\"è¨“ç·´æ•¸æ“šé›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"é©—è­‰æ•¸æ“šé›†å¤§å°: {len(val_dataset)}\")\n",
    "\n",
    "# æª¢æŸ¥æ•¸æ“šæ ¼å¼\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\næ•¸æ“šæ ¼å¼æª¢æŸ¥:\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    print(f\"    ç¯„ä¾‹: {value[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DDP æ¨¡å‹åŒ…è£æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_model_for_ddp(model, device_id=None, find_unused_parameters=False):\n",
    "    \"\"\"\n",
    "    å°‡æ¨¡å‹åŒ…è£ç‚ºDDPæ¨¡å‹\n",
    "    \n",
    "    Args:\n",
    "        model: è¦åŒ…è£çš„æ¨¡å‹\n",
    "        device_id: GPUè¨­å‚™ID\n",
    "        find_unused_parameters: æ˜¯å¦æŸ¥æ‰¾æœªä½¿ç”¨çš„åƒæ•¸\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() > 1 and dist.is_initialized():\n",
    "        # çœŸæ­£çš„å¤šGPU DDPåŒ…è£\n",
    "        ddp_model = DDP(\n",
    "            model,\n",
    "            device_ids=[device_id] if device_id is not None else None,\n",
    "            find_unused_parameters=find_unused_parameters\n",
    "        )\n",
    "        print(f\"æ¨¡å‹å·²åŒ…è£ç‚ºDDPï¼Œä½¿ç”¨GPU {device_id}\")\n",
    "    else:\n",
    "        # å–®GPUç’°å¢ƒï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹\n",
    "        ddp_model = model\n",
    "        print(\"å–®GPUç’°å¢ƒï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹ï¼ˆéDDPï¼‰\")\n",
    "    \n",
    "    return ddp_model\n",
    "\n",
    "# æ¼”ç¤ºDDPåŒ…è£\n",
    "print(\"=== DDP æ¨¡å‹åŒ…è£æ¼”ç¤º ===\")\n",
    "print()\n",
    "\n",
    "# åœ¨å–®GPUç’°å¢ƒä¸­çš„æ¼”ç¤º\n",
    "ddp_model = wrap_model_for_ddp(model, device_id=0 if torch.cuda.is_available() else None)\n",
    "\n",
    "print(f\"\\nåŸå§‹æ¨¡å‹é¡å‹: {type(model).__name__}\")\n",
    "print(f\"DDPæ¨¡å‹é¡å‹: {type(ddp_model).__name__}\")\n",
    "\n",
    "# é¡¯ç¤ºDDPçš„é—œéµç‰¹æ€§\n",
    "print(\"\\n=== DDP é—œéµç‰¹æ€§ ===\")\n",
    "print(\"1. åƒæ•¸åŒæ­¥: DDPæœƒè‡ªå‹•åŒæ­¥æ‰€æœ‰GPUä¸Šçš„æ¨¡å‹åƒæ•¸\")\n",
    "print(\"2. æ¢¯åº¦èšåˆ: ä½¿ç”¨All-Reduceç®—æ³•èšåˆæ¢¯åº¦\")\n",
    "print(\"3. å»£æ’­: åœ¨è¨“ç·´é–‹å§‹æ™‚å»£æ’­æ¨¡å‹åƒæ•¸\")\n",
    "print(\"4. éŒ¯èª¤æª¢æ¸¬: æª¢æ¸¬æœªä½¿ç”¨çš„åƒæ•¸å’Œæ¢¯åº¦ç•°å¸¸\")\n",
    "\n",
    "if hasattr(ddp_model, 'module'):\n",
    "    print(\"\\næ³¨æ„: åœ¨DDPæ¨¡å‹ä¸­ï¼ŒåŸå§‹æ¨¡å‹å¯é€šé .module å±¬æ€§è¨ªå•\")\n",
    "    print(f\"ddp_model.module é¡å‹: {type(ddp_model.module).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. è¨“ç·´é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´é…ç½®\n",
    "config = {\n",
    "    # æ¨¡å‹é…ç½®\n",
    "    'model_name': 'SimpleTransformer',\n",
    "    'vocab_size': 8000,\n",
    "    'd_model': 256,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 4,\n",
    "    'max_seq_len': 128,\n",
    "    \n",
    "    # è¨“ç·´é…ç½®\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 5e-4,\n",
    "    'num_epochs': 3,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "    \n",
    "    # DDPé…ç½®\n",
    "    'backend': 'nccl' if torch.cuda.is_available() else 'gloo',\n",
    "    'find_unused_parameters': False,\n",
    "    'gradient_clipping': 1.0,\n",
    "    \n",
    "    # æ•¸æ“šé…ç½®\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # æ—¥èªŒé…ç½®\n",
    "    'log_interval': 50,\n",
    "    'save_interval': 500,\n",
    "    'eval_interval': 200,\n",
    "}\n",
    "\n",
    "print(\"=== è¨“ç·´é…ç½® ===\")\n",
    "for category in ['æ¨¡å‹é…ç½®', 'è¨“ç·´é…ç½®', 'DDPé…ç½®', 'æ•¸æ“šé…ç½®', 'æ—¥èªŒé…ç½®']:\n",
    "    print(f\"\\n{category}:\")\n",
    "    if category == 'æ¨¡å‹é…ç½®':\n",
    "        keys = ['model_name', 'vocab_size', 'd_model', 'nhead', 'num_layers', 'max_seq_len']\n",
    "    elif category == 'è¨“ç·´é…ç½®':\n",
    "        keys = ['batch_size', 'learning_rate', 'num_epochs', 'warmup_steps', 'weight_decay']\n",
    "    elif category == 'DDPé…ç½®':\n",
    "        keys = ['backend', 'find_unused_parameters', 'gradient_clipping']\n",
    "    elif category == 'æ•¸æ“šé…ç½®':\n",
    "        keys = ['num_workers', 'pin_memory']\n",
    "    else:\n",
    "        keys = ['log_interval', 'save_interval', 'eval_interval']\n",
    "    \n",
    "    for key in keys:\n",
    "        if key in config:\n",
    "            print(f\"  {key}: {config[key]}\")\n",
    "\n",
    "# å‰µå»ºå„ªåŒ–å™¨\n",
    "optimizer = torch.optim.AdamW(\n",
    "    ddp_model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# å‰µå»ºå­¸ç¿’ç‡èª¿åº¦å™¨\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "# Warmup + Cosineèª¿åº¦\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    total_iters=config['warmup_steps']\n",
    ")\n",
    "\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['num_epochs'] * len(train_dataset) // config['batch_size'] - config['warmup_steps']\n",
    ")\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "    milestones=[config['warmup_steps']]\n",
    ")\n",
    "\n",
    "print(f\"\\nå„ªåŒ–å™¨: {type(optimizer).__name__}\")\n",
    "print(f\"èª¿åº¦å™¨: Warmup + CosineAnnealing\")\n",
    "print(f\"åˆå§‹å­¸ç¿’ç‡: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å¤šGPUè¨“ç·´è…³æœ¬ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºå¤šGPUè¨“ç·´è…³æœ¬ç¯„ä¾‹\n",
    "multi_gpu_script = '''\n",
    "#!/usr/bin/env python3\n",
    "# train_ddp.py - å¤šGPU DDPè¨“ç·´è…³æœ¬\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import argparse\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"åˆå§‹åŒ–åˆ†æ•£å¼è¨“ç·´ç’°å¢ƒ\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # åˆå§‹åŒ–é€²ç¨‹çµ„\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"æ¸…ç†åˆ†æ•£å¼è¨“ç·´ç’°å¢ƒ\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train_ddp(rank, world_size, args):\n",
    "    \"\"\"DDPè¨“ç·´ä¸»å‡½æ•¸\"\"\"\n",
    "    print(f\"[Rank {rank}] é–‹å§‹è¨“ç·´\")\n",
    "    \n",
    "    # è¨­ç½®åˆ†æ•£å¼ç’°å¢ƒ\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # å‰µå»ºæ¨¡å‹\n",
    "    model = YourModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # å‰µå»ºæ•¸æ“šé›†å’ŒDataLoader\n",
    "    dataset = YourDataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, sampler=sampler)\n",
    "    \n",
    "    # è¨“ç·´å¾ªç’°\n",
    "    for epoch in range(args.epochs):\n",
    "        sampler.set_epoch(epoch)  # ç¢ºä¿æ¯å€‹epochçš„æ•¸æ“šåˆ†å‰²ä¸åŒ\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(rank), target.to(rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if rank == 0 and batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=32)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train_ddp, args=(world_size, args), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# é¡¯ç¤ºè…³æœ¬\n",
    "print(\"=== å¤šGPU DDPè¨“ç·´è…³æœ¬ç¯„ä¾‹ ===\")\n",
    "print(\"ä»¥ä¸‹æ˜¯å®Œæ•´çš„å¤šGPU DDPè¨“ç·´è…³æœ¬çµæ§‹:\")\n",
    "print(multi_gpu_script)\n",
    "\n",
    "print(\"\\n=== åŸ·è¡Œå‘½ä»¤ ===\")\n",
    "print(\"# ä½¿ç”¨ torchrun (æ¨è–¦)\")\n",
    "print(\"torchrun --nproc_per_node=4 train_ddp.py --epochs 10 --batch-size 32\")\n",
    "print()\n",
    "print(\"# ä½¿ç”¨ mp.spawn\")\n",
    "print(\"python train_ddp.py --epochs 10 --batch-size 32\")\n",
    "print()\n",
    "print(\"# å¤šç¯€é»è¨“ç·´\")\n",
    "print(\"torchrun --nnodes=2 --nproc_per_node=4 --node_rank=0 \\\\\")\n",
    "print(\"         --master_addr=192.168.1.1 --master_port=29500 \\\\\")\n",
    "print(\"         train_ddp.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Lab-1.2 Setup å®Œæˆ ===\")\n",
    "print()\n",
    "print(\"âœ… å·²å®Œæˆ:\")\n",
    "print(\"  1. ç’°å¢ƒæª¢æŸ¥å’Œä¾è³´é©—è­‰\")\n",
    "print(\"  2. DDPæ ¸å¿ƒæ¦‚å¿µç†è§£\")\n",
    "print(\"  3. é€²ç¨‹åˆå§‹åŒ–å‡½æ•¸å®šç¾©\")\n",
    "print(\"  4. ç¤ºä¾‹æ¨¡å‹å‰µå»º\")\n",
    "print(\"  5. æ•¸æ“šé›†å’ŒDataLoaderæº–å‚™\")\n",
    "print(\"  6. DDPæ¨¡å‹åŒ…è£æ¼”ç¤º\")\n",
    "print(\"  7. è¨“ç·´é…ç½®è¨­å®š\")\n",
    "print(\"  8. å¤šGPUè¨“ç·´è…³æœ¬ç¯„ä¾‹\")\n",
    "print()\n",
    "print(\"ğŸ“ ä¸‹ä¸€æ­¥:\")\n",
    "print(\"  - 02-Train.ipynb: å¯¦éš›è¨“ç·´éç¨‹\")\n",
    "print(\"  - 03-Optimization.ipynb: é€šè¨Šå„ªåŒ–\")\n",
    "print(\"  - 04-Advanced.ipynb: é€²éšæŠ€è¡“\")\n",
    "print()\n",
    "print(\"ğŸ’¡ é‡è¦æé†’:\")\n",
    "print(\"  - ç•¶å‰ç‚ºå–®GPUç’°å¢ƒæ¼”ç¤º\")\n",
    "print(\"  - å¤šGPUç’°å¢ƒä¸­å°‡çœ‹åˆ°çœŸæ­£çš„åŠ é€Ÿæ•ˆæœ\")\n",
    "print(\"  - é‡é»ç†è§£DDPçš„è¨­è¨ˆç†å¿µå’Œé…ç½®æ–¹æ³•\")\n",
    "\n",
    "# ä¿å­˜é…ç½®ä¾›ä¸‹ä¸€å€‹notebookä½¿ç”¨\n",
    "torch.save({\n",
    "    'config': config,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': 8000,\n",
    "    'device': str(device)\n",
    "}, 'ddp_setup.pth')\n",
    "\n",
    "print(\"\\nğŸ’¾ é…ç½®å·²ä¿å­˜åˆ° ddp_setup.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}