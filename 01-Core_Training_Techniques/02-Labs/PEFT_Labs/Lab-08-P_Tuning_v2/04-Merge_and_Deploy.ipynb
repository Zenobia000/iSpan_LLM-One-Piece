{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: P-Tuning v2 - Deployment Guide\n",
    "\n",
    "## ğŸ¯ å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬ Notebook æ¢è¨ **P-Tuning v2 çš„éƒ¨ç½²ç­–ç•¥èˆ‡å„ªåŒ–æ–¹æ¡ˆ**ã€‚ä½œç‚ºæ·±åº¦æç¤ºèª¿å„ªæ–¹æ³•,P-Tuning v2 åœ¨æ¯å€‹ Transformer å±¤éƒ½æ·»åŠ å¯è¨“ç·´æç¤º,åœ¨æŠ€è¡“ä¸Š**ç„¡æ³•åˆä½µ**åˆ°åŸºç¤æ¨¡å‹ä¸­,ä½†å…¶é€šç”¨æ€§å’Œæ¥µè‡´çš„åƒæ•¸æ•ˆç‡ä½¿å…¶æˆç‚ºPEFTé ˜åŸŸçš„é‡è¦æ–¹æ³•ã€‚\n",
    "\n",
    "### é—œéµå­¸ç¿’è¦é»\n",
    "- ç†è§£ P-Tuning v2 æ·±åº¦æç¤ºçš„éƒ¨ç½²ç‰¹æ€§\n",
    "- æŒæ¡å¤šå±¤ Prompt çš„å„ªåŒ–èˆ‡ç®¡ç†\n",
    "- åˆ†æ P-Tuning v2 çš„æ¨ç†é–‹éŠ·ç‰¹æ€§\n",
    "- å­¸ç¿’è·¨ä»»å‹™ã€è·¨è¦æ¨¡çš„éƒ¨ç½²ç­–ç•¥\n",
    "- å°æ¯” P-Tuning v2 èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„éƒ¨ç½²å·®ç•°\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. P-Tuning v2 éƒ¨ç½²ç‰¹æ€§åˆ†æ\n",
    "\n",
    "### 1.1 ç‚ºä»€éº¼ P-Tuning v2 ç„¡æ³•åˆä½µ?\n",
    "\n",
    "**æŠ€è¡“åŸå› **:\n",
    "- **å¤šå±¤æ·±åº¦ä¿®æ”¹**: åœ¨**æ¯å€‹** Transformer å±¤éƒ½æ·»åŠ è™›æ“¬æ¨™è¨˜,æ”¹è®Šäº†æ•´å€‹æ¨¡å‹çš„å‰å‘å‚³æ’­çµæ§‹\n",
    "- **å‹•æ…‹åºåˆ—æ‹¼æ¥**: æ¯å±¤éƒ½éœ€è¦å‹•æ…‹æ‹¼æ¥æç¤ºå‘é‡,ç„¡æ³•éœæ…‹èåˆåˆ°æ¬Šé‡\n",
    "- **å±¤ç´šä¾è³´æ€§**: ä¸åŒå±¤çš„æç¤ºåƒæ•¸ç›¸äº’ç¨ç«‹,ç„¡æ³•ç°¡å–®åˆä½µ\n",
    "\n",
    "**èˆ‡å…¶ä»– PEFT æ–¹æ³•çš„æ¶æ§‹å°æ¯”**:\n",
    "\n",
    "| æ–¹æ³• | ä¿®æ”¹å±¤ç´š | æ˜¯å¦å¯åˆä½µ | æ¨ç†é–‹éŠ·ä¾†æº |\n",
    "|:---|:---|:---|:---|\n",
    "| **LoRA** | æ¬Šé‡çŸ©é™£ | âœ… å¯åˆä½µ | ç„¡ (åˆä½µå¾Œ) |\n",
    "| **IAÂ³** | æ¿€æ´»å€¼ | âœ… å¯åˆä½µ | ç„¡ (åˆä½µå¾Œ) |\n",
    "| **P-Tuning v1** | åƒ…è¼¸å…¥å±¤ | âŒ ä¸å¯åˆä½µ | ä½ (MLP + åºåˆ—æ‹¼æ¥) |\n",
    "| **P-Tuning v2** | **æ¯å€‹å±¤** | âŒ **ä¸å¯åˆä½µ** | **ä¸­ç­‰** (æ¯å±¤æ‹¼æ¥) |\n",
    "| **Prefix Tuning** | æ¯å±¤ KV | âŒ ä¸å¯åˆä½µ | ä¸­ç­‰ (KVæ³¨å…¥) |\n",
    "| **Adapter** | æ–°å¢æ¨¡çµ„ | âŒ ä¸å¯åˆä½µ | é«˜ (é¡å¤–å±¤) |\n",
    "\n",
    "### 1.2 P-Tuning v2 çš„æ¨ç†é–‹éŠ·åˆ†æ\n",
    "\n",
    "```python\n",
    "# P-Tuning v2 çš„å‰å‘å‚³æ’­\n",
    "for layer in transformer_layers:\n",
    "    # 1. æ‹¼æ¥è©²å±¤çš„æç¤ºå‘é‡\n",
    "    layer_prompts = layer_prompt_embeddings  # å½¢ç‹€: [batch, num_prompts, hidden]\n",
    "    layer_input = concat([layer_prompts, previous_output])  # â† æ¯å±¤éƒ½éœ€æ‹¼æ¥\n",
    "    \n",
    "    # 2. æ­£å¸¸çš„Transformerè¨ˆç®—\n",
    "    layer_output = TransformerLayer(layer_input)\n",
    "    \n",
    "    # 3. ç§»é™¤æç¤ºéƒ¨åˆ†,åªä¿ç•™çœŸå¯¦åºåˆ—\n",
    "    previous_output = layer_output[:, num_prompts:, :]\n",
    "```\n",
    "\n",
    "**é–‹éŠ·ä¾†æº**:\n",
    "- æ¯å±¤éƒ½éœ€è¦æ‹¼æ¥/æˆªå–æ“ä½œ\n",
    "- åºåˆ—é•·åº¦å¢åŠ å°è‡´æ³¨æ„åŠ›è¨ˆç®—é‡å¢åŠ \n",
    "- éœ€è¦é¡å¤–çš„è¨˜æ†¶é«”å­˜å„²æ¯å±¤çš„æç¤ºåƒæ•¸\n",
    "\n",
    "**å°æ¯”å…¶ä»–æ–¹æ³•**:\n",
    "- **vs P-Tuning v1**: é–‹éŠ·æ›´å¤§ (æ¯å±¤éƒ½æœ‰æ‹¼æ¥)\n",
    "- **vs Prefix Tuning**: é¡ä¼¼,ä½† P-Tuning v2 æ›´ç°¡å–® (ç„¡MLP)\n",
    "- **vs Adapter**: P-Tuning v2 é–‹éŠ·è¼ƒå° (ç„¡é¡å¤–FFNå±¤)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç’°å¢ƒæº–å‚™èˆ‡æ¨¡å‹è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„åº«\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PromptTuningConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# è¨­å®šè¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 è¼‰å…¥è¨“ç·´å¥½çš„ P-Tuning v2 æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è·¯å¾‘è¨­å®š\n",
    "base_model_name = \"bert-base-uncased\"\n",
    "adapter_path = \"./bert-ptuning-v2-cola\"  # å‡è¨­é€™æ˜¯è¨“ç·´å¥½çš„ P-Tuning v2 è·¯å¾‘\n",
    "num_labels = 2  # CoLA æ˜¯äºŒåˆ†é¡ä»»å‹™\n",
    "\n",
    "# è¼‰å…¥åŸºç¤æ¨¡å‹å’Œ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"åŸºç¤æ¨¡å‹åƒæ•¸é‡: {base_model.num_parameters():,}\")\n",
    "print(f\"åŸºç¤æ¨¡å‹è¨˜æ†¶é«”ä½”ç”¨: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ P-Tuning v2 adapter (å¦‚æœå­˜åœ¨)\n",
    "if os.path.exists(adapter_path):\n",
    "    # æ‰¾åˆ°æœ€æ–°çš„ checkpoint\n",
    "    checkpoints = [d for d in os.listdir(adapter_path) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(\n",
    "            [os.path.join(adapter_path, d) for d in checkpoints],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
    "        print(f\"æˆåŠŸè¼‰å…¥ P-Tuning v2 adapter: {latest_checkpoint}\")\n",
    "    else:\n",
    "        peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        print(f\"æˆåŠŸè¼‰å…¥ P-Tuning v2 adapter: {adapter_path}\")\n",
    "else:\n",
    "    # å¦‚æœæ²’æœ‰è¨“ç·´å¥½çš„ adapter,å‰µå»ºç¤ºç¯„é…ç½®\n",
    "    print(\"æœªæ‰¾åˆ°è¨“ç·´å¥½çš„ adapter,å‰µå»ºç¤ºç¯„ P-Tuning v2 é…ç½®...\")\n",
    "    \n",
    "    # P-Tuning v2 ä½¿ç”¨ PromptTuningConfig,ä½†æœƒæ‡‰ç”¨åˆ°æ¯ä¸€å±¤\n",
    "    ptuning_v2_config = PromptTuningConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        num_virtual_tokens=100,  # P-Tuning v2 é€šå¸¸ç”¨æ›´å¤šçš„è™›æ“¬æ¨™è¨˜\n",
    "        prompt_tuning_init=\"RANDOM\"\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(base_model, ptuning_v2_config)\n",
    "    print(\"å‰µå»ºäº†ç¤ºç¯„ P-Tuning v2 æ¨¡å‹\")\n",
    "\n",
    "# é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸çµ±è¨ˆ\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. P-Tuning v2 æ·±åº¦æç¤ºçµæ§‹åˆ†æ\n",
    "\n",
    "### 3.1 å¤šå±¤ Prompt åƒæ•¸æ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ptuning_v2_structure(model):\n",
    "    \"\"\"åˆ†æ P-Tuning v2 çš„å¤šå±¤çµæ§‹å’Œåƒæ•¸\"\"\"\n",
    "    ptuning_params = {}\n",
    "    total_params = 0\n",
    "    layer_count = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prompt' in name.lower() or 'virtual' in name.lower():\n",
    "            ptuning_params[name] = {\n",
    "                'shape': param.shape,\n",
    "                'dtype': param.dtype,\n",
    "                'requires_grad': param.requires_grad,\n",
    "                'num_params': param.numel()\n",
    "            }\n",
    "            total_params += param.numel()\n",
    "            layer_count += 1\n",
    "            \n",
    "            print(f\"P-Tuning v2 åƒæ•¸: {name}\")\n",
    "            print(f\"  å½¢ç‹€: {param.shape}\")\n",
    "            print(f\"  åƒæ•¸é‡: {param.numel():,}\")\n",
    "            print()\n",
    "    \n",
    "    base_params = sum(p.numel() for p in model.base_model.parameters())\n",
    "    print(f\"=== çµ±è¨ˆæ‘˜è¦ ===\")\n",
    "    print(f\"æª¢æ¸¬åˆ°çš„ Prompt å±¤æ•¸: {layer_count}\")\n",
    "    print(f\"ç¸½ P-Tuning v2 åƒæ•¸é‡: {total_params:,}\")\n",
    "    print(f\"åŸºç¤æ¨¡å‹åƒæ•¸é‡: {base_params:,}\")\n",
    "    print(f\"P-Tuning v2 åƒæ•¸ä½”æ¯”: {total_params / base_params * 100:.4f}%\")\n",
    "    \n",
    "    return ptuning_params, layer_count\n",
    "\n",
    "ptuning_v2_structure, num_layers = analyze_ptuning_v2_structure(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æ·±åº¦æç¤ºçš„è¨ˆç®—æµç¨‹å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ptuning_v2_computation(num_virtual_tokens=100, hidden_size=768, num_layers=12):\n",
    "    \"\"\"\n",
    "    å¯è¦–åŒ– P-Tuning v2 çš„æ·±åº¦æç¤ºè¨ˆç®—æµç¨‹\n",
    "    \"\"\"\n",
    "    print(\"=== P-Tuning v2 æ·±åº¦æç¤ºè¨ˆç®—æµç¨‹ ===\")\n",
    "    print()\n",
    "    print(f\"é…ç½®åƒæ•¸:\")\n",
    "    print(f\"  è™›æ“¬æ¨™è¨˜æ•¸é‡: {num_virtual_tokens} tokens/layer\")\n",
    "    print(f\"  éš±è—ç¶­åº¦: {hidden_size}\")\n",
    "    print(f\"  Transformer å±¤æ•¸: {num_layers}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"è¨ˆç®—æµç¨‹:\")\n",
    "    print()\n",
    "    \n",
    "    seq_len = 128  # å‡è¨­çš„è¼¸å…¥åºåˆ—é•·åº¦\n",
    "    \n",
    "    print(f\"å±¤ 0 (Embedding å±¤):\")\n",
    "    print(f\"  è¼¸å…¥: [{seq_len}, {hidden_size}]\")\n",
    "    print(f\"  æ‹¼æ¥è™›æ“¬æ¨™è¨˜: [{num_virtual_tokens}, {hidden_size}]\")\n",
    "    print(f\"  è¼¸å‡º: [{num_virtual_tokens + seq_len}, {hidden_size}]\")\n",
    "    print()\n",
    "    \n",
    "    for layer_idx in range(1, num_layers + 1):\n",
    "        if layer_idx <= 2 or layer_idx == num_layers:\n",
    "            print(f\"å±¤ {layer_idx} (Transformer Layer {layer_idx}):\")\n",
    "            print(f\"  è¼¸å…¥: [{num_virtual_tokens + seq_len}, {hidden_size}]\")\n",
    "            print(f\"  Self-Attention: åŒ…å« {num_virtual_tokens} å€‹è™›æ“¬æ¨™è¨˜\")\n",
    "            print(f\"  FFN è¨ˆç®—\")\n",
    "            print(f\"  è¼¸å‡º: [{num_virtual_tokens + seq_len}, {hidden_size}]\")\n",
    "            print()\n",
    "        elif layer_idx == 3:\n",
    "            print(f\"  ... (ä¸­é–“å±¤çœç•¥,çµæ§‹ç›¸åŒ) ...\")\n",
    "            print()\n",
    "    \n",
    "    print(\"æœ€çµ‚è¼¸å‡º:\")\n",
    "    print(f\"  ç§»é™¤è™›æ“¬æ¨™è¨˜éƒ¨åˆ†\")\n",
    "    print(f\"  ä¿ç•™çœŸå¯¦åºåˆ—: [{seq_len}, {hidden_size}]\")\n",
    "    print()\n",
    "    \n",
    "    # åƒæ•¸é‡è¨ˆç®—\n",
    "    total_params = num_layers * num_virtual_tokens * hidden_size\n",
    "    print(\"åƒæ•¸çµ±è¨ˆ:\")\n",
    "    print(f\"  æ¯å±¤åƒæ•¸é‡: {num_virtual_tokens * hidden_size:,}\")\n",
    "    print(f\"  ç¸½åƒæ•¸é‡: {total_params:,}\")\n",
    "    \n",
    "    base_params = 110 * 1e6  # BERT-base\n",
    "    print(f\"  ä½”åŸºç¤æ¨¡å‹æ¯”ä¾‹: {total_params / base_params * 100:.4f}%\")\n",
    "    print()\n",
    "    \n",
    "    # è¨ˆç®—é–‹éŠ·åˆ†æ\n",
    "    attention_overhead = ((num_virtual_tokens + seq_len)**2 / seq_len**2 - 1) * 100\n",
    "    print(\"æ¨ç†é–‹éŠ·åˆ†æ:\")\n",
    "    print(f\"  æ³¨æ„åŠ›è¨ˆç®—é‡å¢åŠ : ~{attention_overhead:.1f}% (æ¯å±¤)\")\n",
    "    print(f\"  è¨˜æ†¶é«”å¢åŠ : ~{total_params * 2 / 1024**2:.2f} MB (FP16)\")\n",
    "    print(f\"  é¡å¤–æ“ä½œ: {num_layers} æ¬¡æ‹¼æ¥/æˆªå–\")\n",
    "\n",
    "visualize_ptuning_v2_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, test_texts, num_runs=10):\n",
    "    \"\"\"æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for text in test_texts:\n",
    "                inputs = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128\n",
    "                )\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                prediction = torch.argmax(outputs.logits, dim=-1).cpu().item()\n",
    "                \n",
    "                if run == 0:\n",
    "                    results.append({'text': text, 'prediction': prediction})\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    return total_time / num_runs, results\n",
    "\n",
    "# æ¸¬è©¦æ–‡æœ¬\n",
    "test_texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"She enjoys reading books.\",\n",
    "    \"Books enjoys reading she.\",  # èªæ³•éŒ¯èª¤\n",
    "]\n",
    "\n",
    "print(\"=== P-Tuning v2 æ¨ç†æ€§èƒ½æ¸¬è©¦ ===\")\n",
    "ptuning_v2_time, results = benchmark_inference(peft_model, tokenizer, test_texts)\n",
    "print(f\"å¹³å‡æ¨ç†æ™‚é–“: {ptuning_v2_time:.4f} ç§’\")\n",
    "print(f\"è¨˜æ†¶é«”ä½¿ç”¨: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== èˆ‡åŸºç¤æ¨¡å‹å°æ¯” ===\")\n",
    "base_time, _ = benchmark_inference(base_model, tokenizer, test_texts)\n",
    "print(f\"åŸºç¤æ¨¡å‹æ¨ç†æ™‚é–“: {base_time:.4f} ç§’\")\n",
    "print(f\"æ™‚é–“é–‹éŠ·: {(ptuning_v2_time - base_time) / base_time * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹ä¿å­˜èˆ‡éƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./bert-ptuning-v2-deployed\"\n",
    "\n",
    "print(f\"=== ä¿å­˜ P-Tuning v2 æ¨¡çµ„åˆ°: {save_path} ===\")\n",
    "peft_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"âœ… ä¿å­˜æˆåŠŸ!\")\n",
    "\n",
    "# è¨ˆç®—æ¨¡çµ„å¤§å°\n",
    "module_size = sum(os.path.getsize(os.path.join(save_path, f)) \n",
    "                  for f in os.listdir(save_path) if os.path.isfile(os.path.join(save_path, f)))\n",
    "print(f\"æ¨¡çµ„å¤§å°: {module_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. éƒ¨ç½²é…ç½®èˆ‡æœ€ä½³å¯¦è¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"peft_method\": \"P-Tuning v2\",\n",
    "        \"num_virtual_tokens_per_layer\": 100,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"mergeable\": False,\n",
    "        \"universal_across_tasks\": True\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"inference_overhead_percent\": (ptuning_v2_time - base_time) / base_time * 100,\n",
    "        \"parameter_efficiency\": \"0.01-0.1%\"\n",
    "    },\n",
    "    \"advantages\": [\n",
    "        \"è·¨ä»»å‹™é€šç”¨æ€§æ¥µå¼· (NLU + NLG)\",\n",
    "        \"è¦æ¨¡ä¸è®Šæ€§ - å¤§æ¨¡å‹ä¸Šæ•ˆæœæ›´å¥½\",\n",
    "        \"ç„¡éœ€ MLP ç·¨ç¢¼å™¨,å¯¦ç¾ç°¡å–®\",\n",
    "        \"æ¥è¿‘å…¨åƒæ•¸å¾®èª¿çš„æ€§èƒ½\"\n",
    "    ],\n",
    "    \"deployment_notes\": {\n",
    "        \"best_for\": \"éœ€è¦è·¨ä»»å‹™ã€è·¨è¦æ¨¡é€šç”¨æ€§çš„å ´æ™¯\",\n",
    "        \"inference_overhead\": \"ä¸­ç­‰ (~10-15%),ä½†æ€§èƒ½æ”¶ç›Šé¡¯è‘—\",\n",
    "        \"recommended_scenarios\": [\n",
    "            \"å¤šä»»å‹™å­¸ç¿’\",\n",
    "            \"å¤§è¦æ¨¡æ¨¡å‹å¾®èª¿\",\n",
    "            \"éœ€è¦ç†è§£+ç”Ÿæˆèƒ½åŠ›\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = os.path.join(save_path, \"deployment_config.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=== P-Tuning v2 éƒ¨ç½²é…ç½® ===\")\n",
    "print(json.dumps(deployment_config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç¸½çµ\n",
    "\n",
    "### P-Tuning v2 çš„æ ¸å¿ƒåƒ¹å€¼\n",
    "\n",
    "**ç¨ç‰¹å„ªå‹¢**:\n",
    "1. **é€šç”¨æ€§ç„¡èˆ‡å€«æ¯”**: åŒæ™‚é©ç”¨æ–¼ NLU å’Œ NLG ä»»å‹™\n",
    "2. **è¦æ¨¡ä¸è®Šæ€§**: æ¨¡å‹è¶Šå¤§,æ•ˆæœè¶Šæ¥è¿‘å…¨åƒæ•¸å¾®èª¿\n",
    "3. **å¯¦ç¾ç°¡å–®**: ç„¡éœ€è¤‡é›œçš„ MLP ç·¨ç¢¼å™¨\n",
    "4. **æ€§èƒ½å“è¶Š**: åƒ… 0.1% åƒæ•¸é”åˆ° 99%+ å…¨åƒæ•¸æ€§èƒ½\n",
    "\n",
    "**éƒ¨ç½²æ¬Šè¡¡**:\n",
    "- âœ… æ¥µè‡´çš„åƒæ•¸æ•ˆç‡å’Œé€šç”¨æ€§\n",
    "- âš ï¸ å­˜åœ¨æ¨ç†é–‹éŠ· (ä½†ç›¸æ¯”æ€§èƒ½æ”¶ç›Šæ˜¯å€¼å¾—çš„)\n",
    "- âš ï¸ ç„¡æ³•åˆä½µ,éœ€ç®¡ç†å¤šå±¤æç¤ºåƒæ•¸\n",
    "\n",
    "**æœ€ä½³æ‡‰ç”¨å ´æ™¯**:\n",
    "- å¤§è¦æ¨¡æ¨¡å‹çš„é«˜æ•ˆå¾®èª¿\n",
    "- éœ€è¦è·¨ä»»å‹™é€šç”¨æ€§\n",
    "- è¿½æ±‚æœ€å„ªåƒæ•¸æ•ˆç‡-æ€§èƒ½å¹³è¡¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†è³‡æº\n",
    "print(\"=== æ¸…ç†è³‡æº ===\")\n",
    "del peft_model, base_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"âœ… å®Œæˆ\")\n",
    "\n",
    "print(\"\\nğŸ‰ Lab 8 - P-Tuning v2 éƒ¨ç½²æŒ‡å—å®Œæˆ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
