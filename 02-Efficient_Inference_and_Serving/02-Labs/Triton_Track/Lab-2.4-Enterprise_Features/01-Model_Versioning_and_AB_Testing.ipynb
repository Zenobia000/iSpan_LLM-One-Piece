{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Lab 2.4.1 - æ¨¡å‹ç‰ˆæœ¬ç®¡ç†èˆ‡ A/B æ¸¬è©¦\n",
    "\n",
    "## ğŸ¯ å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬å¯¦é©—å°‡æ•™æ‚¨å¦‚ä½•ï¼š\n",
    "1. å¯¦ç¾ä¼æ¥­ç´šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†ç­–ç•¥\n",
    "2. è¨­è¨ˆå’ŒåŸ·è¡Œ A/B æ¸¬è©¦æ¡†æ¶\n",
    "3. å¯¦ç¾æ¼¸é€²å¼æ¨¡å‹éƒ¨ç½² (Canary Deployment)\n",
    "4. æ§‹å»ºæ¨¡å‹æ€§èƒ½ç›£æ§å’Œå›æ»¾æ©Ÿåˆ¶\n",
    "5. è¨­ç½®æµé‡åˆ†é…å’Œè·¯ç”±ç­–ç•¥\n",
    "\n",
    "## ğŸ“‹ å‰ç½®éœ€æ±‚\n",
    "\n",
    "- å®Œæˆ Lab 2.1ï¼ˆTriton åŸºç¤è¨­ç½®ï¼‰\n",
    "- ç†Ÿæ‚‰å®¹å™¨æŠ€è¡“å’Œ Kubernetes\n",
    "- äº†è§£ CI/CD æµç¨‹å’Œç‰ˆæœ¬æ§åˆ¶\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## ğŸ“š ç†è«–èƒŒæ™¯\n",
    "\n",
    "### ä¼æ¥­ç´šæ¨¡å‹ç®¡ç†æŒ‘æˆ°\n",
    "\n",
    "**1. ç‰ˆæœ¬ç®¡ç†è¤‡é›œæ€§**\n",
    "- å¤šå€‹æ¨¡å‹ç‰ˆæœ¬ä¸¦å­˜\n",
    "- ä¸åŒç’°å¢ƒé–“çš„ç‰ˆæœ¬åŒæ­¥\n",
    "- å›æ»¾ç­–ç•¥å’Œæ•¸æ“šä¸€è‡´æ€§\n",
    "\n",
    "**2. A/B æ¸¬è©¦éœ€æ±‚**\n",
    "- æ¥­å‹™æŒ‡æ¨™è©•ä¼°\n",
    "- ç”¨æˆ¶é«”é©—æ¯”è¼ƒ\n",
    "- é¢¨éšªæ§åˆ¶å’Œæ¼¸é€²éƒ¨ç½²\n",
    "\n",
    "**3. ç”Ÿç”¢ç’°å¢ƒç©©å®šæ€§**\n",
    "- é›¶åœæ©Ÿéƒ¨ç½²\n",
    "- æ€§èƒ½ç›£æ§å’Œå‘Šè­¦\n",
    "- è‡ªå‹•æ•…éšœæ¢å¾©\n",
    "\n",
    "### Triton ç‰ˆæœ¬ç®¡ç†æ¶æ§‹\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Model Repository] --> B[Version 1]\n",
    "    A --> C[Version 2]\n",
    "    A --> D[Version 3]\n",
    "    \n",
    "    B --> E[Production 80%]\n",
    "    C --> F[Canary 15%]\n",
    "    D --> G[Shadow 5%]\n",
    "    \n",
    "    E --> H[Load Balancer]\n",
    "    F --> H\n",
    "    G --> I[Metrics Only]\n",
    "    \n",
    "    H --> J[User Traffic]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Triton å®¢æˆ¶ç«¯\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# è¨­ç½®æ¨£å¼\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Environment ready at {datetime.now()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­ç½®å¯¦é©—ç’°å¢ƒ\n",
    "BASE_DIR = \"/opt/tritonserver\"\n",
    "MODEL_REPO = f\"{BASE_DIR}/models\"\n",
    "EXPERIMENT_DIR = f\"{BASE_DIR}/experiments/ab_testing\"\n",
    "\n",
    "# å‰µå»ºå¯¦é©—ç›®éŒ„\n",
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/metrics\", exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/configs\", exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/logs\", exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ å¯¦é©—ç›®éŒ„: {EXPERIMENT_DIR}\")\n",
    "print(f\"ğŸ“ æ¨¡å‹å€‰åº«: {MODEL_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 1ï¼šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†ç³»çµ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 1.1 ç‰ˆæœ¬ç®¡ç†é¡è¨­è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelVersion:\n",
    "    \"\"\"æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯\"\"\"\n",
    "    name: str\n",
    "    version: int\n",
    "    created_at: datetime\n",
    "    status: str  # \"active\", \"inactive\", \"testing\", \"deprecated\"\n",
    "    traffic_percentage: float\n",
    "    performance_metrics: Dict[str, float]\n",
    "    metadata: Dict[str, str]\n",
    "\n",
    "\n",
    "class ModelVersionManager:\n",
    "    \"\"\"æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, triton_url: str = \"localhost:8000\"):\n",
    "        self.model_name = model_name\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.versions: Dict[int, ModelVersion] = {}\n",
    "        self.traffic_rules = {}\n",
    "        \n",
    "        # åŠ è¼‰ç¾æœ‰ç‰ˆæœ¬\n",
    "        self._discover_versions()\n",
    "    \n",
    "    def _discover_versions(self):\n",
    "        \"\"\"ç™¼ç¾ç¾æœ‰æ¨¡å‹ç‰ˆæœ¬\"\"\"\n",
    "        try:\n",
    "            model_config = self.client.get_model_config(self.model_name)\n",
    "            print(f\"âœ… ç™¼ç¾æ¨¡å‹: {self.model_name}\")\n",
    "            \n",
    "            # æ¨¡æ“¬ç‰ˆæœ¬ç™¼ç¾ï¼ˆåœ¨å¯¦éš›ç’°å¢ƒä¸­æœƒå¾æ¨¡å‹å€‰åº«è®€å–ï¼‰\n",
    "            for version in [1, 2, 3]:\n",
    "                self.versions[version] = ModelVersion(\n",
    "                    name=self.model_name,\n",
    "                    version=version,\n",
    "                    created_at=datetime.now() - timedelta(days=version*10),\n",
    "                    status=\"active\" if version == 2 else \"inactive\",\n",
    "                    traffic_percentage=100.0 if version == 2 else 0.0,\n",
    "                    performance_metrics={\n",
    "                        \"latency_p99\": random.uniform(50, 200),\n",
    "                        \"throughput\": random.uniform(100, 1000),\n",
    "                        \"error_rate\": random.uniform(0, 0.05)\n",
    "                    },\n",
    "                    metadata={\n",
    "                        \"framework\": \"pytorch\",\n",
    "                        \"precision\": \"fp16\" if version > 1 else \"fp32\"\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹ç™¼ç¾å¤±æ•—: {str(e)}\")\n",
    "    \n",
    "    def register_version(self, version: int, metadata: Dict[str, str] = None) -> bool:\n",
    "        \"\"\"è¨»å†Šæ–°æ¨¡å‹ç‰ˆæœ¬\"\"\"\n",
    "        try:\n",
    "            new_version = ModelVersion(\n",
    "                name=self.model_name,\n",
    "                version=version,\n",
    "                created_at=datetime.now(),\n",
    "                status=\"inactive\",\n",
    "                traffic_percentage=0.0,\n",
    "                performance_metrics={},\n",
    "                metadata=metadata or {}\n",
    "            )\n",
    "            \n",
    "            self.versions[version] = new_version\n",
    "            \n",
    "            print(f\"âœ… ç‰ˆæœ¬ {version} è¨»å†ŠæˆåŠŸ\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç‰ˆæœ¬è¨»å†Šå¤±æ•—: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def get_version_info(self, version: int) -> Optional[ModelVersion]:\n",
    "        \"\"\"ç²å–ç‰ˆæœ¬ä¿¡æ¯\"\"\"\n",
    "        return self.versions.get(version)\n",
    "    \n",
    "    def list_versions(self) -> List[ModelVersion]:\n",
    "        \"\"\"åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬\"\"\"\n",
    "        return list(self.versions.values())\n",
    "    \n",
    "    def set_traffic_split(self, traffic_config: Dict[int, float]):\n",
    "        \"\"\"è¨­ç½®æµé‡åˆ†é…\"\"\"\n",
    "        total_percentage = sum(traffic_config.values())\n",
    "        \n",
    "        if abs(total_percentage - 100.0) > 0.001:\n",
    "            raise ValueError(f\"æµé‡åˆ†é…ç¸½å’Œå¿…é ˆç‚º 100%ï¼Œç•¶å‰ç‚º {total_percentage}%\")\n",
    "        \n",
    "        # æ›´æ–°ç‰ˆæœ¬æµé‡\n",
    "        for version_num, version in self.versions.items():\n",
    "            version.traffic_percentage = traffic_config.get(version_num, 0.0)\n",
    "            version.status = \"active\" if version.traffic_percentage > 0 else \"inactive\"\n",
    "        \n",
    "        self.traffic_rules = traffic_config\n",
    "        \n",
    "        print(f\"âœ… æµé‡åˆ†é…å·²æ›´æ–°: {traffic_config}\")\n",
    "    \n",
    "    def get_version_for_request(self, request_id: str = None) -> int:\n",
    "        \"\"\"æ ¹æ“šæµé‡è¦å‰‡é¸æ“‡ç‰ˆæœ¬\"\"\"\n",
    "        if not self.traffic_rules:\n",
    "            # é»˜èªä½¿ç”¨æœ€æ–°çš„æ´»èºç‰ˆæœ¬\n",
    "            active_versions = [v for v in self.versions.values() if v.status == \"active\"]\n",
    "            if active_versions:\n",
    "                return max(active_versions, key=lambda x: x.version).version\n",
    "            return max(self.versions.keys())\n",
    "        \n",
    "        # åŸºæ–¼æ¬Šé‡éš¨æ©Ÿé¸æ“‡\n",
    "        rand_val = random.uniform(0, 100)\n",
    "        cumulative = 0\n",
    "        \n",
    "        for version, percentage in sorted(self.traffic_rules.items()):\n",
    "            cumulative += percentage\n",
    "            if rand_val <= cumulative:\n",
    "                return version\n",
    "        \n",
    "        # å›é€€åˆ°é»˜èªç‰ˆæœ¬\n",
    "        return max(self.traffic_rules.keys())\n",
    "    \n",
    "    def export_config(self, filepath: str):\n",
    "        \"\"\"å°å‡ºé…ç½®åˆ°æ–‡ä»¶\"\"\"\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"versions\": {},\n",
    "            \"traffic_rules\": self.traffic_rules,\n",
    "            \"exported_at\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        for version_num, version in self.versions.items():\n",
    "            config[\"versions\"][str(version_num)] = {\n",
    "                \"status\": version.status,\n",
    "                \"traffic_percentage\": version.traffic_percentage,\n",
    "                \"performance_metrics\": version.performance_metrics,\n",
    "                \"metadata\": version.metadata,\n",
    "                \"created_at\": version.created_at.isoformat()\n",
    "            }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… é…ç½®å·²å°å‡ºåˆ°: {filepath}\")\n",
    "\n",
    "\n",
    "# å‰µå»ºç‰ˆæœ¬ç®¡ç†å™¨å¯¦ä¾‹\n",
    "print(\"ğŸ”§ å‰µå»ºæ¨¡å‹ç‰ˆæœ¬ç®¡ç†å™¨...\")\n",
    "version_manager = ModelVersionManager(\"text_classifier\")\n",
    "print(f\"ğŸ“Š ç™¼ç¾ç‰ˆæœ¬æ•¸é‡: {len(version_manager.versions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 1.2 ç‰ˆæœ¬ä¿¡æ¯å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤ºæ‰€æœ‰ç‰ˆæœ¬ä¿¡æ¯\n",
    "def display_version_summary(manager: ModelVersionManager):\n",
    "    \"\"\"é¡¯ç¤ºç‰ˆæœ¬æ‘˜è¦\"\"\"\n",
    "    print(f\"\\nğŸ“‹ æ¨¡å‹ '{manager.model_name}' ç‰ˆæœ¬æ‘˜è¦\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for version in sorted(manager.list_versions(), key=lambda x: x.version):\n",
    "        print(f\"\\nğŸ·ï¸  ç‰ˆæœ¬ {version.version} ({version.status.upper()})\")\n",
    "        print(f\"   ğŸ“… å‰µå»ºæ™‚é–“: {version.created_at.strftime('%Y-%m-%d %H:%M')}\")\n",
    "        print(f\"   ğŸš¦ æµé‡æ¯”ä¾‹: {version.traffic_percentage:.1f}%\")\n",
    "        print(f\"   ğŸ“Š æ€§èƒ½æŒ‡æ¨™:\")\n",
    "        for metric, value in version.performance_metrics.items():\n",
    "            if metric == \"error_rate\":\n",
    "                print(f\"      â€¢ {metric}: {value:.3f}\")\n",
    "            else:\n",
    "                print(f\"      â€¢ {metric}: {value:.1f}\")\n",
    "        print(f\"   ğŸ·ï¸  å…ƒæ•¸æ“š: {version.metadata}\")\n",
    "\n",
    "\n",
    "display_version_summary(version_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–ç‰ˆæœ¬æ€§èƒ½å°æ¯”\n",
    "def plot_version_performance(manager: ModelVersionManager):\n",
    "    \"\"\"å¯è¦–åŒ–ç‰ˆæœ¬æ€§èƒ½\"\"\"\n",
    "    versions = manager.list_versions()\n",
    "    \n",
    "    if not versions:\n",
    "        print(\"âŒ æ²’æœ‰ç‰ˆæœ¬æ•¸æ“šå¯ä¾›åˆ†æ\")\n",
    "        return\n",
    "    \n",
    "    # æº–å‚™æ•¸æ“š\n",
    "    version_nums = [v.version for v in versions]\n",
    "    latencies = [v.performance_metrics.get(\"latency_p99\", 0) for v in versions]\n",
    "    throughputs = [v.performance_metrics.get(\"throughput\", 0) for v in versions]\n",
    "    error_rates = [v.performance_metrics.get(\"error_rate\", 0) * 100 for v in versions]\n",
    "    traffic = [v.traffic_percentage for v in versions]\n",
    "    \n",
    "    # å‰µå»ºå­åœ–\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # å»¶é²å°æ¯”\n",
    "    bars1 = ax1.bar(version_nums, latencies, alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('P99 å»¶é²å°æ¯” (ms)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('ç‰ˆæœ¬')\n",
    "    ax1.set_ylabel('å»¶é² (ms)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for bar, val in zip(bars1, latencies):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # ååé‡å°æ¯”\n",
    "    bars2 = ax2.bar(version_nums, throughputs, alpha=0.7, color='lightgreen')\n",
    "    ax2.set_title('ååé‡å°æ¯” (QPS)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('ç‰ˆæœ¬')\n",
    "    ax2.set_ylabel('ååé‡ (QPS)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars2, throughputs):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "                f'{val:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # éŒ¯èª¤ç‡å°æ¯”\n",
    "    bars3 = ax3.bar(version_nums, error_rates, alpha=0.7, color='salmon')\n",
    "    ax3.set_title('éŒ¯èª¤ç‡å°æ¯” (%)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('ç‰ˆæœ¬')\n",
    "    ax3.set_ylabel('éŒ¯èª¤ç‡ (%)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars3, error_rates):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{val:.2f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # æµé‡åˆ†é…\n",
    "    colors = ['gold', 'lightcoral', 'lightblue']\n",
    "    wedges, texts, autotexts = ax4.pie(traffic, labels=[f'V{v}' for v in version_nums],\n",
    "                                      autopct='%1.1f%%', colors=colors[:len(version_nums)])\n",
    "    ax4.set_title('æµé‡åˆ†é…', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_version_performance(version_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 2ï¼šA/B æ¸¬è©¦æ¡†æ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 2.1 A/B æ¸¬è©¦ç®¡ç†å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ABTestConfig:\n",
    "    \"\"\"A/B æ¸¬è©¦é…ç½®\"\"\"\n",
    "    test_name: str\n",
    "    model_name: str\n",
    "    control_version: int\n",
    "    treatment_version: int\n",
    "    traffic_split: float  # treatment ç‰ˆæœ¬çš„æµé‡æ¯”ä¾‹\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    success_metrics: List[str]\n",
    "    min_sample_size: int\n",
    "    significance_level: float\n",
    "    status: str  # \"planned\", \"running\", \"completed\", \"stopped\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestMetrics:\n",
    "    \"\"\"æ¸¬è©¦æŒ‡æ¨™æ•¸æ“š\"\"\"\n",
    "    version: int\n",
    "    request_count: int\n",
    "    success_count: int\n",
    "    total_latency: float\n",
    "    error_count: int\n",
    "    timestamp: datetime\n",
    "\n",
    "\n",
    "class ABTestManager:\n",
    "    \"\"\"A/B æ¸¬è©¦ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, version_manager: ModelVersionManager):\n",
    "        self.version_manager = version_manager\n",
    "        self.active_tests: Dict[str, ABTestConfig] = {}\n",
    "        self.test_metrics: Dict[str, List[TestMetrics]] = {}\n",
    "        self.test_results: Dict[str, Dict] = {}\n",
    "    \n",
    "    def create_test(self, config: ABTestConfig) -> bool:\n",
    "        \"\"\"å‰µå»ºæ–°çš„ A/B æ¸¬è©¦\"\"\"\n",
    "        try:\n",
    "            # é©—è­‰ç‰ˆæœ¬å­˜åœ¨\n",
    "            control_version = self.version_manager.get_version_info(config.control_version)\n",
    "            treatment_version = self.version_manager.get_version_info(config.treatment_version)\n",
    "            \n",
    "            if not control_version or not treatment_version:\n",
    "                raise ValueError(\"æŒ‡å®šçš„ç‰ˆæœ¬ä¸å­˜åœ¨\")\n",
    "            \n",
    "            # æª¢æŸ¥æ™‚é–“é…ç½®\n",
    "            if config.start_time >= config.end_time:\n",
    "                raise ValueError(\"çµæŸæ™‚é–“å¿…é ˆæ™šæ–¼é–‹å§‹æ™‚é–“\")\n",
    "            \n",
    "            # æ·»åŠ åˆ°æ´»èºæ¸¬è©¦\n",
    "            self.active_tests[config.test_name] = config\n",
    "            self.test_metrics[config.test_name] = []\n",
    "            \n",
    "            print(f\"âœ… A/B æ¸¬è©¦ '{config.test_name}' å‰µå»ºæˆåŠŸ\")\n",
    "            print(f\"   ğŸ“Š æ§åˆ¶çµ„: V{config.control_version} ({100-config.traffic_split:.1f}%)\")\n",
    "            print(f\"   ğŸ§ª å¯¦é©—çµ„: V{config.treatment_version} ({config.traffic_split:.1f}%)\")\n",
    "            print(f\"   â° æ¸¬è©¦æœŸé–“: {config.start_time.strftime('%Y-%m-%d')} - {config.end_time.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ A/B æ¸¬è©¦å‰µå»ºå¤±æ•—: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def start_test(self, test_name: str) -> bool:\n",
    "        \"\"\"å•Ÿå‹• A/B æ¸¬è©¦\"\"\"\n",
    "        if test_name not in self.active_tests:\n",
    "            print(f\"âŒ æ¸¬è©¦ '{test_name}' ä¸å­˜åœ¨\")\n",
    "            return False\n",
    "        \n",
    "        test_config = self.active_tests[test_name]\n",
    "        \n",
    "        if datetime.now() < test_config.start_time:\n",
    "            print(f\"âŒ æ¸¬è©¦å°šæœªåˆ°é”é–‹å§‹æ™‚é–“\")\n",
    "            return False\n",
    "        \n",
    "        # æ›´æ–°æµé‡åˆ†é…\n",
    "        traffic_config = {\n",
    "            test_config.control_version: 100 - test_config.traffic_split,\n",
    "            test_config.treatment_version: test_config.traffic_split\n",
    "        }\n",
    "        \n",
    "        self.version_manager.set_traffic_split(traffic_config)\n",
    "        test_config.status = \"running\"\n",
    "        \n",
    "        print(f\"ğŸš€ A/B æ¸¬è©¦ '{test_name}' å·²å•Ÿå‹•\")\n",
    "        return True\n",
    "    \n",
    "    def stop_test(self, test_name: str, reason: str = \"Manual stop\") -> bool:\n",
    "        \"\"\"åœæ­¢ A/B æ¸¬è©¦\"\"\"\n",
    "        if test_name not in self.active_tests:\n",
    "            print(f\"âŒ æ¸¬è©¦ '{test_name}' ä¸å­˜åœ¨\")\n",
    "            return False\n",
    "        \n",
    "        test_config = self.active_tests[test_name]\n",
    "        test_config.status = \"stopped\"\n",
    "        \n",
    "        # æ¢å¾©åˆ°æ§åˆ¶çµ„ç‰ˆæœ¬\n",
    "        self.version_manager.set_traffic_split({test_config.control_version: 100.0})\n",
    "        \n",
    "        print(f\"â¹ï¸  A/B æ¸¬è©¦ '{test_name}' å·²åœæ­¢\")\n",
    "        print(f\"   ğŸ“ åŸå› : {reason}\")\n",
    "        return True\n",
    "    \n",
    "    def record_metrics(self, test_name: str, version: int, \n",
    "                      latency: float, success: bool):\n",
    "        \"\"\"è¨˜éŒ„æ¸¬è©¦æŒ‡æ¨™\"\"\"\n",
    "        if test_name not in self.test_metrics:\n",
    "            self.test_metrics[test_name] = []\n",
    "        \n",
    "        # å°‹æ‰¾æˆ–å‰µå»ºè©²ç‰ˆæœ¬çš„æŒ‡æ¨™è¨˜éŒ„\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # æ‰¾åˆ°ç•¶å‰åˆ†é˜çš„æŒ‡æ¨™è¨˜éŒ„\n",
    "        minute_key = current_time.replace(second=0, microsecond=0)\n",
    "        \n",
    "        # æŸ¥æ‰¾ç¾æœ‰è¨˜éŒ„\n",
    "        existing_metric = None\n",
    "        for metric in self.test_metrics[test_name]:\n",
    "            if (metric.version == version and \n",
    "                metric.timestamp.replace(second=0, microsecond=0) == minute_key):\n",
    "                existing_metric = metric\n",
    "                break\n",
    "        \n",
    "        if existing_metric:\n",
    "            # æ›´æ–°ç¾æœ‰è¨˜éŒ„\n",
    "            existing_metric.request_count += 1\n",
    "            existing_metric.total_latency += latency\n",
    "            if success:\n",
    "                existing_metric.success_count += 1\n",
    "            else:\n",
    "                existing_metric.error_count += 1\n",
    "        else:\n",
    "            # å‰µå»ºæ–°è¨˜éŒ„\n",
    "            new_metric = TestMetrics(\n",
    "                version=version,\n",
    "                request_count=1,\n",
    "                success_count=1 if success else 0,\n",
    "                total_latency=latency,\n",
    "                error_count=0 if success else 1,\n",
    "                timestamp=current_time\n",
    "            )\n",
    "            self.test_metrics[test_name].append(new_metric)\n",
    "    \n",
    "    def get_test_summary(self, test_name: str) -> Dict:\n",
    "        \"\"\"ç²å–æ¸¬è©¦æ‘˜è¦\"\"\"\n",
    "        if test_name not in self.active_tests:\n",
    "            return {\"error\": \"æ¸¬è©¦ä¸å­˜åœ¨\"}\n",
    "        \n",
    "        test_config = self.active_tests[test_name]\n",
    "        metrics = self.test_metrics.get(test_name, [])\n",
    "        \n",
    "        # æŒ‰ç‰ˆæœ¬åˆ†çµ„çµ±è¨ˆ\n",
    "        control_metrics = [m for m in metrics if m.version == test_config.control_version]\n",
    "        treatment_metrics = [m for m in metrics if m.version == test_config.treatment_version]\n",
    "        \n",
    "        def calculate_stats(metric_list):\n",
    "            if not metric_list:\n",
    "                return {\n",
    "                    \"requests\": 0,\n",
    "                    \"success_rate\": 0.0,\n",
    "                    \"avg_latency\": 0.0,\n",
    "                    \"error_rate\": 0.0\n",
    "                }\n",
    "            \n",
    "            total_requests = sum(m.request_count for m in metric_list)\n",
    "            total_success = sum(m.success_count for m in metric_list)\n",
    "            total_latency = sum(m.total_latency for m in metric_list)\n",
    "            total_errors = sum(m.error_count for m in metric_list)\n",
    "            \n",
    "            return {\n",
    "                \"requests\": total_requests,\n",
    "                \"success_rate\": (total_success / total_requests * 100) if total_requests > 0 else 0,\n",
    "                \"avg_latency\": (total_latency / total_requests) if total_requests > 0 else 0,\n",
    "                \"error_rate\": (total_errors / total_requests * 100) if total_requests > 0 else 0\n",
    "            }\n",
    "        \n",
    "        control_stats = calculate_stats(control_metrics)\n",
    "        treatment_stats = calculate_stats(treatment_metrics)\n",
    "        \n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"status\": test_config.status,\n",
    "            \"control_version\": test_config.control_version,\n",
    "            \"treatment_version\": test_config.treatment_version,\n",
    "            \"control_stats\": control_stats,\n",
    "            \"treatment_stats\": treatment_stats,\n",
    "            \"total_requests\": control_stats[\"requests\"] + treatment_stats[\"requests\"],\n",
    "            \"test_duration\": (datetime.now() - test_config.start_time).total_seconds() / 3600\n",
    "        }\n",
    "\n",
    "\n",
    "# å‰µå»º A/B æ¸¬è©¦ç®¡ç†å™¨\n",
    "ab_test_manager = ABTestManager(version_manager)\n",
    "print(\"âœ… A/B æ¸¬è©¦ç®¡ç†å™¨å·²å‰µå»º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 2.2 å‰µå»ºå’Œå•Ÿå‹• A/B æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»º A/B æ¸¬è©¦é…ç½®\n",
    "ab_test_config = ABTestConfig(\n",
    "    test_name=\"model_v2_vs_v3_performance\",\n",
    "    model_name=\"text_classifier\",\n",
    "    control_version=2,\n",
    "    treatment_version=3,\n",
    "    traffic_split=20.0,  # 20% æµé‡åˆ°æ–°ç‰ˆæœ¬\n",
    "    start_time=datetime.now(),\n",
    "    end_time=datetime.now() + timedelta(hours=24),\n",
    "    success_metrics=[\"latency\", \"accuracy\", \"error_rate\"],\n",
    "    min_sample_size=1000,\n",
    "    significance_level=0.05,\n",
    "    status=\"planned\"\n",
    ")\n",
    "\n",
    "# å‰µå»ºæ¸¬è©¦\n",
    "ab_test_manager.create_test(ab_test_config)\n",
    "\n",
    "# å•Ÿå‹•æ¸¬è©¦\n",
    "ab_test_manager.start_test(\"model_v2_vs_v3_performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 2.3 æ¨¡æ“¬æ¸¬è©¦æ•¸æ“šæ”¶é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬è«‹æ±‚å’Œæ•¸æ“šæ”¶é›†\n",
    "def simulate_ab_test_traffic(ab_manager: ABTestManager, test_name: str, \n",
    "                           num_requests: int = 500):\n",
    "    \"\"\"æ¨¡æ“¬ A/B æ¸¬è©¦æµé‡\"\"\"\n",
    "    if test_name not in ab_manager.active_tests:\n",
    "        print(f\"âŒ æ¸¬è©¦ '{test_name}' ä¸å­˜åœ¨\")\n",
    "        return\n",
    "    \n",
    "    test_config = ab_manager.active_tests[test_name]\n",
    "    print(f\"ğŸ“Š é–‹å§‹æ¨¡æ“¬ {num_requests} å€‹è«‹æ±‚çš„ A/B æ¸¬è©¦æµé‡...\")\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        # æ ¹æ“šæµé‡åˆ†é…é¸æ“‡ç‰ˆæœ¬\n",
    "        if random.uniform(0, 100) < test_config.traffic_split:\n",
    "            version = test_config.treatment_version\n",
    "            # Treatment ç‰ˆæœ¬é€šå¸¸æœ‰ä¸åŒçš„æ€§èƒ½ç‰¹æ€§\n",
    "            base_latency = 80\n",
    "            base_success_rate = 0.96\n",
    "        else:\n",
    "            version = test_config.control_version\n",
    "            # Control ç‰ˆæœ¬çš„åŸºæº–æ€§èƒ½\n",
    "            base_latency = 100\n",
    "            base_success_rate = 0.95\n",
    "        \n",
    "        # æ¨¡æ“¬è«‹æ±‚å»¶é²ï¼ˆæ·»åŠ éš¨æ©Ÿè®ŠåŒ–ï¼‰\n",
    "        latency = base_latency + random.gauss(0, 20)\n",
    "        latency = max(10, latency)  # ç¢ºä¿å»¶é²ç‚ºæ­£æ•¸\n",
    "        \n",
    "        # æ¨¡æ“¬æˆåŠŸç‡\n",
    "        success = random.random() < base_success_rate\n",
    "        \n",
    "        # è¨˜éŒ„æŒ‡æ¨™\n",
    "        ab_manager.record_metrics(test_name, version, latency, success)\n",
    "        \n",
    "        # æ¯100å€‹è«‹æ±‚é¡¯ç¤ºé€²åº¦\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"   ğŸ“ˆ å·²è™•ç† {i + 1}/{num_requests} è«‹æ±‚\")\n",
    "        \n",
    "        # æ¨¡æ“¬è«‹æ±‚é–“éš”\n",
    "        time.sleep(0.001)\n",
    "    \n",
    "    print(f\"âœ… æ¨¡æ“¬å®Œæˆï¼Œå…±è™•ç† {num_requests} å€‹è«‹æ±‚\")\n",
    "\n",
    "\n",
    "# åŸ·è¡Œæ¨¡æ“¬\n",
    "simulate_ab_test_traffic(ab_test_manager, \"model_v2_vs_v3_performance\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 2.4 A/B æ¸¬è©¦çµæœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡¯ç¤ºæ¸¬è©¦æ‘˜è¦\n",
    "def display_ab_test_results(ab_manager: ABTestManager, test_name: str):\n",
    "    \"\"\"é¡¯ç¤º A/B æ¸¬è©¦çµæœ\"\"\"\n",
    "    summary = ab_manager.get_test_summary(test_name)\n",
    "    \n",
    "    if \"error\" in summary:\n",
    "        print(f\"âŒ {summary['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“Š A/B æ¸¬è©¦çµæœå ±å‘Š: {test_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"ğŸ”¬ æ¸¬è©¦ç‹€æ…‹: {summary['status'].upper()}\")\n",
    "    print(f\"â±ï¸  æ¸¬è©¦æ™‚é•·: {summary['test_duration']:.1f} å°æ™‚\")\n",
    "    print(f\"ğŸ“ˆ ç¸½è«‹æ±‚æ•¸: {summary['total_requests']}\")\n",
    "    \n",
    "    print(\"\\nğŸ…°ï¸  æ§åˆ¶çµ„ (Version {}):\".format(summary['control_version']))\n",
    "    control = summary['control_stats']\n",
    "    print(f\"   ğŸ“Š è«‹æ±‚æ•¸: {control['requests']}\")\n",
    "    print(f\"   âœ… æˆåŠŸç‡: {control['success_rate']:.2f}%\")\n",
    "    print(f\"   â±ï¸  å¹³å‡å»¶é²: {control['avg_latency']:.1f}ms\")\n",
    "    print(f\"   âŒ éŒ¯èª¤ç‡: {control['error_rate']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nğŸ…±ï¸  å¯¦é©—çµ„ (Version {}):\".format(summary['treatment_version']))\n",
    "    treatment = summary['treatment_stats']\n",
    "    print(f\"   ğŸ“Š è«‹æ±‚æ•¸: {treatment['requests']}\")\n",
    "    print(f\"   âœ… æˆåŠŸç‡: {treatment['success_rate']:.2f}%\")\n",
    "    print(f\"   â±ï¸  å¹³å‡å»¶é²: {treatment['avg_latency']:.1f}ms\")\n",
    "    print(f\"   âŒ éŒ¯èª¤ç‡: {treatment['error_rate']:.2f}%\")\n",
    "    \n",
    "    # è¨ˆç®—æ”¹é€²åº¦\n",
    "    if control['avg_latency'] > 0 and treatment['avg_latency'] > 0:\n",
    "        latency_improvement = ((control['avg_latency'] - treatment['avg_latency']) / \n",
    "                              control['avg_latency']) * 100\n",
    "        success_improvement = treatment['success_rate'] - control['success_rate']\n",
    "        \n",
    "        print(\"\\nğŸ“ˆ æ€§èƒ½å°æ¯”:\")\n",
    "        print(f\"   âš¡ å»¶é²æ”¹å–„: {latency_improvement:+.1f}%\")\n",
    "        print(f\"   âœ… æˆåŠŸç‡è®ŠåŒ–: {success_improvement:+.2f}%\")\n",
    "        \n",
    "        # ç°¡å–®çš„çµ±è¨ˆé¡¯è‘—æ€§åˆ¤æ–·\n",
    "        min_sample_size = 100\n",
    "        if (control['requests'] >= min_sample_size and \n",
    "            treatment['requests'] >= min_sample_size):\n",
    "            \n",
    "            if abs(latency_improvement) > 5:\n",
    "                significance = \"é¡¯è‘—\" if abs(latency_improvement) > 10 else \"ä¸­ç­‰\"\n",
    "                print(f\"   ğŸ”¬ å»¶é²å·®ç•°: {significance}\")\n",
    "            \n",
    "            if abs(success_improvement) > 1:\n",
    "                significance = \"é¡¯è‘—\" if abs(success_improvement) > 2 else \"ä¸­ç­‰\"\n",
    "                print(f\"   ğŸ”¬ æˆåŠŸç‡å·®ç•°: {significance}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  æ¨£æœ¬é‡ä¸è¶³ï¼Œéœ€è¦æ›´å¤šæ•¸æ“šé€²è¡Œçµ±è¨ˆæ¨æ–·\")\n",
    "\n",
    "\n",
    "# é¡¯ç¤ºæ¸¬è©¦çµæœ\n",
    "display_ab_test_results(ab_test_manager, \"model_v2_vs_v3_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ– A/B æ¸¬è©¦çµæœ\n",
    "def plot_ab_test_comparison(ab_manager: ABTestManager, test_name: str):\n",
    "    \"\"\"å¯è¦–åŒ– A/B æ¸¬è©¦å°æ¯”\"\"\"\n",
    "    summary = ab_manager.get_test_summary(test_name)\n",
    "    \n",
    "    if \"error\" in summary:\n",
    "        print(f\"âŒ {summary['error']}\")\n",
    "        return\n",
    "    \n",
    "    control = summary['control_stats']\n",
    "    treatment = summary['treatment_stats']\n",
    "    \n",
    "    # å‰µå»ºå°æ¯”åœ–\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # å»¶é²å°æ¯”\n",
    "    versions = ['Control (V{})'.format(summary['control_version']), \n",
    "               'Treatment (V{})'.format(summary['treatment_version'])]\n",
    "    latencies = [control['avg_latency'], treatment['avg_latency']]\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    \n",
    "    bars1 = ax1.bar(versions, latencies, color=colors, alpha=0.7)\n",
    "    ax1.set_title('å¹³å‡å»¶é²å°æ¯” (ms)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('å»¶é² (ms)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars1, latencies):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # æˆåŠŸç‡å°æ¯”\n",
    "    success_rates = [control['success_rate'], treatment['success_rate']]\n",
    "    bars2 = ax2.bar(versions, success_rates, color=colors, alpha=0.7)\n",
    "    ax2.set_title('æˆåŠŸç‡å°æ¯” (%)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('æˆåŠŸç‡ (%)')\n",
    "    ax2.set_ylim(90, 100)  # èšç„¦åœ¨ç›¸é—œç¯„åœ\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars2, success_rates):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{val:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # è«‹æ±‚é‡åˆ†å¸ƒ\n",
    "    request_counts = [control['requests'], treatment['requests']]\n",
    "    bars3 = ax3.bar(versions, request_counts, color=colors, alpha=0.7)\n",
    "    ax3.set_title('è«‹æ±‚é‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('è«‹æ±‚æ•¸')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars3, request_counts):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                f'{val}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # éŒ¯èª¤ç‡å°æ¯”\n",
    "    error_rates = [control['error_rate'], treatment['error_rate']]\n",
    "    bars4 = ax4.bar(versions, error_rates, color=['#e67e22', '#e67e22'], alpha=0.7)\n",
    "    ax4.set_title('éŒ¯èª¤ç‡å°æ¯” (%)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('éŒ¯èª¤ç‡ (%)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars4, error_rates):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                f'{val:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'A/B æ¸¬è©¦çµæœå°æ¯”: {test_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_ab_test_comparison(ab_test_manager, \"model_v2_vs_v3_performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 3ï¼šæ¼¸é€²å¼éƒ¨ç½² (Canary Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 3.1 Canary éƒ¨ç½²ç®¡ç†å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CanaryConfig:\n",
    "    \"\"\"Canary éƒ¨ç½²é…ç½®\"\"\"\n",
    "    deployment_name: str\n",
    "    model_name: str\n",
    "    stable_version: int\n",
    "    canary_version: int\n",
    "    initial_traffic: float\n",
    "    target_traffic: float\n",
    "    increment_step: float\n",
    "    step_duration: int  # åˆ†é˜\n",
    "    success_threshold: Dict[str, float]\n",
    "    rollback_threshold: Dict[str, float]\n",
    "    auto_promote: bool\n",
    "\n",
    "\n",
    "class CanaryDeploymentManager:\n",
    "    \"\"\"Canary éƒ¨ç½²ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, version_manager: ModelVersionManager):\n",
    "        self.version_manager = version_manager\n",
    "        self.active_deployments: Dict[str, CanaryConfig] = {}\n",
    "        self.deployment_metrics: Dict[str, List] = {}\n",
    "        self.deployment_history: Dict[str, List] = {}\n",
    "    \n",
    "    def create_canary_deployment(self, config: CanaryConfig) -> bool:\n",
    "        \"\"\"å‰µå»º Canary éƒ¨ç½²\"\"\"\n",
    "        try:\n",
    "            # é©—è­‰ç‰ˆæœ¬\n",
    "            stable_version = self.version_manager.get_version_info(config.stable_version)\n",
    "            canary_version = self.version_manager.get_version_info(config.canary_version)\n",
    "            \n",
    "            if not stable_version or not canary_version:\n",
    "                raise ValueError(\"æŒ‡å®šçš„ç‰ˆæœ¬ä¸å­˜åœ¨\")\n",
    "            \n",
    "            # è¨­ç½®åˆå§‹æµé‡åˆ†é…\n",
    "            initial_traffic_config = {\n",
    "                config.stable_version: 100 - config.initial_traffic,\n",
    "                config.canary_version: config.initial_traffic\n",
    "            }\n",
    "            \n",
    "            self.version_manager.set_traffic_split(initial_traffic_config)\n",
    "            self.active_deployments[config.deployment_name] = config\n",
    "            self.deployment_metrics[config.deployment_name] = []\n",
    "            self.deployment_history[config.deployment_name] = []\n",
    "            \n",
    "            # è¨˜éŒ„åˆå§‹ç‹€æ…‹\n",
    "            self.deployment_history[config.deployment_name].append({\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"action\": \"deployment_started\",\n",
    "                \"canary_traffic\": config.initial_traffic,\n",
    "                \"status\": \"active\"\n",
    "            })\n",
    "            \n",
    "            print(f\"ğŸš€ Canary éƒ¨ç½² '{config.deployment_name}' å·²å‰µå»º\")\n",
    "            print(f\"   ğŸ“Š ç©©å®šç‰ˆæœ¬: V{config.stable_version} ({100-config.initial_traffic:.1f}%)\")\n",
    "            print(f\"   ğŸ¤ Canary ç‰ˆæœ¬: V{config.canary_version} ({config.initial_traffic:.1f}%)\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Canary éƒ¨ç½²å‰µå»ºå¤±æ•—: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def monitor_and_scale(self, deployment_name: str, \n",
    "                         current_metrics: Dict[str, float]) -> str:\n",
    "        \"\"\"ç›£æ§ä¸¦è‡ªå‹•èª¿æ•´æµé‡\"\"\"\n",
    "        if deployment_name not in self.active_deployments:\n",
    "            return \"deployment_not_found\"\n",
    "        \n",
    "        config = self.active_deployments[deployment_name]\n",
    "        current_traffic = self.version_manager.get_version_info(\n",
    "            config.canary_version\n",
    "        ).traffic_percentage\n",
    "        \n",
    "        # æª¢æŸ¥å›æ»¾æ¢ä»¶\n",
    "        for metric, threshold in config.rollback_threshold.items():\n",
    "            if metric in current_metrics:\n",
    "                if metric == \"error_rate\" and current_metrics[metric] > threshold:\n",
    "                    return self._rollback_deployment(deployment_name, \n",
    "                                                   f\"High {metric}: {current_metrics[metric]:.3f}\")\n",
    "                elif metric == \"latency\" and current_metrics[metric] > threshold:\n",
    "                    return self._rollback_deployment(deployment_name, \n",
    "                                                   f\"High {metric}: {current_metrics[metric]:.1f}ms\")\n",
    "        \n",
    "        # æª¢æŸ¥æˆåŠŸæ¢ä»¶\n",
    "        success_criteria_met = True\n",
    "        for metric, threshold in config.success_threshold.items():\n",
    "            if metric in current_metrics:\n",
    "                if metric == \"error_rate\" and current_metrics[metric] > threshold:\n",
    "                    success_criteria_met = False\n",
    "                elif metric == \"latency\" and current_metrics[metric] > threshold:\n",
    "                    success_criteria_met = False\n",
    "        \n",
    "        # å¦‚æœæˆåŠŸæ¢ä»¶æ»¿è¶³ï¼Œå¢åŠ æµé‡\n",
    "        if success_criteria_met and current_traffic < config.target_traffic:\n",
    "            new_traffic = min(current_traffic + config.increment_step, \n",
    "                            config.target_traffic)\n",
    "            \n",
    "            new_traffic_config = {\n",
    "                config.stable_version: 100 - new_traffic,\n",
    "                config.canary_version: new_traffic\n",
    "            }\n",
    "            \n",
    "            self.version_manager.set_traffic_split(new_traffic_config)\n",
    "            \n",
    "            # è¨˜éŒ„æ­·å²\n",
    "            self.deployment_history[deployment_name].append({\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"action\": \"traffic_increased\",\n",
    "                \"canary_traffic\": new_traffic,\n",
    "                \"metrics\": current_metrics.copy(),\n",
    "                \"status\": \"scaling\"\n",
    "            })\n",
    "            \n",
    "            print(f\"ğŸ“ˆ Canary æµé‡å¢åŠ åˆ° {new_traffic:.1f}%\")\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™\n",
    "            if new_traffic >= config.target_traffic and config.auto_promote:\n",
    "                return self._promote_canary(deployment_name)\n",
    "            \n",
    "            return \"traffic_increased\"\n",
    "        \n",
    "        return \"stable\"\n",
    "    \n",
    "    def _rollback_deployment(self, deployment_name: str, reason: str) -> str:\n",
    "        \"\"\"å›æ»¾éƒ¨ç½²\"\"\"\n",
    "        config = self.active_deployments[deployment_name]\n",
    "        \n",
    "        # æ¢å¾©åˆ°ç©©å®šç‰ˆæœ¬\n",
    "        self.version_manager.set_traffic_split({config.stable_version: 100.0})\n",
    "        \n",
    "        # è¨˜éŒ„å›æ»¾\n",
    "        self.deployment_history[deployment_name].append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"action\": \"rollback\",\n",
    "            \"reason\": reason,\n",
    "            \"canary_traffic\": 0.0,\n",
    "            \"status\": \"rolled_back\"\n",
    "        })\n",
    "        \n",
    "        print(f\"ğŸ”™ Canary éƒ¨ç½²å·²å›æ»¾: {reason}\")\n",
    "        return \"rolled_back\"\n",
    "    \n",
    "    def _promote_canary(self, deployment_name: str) -> str:\n",
    "        \"\"\"æå‡ Canary ç‚ºç©©å®šç‰ˆæœ¬\"\"\"\n",
    "        config = self.active_deployments[deployment_name]\n",
    "        \n",
    "        # å°‡ Canary ç‰ˆæœ¬è¨­ç‚º 100% æµé‡\n",
    "        self.version_manager.set_traffic_split({config.canary_version: 100.0})\n",
    "        \n",
    "        # è¨˜éŒ„æå‡\n",
    "        self.deployment_history[deployment_name].append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"action\": \"promoted\",\n",
    "            \"canary_traffic\": 100.0,\n",
    "            \"status\": \"completed\"\n",
    "        })\n",
    "        \n",
    "        print(f\"ğŸ‰ Canary ç‰ˆæœ¬å·²æå‡ç‚ºç©©å®šç‰ˆæœ¬\")\n",
    "        return \"promoted\"\n",
    "    \n",
    "    def get_deployment_status(self, deployment_name: str) -> Dict:\n",
    "        \"\"\"ç²å–éƒ¨ç½²ç‹€æ…‹\"\"\"\n",
    "        if deployment_name not in self.active_deployments:\n",
    "            return {\"error\": \"éƒ¨ç½²ä¸å­˜åœ¨\"}\n",
    "        \n",
    "        config = self.active_deployments[deployment_name]\n",
    "        history = self.deployment_history.get(deployment_name, [])\n",
    "        \n",
    "        current_canary_traffic = self.version_manager.get_version_info(\n",
    "            config.canary_version\n",
    "        ).traffic_percentage\n",
    "        \n",
    "        latest_status = \"unknown\"\n",
    "        if history:\n",
    "            latest_status = history[-1][\"status\"]\n",
    "        \n",
    "        return {\n",
    "            \"deployment_name\": deployment_name,\n",
    "            \"stable_version\": config.stable_version,\n",
    "            \"canary_version\": config.canary_version,\n",
    "            \"current_canary_traffic\": current_canary_traffic,\n",
    "            \"target_traffic\": config.target_traffic,\n",
    "            \"status\": latest_status,\n",
    "            \"steps_completed\": len(history),\n",
    "            \"progress\": (current_canary_traffic / config.target_traffic) * 100\n",
    "        }\n",
    "\n",
    "\n",
    "# å‰µå»º Canary éƒ¨ç½²ç®¡ç†å™¨\n",
    "canary_manager = CanaryDeploymentManager(version_manager)\n",
    "print(\"âœ… Canary éƒ¨ç½²ç®¡ç†å™¨å·²å‰µå»º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 3.2 å•Ÿå‹• Canary éƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»º Canary éƒ¨ç½²é…ç½®\n",
    "canary_config = CanaryConfig(\n",
    "    deployment_name=\"v3_canary_rollout\",\n",
    "    model_name=\"text_classifier\",\n",
    "    stable_version=2,\n",
    "    canary_version=3,\n",
    "    initial_traffic=5.0,\n",
    "    target_traffic=100.0,\n",
    "    increment_step=15.0,\n",
    "    step_duration=5,  # 5åˆ†é˜\n",
    "    success_threshold={\n",
    "        \"error_rate\": 0.05,  # 5% ä»¥ä¸‹\n",
    "        \"latency\": 120.0     # 120ms ä»¥ä¸‹\n",
    "    },\n",
    "    rollback_threshold={\n",
    "        \"error_rate\": 0.10,  # 10% ä»¥ä¸Šå›æ»¾\n",
    "        \"latency\": 200.0     # 200ms ä»¥ä¸Šå›æ»¾\n",
    "    },\n",
    "    auto_promote=True\n",
    ")\n",
    "\n",
    "# å‰µå»ºä¸¦å•Ÿå‹• Canary éƒ¨ç½²\n",
    "canary_manager.create_canary_deployment(canary_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 3.3 æ¨¡æ“¬ Canary éƒ¨ç½²éç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬ Canary éƒ¨ç½²çš„ç›£æ§å’Œè‡ªå‹•èª¿æ•´éç¨‹\n",
    "def simulate_canary_deployment(canary_manager: CanaryDeploymentManager, \n",
    "                             deployment_name: str, steps: int = 6):\n",
    "    \"\"\"æ¨¡æ“¬ Canary éƒ¨ç½²éç¨‹\"\"\"\n",
    "    print(f\"ğŸ¤ é–‹å§‹æ¨¡æ“¬ Canary éƒ¨ç½²: {deployment_name}\")\n",
    "    \n",
    "    for step in range(steps):\n",
    "        print(f\"\\n--- æ­¥é©Ÿ {step + 1}/{steps} ---\")\n",
    "        \n",
    "        # æ¨¡æ“¬ç•¶å‰æ€§èƒ½æŒ‡æ¨™ï¼ˆé€æ­¥æ”¹å–„ï¼‰\n",
    "        base_error_rate = 0.03 + random.uniform(-0.01, 0.02)\n",
    "        base_latency = 85 + random.uniform(-15, 25)\n",
    "        \n",
    "        # å¶çˆ¾æ¨¡æ“¬æ€§èƒ½å•é¡Œ\n",
    "        if step == 3 and random.random() < 0.3:  # 30% æ©Ÿç‡åœ¨ç¬¬3æ­¥å‡ºç¾å•é¡Œ\n",
    "            base_error_rate = 0.12  # è§¸ç™¼å›æ»¾\n",
    "            base_latency = 220\n",
    "            print(\"âš ï¸  æª¢æ¸¬åˆ°æ€§èƒ½å•é¡Œ...\")\n",
    "        \n",
    "        current_metrics = {\n",
    "            \"error_rate\": base_error_rate,\n",
    "            \"latency\": base_latency,\n",
    "            \"throughput\": random.uniform(800, 1200)\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ“Š ç•¶å‰æŒ‡æ¨™:\")\n",
    "        print(f\"   éŒ¯èª¤ç‡: {current_metrics['error_rate']:.3f}\")\n",
    "        print(f\"   å»¶é²: {current_metrics['latency']:.1f}ms\")\n",
    "        print(f\"   ååé‡: {current_metrics['throughput']:.0f} QPS\")\n",
    "        \n",
    "        # ç›£æ§ä¸¦èª¿æ•´\n",
    "        result = canary_manager.monitor_and_scale(deployment_name, current_metrics)\n",
    "        \n",
    "        # é¡¯ç¤ºéƒ¨ç½²ç‹€æ…‹\n",
    "        status = canary_manager.get_deployment_status(deployment_name)\n",
    "        if \"error\" not in status:\n",
    "            print(f\"ğŸ¯ ç•¶å‰é€²åº¦: {status['progress']:.1f}% å®Œæˆ\")\n",
    "            print(f\"ğŸ“ˆ Canary æµé‡: {status['current_canary_traffic']:.1f}%\")\n",
    "        \n",
    "        # æª¢æŸ¥éƒ¨ç½²çµæœ\n",
    "        if result == \"rolled_back\":\n",
    "            print(\"ğŸ”™ éƒ¨ç½²å·²å›æ»¾ï¼Œåœæ­¢æ¨¡æ“¬\")\n",
    "            break\n",
    "        elif result == \"promoted\":\n",
    "            print(\"ğŸ‰ Canary ç‰ˆæœ¬å·²æå‡ï¼Œéƒ¨ç½²å®Œæˆ\")\n",
    "            break\n",
    "        \n",
    "        # æ¨¡æ“¬æ™‚é–“é–“éš”\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"\\nğŸ Canary éƒ¨ç½²æ¨¡æ“¬å®Œæˆ\")\n",
    "\n",
    "\n",
    "# åŸ·è¡Œæ¨¡æ“¬\n",
    "simulate_canary_deployment(canary_manager, \"v3_canary_rollout\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 3.4 Canary éƒ¨ç½²æ­·å²åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ– Canary éƒ¨ç½²æ­·å²\n",
    "def plot_canary_deployment_history(canary_manager: CanaryDeploymentManager, \n",
    "                                 deployment_name: str):\n",
    "    \"\"\"å¯è¦–åŒ– Canary éƒ¨ç½²æ­·å²\"\"\"\n",
    "    if deployment_name not in canary_manager.deployment_history:\n",
    "        print(f\"âŒ éƒ¨ç½² '{deployment_name}' çš„æ­·å²è¨˜éŒ„ä¸å­˜åœ¨\")\n",
    "        return\n",
    "    \n",
    "    history = canary_manager.deployment_history[deployment_name]\n",
    "    \n",
    "    if not history:\n",
    "        print(\"âŒ æ²’æœ‰æ­·å²æ•¸æ“šå¯ä¾›åˆ†æ\")\n",
    "        return\n",
    "    \n",
    "    # æº–å‚™æ•¸æ“š\n",
    "    timestamps = []\n",
    "    traffic_percentages = []\n",
    "    actions = []\n",
    "    \n",
    "    for record in history:\n",
    "        timestamps.append(record[\"timestamp\"])\n",
    "        traffic_percentages.append(record[\"canary_traffic\"])\n",
    "        actions.append(record[\"action\"])\n",
    "    \n",
    "    # å‰µå»ºåœ–è¡¨\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # æµé‡è®ŠåŒ–è¶¨å‹¢\n",
    "    ax1.plot(timestamps, traffic_percentages, marker='o', linewidth=2, \n",
    "            markersize=8, color='#3498db')\n",
    "    ax1.set_title(f'Canary éƒ¨ç½²æµé‡è®ŠåŒ–: {deployment_name}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Canary æµé‡ (%)', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 105)\n",
    "    \n",
    "    # æ¨™è¨˜é—œéµäº‹ä»¶\n",
    "    for i, (ts, traffic, action) in enumerate(zip(timestamps, traffic_percentages, actions)):\n",
    "        if action == \"rollback\":\n",
    "            ax1.annotate('å›æ»¾', xy=(ts, traffic), xytext=(ts, traffic + 10),\n",
    "                        arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                        color='red', fontweight='bold')\n",
    "        elif action == \"promoted\":\n",
    "            ax1.annotate('æå‡', xy=(ts, traffic), xytext=(ts, traffic - 10),\n",
    "                        arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                        color='green', fontweight='bold')\n",
    "    \n",
    "    # äº‹ä»¶æ™‚é–“ç·š\n",
    "    action_colors = {\n",
    "        'deployment_started': '#3498db',\n",
    "        'traffic_increased': '#2ecc71',\n",
    "        'rollback': '#e74c3c',\n",
    "        'promoted': '#f39c12'\n",
    "    }\n",
    "    \n",
    "    for i, action in enumerate(actions):\n",
    "        color = action_colors.get(action, '#95a5a6')\n",
    "        ax2.barh(i, 1, color=color, alpha=0.7)\n",
    "        ax2.text(0.5, i, action.replace('_', ' ').title(), \n",
    "                ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('éƒ¨ç½²äº‹ä»¶æ™‚é–“ç·š', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('æ™‚é–“é€²åº¦', fontsize=12)\n",
    "    ax2.set_yticks(range(len(actions)))\n",
    "    ax2.set_yticklabels([f'{i+1}' for i in range(len(actions))])\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # é¡¯ç¤ºæ‘˜è¦çµ±è¨ˆ\n",
    "    print(f\"\\nğŸ“Š Canary éƒ¨ç½²æ‘˜è¦: {deployment_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“… é–‹å§‹æ™‚é–“: {timestamps[0].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ğŸ“… çµæŸæ™‚é–“: {timestamps[-1].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"â±ï¸  ç¸½è€—æ™‚: {(timestamps[-1] - timestamps[0]).total_seconds():.1f} ç§’\")\n",
    "    print(f\"ğŸ”„ æ­¥é©Ÿæ•¸: {len(history)}\")\n",
    "    print(f\"ğŸ¯ æœ€çµ‚ç‹€æ…‹: {history[-1]['status']}\")\n",
    "    print(f\"ğŸ“ˆ æœ€çµ‚æµé‡: {history[-1]['canary_traffic']:.1f}%\")\n",
    "\n",
    "\n",
    "# å¯è¦–åŒ–éƒ¨ç½²æ­·å²\n",
    "plot_canary_deployment_history(canary_manager, \"v3_canary_rollout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 4ï¼šé…ç½®å°å‡ºå’ŒæŒä¹…åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å‡ºå®Œæ•´çš„å¯¦é©—é…ç½®å’Œçµæœ\n",
    "def export_experiment_results(version_manager, ab_test_manager, canary_manager):\n",
    "    \"\"\"å°å‡ºå¯¦é©—çµæœ\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # ç‰ˆæœ¬ç®¡ç†é…ç½®\n",
    "    version_config_file = f\"{EXPERIMENT_DIR}/configs/version_config_{timestamp}.json\"\n",
    "    version_manager.export_config(version_config_file)\n",
    "    \n",
    "    # A/B æ¸¬è©¦çµæœ\n",
    "    ab_results = {}\n",
    "    for test_name in ab_test_manager.active_tests.keys():\n",
    "        ab_results[test_name] = ab_test_manager.get_test_summary(test_name)\n",
    "    \n",
    "    ab_results_file = f\"{EXPERIMENT_DIR}/configs/ab_test_results_{timestamp}.json\"\n",
    "    with open(ab_results_file, 'w') as f:\n",
    "        json.dump(ab_results, f, indent=2, default=str)\n",
    "    \n",
    "    # Canary éƒ¨ç½²çµæœ\n",
    "    canary_results = {}\n",
    "    for deployment_name in canary_manager.active_deployments.keys():\n",
    "        canary_results[deployment_name] = {\n",
    "            \"status\": canary_manager.get_deployment_status(deployment_name),\n",
    "            \"history\": canary_manager.deployment_history.get(deployment_name, [])\n",
    "        }\n",
    "    \n",
    "    canary_results_file = f\"{EXPERIMENT_DIR}/configs/canary_results_{timestamp}.json\"\n",
    "    with open(canary_results_file, 'w') as f:\n",
    "        json.dump(canary_results, f, indent=2, default=str)\n",
    "    \n",
    "    # å‰µå»ºç¶œåˆå ±å‘Š\n",
    "    comprehensive_report = {\n",
    "        \"experiment_timestamp\": timestamp,\n",
    "        \"model_name\": version_manager.model_name,\n",
    "        \"total_versions\": len(version_manager.versions),\n",
    "        \"ab_tests_count\": len(ab_test_manager.active_tests),\n",
    "        \"canary_deployments_count\": len(canary_manager.active_deployments),\n",
    "        \"files_generated\": {\n",
    "            \"version_config\": version_config_file,\n",
    "            \"ab_test_results\": ab_results_file,\n",
    "            \"canary_results\": canary_results_file\n",
    "        },\n",
    "        \"summary\": {\n",
    "            \"current_traffic_split\": version_manager.traffic_rules,\n",
    "            \"active_ab_tests\": list(ab_test_manager.active_tests.keys()),\n",
    "            \"active_canary_deployments\": list(canary_manager.active_deployments.keys())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    report_file = f\"{EXPERIMENT_DIR}/experiment_report_{timestamp}.json\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(comprehensive_report, f, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ“Š å¯¦é©—çµæœå·²å°å‡º:\")\n",
    "    print(f\"   ğŸ“„ ç¶œåˆå ±å‘Š: {report_file}\")\n",
    "    print(f\"   âš™ï¸  ç‰ˆæœ¬é…ç½®: {version_config_file}\")\n",
    "    print(f\"   ğŸ§ª A/B æ¸¬è©¦: {ab_results_file}\")\n",
    "    print(f\"   ğŸ¤ Canary éƒ¨ç½²: {canary_results_file}\")\n",
    "\n",
    "\n",
    "# å°å‡ºå¯¦é©—çµæœ\n",
    "export_experiment_results(version_manager, ab_test_manager, canary_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## ğŸ“Š æœ€ä½³å¯¦è¸ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ä½³å¯¦è¸æŒ‡å—\n",
    "best_practices = \"\"\"\n",
    "ğŸ¯ ä¼æ¥­ç´šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†èˆ‡ A/B æ¸¬è©¦æœ€ä½³å¯¦è¸\n",
    "\n",
    "ğŸ“‹ ç‰ˆæœ¬ç®¡ç†ç­–ç•¥:\n",
    "   âœ… èªç¾©åŒ–ç‰ˆæœ¬æ§åˆ¶ (Semantic Versioning)\n",
    "   âœ… å®Œæ•´çš„ç‰ˆæœ¬å…ƒæ•¸æ“šè¨˜éŒ„\n",
    "   âœ… è‡ªå‹•åŒ–ç‰ˆæœ¬ç™¼ç¾å’Œè¨»å†Š\n",
    "   âœ… ç‰ˆæœ¬é–“ç›¸å®¹æ€§æª¢æŸ¥\n",
    "\n",
    "ğŸ§ª A/B æ¸¬è©¦è¨­è¨ˆåŸå‰‡:\n",
    "   âœ… æ˜ç¢ºå®šç¾©æˆåŠŸæŒ‡æ¨™\n",
    "   âœ… é©ç•¶çš„æ¨£æœ¬é‡è¨ˆç®—\n",
    "   âœ… çµ±è¨ˆé¡¯è‘—æ€§é©—è­‰\n",
    "   âœ… å¤šç¶­åº¦æ€§èƒ½è©•ä¼°\n",
    "\n",
    "ğŸ¤ Canary éƒ¨ç½²ç­–ç•¥:\n",
    "   âœ… æ¼¸é€²å¼æµé‡å¢åŠ \n",
    "   âœ… å¯¦æ™‚ç›£æ§å’Œè‡ªå‹•å›æ»¾\n",
    "   âœ… å¤šå±¤ç´šå¥åº·æª¢æŸ¥\n",
    "   âœ… æ¥­å‹™æŒ‡æ¨™æŒçºŒç›£æ§\n",
    "\n",
    "âš ï¸ é¢¨éšªæ§åˆ¶æªæ–½:\n",
    "   âœ… å¿«é€Ÿå›æ»¾æ©Ÿåˆ¶\n",
    "   âœ… å¤šç´šå‘Šè­¦ç³»çµ±\n",
    "   âœ… æ¥­å‹™å½±éŸ¿è©•ä¼°\n",
    "   âœ… ç½é›£æ¢å¾©é æ¡ˆ\n",
    "\n",
    "ğŸ“ˆ ç›£æ§å’Œå¯è§€æ¸¬æ€§:\n",
    "   âœ… å…¨é¢çš„æŒ‡æ¨™æ”¶é›†\n",
    "   âœ… å³æ™‚æ€§èƒ½å„€è¡¨æ¿\n",
    "   âœ… ç•°å¸¸æª¢æ¸¬å’Œå‘Šè­¦\n",
    "   âœ… æ­·å²è¶¨å‹¢åˆ†æ\n",
    "\n",
    "ğŸ”§ é‹ç¶­è‡ªå‹•åŒ–:\n",
    "   âœ… CI/CD ç®¡é“æ•´åˆ\n",
    "   âœ… è‡ªå‹•åŒ–æ¸¬è©¦æµç¨‹\n",
    "   âœ… æ™ºèƒ½æ±ºç­–å¼•æ“\n",
    "   âœ… è‡ªå‹•åŒ–æ•…éšœæ¢å¾©\n",
    "\n",
    "ğŸ’¡ æˆåŠŸè¦ç´ :\n",
    "   ğŸ¯ æ¥­å‹™ç›®æ¨™å°é½Š\n",
    "   ğŸ“Š æ•¸æ“šé©…å‹•æ±ºç­–\n",
    "   ğŸš€ å¿«é€Ÿè¿­ä»£èƒ½åŠ›\n",
    "   ğŸ›¡ï¸ é¢¨éšªæ§åˆ¶æ„è­˜\n",
    "   ğŸ‘¥ è·¨åœ˜éšŠå”ä½œ\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## ğŸ“– ç¸½çµ\n",
    "\n",
    "æœ¬å¯¦é©—å®Œæˆäº†ä¼æ¥­ç´šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†èˆ‡ A/B æ¸¬è©¦çš„å®Œæ•´å¯¦ç¾ï¼š\n",
    "\n",
    "### ğŸ¯ å¯¦é©—æˆæœ\n",
    "1. **ç‰ˆæœ¬ç®¡ç†ç³»çµ±** - å¯¦ç¾äº†å®Œæ•´çš„æ¨¡å‹ç‰ˆæœ¬ç”Ÿå‘½é€±æœŸç®¡ç†\n",
    "2. **A/B æ¸¬è©¦æ¡†æ¶** - æ§‹å»ºäº†è‡ªå‹•åŒ–çš„å¯¦é©—è¨­è¨ˆå’Œåˆ†æç³»çµ±\n",
    "3. **Canary éƒ¨ç½²** - é–‹ç™¼äº†æ¼¸é€²å¼éƒ¨ç½²å’Œè‡ªå‹•åŒ–å›æ»¾æ©Ÿåˆ¶\n",
    "4. **ç›£æ§å’Œå¯è¦–åŒ–** - æä¾›äº†å…¨é¢çš„æ€§èƒ½åˆ†æå’Œæ±ºç­–æ”¯æŒå·¥å…·\n",
    "\n",
    "### ğŸ”§ é—œéµæŠ€è¡“é»\n",
    "- ä¼æ¥­ç´šç‰ˆæœ¬ç®¡ç†ç­–ç•¥\n",
    "- çµ±è¨ˆå­¸é©…å‹•çš„ A/B æ¸¬è©¦\n",
    "- æ™ºèƒ½åŒ–çš„ Canary éƒ¨ç½²\n",
    "- å¯¦æ™‚ç›£æ§å’Œè‡ªå‹•åŒ–æ±ºç­–\n",
    "\n",
    "### ğŸš€ å¯¦éš›æ‡‰ç”¨åƒ¹å€¼\n",
    "1. **é™ä½éƒ¨ç½²é¢¨éšª** - é€šéæ¼¸é€²å¼éƒ¨ç½²æ¸›å°‘ç”Ÿç”¢äº‹æ•…\n",
    "2. **æå‡æ±ºç­–å“è³ª** - åŸºæ–¼æ•¸æ“šçš„ç§‘å­¸æ±ºç­–æµç¨‹\n",
    "3. **åŠ å¿«è¿­ä»£é€Ÿåº¦** - è‡ªå‹•åŒ–æµç¨‹æå‡éƒ¨ç½²æ•ˆç‡\n",
    "4. **å¢å¼·ç³»çµ±ç©©å®šæ€§** - å¤šé‡ä¿éšœæ©Ÿåˆ¶ç¢ºä¿æœå‹™å¯ç”¨æ€§\n",
    "\n",
    "### ğŸ’¡ å­¸ç¿’è¦é»\n",
    "- ä¼æ¥­ç´šéƒ¨ç½²éœ€è¦è€ƒæ…®é¢¨éšªæ§åˆ¶å’Œæ¥­å‹™é€£çºŒæ€§\n",
    "- A/B æ¸¬è©¦éœ€è¦çµ±è¨ˆå­¸åŸºç¤å’Œæ¥­å‹™ç†è§£\n",
    "- Canary éƒ¨ç½²æ˜¯å¹³è¡¡å‰µæ–°å’Œç©©å®šçš„æœ‰æ•ˆç­–ç•¥\n",
    "- ç›£æ§å’Œå¯è§€æ¸¬æ€§æ˜¯æˆåŠŸéƒ¨ç½²çš„é—œéµ\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆ Lab 2.4.1ï¼**\n",
    "\n",
    "æ‚¨å·²ç¶“æŒæ¡äº†ä¼æ¥­ç´šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’Œ A/B æ¸¬è©¦çš„æ ¸å¿ƒæŠ€è¡“ï¼Œå¯ä»¥æ§‹å»ºå®‰å…¨ã€å¯é çš„æ¨¡å‹éƒ¨ç½²æµç¨‹ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}