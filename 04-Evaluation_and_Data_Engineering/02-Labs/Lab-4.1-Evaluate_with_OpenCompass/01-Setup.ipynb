{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1 - OpenCompass è©•ä¼°å¯¦æˆ°\n",
    "## Notebook 01: ç’°å¢ƒé…ç½®èˆ‡åŸºæº–æº–å‚™\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "1. å®‰è£ä¸¦é…ç½® OpenCompass è©•ä¼°æ¡†æ¶\n",
    "2. æº–å‚™è©•ä¼°æ•¸æ“šé›† (C-Eval, CMMLU å­é›†)\n",
    "3. åŠ è¼‰å¾…è©•ä¼°æ¨¡å‹ (Llama-2-7B, Qwen-7B)\n",
    "4. é©—è­‰ç’°å¢ƒèˆ‡ä¾è³´\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 30-45 åˆ†é˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæª¢æŸ¥\n",
    "\n",
    "é¦–å…ˆé©—è­‰ GPU å’ŒåŸºæœ¬ä¾è³´æ˜¯å¦æ­£å¸¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ç’°å¢ƒæª¢æŸ¥\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Python ç‰ˆæœ¬\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "# PyTorch ç‰ˆæœ¬\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "\n",
    "# CUDA å¯ç”¨æ€§\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    è¨˜æ†¶é«”: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ è­¦å‘Š: CUDA ä¸å¯ç”¨,å°‡ä½¿ç”¨ CPU é‹è¡Œ (é€Ÿåº¦æœƒéå¸¸æ…¢)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å®‰è£ OpenCompass\n",
    "\n",
    "OpenCompass æ˜¯ä¸€å€‹å…¨é¢çš„ LLM è©•ä¼°å¹³å°,æ”¯æŒå¤šç¨®è©•ä¼°åŸºæº–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦å·²å®‰è£ OpenCompass\n",
    "if python -c \"import opencompass\" 2>/dev/null; then\n",
    "    echo \"âœ… OpenCompass å·²å®‰è£\"\n",
    "    python -c \"import opencompass; print(f'ç‰ˆæœ¬: {opencompass.__version__}')\"\n",
    "else\n",
    "    echo \"ğŸ“¦ å®‰è£ OpenCompass...\"\n",
    "    \n",
    "    # Clone OpenCompass repository\n",
    "    if [ ! -d \"opencompass\" ]; then\n",
    "        git clone https://github.com/open-compass/opencompass.git\n",
    "    fi\n",
    "    \n",
    "    cd opencompass\n",
    "    pip install -e . --quiet\n",
    "    \n",
    "    echo \"âœ… OpenCompass å®‰è£å®Œæˆ\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æº–å‚™è©•ä¼°æ•¸æ“šé›†\n",
    "\n",
    "æˆ‘å€‘å°‡ä½¿ç”¨ä»¥ä¸‹åŸºæº–çš„å­é›†é€²è¡Œè©•ä¼°:\n",
    "- **C-Eval**: ä¸­æ–‡è©•ä¼°åŸºæº– (52 å€‹å­¸ç§‘)\n",
    "- **CMMLU**: ä¸­æ–‡å¤šä»»å‹™èªè¨€ç†è§£ (67 å€‹ä»»å‹™)\n",
    "\n",
    "ç‚ºäº†åŠ å¿«å¯¦é©—é€Ÿåº¦,æˆ‘å€‘å°‡ä½¿ç”¨éƒ¨åˆ†å­¸ç§‘çš„å­é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# è¨­å®šæ•¸æ“šç›®éŒ„\n",
    "DATA_DIR = Path(\"./data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“¥ ä¸‹è¼‰è©•ä¼°æ•¸æ“šé›†...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# C-Eval æ•¸æ“šé›†\n",
    "try:\n",
    "    print(\"\\n1. C-Eval æ•¸æ“šé›†\")\n",
    "    ceval_subjects = [\n",
    "        \"computer_science\",      # STEM\n",
    "        \"physics\",               # STEM\n",
    "        \"mathematics\",           # STEM\n",
    "        \"chinese_language_and_literature\",  # Humanities\n",
    "        \"history\",               # Humanities\n",
    "        \"law\",                   # Social Science\n",
    "        \"economics\"              # Social Science\n",
    "    ]\n",
    "    \n",
    "    ceval_data = {}\n",
    "    for subject in ceval_subjects:\n",
    "        dataset = load_dataset(\n",
    "            \"ceval/ceval-exam\",\n",
    "            subject,\n",
    "            split=\"val\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        ceval_data[subject] = dataset\n",
    "        print(f\"  âœ… {subject}: {len(dataset)} é¡Œ\")\n",
    "    \n",
    "    print(f\"\\n  ç¸½è¨ˆ: {sum(len(d) for d in ceval_data.values())} é¡Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  âŒ éŒ¯èª¤: {e}\")\n",
    "    print(\"  ğŸ’¡ æç¤º: å¯èƒ½éœ€è¦ HuggingFace å¸³è™Ÿèªè­‰\")\n",
    "    ceval_data = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… æ•¸æ“šé›†æº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æª¢æŸ¥æ•¸æ“šé›†æ ¼å¼\n",
    "\n",
    "è®“æˆ‘å€‘æŸ¥çœ‹ä¸€å€‹æ¨£æœ¬,äº†è§£æ•¸æ“šæ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ceval_data:\n",
    "    # å–å¾—ç¬¬ä¸€å€‹å­¸ç§‘çš„ç¬¬ä¸€é¡Œ\n",
    "    first_subject = list(ceval_data.keys())[0]\n",
    "    sample = ceval_data[first_subject][0]\n",
    "    \n",
    "    print(f\"å­¸ç§‘: {first_subject}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"å•é¡Œ: {sample['question']}\")\n",
    "    print(f\"\\né¸é …:\")\n",
    "    print(f\"  A: {sample['A']}\")\n",
    "    print(f\"  B: {sample['B']}\")\n",
    "    print(f\"  C: {sample['C']}\")\n",
    "    print(f\"  D: {sample['D']}\")\n",
    "    print(f\"\\næ­£ç¢ºç­”æ¡ˆ: {sample['answer']}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸ C-Eval æ•¸æ“šæœªåŠ è¼‰,è·³éæ¨£æœ¬é¡¯ç¤º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æº–å‚™è©•ä¼°æ¨¡å‹\n",
    "\n",
    "æˆ‘å€‘å°‡è©•ä¼°å…©å€‹ 7B åƒæ•¸çš„æ¨¡å‹:\n",
    "- **Llama-2-7B**: Meta çš„é–‹æºåŸºç¤æ¨¡å‹\n",
    "- **Qwen-7B**: é˜¿é‡Œå·´å·´çš„ä¸­æ–‡å„ªåŒ–æ¨¡å‹\n",
    "\n",
    "ç‚ºäº†åŠ é€Ÿè¼‰å…¥å’Œé™ä½è¨˜æ†¶é«”éœ€æ±‚,æˆ‘å€‘å°‡ä½¿ç”¨ 4-bit é‡åŒ–ç‰ˆæœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch\n",
    "\n",
    "# é‡åŒ–é…ç½® (4-bit)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str, use_quantization: bool = True):\n",
    "    \"\"\"\n",
    "    åŠ è¼‰æ¨¡å‹å’Œ tokenizer\n",
    "    \n",
    "    Args:\n",
    "        model_name: æ¨¡å‹åç¨±æˆ–è·¯å¾‘\n",
    "        use_quantization: æ˜¯å¦ä½¿ç”¨é‡åŒ–\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“¥ åŠ è¼‰æ¨¡å‹: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åŠ è¼‰ tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # åŠ è¼‰æ¨¡å‹\n",
    "    model_kwargs = {\n",
    "        \"trust_remote_code\": True,\n",
    "        \"device_map\": \"auto\",\n",
    "    }\n",
    "    \n",
    "    if use_quantization and torch.cuda.is_available():\n",
    "        model_kwargs[\"quantization_config\"] = quantization_config\n",
    "        print(\"  ä½¿ç”¨ 4-bit é‡åŒ–\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        **model_kwargs\n",
    "    )\n",
    "    \n",
    "    # é¡¯ç¤ºæ¨¡å‹è³‡è¨Š\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ\")\n",
    "    print(f\"  ç¸½åƒæ•¸é‡: {total_params:,}\")\n",
    "    print(f\"  å¯è¨“ç·´åƒæ•¸: {trainable_params:,}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"  GPU è¨˜æ†¶é«”ä½¿ç”¨: {memory_allocated:.2f} GB\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è¼‰ Llama-2-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¨æ„: éœ€è¦å…ˆå¾ HuggingFace ç”³è«‹ Llama-2 çš„è¨ªå•æ¬Šé™\n",
    "# å¦‚æœæ²’æœ‰æ¬Šé™,å¯ä»¥ä½¿ç”¨å…¶ä»–é–‹æºæ¨¡å‹å¦‚ Mistral-7B\n",
    "\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "try:\n",
    "    llama_model, llama_tokenizer = load_model_and_tokenizer(\n",
    "        LLAMA_MODEL_NAME,\n",
    "        use_quantization=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ éŒ¯èª¤: {e}\")\n",
    "    print(\"\\nğŸ’¡ æç¤º: å¦‚æœæ²’æœ‰ Llama-2 è¨ªå•æ¬Šé™,è«‹:\")\n",
    "    print(\"  1. è¨ªå• https://huggingface.co/meta-llama/Llama-2-7b-hf\")\n",
    "    print(\"  2. ç”³è«‹è¨ªå•æ¬Šé™\")\n",
    "    print(\"  3. ä½¿ç”¨ huggingface-cli login ç™»å…¥\")\n",
    "    llama_model = None\n",
    "    llama_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è¼‰ Qwen-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_MODEL_NAME = \"Qwen/Qwen-7B\"\n",
    "\n",
    "try:\n",
    "    qwen_model, qwen_tokenizer = load_model_and_tokenizer(\n",
    "        QWEN_MODEL_NAME,\n",
    "        use_quantization=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ éŒ¯èª¤: {e}\")\n",
    "    print(\"\\nğŸ’¡ æç¤º: è«‹ç¢ºèªç¶²è·¯é€£æ¥æ­£å¸¸\")\n",
    "    qwen_model = None\n",
    "    qwen_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¸¬è©¦æ¨¡å‹æ¨ç†\n",
    "\n",
    "åœ¨æ­£å¼è©•ä¼°å‰,è®“æˆ‘å€‘æ¸¬è©¦æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸ç”Ÿæˆå›ç­”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_inference(model, tokenizer, prompt: str, max_length: int = 100):\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦æ¨¡å‹æ¨ç†åŠŸèƒ½\n",
    "    \n",
    "    Args:\n",
    "        model: æ¨¡å‹\n",
    "        tokenizer: Tokenizer\n",
    "        prompt: è¼¸å…¥æç¤º\n",
    "        max_length: æœ€å¤§ç”Ÿæˆé•·åº¦\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"âš ï¸ æ¨¡å‹æœªåŠ è¼‰,è·³éæ¸¬è©¦\")\n",
    "        return\n",
    "    \n",
    "    print(f\"æç¤º: {prompt}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ç·¨ç¢¼è¼¸å…¥\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # ç”Ÿæˆå›ç­”\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # è§£ç¢¼è¼¸å‡º\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"å›ç­”: {response}\")\n",
    "    print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦æç¤º\n",
    "test_prompt = \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?\"\n",
    "\n",
    "print(\"\\nğŸ§ª æ¸¬è©¦ Llama-2-7B\")\n",
    "test_model_inference(llama_model, llama_tokenizer, test_prompt)\n",
    "\n",
    "print(\"\\nğŸ§ª æ¸¬è©¦ Qwen-7B\")\n",
    "test_model_inference(qwen_model, qwen_tokenizer, test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æº–å‚™è©•ä¼°é…ç½®\n",
    "\n",
    "å»ºç«‹è©•ä¼°æ‰€éœ€çš„é…ç½®æª”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# è©•ä¼°é…ç½®\n",
    "eval_config = {\n",
    "    \"models\": {\n",
    "        \"llama-2-7b\": {\n",
    "            \"name\": LLAMA_MODEL_NAME,\n",
    "            \"loaded\": llama_model is not None\n",
    "        },\n",
    "        \"qwen-7b\": {\n",
    "            \"name\": QWEN_MODEL_NAME,\n",
    "            \"loaded\": qwen_model is not None\n",
    "        }\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"ceval\": {\n",
    "            \"subjects\": list(ceval_data.keys()),\n",
    "            \"total_samples\": sum(len(d) for d in ceval_data.values())\n",
    "        }\n",
    "    },\n",
    "    \"eval_params\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¿å­˜é…ç½®\n",
    "config_path = DATA_DIR / \"eval_config.json\"\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… è©•ä¼°é…ç½®å·²ä¿å­˜\")\n",
    "print(f\"è·¯å¾‘: {config_path}\")\n",
    "print(\"\\né…ç½®å…§å®¹:\")\n",
    "print(json.dumps(eval_config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç’°å¢ƒé©—è­‰ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ç’°å¢ƒé©—è­‰ç¸½çµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = [\n",
    "    (\"CUDA å¯ç”¨\", torch.cuda.is_available()),\n",
    "    (\"C-Eval æ•¸æ“šåŠ è¼‰\", len(ceval_data) > 0),\n",
    "    (\"Llama-2-7B åŠ è¼‰\", llama_model is not None),\n",
    "    (\"Qwen-7B åŠ è¼‰\", qwen_model is not None),\n",
    "    (\"è©•ä¼°é…ç½®å‰µå»º\", config_path.exists())\n",
    "]\n",
    "\n",
    "for check_name, status in checks:\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"{status_icon} {check_name}\")\n",
    "\n",
    "all_passed = all(status for _, status in checks[1:])  # è·³é CUDA (å¯é¸)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_passed:\n",
    "    print(\"âœ… æ‰€æœ‰æª¢æŸ¥é€šé! å¯ä»¥ç¹¼çºŒé€²è¡Œè©•ä¼° (Notebook 02)\")\n",
    "else:\n",
    "    print(\"âš ï¸ éƒ¨åˆ†æª¢æŸ¥æœªé€šé,è«‹è§£æ±ºä¸Šè¿°å•é¡Œå¾Œå†ç¹¼çºŒ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ç¸½çµ\n",
    "\n",
    "åœ¨æœ¬ notebook ä¸­,æˆ‘å€‘å®Œæˆäº†:\n",
    "\n",
    "1. âœ… ç’°å¢ƒæª¢æŸ¥ (CUDA, PyTorch)\n",
    "2. âœ… å®‰è£ OpenCompass è©•ä¼°æ¡†æ¶\n",
    "3. âœ… æº–å‚™ C-Eval è©•ä¼°æ•¸æ“šé›†\n",
    "4. âœ… åŠ è¼‰ Llama-2-7B å’Œ Qwen-7B æ¨¡å‹ (4-bit é‡åŒ–)\n",
    "5. âœ… æ¸¬è©¦æ¨¡å‹æ¨ç†åŠŸèƒ½\n",
    "6. âœ… å‰µå»ºè©•ä¼°é…ç½®æ–‡ä»¶\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ **02-Evaluate.ipynb** åŸ·è¡Œå¯¦éš›è©•ä¼°æµç¨‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**è¨˜å¾—**:\n",
    "- å¦‚æœé‡åˆ° OOM (Out of Memory) éŒ¯èª¤,å¯ä»¥æ¸›å° batch_size\n",
    "- C-Eval å®Œæ•´è©•ä¼°éœ€è¦ 2-3 å°æ™‚,å»ºè­°å…ˆç”¨å­é›†æ¸¬è©¦\n",
    "- ä¿å­˜é‡è¦çš„ä¸­é–“çµæœ,é¿å…éœ€è¦é‡æ–°é‹è¡Œ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
