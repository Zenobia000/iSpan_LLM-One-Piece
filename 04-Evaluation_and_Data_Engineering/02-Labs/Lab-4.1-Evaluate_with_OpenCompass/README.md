# Lab 4.1: OpenCompass æ¨¡å‹è©•ä¼° - å…¨é¢çš„èƒ½åŠ›åŸºæº–æ¸¬è©¦

## æ¦‚è¿°

**OpenCompass** æ˜¯ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å¯¦é©—å®¤é–‹ç™¼çš„é–‹æºå¤§æ¨¡å‹è©•æ¸¬å¹³å°ï¼Œç‚ºå¤§å‹èªè¨€æ¨¡å‹æä¾›å…¨é¢ã€å®¢è§€ã€é–‹æ”¾çš„è©•ä¼°é«”ç³»ã€‚å®ƒæ”¯æŒè¶…é 100 å€‹æ•¸æ“šé›†ã€80+ æ¨¡å‹çš„è©•ä¼°ï¼Œæ¶µè“‹èªè¨€ç†è§£ã€æ¨ç†ã€ä»£ç¢¼ã€çŸ¥è­˜ç­‰å¤šå€‹ç¶­åº¦ã€‚

æœ¬å¯¦é©—å°‡å¸¶ä½ æŒæ¡ä½¿ç”¨ OpenCompass è©•ä¼° LLM çš„å®Œæ•´æµç¨‹ï¼Œå¾ç’°å¢ƒé…ç½®ã€åŸ·è¡Œè©•ä¼°ã€çµæœåˆ†æåˆ°è‡ªå‹•åŒ–å ±å‘Šç”Ÿæˆã€‚ä½ å°‡å­¸æœƒå¦‚ä½•å®¢è§€åœ°è©•ä¼°æ¨¡å‹èƒ½åŠ›ï¼Œè­˜åˆ¥æ¨¡å‹å„ªåŠ£å‹¢ï¼Œä¸¦ç‚ºæ¨¡å‹é¸å‹èˆ‡å„ªåŒ–æä¾›æ•¸æ“šæ”¯æŒã€‚

![OpenCompass Architecture](https://github.com/open-compass/opencompass/raw/main/docs/zh_cn/_static/image/compass_overview.png)

---

## 1. æŠ€è¡“èƒŒæ™¯èˆ‡å‹•æ©Ÿ

### 1.1 ç‚ºä½•éœ€è¦æ¨¡å‹è©•ä¼°ï¼Ÿ

åœ¨ LLM å·¥ç¨‹åŒ–éç¨‹ä¸­ï¼Œè©•ä¼°æ˜¯ä¸å¯æˆ–ç¼ºçš„ç’°ç¯€ï¼š

- **æ¨¡å‹é¸å‹**: åœ¨çœ¾å¤šé–‹æºæ¨¡å‹ä¸­é¸æ“‡æœ€é©åˆæ¥­å‹™å ´æ™¯çš„æ¨¡å‹
- **è¨“ç·´é©—è­‰**: é©—è­‰å¾®èª¿ã€å£“ç¸®ã€å°é½Šç­‰æŠ€è¡“æ˜¯å¦æœ‰æ•ˆ
- **èƒ½åŠ›è¨ºæ–·**: è­˜åˆ¥æ¨¡å‹åœ¨å“ªäº›é ˜åŸŸè¡¨ç¾å„ªç§€ï¼Œå“ªäº›é ˜åŸŸéœ€è¦æ”¹é€²
- **æ€§èƒ½è¿½è¹¤**: ç›£æ§æ¨¡å‹è¿­ä»£éç¨‹ä¸­çš„èƒ½åŠ›è®ŠåŒ–
- **å°å¤–æºé€š**: å‘åˆ©ç›Šç›¸é—œè€…å±•ç¤ºæ¨¡å‹èƒ½åŠ›çš„å®¢è§€è­‰æ“š

### 1.2 è©•ä¼°çš„æŒ‘æˆ°

**ä¸»è§€æ€§å•é¡Œ**:
- é–‹æ”¾å¼ç”Ÿæˆä»»å‹™é›£ä»¥é‡åŒ–ï¼ˆå‰µæ„å¯«ä½œã€å°è©±è³ªé‡ï¼‰
- ä¸åŒè©•ä¼°è€…æ¨™æº–ä¸ä¸€è‡´

**è¦†è“‹åº¦å•é¡Œ**:
- å–®ä¸€åŸºæº–ç„¡æ³•å…¨é¢è©•ä¼°æ¨¡å‹èƒ½åŠ›
- éœ€è¦å¤šç¶­åº¦ã€å¤šé ˜åŸŸçš„è©•ä¼°é«”ç³»

**å¯é‡ç¾æ€§å•é¡Œ**:
- è©•ä¼°å”è­°ä¸çµ±ä¸€ï¼ˆFew-shot æ•¸é‡ã€æº«åº¦è¨­å®šï¼‰
- éš¨æ©Ÿæ€§å°è‡´çµæœæ³¢å‹•

**æˆæœ¬å•é¡Œ**:
- å¤§è¦æ¨¡è©•ä¼°è€—æ™‚é•·ã€ç®—åŠ›æˆæœ¬é«˜
- äººå·¥è©•ä¼°æˆæœ¬æ›´é«˜

### 1.3 OpenCompass çš„è§£æ±ºæ–¹æ¡ˆ

OpenCompass é€šéä»¥ä¸‹æ–¹å¼è§£æ±ºè©•ä¼°æŒ‘æˆ°ï¼š

| ç‰¹æ€§ | èªªæ˜ |
|:---|:---|
| **å…¨é¢æ€§** | æ”¯æŒ 100+ æ•¸æ“šé›†ï¼Œæ¶µè“‹èªè¨€ç†è§£ã€æ¨ç†ã€çŸ¥è­˜ã€ä»£ç¢¼ç­‰ |
| **å®¢è§€æ€§** | æ¨™æº–åŒ–è©•ä¼°å”è­°ï¼Œè‡ªå‹•åŒ–è©•åˆ†ç³»çµ± |
| **é–‹æ”¾æ€§** | é–‹æºæ¡†æ¶ï¼Œæ”¯æŒè‡ªå®šç¾©æ•¸æ“šé›†èˆ‡è©•ä¼°æŒ‡æ¨™ |
| **é«˜æ•ˆæ€§** | åˆ†ä½ˆå¼è©•ä¼°ï¼Œæ”¯æŒå¤š GPU ä¸¦è¡Œ |
| **å¯é‡ç¾** | å›ºå®šè©•ä¼°é…ç½®ï¼Œç¢ºä¿çµæœä¸€è‡´æ€§ |

---

## 2. OpenCompass æ ¸å¿ƒåŸç†

### 2.1 è©•ä¼°ç¯„å¼

OpenCompass æ”¯æŒå¤šç¨®è©•ä¼°ç¯„å¼ï¼š

#### åˆ¤åˆ¥å¼è©•ä¼° (Discriminative Evaluation)
- **é©ç”¨ä»»å‹™**: å¤šé¸é¡Œã€åˆ†é¡ä»»å‹™
- **è©•ä¼°æ–¹å¼**: æ¯”è¼ƒå„é¸é …çš„å°æ•¸ä¼¼ç„¶ï¼Œé¸æ“‡æœ€é«˜çš„
- **å„ªå‹¢**: æº–ç¢ºã€é«˜æ•ˆã€ç„¡éœ€ç”Ÿæˆ
- **ç¤ºä¾‹**: C-Eval, MMLU, HellaSwag

```python
# åˆ¤åˆ¥å¼è©•ä¼°ç¤ºä¾‹
question = "åœ°çƒçš„è¡›æ˜Ÿæ˜¯ï¼ŸA. ç«æ˜Ÿ B. æœˆçƒ C. å¤ªé™½ D. æœ¨æ˜Ÿ"
options = ["A", "B", "C", "D"]

# è¨ˆç®—æ¯å€‹é¸é …çš„å°æ•¸ä¼¼ç„¶
logits = model.get_logits(question, options)
prediction = options[logits.argmax()]  # "B"
```

#### ç”Ÿæˆå¼è©•ä¼° (Generative Evaluation)
- **é©ç”¨ä»»å‹™**: é–‹æ”¾å¼å•ç­”ã€ä»£ç¢¼ç”Ÿæˆã€æ‘˜è¦
- **è©•ä¼°æ–¹å¼**: ç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼Œèˆ‡åƒè€ƒç­”æ¡ˆæ¯”å°
- **å„ªå‹¢**: æ›´æ¥è¿‘å¯¦éš›ä½¿ç”¨å ´æ™¯
- **ç¤ºä¾‹**: GSM8K, HumanEval, TriviaQA

```python
# ç”Ÿæˆå¼è©•ä¼°ç¤ºä¾‹
question = "1+1=?"
generated = model.generate(question)  # "2"

# èˆ‡æ¨™æº–ç­”æ¡ˆæ¯”å°
is_correct = (generated == ground_truth)
```

### 2.2 è©•ä¼°æµç¨‹

OpenCompass çš„è©•ä¼°æµç¨‹åˆ†ç‚ºå››å€‹éšæ®µï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. é…ç½®éšæ®µ â”‚  å®šç¾©æ¨¡å‹ã€æ•¸æ“šé›†ã€è©•ä¼°åƒæ•¸
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ 2. æ¨ç†éšæ®µ â”‚  æ¨¡å‹å°æ•¸æ“šé›†é€²è¡Œé æ¸¬
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ 3. è©•åˆ†éšæ®µ â”‚  æ ¹æ“šé æ¸¬çµæœè¨ˆç®—æŒ‡æ¨™
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ 4. å ±å‘Šéšæ®µ â”‚  ç”Ÿæˆå¯è¦–åŒ–å ±å‘Šèˆ‡åˆ†æ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 æ ¸å¿ƒçµ„ä»¶

**æ•¸æ“šé›†æŠ½è±¡ (Dataset)**:
```python
from opencompass.datasets import CEvalDataset

dataset = CEvalDataset(
    path='ceval-exam',
    name='computer_network',
    split='val'
)
```

**æ¨¡å‹æŠ½è±¡ (Model)**:
```python
from opencompass.models import HuggingFaceCausalLM

model = HuggingFaceCausalLM(
    path='Qwen/Qwen-7B',
    tokenizer_path='Qwen/Qwen-7B',
    max_seq_len=2048,
    batch_size=16
)
```

**è©•ä¼°å™¨ (Evaluator)**:
```python
from opencompass.evaluators import AccuracyEvaluator

evaluator = AccuracyEvaluator()
accuracy = evaluator.score(predictions, references)
```

---

## 3. å¯¦ç¾åŸç†èˆ‡æ­¥é©Ÿ

### 3.1 è©•ä¼°é…ç½®æ–‡ä»¶

OpenCompass ä½¿ç”¨ Python é…ç½®æ–‡ä»¶å®šç¾©è©•ä¼°ä»»å‹™ï¼š

```python
# configs/eval_llama_ceval.py
from opencompass.models import HuggingFaceCausalLM
from opencompass.datasets import CEvalDataset
from opencompass.evaluators import AccuracyEvaluator

# å®šç¾©æ¨¡å‹
models = [
    dict(
        type=HuggingFaceCausalLM,
        abbr='llama-2-7b',
        path='meta-llama/Llama-2-7b-hf',
        tokenizer_path='meta-llama/Llama-2-7b-hf',
        max_seq_len=2048,
        batch_size=16,
        run_cfg=dict(num_gpus=1)
    )
]

# å®šç¾©æ•¸æ“šé›†
datasets = [
    dict(
        type=CEvalDataset,
        path='ceval-exam',
        name='computer_network',
        abbr='ceval-computer_network'
    ),
    dict(
        type=CEvalDataset,
        path='ceval-exam',
        name='operating_system',
        abbr='ceval-operating_system'
    )
]

# è©•ä¼°é…ç½®
work_dir = './outputs/llama_ceval'
```

### 3.2 é—œéµåƒæ•¸èªªæ˜

| åƒæ•¸åç¨± | å«ç¾© | æ¨è–¦å€¼ | å½±éŸ¿ |
|:---|:---|:---|:---|
| `max_seq_len` | æœ€å¤§åºåˆ—é•·åº¦ | 2048/4096 | é•·æ–‡æœ¬è™•ç†èƒ½åŠ› |
| `batch_size` | æ‰¹æ¬¡å¤§å° | 8/16/32 | è©•ä¼°é€Ÿåº¦ vs è¨˜æ†¶é«” |
| `num_gpus` | GPU æ•¸é‡ | 1/2/4 | ä¸¦è¡ŒåŠ é€Ÿ |
| `num_fewshot` | Few-shot æ¨£æœ¬æ•¸ | 0/5 | è©•ä¼°é›£åº¦ |
| `temperature` | ç”Ÿæˆæº«åº¦ | 0.0 | è©•ä¼°æ™‚ä½¿ç”¨è²ªå©ªè§£ç¢¼ |
| `max_out_len` | æœ€å¤§è¼¸å‡ºé•·åº¦ | 512/1024 | ç”Ÿæˆä»»å‹™é™åˆ¶ |

### 3.3 è©•ä¼°åŸ·è¡Œæµç¨‹

#### Step 1: ç’°å¢ƒæº–å‚™
```bash
# å®‰è£ OpenCompass
git clone https://github.com/open-compass/opencompass
cd opencompass
pip install -e .

# ä¸‹è¼‰æ•¸æ“šé›†
python tools/download_dataset.py --dataset ceval
```

#### Step 2: åŸ·è¡Œè©•ä¼°
```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶åŸ·è¡Œ
python run.py configs/eval_llama_ceval.py

# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸
python run.py \
    --models hf_llama_7b \
    --datasets ceval_gen \
    --work-dir ./outputs
```

#### Step 3: ç›£æ§é€²åº¦
```bash
# æŸ¥çœ‹è©•ä¼°é€²åº¦
tail -f outputs/logs/infer_*.log

# æŸ¥çœ‹ä¸­é–“çµæœ
ls outputs/predictions/
```

#### Step 4: æŸ¥çœ‹çµæœ
```bash
# æŸ¥çœ‹åŒ¯ç¸½çµæœ
cat outputs/summary/summary_*.txt

# æˆ–ä½¿ç”¨ Python API
from opencompass.utils import read_results
results = read_results('outputs/summary/summary_*.csv')
```

---

## 4. æ€§èƒ½è¡¨ç¾èˆ‡å°æ¯”

### 4.1 ä¸»æµæ¨¡å‹åœ¨ä¸­æ–‡åŸºæº–ä¸Šçš„è¡¨ç¾

#### C-Eval (Chinese Evaluation Suite)

| æ¨¡å‹ | æ•´é«” | STEM | ç¤¾æœƒç§‘å­¸ | äººæ–‡å­¸ç§‘ | å…¶ä»– |
|:---|---:|---:|---:|---:|---:|
| **GPT-4** | 68.7% | 67.1% | 77.6% | 64.5% | 67.8% |
| **Qwen-14B** | 72.1% | 70.2% | 81.8% | 67.1% | 68.9% |
| **Qwen-7B** | 59.7% | 56.2% | 74.1% | 63.1% | 56.2% |
| **ChatGLM3-6B** | 51.7% | 48.9% | 60.3% | 55.2% | 49.8% |
| **Llama-2-7B** | 45.3% | 42.1% | 52.9% | 48.7% | 44.9% |
| **Llama-2-13B** | 50.8% | 47.5% | 59.1% | 54.3% | 49.2% |

**é—œéµè§€å¯Ÿ**:
- Qwen ç³»åˆ—åœ¨ä¸­æ–‡ä»»å‹™ä¸Šå…·æœ‰é¡¯è‘—å„ªå‹¢ï¼ˆæ›´å¤šä¸­æ–‡é è¨“ç·´æ•¸æ“šï¼‰
- Llama-2 åœ¨ä¸­æ–‡åŸºæº–ä¸Šè¡¨ç¾è¼ƒå¼±ï¼ˆä¸»è¦æ˜¯è‹±æ–‡é è¨“ç·´ï¼‰
- ç¤¾æœƒç§‘å­¸é¡åˆ¥å¾—åˆ†æ™®éé«˜æ–¼ STEMï¼ˆå¯èƒ½å› ç‚º STEM éœ€è¦æ›´å°ˆæ¥­çš„çŸ¥è­˜ï¼‰

#### CMMLU (Chinese Massive Multitask Language Understanding)

| æ¨¡å‹ | æ•´é«” | é†«å­¸ | æ³•å¾‹ | æ­·å² | å·¥ç¨‹ |
|:---|---:|---:|---:|---:|---:|
| **GPT-4** | 71.0% | 69.2% | 73.8% | 72.4% | 68.5% |
| **Qwen-14B** | 70.2% | 68.1% | 72.6% | 71.3% | 67.9% |
| **Qwen-7B** | 58.2% | 55.7% | 60.9% | 59.8% | 56.3% |
| **ChatGLM3-6B** | 50.3% | 48.2% | 52.1% | 51.6% | 49.7% |
| **Llama-2-7B** | 42.1% | 39.8% | 44.3% | 43.2% | 40.9% |

### 4.2 è‹±æ–‡åŸºæº–è¡¨ç¾

#### MMLU (Massive Multitask Language Understanding)

| æ¨¡å‹ | æ•´é«” | STEM | ç¤¾æœƒç§‘å­¸ | äººæ–‡ | å…¶ä»– |
|:---|---:|---:|---:|---:|---:|
| **GPT-4** | 86.4% | 83.2% | 88.6% | 85.1% | 87.2% |
| **Llama-2-70B** | 69.8% | 67.3% | 71.4% | 69.2% | 70.1% |
| **Llama-2-13B** | 54.8% | 52.1% | 56.3% | 54.2% | 55.7% |
| **Llama-2-7B** | 46.8% | 43.9% | 48.2% | 46.1% | 47.9% |
| **Qwen-7B** | 56.4% | 54.2% | 58.1% | 55.9% | 57.3% |

**é—œéµè§€å¯Ÿ**:
- Llama-2 åœ¨è‹±æ–‡åŸºæº–ä¸Šè¡¨ç¾æ›´å¥½ï¼ˆè‹±æ–‡é è¨“ç·´ç‚ºä¸»ï¼‰
- æ¨¡å‹è¦æ¨¡å°æ€§èƒ½æœ‰é¡¯è‘—å½±éŸ¿ï¼ˆ7B â†’ 13B â†’ 70B éå¢ï¼‰
- GPT-4 ä»æ˜¯çµ•å°é ˜å…ˆè€…

### 4.3 æ¨ç†èˆ‡ä»£ç¢¼èƒ½åŠ›

| æ¨¡å‹ | GSM8K (æ•¸å­¸) | HumanEval (ä»£ç¢¼) | BBH (æ¨ç†) |
|:---|---:|---:|---:|
| **GPT-4** | 92.0% | 67.0% | 86.7% |
| **Qwen-14B** | 61.3% | 43.9% | 67.8% |
| **ChatGLM3-6B** | 72.3% | 58.0% | 54.2% |
| **Llama-2-7B** | 14.6% | 12.8% | 38.9% |

**é—œéµè§€å¯Ÿ**:
- æ•¸å­¸æ¨ç†èˆ‡ä»£ç¢¼ç”Ÿæˆéœ€è¦å¼·å¤§çš„é‚è¼¯èƒ½åŠ›
- å°ˆé–€å„ªåŒ–éçš„æ¨¡å‹ï¼ˆChatGLM3ï¼‰åœ¨ç‰¹å®šä»»å‹™ä¸Šå¯è¶…è¶Šæ›´å¤§çš„æ¨¡å‹

---

## 5. æŠ€è¡“å„ªå‹¢

### 5.1 OpenCompass æ ¸å¿ƒå„ªå‹¢

| å„ªå‹¢é …ç›® | èªªæ˜ |
|:---|:---|
| **å…¨é¢è¦†è“‹** | æ”¯æŒ 100+ æ•¸æ“šé›†ï¼Œæ¶µè“‹èªè¨€ã€æ¨ç†ã€çŸ¥è­˜ã€ä»£ç¢¼ã€å¤šæ¨¡æ…‹ |
| **æ¨™æº–åŒ–è©•ä¼°** | çµ±ä¸€è©•ä¼°å”è­°ï¼Œç¢ºä¿ä¸åŒæ¨¡å‹çµæœå¯æ¯”è¼ƒ |
| **é«˜æ•ˆä¸¦è¡Œ** | æ”¯æŒå¤š GPU åˆ†ä½ˆå¼è©•ä¼°ï¼Œå¤§å¹…ç¸®çŸ­è©•ä¼°æ™‚é–“ |
| **éˆæ´»æ“´å±•** | æ˜“æ–¼æ·»åŠ è‡ªå®šç¾©æ•¸æ“šé›†ã€æ¨¡å‹ã€è©•ä¼°æŒ‡æ¨™ |
| **è±å¯Œå¯è¦–åŒ–** | è‡ªå‹•ç”Ÿæˆé›·é”åœ–ã€ç†±åŠ›åœ–ã€å°æ¯”è¡¨æ ¼ |
| **é–‹æºç¤¾å€** | æ´»èºçš„é–‹æºç¤¾å€ï¼ŒæŒçºŒæ›´æ–°åŸºæº–èˆ‡æ¨¡å‹ |

### 5.2 vs å…¶ä»–è©•ä¼°æ¡†æ¶

| ç‰¹æ€§ | OpenCompass | LM-Eval-Harness | EleutherAI Eval |
|:---|:---:|:---:|:---:|
| **ä¸­æ–‡åŸºæº–** | âœ… è±å¯Œ | âš ï¸ æœ‰é™ | âŒ ç¼ºä¹ |
| **æ˜“ç”¨æ€§** | âœ… é…ç½®ç°¡å–® | âœ… å‘½ä»¤è¡Œå‹å¥½ | âš ï¸ è¼ƒè¤‡é›œ |
| **å¯è¦–åŒ–** | âœ… å…§å»º | âš ï¸ åŸºç¤ | âŒ ç„¡ |
| **åˆ†ä½ˆå¼** | âœ… æ”¯æŒ | âš ï¸ æœ‰é™ | âš ï¸ æœ‰é™ |
| **ç¤¾å€æ´»èºåº¦** | âœ… é«˜ | âœ… é«˜ | âš ï¸ ä¸­ç­‰ |

**ä½¿ç”¨å»ºè­°**:
- **ä¸­æ–‡æ¨¡å‹è©•ä¼°**: å„ªå…ˆé¸æ“‡ OpenCompass
- **è‹±æ–‡æ¨¡å‹è©•ä¼°**: OpenCompass æˆ– LM-Eval-Harness
- **ç ”ç©¶ç”¨é€”**: OpenCompassï¼ˆæ›´å…¨é¢çš„åŸºæº–ï¼‰
- **å¿«é€Ÿé©—è­‰**: LM-Eval-Harnessï¼ˆæ›´è¼•é‡ï¼‰

---

## 6. å¯¦é©—è¨­è¨ˆèˆ‡å¯¦ä½œ

### 6.1 å¯¦é©—ç’°å¢ƒ

- **è©•ä¼°æ¡†æ¶**: OpenCompass 0.2.3+
- **æ¨¡å‹**:
  - `meta-llama/Llama-2-7b-hf` (7B åƒæ•¸)
  - `Qwen/Qwen-7B` (7B åƒæ•¸)
- **è©•ä¼°åŸºæº–**:
  - C-Eval (ä¸­æ–‡ç¶œåˆè©•ä¼°)
  - CMMLU (ä¸­æ–‡å¤šä»»å‹™ç†è§£)
  - MMLU (è‹±æ–‡å¤šä»»å‹™ç†è§£)
- **ç¡¬é«”**: NVIDIA GPU (16GB+ VRAM æ¨è–¦)

### 6.2 å¯¦é©—æµç¨‹

#### 1. **ç’°å¢ƒæº–å‚™** (`01-Setup.ipynb`)
**ç›®æ¨™**: é…ç½® OpenCompass è©•ä¼°ç’°å¢ƒ

**æ­¥é©Ÿ**:
- å®‰è£ OpenCompass æ¡†æ¶èˆ‡ä¾è³´
- ä¸‹è¼‰è©•ä¼°æ•¸æ“šé›†ï¼ˆC-Eval, CMMLU å­é›†ï¼‰
- é©—è­‰ GPU ç’°å¢ƒ
- è¼‰å…¥å¾…è©•ä¼°æ¨¡å‹
- åŸ·è¡Œç°¡å–®æ¸¬è©¦ç¢ºä¿ç’°å¢ƒæ­£å¸¸

**é æœŸè¼¸å‡º**:
```
âœ… OpenCompass installed successfully
âœ… C-Eval dataset downloaded (52 subjects)
âœ… GPU detected: NVIDIA A100 (40GB)
âœ… Llama-2-7B loaded successfully
âœ… Qwen-7B loaded successfully
```

#### 2. **åŸ·è¡Œè©•ä¼°** (`02-Evaluate.ipynb`)
**ç›®æ¨™**: åœ¨å¤šå€‹åŸºæº–ä¸Šè©•ä¼°å…©å€‹æ¨¡å‹

**æ­¥é©Ÿ**:
- é…ç½®è©•ä¼°ä»»å‹™ï¼ˆé¸æ“‡æ•¸æ“šé›†ã€Few-shot è¨­å®šï¼‰
- åŸ·è¡Œ C-Eval è©•ä¼°ï¼ˆSTEMã€ç¤¾æœƒç§‘å­¸ã€äººæ–‡å­¸ç§‘å­é›†ï¼‰
- åŸ·è¡Œ CMMLU è©•ä¼°ï¼ˆé†«å­¸ã€æ³•å¾‹ã€æ­·å²å­é›†ï¼‰
- æ”¶é›†è©•ä¼°æ—¥èªŒèˆ‡ä¸­é–“çµæœ
- ä¿å­˜é æ¸¬çµæœèˆ‡è©•åˆ†

**è©•ä¼°é…ç½®**:
```python
eval_config = {
    'models': ['Llama-2-7B', 'Qwen-7B'],
    'datasets': [
        'ceval-computer_network',
        'ceval-operating_system',
        'ceval-computer_architecture',
        'cmmlu-anatomy',
        'cmmlu-clinical_knowledge'
    ],
    'num_fewshot': 5,
    'batch_size': 16,
    'max_seq_len': 2048
}
```

**é æœŸè¼¸å‡º**:
```
Evaluating Llama-2-7B on C-Eval:
  computer_network: 45.2% (113/250 correct)
  operating_system: 42.8% (107/250 correct)
  ...

Evaluating Qwen-7B on C-Eval:
  computer_network: 58.4% (146/250 correct)
  operating_system: 57.2% (143/250 correct)
  ...
```

#### 3. **çµæœåˆ†æ** (`03-Analyze.ipynb`)
**ç›®æ¨™**: æ·±åº¦åˆ†æè©•ä¼°çµæœ

**åˆ†æç¶­åº¦**:
- **æ•´é«”æ€§èƒ½**: å¹³å‡æº–ç¢ºç‡ã€å­¸ç§‘åˆ†ä½ˆ
- **å°æ¯”åˆ†æ**: Llama-2-7B vs Qwen-7B å·®ç•°
- **å­¸ç§‘ç´°åˆ†**: å“ªäº›å­¸ç§‘è¡¨ç¾å¥½/å·®
- **éŒ¯èª¤æ¡ˆä¾‹**: åˆ†æå…¸å‹éŒ¯èª¤æ¨¡å¼

**é—œéµæŒ‡æ¨™**:
```python
metrics = {
    'accuracy': æº–ç¢ºç‡,
    'f1_score': F1 åˆ†æ•¸ï¼ˆå¤šåˆ†é¡ï¼‰,
    'score_by_category': æŒ‰é¡åˆ¥çµ±è¨ˆ,
    'error_analysis': éŒ¯èª¤é¡å‹åˆ†é¡
}
```

**é æœŸè¼¸å‡º**:
```
ğŸ“Š Overall Performance:
  Llama-2-7B: 45.3% avg accuracy
  Qwen-7B: 59.7% avg accuracy
  Î”: +14.4% (Qwen wins)

ğŸ“Š By Category:
  STEM:
    Llama-2-7B: 42.1%
    Qwen-7B: 56.2% (+14.1%)

  Social Science:
    Llama-2-7B: 48.7%
    Qwen-7B: 62.4% (+13.7%)

âŒ Common Errors (Llama-2-7B):
  - Factual errors: 38%
  - Reasoning errors: 29%
  - Language understanding: 33%
```

#### 4. **å¯è¦–åŒ–èˆ‡å ±å‘Š** (`04-Visualize_and_Report.ipynb`)
**ç›®æ¨™**: ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨èˆ‡è‡ªå‹•åŒ–å ±å‘Š

**å¯è¦–åŒ–é¡å‹**:
- **é›·é”åœ–**: å¤šç¶­åº¦èƒ½åŠ›å°æ¯”
- **ç†±åŠ›åœ–**: å­¸ç§‘è¡¨ç¾çŸ©é™£
- **æŸ±ç‹€åœ–**: æ•´é«”æ€§èƒ½å°æ¯”
- **åˆ†ä½ˆåœ–**: åˆ†æ•¸åˆ†ä½ˆç›´æ–¹åœ–

**è‡ªå‹•å ±å‘Šå…§å®¹**:
```markdown
# OpenCompass Evaluation Report

## Executive Summary
- Qwen-7B outperforms Llama-2-7B by 14.4% on C-Eval
- Largest gap in social science subjects (+13.7%)
- Both models struggle with advanced STEM topics

## Recommendations
1. For Chinese tasks: Use Qwen-7B
2. For English tasks: Consider Llama-2-7B
3. For specialized domains: Fine-tune on domain data
```

---

## 7. å¯¦æˆ°åƒæ•¸èª¿å„ªç­–ç•¥ (2024 å¹´è¡Œæ¥­æœ€ä½³å¯¦è¸)

### 7.1 åŸºæ–¼è©•ä¼°å ´æ™¯çš„é…ç½®

#### å¿«é€Ÿé©—è­‰å ´æ™¯
**ç›®æ¨™**: å¿«é€Ÿé©—è­‰æ¨¡å‹åŸºæœ¬èƒ½åŠ›ï¼ˆ30 åˆ†é˜å…§ï¼‰

```python
quick_eval_config = {
    'num_fewshot': 0,           # Zero-shot æœ€å¿«
    'batch_size': 32,           # å¤§æ‰¹æ¬¡æé€Ÿ
    'max_seq_len': 1024,        # è¼ƒçŸ­åºåˆ—
    'datasets': [               # é¸æ“‡ä»£è¡¨æ€§å­é›†
        'ceval-computer_network',  # STEM ä»£è¡¨
        'ceval-chinese_language',  # äººæ–‡ä»£è¡¨
        'ceval-marxism'           # ç¤¾æœƒç§‘å­¸ä»£è¡¨
    ],
    'num_samples': 100          # æ¯å€‹æ•¸æ“šé›†åƒ…è©•ä¼° 100 æ¨£æœ¬
}
```

**é©ç”¨æ™‚æ©Ÿ**:
- æ¨¡å‹é¸å‹åˆæœŸ
- è¨“ç·´éç¨‹ä¸­çš„æª¢æŸ¥é»é©—è­‰
- CI/CD è‡ªå‹•åŒ–æ¸¬è©¦

#### æ¨™æº–è©•ä¼°å ´æ™¯
**ç›®æ¨™**: å…¨é¢è©•ä¼°æ¨¡å‹èƒ½åŠ›ï¼ˆ2-4 å°æ™‚ï¼‰

```python
standard_eval_config = {
    'num_fewshot': 5,           # 5-shot è©•ä¼°
    'batch_size': 16,           # å¹³è¡¡é€Ÿåº¦èˆ‡è¨˜æ†¶é«”
    'max_seq_len': 2048,        # æ¨™æº–åºåˆ—é•·åº¦
    'datasets': [               # C-Eval å®Œæ•´ 52 å€‹å­¸ç§‘
        'ceval-*'
    ],
    'num_samples': None         # ä½¿ç”¨å®Œæ•´æ•¸æ“šé›†
}
```

**é©ç”¨æ™‚æ©Ÿ**:
- æ¨¡å‹æ­£å¼ç™¼å¸ƒå‰
- ç ”ç©¶è«–æ–‡å¯¦é©—
- å°å¤–å±•ç¤ºçµæœ

#### æ·±åº¦è©•ä¼°å ´æ™¯
**ç›®æ¨™**: æ¥µè‡´æº–ç¢ºçš„è©•ä¼°ï¼ˆ6-12 å°æ™‚ï¼‰

```python
deep_eval_config = {
    'num_fewshot': 5,
    'batch_size': 8,            # å°æ‰¹æ¬¡ç¢ºä¿ç©©å®š
    'max_seq_len': 4096,        # æ”¯æŒé•·æ–‡æœ¬
    'datasets': [
        'ceval-*',              # C-Eval å®Œæ•´
        'cmmlu-*',              # CMMLU å®Œæ•´
        'mmlu-*'                # MMLU å®Œæ•´
    ],
    'num_runs': 3,              # å¤šæ¬¡é‹è¡Œå–å¹³å‡
    'temperature': 0.0,         # ç¢ºå®šæ€§è§£ç¢¼
    'seed': 42                  # å›ºå®šéš¨æ©Ÿç¨®å­
}
```

**é©ç”¨æ™‚æ©Ÿ**:
- å­¸è¡“è«–æ–‡æŠ•ç¨¿
- ç«¶çˆ­æ€§åŸºæº–æ’è¡Œæ¦œ
- é«˜é¢¨éšªæ±ºç­–ï¼ˆå¦‚ç”Ÿç”¢æ¨¡å‹é¸å‹ï¼‰

### 7.2 ä¸åŒæ¨¡å‹è¦æ¨¡çš„å„ªåŒ–ç­–ç•¥

| æ¨¡å‹è¦æ¨¡ | Batch Size | åºåˆ—é•·åº¦ | Few-shot | é è¨ˆæ™‚é–“ |
|:---|---:|---:|---:|:---|
| **å°å‹** (<3B) | 32 | 2048 | 5 | 1-2h |
| **ä¸­å‹** (3-10B) | 16 | 2048 | 5 | 2-4h |
| **å¤§å‹** (10-30B) | 8 | 2048 | 5 | 4-8h |
| **è¶…å¤§å‹** (>30B) | 4 | 2048 | 5 | 8-16h |

### 7.3 ç²¾åº¦ä¿è­‰ç­–ç•¥

#### å›ºå®šæ‰€æœ‰éš¨æ©Ÿæ€§
```python
import random
import numpy as np
import torch

def set_seed(seed=42):
    """ç¢ºä¿è©•ä¼°å¯é‡ç¾"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# åœ¨è©•ä¼°é–‹å§‹å‰èª¿ç”¨
set_seed(42)
```

#### çµ±ä¸€è©•ä¼°å”è­°
```python
# è©•ä¼°å”è­°é…ç½®
evaluation_protocol = {
    'generation': {
        'do_sample': False,      # ä½¿ç”¨è²ªå©ªè§£ç¢¼
        'temperature': 0.0,      # æº«åº¦ç‚º 0
        'top_p': 1.0,           # ä¸ä½¿ç”¨ nucleus sampling
        'top_k': None,          # ä¸ä½¿ç”¨ top-k
        'num_beams': 1,         # ä¸ä½¿ç”¨ beam search
        'repetition_penalty': 1.0
    },
    'fewshot': {
        'num_shots': 5,         # å›ºå®š 5-shot
        'shuffle': False,       # ä¸æ‰“äº‚æ¨£æœ¬é †åº
        'seed': 42              # å›ºå®šé¸æ“‡çš„æ¨£æœ¬
    }
}
```

### 7.4 æ•…éšœè¨ºæ–·æŒ‡å—

| å•é¡Œç¾è±¡ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|:---|:---|:---|
| è©•ä¼°åˆ†æ•¸ç•°å¸¸ä½ (<20%) | æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥ / Few-shot æ ¼å¼éŒ¯èª¤ | æª¢æŸ¥æ¨¡å‹è·¯å¾‘ã€é©—è­‰ Few-shot æ¨¡æ¿ |
| è©•ä¼°é€Ÿåº¦æ¥µæ…¢ (>10h) | Batch size éå° / å–® GPU | å¢å¤§ batch sizeã€å•Ÿç”¨å¤š GPU |
| OOM éŒ¯èª¤ | åºåˆ—é•·åº¦éé•· / Batch size éå¤§ | æ¸›å° max_seq_len æˆ– batch_size |
| çµæœä¸å¯é‡ç¾ | éš¨æ©Ÿç¨®å­æœªå›ºå®š | ä½¿ç”¨ `set_seed()` å›ºå®šæ‰€æœ‰éš¨æ©Ÿæº |
| ç”Ÿæˆå¼è©•ä¼°å¤±æ•— | ç­”æ¡ˆæ ¼å¼ä¸åŒ¹é… | æª¢æŸ¥å¾Œè™•ç†å‡½æ•¸ï¼Œèª¿æ•´æ­£å‰‡è¡¨é”å¼ |
| å¤šé¸é¡Œæº–ç¢ºç‡ç‚º 0 | æ¨¡å‹è¼¸å‡ºæ ¼å¼éŒ¯èª¤ | æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒé¸é …æå– |

**å¸¸è¦‹éŒ¯èª¤æ’æŸ¥æ­¥é©Ÿ**:
1. æª¢æŸ¥æ—¥èªŒæ–‡ä»¶ï¼š`tail -f outputs/logs/infer_*.log`
2. é©—è­‰å–®å€‹æ¨£æœ¬ï¼šæ‰‹å‹•é‹è¡Œä¸€å€‹æ¸¬è©¦æ¨£æœ¬
3. æª¢æŸ¥é æ¸¬æ–‡ä»¶ï¼šæŸ¥çœ‹ `outputs/predictions/*.json`
4. å°æ¯”åƒè€ƒå¯¦ç¾ï¼šèˆ‡å®˜æ–¹é…ç½®æ–‡ä»¶å°æ¯”

---

## 8. è©•ä¼°ç®¡ç·šèˆ‡ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸

### 8.1 è‡ªå‹•åŒ–è©•ä¼° CI/CD

#### GitHub Actions ç¯„ä¾‹
```yaml
# .github/workflows/evaluate.yml
name: Model Evaluation

on:
  push:
    branches: [main]
    paths: ['models/**']

jobs:
  evaluate:
    runs-on: self-hosted  # ä½¿ç”¨è‡ªå»º GPU æ©Ÿå™¨
    steps:
      - uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install opencompass
          pip install -r requirements.txt

      - name: Run evaluation
        run: |
          python run.py configs/eval_model.py \
            --work-dir ./outputs

      - name: Check regression
        run: |
          python scripts/check_regression.py \
            --baseline results/baseline.json \
            --current outputs/summary/summary.json \
            --threshold 0.02  # å…è¨± 2% æ³¢å‹•

      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-results
          path: outputs/
```

### 8.2 è©•ä¼°çµæœè³‡æ–™åº«

```python
import sqlite3
import json
from datetime import datetime

class EvaluationDatabase:
    def __init__(self, db_path='evaluations.db'):
        self.conn = sqlite3.connect(db_path)
        self.create_tables()

    def create_tables(self):
        """å‰µå»ºè©•ä¼°çµæœè¡¨"""
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS evaluations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model_name TEXT NOT NULL,
                model_version TEXT,
                dataset TEXT NOT NULL,
                accuracy REAL,
                f1_score REAL,
                eval_config TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        self.conn.commit()

    def insert_result(self, model_name, dataset, accuracy, f1_score, config):
        """æ’å…¥è©•ä¼°çµæœ"""
        self.conn.execute('''
            INSERT INTO evaluations
            (model_name, dataset, accuracy, f1_score, eval_config)
            VALUES (?, ?, ?, ?, ?)
        ''', (model_name, dataset, accuracy, f1_score, json.dumps(config)))
        self.conn.commit()

    def get_model_history(self, model_name):
        """æŸ¥è©¢æ¨¡å‹æ­·å²è©•ä¼°çµæœ"""
        cursor = self.conn.execute('''
            SELECT dataset, accuracy, f1_score, timestamp
            FROM evaluations
            WHERE model_name = ?
            ORDER BY timestamp DESC
        ''', (model_name,))
        return cursor.fetchall()

    def compare_models(self, model_a, model_b, dataset):
        """æ¯”è¼ƒå…©å€‹æ¨¡å‹åœ¨åŒä¸€æ•¸æ“šé›†ä¸Šçš„è¡¨ç¾"""
        cursor = self.conn.execute('''
            SELECT model_name, AVG(accuracy) as avg_acc
            FROM evaluations
            WHERE model_name IN (?, ?) AND dataset = ?
            GROUP BY model_name
        ''', (model_a, model_b, dataset))
        return dict(cursor.fetchall())
```

### 8.3 å¯¦æ™‚ç›£æ§å„€è¡¨æ¿

```python
import streamlit as st
import pandas as pd
import plotly.express as px

def create_dashboard():
    st.title("ğŸ¯ OpenCompass Evaluation Dashboard")

    # è¼‰å…¥è©•ä¼°çµæœ
    db = EvaluationDatabase()

    # æ¨¡å‹é¸æ“‡
    models = st.multiselect(
        "Select models",
        options=['Llama-2-7B', 'Qwen-7B', 'ChatGLM3-6B']
    )

    # æ•¸æ“šé›†é¸æ“‡
    datasets = st.multiselect(
        "Select datasets",
        options=['C-Eval', 'CMMLU', 'MMLU']
    )

    # æ™‚é–“ç¯„åœ
    date_range = st.date_input("Date range", [])

    # è¼‰å…¥æ•¸æ“š
    df = load_evaluation_data(models, datasets, date_range)

    # å¯è¦–åŒ–
    col1, col2 = st.columns(2)

    with col1:
        # æ•´é«”æº–ç¢ºç‡å°æ¯”
        fig = px.bar(
            df, x='model', y='accuracy',
            color='dataset',
            title='Model Accuracy Comparison'
        )
        st.plotly_chart(fig)

    with col2:
        # æ™‚é–“è¶¨å‹¢
        fig = px.line(
            df, x='timestamp', y='accuracy',
            color='model',
            title='Accuracy Trend Over Time'
        )
        st.plotly_chart(fig)

    # è©³ç´°è¡¨æ ¼
    st.dataframe(df, use_container_width=True)

# é‹è¡Œ
if __name__ == '__main__':
    create_dashboard()
```

**å•Ÿå‹•å„€è¡¨æ¿**:
```bash
streamlit run dashboard.py
```

### 8.4 å‘Šè­¦ç³»çµ±

```python
class RegressionDetector:
    def __init__(self, baseline_results, threshold=0.02):
        """
        Args:
            baseline_results: åŸºç·šè©•ä¼°çµæœ
            threshold: å…è¨±çš„æ€§èƒ½ä¸‹é™é–¾å€¼ï¼ˆ2%ï¼‰
        """
        self.baseline = baseline_results
        self.threshold = threshold

    def check_regression(self, current_results):
        """æª¢æ¸¬æ€§èƒ½å›æ­¸"""
        regressions = []

        for dataset, current_acc in current_results.items():
            baseline_acc = self.baseline.get(dataset, 0)
            diff = current_acc - baseline_acc

            if diff < -self.threshold:
                regressions.append({
                    'dataset': dataset,
                    'baseline': baseline_acc,
                    'current': current_acc,
                    'diff': diff,
                    'severity': 'CRITICAL' if diff < -0.05 else 'WARNING'
                })

        return regressions

    def send_alert(self, regressions):
        """ç™¼é€å‘Šè­¦é€šçŸ¥"""
        if not regressions:
            print("âœ… No regression detected")
            return

        message = "âš ï¸  Performance Regression Detected!\n\n"
        for reg in regressions:
            message += f"{reg['severity']}: {reg['dataset']}\n"
            message += f"  Baseline: {reg['baseline']:.2%}\n"
            message += f"  Current:  {reg['current']:.2%}\n"
            message += f"  Diff:     {reg['diff']:.2%}\n\n"

        # ç™¼é€åˆ° Slack / éƒµä»¶ / ä¼æ¥­å¾®ä¿¡
        self.send_to_slack(message)
        self.send_email(message)

    def send_to_slack(self, message):
        """ç™¼é€åˆ° Slack"""
        import requests
        webhook_url = "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
        requests.post(webhook_url, json={'text': message})
```

---

## 9. çµè«–èˆ‡å­¸ç¿’æˆæœ

é€šéæœ¬å¯¦é©—ï¼Œä½ å°‡ç²å¾—ï¼š

### 9.1 æ ¸å¿ƒæŠ€èƒ½

1. **è©•ä¼°æ¡†æ¶æŒæ¡**
   - âœ… ç†Ÿç·´ä½¿ç”¨ OpenCompass è©•ä¼° LLM
   - âœ… ç†è§£è©•ä¼°é…ç½®èˆ‡åƒæ•¸èª¿å„ª
   - âœ… æŒæ¡åˆ†ä½ˆå¼è©•ä¼°åŠ é€ŸæŠ€å·§

2. **çµæœåˆ†æèƒ½åŠ›**
   - âœ… è§£è®€å¤šç¶­åº¦è©•ä¼°æŒ‡æ¨™
   - âœ… è­˜åˆ¥æ¨¡å‹å„ªå‹¢èˆ‡åŠ£å‹¢é ˜åŸŸ
   - âœ… é€²è¡Œæ¨¡å‹é–“æ©«å‘å°æ¯”
   - âœ… åˆ†æéŒ¯èª¤æ¡ˆä¾‹ä¸¦å®šä½å•é¡Œ

3. **å·¥ç¨‹å¯¦è¸**
   - âœ… å»ºç«‹è‡ªå‹•åŒ–è©•ä¼°ç®¡ç·š
   - âœ… å¯¦ç¾è©•ä¼°çµæœç›£æ§èˆ‡å‘Šè­¦
   - âœ… ç”Ÿæˆå°ˆæ¥­è©•ä¼°å ±å‘Š
   - âœ… æ•´åˆè©•ä¼°åˆ° CI/CD æµç¨‹

4. **æ±ºç­–æ”¯æŒ**
   - âœ… åŸºæ–¼è©•ä¼°çµæœé¸æ“‡åˆé©æ¨¡å‹
   - âœ… ç‚ºæ¨¡å‹å„ªåŒ–æä¾›æ•¸æ“šæ”¯æŒ
   - âœ… é‡åŒ–è¨“ç·´/å£“ç¸®æŠ€è¡“çš„æ•ˆæœ

### 9.2 å¯¦éš›æ‡‰ç”¨å ´æ™¯

**æ¨¡å‹é¸å‹**:
```
å ´æ™¯: é–‹ç™¼ä¸­æ–‡å®¢æœç³»çµ±
æ­¥é©Ÿ:
  1. ä½¿ç”¨ OpenCompass è©•ä¼°å€™é¸æ¨¡å‹ï¼ˆQwen, ChatGLM, Llama-2ï¼‰
  2. é—œæ³¨ CMMLU ç¤¾æœƒç§‘å­¸é¡åˆ¥ï¼ˆå®¢æœç›¸é—œï¼‰
  3. çµæœ: Qwen-7B åœ¨è©²é¡åˆ¥å¾—åˆ† 62.4%ï¼Œé¸å®šç‚ºåŸºç¤æ¨¡å‹
```

**è¨“ç·´é©—è­‰**:
```
å ´æ™¯: é©—è­‰ LoRA å¾®èª¿æ•ˆæœ
æ­¥é©Ÿ:
  1. è©•ä¼°åŸºç¤æ¨¡å‹ï¼šC-Eval 45.3%
  2. å¾®èª¿å¾Œè©•ä¼°ï¼šC-Eval 48.7% (+3.4%)
  3. çµè«–: LoRA å¾®èª¿æœ‰æ•ˆï¼Œå¯é€²å…¥ç”Ÿç”¢
```

**å£“ç¸®è©•ä¼°**:
```
å ´æ™¯: é‡åŒ–æ¨¡å‹è³ªé‡æª¢æŸ¥
æ­¥é©Ÿ:
  1. è©•ä¼°åŸå§‹æ¨¡å‹ï¼šMMLU 56.4%
  2. è©•ä¼° 4-bit GPTQï¼šMMLU 55.1% (-1.3%)
  3. çµè«–: ç²¾åº¦æå¤±å¯æ¥å—ï¼Œå¯éƒ¨ç½²é‡åŒ–ç‰ˆæœ¬
```

---

## 10. æŠ€è¡“é™åˆ¶èˆ‡æ”¹é€²æ–¹å‘

### 10.1 ç•¶å‰é™åˆ¶åˆ†æ

| é™åˆ¶é …ç›® | å…·é«”è¡¨ç¾ | å½±éŸ¿ | ç·©è§£æ–¹æ¡ˆ |
|:---|:---|:---|:---|
| **åŸºæº–è¦†è“‹** | ç¾æœ‰åŸºæº–ç„¡æ³•æ¶µè“‹æ‰€æœ‰èƒ½åŠ›ï¼ˆå‰µæ„ã€æƒ…æ„Ÿï¼‰ | è©•ä¼°ç‰‡é¢ | çµåˆäººå·¥è©•ä¼°ã€GPT-4 è©•åˆ¤ |
| **è©•ä¼°æˆæœ¬** | å¤§è¦æ¨¡è©•ä¼°è€—æ™‚é•·ï¼ˆ8-16hï¼‰ | è¿­ä»£é€Ÿåº¦æ…¢ | ä½¿ç”¨å­é›†å¿«é€Ÿé©—è­‰ã€å¢é‡è©•ä¼° |
| **ä¸­è‹±æ–‡å·®ç•°** | åŒä¸€æ¨¡å‹ä¸­è‹±æ–‡è¡¨ç¾å·®ç•°å¤§ | é›£ä»¥ç¶œåˆè©•åƒ¹ | åˆ†åˆ¥è©•ä¼°ã€åŠ æ¬Šçµ„åˆ |
| **Few-shot æ•æ„Ÿ** | æ¨£æœ¬é¸æ“‡å½±éŸ¿çµæœç©©å®šæ€§ | å¯é‡ç¾æ€§å·® | å›ºå®šæ¨£æœ¬æ± ã€å¤šæ¬¡è©•ä¼°å–å¹³å‡ |
| **é•·æ–‡æœ¬é™åˆ¶** | å¤§å¤šæ•¸åŸºæº–é™åˆ¶åœ¨ 2K tokens å…§ | ç„¡æ³•æ¸¬è©¦é•·æ–‡æœ¬èƒ½åŠ› | ä½¿ç”¨é•·æ–‡æœ¬å°ˆç”¨åŸºæº–ï¼ˆå¦‚ LongBenchï¼‰ |

### 10.2 æœªä¾†ç ”ç©¶æ–¹å‘

#### å‹•æ…‹è©•ä¼°
```python
# æ ¹æ“šæ¨¡å‹èƒ½åŠ›å‹•æ…‹èª¿æ•´è©•ä¼°é›£åº¦
def adaptive_evaluation(model, dataset):
    # å¾ä¸­ç­‰é›£åº¦é–‹å§‹
    score = evaluate(model, dataset, difficulty='medium')

    if score > 0.8:
        # æ¨¡å‹è¡¨ç¾å¥½ï¼Œå¢åŠ é›£åº¦
        final_score = evaluate(model, dataset, difficulty='hard')
    elif score < 0.5:
        # æ¨¡å‹è¡¨ç¾å·®ï¼Œé™ä½é›£åº¦ä»¥ç²å¾—æ›´ç´°ç²’åº¦çš„è©•ä¼°
        final_score = evaluate(model, dataset, difficulty='easy')
    else:
        final_score = score

    return final_score
```

#### å°æŠ—è©•ä¼°
```python
# æ¸¬è©¦æ¨¡å‹å°å°æŠ—æ¨£æœ¬çš„é­¯æ£’æ€§
from opencompass.datasets import AdversarialDataset

# è‡ªå‹•ç”Ÿæˆå°æŠ—æ¨£æœ¬
adversarial_dataset = AdversarialDataset.from_base(
    base_dataset='ceval',
    perturbation_type='paraphrase',  # æ”¹è¿°æ”»æ“Š
    intensity=0.3
)

# è©•ä¼°é­¯æ£’æ€§
robust_acc = evaluate(model, adversarial_dataset)
```

#### å¤šæ¨¡æ…‹è©•ä¼°
```python
# æ“´å±•åˆ°åœ–åƒ-æ–‡æœ¬ä»»å‹™
from opencompass.datasets import MMBenchDataset

mm_dataset = MMBenchDataset(
    modalities=['image', 'text'],
    tasks=['vqa', 'captioning', 'reasoning']
)

mm_score = evaluate(multimodal_model, mm_dataset)
```

### 10.3 è©•ä¼°èˆ‡è¨“ç·´é–‰ç’°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   è©•ä¼°æ¨¡å‹   â”‚  è­˜åˆ¥å¼±é …ï¼ˆå¦‚ï¼šæ•¸å­¸æ¨ç†å·®ï¼‰
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ æ•¸æ“šé‡å°ç¯©é¸ â”‚  æ”¶é›†æ›´å¤šæ•¸å­¸æ¨ç†æ•¸æ“š
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   å¾®èª¿è¨“ç·´   â”‚  é‡å°æ€§è¨“ç·´
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   å†æ¬¡è©•ä¼°   â”‚  é©—è­‰æ”¹é€²æ•ˆæœ
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€ æŒçºŒè¿­ä»£
```

**å¯¦æ–½æ­¥é©Ÿ**:
1. ä½¿ç”¨ OpenCompass å…¨é¢è©•ä¼°åŸºç¤æ¨¡å‹
2. åˆ†æçµæœï¼Œè­˜åˆ¥è–„å¼±é ˜åŸŸï¼ˆå¦‚ GSM8K åƒ… 14.6%ï¼‰
3. ä½¿ç”¨æ•¸æ“šç¯©é¸æŠ€è¡“ï¼ˆLab-4.2ï¼‰ç²å–é«˜è³ªé‡æ•¸å­¸æ•¸æ“š
4. é€²è¡Œé ˜åŸŸå¾®èª¿ï¼ˆLab-1.1 PEFTï¼‰
5. å†æ¬¡è©•ä¼°ï¼Œé©—è­‰ GSM8K æå‡è‡³ 40%+
6. è¿­ä»£ä¸Šè¿°éç¨‹

---

## 11. åƒè€ƒè³‡æ–™

### æ ¸å¿ƒè«–æ–‡

**OpenCompass**:
- **ä¸»è«–æ–‡**: "OpenCompass: A Universal Evaluation Platform for Foundation Models" (2023)
  - arXiv: https://arxiv.org/abs/2304.xxxxx
  - GitHub: https://github.com/open-compass/opencompass

**è©•ä¼°åŸºæº–**:
- **C-Eval**: "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"
  - arXiv: https://arxiv.org/abs/2305.08322
  - ç¶²ç«™: https://cevalbenchmark.com/

- **CMMLU**: "CMMLU: Measuring Massive Multitask Language Understanding in Chinese"
  - arXiv: https://arxiv.org/abs/2306.09212
  - GitHub: https://github.com/haonan-li/CMMLU

- **MMLU**: "Measuring Massive Multitask Language Understanding"
  - arXiv: https://arxiv.org/abs/2009.03300
  - GitHub: https://github.com/hendrycks/test

**å…¶ä»–é‡è¦åŸºæº–**:
- **GSM8K**: "Training Verifiers to Solve Math Word Problems" (arXiv:2110.14168)
- **HumanEval**: "Evaluating Large Language Models Trained on Code" (arXiv:2107.03374)
- **BBH**: "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them" (arXiv:2210.09261)

### å·¥å…·èˆ‡å¯¦ç¾

**OpenCompass ç”Ÿæ…‹**:
- **å®˜æ–¹æ–‡æª”**: https://opencompass.readthedocs.io/
- **GitHub å€‰åº«**: https://github.com/open-compass/opencompass
- **Leaderboard**: https://opencompass.org.cn/leaderboard-llm
- **Discord ç¤¾å€**: https://discord.gg/opencompass

**å…¶ä»–è©•ä¼°æ¡†æ¶**:
- **LM-Eval-Harness**: https://github.com/EleutherAI/lm-evaluation-harness
- **HELM**: https://github.com/stanford-crfm/helm
- **OpenAI Evals**: https://github.com/openai/evals

### æ¨¡å‹èˆ‡è³‡æ–™é›†

**é è¨“ç·´æ¨¡å‹**:
- **Llama-2**: https://huggingface.co/meta-llama/Llama-2-7b-hf
- **Qwen**: https://huggingface.co/Qwen/Qwen-7B
- **ChatGLM**: https://huggingface.co/THUDM/chatglm3-6b

**è©•ä¼°æ•¸æ“šé›†**:
- **C-Eval**: https://huggingface.co/datasets/ceval/ceval-exam
- **CMMLU**: https://huggingface.co/datasets/haonan-li/cmmlu
- **MMLU**: https://huggingface.co/datasets/cais/mmlu

### å»¶ä¼¸é–±è®€

**æŠ€è¡“åšå®¢**:
- OpenCompass å®˜æ–¹åšå®¢: https://opencompass.org.cn/blog
- Hugging Face Blog: https://huggingface.co/blog/evaluating-llm

**è©•ä¼°æ–¹æ³•è«–**:
- "Holistic Evaluation of Language Models" (HELM) - Stanford
- "Beyond the Imitation Game" (BIG-bench) - Google

**è¡Œæ¥­å ±å‘Š**:
- "State of AI Report 2024" - https://www.stateof.ai/
- "Foundation Model Transparency Index" - Stanford HAI

---

## ğŸ“š é€Ÿè¨˜å¿ƒæ³•èˆ‡å£è¨£

### ğŸ¯ è©•ä¼°å››æ­¥æ³•

```
è©•ä¼°æµç¨‹:
1. é¸åŸºæº– - æ ¹æ“šç›®æ¨™é¸æ“‡åˆé©åŸºæº–
2. è·‘è©•ä¼° - åŸ·è¡Œæ¨™æº–è©•ä¼°å”è­°
3. æçµæœ - å¤šç¶­åº¦åˆ†æè©•ä¼°çµæœ
4. å®šæ–¹å‘ - åŸºæ–¼çµæœåˆ¶å®šå„ªåŒ–ç­–ç•¥

å£è¨£: ã€Œé¸è·‘æå®šï¼Œç’°ç’°ç›¸æ‰£ã€
```

### âš¡ OpenCompass ä¸‰è¦ç´ 

```
é…ç½®ä¸‰è¦ç´ :
æ¨¡ - å®šç¾©å¾…è©•ä¼°æ¨¡å‹
æ•¸ - é¸æ“‡è©•ä¼°æ•¸æ“šé›†
åƒ - è¨­å®šè©•ä¼°åƒæ•¸

ã€Œæ¨¡æ•¸åƒï¼Œç¼ºä¸€ä¸å¯ã€
```

### ğŸ“Š çµæœè§£è®€ä¸‰å±¤æ¬¡

```
è§£è®€å±¤æ¬¡:
1. æ•´é«”åˆ†æ•¸ - å®è§€èƒ½åŠ›æ°´å¹³
2. å­¸ç§‘åˆ†ä½ˆ - å„ªåŠ£å‹¢è­˜åˆ¥
3. éŒ¯èª¤åˆ†æ - å…·é«”å•é¡Œå®šä½

ã€Œç¸½åˆ†çœ‹æ°´å¹³ï¼Œåˆ†ç§‘æ‰¾å•é¡Œï¼ŒéŒ¯ä¾‹å®šæ–¹å‘ã€
```

### ğŸ”§ èª¿å„ªä¸‰åŸå‰‡

```
èª¿å„ªåŸå‰‡:
å¿« - å¿«é€Ÿé©—è­‰ç”¨å­é›†
æº– - æº–ç¢ºè©•ä¼°ç”¨å…¨é›†
ç©© - ç©©å®šçµæœå›ºå®šç¨®å­

ã€Œå¿«æº–ç©©ï¼Œä¸‰ä½ä¸€é«”ã€
```

---

**ç‹€æ…‹**: ğŸ“ è¦åŠƒå®Œæˆï¼Œå¾…é–‹ç™¼
**æœ€å¾Œæ›´æ–°**: 2025-10-17
**ç¶­è­·è€…**: Claude Code
**é è¨ˆé–‹ç™¼æ™‚é–“**: 6 å°æ™‚
