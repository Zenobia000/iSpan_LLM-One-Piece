{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Prompt Tuning - Deployment Guide\n",
    "---\n",
    "## 🎯 實驗目標\n",
    "\n",
    "本 Notebook 展示 Prompt Tuning 的核心特性:**軟提示的獨立部署與多任務切換**。\n",
    "\n",
    "與其他 PEFT 方法不同,Prompt Tuning 的軟提示參數**無需合併**到基礎模型中,這帶來獨特的部署優勢:\n",
    "\n",
    "### 關鍵學習要點\n",
    "- 理解 Prompt Tuning 的部署特性與其他 PEFT 方法的差異\n",
    "- 掌握軟提示的儲存、載入與版本管理\n",
    "- 實現多任務軟提示的動態切換機制\n",
    "- 優化生產環境的推理效能\n",
    "- 學習最佳實踐與故障排除策略\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設置與依賴檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要的函式庫\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PromptTuningConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 設定隨機種子\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Tuning 部署特性分析\n",
    "\n",
    "### 2.1 為什麼 Prompt Tuning 不需要合併?\n",
    "\n",
    "**核心原理**:\n",
    "```\n",
    "Prompt Tuning: [軟提示向量] + [輸入文本] → 模型處理\n",
    "LoRA/IA³:      修改權重矩陣 W' = W + ΔW → 可合併\n",
    "```\n",
    "\n",
    "**關鍵差異對比**:\n",
    "\n",
    "| 對比維度 | Prompt Tuning | LoRA | IA³ |\n",
    "|---------|--------------|------|-----|\n",
    "| **參數位置** | 輸入層(軟提示嵌入) | 權重矩陣分解 | 縮放向量 |\n",
    "| **是否可合併** | ❌ 不需要 | ✅ 可合併 | ✅ 可合併 |\n",
    "| **部署方式** | 軟提示 + 基礎模型 | 合併後單一模型 | 合併後單一模型 |\n",
    "| **推理開銷** | 軟提示長度影響 | 零開銷(合併後) | 零開銷(合併後) |\n",
    "| **多任務切換** | **極其便捷** | 需重新載入 | 需重新載入 |\n",
    "| **儲存成本** | 極低(僅軟提示) | 中等 | 低 |\n",
    "\n",
    "### 2.2 Prompt Tuning 的獨特部署優勢\n",
    "\n",
    "1. **多任務共享基礎模型**: 一個模型 + N 個軟提示 = N 個任務能力\n",
    "2. **動態任務切換**: 僅需替換軟提示,無需重新載入模型\n",
    "3. **版本管理簡單**: 軟提示參數量極小,易於版本控制與實驗追蹤\n",
    "4. **記憶體效率高**: 多任務部署僅增加少量記憶體開銷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 載入訓練好的 Prompt Tuning 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型與適配器路徑\n",
    "base_model_name = \"t5-small\"\n",
    "adapter_path = \"./t5-prompt-tuning-billsum\"  # 假設這是訓練好的適配器路徑\n",
    "\n",
    "# 載入基礎模型與分詞器\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float32,  # T5 通常使用 FP32\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"基礎模型參數量: {base_model.num_parameters():,}\")\n",
    "print(f\"基礎模型記憶體占用: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 Prompt Tuning 適配器\n",
    "if os.path.exists(adapter_path):\n",
    "    # 從檢查點載入最新的適配器\n",
    "    checkpoints = [d for d in os.listdir(adapter_path) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(\n",
    "            [os.path.join(adapter_path, d) for d in checkpoints],\n",
    "            key=os.path.getmtime\n",
    "        )\n",
    "        print(f\"載入最新檢查點: {latest_checkpoint}\")\n",
    "        peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
    "    else:\n",
    "        # 直接從目錄載入\n",
    "        peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    print(f\"✅ 成功載入 Prompt Tuning 適配器: {adapter_path}\")\n",
    "else:\n",
    "    # 若無訓練好的適配器,創建示範配置\n",
    "    print(\"未找到訓練好的適配器,創建示範 Prompt Tuning 配置...\")\n",
    "    \n",
    "    prompt_config = PromptTuningConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        prompt_tuning_init=\"TEXT\",\n",
    "        prompt_tuning_init_text=\"Summarize the following text:\",\n",
    "        num_virtual_tokens=8,\n",
    "        tokenizer_name_or_path=base_model_name\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(base_model, prompt_config)\n",
    "    print(\"已創建示範 Prompt Tuning 模型\")\n",
    "\n",
    "# 顯示可訓練參數統計\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 軟提示結構分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_soft_prompts(model):\n",
    "    \"\"\"分析軟提示結構與參數\"\"\"\n",
    "    prompt_params = {}\n",
    "    total_prompt_params = 0\n",
    "    \n",
    "    print(\"=== 軟提示參數分析 ===\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'prompt' in name.lower():\n",
    "            prompt_params[name] = {\n",
    "                'shape': param.shape,\n",
    "                'dtype': param.dtype,\n",
    "                'numel': param.numel()\n",
    "            }\n",
    "            total_prompt_params += param.numel()\n",
    "            \n",
    "            print(f\"\\n軟提示參數: {name}\")\n",
    "            print(f\"  形狀: {param.shape}\")\n",
    "            print(f\"  數據類型: {param.dtype}\")\n",
    "            print(f\"  參數量: {param.numel():,}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    prompt_ratio = (total_prompt_params / total_params) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"軟提示總參數量: {total_prompt_params:,}\")\n",
    "    print(f\"模型總參數量: {total_params:,}\")\n",
    "    print(f\"軟提示占比: {prompt_ratio:.4f}%\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return prompt_params\n",
    "\n",
    "soft_prompt_structure = analyze_soft_prompts(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 推理效能基準測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_summarization(model, tokenizer, test_samples, num_runs=3):\n",
    "    \"\"\"摘要生成效能基準測試\"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for sample in test_samples:\n",
    "                # 準備輸入\n",
    "                prompt = \"summarize: \" + sample['text'][:1024]  # 限制輸入長度\n",
    "                inputs = tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=1024,\n",
    "                    truncation=True\n",
    "                )\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                # 生成摘要\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    max_length=150,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                \n",
    "                generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                if run == 0:  # 僅在第一輪保存結果\n",
    "                    results.append({\n",
    "                        'original': sample['text'][:200] + '...',\n",
    "                        'reference': sample['summary'],\n",
    "                        'generated': generated_summary\n",
    "                    })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    return avg_time, results\n",
    "\n",
    "# 載入測試樣本\n",
    "print(\"載入測試數據集...\")\n",
    "test_dataset = load_dataset(\"billsum\", split=\"test[:5]\")\n",
    "\n",
    "print(\"\\n=== 推理效能測試 ===\")\n",
    "inference_time, inference_results = benchmark_summarization(\n",
    "    peft_model,\n",
    "    tokenizer,\n",
    "    test_dataset\n",
    ")\n",
    "\n",
    "print(f\"平均推理時間: {inference_time:.4f} 秒 (5 個樣本)\")\n",
    "print(f\"每個樣本平均時間: {inference_time / len(test_dataset):.4f} 秒\")\n",
    "print(f\"記憶體占用: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示生成範例\n",
    "print(\"\\n=== 摘要生成範例 ===\")\n",
    "for i, result in enumerate(inference_results[:2]):  # 僅顯示前兩個範例\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"範例 {i+1}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"原始文本: {result['original']}\")\n",
    "    print(f\"\\n參考摘要: {result['reference']}\")\n",
    "    print(f\"\\n生成摘要: {result['generated']}\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 軟提示儲存與版本管理\n",
    "\n",
    "### 6.1 軟提示的獨立儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定儲存路徑\n",
    "save_path = \"./prompt-tuning-deployed\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "print(f\"=== 儲存 Prompt Tuning 軟提示到: {save_path} ===\")\n",
    "\n",
    "# 儲存軟提示適配器(不包含基礎模型)\n",
    "peft_model.save_pretrained(save_path)\n",
    "\n",
    "print(\"✅ 軟提示儲存完成!\")\n",
    "\n",
    "# 檢查儲存的檔案\n",
    "saved_files = os.listdir(save_path)\n",
    "print(f\"\\n儲存的檔案: {saved_files}\")\n",
    "\n",
    "# 計算軟提示檔案大小\n",
    "total_size = 0\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(save_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        total_size += size\n",
    "        print(f\"  {file}: {size / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\n軟提示總大小: {total_size / 1024:.2f} KB\")\n",
    "print(f\"基礎模型大小(參考): ~242 MB (t5-small)\")\n",
    "print(f\"儲存效率: {(total_size / (242 * 1024 * 1024)) * 100:.4f}% 的基礎模型大小\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 軟提示配置分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取並顯示軟提示配置\n",
    "config_path = os.path.join(save_path, \"adapter_config.json\")\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        prompt_config = json.load(f)\n",
    "    \n",
    "    print(\"=== Prompt Tuning 配置詳情 ===\")\n",
    "    print(json.dumps(prompt_config, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    print(f\"\\n關鍵配置參數:\")\n",
    "    print(f\"  任務類型: {prompt_config.get('task_type', 'N/A')}\")\n",
    "    print(f\"  軟提示長度: {prompt_config.get('num_virtual_tokens', 'N/A')} tokens\")\n",
    "    print(f\"  初始化方法: {prompt_config.get('prompt_tuning_init', 'N/A')}\")\n",
    "    print(f\"  初始化文本: {prompt_config.get('prompt_tuning_init_text', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 多任務軟提示管理與動態切換\n",
    "\n",
    "### 7.1 模擬多任務場景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskPromptManager:\n",
    "    \"\"\"\n",
    "    多任務軟提示管理器\n",
    "    \n",
    "    實現功能:\n",
    "    - 管理多個任務的軟提示\n",
    "    - 動態切換任務\n",
    "    - 記憶體效率優化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, tokenizer):\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_prompts = {}  # 儲存任務特定的軟提示路徑\n",
    "        self.current_task = None\n",
    "        self.current_model = None\n",
    "    \n",
    "    def register_task(self, task_name, adapter_path):\n",
    "        \"\"\"註冊新任務的軟提示\"\"\"\n",
    "        if os.path.exists(adapter_path):\n",
    "            self.task_prompts[task_name] = adapter_path\n",
    "            print(f\"✅ 已註冊任務 '{task_name}': {adapter_path}\")\n",
    "        else:\n",
    "            print(f\"❌ 任務路徑不存在: {adapter_path}\")\n",
    "    \n",
    "    def switch_task(self, task_name):\n",
    "        \"\"\"切換到指定任務\"\"\"\n",
    "        if task_name not in self.task_prompts:\n",
    "            raise ValueError(f\"任務 '{task_name}' 尚未註冊\")\n",
    "        \n",
    "        # 清理當前模型(若存在)\n",
    "        if self.current_model is not None:\n",
    "            del self.current_model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # 載入新任務的軟提示\n",
    "        adapter_path = self.task_prompts[task_name]\n",
    "        self.current_model = PeftModel.from_pretrained(\n",
    "            self.base_model,\n",
    "            adapter_path\n",
    "        )\n",
    "        self.current_task = task_name\n",
    "        \n",
    "        print(f\"🔄 已切換到任務: {task_name}\")\n",
    "    \n",
    "    def generate(self, input_text, **generation_kwargs):\n",
    "        \"\"\"使用當前任務的軟提示生成輸出\"\"\"\n",
    "        if self.current_model is None:\n",
    "            raise RuntimeError(\"尚未選擇任務,請先調用 switch_task()\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=1024,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.current_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                **generation_kwargs\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def list_tasks(self):\n",
    "        \"\"\"列出所有已註冊的任務\"\"\"\n",
    "        print(\"\\n=== 已註冊的任務 ===\")\n",
    "        for task_name, adapter_path in self.task_prompts.items():\n",
    "            status = \"✓\" if task_name == self.current_task else \" \"\n",
    "            print(f\"[{status}] {task_name}: {adapter_path}\")\n",
    "\n",
    "# 創建多任務管理器示範\n",
    "manager = MultiTaskPromptManager(base_model, tokenizer)\n",
    "\n",
    "# 註冊任務(實際使用時,這些應該是不同任務的軟提示)\n",
    "manager.register_task(\"summarization\", save_path)\n",
    "# manager.register_task(\"translation\", \"./prompt-translation\")\n",
    "# manager.register_task(\"qa\", \"./prompt-qa\")\n",
    "\n",
    "manager.list_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 測試任務切換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切換到摘要任務\n",
    "manager.switch_task(\"summarization\")\n",
    "\n",
    "# 測試生成\n",
    "test_text = \"summarize: \" + test_dataset[0]['text'][:500]\n",
    "generated = manager.generate(\n",
    "    test_text,\n",
    "    max_length=150,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== 任務切換測試 ===\")\n",
    "print(f\"當前任務: {manager.current_task}\")\n",
    "print(f\"\\n輸入文本: {test_dataset[0]['text'][:200]}...\")\n",
    "print(f\"\\n生成摘要: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 生產部署配置\n",
    "\n",
    "### 8.1 部署配置檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建部署配置檔案\n",
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"peft_method\": \"Prompt Tuning\",\n",
    "        \"task_type\": \"Seq2Seq Summarization\",\n",
    "        \"mergeable\": False,\n",
    "        \"soft_prompt_tokens\": prompt_config.get('num_virtual_tokens', 8),\n",
    "        \"soft_prompt_size_kb\": total_size / 1024\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"avg_inference_time_seconds\": inference_time / len(test_dataset),\n",
    "        \"memory_usage_mb\": peft_model.get_memory_footprint() / 1024**2,\n",
    "        \"soft_prompt_overhead\": \"Minimal (input length reduction)\"\n",
    "    },\n",
    "    \"deployment_strategy\": {\n",
    "        \"type\": \"Soft Prompt + Base Model\",\n",
    "        \"base_model_loading\": \"Load once, reuse for multiple tasks\",\n",
    "        \"task_switching\": \"Dynamic soft prompt swapping\",\n",
    "        \"multi_task_support\": True\n",
    "    },\n",
    "    \"deployment_requirements\": {\n",
    "        \"python_packages\": [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.20.0\",\n",
    "            \"peft>=0.3.0\"\n",
    "        ],\n",
    "        \"minimum_gpu_memory_gb\": 4,\n",
    "        \"recommended_gpu_memory_gb\": 8\n",
    "    },\n",
    "    \"usage_example\": {\n",
    "        \"load_base_model\": f\"AutoModelForSeq2SeqLM.from_pretrained('{base_model_name}')\",\n",
    "        \"load_soft_prompt\": f\"PeftModel.from_pretrained(base_model, '{save_path}')\",\n",
    "        \"inference_note\": \"Add task-specific prefix (e.g., 'summarize:') to input\"\n",
    "    },\n",
    "    \"best_practices\": {\n",
    "        \"multi_task_deployment\": \"Load base model once, swap soft prompts for different tasks\",\n",
    "        \"version_control\": \"Store soft prompts separately for easy A/B testing\",\n",
    "        \"scaling\": \"Base model can be shared across multiple task endpoints\",\n",
    "        \"monitoring\": \"Track soft prompt switching frequency and cache hit rate\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 儲存配置檔案\n",
    "config_output_path = os.path.join(save_path, \"deployment_config.json\")\n",
    "with open(config_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=== 生產部署配置 ===\")\n",
    "print(json.dumps(deployment_config, indent=2, ensure_ascii=False))\n",
    "print(f\"\\n✅ 配置檔案已儲存到: {config_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 部署架構圖\n",
    "\n",
    "```\n",
    "生產環境部署架構 (多任務場景)\n",
    "═════════════════════════════════════════════════════════════\n",
    "\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    API Gateway                          │\n",
    "└────────────────┬────────────────────────────────────────┘\n",
    "                 │\n",
    "        ┌────────┼────────┐\n",
    "        │        │        │\n",
    "    ┌───▼──┐ ┌──▼───┐ ┌─▼────┐\n",
    "    │Task A│ │Task B│ │Task C│\n",
    "    │ (摘要)│ │(翻譯)│ │ (QA) │\n",
    "    └───┬──┘ └──┬───┘ └─┬────┘\n",
    "        │        │       │\n",
    "        └────────┼───────┘\n",
    "                 │\n",
    "    ┌────────────▼──────────────┐\n",
    "    │  Soft Prompt Selector     │  ← 動態切換軟提示\n",
    "    ├───────────────────────────┤\n",
    "    │  Prompt A  │ Prompt B │ C │  (僅幾 KB 大小)\n",
    "    └────────────┬──────────────┘\n",
    "                 │\n",
    "    ┌────────────▼──────────────┐\n",
    "    │    T5 Base Model          │  ← 共享基礎模型\n",
    "    │    (僅載入一次)            │  (242 MB)\n",
    "    └───────────────────────────┘\n",
    "\n",
    "優勢:\n",
    "✅ 單一基礎模型支援多任務\n",
    "✅ 軟提示切換成本極低 (<1ms)\n",
    "✅ 總記憶體占用 ≈ 1個基礎模型 + N個軟提示\n",
    "✅ 易於 A/B 測試與版本迭代\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prompt Tuning 部署最佳實踐\n",
    "\n",
    "### 9.1 何時選擇 Prompt Tuning 部署策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prompt Tuning 部署場景推薦 ===\")\n",
    "print(\"\")\n",
    "print(\"🎯 最適合場景:\")\n",
    "print(\"  • 需要支援多個相似任務(共享基礎模型)\")\n",
    "print(\"  • 頻繁的模型版本迭代與 A/B 測試\")\n",
    "print(\"  • 使用超大規模模型(>10B 參數,規模效應顯著)\")\n",
    "print(\"  • 需要快速部署新任務能力\")\n",
    "print(\"  • 儲存與傳輸成本敏感(軟提示僅幾 KB)\")\n",
    "print(\"\")\n",
    "print(\"⚠️ 需要謹慎考慮的場景:\")\n",
    "print(\"  • 單一任務且對推理延遲極其敏感(軟提示會略微增加輸入長度)\")\n",
    "print(\"  • 小型模型(<1B 參數,規模效應不明顯)\")\n",
    "print(\"  • 任務需要大幅修改模型內部表示\")\n",
    "print(\"  • 需要極致的推理效能優化(考慮 LoRA/IA³ 合併)\")\n",
    "print(\"\")\n",
    "print(\"🔄 與其他方法的組合策略:\")\n",
    "print(\"  • Prompt Tuning + LoRA: 大模型用 Prompt Tuning,小模型用 LoRA\")\n",
    "print(\"  • 先用 Prompt Tuning 快速驗證,再用其他方法精調\")\n",
    "print(\"  • 不同層使用不同 PEFT 方法(混合策略)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 部署檢核清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prompt Tuning 部署檢核清單 ===\")\n",
    "print(\"\")\n",
    "print(\"📋 部署前檢查:\")\n",
    "print(\"  □ 確認軟提示訓練完成並儲存\")\n",
    "print(\"  □ 在驗證集上驗證軟提示效果\")\n",
    "print(\"  □ 記錄軟提示配置(長度、初始化方法等)\")\n",
    "print(\"  □ 測試基礎模型 + 軟提示的載入流程\")\n",
    "print(\"\")\n",
    "print(\"🔄 多任務部署:\")\n",
    "print(\"  □ 設計軟提示命名與版本管理規範\")\n",
    "print(\"  □ 實現軟提示動態載入與切換機制\")\n",
    "print(\"  □ 配置軟提示快取策略(熱門任務常駐記憶體)\")\n",
    "print(\"  □ 監控任務切換頻率與效能影響\")\n",
    "print(\"\")\n",
    "print(\"✅ 部署後驗證:\")\n",
    "print(\"  □ 推理速度測試\")\n",
    "print(\"  □ 記憶體占用監控\")\n",
    "print(\"  □ 輸出品質抽樣檢查\")\n",
    "print(\"  □ 長時間穩定性測試\")\n",
    "print(\"  □ 軟提示切換正確性驗證(多任務場景)\")\n",
    "print(\"\")\n",
    "print(\"📚 文件與監控:\")\n",
    "print(\"  □ 儲存部署配置檔案\")\n",
    "print(\"  □ 記錄效能基準數據\")\n",
    "print(\"  □ 準備回滾方案(保留多版本軟提示)\")\n",
    "print(\"  □ 更新 API 文件與使用範例\")\n",
    "print(\"  □ 設置軟提示版本追蹤系統\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 故障排除指南"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Prompt Tuning 常見問題與解決方案 ===\")\n",
    "print(\"\")\n",
    "print(\"❓ 問題 1: 載入軟提示時出現維度不匹配\")\n",
    "print(\"   原因: 軟提示訓練時的基礎模型版本與部署時不一致\")\n",
    "print(\"   解決: 確保使用完全相同的基礎模型(名稱與版本)\")\n",
    "print(\"\")\n",
    "print(\"❓ 問題 2: 推理結果與訓練時差異很大\")\n",
    "print(\"   原因: 可能缺少任務前綴(如 'summarize:')\")\n",
    "print(\"   解決: 檢查輸入格式,確保包含正確的任務前綴\")\n",
    "print(\"\")\n",
    "print(\"❓ 問題 3: 多任務切換後輸出異常\")\n",
    "print(\"   原因: 軟提示未正確切換或基礎模型狀態污染\")\n",
    "print(\"   解決: 確保每次切換都完整載入新軟提示,清空 CUDA cache\")\n",
    "print(\"\")\n",
    "print(\"❓ 問題 4: 記憶體占用持續增長\")\n",
    "print(\"   原因: 多次軟提示切換未釋放舊模型\")\n",
    "print(\"   解決: 在切換前執行 del model + gc.collect() + torch.cuda.empty_cache()\")\n",
    "print(\"\")\n",
    "print(\"❓ 問題 5: 生成品質低於預期\")\n",
    "print(\"   原因: 可能使用了小型基礎模型(規模效應不足)\")\n",
    "print(\"   解決: 考慮升級到更大的基礎模型(>1B),或改用 LoRA 方法\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. PEFT 方法部署策略對比總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 方法部署策略對比\n",
    "deployment_comparison = {\n",
    "    \"Prompt Tuning\": {\n",
    "        \"can_merge\": False,\n",
    "        \"deployment_unit\": \"Soft Prompt (幾 KB) + Base Model\",\n",
    "        \"inference_overhead\": \"Input length increase\",\n",
    "        \"multi_task_support\": \"Excellent (dynamic switching)\",\n",
    "        \"deployment_complexity\": \"Medium\",\n",
    "        \"best_for\": \"Multi-task, large models, frequent updates\"\n",
    "    },\n",
    "    \"LoRA\": {\n",
    "        \"can_merge\": True,\n",
    "        \"deployment_unit\": \"Merged single model OR Adapter + Base Model\",\n",
    "        \"inference_overhead\": \"Zero (if merged)\",\n",
    "        \"multi_task_support\": \"Good (need reload)\",\n",
    "        \"deployment_complexity\": \"Simple (if merged)\",\n",
    "        \"best_for\": \"Single task, balanced efficiency and quality\"\n",
    "    },\n",
    "    \"IA3\": {\n",
    "        \"can_merge\": True,\n",
    "        \"deployment_unit\": \"Merged single model\",\n",
    "        \"inference_overhead\": \"Zero (fully merged)\",\n",
    "        \"multi_task_support\": \"Medium (need reload)\",\n",
    "        \"deployment_complexity\": \"Simple\",\n",
    "        \"best_for\": \"Extreme efficiency, inference-critical\"\n",
    "    },\n",
    "    \"Prefix Tuning\": {\n",
    "        \"can_merge\": False,\n",
    "        \"deployment_unit\": \"Prefix + Base Model\",\n",
    "        \"inference_overhead\": \"Attention computation increase\",\n",
    "        \"multi_task_support\": \"Good (dynamic switching)\",\n",
    "        \"deployment_complexity\": \"Complex\",\n",
    "        \"best_for\": \"Generation tasks, need expressiveness\"\n",
    "    },\n",
    "    \"AdapterLayers\": {\n",
    "        \"can_merge\": False,\n",
    "        \"deployment_unit\": \"Adapter Layers + Base Model\",\n",
    "        \"inference_overhead\": \"Bottleneck layer computation\",\n",
    "        \"multi_task_support\": \"Excellent (modular design)\",\n",
    "        \"deployment_complexity\": \"Medium\",\n",
    "        \"best_for\": \"Multi-task modular deployment\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== PEFT 方法部署策略對比 ===\")\n",
    "print(f\"{'方法':<15} {'可合併':<10} {'部署單元':<30} {'推理開銷':<20} {'多任務支援':<20}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for method, info in deployment_comparison.items():\n",
    "    mergeable = \"✅\" if info[\"can_merge\"] else \"❌\"\n",
    "    print(f\"{method:<15} {mergeable:<10} {info['deployment_unit']:<30} {info['inference_overhead']:<20} {info['multi_task_support']:<20}\")\n",
    "\n",
    "print(\"\\n=== Prompt Tuning 獨特部署優勢 ===\")\n",
    "print(\"✅ 多任務共享: 一個基礎模型 + N 個軟提示 = N 個任務能力\")\n",
    "print(\"✅ 動態切換: 軟提示切換成本極低(<1ms)\")\n",
    "print(\"✅ 版本管理: 軟提示檔案極小,易於版本控制與實驗追蹤\")\n",
    "print(\"✅ 規模友好: 大模型上效果逼近全參數微調,但儲存成本極低\")\n",
    "print(\"✅ 無需合併: 保持基礎模型原始狀態,易於回滾與更新\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 實驗總結\n",
    "\n",
    "### 11.1 核心學習成果\n",
    "\n",
    "通過本實驗,我們深入探索了 **Prompt Tuning 的獨特部署特性**:\n",
    "\n",
    "1. **理論理解**: 掌握了為何 Prompt Tuning 不需要也不應該合併的原理\n",
    "2. **實踐經驗**: 完成了軟提示的儲存、載入、版本管理全流程\n",
    "3. **多任務管理**: 實現了基於軟提示動態切換的多任務部署架構\n",
    "4. **最佳實踐**: 學習了生產環境部署的完整檢核清單與故障排除策略\n",
    "\n",
    "### 11.2 Prompt Tuning 的獨特價值\n",
    "\n",
    "- **多任務友好**: 單一基礎模型支援無限任務,僅需切換軟提示\n",
    "- **極致儲存效率**: 每個任務僅需幾 KB 的軟提示檔案\n",
    "- **部署靈活性**: 無需修改基礎模型,易於版本管理與回滾\n",
    "- **規模驅動**: 隨著模型規模增大,效果逐步逼近全參數微調\n",
    "\n",
    "### 11.3 部署策略選擇建議\n",
    "\n",
    "| 場景 | 推薦方法 | 理由 |\n",
    "|------|---------|------|\n",
    "| **多任務系統** | **Prompt Tuning** | 共享基礎模型,動態切換 |\n",
    "| **單任務高效能** | LoRA(合併) 或 IA³ | 零推理開銷 |\n",
    "| **頻繁更新迭代** | Prompt Tuning | 版本管理簡單 |\n",
    "| **極致推理速度** | IA³(合併) | 完全無開銷 |\n",
    "| **大模型(>10B)** | Prompt Tuning | 規模效應顯著 |\n",
    "| **小模型(<1B)** | LoRA | 表達能力更強 |\n",
    "\n",
    "Prompt Tuning 完美詮釋了 **「簡潔而不簡單」** 的設計哲學,在適當場景下,極簡的方法往往帶來最優雅的解決方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 資源清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理實驗資源\n",
    "print(\"=== 清理實驗資源 ===\")\n",
    "\n",
    "# 清理模型\n",
    "if 'peft_model' in locals():\n",
    "    del peft_model\n",
    "if 'base_model' in locals():\n",
    "    del base_model\n",
    "if 'manager' in locals():\n",
    "    del manager\n",
    "\n",
    "# 垃圾回收\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✅ 資源清理完成\")\n",
    "print(\"\\n🎉 Lab 3 - Prompt Tuning 部署指南實驗完成!\")\n",
    "print(f\"軟提示已儲存到: {save_path}\")\n",
    "print(\"您現在可以將軟提示與基礎模型分別部署,享受多任務動態切換的便利!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
