{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: LoRA & QLoRA - Fine-Tuning a Llama-2 Model\n",
    "---\n",
    "## Notebook 2: The Training Process\n",
    "\n",
    "**Goal:** In this notebook, you will use QLoRA to fine-tune a `Llama-2-7B` model on an instruction-following dataset.\n",
    "\n",
    "**You will learn to:**\n",
    "-   Load a dataset for supervised fine-tuning and preprocess it with a tokenizer.\n",
    "-   Configure `BitsAndBytesConfig` to load a model in 4-bit precision (QLoRA).\n",
    "-   Load a pre-trained Llama-2 model for causal language modeling.\n",
    "-   Configure `peft.LoraConfig` to apply LoRA adapters to the model.\n",
    "-   Use the `transformers.Trainer` to efficiently fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Dataset and Preprocess\n",
    "\n",
    "First, we'll load our instruction-tuning dataset. We will use the `guanaco-llama2-1k` dataset, which is a small, high-quality dataset of prompts and responses formatted for Llama-2.\n",
    "\n",
    "#### Key Hugging Face Components:\n",
    "\n",
    "-   `datasets.load_dataset`: Fetches a dataset from the Hugging Face Hub.\n",
    "-   `transformers.AutoTokenizer`: Loads the appropriate tokenizer for our model.\n",
    "-   `dataset.map()`: A powerful method to apply a processing function to every example in the dataset. We use `batched=True` for efficient processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`mlabonne/guanaco-llama2-1k` è³‡æ–™é›†**\n",
    "\n",
    "#### **ç‚ºä»€éº¼é¸æ“‡å®ƒï¼Ÿ**\n",
    "- **æ ¼å¼ç›¸å®¹**ï¼šå°ˆç‚º Llama 2 æç¤ºæ ¼å¼è¨­è¨ˆï¼Œç„¡éœ€é¡å¤–è™•ç†ã€‚  \n",
    "- **è¼•é‡é«˜æ•ˆ**ï¼š1000 æ¢æ¨£æœ¬ï¼Œé©åˆå¿«é€Ÿå¾®èª¿èˆ‡æ¸¬è©¦ã€‚  \n",
    "- **é–‹ç™¼è€…å‹å¥½**ï¼šé™„å®˜æ–¹æ•™å­¸èˆ‡ Colab Notebookï¼Œæ˜“æ–¼ä¸Šæ‰‹ã€‚\n",
    "\n",
    "#### **ç‰¹é»**\n",
    "- **é«˜è³ªé‡ä¾†æº**ï¼šåŸºæ–¼ OpenAssistant æ•¸æ“šï¼Œæ¶µè“‹å¤šæ¨£åŒ–æŒ‡ä»¤èˆ‡å›ç­”å ´æ™¯ã€‚  \n",
    "- **å³æ’å³ç”¨**ï¼šæ ¼å¼åŒ–å®Œæˆï¼Œæ”¯æŒ Llama 2 å¾®èª¿éœ€æ±‚ã€‚\n",
    "\n",
    "#### **ç¼ºé»**\n",
    "- **æ•¸æ“šé‡å°**ï¼šåƒ… 1000 æ¢æ¨£æœ¬ï¼Œé©åˆå°å‹å¯¦é©—ä½†ä¸é©ç”¨æ–¼å¤§è¦æ¨¡è¨“ç·´ã€‚  \n",
    "- **è¦†è“‹ç¯„åœæœ‰é™**ï¼šæ¨£æœ¬å¤šæ¨£æ€§ä¸è¶³ï¼Œéœ€é¡å¤–æ“´å±•æ‡‰ç”¨å ´æ™¯ã€‚\n",
    "\n",
    "#### **å±•æœ›**\n",
    "- **æ•¸æ“šæ“´å±•**ï¼šçµåˆå…¶ä»–è³‡æ–™é›†ï¼Œæ“´å……æ¨£æœ¬è¦æ¨¡ã€‚  \n",
    "- **é·ç§»å­¸ç¿’**ï¼šä½œç‚ºåˆå§‹å¾®èª¿åŸºç¤ï¼Œé€²ä¸€æ­¥æå‡æ€§èƒ½ã€‚  \n",
    "- **å®šåˆ¶åŒ–å„ªåŒ–**ï¼šé‡å°ç‰¹å®šé ˜åŸŸé€²è¡Œå°ˆå±¬å¾®èª¿ã€‚\n",
    "\n",
    "#### **çµè«–**\n",
    "`mlabonne/guanaco-llama2-1k` æ˜¯ä¸€æ¬¾è¼•é‡é«˜æ•ˆçš„è³‡æ–™é›†ï¼Œç‚º Llama 2 å¾®èª¿æä¾›ç†æƒ³åŸºç¤ï¼Œé©åˆå¿«é€Ÿå¯¦é©—èˆ‡æ¨¡å‹æ¸¬è©¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb8c68cc45a4c4199ded34be166362f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a468d7110c94efa87d48c549a766877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded and tokenized.\n",
      "Train samples: 900\n",
      "Test samples: 100\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"NousResearch/Llama-2-7b-hf\"\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Important for Causal LM\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# --- Preprocess and Tokenize ---\n",
    "def preprocess_function(examples):\n",
    "    # The 'text' field contains the full formatted prompt.\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
    "\n",
    "# Split the dataset\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Tokenize both splits\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"]) # Remove original text column\n",
    "\n",
    "print(\"âœ… Dataset loaded and tokenized.\")\n",
    "print(f\"Train samples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Test samples: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Base Model\n",
    "\n",
    "Now, we load the base model. The key to QLoRA is loading the base model in a quantized format.\n",
    "\n",
    "#### Key Hugging Face `transformers` Components:\n",
    "\n",
    "-   `transformers.BitsAndBytesConfig`: This configuration class is used to specify all the parameters for quantization.\n",
    "    -   `load_in_4bit=True`: This is the master switch to enable 4-bit loading.\n",
    "    -   `bnb_4bit_quant_type=\"nf4\"`: We use the \"nf4\" (Normalized Float 4) quantization type, which is recommended for QLoRA.\n",
    "    -   `bnb_4bit_compute_dtype=torch.bfloat16`: This sets the compute data type during the forward and backward passes. `bfloat16` is a good choice for modern GPUs.\n",
    "-   `transformers.AutoModelForCausalLM`: We use this to load our Llama-2 model, passing the `quantization_config` to it.\n",
    "\n",
    "\n",
    "å¥½çš„ï¼Œé€™æ˜¯åœ¨ `02-Train.ipynb` æª”æ¡ˆä¸­å° `Normalized Float 4 (NF4)` æ ¸å¿ƒæ€æƒ³çš„ç²¾ç°¡ Markdown è¡¨è¿°ï¼Œæ‚¨å¯ä»¥å°‡å…¶æ•´åˆåˆ°æ‚¨çš„ç­†è¨˜ä¸­ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### `Normalized Float 4 (NF4)` æ ¸å¿ƒæ€æƒ³è§£æ\n",
    "\n",
    "`Normalized Float 4 (NF4)` æ˜¯ QLoRA ä¸­ä½¿ç”¨çš„ä¸€ç¨®å‰µæ–°çš„ 4-bit è³‡æ–™å‹æ…‹ï¼Œå…¶æ ¸å¿ƒç†å¿µæ˜¯ï¼š**ç‚ºå¸¸æ…‹åˆ†ä½ˆçš„æ¨¡å‹æ¬Šé‡ï¼Œè¨­è¨ˆä¸€ç¨®è³‡è¨Šç†è«–ä¸Šæœ€å„ªçš„ 4-bit é‡åŒ–æ–¹æ³•**ã€‚\n",
    "\n",
    "#### æ ¸å¿ƒæ–¹æ³•ï¼šåˆ†ä½æ•¸é‡åŒ– (Quantile Quantization)\n",
    "\n",
    "1.  **å•é¡Œå‰æ**ï¼š\n",
    "    å¤§å‹èªè¨€æ¨¡å‹çš„æ¬Šé‡ï¼ˆweightsï¼‰ä¸¦éå‡å‹»åˆ†ä½ˆï¼Œè€Œæ˜¯é«˜åº¦é›†ä¸­æ–¼ 0 é™„è¿‘ï¼Œå‘ˆç¾**å¸¸æ…‹åˆ†ä½ˆ**ï¼ˆé˜å½¢æ›²ç·šï¼‰ã€‚å‚³çµ±çš„ 4-bit æ ¼å¼ï¼ˆå¦‚ FP4ï¼‰æ˜¯å‡å‹»é‡åŒ–çš„ï¼Œé€™æœƒå°è‡´åœ¨æ¬Šé‡å¯†é›†å€ç²¾åº¦æå¤±è¼ƒå¤§ã€‚\n",
    "\n",
    "2.  **NF4 çš„è§£æ±ºæ–¹æ¡ˆ**ï¼š\n",
    "    *   **å»ºç«‹ç†æƒ³æ¨¡æ¿**ï¼šé¦–å…ˆï¼Œæ¼”ç®—æ³•æœƒå»ºç«‹ä¸€å€‹æ¨™æº–å¸¸æ…‹åˆ†ä½ˆçš„ã€Œæ¨¡æ¿ã€ã€‚\n",
    "    *   **è¨ˆç®—åˆ†ä½æ•¸**ï¼šæ¥è‘—ï¼Œå®ƒå°‡é€™å€‹æ¨¡æ¿åŠƒåˆ†ç‚º `2^4 = 16` å€‹**ç­‰æ©Ÿç‡**çš„å€é–“ã€‚é€™æ„å‘³è‘—æ¯å€‹å€é–“åŒ…å«çš„æ•¸æ“šé»æ•¸é‡æ˜¯ç›¸åŒçš„ã€‚\n",
    "    *   **éå‡å‹»é‡åŒ–**ï¼šé€™ 16 å€‹å€é–“çš„åˆ†å‰²é»å°±æˆç‚ºäº† NF4 çš„é‡åŒ–å€¼ã€‚çµæœæ˜¯ä¸€ç¨®éå‡å‹»çš„åˆ»åº¦ï¼Œåœ¨æ•¸æ“šå¯†é›†çš„ä¸­å¿ƒå€åŸŸï¼ˆé è¿‘ 0ï¼‰åˆ»åº¦æ›´ç²¾ç´°ï¼Œè€Œåœ¨æ•¸æ“šç¨€ç–çš„å…©ç«¯å€åŸŸåˆ»åº¦è¼ƒç²—ã€‚\n",
    "\n",
    "3.  **é¡æ¯”**ï¼š\n",
    "    *   å‚³çµ± FP4 åƒä¸€æŠŠ**æ™®é€šå°º**ï¼Œåˆ»åº¦æ˜¯å‡å‹»çš„ã€‚\n",
    "    *   NF4 å‰‡åƒä¸€æŠŠ**ç‰¹è£½çš„å°æ•¸å°º**ï¼Œå°ˆç‚ºæ¸¬é‡å¸¸æ…‹åˆ†ä½ˆè€Œè¨­è¨ˆï¼Œåœ¨æœ€éœ€è¦çš„åœ°æ–¹æä¾›æœ€é«˜ç²¾åº¦ã€‚\n",
    "\n",
    "#### ä¸»è¦å„ªå‹¢\n",
    "\n",
    "*   **æ›´å°‘è³‡è¨Šæå¤±**ï¼šç›¸æ¯”å‚³çµ± 4-bit æ ¼å¼ï¼ŒNF4 åœ¨é‡åŒ–éç¨‹ä¸­èƒ½ä¿ç•™æ›´å¤šåŸå§‹æ¬Šé‡çš„æœ‰æ•ˆè³‡è¨Šã€‚\n",
    "*   **æ›´é«˜æ¨¡å‹æ€§èƒ½**ï¼šç”±æ–¼é‡åŒ–èª¤å·®æ›´å°ï¼Œä½¿ç”¨ QLoRA å¾®èª¿å¾Œçš„æ¨¡å‹è¡¨ç¾æ›´æ¥è¿‘æ–¼ä½¿ç”¨ bfloat16 ç­‰æ›´é«˜ç²¾åº¦çš„çµæœã€‚\n",
    "\n",
    "https://lh3.googleusercontent.com/d/1-Ms-lrWOQkFsPG_D9oMcA1UrWzVKSdC7\n",
    "https://i.ytimg.com/vi/aZPAqBov3tQ/maxresdefault.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2db96357b5449cab72c42c4d5104e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Base model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# --- Quantization Configuration ---\n",
    "# Load model in 4-bit\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# --- Load Base Model ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"âœ… Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure LoRA\n",
    "\n",
    "Now we configure LoRA using the `peft` library.\n",
    "\n",
    "#### Key Hugging Face `peft` Components:\n",
    "\n",
    "-   `peft.LoraConfig`: The main configuration class for LoRA.\n",
    "    -   `r`: The rank of the update matrices. A lower rank means fewer trainable parameters. A common range is 8-64.\n",
    "    -   `lora_alpha`: The scaling factor for the LoRA matrices. It's often set to twice the rank (`2*r`).\n",
    "    -   `target_modules`: A list of the names of the modules (e.g., attention layers) to apply LoRA to. For Llama models, this is typically `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`.\n",
    "    -   `lora_dropout`: Dropout probability for the LoRA layers to reduce overfitting.\n",
    "    -   `bias=\"none\"`: Specifies which biases to train. \"none\" is common.\n",
    "    -   `task_type=\"CAUSAL_LM\"`: Specifies the task type.\n",
    "-   `peft.get_peft_model`: This function takes the base model and the LoRA config and returns a PEFT model ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --- LoRA Configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# --- Create PEFT Model ---\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- Print Trainable Parameters ---\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set Up Training\n",
    "\n",
    "The final step is to configure and run the training process using the `transformers.Trainer`.\n",
    "\n",
    "#### Key Hugging Face Components:\n",
    "\n",
    "-   `transformers.TrainingArguments`: This class holds all the hyperparameters for the training run, such as learning rate, number of epochs, batch size, and logging settings.\n",
    "    -   `metric_for_best_model=\"perplexity\"`: We specify perplexity as the metric to determine the best model.\n",
    "    -   `greater_is_better=False`: Lower perplexity indicates better performance.\n",
    "-   `transformers.Trainer`: The standard trainer class from the `transformers` library. It requires a model, training arguments, datasets, a tokenizer, and a data collator.\n",
    "-   `transformers.DataCollatorForLanguageModeling`: This data collator will be used to form batches of tokenized data. It also handles the creation of the `labels` for causal language modeling, where the model predicts the next token.\n",
    "-   `compute_metrics`: A custom function to calculate **perplexity**, which is the standard evaluation metric for language modeling tasks. Perplexity measures how well the model predicts the next token - lower values indicate better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gradient configuration complete! Trainable: 4,194,304 / Total: 3,504,607,232\n",
      "ğŸ§¹ GPU memory cleared before training\n",
      "[2025-10-15 12:27:24,422] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting PRODUCTION QLoRA training with Early Stopping...\n",
      "ğŸ“Š Training config: 1 batch Ã— 32 accum = 32 effective batch\n",
      "ğŸ¯ Max epochs: 3, Max steps: 1000\n",
      "ğŸ“ˆ Evaluation every 50 steps\n",
      "â¹ï¸  Early stopping: patience=3, threshold=0.01\n",
      "ğŸ’¾ Memory-optimized: fp16=True, gradient_checkpointing=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  19/1000 13:38 < 13:07:36, 0.02 it/s, Epoch 0.64/35]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# --- Custom Evaluation Metrics ---\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute perplexity from the model's predictions.\n",
    "    Perplexity is the standard metric for language modeling tasks.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Convert to tensors\n",
    "    predictions = torch.from_numpy(predictions).float()\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "    \n",
    "    # Shift for causal language modeling\n",
    "    shift_logits = predictions[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    # Calculate cross entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), \n",
    "        shift_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    # Calculate perplexity\n",
    "    try:\n",
    "        perplexity = math.exp(loss.item())\n",
    "    except OverflowError:\n",
    "        perplexity = float('inf')\n",
    "    \n",
    "    return {\n",
    "        \"perplexity\": perplexity,\n",
    "        \"eval_loss\": loss.item()\n",
    "    }\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- Gradient & Model Config ---\n",
    "peft_model.train()\n",
    "peft_model.enable_input_require_grads()      # âœ… ç¢ºä¿æ¢¯åº¦å¯å›å‚³\n",
    "peft_model.gradient_checkpointing_enable()   # âœ… ç¯€çœé¡¯å­˜\n",
    "peft_model.config.use_cache = False          # âš ï¸ éœ€é—œé–‰ cache ä»¥å•Ÿç”¨åå‚³\n",
    "\n",
    "# Freeze all except LoRA layers\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if 'lora_' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "print(f\"âœ… Gradient configuration complete! Trainable: {trainable_params:,} / Total: {total_params:,}\")\n",
    "\n",
    "# ğŸ§¹ æ¸…ç†GPUè¨˜æ†¶é«”\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ğŸ§¹ GPU memory cleared before training\")\n",
    "\n",
    "# --- Production Training Arguments for RTX 2000 Ada (15.6 GB) ---\n",
    "# âœ… STABLE CONFIG: å·²ç¢ºèªè¨˜æ†¶é«”ç©©å®šï¼Œå‡ç´šè‡³æ­£å¼è¨“ç·´é…ç½® + Early Stopping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-llama2-7b-guanaco\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # ğŸ“ˆ å­¸ç¿’ç­–ç•¥\n",
    "    learning_rate=3e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,                       # 3% warmup for stable learning\n",
    "    weight_decay=0.01,                       # L2 regularization\n",
    "    \n",
    "    # ğŸ¯ è¨“ç·´è¨­å®š - å¹³è¡¡æ•ˆèƒ½èˆ‡ç©©å®šæ€§\n",
    "    per_device_train_batch_size=1,           # ä¿æŒæœ€å°ä»¥ç¢ºä¿è¨˜æ†¶é«”ç©©å®š\n",
    "    per_device_eval_batch_size=1,            # è©•ä¼°æ‰¹æ¬¡ä¹Ÿä¿æŒæœ€å°\n",
    "    gradient_accumulation_steps=32,          # æ¢å¾©åŸå€¼æ¨¡æ“¬ batch_size=16\n",
    "    max_grad_norm=1.0,                       # æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸\n",
    "    \n",
    "    # ğŸ• è¨“ç·´é€±æœŸ\n",
    "    num_train_epochs=3,                      # å¢åŠ  epochs è®“ early stopping æœ‰æ©Ÿæœƒç”Ÿæ•ˆ\n",
    "    max_steps=1000,                          # å¢åŠ æœ€å¤§æ­¥æ•¸ä¸Šé™\n",
    "    \n",
    "    # ğŸ’¾ è¨˜æ†¶é«”æœ€ä½³åŒ– (ä¿æŒç©©å®šé…ç½®)\n",
    "    fp16=True,                              # åŠç²¾åº¦è¨“ç·´\n",
    "    gradient_checkpointing=True,            # æ¢¯åº¦æª¢æŸ¥é»ç¯€çœè¨˜æ†¶é«”\n",
    "    optim=\"paged_adamw_8bit\",              # 8-bit å„ªåŒ–å™¨\n",
    "    dataloader_pin_memory=False,           # é—œé–‰ä»¥é¿å…RAMå£“åŠ›\n",
    "    dataloader_num_workers=0,              # å–®ç¨‹åºé¿å…è¨˜æ†¶é«”ç«¶çˆ­\n",
    "    remove_unused_columns=True,            # ç§»é™¤æœªä½¿ç”¨è³‡æ–™æ¬„ä½\n",
    "    \n",
    "    # ğŸ“Š è©•ä¼°èˆ‡å„²å­˜ç­–ç•¥\n",
    "    eval_strategy=\"steps\",                  # å®šæœŸè©•ä¼°\n",
    "    eval_steps=50,                         # ç¸®çŸ­è©•ä¼°é–“éš”ä»¥ä¾¿ early stopping æ›´æ•æ„Ÿ\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,                         # èˆ‡è©•ä¼°åŒæ­¥ä¿å­˜\n",
    "    save_total_limit=3,                    # ä¿ç•™æœ€è¿‘3å€‹æª¢æŸ¥é»\n",
    "    load_best_model_at_end=True,           # è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "    \n",
    "    # ğŸ“ è©•ä¼°æŒ‡æ¨™ & Early Stopping\n",
    "    metric_for_best_model=\"eval_perplexity\", # ä»¥å›°æƒ‘åº¦ç‚ºæœ€ä½³æ¨¡å‹æŒ‡æ¨™\n",
    "    greater_is_better=False,               # è¶Šä½è¶Šå¥½ (perplexity)\n",
    "    \n",
    "    # ğŸ“ æ—¥èªŒè¨­å®š\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,                      # æ¯10æ­¥è¨˜éŒ„ä¸€æ¬¡\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # ğŸ”• å ±å‘Šè¨­å®š\n",
    "    report_to=None,                        # ä¸ä¸Šå‚³è‡³å¤–éƒ¨å¹³å°\n",
    "    disable_tqdm=False,                    # ä¿ç•™é€²åº¦æ¢\n",
    "    \n",
    "    # ğŸš€ é€²éšè¨­å®š\n",
    "    push_to_hub=False,                     # ä¸æ¨é€è‡³ Hub\n",
    "    hub_token=None,\n",
    "    ignore_data_skip=True,                 # å¿½ç•¥è³‡æ–™è·³éè­¦å‘Š\n",
    ")\n",
    "\n",
    "# --- Early Stopping Callback Configuration ---\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,              # é€£çºŒ5æ¬¡è©•ä¼°ç„¡æ”¹å–„å°±åœæ­¢\n",
    "    early_stopping_threshold=0.01,          # æ”¹å–„é–¾å€¼ (perplexity ä¸‹é™ < 0.01 è¦–ç‚ºç„¡æ”¹å–„)\n",
    ")\n",
    "\n",
    "# --- Create Trainer with Early Stopping ---\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],     # æ¢å¾©è©•ä¼°è³‡æ–™é›†\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,             # æ¢å¾©æŒ‡æ¨™è¨ˆç®—\n",
    "    callbacks=[early_stopping_callback],        # âœ… åŠ å…¥ Early Stopping\n",
    ")\n",
    "\n",
    "# --- Start Production Training with Early Stopping ---\n",
    "print(\"ğŸš€ Starting PRODUCTION QLoRA training with Early Stopping...\")\n",
    "print(f\"ğŸ“Š Training config: {training_args.per_device_train_batch_size} batch Ã— {training_args.gradient_accumulation_steps} accum = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps} effective batch\")\n",
    "print(f\"ğŸ¯ Max epochs: {training_args.num_train_epochs}, Max steps: {training_args.max_steps}\")\n",
    "print(f\"ğŸ“ˆ Evaluation every {training_args.eval_steps} steps\")\n",
    "print(f\"â¹ï¸  Early stopping: patience={early_stopping_callback.early_stopping_patience}, threshold={early_stopping_callback.early_stopping_threshold}\")\n",
    "print(f\"ğŸ’¾ Memory-optimized: fp16={training_args.fp16}, gradient_checkpointing={training_args.gradient_checkpointing}\")\n",
    "\n",
    "# Train with automatic early stopping\n",
    "train_result = trainer.train()\n",
    "print(\"âœ… Production training complete!\")\n",
    "\n",
    "# Show training summary\n",
    "if hasattr(train_result, 'metrics'):\n",
    "    print(f\"\\nğŸ“ˆ Training Summary:\")\n",
    "    print(f\"Total steps completed: {train_result.metrics.get('train_steps', 'N/A')}\")\n",
    "    print(f\"Training runtime: {train_result.metrics.get('train_runtime', 'N/A'):.2f}s\" if 'train_runtime' in train_result.metrics else \"\")\n",
    "    \n",
    "    # Check if training was stopped early\n",
    "    if train_result.metrics.get('train_steps', 0) < training_args.max_steps:\n",
    "        print(\"â¹ï¸  Training stopped early due to no improvement!\")\n",
    "    else:\n",
    "        print(\"ğŸ Training completed full course!\")\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "print(\"\\nğŸ“Š Final Evaluation Results:\")\n",
    "final_metrics = trainer.evaluate()\n",
    "print(f\"Final Perplexity: {final_metrics['eval_perplexity']:.4f}\")\n",
    "print(f\"Final Loss: {final_metrics['eval_loss']:.4f}\")\n",
    "print(f\"Training Samples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Evaluation Samples: {len(tokenized_datasets['test'])}\")\n",
    "\n",
    "# Display early stopping information\n",
    "print(f\"\\nâ¹ï¸  Early Stopping Configuration:\")\n",
    "print(f\"   - Patience: {early_stopping_callback.early_stopping_patience} evaluations\")\n",
    "print(f\"   - Threshold: {early_stopping_callback.early_stopping_threshold} perplexity improvement\")\n",
    "print(f\"   - Evaluation frequency: Every {training_args.eval_steps} steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
