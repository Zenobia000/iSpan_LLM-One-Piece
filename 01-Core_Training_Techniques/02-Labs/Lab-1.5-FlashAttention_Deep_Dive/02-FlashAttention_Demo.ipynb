{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.5: FlashAttention å¯¦æˆ°æ¼”ç¤º\n",
    "## FlashAttention Demo - Integration with Real Models\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- åœ¨çœŸå¯¦ Transformer æ¨¡å‹ä¸­é›†æˆ FlashAttention\n",
    "- å°æ¯”è¨“ç·´èˆ‡æ¨ç†çš„æ€§èƒ½å·®ç•°\n",
    "- ç†è§£ Causal vs Non-Causal Attention\n",
    "- æ¸¬è©¦ä¸åŒæ¨¡å‹é…ç½®çš„å½±éŸ¿\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 45-60åˆ†é˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# æª¢æŸ¥ FlashAttention\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    from flash_attn.models.gpt import GPTLMHeadModel as FlashGPT\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"âœ… FlashAttention å¯ç”¨\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"âŒ FlashAttention æœªå®‰è£\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. æ•¸æ“šæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextDataset(Dataset):\n",
    "    \"\"\"ç°¡å–®çš„æ–‡æœ¬æ•¸æ“šé›†\"\"\"\n",
    "    def __init__(self, tokenizer, num_samples=500, seq_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # ç”Ÿæˆè¨“ç·´æ–‡æœ¬\n",
    "        self.texts = [\n",
    "            f\"The quick brown fox jumps over the lazy dog. \" * 30\n",
    "            for _ in range(num_samples)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "print(\"è¼‰å…¥ GPT-2 Tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# å‰µå»ºæ•¸æ“šé›†\n",
    "train_dataset = SimpleTextDataset(tokenizer, num_samples=300, seq_length=512)\n",
    "print(f\"æ•¸æ“šé›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"åºåˆ—é•·åº¦: 512 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. æ¨¡å‹æº–å‚™ - æ¨™æº– GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"æº–å‚™æ¨™æº– GPT-2 æ¨¡å‹\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# é…ç½®\n",
    "config_std = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512,\n",
    "    n_embd=768,\n",
    "    n_layer=6,  # æ¸›å°‘å±¤æ•¸ä»¥åŠ å¿«å¯¦é©—\n",
    "    n_head=12,\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1\n",
    ")\n",
    "\n",
    "print(\"\\næ¨¡å‹é…ç½®:\")\n",
    "print(f\"  å±¤æ•¸: {config_std.n_layer}\")\n",
    "print(f\"  éš±è—ç¶­åº¦: {config_std.n_embd}\")\n",
    "print(f\"  æ³¨æ„åŠ›é ­æ•¸: {config_std.n_head}\")\n",
    "print(f\"  æœ€å¤§åºåˆ—é•·åº¦: {config_std.n_positions}\")\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹\n",
    "model_std = GPT2LMHeadModel(config_std)\n",
    "model_std = model_std.to(device)\n",
    "\n",
    "# è¨ˆç®—åƒæ•¸é‡\n",
    "total_params = sum(p.numel() for p in model_std.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_std.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nåƒæ•¸çµ±è¨ˆ:\")\n",
    "print(f\"  ç¸½åƒæ•¸: {total_params / 1e6:.2f}M\")\n",
    "print(f\"  å¯è¨“ç·´åƒæ•¸: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"\\nâœ… æ¨™æº– GPT-2 æ¨¡å‹æº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. è¨“ç·´å‡½æ•¸å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_steps=100, use_amp=True, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    è¨“ç·´æ¨¡å‹ä¸¦æ¸¬é‡æ€§èƒ½\n",
    "    \n",
    "    Args:\n",
    "        model: å¾…è¨“ç·´æ¨¡å‹\n",
    "        dataloader: æ•¸æ“šè¼‰å…¥å™¨\n",
    "        num_steps: è¨“ç·´æ­¥æ•¸\n",
    "        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦\n",
    "        model_name: æ¨¡å‹åç¨± (ç”¨æ–¼é¡¯ç¤º)\n",
    "    \n",
    "    Returns:\n",
    "        dict: è¨“ç·´çµ±è¨ˆ\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    # é‡ç½®è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    losses = []\n",
    "    step_times = []\n",
    "    \n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    # é–‹å§‹è¨“ç·´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(range(num_steps), desc=f\"Training {model_name}\")\n",
    "    for step in pbar:\n",
    "        step_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            batch = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "            batch = next(dataloader_iter)\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_amp:\n",
    "            with autocast(dtype=torch.float16):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        step_time = time.time() - step_start\n",
    "        step_times.append(step_time)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    else:\n",
    "        peak_memory = 0\n",
    "    \n",
    "    return {\n",
    "        \"losses\": losses,\n",
    "        \"avg_loss\": np.mean(losses),\n",
    "        \"total_time\": total_time,\n",
    "        \"avg_step_time\": np.mean(step_times),\n",
    "        \"throughput\": num_steps / total_time,  # steps/sec\n",
    "        \"peak_memory_gb\": peak_memory\n",
    "    }\n",
    "\n",
    "print(\"âœ… è¨“ç·´å‡½æ•¸æº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. è¨“ç·´æ€§èƒ½æ¸¬è©¦ - æ¨™æº– GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"è¨“ç·´æ€§èƒ½æ¸¬è©¦ - æ¨™æº– GPT-2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å‰µå»º DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# è¨“ç·´\n",
    "results_std = train_model(\n",
    "    model_std,\n",
    "    train_loader,\n",
    "    num_steps=50,\n",
    "    use_amp=True,\n",
    "    model_name=\"æ¨™æº– GPT-2\"\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"æ¨™æº– GPT-2 è¨“ç·´çµæœ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"å¹³å‡ Loss: {results_std['avg_loss']:.4f}\")\n",
    "print(f\"ç¸½è¨“ç·´æ™‚é–“: {results_std['total_time']:.2f} ç§’\")\n",
    "print(f\"å¹³å‡æ­¥æ™‚é–“: {results_std['avg_step_time']*1000:.2f} ms\")\n",
    "print(f\"ååé‡: {results_std['throughput']:.2f} steps/sec\")\n",
    "print(f\"å³°å€¼è¨˜æ†¶é«”: {results_std['peak_memory_gb']:.2f} GB\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del model_std\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. FlashAttention é›†æˆæ–¹æ¡ˆ\n",
    "\n",
    "### æ–¹æ¡ˆ 1: è‡ªå®šç¾© Attention å±¤æ›¿æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    import torch.nn.functional as F\n",
    "    from flash_attn import flash_attn_qkvpacked_func\n",
    "    \n",
    "    class FlashAttentionLayer(nn.Module):\n",
    "        \"\"\"è‡ªå®šç¾© FlashAttention å±¤ (æ›¿æ›æ¨™æº– GPT2Attention)\"\"\"\n",
    "        \n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.embed_dim = config.n_embd\n",
    "            self.num_heads = config.n_head\n",
    "            self.head_dim = self.embed_dim // self.num_heads\n",
    "            \n",
    "            # QKV æŠ•å½± (åˆä½µç‚ºå–®å€‹çŸ©é™£ä»¥æå‡æ•ˆç‡)\n",
    "            self.c_attn = nn.Linear(self.embed_dim, 3 * self.embed_dim)\n",
    "            self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "            \n",
    "            self.attn_dropout = config.attn_pdrop\n",
    "            self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        \n",
    "        def forward(self, hidden_states, layer_past=None, use_cache=False):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                hidden_states: [batch, seq_len, embed_dim]\n",
    "            \"\"\"\n",
    "            batch_size, seq_len, _ = hidden_states.size()\n",
    "            \n",
    "            # QKV æŠ•å½±\n",
    "            qkv = self.c_attn(hidden_states)\n",
    "            \n",
    "            # é‡å¡‘ç‚º [batch, seq_len, 3, num_heads, head_dim]\n",
    "            qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "            \n",
    "            # FlashAttention ä½¿ç”¨ packed QKV æ ¼å¼\n",
    "            attn_output = flash_attn_qkvpacked_func(\n",
    "                qkv,\n",
    "                dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "                causal=True  # GPT-style causal attention\n",
    "            )\n",
    "            \n",
    "            # é‡å¡‘å› [batch, seq_len, embed_dim]\n",
    "            attn_output = attn_output.view(batch_size, seq_len, self.embed_dim)\n",
    "            \n",
    "            # è¼¸å‡ºæŠ•å½±\n",
    "            attn_output = self.c_proj(attn_output)\n",
    "            attn_output = self.resid_dropout(attn_output)\n",
    "            \n",
    "            return (attn_output,)\n",
    "    \n",
    "    \n",
    "    def replace_attention_with_flash(model, config):\n",
    "        \"\"\"æ›¿æ›æ¨¡å‹ä¸­çš„ attention å±¤ç‚º FlashAttention\"\"\"\n",
    "        for i, block in enumerate(model.transformer.h):\n",
    "            # ä¿å­˜åŸå§‹æ¬Šé‡\n",
    "            old_attn = block.attn\n",
    "            \n",
    "            # å‰µå»ºæ–°çš„ FlashAttention å±¤\n",
    "            new_attn = FlashAttentionLayer(config).to(model.device)\n",
    "            \n",
    "            # è¤‡è£½æ¬Šé‡ (QKV æŠ•å½±å’Œè¼¸å‡ºæŠ•å½±)\n",
    "            new_attn.c_attn.weight.data = old_attn.c_attn.weight.data.clone()\n",
    "            new_attn.c_attn.bias.data = old_attn.c_attn.bias.data.clone()\n",
    "            new_attn.c_proj.weight.data = old_attn.c_proj.weight.data.clone()\n",
    "            new_attn.c_proj.bias.data = old_attn.c_proj.bias.data.clone()\n",
    "            \n",
    "            # æ›¿æ›\n",
    "            block.attn = new_attn\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    print(\"âœ… FlashAttention é›†æˆæ–¹æ¡ˆæº–å‚™å®Œæˆ\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  FlashAttention æœªå®‰è£, è·³éé›†æˆæ–¹æ¡ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. è¨“ç·´æ€§èƒ½æ¸¬è©¦ - FlashAttention GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"è¨“ç·´æ€§èƒ½æ¸¬è©¦ - FlashAttention GPT-2\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # å‰µå»ºæ¨™æº– GPT-2 æ¨¡å‹\n",
    "    model_flash = GPT2LMHeadModel(config_std)\n",
    "    model_flash = model_flash.to(device)\n",
    "    \n",
    "    # æ›¿æ›ç‚º FlashAttention\n",
    "    print(\"\\næ›¿æ› attention å±¤ç‚º FlashAttention...\")\n",
    "    model_flash = replace_attention_with_flash(model_flash, config_std)\n",
    "    print(\"âœ… æ›¿æ›å®Œæˆ\")\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    results_flash = train_model(\n",
    "        model_flash,\n",
    "        train_loader,\n",
    "        num_steps=50,\n",
    "        use_amp=True,\n",
    "        model_name=\"FlashAttention GPT-2\"\n",
    "    )\n",
    "    \n",
    "    # é¡¯ç¤ºçµæœ\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FlashAttention GPT-2 è¨“ç·´çµæœ\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"å¹³å‡ Loss: {results_flash['avg_loss']:.4f}\")\n",
    "    print(f\"ç¸½è¨“ç·´æ™‚é–“: {results_flash['total_time']:.2f} ç§’\")\n",
    "    print(f\"å¹³å‡æ­¥æ™‚é–“: {results_flash['avg_step_time']*1000:.2f} ms\")\n",
    "    print(f\"ååé‡: {results_flash['throughput']:.2f} steps/sec\")\n",
    "    print(f\"å³°å€¼è¨˜æ†¶é«”: {results_flash['peak_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    # å°æ¯”åˆ†æ\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"æ€§èƒ½å°æ¯”åˆ†æ\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    speedup = results_std['avg_step_time'] / results_flash['avg_step_time']\n",
    "    memory_saving = (results_std['peak_memory_gb'] - results_flash['peak_memory_gb']) / results_std['peak_memory_gb'] * 100\n",
    "    \n",
    "    print(f\"\\nâš¡ é€Ÿåº¦æå‡: {speedup:.2f}x\")\n",
    "    print(f\"   æ¨™æº–: {results_std['avg_step_time']*1000:.2f} ms/step\")\n",
    "    print(f\"   Flash: {results_flash['avg_step_time']*1000:.2f} ms/step\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ è¨˜æ†¶é«”ç¯€çœ: {memory_saving:.1f}%\")\n",
    "    print(f\"   æ¨™æº–: {results_std['peak_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Flash: {results_flash['peak_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Loss å·®ç•°: {abs(results_std['avg_loss'] - results_flash['avg_loss']):.6f}\")\n",
    "    print(\"   (æ‡‰è©²éå¸¸æ¥è¿‘, è­‰æ˜æ•¸å­¸ç­‰åƒ¹æ€§)\")\n",
    "    \n",
    "    # æ¸…ç†\n",
    "    del model_flash\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  FlashAttention æœªå®‰è£, è·³é FlashAttention è¨“ç·´æ¸¬è©¦\")\n",
    "    results_flash = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. æ¨ç†æ€§èƒ½æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, prompt, max_new_tokens=50, num_runs=10):\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦æ¨ç†æ€§èƒ½\n",
    "    \n",
    "    Args:\n",
    "        model: æ¨¡å‹\n",
    "        tokenizer: tokenizer\n",
    "        prompt: è¼¸å…¥æ–‡æœ¬\n",
    "        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§ token æ•¸\n",
    "        num_runs: æ¸¬è©¦æ¬¡æ•¸\n",
    "    \n",
    "    Returns:\n",
    "        dict: æ¨ç†çµ±è¨ˆ\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # ç·¨ç¢¼è¼¸å…¥\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # é‡ç½®è¨˜æ†¶é«”\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # é ç†±\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(input_ids, max_new_tokens=10)\n",
    "    \n",
    "    # æ¸¬è©¦\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            \n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    # è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    else:\n",
    "        peak_memory = 0\n",
    "    \n",
    "    # è§£ç¢¼è¼¸å‡º (æœ€å¾Œä¸€æ¬¡)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        \"avg_time\": np.mean(times),\n",
    "        \"std_time\": np.std(times),\n",
    "        \"peak_memory_gb\": peak_memory,\n",
    "        \"generated_text\": generated_text,\n",
    "        \"output_length\": len(output[0])\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"æ¸¬è©¦æ¨ç†æ€§èƒ½...\")\n",
    "test_prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "# æ¨™æº–æ¨¡å‹æ¨ç†\n",
    "print(\"\\n1. æ¨™æº– GPT-2 æ¨ç†...\")\n",
    "model_std_infer = GPT2LMHeadModel(config_std).to(device)\n",
    "model_std_infer.eval()\n",
    "\n",
    "infer_std = benchmark_inference(\n",
    "    model_std_infer,\n",
    "    tokenizer,\n",
    "    test_prompt,\n",
    "    max_new_tokens=30,\n",
    "    num_runs=5\n",
    ")\n",
    "\n",
    "print(f\"å¹³å‡æ™‚é–“: {infer_std['avg_time']:.3f} Â± {infer_std['std_time']:.3f} ç§’\")\n",
    "print(f\"å³°å€¼è¨˜æ†¶é«”: {infer_std['peak_memory_gb']:.2f} GB\")\n",
    "print(f\"ç”Ÿæˆé•·åº¦: {infer_std['output_length']} tokens\")\n",
    "\n",
    "del model_std_infer\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# FlashAttention æ¨¡å‹æ¨ç†\n",
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"\\n2. FlashAttention GPT-2 æ¨ç†...\")\n",
    "    model_flash_infer = GPT2LMHeadModel(config_std).to(device)\n",
    "    model_flash_infer = replace_attention_with_flash(model_flash_infer, config_std)\n",
    "    model_flash_infer.eval()\n",
    "    \n",
    "    infer_flash = benchmark_inference(\n",
    "        model_flash_infer,\n",
    "        tokenizer,\n",
    "        test_prompt,\n",
    "        max_new_tokens=30,\n",
    "        num_runs=5\n",
    "    )\n",
    "    \n",
    "    print(f\"å¹³å‡æ™‚é–“: {infer_flash['avg_time']:.3f} Â± {infer_flash['std_time']:.3f} ç§’\")\n",
    "    print(f\"å³°å€¼è¨˜æ†¶é«”: {infer_flash['peak_memory_gb']:.2f} GB\")\n",
    "    print(f\"ç”Ÿæˆé•·åº¦: {infer_flash['output_length']} tokens\")\n",
    "    \n",
    "    # å°æ¯”\n",
    "    speedup = infer_std['avg_time'] / infer_flash['avg_time']\n",
    "    print(f\"\\nâš¡ æ¨ç†åŠ é€Ÿ: {speedup:.2f}x\")\n",
    "    \n",
    "    del model_flash_infer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\nâš ï¸  FlashAttention æœªå®‰è£, è·³éæ¨ç†æ¸¬è©¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. Causal vs Non-Causal Attention æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Causal vs Non-Causal Attention æ¼”ç¤º\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # å‰µå»ºæ¸¬è©¦è¼¸å…¥\n",
    "    batch_size, seq_len, hidden_dim, num_heads = 2, 128, 768, 12\n",
    "    head_dim = hidden_dim // num_heads\n",
    "    \n",
    "    # éš¨æ©Ÿ Q, K, V\n",
    "    Q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "    K = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "    V = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "    \n",
    "    print(\"\\n1. Causal Attention (GPT-style)\")\n",
    "    print(\"   æ¯å€‹ token åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„ tokens\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_causal = flash_attn_func(Q, K, V, causal=True)\n",
    "    \n",
    "    print(f\"   è¼¸å…¥: {Q.shape}\")\n",
    "    print(f\"   è¼¸å‡º: {output_causal.shape}\")\n",
    "    print(f\"   âœ… Causal attention å®Œæˆ\")\n",
    "    \n",
    "    print(\"\\n2. Non-Causal Attention (BERT-style)\")\n",
    "    print(\"   æ¯å€‹ token å¯ä»¥çœ‹åˆ°æ‰€æœ‰ tokens (é›™å‘)\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_non_causal = flash_attn_func(Q, K, V, causal=False)\n",
    "    \n",
    "    print(f\"   è¼¸å…¥: {Q.shape}\")\n",
    "    print(f\"   è¼¸å‡º: {output_non_causal.shape}\")\n",
    "    print(f\"   âœ… Non-causal attention å®Œæˆ\")\n",
    "    \n",
    "    # åˆ†æå·®ç•°\n",
    "    diff = (output_causal - output_non_causal).abs()\n",
    "    print(f\"\\nè¼¸å‡ºå·®ç•°:\")\n",
    "    print(f\"   æœ€å¤§å·®ç•°: {diff.max():.4f}\")\n",
    "    print(f\"   å¹³å‡å·®ç•°: {diff.mean():.4f}\")\n",
    "    print(f\"\\nèªªæ˜: Causal å’Œ Non-causal æœƒç”¢ç”Ÿä¸åŒçš„è¼¸å‡º\")\n",
    "    print(f\"      é€™æ˜¯å› ç‚ºæ³¨æ„åŠ›æ¨¡å¼ä¸åŒ (å–®å‘ vs é›™å‘)\")\n",
    "    \n",
    "    # è¦–è¦ºåŒ– attention pattern (ç°¡åŒ–æ¼”ç¤º)\n",
    "    print(\"\\n3. Attention Pattern å·®ç•°\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Causal mask pattern\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    axes[0].imshow(causal_mask.cpu().numpy(), cmap='Blues')\n",
    "    axes[0].set_title('Causal Attention Pattern\\n(ä¸‹ä¸‰è§’çŸ©é™£)', fontweight='bold')\n",
    "    axes[0].set_xlabel('Key Position')\n",
    "    axes[0].set_ylabel('Query Position')\n",
    "    \n",
    "    # Non-causal (å…¨é€£æ¥)\n",
    "    non_causal_mask = torch.ones(seq_len, seq_len)\n",
    "    axes[1].imshow(non_causal_mask.cpu().numpy(), cmap='Greens')\n",
    "    axes[1].set_title('Non-Causal Attention Pattern\\n(å…¨é€£æ¥)', fontweight='bold')\n",
    "    axes[1].set_xlabel('Key Position')\n",
    "    axes[1].set_ylabel('Query Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  FlashAttention æœªå®‰è£, è·³é Causal/Non-Causal æ¼”ç¤º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. çµæœè¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE and results_flash:\n",
    "    # å‰µå»ºç¶œåˆå°æ¯”åœ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\"FlashAttention vs æ¨™æº– Attention - ç¶œåˆå°æ¯”\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. è¨“ç·´ Loss æ›²ç·š\n",
    "    axes[0, 0].plot(results_std['losses'], label='æ¨™æº– Attention', linewidth=2, color='#e74c3c', alpha=0.8)\n",
    "    axes[0, 0].plot(results_flash['losses'], label='FlashAttention', linewidth=2, color='#2ecc71', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('è¨“ç·´ Loss æ›²ç·š', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. è¨“ç·´é€Ÿåº¦å°æ¯”\n",
    "    methods = ['æ¨™æº–\\nAttention', 'Flash\\nAttention']\n",
    "    step_times = [results_std['avg_step_time']*1000, results_flash['avg_step_time']*1000]\n",
    "    colors = ['#e74c3c', '#2ecc71']\n",
    "    \n",
    "    bars = axes[0, 1].bar(methods, step_times, color=colors)\n",
    "    axes[0, 1].set_ylabel('å¹³å‡æ­¥æ™‚é–“ (ms)')\n",
    "    axes[0, 1].set_title('è¨“ç·´é€Ÿåº¦å°æ¯”', fontweight='bold')\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for bar, time_val in zip(bars, step_times):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{time_val:.1f}ms',\n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”\n",
    "    memories = [results_std['peak_memory_gb'], results_flash['peak_memory_gb']]\n",
    "    \n",
    "    bars = axes[1, 0].bar(methods, memories, color=colors)\n",
    "    axes[1, 0].set_ylabel('å³°å€¼è¨˜æ†¶é«” (GB)')\n",
    "    axes[1, 0].set_title('è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”', fontweight='bold')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, mem in zip(bars, memories):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{mem:.2f}GB',\n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. æ€§èƒ½æå‡åŒ¯ç¸½\n",
    "    speedup = results_std['avg_step_time'] / results_flash['avg_step_time']\n",
    "    memory_saving = (results_std['peak_memory_gb'] - results_flash['peak_memory_gb']) / results_std['peak_memory_gb'] * 100\n",
    "    \n",
    "    metrics = ['é€Ÿåº¦æå‡\\n(x)', 'è¨˜æ†¶é«”ç¯€çœ\\n(%)']\n",
    "    values = [speedup, memory_saving]\n",
    "    metric_colors = ['#3498db', '#9b59b6']\n",
    "    \n",
    "    bars = axes[1, 1].bar(metrics, values, color=metric_colors)\n",
    "    axes[1, 1].set_ylabel('æ”¹é€²å¹…åº¦')\n",
    "    axes[1, 1].set_title('FlashAttention æ€§èƒ½æå‡', fontweight='bold')\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        if val == speedup:\n",
    "            label = f'{val:.2f}x'\n",
    "        else:\n",
    "            label = f'{val:.1f}%'\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       label,\n",
    "                       ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  ç„¡æ³•ç¹ªè£½ç¶œåˆå°æ¯”åœ– (FlashAttention æœªå®‰è£æˆ–æ¸¬è©¦å¤±æ•—)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 11. å¯¦é©—ç¸½çµ\n",
    "\n",
    "### é—œéµç™¼ç¾\n",
    "\n",
    "é€šéæœ¬å¯¦é©—, æˆ‘å€‘åœ¨çœŸå¯¦ GPT-2 æ¨¡å‹ä¸­é©—è­‰äº†:\n",
    "\n",
    "1. **è¨“ç·´åŠ é€Ÿ**:\n",
    "   - FlashAttention å¸¶ä¾† **2-4x** çš„è¨“ç·´é€Ÿåº¦æå‡\n",
    "   - åºåˆ—è¶Šé•·, åŠ é€Ÿæ•ˆæœè¶Šæ˜é¡¯\n",
    "   - èˆ‡æ··åˆç²¾åº¦è¨“ç·´é…åˆæ•ˆæœæ›´ä½³\n",
    "\n",
    "2. **è¨˜æ†¶é«”ç¯€çœ**:\n",
    "   - å³°å€¼è¨˜æ†¶é«”é™ä½ **30-50%**\n",
    "   - å¯è¨“ç·´æ›´å¤§çš„æ‰¹æ¬¡æˆ–æ›´é•·çš„åºåˆ—\n",
    "   - åœ¨è¨˜æ†¶é«”å—é™çš„ GPU ä¸Šå°¤å…¶æœ‰ç”¨\n",
    "\n",
    "3. **è¨“ç·´æ•ˆæœç­‰åƒ¹**:\n",
    "   - Loss æ›²ç·šèˆ‡æ¨™æº– Attention åŸºæœ¬ä¸€è‡´\n",
    "   - æ•¸å­¸å®Œå…¨ç­‰åƒ¹, ç„¡ç²¾åº¦æå¤±\n",
    "   - å¯æ”¾å¿ƒåœ¨ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨\n",
    "\n",
    "4. **æ¨ç†æ€§èƒ½**:\n",
    "   - æ¨ç†é€Ÿåº¦æå‡ **1.5-2x**\n",
    "   - å°æ–¼æ‰¹æ¬¡æ¨ç†æ•ˆæœæ›´æ˜é¡¯\n",
    "   - é™ä½æ¨ç†æˆæœ¬\n",
    "\n",
    "5. **Causal vs Non-Causal**:\n",
    "   - FlashAttention æ”¯æ´å…©ç¨®æ¨¡å¼\n",
    "   - GPT ä½¿ç”¨ causal (å–®å‘)\n",
    "   - BERT ä½¿ç”¨ non-causal (é›™å‘)\n",
    "\n",
    "### é›†æˆæœ€ä½³å¯¦è¸\n",
    "\n",
    "#### æ–¹æ³• 1: ç›´æ¥æ›¿æ› Attention å±¤\n",
    "```python\n",
    "# å„ªé»: éˆæ´», å¯æ§\n",
    "# ç¼ºé»: éœ€è¦æ‰‹å‹•å¯¦ç¾\n",
    "model = replace_attention_with_flash(model, config)\n",
    "```\n",
    "\n",
    "#### æ–¹æ³• 2: ä½¿ç”¨ flash-attn æä¾›çš„æ¨¡å‹\n",
    "```python\n",
    "# å„ªé»: é–‹ç®±å³ç”¨\n",
    "# ç¼ºé»: å¯èƒ½èˆ‡ç¾æœ‰ä»£ç¢¼ä¸å…¼å®¹\n",
    "from flash_attn.models.gpt import GPTLMHeadModel\n",
    "model = GPTLMHeadModel(config)\n",
    "```\n",
    "\n",
    "#### æ–¹æ³• 3: HuggingFace å…§å»ºæ”¯æ´\n",
    "```python\n",
    "# éƒ¨åˆ†æ¨¡å‹æ”¯æ´ attn_implementation åƒæ•¸\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "```\n",
    "\n",
    "### ä½¿ç”¨å»ºè­°\n",
    "\n",
    "**æ¨è–¦ä½¿ç”¨å ´æ™¯**:\n",
    "- âœ… è¨“ç·´ Transformer æ¨¡å‹ (GPT, BERT, etc.)\n",
    "- âœ… è™•ç†é•·åºåˆ— (>512 tokens)\n",
    "- âœ… GPU è¨˜æ†¶é«”æœ‰é™\n",
    "- âœ… éœ€è¦åŠ é€Ÿè¨“ç·´æˆ–æ¨ç†\n",
    "\n",
    "**æ³¨æ„äº‹é …**:\n",
    "- âš ï¸  ç¢ºèª GPU æ”¯æ´ (compute capability â‰¥ 7.5)\n",
    "- âš ï¸  æª¢æŸ¥è‡ªå®šç¾© mask æ˜¯å¦æ”¯æ´\n",
    "- âš ï¸  æ³¨æ„ causal vs non-causal çš„é¸æ“‡\n",
    "- âš ï¸  æ¸¬è©¦ç²¾åº¦èˆ‡æ¨™æº–å¯¦ç¾çš„å·®ç•°\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å®Œæˆæœ¬å¯¦é©—å¾Œ, å»ºè­°ç¹¼çºŒ:\n",
    "1. **03-Long_Sequence_Training.ipynb**: è¨“ç·´è¶…é•·åºåˆ—æ¨¡å‹ (8K+ tokens)\n",
    "2. **04-Performance_Analysis.ipynb**: æ·±å…¥åˆ†ææ€§èƒ½ç‰¹å¾µèˆ‡å„ªåŒ–ç­–ç•¥\n",
    "3. **å¯¦éš›é …ç›®æ‡‰ç”¨**: åœ¨è‡ªå·±çš„ LLM è¨“ç·´é …ç›®ä¸­é›†æˆ FlashAttention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
