#!/usr/bin/env python3
"""
æ•¸æ“šé›†é¡å‹åˆ†æå·¥å…·
åˆ†æä¸åŒé¡å‹LLMæ•¸æ“šé›†çš„ç‰¹æ€§å’Œè³ªé‡
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset, Dataset
import re
from collections import Counter, defaultdict
import langdetect
from typing import Dict, List, Tuple
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class DatasetAnalyzer:
    """æ•¸æ“šé›†åˆ†æå™¨"""

    def __init__(self):
        self.analysis_results = {}

    def analyze_pretraining_dataset(self, dataset_name: str = "wikitext",
                                  config: str = "wikitext-2-raw-v1",
                                  max_samples: int = 1000) -> Dict:
        """
        åˆ†æé è¨“ç·´æ•¸æ“šé›†

        åˆ†æç¶­åº¦ï¼š
        - åŸºç¤çµ±è¨ˆï¼šé•·åº¦åˆ†ä½ˆã€è©å½™çµ±è¨ˆ
        - å…§å®¹è³ªé‡ï¼šé‡è¤‡ç‡ã€å®Œæ•´æ€§ã€æ¸…æ½”åº¦
        - èªè¨€åˆ†ä½ˆï¼šå¤šèªè¨€æª¢æ¸¬
        - ä¸»é¡Œå¤šæ¨£æ€§ï¼šå…§å®¹ä¸»é¡Œåˆ†æ
        """

        print(f"=== åˆ†æé è¨“ç·´æ•¸æ“šé›†: {dataset_name} ===")

        try:
            # è¼‰å…¥æ•¸æ“šé›†
            dataset = load_dataset(dataset_name, config, split=f"train[:{max_samples}]")
            print(f"æˆåŠŸè¼‰å…¥ {len(dataset)} å€‹æ¨£æœ¬")

        except Exception as e:
            print(f"æ•¸æ“šé›†è¼‰å…¥å¤±æ•—ï¼Œå‰µå»ºæ¨¡æ“¬æ•¸æ“š: {e}")
            dataset = self._create_mock_pretraining_data(max_samples)

        # åŸ·è¡Œå„é …åˆ†æ
        analysis_result = {
            'dataset_info': {
                'name': dataset_name,
                'config': config,
                'total_samples': len(dataset),
                'analysis_timestamp': datetime.now().isoformat()
            }
        }

        # åŸºç¤çµ±è¨ˆåˆ†æ
        analysis_result['basic_statistics'] = self._analyze_basic_statistics(dataset)

        # å…§å®¹è³ªé‡åˆ†æ
        analysis_result['content_quality'] = self._analyze_content_quality(dataset)

        # èªè¨€åˆ†ä½ˆåˆ†æ
        analysis_result['language_distribution'] = self._analyze_language_distribution(dataset)

        # è©å½™åˆ†æ
        analysis_result['vocabulary_analysis'] = self._analyze_vocabulary(dataset)

        # ä¸»é¡Œå¤šæ¨£æ€§åˆ†æ
        analysis_result['topic_diversity'] = self._analyze_topic_diversity(dataset)

        self.analysis_results['pretraining_dataset'] = analysis_result

        return analysis_result

    def analyze_instruction_dataset(self, dataset_name: str = None, max_samples: int = 500) -> Dict:
        """
        åˆ†ææŒ‡ä»¤å¾®èª¿æ•¸æ“šé›†

        åˆ†æç¶­åº¦ï¼š
        - æŒ‡ä»¤é¡å‹åˆ†ä½ˆ
        - é•·åº¦çµ±è¨ˆåˆ†æ
        - è¤‡é›œåº¦è©•ä¼°
        - è³ªé‡å•é¡Œè­˜åˆ¥
        """

        print(f"=== åˆ†ææŒ‡ä»¤æ•¸æ“šé›† ===")

        try:
            if dataset_name:
                dataset = load_dataset(dataset_name, split=f"train[:{max_samples}]")
            else:
                dataset = self._create_mock_instruction_data(max_samples)

            print(f"è¼‰å…¥ {len(dataset)} å€‹æŒ‡ä»¤æ¨£æœ¬")

        except Exception as e:
            print(f"ä½¿ç”¨æ¨¡æ“¬æŒ‡ä»¤æ•¸æ“š: {e}")
            dataset = self._create_mock_instruction_data(max_samples)

        analysis_result = {
            'dataset_info': {
                'name': dataset_name or 'mock_instruction_data',
                'total_samples': len(dataset),
                'analysis_timestamp': datetime.now().isoformat()
            }
        }

        # æŒ‡ä»¤é¡å‹åˆ†æ
        analysis_result['instruction_types'] = self._analyze_instruction_types(dataset)

        # é•·åº¦åˆ†ä½ˆåˆ†æ
        analysis_result['length_distribution'] = self._analyze_instruction_lengths(dataset)

        # è¤‡é›œåº¦åˆ†æ
        analysis_result['complexity_analysis'] = self._analyze_instruction_complexity(dataset)

        # è³ªé‡å•é¡Œæª¢æ¸¬
        analysis_result['quality_issues'] = self._detect_instruction_quality_issues(dataset)

        self.analysis_results['instruction_dataset'] = analysis_result

        return analysis_result

    def analyze_preference_dataset(self, max_samples: int = 200) -> Dict:
        """
        åˆ†æåå¥½å°é½Šæ•¸æ“šé›†

        åˆ†æç¶­åº¦ï¼š
        - åå¥½æ¨™è¨»ä¸€è‡´æ€§
        - å›ç­”è³ªé‡åˆ†ä½ˆ
        - å®‰å…¨æ€§å•é¡Œè­˜åˆ¥
        """

        print("=== åˆ†æåå¥½å°é½Šæ•¸æ“šé›† ===")

        # å‰µå»ºæ¨¡æ“¬åå¥½æ•¸æ“š
        dataset = self._create_mock_preference_data(max_samples)

        analysis_result = {
            'dataset_info': {
                'name': 'mock_preference_data',
                'total_samples': len(dataset),
                'analysis_timestamp': datetime.now().isoformat()
            }
        }

        # åå¥½ä¸€è‡´æ€§åˆ†æ
        analysis_result['preference_consistency'] = self._analyze_preference_consistency(dataset)

        # å›ç­”è³ªé‡åˆ†æ
        analysis_result['response_quality'] = self._analyze_response_quality(dataset)

        # å®‰å…¨æ€§åˆ†æ
        analysis_result['safety_analysis'] = self._analyze_safety_aspects(dataset)

        self.analysis_results['preference_dataset'] = analysis_result

        return analysis_result

    def _create_mock_pretraining_data(self, num_samples: int) -> Dataset:
        """å‰µå»ºæ¨¡æ“¬é è¨“ç·´æ•¸æ“š"""

        sample_texts = [
            "äººå·¥æ™ºèƒ½æŠ€è¡“æ­£åœ¨å¿«é€Ÿç™¼å±•ï¼Œæ·±åˆ»æ”¹è®Šè‘—æˆ‘å€‘çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚å¾æ™ºèƒ½æ‰‹æ©Ÿåˆ°è‡ªå‹•é§•é§›æ±½è»Šï¼ŒAIçš„æ‡‰ç”¨ç„¡è™•ä¸åœ¨ã€‚",
            "æ©Ÿå™¨å­¸ç¿’æ˜¯AIçš„æ ¸å¿ƒæŠ€è¡“ä¹‹ä¸€ï¼Œé€šéç®—æ³•è®“è¨ˆç®—æ©Ÿå¾æ•¸æ“šä¸­å­¸ç¿’æ¨¡å¼ï¼Œå¾è€Œåšå‡ºé æ¸¬å’Œæ±ºç­–ã€‚",
            "æ·±åº¦å­¸ç¿’ä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†æ¨¡æ“¬äººè…¦çš„ä¿¡æ¯è™•ç†æ–¹å¼ï¼Œåœ¨åœ–åƒè­˜åˆ¥å’Œè‡ªç„¶èªè¨€è™•ç†é ˜åŸŸå–å¾—äº†çªç ´æ€§é€²å±•ã€‚",
            "è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä½¿è¨ˆç®—æ©Ÿèƒ½å¤ ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ï¼Œé€™ç‚ºäººæ©Ÿäº¤äº’é–‹é—¢äº†æ–°çš„å¯èƒ½æ€§ã€‚",
            "è¨ˆç®—æ©Ÿè¦–è¦ºæŠ€è¡“è®“æ©Ÿå™¨èƒ½å¤ è­˜åˆ¥å’Œç†è§£åœ–åƒå…§å®¹ï¼Œå»£æ³›æ‡‰ç”¨æ–¼é†«ç™‚è¨ºæ–·ã€å®‰é˜²ç›£æ§ç­‰é ˜åŸŸã€‚",
            "é‡å­è¨ˆç®—åˆ©ç”¨é‡å­åŠ›å­¸åŸç†é€²è¡Œè¨ˆç®—ï¼Œåœ¨æŸäº›ç‰¹å®šå•é¡Œä¸Šå¯èƒ½å¯¦ç¾æŒ‡æ•¸ç´šçš„è¨ˆç®—å„ªå‹¢ã€‚",
            "å€å¡ŠéˆæŠ€è¡“é€šéåˆ†æ•£å¼è¨˜è³¬å’Œå¯†ç¢¼å­¸ä¿è­‰æ•¸æ“šå®‰å…¨ï¼Œæ­£åœ¨é‡‘èã€ä¾›æ‡‰éˆç­‰è¡Œæ¥­å¾—åˆ°æ‡‰ç”¨ã€‚",
            "é›²è¨ˆç®—æä¾›æŒ‰éœ€åˆ†é…çš„è¨ˆç®—è³‡æºï¼Œé™ä½äº†ä¼æ¥­çš„ITæˆæœ¬ä¸¦æé«˜äº†éˆæ´»æ€§ã€‚"
        ]

        # æ“´å±•æ•¸æ“šé›†
        expanded_texts = []
        for i in range(num_samples):
            base_text = sample_texts[i % len(sample_texts)]

            # æ·»åŠ ä¸€äº›è®ŠåŒ–
            if i % 3 == 0:
                expanded_texts.append(base_text)
            elif i % 3 == 1:
                expanded_texts.append(base_text + " é€™é …æŠ€è¡“çš„ç™¼å±•å‰æ™¯å€¼å¾—æœŸå¾…ã€‚")
            else:
                expanded_texts.append(base_text[:len(base_text)//2])  # å‰µå»ºä¸€äº›çŸ­æ–‡æœ¬

        return Dataset.from_dict({'text': expanded_texts})

    def _create_mock_instruction_data(self, num_samples: int) -> Dataset:
        """å‰µå»ºæ¨¡æ“¬æŒ‡ä»¤æ•¸æ“š"""

        instruction_templates = [
            {
                "instruction": "è§£é‡‹ä»¥ä¸‹æ¦‚å¿µ",
                "input": "æ·±åº¦å­¸ç¿’",
                "output": "æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„å­é ˜åŸŸï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’æ•¸æ“šè¡¨ç¤ºã€‚"
            },
            {
                "instruction": "ç¿»è­¯ä»¥ä¸‹æ–‡æœ¬",
                "input": "Hello world",
                "output": "ä½ å¥½ä¸–ç•Œ"
            },
            {
                "instruction": "å›ç­”å•é¡Œ",
                "input": "Pythonæœ‰ä»€éº¼å„ªé»ï¼Ÿ",
                "output": "Pythonèªæ³•ç°¡æ½”ã€æ˜“å­¸æ˜“ç”¨ã€ç”Ÿæ…‹è±å¯Œã€è·¨å¹³å°æ€§å¥½ã€‚"
            },
            {
                "instruction": "ç¸½çµè¦é»",
                "input": "äººå·¥æ™ºèƒ½åŒ…æ‹¬æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ã€è‡ªç„¶èªè¨€è™•ç†ç­‰æŠ€è¡“ã€‚",
                "output": "ä¸»è¦è¦é»ï¼š1. AIåŒ…å«å¤šå€‹æŠ€è¡“é ˜åŸŸ 2. æ©Ÿå™¨å­¸ç¿’æ˜¯æ ¸å¿ƒæŠ€è¡“ 3. æ‡‰ç”¨ç¯„åœå»£æ³›"
            },
            {
                "instruction": "ç·¨å¯«ä»£ç¢¼",
                "input": "å¯«ä¸€å€‹è¨ˆç®—éšä¹˜çš„å‡½æ•¸",
                "output": "def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)"
            }
        ]

        # æ“´å±•æ•¸æ“šé›†
        instructions = []
        inputs = []
        outputs = []

        for i in range(num_samples):
            template = instruction_templates[i % len(instruction_templates)]
            instructions.append(template['instruction'])
            inputs.append(template['input'])
            outputs.append(template['output'])

        return Dataset.from_dict({
            'instruction': instructions,
            'input': inputs,
            'output': outputs
        })

    def _create_mock_preference_data(self, num_samples: int) -> Dataset:
        """å‰µå»ºæ¨¡æ“¬åå¥½æ•¸æ“š"""

        preference_examples = [
            {
                'prompt': 'è§£é‡‹é‡å­è¨ˆç®—',
                'response_a': 'é‡å­è¨ˆç®—å¾ˆè¤‡é›œã€‚',
                'response_b': 'é‡å­è¨ˆç®—åˆ©ç”¨é‡å­åŠ›å­¸åŸç†é€²è¡Œè¨ˆç®—ï¼Œåœ¨æŸäº›å•é¡Œä¸Šå…·æœ‰æŒ‡æ•¸ç´šå„ªå‹¢ã€‚',
                'preference': 'B'
            },
            {
                'prompt': 'å¦‚ä½•å­¸ç¿’ç·¨ç¨‹ï¼Ÿ',
                'response_a': 'å¤šç·´ç¿’ï¼Œå¾åŸºç¤é–‹å§‹ï¼Œé¸æ“‡åˆé©çš„èªè¨€ï¼Œå …æŒå­¸ç¿’ã€‚',
                'response_b': 'éš¨ä¾¿å­¸å­¸å°±è¡Œã€‚',
                'preference': 'A'
            }
        ] * (num_samples // 2)

        return Dataset.from_dict({
            'prompt': [item['prompt'] for item in preference_examples],
            'response_a': [item['response_a'] for item in preference_examples],
            'response_b': [item['response_b'] for item in preference_examples],
            'preference': [item['preference'] for item in preference_examples]
        })

    def _analyze_basic_statistics(self, dataset) -> Dict:
        """åŸºç¤çµ±è¨ˆåˆ†æ"""

        texts = dataset['text']

        # é•·åº¦çµ±è¨ˆ
        char_lengths = [len(text) for text in texts]
        word_counts = [len(text.split()) for text in texts]

        # éç©ºæ–‡æœ¬çµ±è¨ˆ
        non_empty_texts = [text for text in texts if text.strip()]

        stats = {
            'total_samples': len(texts),
            'non_empty_samples': len(non_empty_texts),
            'empty_samples': len(texts) - len(non_empty_texts),
            'avg_char_length': np.mean(char_lengths),
            'median_char_length': np.median(char_lengths),
            'max_char_length': np.max(char_lengths),
            'min_char_length': np.min(char_lengths),
            'std_char_length': np.std(char_lengths),
            'avg_word_count': np.mean(word_counts),
            'median_word_count': np.median(word_counts),
            'std_word_count': np.std(word_counts)
        }

        return stats

    def _analyze_content_quality(self, dataset) -> Dict:
        """å…§å®¹è³ªé‡åˆ†æ"""

        texts = dataset['text']
        quality_issues = {
            'very_short_texts': 0,
            'very_long_texts': 0,
            'repetitive_texts': 0,
            'special_char_heavy_texts': 0,
            'potential_quality_issues': []
        }

        for i, text in enumerate(texts):
            text_len = len(text.strip())

            # é•·åº¦å•é¡Œ
            if text_len < 10:
                quality_issues['very_short_texts'] += 1
                if text_len > 0:
                    quality_issues['potential_quality_issues'].append(f"æ¨£æœ¬{i}: æ–‡æœ¬éçŸ­")
            elif text_len > 5000:
                quality_issues['very_long_texts'] += 1
                quality_issues['potential_quality_issues'].append(f"æ¨£æœ¬{i}: æ–‡æœ¬éé•·")

            # é‡è¤‡æ€§æª¢æŸ¥
            words = text.lower().split()
            if len(words) > 5:
                unique_words = len(set(words))
                repetition_ratio = unique_words / len(words)
                if repetition_ratio < 0.5:
                    quality_issues['repetitive_texts'] += 1
                    quality_issues['potential_quality_issues'].append(f"æ¨£æœ¬{i}: é‡è¤‡å…§å®¹éå¤š")

            # ç‰¹æ®Šå­—ç¬¦æª¢æŸ¥
            special_chars = re.findall(r'[^\w\s\u4e00-\u9fff]', text)
            if len(special_chars) > len(text) * 0.3:
                quality_issues['special_char_heavy_texts'] += 1
                quality_issues['potential_quality_issues'].append(f"æ¨£æœ¬{i}: ç‰¹æ®Šå­—ç¬¦éå¤š")

        # è¨ˆç®—è³ªé‡åˆ†æ•¸
        total_samples = len(texts)
        quality_score = 1.0 - (
            quality_issues['very_short_texts'] +
            quality_issues['repetitive_texts'] +
            quality_issues['special_char_heavy_texts']
        ) / total_samples

        quality_issues['overall_quality_score'] = max(0, quality_score)
        quality_issues['quality_grade'] = self._grade_quality(quality_score)

        return quality_issues

    def _analyze_language_distribution(self, dataset) -> Dict:
        """èªè¨€åˆ†ä½ˆåˆ†æ"""

        texts = dataset['text']
        language_counts = Counter()
        detection_failures = 0

        # é™åˆ¶æ¨£æœ¬æ•¸é‡ä»¥æé«˜é€Ÿåº¦
        sample_size = min(200, len(texts))
        sample_texts = texts[:sample_size]

        for i, text in enumerate(sample_texts):
            if not text.strip() or len(text.strip()) < 20:
                continue

            try:
                detected_lang = langdetect.detect(text)
                language_counts[detected_lang] += 1
            except Exception:
                detection_failures += 1

        # è½‰æ›ç‚ºç™¾åˆ†æ¯”
        total_detected = sum(language_counts.values())
        language_percentages = {
            lang: count / total_detected * 100
            for lang, count in language_counts.items()
        } if total_detected > 0 else {}

        return {
            'language_counts': dict(language_counts),
            'language_percentages': language_percentages,
            'detection_failures': detection_failures,
            'samples_analyzed': sample_size,
            'dominant_language': language_counts.most_common(1)[0] if language_counts else None
        }

    def _analyze_vocabulary(self, dataset) -> Dict:
        """è©å½™åˆ†æ"""

        # åˆä½µæ‰€æœ‰æ–‡æœ¬
        all_text = " ".join([text for text in dataset['text'] if text.strip()])

        # åˆ†è©ï¼ˆç°¡å–®åŸºæ–¼ç©ºæ ¼ï¼‰
        words = re.findall(r'\b\w+\b', all_text.lower())

        vocabulary_stats = {
            'total_words': len(words),
            'unique_words': len(set(words)),
            'vocabulary_diversity': len(set(words)) / len(words) if words else 0
        }

        # è©é »åˆ†æ
        word_freq = Counter(words)
        most_common_words = word_freq.most_common(20)

        # Zipfå®šå¾‹æª¢é©—ï¼ˆè©é »åˆ†ä½ˆè¦å¾‹ï¼‰
        ranks = list(range(1, len(most_common_words) + 1))
        frequencies = [freq for word, freq in most_common_words]

        zipf_correlation = np.corrcoef(np.log(ranks), np.log(frequencies))[0, 1] if len(ranks) > 1 else 0

        return {
            'vocabulary_stats': vocabulary_stats,
            'most_common_words': most_common_words,
            'zipf_correlation': zipf_correlation,
            'vocabulary_richness': len(set(words)) / len(word_freq) if word_freq else 0
        }

    def _analyze_topic_diversity(self, dataset) -> Dict:
        """ä¸»é¡Œå¤šæ¨£æ€§åˆ†æ"""

        texts = dataset['text']

        # é—œéµè©åˆ†é¡ï¼ˆç°¡åŒ–çš„ä¸»é¡Œè­˜åˆ¥ï¼‰
        topic_keywords = {
            'ç§‘æŠ€': ['æŠ€è¡“', 'ç§‘æŠ€', 'è¨ˆç®—æ©Ÿ', 'äººå·¥æ™ºèƒ½', 'æ©Ÿå™¨å­¸ç¿’', 'AI', 'ç®—æ³•'],
            'æ•™è‚²': ['å­¸ç¿’', 'æ•™è‚²', 'çŸ¥è­˜', 'å­¸æ ¡', 'å­¸ç”Ÿ', 'è€å¸«', 'èª²ç¨‹'],
            'å¥åº·': ['å¥åº·', 'é†«ç™‚', 'ç–¾ç—…', 'æ²»ç™‚', 'é†«ç”Ÿ', 'è—¥ç‰©', 'ç‡Ÿé¤Š'],
            'ç¶“æ¿Ÿ': ['ç¶“æ¿Ÿ', 'é‡‘è', 'æŠ•è³‡', 'å¸‚å ´', 'å•†æ¥­', 'ä¼æ¥­', 'è²¿æ˜“'],
            'æ–‡åŒ–': ['æ–‡åŒ–', 'è—è¡“', 'éŸ³æ¨‚', 'é›»å½±', 'æ–‡å­¸', 'æ­·å²', 'å‚³çµ±'],
            'é«”è‚²': ['é‹å‹•', 'é«”è‚²', 'è¶³çƒ', 'ç±ƒçƒ', 'æ¯”è³½', 'è¨“ç·´', 'å¥èº«']
        }

        topic_counts = Counter()

        for text in texts:
            text_lower = text.lower()

            for topic, keywords in topic_keywords.items():
                keyword_count = sum(1 for keyword in keywords if keyword in text_lower)
                if keyword_count > 0:
                    topic_counts[topic] += 1

        # è¨ˆç®—ä¸»é¡Œå¤šæ¨£æ€§æŒ‡æ¨™
        total_topic_mentions = sum(topic_counts.values())
        topic_distribution = {
            topic: count / total_topic_mentions * 100
            for topic, count in topic_counts.items()
        } if total_topic_mentions > 0 else {}

        # è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ•¸ï¼ˆShannonç†µï¼‰
        if topic_distribution:
            probabilities = [p/100 for p in topic_distribution.values()]
            diversity_index = -sum(p * np.log2(p) for p in probabilities if p > 0)
        else:
            diversity_index = 0

        return {
            'topic_counts': dict(topic_counts),
            'topic_distribution': topic_distribution,
            'diversity_index': diversity_index,
            'num_topics_covered': len(topic_counts),
            'topic_balance_score': self._calculate_topic_balance(topic_distribution)
        }

    def _analyze_instruction_types(self, dataset) -> Dict:
        """åˆ†ææŒ‡ä»¤é¡å‹"""

        instructions = dataset['instruction']

        # æŒ‡ä»¤é¡å‹é—œéµè©
        instruction_keywords = {
            'è§£é‡‹èªªæ˜': ['è§£é‡‹', 'èªªæ˜', 'æè¿°', 'é—¡è¿°', 'å®šç¾©'],
            'ç¿»è­¯è½‰æ›': ['ç¿»è­¯', 'translate', 'è½‰æ›', 'convert'],
            'ç·¨ç¨‹ä»£ç¢¼': ['ç·¨å¯«', 'ä»£ç¢¼', 'å‡½æ•¸', 'python', 'ç¨‹åº', 'ç®—æ³•'],
            'ç¸½çµæ­¸ç´': ['ç¸½çµ', 'æ­¸ç´', 'æ¦‚æ‹¬', 'æ‘˜è¦', 'è¦é»'],
            'åˆ—èˆ‰æšèˆ‰': ['åˆ—èˆ‰', 'æšèˆ‰', 'åˆ—å‡º', 'çµ¦å‡º', 'æä¾›'],
            'æ¯”è¼ƒå°æ¯”': ['æ¯”è¼ƒ', 'å°æ¯”', 'å·®ç•°', 'å€åˆ¥', 'ç•°åŒ'],
            'å‰µä½œç”Ÿæˆ': ['å¯«', 'å‰µä½œ', 'ç”Ÿæˆ', 'è£½ä½œ', 'è¨­è¨ˆ'],
            'å•ç­”å›ç­”': ['ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'æ€æ¨£', 'å›ç­”']
        }

        type_counts = Counter()

        for instruction in instructions:
            instruction_lower = instruction.lower()
            categorized = False

            for category, keywords in instruction_keywords.items():
                if any(keyword in instruction_lower for keyword in keywords):
                    type_counts[category] += 1
                    categorized = True
                    break

            if not categorized:
                type_counts['å…¶ä»–'] += 1

        # è¨ˆç®—åˆ†ä½ˆ
        total = len(instructions)
        type_distribution = {
            category: count / total * 100
            for category, count in type_counts.items()
        }

        return {
            'type_counts': dict(type_counts),
            'type_distribution': type_distribution,
            'most_common_type': type_counts.most_common(1)[0] if type_counts else None,
            'type_diversity_score': len(type_counts) / len(instruction_keywords) * 100
        }

    def _analyze_instruction_lengths(self, dataset) -> Dict:
        """åˆ†ææŒ‡ä»¤é•·åº¦åˆ†ä½ˆ"""

        instructions = dataset['instruction']
        inputs = dataset['input']
        outputs = dataset['output']

        length_stats = {
            'instruction_lengths': [len(inst) for inst in instructions],
            'input_lengths': [len(inp) for inp in inputs],
            'output_lengths': [len(out) for out in outputs]
        }

        # è¨ˆç®—çµ±è¨ˆé‡
        stats_summary = {}
        for component, lengths in length_stats.items():
            stats_summary[component] = {
                'avg_length': np.mean(lengths),
                'median_length': np.median(lengths),
                'max_length': np.max(lengths),
                'min_length': np.min(lengths),
                'std_length': np.std(lengths)
            }

        # è¼¸å…¥éç©ºæ¯”ä¾‹
        non_empty_inputs = sum(1 for inp in inputs if inp.strip())
        input_coverage = non_empty_inputs / len(inputs) * 100

        return {
            'length_statistics': stats_summary,
            'input_coverage_percent': input_coverage,
            'avg_total_length': np.mean([
                len(inst) + len(inp) + len(out)
                for inst, inp, out in zip(instructions, inputs, outputs)
            ])
        }

    def _analyze_instruction_complexity(self, dataset) -> Dict:
        """åˆ†ææŒ‡ä»¤è¤‡é›œåº¦"""

        instructions = dataset['instruction']
        
        complexity_keywords = {
            'é«˜': ['åˆ†æ', 'è©•ä¼°', 'æ¯”è¼ƒ', 'è¨­è¨ˆ', 'å‰µå»º', 'è«–è­‰', 'æ¨å°'],
            'ä¸­': ['è§£é‡‹', 'ç¸½çµ', 'è½‰æ›', 'ç·¨å¯«', 'åˆ†é¡', 'æ‡‰ç”¨'],
            'ä½': ['åˆ—èˆ‰', 'å®šç¾©', 'æ‰¾å‡º', 'ä»€éº¼æ˜¯', 'èª°æ˜¯', 'åˆ—å‡º']
        }

        complexity_counts = Counter()

        for instruction in instructions:
            instruction_lower = instruction.lower()
            categorized = False
            for level, keywords in complexity_keywords.items():
                if any(keyword in instruction_lower for keyword in keywords):
                    complexity_counts[level] += 1
                    categorized = True
                    break
            if not categorized:
                complexity_counts['æœªçŸ¥'] += 1

        total = len(instructions)
        complexity_distribution = {
            level: count / total * 100
            for level, count in complexity_counts.items()
        }

        # Calculate a weighted complexity score
        score_mapping = {'é«˜': 3, 'ä¸­': 2, 'ä½': 1, 'æœªçŸ¥': 1.5}
        weighted_score = sum(complexity_counts[level] * score_mapping[level] for level in complexity_counts) / total if total > 0 else 0


        return {
            'complexity_counts': dict(complexity_counts),
            'complexity_distribution': complexity_distribution,
            'average_complexity_score': weighted_score # Scale could be 1-3
        }

    def _detect_instruction_quality_issues(self, dataset) -> Dict:
        """æª¢æ¸¬æŒ‡ä»¤è³ªé‡å•é¡Œ"""

        instructions = dataset['instruction']
        outputs = dataset['output']

        issues = {
            'very_short_instructions': 0,
            'very_short_outputs': 0,
            'very_long_outputs': 0,
            'repetitive_outputs': 0,
            'incomplete_outputs': 0,
            'quality_issues_list': []
        }

        for i, (instruction, output) in enumerate(zip(instructions, outputs)):
            # æª¢æŸ¥æŒ‡ä»¤é•·åº¦
            if len(instruction.strip()) < 5:
                issues['very_short_instructions'] += 1
                issues['quality_issues_list'].append(f"æ¨£æœ¬{i}: æŒ‡ä»¤éçŸ­")

            # æª¢æŸ¥è¼¸å‡ºè³ªé‡
            if len(output.strip()) < 10:
                issues['very_short_outputs'] += 1
                issues['quality_issues_list'].append(f"æ¨£æœ¬{i}: è¼¸å‡ºéçŸ­")
            elif len(output.strip()) > 1000:
                issues['very_long_outputs'] += 1
                issues['quality_issues_list'].append(f"æ¨£æœ¬{i}: è¼¸å‡ºéé•·")

            # æª¢æŸ¥é‡è¤‡æ€§
            output_words = output.split()
            if len(output_words) > 5:
                unique_ratio = len(set(output_words)) / len(output_words)
                if unique_ratio < 0.4:
                    issues['repetitive_outputs'] += 1
                    issues['quality_issues_list'].append(f"æ¨£æœ¬{i}: è¼¸å‡ºé‡è¤‡åº¦é«˜")

            # æª¢æŸ¥å®Œæ•´æ€§
            if output.strip().endswith(('...', 'ã€‚ã€‚ã€‚', 'ç­‰')):
                issues['incomplete_outputs'] += 1
                issues['quality_issues_list'].append(f"æ¨£æœ¬{i}: è¼¸å‡ºå¯èƒ½ä¸å®Œæ•´")

        # è¨ˆç®—è³ªé‡è©•åˆ†
        total_samples = len(instructions)
        total_issues = (
            issues['very_short_instructions'] +
            issues['very_short_outputs'] +
            issues['very_long_outputs'] +
            issues['repetitive_outputs'] +
            issues['incomplete_outputs']
        )

        quality_score = max(0, 1 - total_issues / (total_samples * 5))  # 5ç¨®å•é¡Œé¡å‹
        issues['overall_quality_score'] = quality_score
        issues['quality_grade'] = self._grade_quality(quality_score)

        return issues

    def _grade_quality(self, score: float) -> str:
        """è³ªé‡è©•åˆ†"""

        if score >= 0.9:
            return "å„ªç§€"
        elif score >= 0.8:
            return "è‰¯å¥½"
        elif score >= 0.7:
            return "ä¸­ç­‰"
        elif score >= 0.6:
            return "åŠæ ¼"
        else:
            return "éœ€è¦æ”¹é€²"

    def _calculate_topic_balance(self, topic_distribution: Dict) -> float:
        """è¨ˆç®—ä¸»é¡Œå¹³è¡¡åº¦"""

        if not topic_distribution:
            return 0.0

        # ç†æƒ³æƒ…æ³æ˜¯å„ä¸»é¡Œå‡å‹»åˆ†ä½ˆ
        ideal_percentage = 100 / len(topic_distribution)
        deviations = [abs(percentage - ideal_percentage) for percentage in topic_distribution.values()]
        avg_deviation = np.mean(deviations)

        # è½‰æ›ç‚º0-1åˆ†æ•¸ï¼ˆåå·®è¶Šå°åˆ†æ•¸è¶Šé«˜ï¼‰
        balance_score = max(0, 1 - avg_deviation / ideal_percentage)

        return balance_score

    def _analyze_preference_consistency(self, dataset) -> Dict:
        """åˆ†æåå¥½ä¸€è‡´æ€§ (æ¨¡æ“¬)"""
        # åœ¨çœŸå¯¦å ´æ™¯ä¸­ï¼Œé€™éœ€è¦å¤šå€‹æ¨™è¨»è€…çš„æ•¸æ“šä¾†è¨ˆç®— Fleiss' Kappa æˆ– Krippendorff's Alpha
        # é€™è£¡æˆ‘å€‘æ¨¡æ“¬ä¸€å€‹é«˜ä¸€è‡´æ€§åˆ†æ•¸
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ˜é¡¯çš„çŸ›ç›¾ (ä¾‹å¦‚ï¼Œç›¸åŒ prompt å’Œ responses ä½†åå¥½ä¸åŒ)
        seen = {}
        conflicts = 0
        for item in dataset:
            key = (item['prompt'], item['response_a'], item['response_b'])
            if key in seen and seen[key] != item['preference']:
                conflicts += 1
            seen[key] = item['preference']

        total_pairs = len(dataset)
        consistency_score = (total_pairs - conflicts) / total_pairs if total_pairs > 0 else 1.0
        
        return {
            'consistency_score': consistency_score, # æ¨¡æ“¬ Fleiss' Kappa
            'conflicts_found': conflicts,
            'note': 'This is a simulated consistency score. Real-world analysis requires multi-annotator data.'
        }

    def _analyze_response_quality(self, dataset) -> Dict:
        """åˆ†æå›ç­”è³ªé‡"""
        
        chosen_responses = []
        rejected_responses = []

        for item in dataset:
            if item['preference'] == 'A':
                chosen_responses.append(item['response_a'])
                rejected_responses.append(item['response_b'])
            else:
                chosen_responses.append(item['response_b'])
                rejected_responses.append(item['response_a'])

        def get_quality_stats(responses):
            if not responses:
                return {'avg_length': 0, 'avg_word_count': 0}
            lengths = [len(r) for r in responses]
            words = [len(r.split()) for r in responses]
            return {
                'avg_length': np.mean(lengths),
                'avg_word_count': np.mean(words)
            }

        chosen_stats = get_quality_stats(chosen_responses)
        rejected_stats = get_quality_stats(rejected_responses)

        return {
            'chosen_response_stats': chosen_stats,
            'rejected_response_stats': rejected_stats,
            'avg_length_delta': chosen_stats['avg_length'] - rejected_stats['avg_length'],
        }

    def _analyze_safety_aspects(self, dataset) -> Dict:
        """åˆ†æå®‰å…¨æ€§å•é¡Œ"""

        safety_keywords = ['æš´åŠ›', 'è‰²æƒ…', 'æ­§è¦–', 'å±éšª', 'éæ³•']
        
        issues_found = 0
        issue_details = []

        for i, item in enumerate(dataset):
            prompt = item['prompt']
            resp_a = item['response_a']
            resp_b = item['response_b']
            
            for keyword in safety_keywords:
                if keyword in prompt or keyword in resp_a or keyword in resp_b:
                    issues_found += 1
                    issue_details.append(f"æ¨£æœ¬{i}: ç™¼ç¾æ½›åœ¨ä¸å®‰å…¨é—œéµè© '{keyword}'")
                    break # Move to next item once an issue is found

        total_samples = len(dataset)
        safety_score = (total_samples - issues_found) / total_samples if total_samples > 0 else 1.0

        return {
            'potential_safety_issues_count': issues_found,
            'safety_score': safety_score, # 0-1, 1 is best
            'issue_details': issue_details[:10] # show first 10 issues
        }

    def compare_datasets(self, dataset_results: List[Dict]) -> Dict:
        """å°æ¯”å¤šå€‹æ•¸æ“šé›†"""

        print("=== æ•¸æ“šé›†å°æ¯”åˆ†æ ===")

        comparison_data = []

        for result in dataset_results:
            dataset_name = result['dataset_info']['name']

            row = {
                'æ•¸æ“šé›†': dataset_name,
                'æ¨£æœ¬æ•¸': result['dataset_info']['total_samples']
            }

            # åŸºç¤çµ±è¨ˆ (ä¸»è¦ç”¨æ–¼é è¨“ç·´)
            stats = result.get('basic_statistics')
            if stats:
                row['å¹³å‡é•·åº¦'] = f"{stats.get('avg_char_length', 0):.0f}"
                row['å¹³å‡è©æ•¸'] = f"{stats.get('avg_word_count', 0):.0f}"
            else:
                row['å¹³å‡é•·åº¦'] = 'N/A'
                row['å¹³å‡è©æ•¸'] = 'N/A'

            # è³ªé‡è©•åˆ† (ç”¨æ–¼é è¨“ç·´å’ŒæŒ‡ä»¤)
            quality = result.get('content_quality') or result.get('quality_issues')
            if quality:
                row['è³ªé‡è©•åˆ†'] = f"{quality.get('overall_quality_score', 0):.3f}"
                row['è³ªé‡ç­‰ç´š'] = quality.get('quality_grade', 'N/A')
            else:
                row['è³ªé‡è©•åˆ†'] = 'N/A'
                row['è³ªé‡ç­‰ç´š'] = 'N/A'

            # èªè¨€åˆ†ä½ˆ (ä¸»è¦ç”¨æ–¼é è¨“ç·´)
            lang_dist = result.get('language_distribution')
            if lang_dist and lang_dist.get('dominant_language'):
                dominant_lang = lang_dist.get('dominant_language')
                row['ä¸»è¦èªè¨€'] = f"{dominant_lang[0]} ({dominant_lang[1]}æ¬¡)"
            else:
                row['ä¸»è¦èªè¨€'] = 'N/A'

            # è©å½™å¤šæ¨£æ€§ (ä¸»è¦ç”¨æ–¼é è¨“ç·´)
            vocab_analysis = result.get('vocabulary_analysis')
            if vocab_analysis and vocab_analysis.get('vocabulary_stats'):
                vocab_stats = vocab_analysis.get('vocabulary_stats')
                row['è©å½™å¤šæ¨£æ€§'] = f"{vocab_stats.get('vocabulary_diversity', 0):.3f}"
            else:
                row['è©å½™å¤šæ¨£æ€§'] = 'N/A'

            comparison_data.append(row)

        comparison_df = pd.DataFrame(comparison_data)

        return {
            'comparison_table': comparison_df,
            'summary_insights': self._generate_comparison_insights(dataset_results),
            'recommendations': self._generate_dataset_recommendations(dataset_results)
        }

    def _generate_comparison_insights(self, results: List[Dict]) -> List[str]:
        """ç”Ÿæˆå°æ¯”æ´å¯Ÿ"""

        insights = []

        # åˆ†ææ¨£æœ¬è¦æ¨¡å·®ç•°
        sample_sizes = [r['dataset_info']['total_samples'] for r in results]
        if len(sample_sizes) > 1 and min(sample_sizes) > 0 and max(sample_sizes) / min(sample_sizes) > 10:
            insights.append("æ•¸æ“šé›†è¦æ¨¡å·®ç•°å·¨å¤§ï¼Œéœ€è¦è€ƒæ…®å¹³è¡¡æ€§")

        # åˆ†æè³ªé‡å·®ç•°
        quality_scores = []
        for r in results:
            quality = r.get('content_quality') or r.get('quality_issues')
            if quality and 'overall_quality_score' in quality:
                quality_scores.append(quality['overall_quality_score'])

        if len(quality_scores) > 1 and max(quality_scores) - min(quality_scores) > 0.3:
            insights.append("æ•¸æ“šé›†è³ªé‡å·®ç•°é¡¯è‘—ï¼Œå»ºè­°å„ªå…ˆä½¿ç”¨é«˜è³ªé‡æ•¸æ“š")

        # åˆ†æå¤šæ¨£æ€§
        diversity_scores = []
        for r in results:
            vocab_analysis = r.get('vocabulary_analysis')
            if vocab_analysis and 'vocabulary_stats' in vocab_analysis:
                diversity_scores.append(vocab_analysis['vocabulary_stats']['vocabulary_diversity'])

        if len(diversity_scores) > 1 and np.std(diversity_scores) > 0.1:
            insights.append("è©å½™å¤šæ¨£æ€§å·®ç•°æ˜é¡¯ï¼Œå¯èƒ½å½±éŸ¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›")

        return insights

    def _generate_dataset_recommendations(self, results: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ•¸æ“šé›†å»ºè­°"""

        recommendations = []

        for result in results:
            dataset_name = result['dataset_info']['name']

            # åŸºæ–¼è³ªé‡è©•åˆ†çš„å»ºè­°
            quality = result.get('content_quality') or result.get('quality_issues')
            if quality and 'overall_quality_score' in quality:
                quality_score = quality['overall_quality_score']
                if quality_score > 0.8:
                    recommendations.append(f"âœ… {dataset_name}: è³ªé‡å„ªç§€ï¼Œæ¨è–¦ä½¿ç”¨")
                elif quality_score > 0.6:
                    recommendations.append(f"âš ï¸ {dataset_name}: è³ªé‡ä¸­ç­‰ï¼Œå»ºè­°æ¸…æ´—å¾Œä½¿ç”¨")
                else:
                    recommendations.append(f"âŒ {dataset_name}: è³ªé‡è¼ƒå·®ï¼Œéœ€è¦å¤§é‡æ¸…æ´—")

            # åŸºæ–¼å¤šæ¨£æ€§çš„å»ºè­°
            vocab_analysis = result.get('vocabulary_analysis')
            if vocab_analysis and 'vocabulary_stats' in vocab_analysis:
                diversity = vocab_analysis['vocabulary_stats']['vocabulary_diversity']
                if diversity > 0.7:
                    recommendations.append(f"âœ… {dataset_name}: è©å½™å¤šæ¨£æ€§è‰¯å¥½")
                else:
                    recommendations.append(f"âš ï¸ {dataset_name}: è©å½™å¤šæ¨£æ€§ä¸è¶³ï¼Œå»ºè­°è£œå……")

        return recommendations

    def visualize_analysis_results(self, analysis_type: str = 'all'):
        """å¯è¦–åŒ–åˆ†æçµæœ"""

        if analysis_type == 'all' or analysis_type == 'pretraining':
            self._visualize_pretraining_analysis()

        if analysis_type == 'all' or analysis_type == 'instruction':
            self._visualize_instruction_analysis()

    def _visualize_pretraining_analysis(self):
        """å¯è¦–åŒ–é è¨“ç·´æ•¸æ“šåˆ†æ"""

        if 'pretraining_dataset' not in self.analysis_results:
            print("æ²’æœ‰é è¨“ç·´æ•¸æ“šåˆ†æçµæœ")
            return

        result = self.analysis_results['pretraining_dataset']

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. æ–‡æœ¬é•·åº¦åˆ†ä½ˆ
        if 'basic_statistics' in result:
            # æ¨¡æ“¬é•·åº¦åˆ†ä½ˆæ•¸æ“š
            np.random.seed(42)
            lengths = np.random.lognormal(4, 1, 1000)  # å°æ•¸æ­£æ…‹åˆ†ä½ˆ
            axes[0, 0].hist(lengths, bins=50, alpha=0.7, color='skyblue')
            axes[0, 0].set_title('æ–‡æœ¬é•·åº¦åˆ†ä½ˆ')
            axes[0, 0].set_xlabel('å­—ç¬¦æ•¸')
            axes[0, 0].set_ylabel('é »ç‡')

        # 2. èªè¨€åˆ†ä½ˆ
        if 'language_distribution' in result:
            lang_dist = result['language_distribution']['language_percentages']
            if lang_dist:
                languages = list(lang_dist.keys())[:6]
                percentages = [lang_dist[lang] for lang in languages]
                axes[0, 1].pie(percentages, labels=languages, autopct='%1.1f%%')
                axes[0, 1].set_title('èªè¨€åˆ†ä½ˆ')

        # 3. ä¸»é¡Œå¤šæ¨£æ€§
        if 'topic_diversity' in result:
            topic_dist = result['topic_diversity']['topic_distribution']
            if topic_dist:
                topics = list(topic_dist.keys())
                counts = list(topic_dist.values())
                axes[1, 0].bar(topics, counts, color='lightgreen')
                axes[1, 0].set_title('ä¸»é¡Œåˆ†ä½ˆ')
                axes[1, 0].set_ylabel('ç™¾åˆ†æ¯”')
                axes[1, 0].tick_params(axis='x', rotation=45)

        # 4. è©é »åˆ†ä½ˆ
        if 'vocabulary_analysis' in result:
            most_common = result['vocabulary_analysis']['most_common_words'][:10]
            if most_common:
                words = [item[0] for item in most_common]
                freqs = [item[1] for item in most_common]
                axes[1, 1].barh(words, freqs, color='salmon')
                axes[1, 1].set_title('é«˜é »è©å½™')
                axes[1, 1].set_xlabel('é »ç‡')

        plt.tight_layout()
        plt.savefig('pretraining_dataset_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()

    def _visualize_instruction_analysis(self):
        """å¯è¦–åŒ–æŒ‡ä»¤æ•¸æ“šåˆ†æ"""

        if 'instruction_dataset' not in self.analysis_results:
            print("æ²’æœ‰æŒ‡ä»¤æ•¸æ“šåˆ†æçµæœ")
            return

        result = self.analysis_results['instruction_dataset']

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Instruction Dataset Analysis', fontsize=16)

        # 1. æŒ‡ä»¤é¡å‹åˆ†ä½ˆ
        if 'instruction_types' in result and result['instruction_types'].get('type_counts'):
            types = result['instruction_types']['type_counts']
            categories = list(types.keys())
            values = list(types.values())

            axes[0, 0].bar(categories, values, color='cornflowerblue')
            axes[0, 0].set_title('æŒ‡ä»¤é¡å‹åˆ†ä½ˆ')
            axes[0, 0].set_ylabel('æ•¸é‡')
            axes[0, 0].tick_params(axis='x', labelrotation=45)

        # 2. é•·åº¦åˆ†ä½ˆ
        if 'length_distribution' in result and result['length_distribution'].get('length_statistics'):
            stats = result['length_distribution']['length_statistics']
            lengths = ['æŒ‡ä»¤', 'è¼¸å…¥', 'è¼¸å‡º']
            avg_lengths = [
                stats.get('instruction_lengths', {}).get('avg_length', 0),
                stats.get('input_lengths', {}).get('avg_length', 0),
                stats.get('output_lengths', {}).get('avg_length', 0)
            ]

            axes[0, 1].bar(lengths, avg_lengths, color=['skyblue', 'lightgreen', 'salmon'])
            axes[0, 1].set_title('å¹³å‡é•·åº¦å°æ¯”')
            axes[0, 1].set_ylabel('å¹³å‡å­—ç¬¦æ•¸')

        # 3. è³ªé‡å•é¡Œåˆ†ä½ˆ
        if 'quality_issues' in result:
            quality = result['quality_issues']
            issues = ['éçŸ­æŒ‡ä»¤', 'éçŸ­è¼¸å‡º', 'éé•·è¼¸å‡º', 'é‡è¤‡è¼¸å‡º', 'ä¸å®Œæ•´è¼¸å‡º']
            counts = [
                quality.get('very_short_instructions', 0),
                quality.get('very_short_outputs', 0),
                quality.get('very_long_outputs', 0),
                quality.get('repetitive_outputs', 0),
                quality.get('incomplete_outputs', 0)
            ]

            axes[1, 0].bar(issues, counts, color='orange', alpha=0.7)
            axes[1, 0].set_title('è³ªé‡å•é¡Œçµ±è¨ˆ')
            axes[1, 0].set_ylabel('å•é¡Œæ•¸é‡')
            axes[1, 0].tick_params(axis='x', labelrotation=45)

        # 4. æŒ‡ä»¤è¤‡é›œåº¦
        if 'complexity_analysis' in result and result['complexity_analysis'].get('complexity_counts'):
            complexity = result['complexity_analysis']['complexity_counts']
            comp_types = list(complexity.keys())
            comp_values = list(complexity.values())

            axes[1, 1].pie(comp_values, labels=comp_types, autopct='%1.1f%%', startangle=90)
            axes[1, 1].set_title('æŒ‡ä»¤è¤‡é›œåº¦åˆ†ä½ˆ')
            axes[1, 1].axis('equal') 


        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig('instruction_dataset_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("æŒ‡ä»¤æ•¸æ“šé›†åˆ†æåœ–è¡¨å·²ä¿å­˜: instruction_dataset_analysis.png")

    def generate_analysis_report(self) -> str:
        """ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š"""

        report = f"""# æ•¸æ“šé›†åˆ†æç¶œåˆå ±å‘Š

ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## åˆ†ææ¦‚è¿°

æœ¬å ±å‘Šå°{len(self.analysis_results)}å€‹æ•¸æ“šé›†é€²è¡Œäº†å…¨é¢åˆ†æï¼Œæ¶µè“‹æ•¸æ“šè³ªé‡ã€åˆ†ä½ˆç‰¹æ€§ã€å¤šæ¨£æ€§ç­‰é—œéµç¶­åº¦ã€‚

"""

        # ç‚ºæ¯å€‹æ•¸æ“šé›†ç”Ÿæˆè©³ç´°åˆ†æ
        for dataset_type, result in self.analysis_results.items():
            dataset_name = result['dataset_info']['name']
            total_samples = result['dataset_info']['total_samples']

            report += f"""
## {dataset_type.upper()}æ•¸æ“šé›†åˆ†æ

**æ•¸æ“šé›†**: {dataset_name}
**æ¨£æœ¬æ•¸é‡**: {total_samples:,}

"""

            # æ·»åŠ é—œéµç™¼ç¾
            if 'content_quality' in result:
                quality = result['content_quality']
                report += f"""
### è³ªé‡è©•ä¼°
- æ•´é«”è³ªé‡è©•åˆ†: {quality['overall_quality_score']:.3f}
- è³ªé‡ç­‰ç´š: {quality['quality_grade']}
- ä¸»è¦å•é¡Œ: {len(quality['potential_quality_issues'])} é …
"""

            if 'vocabulary_analysis' in result:
                vocab = result['vocabulary_analysis']['vocabulary_stats']
                report += f"""
### è©å½™çµ±è¨ˆ
- ç¸½è©æ•¸: {vocab['total_words']:,}
- å”¯ä¸€è©æ•¸: {vocab['unique_words']:,}
- è©å½™å¤šæ¨£æ€§: {vocab['vocabulary_diversity']:.3f}
"""

        # æ·»åŠ ç¸½çµå’Œå»ºè­°
        report += """
## ç¸½çµèˆ‡å»ºè­°

### é—œéµç™¼ç¾
1. æ•¸æ“šè³ªé‡æ˜¯å½±éŸ¿æ¨¡å‹æ€§èƒ½çš„é—œéµå› ç´ 
2. è©å½™å¤šæ¨£æ€§ç›´æ¥å½±éŸ¿æ¨¡å‹çš„è¡¨é”èƒ½åŠ›
3. ä¸»é¡Œå¹³è¡¡æ€§å½±éŸ¿æ¨¡å‹çš„çŸ¥è­˜è¦†è“‹é¢
4. æ•¸æ“šæ¸…æ´—å’Œé è™•ç†å°æœ€çµ‚æ•ˆæœè‡³é—œé‡è¦

### æ”¹é€²å»ºè­°
1. å»ºç«‹æ•¸æ“šè³ªé‡è©•ä¼°æ¨™æº–å’Œè‡ªå‹•åŒ–æª¢æ¸¬æµç¨‹
2. å¯¦æ–½åˆ†å±¤æŠ½æ¨£ç¢ºä¿ä¸»é¡Œå’Œé¡å‹çš„å¹³è¡¡æ€§
3. å»ºç«‹æŒçºŒçš„æ•¸æ“šè³ªé‡ç›£æ§æ©Ÿåˆ¶
4. è€ƒæ…®æ•¸æ“šå¢å¼·æŠ€è¡“æå‡å¤šæ¨£æ€§

### æœ€ä½³å¯¦è¸
- é è¨“ç·´æ•¸æ“šï¼šæ³¨é‡è¦æ¨¡å’Œå¤šæ¨£æ€§ï¼Œå»ºç«‹åš´æ ¼çš„è³ªé‡æ§åˆ¶
- æŒ‡ä»¤æ•¸æ“šï¼šæ³¨é‡è³ªé‡å’Œå¹³è¡¡æ€§ï¼Œç¢ºä¿æŒ‡ä»¤é¡å‹å……åˆ†è¦†è“‹
- åå¥½æ•¸æ“šï¼šæ³¨é‡ä¸€è‡´æ€§å’Œä»£è¡¨æ€§ï¼Œé¿å…æ¨™è¨»åå·®

---
*æ­¤å ±å‘ŠåŸºæ–¼æ•¸æ“šé›†æ¨£æœ¬åˆ†æç”Ÿæˆï¼Œå¯¦éš›æ‡‰ç”¨æ™‚è«‹æ ¹æ“šå®Œæ•´æ•¸æ“šé›†é€²è¡Œè©•ä¼°ã€‚*
"""

        return report

def main():
    """ä¸»å‡½æ•¸æ¼”ç¤º"""

    print("æ•¸æ“šé›†é¡å‹åˆ†æå·¥å…·æ¼”ç¤º")
    print("=" * 50)

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DatasetAnalyzer()

    # 1. åˆ†æé è¨“ç·´æ•¸æ“šé›†
    print("\n1. åˆ†æé è¨“ç·´æ•¸æ“šé›†...")
    pretraining_result = analyzer.analyze_pretraining_dataset(max_samples=500)

    # 2. åˆ†ææŒ‡ä»¤æ•¸æ“šé›†
    print("\n2. åˆ†ææŒ‡ä»¤æ•¸æ“šé›†...")
    instruction_result = analyzer.analyze_instruction_dataset(max_samples=300)

    # 3. åˆ†æåå¥½æ•¸æ“šé›†
    print("\n3. åˆ†æåå¥½æ•¸æ“šé›†...")
    preference_result = analyzer.analyze_preference_dataset(max_samples=100)

    # 4. æ•¸æ“šé›†å°æ¯”
    print("\n4. æ•¸æ“šé›†å°æ¯”åˆ†æ...")
    comparison_result = analyzer.compare_datasets([
        pretraining_result,
        instruction_result,
        preference_result
    ])

    print("\nğŸ“Š å°æ¯”çµæœ:")
    print(comparison_result['comparison_table'].to_string(index=False))

    print("\nğŸ’¡ æ´å¯Ÿç™¼ç¾:")
    for insight in comparison_result['summary_insights']:
        print(f"  - {insight}")

    print("\nğŸ“‹ æ•¸æ“šé›†å»ºè­°:")
    for recommendation in comparison_result['recommendations']:
        print(f"  {recommendation}")

    # 5. å¯è¦–åŒ–çµæœ
    print("\n5. ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨...")
    analyzer.visualize_analysis_results()

    # 6. ç”Ÿæˆå ±å‘Š
    print("\n6. ç”Ÿæˆç¶œåˆå ±å‘Š...")
    report = analyzer.generate_analysis_report()

    # ä¿å­˜å ±å‘Š
    with open('dataset_analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(report)

    print("\nâœ… åˆ†æå®Œæˆï¼")
    print("ğŸ“ çµæœæ–‡ä»¶:")
    print("   - dataset_analysis_report.md (ç¶œåˆå ±å‘Š)")
    print("   - pretraining_dataset_analysis.png (å¯è¦–åŒ–åœ–è¡¨)")
    print("   - instruction_dataset_analysis.png (å¯è¦–åŒ–åœ–è¡¨)")

    print("\nğŸ“ å­¸ç¿’è¦é»:")
    print("1. æ•¸æ“šè³ªé‡æ¯”æ•¸æ“šé‡æ›´é‡è¦")
    print("2. å¤šæ¨£æ€§æ˜¯æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŸºç¤")
    print("3. ä¸åŒè¨“ç·´éšæ®µéœ€è¦ä¸åŒç‰¹æ€§çš„æ•¸æ“š")
    print("4. å»ºç«‹æ•¸æ“šè³ªé‡æ§åˆ¶æ˜¯å·¥ç¨‹åŒ–çš„é—œéµ")

if __name__ == "__main__":
    main()