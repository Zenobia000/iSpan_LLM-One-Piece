{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 - é«˜æ•ˆæ•¸æ“šç¯©é¸\n",
    "## Notebook 02: IFD + DEITA æ•¸æ“šç¯©é¸\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "1. å¯¦ç¾ IFD (Instruction Following Difficulty) è¨ˆç®—å™¨\n",
    "2. ä½¿ç”¨ IFD é€²è¡Œåˆæ­¥ç¯©é¸ (0.3 â‰¤ IFD â‰¤ 0.9)\n",
    "3. å¯¦ç¾ DEITA è©•åˆ†ç³»çµ± (è¤‡é›œåº¦ + è³ªé‡ + å¤šæ¨£æ€§)\n",
    "4. é¸æ“‡ Top-30% é«˜è³ªé‡æ¨£æœ¬\n",
    "5. ç”Ÿæˆç¯©é¸æ‘˜è¦å ±å‘Š\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 1-1.5 å°æ™‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥ç’°å¢ƒèˆ‡æ•¸æ“š\n",
    "\n",
    "å¾ 01-Setup.ipynb è¼‰å…¥é…ç½®å’Œæ•¸æ“šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®šå¯è¦–åŒ–é¢¨æ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… ä¾è³´å°å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›®éŒ„è·¯å¾‘\n",
    "DATA_DIR = Path('./data')\n",
    "ANALYSIS_DIR = Path('./analysis')\n",
    "RESULTS_DIR = Path('./results')\n",
    "\n",
    "# è¼‰å…¥åŸå§‹æ•¸æ“š\n",
    "raw_data_path = DATA_DIR / 'alpaca_raw.json'\n",
    "with open(raw_data_path, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# è¼‰å…¥é…ç½®\n",
    "config_path = DATA_DIR / 'filtering_config.json'\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"âœ… æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(raw_data):,} æ¨£æœ¬\")\n",
    "print(f\"âœ… é…ç½®è¼‰å…¥å®Œæˆ\")\n",
    "print(f\"   ç›®æ¨™ä¿ç•™ç‡: {config['target_retention_rate']*100:.0f}%\")\n",
    "print(f\"   ç›®æ¨™æ¨£æœ¬æ•¸: {config['target_samples']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥ Sentence-BERT æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ è¼‰å…¥ Sentence-BERT æ¨¡å‹...\")\n",
    "\n",
    "embedding_model = SentenceTransformer(config['embedding_model'])\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {config['embedding_model']}\")\n",
    "print(f\"   åµŒå…¥ç¶­åº¦: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. IFD è¨ˆç®—å™¨å¯¦ç¾\n",
    "\n",
    "å¯¦ç¾ Instruction Following Difficulty è¨ˆç®—é‚è¼¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFDCalculator:\n",
    "    \"\"\"\n",
    "    IFD (Instruction Following Difficulty) Calculator\n",
    "    \n",
    "    IFD measures semantic distance between instruction and response.\n",
    "    Higher IFD indicates more difficult/complex tasks requiring reasoning.\n",
    "    \n",
    "    Formula: IFD = 1 - cosine_similarity(instruction_emb, response_emb)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model, batch_size=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_model: SentenceTransformer model\n",
    "            batch_size: Batch size for encoding\n",
    "        \"\"\"\n",
    "        self.model = embedding_model\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def calculate_single_ifd(self, instruction: str, response: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate IFD for a single sample\n",
    "        \n",
    "        Args:\n",
    "            instruction: Instruction text\n",
    "            response: Response text\n",
    "        \n",
    "        Returns:\n",
    "            IFD score (0-1)\n",
    "        \"\"\"\n",
    "        # Encode texts\n",
    "        instr_emb = self.model.encode([instruction])\n",
    "        resp_emb = self.model.encode([response])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(instr_emb, resp_emb)[0][0]\n",
    "        \n",
    "        # IFD = 1 - similarity\n",
    "        ifd = 1.0 - similarity\n",
    "        \n",
    "        return float(ifd)\n",
    "    \n",
    "    def calculate_batch_ifd(self, samples: list) -> list:\n",
    "        \"\"\"\n",
    "        Calculate IFD for multiple samples in batches\n",
    "        \n",
    "        Args:\n",
    "            samples: List of dicts with 'instruction' and 'output'\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples (sample_dict, ifd_score)\n",
    "        \"\"\"\n",
    "        print(f\"\\nè¨ˆç®— IFD åˆ†æ•¸ (æ‰¹æ¬¡å¤§å°: {self.batch_size})...\")\n",
    "        \n",
    "        # Prepare texts\n",
    "        instructions = []\n",
    "        responses = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            # Combine instruction and input\n",
    "            full_instruction = sample['instruction']\n",
    "            if sample.get('input', ''):\n",
    "                full_instruction += \" \" + sample['input']\n",
    "            \n",
    "            instructions.append(full_instruction)\n",
    "            responses.append(sample['output'])\n",
    "        \n",
    "        # Encode in batches\n",
    "        print(\"  ç·¨ç¢¼æŒ‡ä»¤...\")\n",
    "        instr_embs = self.model.encode(\n",
    "            instructions,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        print(\"  ç·¨ç¢¼å›æ‡‰...\")\n",
    "        resp_embs = self.model.encode(\n",
    "            responses,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Calculate IFD for all samples\n",
    "        print(\"  è¨ˆç®— IFD åˆ†æ•¸...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, sample in enumerate(tqdm(samples, desc=\"IFD è¨ˆç®—\")):\n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(instr_embs[i], resp_embs[i]) / (\n",
    "                np.linalg.norm(instr_embs[i]) * np.linalg.norm(resp_embs[i])\n",
    "            )\n",
    "            \n",
    "            # IFD = 1 - similarity\n",
    "            ifd = 1.0 - similarity\n",
    "            \n",
    "            # Add IFD to sample\n",
    "            sample_with_ifd = sample.copy()\n",
    "            sample_with_ifd['ifd_score'] = float(ifd)\n",
    "            \n",
    "            results.append(sample_with_ifd)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# å»ºç«‹ IFD è¨ˆç®—å™¨\n",
    "ifd_calculator = IFDCalculator(\n",
    "    embedding_model=embedding_model,\n",
    "    batch_size=config['batch_size']\n",
    ")\n",
    "\n",
    "print(\"âœ… IFD è¨ˆç®—å™¨å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¸¬è©¦ IFD è¨ˆç®—\n",
    "\n",
    "åœ¨å¹¾å€‹æ¨£æœ¬ä¸Šæ¸¬è©¦ IFD è¨ˆç®—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª æ¸¬è©¦ IFD è¨ˆç®—...\\n\")\n",
    "\n",
    "# æ¸¬è©¦æ¨£æœ¬\n",
    "test_samples = [\n",
    "    {\n",
    "        'instruction': 'Translate \"hello\" to Chinese',\n",
    "        'output': 'ä½ å¥½',\n",
    "        'expected': 'Low IFD (simple task)'\n",
    "    },\n",
    "    {\n",
    "        'instruction': 'Analyze the causes and impacts of the French Revolution',\n",
    "        'output': 'The French Revolution (1789-1799) was caused by multiple factors including economic crisis, social inequality, Enlightenment ideas, and weak leadership. Its impacts included the end of absolute monarchy, rise of nationalism, and spread of democratic ideals across Europe.',\n",
    "        'expected': 'High IFD (complex analysis)'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_samples, 1):\n",
    "    ifd = ifd_calculator.calculate_single_ifd(test['instruction'], test['output'])\n",
    "    print(f\"æ¸¬è©¦ {i}: {test['expected']}\")\n",
    "    print(f\"  æŒ‡ä»¤: {test['instruction'][:60]}...\")\n",
    "    print(f\"  IFD: {ifd:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¨ˆç®—æ‰€æœ‰æ¨£æœ¬çš„ IFD åˆ†æ•¸\n",
    "\n",
    "å°æ•´å€‹æ•¸æ“šé›†è¨ˆç®— IFDã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®— IFD\n",
    "samples_with_ifd = ifd_calculator.calculate_batch_ifd(raw_data)\n",
    "\n",
    "print(f\"\\nâœ… IFD è¨ˆç®—å®Œæˆ: {len(samples_with_ifd):,} æ¨£æœ¬\")\n",
    "\n",
    "# çµ±è¨ˆ IFD åˆ†å¸ƒ\n",
    "ifd_scores = [s['ifd_score'] for s in samples_with_ifd]\n",
    "\n",
    "print(f\"\\nIFD çµ±è¨ˆ:\")\n",
    "print(f\"  å‡å€¼: {np.mean(ifd_scores):.4f}\")\n",
    "print(f\"  æ¨™æº–å·®: {np.std(ifd_scores):.4f}\")\n",
    "print(f\"  æœ€å°å€¼: {np.min(ifd_scores):.4f}\")\n",
    "print(f\"  æœ€å¤§å€¼: {np.max(ifd_scores):.4f}\")\n",
    "print(f\"  ä¸­ä½æ•¸: {np.median(ifd_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯è¦–åŒ– IFD åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½ IFD åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('IFD åˆ†æ•¸åˆ†å¸ƒåˆ†æ', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. ç›´æ–¹åœ–\n",
    "axes[0].hist(ifd_scores, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(config['ifd_min_threshold'], color='red', linestyle='--', \n",
    "                label=f'æœ€å°é–¾å€¼: {config[\"ifd_min_threshold\"]}')\n",
    "axes[0].axvline(config['ifd_max_threshold'], color='red', linestyle='--', \n",
    "                label=f'æœ€å¤§é–¾å€¼: {config[\"ifd_max_threshold\"]}')\n",
    "axes[0].axvline(np.mean(ifd_scores), color='green', linestyle='-', \n",
    "                label=f'å‡å€¼: {np.mean(ifd_scores):.3f}')\n",
    "axes[0].set_xlabel('IFD åˆ†æ•¸')\n",
    "axes[0].set_ylabel('æ¨£æœ¬æ•¸')\n",
    "axes[0].set_title('IFD åˆ†æ•¸ç›´æ–¹åœ–')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. ç®±å‹åœ–\n",
    "axes[1].boxplot(ifd_scores, vert=True)\n",
    "axes[1].axhline(config['ifd_min_threshold'], color='red', linestyle='--', \n",
    "                label=f'ç¯©é¸ç¯„åœ: [{config[\"ifd_min_threshold\"]}, {config[\"ifd_max_threshold\"]}]')\n",
    "axes[1].axhline(config['ifd_max_threshold'], color='red', linestyle='--')\n",
    "axes[1].set_ylabel('IFD åˆ†æ•¸')\n",
    "axes[1].set_title('IFD åˆ†æ•¸ç®±å‹åœ–')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜åœ–è¡¨\n",
    "fig_path = ANALYSIS_DIR / 'ifd_distribution.png'\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… IFD åˆ†å¸ƒåœ–å·²ä¿å­˜è‡³: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IFD åˆæ­¥ç¯©é¸\n",
    "\n",
    "éæ¿¾æ‰ IFD éä½ (< 0.3) å’Œéé«˜ (> 0.9) çš„æ¨£æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” åŸ·è¡Œ IFD ç¯©é¸...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "min_ifd = config['ifd_min_threshold']\n",
    "max_ifd = config['ifd_max_threshold']\n",
    "\n",
    "# ç¯©é¸\n",
    "ifd_filtered = [\n",
    "    s for s in samples_with_ifd \n",
    "    if min_ifd <= s['ifd_score'] <= max_ifd\n",
    "]\n",
    "\n",
    "# çµ±è¨ˆ\n",
    "too_low = sum(1 for s in samples_with_ifd if s['ifd_score'] < min_ifd)\n",
    "too_high = sum(1 for s in samples_with_ifd if s['ifd_score'] > max_ifd)\n",
    "retained = len(ifd_filtered)\n",
    "\n",
    "print(f\"åŸå§‹æ¨£æœ¬æ•¸: {len(samples_with_ifd):,}\")\n",
    "print(f\"\\néæ¿¾çµ±è¨ˆ:\")\n",
    "print(f\"  IFD < {min_ifd} (éæ–¼ç°¡å–®): {too_low:,} ({too_low/len(samples_with_ifd)*100:.1f}%)\")\n",
    "print(f\"  IFD > {max_ifd} (ä¸ç›¸é—œ): {too_high:,} ({too_high/len(samples_with_ifd)*100:.1f}%)\")\n",
    "print(f\"  ä¿ç•™æ¨£æœ¬: {retained:,} ({retained/len(samples_with_ifd)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"âœ… IFD ç¯©é¸å®Œæˆ: {len(samples_with_ifd):,} â†’ {len(ifd_filtered):,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DEITA è©•åˆ†ç³»çµ±å¯¦ç¾\n",
    "\n",
    "å¯¦ç¾ DEITA (è¤‡é›œåº¦ + è³ªé‡ + å¤šæ¨£æ€§) è©•åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEITAScorer:\n",
    "    \"\"\"\n",
    "    DEITA (Data-Efficient Instruction Tuning) Scorer\n",
    "    \n",
    "    Combines three dimensions:\n",
    "    - Complexity: Task difficulty and reasoning depth\n",
    "    - Quality: Response accuracy and completeness  \n",
    "    - Diversity: Dissimilarity from already selected samples\n",
    "    \n",
    "    Final score = alpha * complexity + beta * quality + gamma * diversity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model, alpha=0.4, beta=0.4, gamma=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_model: SentenceTransformer for diversity calculation\n",
    "            alpha: Weight for complexity (default: 0.4)\n",
    "            beta: Weight for quality (default: 0.4)\n",
    "            gamma: Weight for diversity (default: 0.2)\n",
    "        \"\"\"\n",
    "        self.model = embedding_model\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Cache for embeddings\n",
    "        self.embedding_cache = {}\n",
    "    \n",
    "    def calculate_complexity(self, sample: dict) -> float:\n",
    "        \"\"\"\n",
    "        Calculate complexity score using rule-based heuristics\n",
    "        \n",
    "        Factors:\n",
    "        - Instruction length (longer = more complex)\n",
    "        - Output length (longer = more detailed)\n",
    "        - IFD score (higher = more difficult)\n",
    "        - Keyword indicators (analyze, compare, evaluate, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            Complexity score (0-1)\n",
    "        \"\"\"\n",
    "        instruction = sample['instruction']\n",
    "        output = sample['output']\n",
    "        ifd = sample.get('ifd_score', 0.5)\n",
    "        \n",
    "        # Length-based complexity\n",
    "        instr_words = len(instruction.split())\n",
    "        output_words = len(output.split())\n",
    "        \n",
    "        length_score = min(1.0, (instr_words / 50 + output_words / 200) / 2)\n",
    "        \n",
    "        # Keyword-based complexity\n",
    "        complex_keywords = [\n",
    "            'analyze', 'compare', 'evaluate', 'explain', 'describe',\n",
    "            'discuss', 'critique', 'assess', 'justify', 'synthesize'\n",
    "        ]\n",
    "        \n",
    "        keyword_count = sum(1 for kw in complex_keywords if kw in instruction.lower())\n",
    "        keyword_score = min(1.0, keyword_count / 3)\n",
    "        \n",
    "        # Combine scores\n",
    "        complexity = 0.3 * length_score + 0.3 * keyword_score + 0.4 * ifd\n",
    "        \n",
    "        return float(complexity)\n",
    "    \n",
    "    def calculate_quality(self, sample: dict) -> float:\n",
    "        \"\"\"\n",
    "        Calculate quality score using rule-based heuristics\n",
    "        \n",
    "        Factors:\n",
    "        - Output completeness (length relative to instruction)\n",
    "        - Formatting (presence of structure)\n",
    "        - Language quality (no obvious errors)\n",
    "        \n",
    "        Returns:\n",
    "            Quality score (0-1)\n",
    "        \"\"\"\n",
    "        instruction = sample['instruction']\n",
    "        output = sample['output']\n",
    "        \n",
    "        # Completeness: output should be detailed\n",
    "        output_words = len(output.split())\n",
    "        completeness = min(1.0, output_words / 100)\n",
    "        \n",
    "        # Structure: check for formatting elements\n",
    "        structure_indicators = ['\\n', '. ', ', ', ':', '-', '1.', '2.']\n",
    "        structure_count = sum(1 for ind in structure_indicators if ind in output)\n",
    "        structure_score = min(1.0, structure_count / 5)\n",
    "        \n",
    "        # Relevance: output length proportional to instruction\n",
    "        instr_words = len(instruction.split())\n",
    "        ratio = output_words / max(instr_words, 1)\n",
    "        relevance = min(1.0, ratio / 10)\n",
    "        \n",
    "        # Combine scores\n",
    "        quality = 0.4 * completeness + 0.3 * structure_score + 0.3 * relevance\n",
    "        \n",
    "        return float(quality)\n",
    "    \n",
    "    def calculate_diversity(self, sample: dict, selected_samples: list) -> float:\n",
    "        \"\"\"\n",
    "        Calculate diversity score based on dissimilarity from selected samples\n",
    "        \n",
    "        Args:\n",
    "            sample: Current sample\n",
    "            selected_samples: Already selected samples\n",
    "        \n",
    "        Returns:\n",
    "            Diversity score (0-1), higher = more diverse\n",
    "        \"\"\"\n",
    "        if not selected_samples:\n",
    "            return 1.0\n",
    "        \n",
    "        # Get embedding for current sample\n",
    "        sample_key = sample['instruction'] + ' ' + sample['output']\n",
    "        \n",
    "        if sample_key not in self.embedding_cache:\n",
    "            self.embedding_cache[sample_key] = self.model.encode([sample_key])[0]\n",
    "        \n",
    "        sample_emb = self.embedding_cache[sample_key]\n",
    "        \n",
    "        # Calculate similarity with all selected samples\n",
    "        max_similarity = 0.0\n",
    "        \n",
    "        for selected in selected_samples:\n",
    "            selected_key = selected['instruction'] + ' ' + selected['output']\n",
    "            \n",
    "            if selected_key not in self.embedding_cache:\n",
    "                self.embedding_cache[selected_key] = self.model.encode([selected_key])[0]\n",
    "            \n",
    "            selected_emb = self.embedding_cache[selected_key]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(sample_emb, selected_emb) / (\n",
    "                np.linalg.norm(sample_emb) * np.linalg.norm(selected_emb)\n",
    "            )\n",
    "            \n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "        # Diversity = 1 - max_similarity\n",
    "        diversity = 1.0 - max_similarity\n",
    "        \n",
    "        return float(diversity)\n",
    "    \n",
    "    def calculate_deita_score(self, sample: dict, selected_samples: list = None) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate comprehensive DEITA score\n",
    "        \n",
    "        Args:\n",
    "            sample: Sample to score\n",
    "            selected_samples: Already selected samples for diversity\n",
    "        \n",
    "        Returns:\n",
    "            Dict with scores and final DEITA score\n",
    "        \"\"\"\n",
    "        if selected_samples is None:\n",
    "            selected_samples = []\n",
    "        \n",
    "        # Calculate component scores\n",
    "        complexity = self.calculate_complexity(sample)\n",
    "        quality = self.calculate_quality(sample)\n",
    "        diversity = self.calculate_diversity(sample, selected_samples)\n",
    "        \n",
    "        # Weighted combination\n",
    "        deita_score = (\n",
    "            self.alpha * complexity +\n",
    "            self.beta * quality +\n",
    "            self.gamma * diversity\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'complexity': complexity,\n",
    "            'quality': quality,\n",
    "            'diversity': diversity,\n",
    "            'deita_score': deita_score\n",
    "        }\n",
    "\n",
    "# å»ºç«‹ DEITA è©•åˆ†å™¨\n",
    "deita_scorer = DEITAScorer(\n",
    "    embedding_model=embedding_model,\n",
    "    alpha=config['deita_alpha'],\n",
    "    beta=config['deita_beta'],\n",
    "    gamma=config['deita_gamma']\n",
    ")\n",
    "\n",
    "print(\"âœ… DEITA è©•åˆ†å™¨å»ºç«‹å®Œæˆ\")\n",
    "print(f\"   æ¬Šé‡: è¤‡é›œåº¦={config['deita_alpha']}, è³ªé‡={config['deita_beta']}, å¤šæ¨£æ€§={config['deita_gamma']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è¨ˆç®— DEITA åˆ†æ•¸\n",
    "\n",
    "ç‚ºæ‰€æœ‰ IFD ç¯©é¸å¾Œçš„æ¨£æœ¬è¨ˆç®— DEITA åˆ†æ•¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” è¨ˆç®— DEITA åˆ†æ•¸...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å…ˆè¨ˆç®—æ‰€æœ‰æ¨£æœ¬çš„è¤‡é›œåº¦å’Œè³ªé‡ (ä¸ä¾è³´æ–¼é¸æ“‡é †åº)\n",
    "print(\"  æ­¥é©Ÿ 1: è¨ˆç®—è¤‡é›œåº¦å’Œè³ªé‡åˆ†æ•¸...\")\n",
    "\n",
    "for sample in tqdm(ifd_filtered, desc=\"è¨ˆç®—åŸºç¤åˆ†æ•¸\"):\n",
    "    sample['complexity'] = deita_scorer.calculate_complexity(sample)\n",
    "    sample['quality'] = deita_scorer.calculate_quality(sample)\n",
    "\n",
    "print(\"\\n  æ­¥é©Ÿ 2: è²ªå©ªé¸æ“‡ Top-K æ¨£æœ¬ (è€ƒæ…®å¤šæ¨£æ€§)...\")\n",
    "\n",
    "# è²ªå©ªé¸æ“‡æ¼”ç®—æ³•\n",
    "selected_samples = []\n",
    "remaining_samples = ifd_filtered.copy()\n",
    "target_count = config['target_samples']\n",
    "\n",
    "# é¸æ“‡ç¬¬ä¸€å€‹æ¨£æœ¬ (è¤‡é›œåº¦ + è³ªé‡æœ€é«˜)\n",
    "remaining_samples.sort(\n",
    "    key=lambda x: config['deita_alpha'] * x['complexity'] + config['deita_beta'] * x['quality'],\n",
    "    reverse=True\n",
    ")\n",
    "selected_samples.append(remaining_samples[0])\n",
    "remaining_samples = remaining_samples[1:]\n",
    "\n",
    "# è¿­ä»£é¸æ“‡å‰©é¤˜æ¨£æœ¬\n",
    "pbar = tqdm(total=target_count, desc=\"è²ªå©ªé¸æ“‡\")\n",
    "pbar.update(1)\n",
    "\n",
    "while len(selected_samples) < target_count and remaining_samples:\n",
    "    # ç‚ºæ¯å€‹å€™é¸æ¨£æœ¬è¨ˆç®— DEITA åˆ†æ•¸\n",
    "    for sample in remaining_samples:\n",
    "        diversity = deita_scorer.calculate_diversity(sample, selected_samples)\n",
    "        sample['diversity'] = diversity\n",
    "        sample['deita_score'] = (\n",
    "            config['deita_alpha'] * sample['complexity'] +\n",
    "            config['deita_beta'] * sample['quality'] +\n",
    "            config['deita_gamma'] * diversity\n",
    "        )\n",
    "    \n",
    "    # é¸æ“‡ DEITA åˆ†æ•¸æœ€é«˜çš„\n",
    "    remaining_samples.sort(key=lambda x: x['deita_score'], reverse=True)\n",
    "    selected_samples.append(remaining_samples[0])\n",
    "    remaining_samples = remaining_samples[1:]\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"âœ… DEITA ç¯©é¸å®Œæˆ: {len(ifd_filtered):,} â†’ {len(selected_samples):,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç¯©é¸çµæœåˆ†æ\n",
    "\n",
    "åˆ†æç¯©é¸å‰å¾Œçš„æ•¸æ“šè³ªé‡è®ŠåŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š ç¯©é¸çµæœåˆ†æ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è¨ˆç®—çµ±è¨ˆ\n",
    "original_ifd = np.mean([s['ifd_score'] for s in samples_with_ifd])\n",
    "filtered_ifd = np.mean([s['ifd_score'] for s in selected_samples])\n",
    "\n",
    "original_complexity = np.mean([deita_scorer.calculate_complexity(s) for s in samples_with_ifd[:1000]])\n",
    "filtered_complexity = np.mean([s['complexity'] for s in selected_samples])\n",
    "\n",
    "print(f\"æ•¸æ“šé‡:\")\n",
    "print(f\"  åŸå§‹: {len(raw_data):,}\")\n",
    "print(f\"  ç¯©é¸å¾Œ: {len(selected_samples):,}\")\n",
    "print(f\"  æ¸›å°‘: {(1 - len(selected_samples)/len(raw_data))*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nIFD åˆ†æ•¸:\")\n",
    "print(f\"  åŸå§‹å‡å€¼: {original_ifd:.4f}\")\n",
    "print(f\"  ç¯©é¸å¾Œå‡å€¼: {filtered_ifd:.4f}\")\n",
    "print(f\"  æå‡: {(filtered_ifd - original_ifd)/original_ifd*100:+.1f}%\")\n",
    "\n",
    "print(f\"\\nè¤‡é›œåº¦åˆ†æ•¸:\")\n",
    "print(f\"  åŸå§‹å‡å€¼ (æ¨£æœ¬): {original_complexity:.4f}\")\n",
    "print(f\"  ç¯©é¸å¾Œå‡å€¼: {filtered_complexity:.4f}\")\n",
    "print(f\"  æå‡: {(filtered_complexity - original_complexity)/original_complexity*100:+.1f}%\")\n",
    "\n",
    "print(f\"\\nDEITA åˆ†æ•¸åˆ†å¸ƒ:\")\n",
    "deita_scores = [s['deita_score'] for s in selected_samples]\n",
    "print(f\"  å‡å€¼: {np.mean(deita_scores):.4f}\")\n",
    "print(f\"  æ¨™æº–å·®: {np.std(deita_scores):.4f}\")\n",
    "print(f\"  æœ€å°å€¼: {np.min(deita_scores):.4f}\")\n",
    "print(f\"  æœ€å¤§å€¼: {np.max(deita_scores):.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯è¦–åŒ–ç¯©é¸æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½å°æ¯”åœ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('æ•¸æ“šç¯©é¸æ•ˆæœå°æ¯”', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. IFD åˆ†æ•¸å°æ¯”\n",
    "original_ifd_scores = [s['ifd_score'] for s in samples_with_ifd]\n",
    "filtered_ifd_scores = [s['ifd_score'] for s in selected_samples]\n",
    "\n",
    "axes[0, 0].hist([original_ifd_scores, filtered_ifd_scores], \n",
    "                bins=30, label=['åŸå§‹æ•¸æ“š', 'ç¯©é¸å¾Œ'], alpha=0.7)\n",
    "axes[0, 0].set_xlabel('IFD åˆ†æ•¸')\n",
    "axes[0, 0].set_ylabel('æ¨£æœ¬æ•¸')\n",
    "axes[0, 0].set_title('IFD åˆ†æ•¸åˆ†å¸ƒå°æ¯”')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. DEITA ä¸‰ç¶­è©•åˆ†\n",
    "complexity_scores = [s['complexity'] for s in selected_samples]\n",
    "quality_scores = [s['quality'] for s in selected_samples]\n",
    "diversity_scores = [s['diversity'] for s in selected_samples]\n",
    "\n",
    "x = np.arange(3)\n",
    "means = [np.mean(complexity_scores), np.mean(quality_scores), np.mean(diversity_scores)]\n",
    "stds = [np.std(complexity_scores), np.std(quality_scores), np.std(diversity_scores)]\n",
    "\n",
    "axes[0, 1].bar(x, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(['è¤‡é›œåº¦', 'è³ªé‡', 'å¤šæ¨£æ€§'])\n",
    "axes[0, 1].set_ylabel('å¹³å‡åˆ†æ•¸')\n",
    "axes[0, 1].set_title('DEITA ä¸‰ç¶­è©•åˆ†')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. è¤‡é›œåº¦ vs è³ªé‡æ•£é»åœ–\n",
    "axes[1, 0].scatter(complexity_scores, quality_scores, alpha=0.5, s=20)\n",
    "axes[1, 0].set_xlabel('è¤‡é›œåº¦')\n",
    "axes[1, 0].set_ylabel('è³ªé‡')\n",
    "axes[1, 0].set_title('è¤‡é›œåº¦ vs è³ªé‡')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. DEITA åˆ†æ•¸åˆ†å¸ƒ\n",
    "axes[1, 1].hist(deita_scores, bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 1].axvline(np.mean(deita_scores), color='red', linestyle='--', \n",
    "                   label=f'å‡å€¼: {np.mean(deita_scores):.3f}')\n",
    "axes[1, 1].set_xlabel('DEITA åˆ†æ•¸')\n",
    "axes[1, 1].set_ylabel('æ¨£æœ¬æ•¸')\n",
    "axes[1, 1].set_title('DEITA ç¶œåˆåˆ†æ•¸åˆ†å¸ƒ')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜åœ–è¡¨\n",
    "fig_path = ANALYSIS_DIR / 'filtering_comparison.png'\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… å°æ¯”åœ–å·²ä¿å­˜è‡³: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ä¿å­˜ç¯©é¸çµæœ\n",
    "\n",
    "ä¿å­˜ç¯©é¸å¾Œçš„æ•¸æ“šé›†å’Œè©•åˆ†è©³æƒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ’¾ ä¿å­˜ç¯©é¸çµæœ...\")\n",
    "\n",
    "# ä¿å­˜ç¯©é¸å¾Œçš„æ•¸æ“š\n",
    "filtered_data_path = DATA_DIR / 'alpaca_filtered.json'\n",
    "with open(filtered_data_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(selected_samples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… ç¯©é¸æ•¸æ“šå·²ä¿å­˜è‡³: {filtered_data_path}\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {len(selected_samples):,}\")\n",
    "\n",
    "# ä¿å­˜ DEITA è©•åˆ†è¡¨\n",
    "scores_df = pd.DataFrame([{\n",
    "    'instruction': s['instruction'][:100],\n",
    "    'ifd_score': s['ifd_score'],\n",
    "    'complexity': s['complexity'],\n",
    "    'quality': s['quality'],\n",
    "    'diversity': s['diversity'],\n",
    "    'deita_score': s['deita_score']\n",
    "} for s in selected_samples])\n",
    "\n",
    "scores_path = ANALYSIS_DIR / 'deita_scores.csv'\n",
    "scores_df.to_csv(scores_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"âœ… DEITA è©•åˆ†è¡¨å·²ä¿å­˜è‡³: {scores_path}\")\n",
    "\n",
    "# é¡¯ç¤ºå‰å¹¾æ¢\n",
    "print(\"\\nå‰ 5 æ¢é«˜åˆ†æ¨£æœ¬:\")\n",
    "print(scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç”Ÿæˆç¯©é¸æ‘˜è¦å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ Markdown å ±å‘Š\n",
    "report = f\"\"\"# æ•¸æ“šç¯©é¸æ‘˜è¦å ±å‘Š\n",
    "\n",
    "**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ç¯©é¸æ¦‚è¦½\n",
    "\n",
    "| æŒ‡æ¨™ | æ•¸å€¼ |\n",
    "|:---|---:|\n",
    "| åŸå§‹æ¨£æœ¬æ•¸ | {len(raw_data):,} |\n",
    "| IFD ç¯©é¸å¾Œ | {len(ifd_filtered):,} |\n",
    "| DEITA ç¯©é¸å¾Œ | {len(selected_samples):,} |\n",
    "| æœ€çµ‚ä¿ç•™ç‡ | {len(selected_samples)/len(raw_data)*100:.1f}% |\n",
    "| æ•¸æ“šæ¸›å°‘ | {(1-len(selected_samples)/len(raw_data))*100:.1f}% |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. è³ªé‡æå‡\n",
    "\n",
    "### IFD åˆ†æ•¸\n",
    "- åŸå§‹å‡å€¼: {original_ifd:.4f}\n",
    "- ç¯©é¸å¾Œå‡å€¼: {filtered_ifd:.4f}\n",
    "- **æå‡**: {(filtered_ifd-original_ifd)/original_ifd*100:+.1f}%\n",
    "\n",
    "### è¤‡é›œåº¦åˆ†æ•¸\n",
    "- åŸå§‹å‡å€¼: {original_complexity:.4f}\n",
    "- ç¯©é¸å¾Œå‡å€¼: {filtered_complexity:.4f}\n",
    "- **æå‡**: {(filtered_complexity-original_complexity)/original_complexity*100:+.1f}%\n",
    "\n",
    "### DEITA è©•åˆ†åˆ†å¸ƒ\n",
    "- å‡å€¼: {np.mean(deita_scores):.4f}\n",
    "- æ¨™æº–å·®: {np.std(deita_scores):.4f}\n",
    "- ç¯„åœ: [{np.min(deita_scores):.4f}, {np.max(deita_scores):.4f}]\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ç¯©é¸åƒæ•¸\n",
    "\n",
    "### IFD é–¾å€¼\n",
    "- æœ€å°é–¾å€¼: {config['ifd_min_threshold']}\n",
    "- æœ€å¤§é–¾å€¼: {config['ifd_max_threshold']}\n",
    "\n",
    "### DEITA æ¬Šé‡\n",
    "- è¤‡é›œåº¦ (Î±): {config['deita_alpha']}\n",
    "- è³ªé‡ (Î²): {config['deita_beta']}\n",
    "- å¤šæ¨£æ€§ (Î³): {config['deita_gamma']}\n",
    "\n",
    "---\n",
    "\n",
    "## 4. é æœŸæ•ˆæœ\n",
    "\n",
    "åŸºæ–¼ DEITA è«–æ–‡çµæœ:\n",
    "- è¨“ç·´æ™‚é–“æ¸›å°‘: ~70%\n",
    "- æ¨¡å‹æ€§èƒ½: æŒå¹³æˆ–æå‡ 1-2%\n",
    "- æˆæœ¬æ•ˆç›Š: 3.4x æ›´é«˜æ•ˆ\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ **03-Validate.ipynb** é€²è¡Œè¨“ç·´é©—è­‰å¯¦é©—:\n",
    "1. è¨“ç·´åŸºç·šæ¨¡å‹ (å…¨é‡æ•¸æ“š 52K)\n",
    "2. è¨“ç·´å°æ¯”æ¨¡å‹ (ç¯©é¸æ•¸æ“š {len(selected_samples):,})\n",
    "3. è©•ä¼°æ€§èƒ½èˆ‡æ•ˆç‡å°æ¯”\n",
    "4. ç”Ÿæˆé©—è­‰å ±å‘Š\n",
    "\n",
    "---\n",
    "\n",
    "**å‚™è¨»**: æ‰€æœ‰è©•åˆ†å’Œçµ±è¨ˆæ•¸æ“šå·²ä¿å­˜è‡³ `analysis/` ç›®éŒ„ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# ä¿å­˜å ±å‘Š\n",
    "report_path = ANALYSIS_DIR / 'filtering_summary.md'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"âœ… ç¯©é¸æ‘˜è¦å·²ä¿å­˜è‡³: {report_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(report)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ç¸½çµ\n",
    "\n",
    "åœ¨æœ¬ notebook ä¸­,æˆ‘å€‘å®Œæˆäº†:\n",
    "\n",
    "1. âœ… å¯¦ç¾ IFD è¨ˆç®—å™¨ (æ‰¹é‡è™•ç†)\n",
    "2. âœ… IFD åˆæ­¥ç¯©é¸ (0.3 â‰¤ IFD â‰¤ 0.9)\n",
    "3. âœ… å¯¦ç¾ DEITA è©•åˆ†ç³»çµ±\n",
    "4. âœ… è²ªå©ªé¸æ“‡ç®—æ³• (è€ƒæ…®å¤šæ¨£æ€§)\n",
    "5. âœ… é¸æ“‡ Top-30% é«˜è³ªé‡æ¨£æœ¬\n",
    "6. âœ… ç”Ÿæˆç¯©é¸æ‘˜è¦å ±å‘Š\n",
    "\n",
    "### é—œéµæˆæœ\n",
    "\n",
    "- **æ•¸æ“šé‡**: 52,002 â†’ 15,600 (æ¸›å°‘ 70%)\n",
    "- **IFD æå‡**: +78% (æ•¸æ“šé›£åº¦é¡¯è‘—æå‡)\n",
    "- **è¤‡é›œåº¦æå‡**: +76% (ä»»å‹™æ›´å…·æŒ‘æˆ°æ€§)\n",
    "- **å¤šæ¨£æ€§**: é€šéè²ªå©ªç®—æ³•ä¿æŒ\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ **03-Validate.ipynb** é©—è­‰ç¯©é¸æ•ˆæœ:\n",
    "- å°æ¯”å…¨é‡æ•¸æ“š vs ç¯©é¸æ•¸æ“šçš„è¨“ç·´æ•ˆæœ\n",
    "- è©•ä¼°æ¨¡å‹æ€§èƒ½èˆ‡è¨“ç·´æ•ˆç‡\n",
    "- é©—è­‰ã€Œè³ªé‡ > æ•¸é‡ã€çš„å‡è¨­\n",
    "\n",
    "---\n",
    "\n",
    "**é‡è¦è§€å¯Ÿ**:\n",
    "- IFD éæ¿¾æ‰äº† ~21% çš„ç°¡å–®æˆ–ä¸ç›¸é—œä»»å‹™\n",
    "- DEITA é€²ä¸€æ­¥é¸æ“‡äº†è¤‡é›œåº¦ã€è³ªé‡ã€å¤šæ¨£æ€§æœ€å„ªçš„æ¨£æœ¬\n",
    "- ç¯©é¸å¾Œçš„æ•¸æ“šæ‡‰è©²èƒ½ä»¥æ›´å°‘çš„è¨“ç·´æ™‚é–“é”åˆ°ç›¸ç•¶æˆ–æ›´å¥½çš„æ€§èƒ½"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
