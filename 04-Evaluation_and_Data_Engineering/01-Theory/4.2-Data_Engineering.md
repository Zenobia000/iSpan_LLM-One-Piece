# 4.2 LLM æ•¸æ“šå·¥ç¨‹å…¨æµç¨‹æŒ‡å—

**æ›´æ–°æ—¥æœŸ**: 2025-10-17
**é©ç”¨å°è±¡**: LLM å·¥ç¨‹å¸«ã€æ•¸æ“šç§‘å­¸å®¶ã€ç ”ç©¶å“¡
**é–±è®€æ™‚é–“**: 45 åˆ†é˜

---

## ğŸ“‹ ç›®éŒ„

1. [ç‚ºä½•æ•¸æ“šè³ªé‡æ±ºå®šæ¨¡å‹è³ªé‡](#1-ç‚ºä½•æ•¸æ“šè³ªé‡æ±ºå®šæ¨¡å‹è³ªé‡)
2. [æ•¸æ“šå·¥ç¨‹ç”Ÿå‘½é€±æœŸ](#2-æ•¸æ“šå·¥ç¨‹ç”Ÿå‘½é€±æœŸ)
3. [é è¨“ç·´æ•¸æ“šè™•ç†](#3-é è¨“ç·´æ•¸æ“šè™•ç†)
4. [å¾®èª¿æ•¸æ“šå„ªåŒ–](#4-å¾®èª¿æ•¸æ“šå„ªåŒ–)
5. [æ•¸æ“šè³ªé‡è©•ä¼°æ–¹æ³•](#5-æ•¸æ“šè³ªé‡è©•ä¼°æ–¹æ³•)
6. [IFD ç®—æ³•åŸç†èˆ‡å¯¦ä½œ](#6-ifd-ç®—æ³•åŸç†èˆ‡å¯¦ä½œ)
7. [DEITA è©•åˆ†ç³»çµ±](#7-deita-è©•åˆ†ç³»çµ±)
8. [LESS æ¢¯åº¦é¸æ“‡æ³•](#8-less-æ¢¯åº¦é¸æ“‡æ³•)
9. [æ•¸æ“šç®¡é“è¨­è¨ˆ](#9-æ•¸æ“šç®¡é“è¨­è¨ˆ)
10. [æ•¸æ“šå€«ç†èˆ‡åˆè¦](#10-æ•¸æ“šå€«ç†èˆ‡åˆè¦)
11. [å¯¦è¸å»ºè­°](#11-å¯¦è¸å»ºè­°)

---

## 1. ç‚ºä½•æ•¸æ“šè³ªé‡æ±ºå®šæ¨¡å‹è³ªé‡

### 1.1 ç¶“å…¸ç ”ç©¶æ¡ˆä¾‹

**LIMA (Less Is More for Alignment)**ï¼š
- åƒ…ç”¨ 1,000 æ¢ç²¾å¿ƒç¯©é¸çš„é«˜è³ªé‡æ•¸æ“š
- é”åˆ°èˆ‡ 52,000 æ¢æ•¸æ“šè¨“ç·´çš„æ¨¡å‹ç›¸ç•¶çš„æ€§èƒ½
- **æ ¸å¿ƒæ´å¯Ÿ**ï¼šæ•¸æ“šè³ªé‡ > æ•¸æ“šæ•¸é‡

**Phi-1.5 (Microsoft Research)**ï¼š
- 1.3B åƒæ•¸çš„å°å‹æ¨¡å‹
- åƒ…ä½¿ç”¨ 7B tokens çš„é«˜è³ªé‡ã€Œæ•™ç§‘æ›¸ç´šã€æ•¸æ“š
- åœ¨æ¨ç†ä»»å‹™ä¸Šè¶…è¶Šè¨±å¤š 10B+ åƒæ•¸æ¨¡å‹
- **æ ¸å¿ƒæ´å¯Ÿ**ï¼šç²¾é¸æ•¸æ“šèƒ½é¡¯è‘—æå‡åƒæ•¸æ•ˆç‡

**TinyStories (Microsoft Research)**ï¼š
- è­‰æ˜åˆæˆæ•¸æ“šçš„åƒ¹å€¼
- ä½¿ç”¨ GPT-4 ç”Ÿæˆçš„é«˜è³ªé‡æ•…äº‹æ•¸æ“š
- å°æ¨¡å‹é”åˆ°é©šäººçš„èªè¨€æµæš¢åº¦

### 1.2 æ•¸æ“šè³ªé‡çš„ä¸‰å€‹ç¶­åº¦

**è¤‡é›œåº¦ (Complexity)**ï¼š
- ä»»å‹™é›£åº¦èˆ‡æ¨ç†æ·±åº¦
- èªç¾©è±å¯Œåº¦
- æŒ‡ä»¤éµå¾ªé›£åº¦ (IFD)

**å¤šæ¨£æ€§ (Diversity)**ï¼š
- ä»»å‹™é¡å‹è¦†è“‹
- èªè¨€é¢¨æ ¼è®ŠåŒ–
- é ˜åŸŸåˆ†å¸ƒå»£åº¦

**æº–ç¢ºæ€§ (Accuracy)**ï¼š
- æ¨™è¨»æ­£ç¢ºæ€§
- é‚è¼¯ä¸€è‡´æ€§
- äº‹å¯¦æº–ç¢ºæ€§

### 1.3 ä½è³ªé‡æ•¸æ“šçš„å±å®³

**éæ“¬åˆé¢¨éšª**ï¼š
- æ¨¡å‹è¨˜æ†¶ä½è³ªé‡æ¨¡å¼
- æ³›åŒ–èƒ½åŠ›ä¸‹é™

**æœ‰å®³åè¦‹**ï¼š
- æ”¾å¤§ç¤¾æœƒåè¦‹
- ç”¢ç”Ÿæœ‰å®³å…§å®¹
- é•åå€«ç†è¦ç¯„

**è¨“ç·´æµªè²»**ï¼š
- è¨ˆç®—è³‡æºæµªè²»
- è¨“ç·´æ™‚é–“å»¶é•·
- ç„¡æ•ˆçš„åƒæ•¸æ›´æ–°

---

## 2. æ•¸æ“šå·¥ç¨‹ç”Ÿå‘½é€±æœŸ

### 2.1 å®Œæ•´æµç¨‹åœ–

```
1. æ•¸æ“šæ”¶é›† (Data Collection)
   â†“
2. æ•¸æ“šæ¸…æ´— (Data Cleaning)
   â†“
3. å»é‡ (Deduplication)
   â†“
4. è³ªé‡è©•ä¼° (Quality Assessment)
   â†“
5. æ•¸æ“šéæ¿¾ (Data Filtering)
   â†“
6. æ•¸æ“šå¢å¼· (Data Augmentation)
   â†“
7. æ ¼å¼åŒ–èˆ‡æ‰“åŒ… (Formatting & Packaging)
   â†“
8. ç‰ˆæœ¬ç®¡ç† (Versioning)
```

### 2.2 å„éšæ®µæ ¸å¿ƒä»»å‹™

| éšæ®µ | æ ¸å¿ƒä»»å‹™ | å¸¸ç”¨å·¥å…· |
|:---|:---|:---|
| **æ”¶é›†** | ç¶²é çˆ¬å–ã€API ç²å–ã€çœ¾åŒ…æ¨™è¨» | Scrapy, BeautifulSoup, Selenium |
| **æ¸…æ´—** | ç§»é™¤ HTMLã€æ¨™é»æ­£è¦åŒ–ã€ç·¨ç¢¼ä¿®å¾© | ftfy, cleantext, regex |
| **å»é‡** | ç²¾ç¢ºå»é‡ã€è¿‘ä¼¼å»é‡ã€è·¨èªè¨€å»é‡ | MinHash, SimHash, Deduplicate |
| **è©•ä¼°** | å›°æƒ‘åº¦ã€IFDã€å¤šæ¨£æ€§ã€æœ‰å®³æ€§æª¢æ¸¬ | Perspective API, toxicity-classifier |
| **éæ¿¾** | åŸºæ–¼è¦å‰‡ã€åŸºæ–¼æ¨¡å‹ã€åŸºæ–¼æ¢¯åº¦ | DEITA, LESS, AlpaGasus |
| **å¢å¼·** | å›è­¯ã€åŒç¾©æ›¿æ›ã€åˆæˆæ•¸æ“š | GPT-4, Claude, nlpaug |
| **æ ¼å¼åŒ–** | JSONLã€Parquetã€Arrow | Datasets, Pandas |
| **ç‰ˆæœ¬ç®¡ç†** | æ•¸æ“šç‰ˆæœ¬ã€è¡€ç·£è¿½è¹¤ã€å¯¦é©—ç®¡ç† | DVC, Git LFS, Weights & Biases |

---

## 3. é è¨“ç·´æ•¸æ“šè™•ç†

### 3.1 æ•¸æ“šä¾†æº

**ç¶²é çˆ¬å–æ•¸æ“š**ï¼š
- Common Crawl (æ•¸ç™¾ TB ç¶²é æ•¸æ“š)
- OpenWebText (Reddit é«˜åˆ†æ–‡ç« )
- C4 (Colossal Clean Crawled Corpus)

**æ›¸ç±èˆ‡å­¸è¡“**ï¼š
- BookCorpus
- arXiv è«–æ–‡
- PubMed é†«å­¸æ–‡ç»

**ä»£ç¢¼æ•¸æ“š**ï¼š
- GitHub å…¬é–‹ä»£ç¢¼
- StackOverflow å•ç­”
- Jupyter Notebooks

### 3.2 é è¨“ç·´æ•¸æ“šæ¸…æ´—æµç¨‹

**ç¬¬ä¸€æ­¥ï¼šåŸºç¤æ¸…æ´—**

```python
import ftfy
import re

def basic_cleaning(text):
    """åŸºç¤æ–‡æœ¬æ¸…æ´—"""
    # ä¿®å¾© Unicode ç·¨ç¢¼å•é¡Œ
    text = ftfy.fix_text(text)
    
    # ç§»é™¤ HTML æ¨™ç±¤
    text = re.sub(r'<[^>]+>', '', text)
    
    # ç§»é™¤éå¤šç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    
    # ç§»é™¤æ§åˆ¶å­—å…ƒ
    text = ''.join(char for char in text if ord(char) >= 32 or char == '\n')
    
    return text.strip()
```

**ç¬¬äºŒæ­¥ï¼šè³ªé‡éæ¿¾**

```python
def quality_filter(text, min_words=50, max_words=100000):
    """åŸºæ–¼è¦å‰‡çš„è³ªé‡éæ¿¾"""
    words = text.split()
    word_count = len(words)
    
    # éæ¿¾éçŸ­æˆ–éé•·çš„æ–‡æœ¬
    if word_count < min_words or word_count > max_words:
        return False
    
    # è¨ˆç®—å¹³å‡è©é•·
    avg_word_length = sum(len(w) for w in words) / word_count
    if avg_word_length < 3 or avg_word_length > 15:
        return False
    
    # è¨ˆç®—ç¬¦è™Ÿæ¯”ä¾‹
    symbol_ratio = len(re.findall(r'[^\w\s]', text)) / len(text)
    if symbol_ratio > 0.3:
        return False
    
    # è¨ˆç®—å¤§å¯«å­—æ¯æ¯”ä¾‹
    upper_ratio = sum(1 for c in text if c.isupper()) / len(text)
    if upper_ratio > 0.3:
        return False
    
    return True
```

**ç¬¬ä¸‰æ­¥ï¼šå»é‡ (MinHash)**

```python
from datasketch import MinHash, MinHashLSH

def create_minhash(text, num_perm=128):
    """å‰µå»º MinHash ç°½å"""
    m = MinHash(num_perm=num_perm)
    for word in text.split():
        m.update(word.encode('utf8'))
    return m

def deduplicate_corpus(texts, threshold=0.8):
    """ä½¿ç”¨ MinHash LSH é€²è¡Œå»é‡"""
    lsh = MinHashLSH(threshold=threshold, num_perm=128)
    
    unique_texts = []
    seen_ids = set()
    
    for idx, text in enumerate(texts):
        minhash = create_minhash(text)
        
        # æŸ¥è©¢ç›¸ä¼¼æ–‡æœ¬
        result = lsh.query(minhash)
        
        if not result:
            # æ²’æœ‰ç›¸ä¼¼æ–‡æœ¬,ä¿ç•™
            lsh.insert(f"doc_{idx}", minhash)
            unique_texts.append(text)
            seen_ids.add(idx)
    
    return unique_texts

# ä½¿ç”¨ç¯„ä¾‹
corpus = ["This is a sample text.", "This is a sample text.", "Different content."]
unique_corpus = deduplicate_corpus(corpus, threshold=0.8)
print(f"Original: {len(corpus)}, Unique: {len(unique_corpus)}")
```

### 3.3 å›°æƒ‘åº¦éæ¿¾ (Perplexity Filtering)

**åŸç†**ï¼šä½¿ç”¨é«˜è³ªé‡èªè¨€æ¨¡å‹è¨ˆç®—æ–‡æœ¬çš„å›°æƒ‘åº¦,éæ¿¾ä½è³ªé‡æ–‡æœ¬ã€‚

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

class PerplexityFilter:
    def __init__(self, model_name='gpt2'):
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.model.eval()
    
    def calculate_perplexity(self, text):
        """è¨ˆç®—æ–‡æœ¬å›°æƒ‘åº¦"""
        encodings = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=1024)
        
        with torch.no_grad():
            outputs = self.model(**encodings, labels=encodings['input_ids'])
            loss = outputs.loss
            perplexity = torch.exp(loss).item()
        
        return perplexity
    
    def filter_by_perplexity(self, texts, max_ppl=1000):
        """æ ¹æ“šå›°æƒ‘åº¦éæ¿¾æ–‡æœ¬"""
        filtered = []
        for text in texts:
            ppl = self.calculate_perplexity(text)
            if ppl < max_ppl:
                filtered.append((text, ppl))
        return filtered

# ä½¿ç”¨ç¯„ä¾‹
ppl_filter = PerplexityFilter()
texts = ["This is a well-formed sentence.", "asdfjkl qwerty zxcvbn"]
filtered_texts = ppl_filter.filter_by_perplexity(texts, max_ppl=500)
```

---

## 4. å¾®èª¿æ•¸æ“šå„ªåŒ–

### 4.1 æŒ‡ä»¤å¾®èª¿æ•¸æ“šæ ¼å¼

**æ¨™æº–æ ¼å¼ (Alpaca)**ï¼š

```json
{
  "instruction": "å°‡ä»¥ä¸‹è‹±æ–‡ç¿»è­¯æˆä¸­æ–‡",
  "input": "The quick brown fox jumps over the lazy dog.",
  "output": "æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³éæ‡¶ç‹—ã€‚"
}
```

**å°è©±æ ¼å¼ (ChatML)**ï¼š

```json
{
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?"},
    {"role": "assistant", "content": "æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹åˆ†æ”¯..."}
  ]
}
```

### 4.2 æ•¸æ“šè³ªé‡ä¸‰è¦ç´ 

**è¤‡é›œåº¦ (Complexity)**ï¼š
- ä»»å‹™é›£åº¦
- æ¨ç†æ­¥é©Ÿæ•¸
- èªç¾©æ·±åº¦

**å¤šæ¨£æ€§ (Diversity)**ï¼š
- ä»»å‹™é¡å‹åˆ†å¸ƒ
- é ˜åŸŸè¦†è“‹
- èªè¨€é¢¨æ ¼

**æº–ç¢ºæ€§ (Accuracy)**ï¼š
- é‚è¼¯æ­£ç¢ºæ€§
- äº‹å¯¦æº–ç¢ºæ€§
- æ ¼å¼è¦ç¯„æ€§

---

## 5. æ•¸æ“šè³ªé‡è©•ä¼°æ–¹æ³•

### 5.1 åŸºæ–¼è¦å‰‡çš„è©•ä¼°

```python
def rule_based_quality_score(sample):
    """åŸºæ–¼è¦å‰‡çš„è³ªé‡è©•åˆ†"""
    instruction = sample['instruction']
    output = sample['output']
    
    score = 0.0
    
    # 1. é•·åº¦åˆç†æ€§ (0-20 åˆ†)
    if 10 <= len(instruction.split()) <= 100:
        score += 10
    if 20 <= len(output.split()) <= 500:
        score += 10
    
    # 2. è¤‡é›œåº¦ (0-20 åˆ†)
    # ä½¿ç”¨è©å½™è±å¯Œåº¦
    unique_ratio = len(set(output.split())) / len(output.split())
    score += unique_ratio * 20
    
    # 3. æ ¼å¼å®Œæ•´æ€§ (0-20 åˆ†)
    if instruction and output:
        score += 20
    
    # 4. æ¨™é»ç¬¦è™Ÿä½¿ç”¨ (0-20 åˆ†)
    punct_count = len(re.findall(r'[.,!?;:]', output))
    if 2 <= punct_count <= 20:
        score += 20
    
    # 5. ç„¡é‡è¤‡å…§å®¹ (0-20 åˆ†)
    words = output.split()
    if len(words) > 0:
        duplicate_ratio = 1 - (len(set(words)) / len(words))
        score += (1 - duplicate_ratio) * 20
    
    return min(score, 100)  # æœ€é«˜ 100 åˆ†
```

### 5.2 åŸºæ–¼æ¨¡å‹çš„è©•ä¼°

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

class ModelBasedQualityScorer:
    def __init__(self, model_name='roberta-base'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=1
        )
        self.model.eval()
    
    def score_sample(self, instruction, output):
        """ä½¿ç”¨æ¨¡å‹è©•ä¼°è³ªé‡"""
        text = f"Instruction: {instruction}\nOutput: {output}"
        
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            score = torch.sigmoid(outputs.logits).item()
        
        return score * 100  # è½‰æ›ç‚º 0-100 åˆ†

# ä½¿ç”¨ç¯„ä¾‹
scorer = ModelBasedQualityScorer()
score = scorer.score_sample(
    instruction="è§£é‡‹ä»€éº¼æ˜¯é‡å­è¨ˆç®—",
    output="é‡å­è¨ˆç®—æ˜¯ä¸€ç¨®åˆ©ç”¨é‡å­åŠ›å­¸åŸç†é€²è¡Œä¿¡æ¯è™•ç†çš„è¨ˆç®—æ–¹å¼..."
)
```

---

## 6. IFD ç®—æ³•åŸç†èˆ‡å¯¦ä½œ

### 6.1 IFD (Instruction Following Difficulty) åŸç†

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¸¬é‡æŒ‡ä»¤èˆ‡å›æ‡‰ä¹‹é–“çš„èªç¾©è·é›¢,è·é›¢è¶Šå¤§è¡¨ç¤ºä»»å‹™è¶Šå›°é›£ã€‚

**å‡è¨­**ï¼š
- ç°¡å–®ä»»å‹™çš„å›æ‡‰èˆ‡æŒ‡ä»¤é«˜åº¦ç›¸é—œ (å¦‚ã€Œç¿»è­¯ã€)
- å›°é›£ä»»å‹™çš„å›æ‡‰éœ€è¦æ¨ç†,èˆ‡æŒ‡ä»¤èªç¾©è·é›¢è¼ƒå¤§ (å¦‚ã€Œåˆ†æã€)

**æ•¸å­¸è¡¨ç¤º**ï¼š

```
IFD = 1 - cosine_similarity(embedding(instruction), embedding(response))
```

### 6.2 IFD å®Œæ•´å¯¦ä½œ

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class IFDCalculator:
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):
        """
        åˆå§‹åŒ– IFD è¨ˆç®—å™¨
        
        Args:
            model_name: å¥å­åµŒå…¥æ¨¡å‹åç¨±
        """
        self.model = SentenceTransformer(model_name)
    
    def calculate_ifd(self, instruction, response):
        """
        è¨ˆç®—å–®å€‹æ¨£æœ¬çš„ IFD
        
        Args:
            instruction: æŒ‡ä»¤æ–‡æœ¬
            response: å›æ‡‰æ–‡æœ¬
        
        Returns:
            float: IFD åˆ†æ•¸ (0-1,è¶Šé«˜è¶Šå›°é›£)
        """
        # ç”ŸæˆåµŒå…¥å‘é‡
        instr_emb = self.model.encode([instruction])
        resp_emb = self.model.encode([response])
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(instr_emb, resp_emb)[0][0]
        
        # IFD = 1 - similarity
        ifd = 1 - similarity
        
        return ifd
    
    def calculate_batch_ifd(self, samples):
        """
        æ‰¹é‡è¨ˆç®— IFD
        
        Args:
            samples: List of dicts with 'instruction' and 'output' keys
        
        Returns:
            List of (sample, ifd_score) tuples
        """
        instructions = [s['instruction'] for s in samples]
        responses = [s['output'] for s in samples]
        
        # æ‰¹é‡ç·¨ç¢¼
        instr_embs = self.model.encode(instructions, show_progress_bar=True)
        resp_embs = self.model.encode(responses, show_progress_bar=True)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarities = np.array([
            cosine_similarity([instr_embs[i]], [resp_embs[i]])[0][0]
            for i in range(len(samples))
        ])
        
        # è¨ˆç®— IFD
        ifds = 1 - similarities
        
        return [(samples[i], ifds[i]) for i in range(len(samples))]
    
    def filter_by_ifd(self, samples, min_ifd=0.3, max_ifd=0.9):
        """
        æ ¹æ“š IFD éæ¿¾æ¨£æœ¬
        
        Args:
            samples: æ¨£æœ¬åˆ—è¡¨
            min_ifd: æœ€å° IFD (éæ¿¾å¤ªç°¡å–®çš„)
            max_ifd: æœ€å¤§ IFD (éæ¿¾å¤ªå›°é›£/ä¸ç›¸é—œçš„)
        
        Returns:
            List of filtered samples
        """
        samples_with_ifd = self.calculate_batch_ifd(samples)
        
        filtered = [
            sample for sample, ifd in samples_with_ifd
            if min_ifd <= ifd <= max_ifd
        ]
        
        return filtered

# ä½¿ç”¨ç¯„ä¾‹
ifd_calc = IFDCalculator()

samples = [
    {
        "instruction": "å°‡ 'hello' ç¿»è­¯æˆä¸­æ–‡",
        "output": "ä½ å¥½"
    },
    {
        "instruction": "åˆ†æé‡å­è¨ˆç®—çš„æœªä¾†ç™¼å±•è¶¨å‹¢",
        "output": "é‡å­è¨ˆç®—æœªä¾†å°‡åœ¨å¯†ç¢¼å­¸ã€è—¥ç‰©è¨­è¨ˆã€ææ–™ç§‘å­¸ç­‰é ˜åŸŸç”¢ç”Ÿé©å‘½æ€§å½±éŸ¿..."
    }
]

# è¨ˆç®— IFD
for sample in samples:
    ifd = ifd_calc.calculate_ifd(sample['instruction'], sample['output'])
    print(f"IFD: {ifd:.3f} - {sample['instruction'][:30]}...")

# éæ¿¾æ¨£æœ¬
filtered = ifd_calc.filter_by_ifd(samples, min_ifd=0.3, max_ifd=0.9)
print(f"\nFiltered: {len(filtered)} / {len(samples)}")
```

### 6.3 IFD çš„å„ªå‹¢èˆ‡é™åˆ¶

**å„ªå‹¢**ï¼š
- ç„¡éœ€è¨“ç·´,ç›´æ¥è¨ˆç®—
- è¨ˆç®—æ•ˆç‡é«˜
- é©ç”¨æ–¼å„ç¨®èªè¨€

**é™åˆ¶**ï¼š
- ä¾è³´åµŒå…¥æ¨¡å‹è³ªé‡
- ç„¡æ³•æ•æ‰é‚è¼¯éŒ¯èª¤
- å°æŸäº›ä»»å‹™é¡å‹ä¸æº–ç¢º (å¦‚å‰µæ„å¯«ä½œ)

---

## 7. DEITA è©•åˆ†ç³»çµ±

### 7.1 DEITA åŸç†

**DEITA (Data-Efficient Instruction Tuning for Alignment)** çµåˆä¸‰å€‹ç¶­åº¦ï¼š

1. **Complexity**: ä½¿ç”¨ LLM è©•ä¼°ä»»å‹™è¤‡é›œåº¦
2. **Quality**: ä½¿ç”¨ LLM è©•ä¼°å›æ‡‰è³ªé‡
3. **Diversity**: ä½¿ç”¨åµŒå…¥å‘é‡è©•ä¼°å¤šæ¨£æ€§

**æœ€çµ‚åˆ†æ•¸**ï¼š

```
DEITA_score = Î± Ã— Complexity + Î² Ã— Quality + Î³ Ã— Diversity
```

å¸¸ç”¨æ¬Šé‡ï¼šÎ±=0.4, Î²=0.4, Î³=0.2

### 7.2 DEITA å®Œæ•´å¯¦ä½œ

```python
import openai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class DEITAScorer:
    def __init__(self, 
                 openai_api_key,
                 model_name='gpt-4',
                 embedding_model='sentence-transformers/all-MiniLM-L6-v2',
                 alpha=0.4,
                 beta=0.4,
                 gamma=0.2):
        """
        åˆå§‹åŒ– DEITA è©•åˆ†å™¨
        
        Args:
            openai_api_key: OpenAI API å¯†é‘°
            model_name: ç”¨æ–¼è©•ä¼°çš„ LLM æ¨¡å‹
            embedding_model: ç”¨æ–¼å¤šæ¨£æ€§è¨ˆç®—çš„åµŒå…¥æ¨¡å‹
            alpha, beta, gamma: ä¸‰å€‹ç¶­åº¦çš„æ¬Šé‡
        """
        openai.api_key = openai_api_key
        self.model_name = model_name
        self.embedding_model = SentenceTransformer(embedding_model)
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
    
    def evaluate_complexity(self, instruction):
        """ä½¿ç”¨ LLM è©•ä¼°æŒ‡ä»¤è¤‡é›œåº¦"""
        prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹æŒ‡ä»¤çš„è¤‡é›œåº¦,å¾ 1 åˆ° 10 æ‰“åˆ†,å…¶ä¸­:
1-3: ç°¡å–® (å¦‚ç¿»è­¯å–®è©ã€åŸºç¤è¨ˆç®—)
4-6: ä¸­ç­‰ (å¦‚æ®µè½ç¿»è­¯ã€å¤šæ­¥é©Ÿæ¨ç†)
7-10: å›°é›£ (å¦‚è¤‡é›œåˆ†æã€å‰µæ„å¯«ä½œã€å°ˆæ¥­é ˜åŸŸ)

æŒ‡ä»¤: {instruction}

è«‹åƒ…å›è¦†ä¸€å€‹ 1-10 çš„æ•¸å­—ã€‚"""

        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        
        try:
            score = int(response.choices[0].message.content.strip())
            return min(max(score, 1), 10) / 10  # æ­£è¦åŒ–åˆ° 0-1
        except:
            return 0.5  # é è¨­ä¸­ç­‰è¤‡é›œåº¦
    
    def evaluate_quality(self, instruction, output):
        """ä½¿ç”¨ LLM è©•ä¼°å›æ‡‰è³ªé‡"""
        prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹å›æ‡‰çš„è³ªé‡,å¾ 1 åˆ° 10 æ‰“åˆ†,è€ƒæ…®:
- æº–ç¢ºæ€§: å›æ‡‰æ˜¯å¦æ­£ç¢º
- å®Œæ•´æ€§: æ˜¯å¦å……åˆ†å›ç­”å•é¡Œ
- æ¸…æ™°åº¦: è¡¨é”æ˜¯å¦æ¸…æ™°
- å¯¦ç”¨æ€§: æ˜¯å¦æœ‰å¯¦éš›åƒ¹å€¼

æŒ‡ä»¤: {instruction}
å›æ‡‰: {output}

è«‹åƒ…å›è¦†ä¸€å€‹ 1-10 çš„æ•¸å­—ã€‚"""

        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        
        try:
            score = int(response.choices[0].message.content.strip())
            return min(max(score, 1), 10) / 10
        except:
            return 0.5
    
    def evaluate_diversity(self, sample, reference_samples):
        """è¨ˆç®—æ¨£æœ¬èˆ‡åƒè€ƒé›†çš„å¤šæ¨£æ€§"""
        sample_text = f"{sample['instruction']} {sample['output']}"
        sample_emb = self.embedding_model.encode([sample_text])
        
        if not reference_samples:
            return 1.0  # ç¬¬ä¸€å€‹æ¨£æœ¬,æœ€å¤§å¤šæ¨£æ€§
        
        ref_texts = [f"{s['instruction']} {s['output']}" for s in reference_samples]
        ref_embs = self.embedding_model.encode(ref_texts)
        
        # è¨ˆç®—èˆ‡æ‰€æœ‰åƒè€ƒæ¨£æœ¬çš„æœ€å¤§ç›¸ä¼¼åº¦
        similarities = cosine_similarity(sample_emb, ref_embs)[0]
        max_similarity = np.max(similarities)
        
        # å¤šæ¨£æ€§ = 1 - æœ€å¤§ç›¸ä¼¼åº¦
        diversity = 1 - max_similarity
        
        return diversity
    
    def score_sample(self, sample, reference_samples=None):
        """è¨ˆç®—æ¨£æœ¬çš„ DEITA åˆ†æ•¸"""
        instruction = sample['instruction']
        output = sample['output']
        
        # è¨ˆç®—ä¸‰å€‹ç¶­åº¦
        complexity = self.evaluate_complexity(instruction)
        quality = self.evaluate_quality(instruction, output)
        diversity = self.evaluate_diversity(sample, reference_samples or [])
        
        # åŠ æ¬Šæ±‚å’Œ
        deita_score = (
            self.alpha * complexity +
            self.beta * quality +
            self.gamma * diversity
        )
        
        return {
            'sample': sample,
            'complexity': complexity,
            'quality': quality,
            'diversity': diversity,
            'deita_score': deita_score
        }
    
    def select_top_k(self, samples, k):
        """é¸æ“‡ DEITA åˆ†æ•¸æœ€é«˜çš„ k å€‹æ¨£æœ¬"""
        scored_samples = []
        selected = []
        
        for sample in samples:
            result = self.score_sample(sample, selected)
            scored_samples.append(result)
            selected.append(sample)
        
        # æ’åºä¸¦é¸æ“‡ top-k
        scored_samples.sort(key=lambda x: x['deita_score'], reverse=True)
        
        return scored_samples[:k]

# ä½¿ç”¨ç¯„ä¾‹ (éœ€è¦ OpenAI API Key)
# scorer = DEITAScorer(openai_api_key='your-key-here')
# top_samples = scorer.select_top_k(all_samples, k=1000)
```

### 7.3 DEITA çš„å„ªå‹¢

- **å…¨é¢æ€§**: åŒæ™‚è€ƒæ…®è¤‡é›œåº¦ã€è³ªé‡ã€å¤šæ¨£æ€§
- **æœ‰æ•ˆæ€§**: ç ”ç©¶é¡¯ç¤ºå¯ç”¨ 6% æ•¸æ“šé”åˆ°å…¨é‡è¨“ç·´ 90% æ•ˆæœ
- **éˆæ´»æ€§**: æ¬Šé‡å¯æ ¹æ“šæ‡‰ç”¨å ´æ™¯èª¿æ•´

---

## 8. LESS æ¢¯åº¦é¸æ“‡æ³•

### 8.1 LESS åŸç†

**LESS (Low-Effort Score Sampling)** åŸºæ–¼æ¢¯åº¦ä¿¡æ¯é¸æ“‡æ•¸æ“šï¼š

**æ ¸å¿ƒæ€æƒ³**ï¼š
- å°æ¨¡å‹å½±éŸ¿å¤§çš„æ¨£æœ¬ = é«˜æ¢¯åº¦ç¯„æ•¸
- é¸æ“‡æ¢¯åº¦ç¯„æ•¸æœ€å¤§çš„ k å€‹æ¨£æœ¬

**å„ªå‹¢**ï¼š
- ç›´æ¥åŸºæ–¼æ¨¡å‹è¨“ç·´ä¿¡è™Ÿ
- ä¸ä¾è³´å¤–éƒ¨ LLM
- è¨ˆç®—æ•ˆç‡é«˜

### 8.2 LESS å¯¦ä½œ

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader

class LESSSelector:
    def __init__(self, model_name, device='cuda'):
        """
        åˆå§‹åŒ– LESS é¸æ“‡å™¨
        
        Args:
            model_name: é è¨“ç·´æ¨¡å‹åç¨±
            device: è¨ˆç®—è¨­å‚™
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
        self.device = device
    
    def compute_gradient_norm(self, sample):
        """è¨ˆç®—å–®å€‹æ¨£æœ¬çš„æ¢¯åº¦ç¯„æ•¸"""
        text = f"{sample['instruction']}\n{sample['output']}"
        
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            max_length=512
        ).to(self.device)
        
        # å‰å‘å‚³æ’­
        outputs = self.model(**inputs, labels=inputs['input_ids'])
        loss = outputs.loss
        
        # åå‘å‚³æ’­
        self.model.zero_grad()
        loss.backward()
        
        # è¨ˆç®—æ¢¯åº¦ç¯„æ•¸
        total_norm = 0.0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.detach().data.norm(2)
                total_norm += param_norm.item() ** 2
        
        total_norm = total_norm ** 0.5
        
        return total_norm
    
    def select_top_k(self, samples, k):
        """é¸æ“‡æ¢¯åº¦ç¯„æ•¸æœ€å¤§çš„ k å€‹æ¨£æœ¬"""
        sample_scores = []
        
        self.model.eval()
        
        for sample in samples:
            grad_norm = self.compute_gradient_norm(sample)
            sample_scores.append((sample, grad_norm))
        
        # æ’åº
        sample_scores.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å› top-k
        return [s[0] for s in sample_scores[:k]]

# ä½¿ç”¨ç¯„ä¾‹
# selector = LESSSelector('gpt2')
# selected = selector.select_top_k(all_samples, k=1000)
```

### 8.3 æ–¹æ³•å°æ¯”

| æ–¹æ³• | è¨ˆç®—æˆæœ¬ | æ˜¯å¦éœ€è¦ LLM API | æ•ˆæœ | é©ç”¨å ´æ™¯ |
|:---|:---:|:---:|:---:|:---|
| **IFD** | ä½ | âŒ | ä¸­ | å¿«é€Ÿéæ¿¾,å¤§è¦æ¨¡æ•¸æ“š |
| **DEITA** | é«˜ | âœ… | é«˜ | ç²¾é¸å°æ•¸æ“šé›† |
| **LESS** | ä¸­ | âŒ | é«˜ | æœ‰é è¨“ç·´æ¨¡å‹å¯ç”¨ |

---

## 9. æ•¸æ“šç®¡é“è¨­è¨ˆ

### 9.1 å®Œæ•´ç®¡é“æ¶æ§‹

```python
from typing import List, Dict, Callable
import json
from pathlib import Path

class DataPipeline:
    def __init__(self, output_dir='./processed_data'):
        """æ•¸æ“šè™•ç†ç®¡é“"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.processors = []
    
    def add_processor(self, name: str, func: Callable):
        """æ·»åŠ è™•ç†æ­¥é©Ÿ"""
        self.processors.append((name, func))
    
    def run(self, input_data: List[Dict], save_intermediate=True):
        """åŸ·è¡Œå®Œæ•´ç®¡é“"""
        data = input_data
        
        for step_num, (name, func) in enumerate(self.processors, 1):
            print(f"Step {step_num}: {name}")
            print(f"  Input size: {len(data)}")
            
            # åŸ·è¡Œè™•ç†
            data = func(data)
            
            print(f"  Output size: {len(data)}")
            
            # ä¿å­˜ä¸­é–“çµæœ
            if save_intermediate:
                output_path = self.output_dir / f"step_{step_num}_{name}.jsonl"
                self._save_jsonl(data, output_path)
                print(f"  Saved to: {output_path}")
            
            print()
        
        return data
    
    def _save_jsonl(self, data, path):
        """ä¿å­˜ç‚º JSONL æ ¼å¼"""
        with open(path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

# ä½¿ç”¨ç¯„ä¾‹ï¼šå®Œæ•´æ•¸æ“šè™•ç†ç®¡é“
def example_pipeline():
    # åˆå§‹åŒ–ç®¡é“
    pipeline = DataPipeline(output_dir='./processed_data')
    
    # 1. æ¸…æ´—
    def clean_step(data):
        return [s for s in data if len(s.get('output', '').split()) > 10]
    
    # 2. å»é‡
    def dedup_step(data):
        seen = set()
        unique = []
        for s in data:
            key = s['instruction'] + s['output']
            if key not in seen:
                seen.add(key)
                unique.append(s)
        return unique
    
    # 3. IFD éæ¿¾
    ifd_calc = IFDCalculator()
    def ifd_step(data):
        return ifd_calc.filter_by_ifd(data, min_ifd=0.3, max_ifd=0.9)
    
    # 4. DEITA é¸æ“‡ top 1000
    # scorer = DEITAScorer(openai_api_key='your-key')
    # def deita_step(data):
    #     return scorer.select_top_k(data, k=1000)
    
    # æ·»åŠ è™•ç†æ­¥é©Ÿ
    pipeline.add_processor('cleaning', clean_step)
    pipeline.add_processor('deduplication', dedup_step)
    pipeline.add_processor('ifd_filtering', ifd_step)
    # pipeline.add_processor('deita_selection', deita_step)
    
    # åŸ·è¡Œç®¡é“
    raw_data = [...]  # è¼‰å…¥åŸå§‹æ•¸æ“š
    processed_data = pipeline.run(raw_data)
    
    return processed_data
```

### 9.2 æ•¸æ“šç‰ˆæœ¬ç®¡ç†

```bash
# ä½¿ç”¨ DVC (Data Version Control) ç®¡ç†æ•¸æ“šç‰ˆæœ¬
dvc init
dvc add datasets/alpaca_filtered.jsonl
git add datasets/alpaca_filtered.jsonl.dvc
git commit -m "Add filtered alpaca dataset v1.0"
```

---

## 10. æ•¸æ“šå€«ç†èˆ‡åˆè¦

### 10.1 æ•¸æ“šä¾†æºåˆè¦

**å¿…é ˆæª¢æŸ¥**ï¼š
- æ•¸æ“šæˆæ¬Šå”è­° (CC-BY, MIT, Apache 2.0)
- å€‹äººéš±ç§ä¿è­· (GDPR, CCPA)
- ç‰ˆæ¬Šå•é¡Œ (æ›¸ç±ã€æ–‡ç« ã€ä»£ç¢¼)

### 10.2 æœ‰å®³å…§å®¹æª¢æ¸¬

```python
from transformers import pipeline

class ToxicityDetector:
    def __init__(self):
        self.classifier = pipeline(
            "text-classification",
            model="unitary/toxic-bert"
        )
    
    def detect_toxicity(self, text, threshold=0.7):
        """æª¢æ¸¬æœ‰å®³å…§å®¹"""
        result = self.classifier(text)[0]
        
        is_toxic = (
            result['label'] == 'toxic' and 
            result['score'] > threshold
        )
        
        return is_toxic, result['score']
    
    def filter_toxic_samples(self, samples):
        """éæ¿¾æœ‰å®³æ¨£æœ¬"""
        safe_samples = []
        
        for sample in samples:
            text = f"{sample['instruction']} {sample['output']}"
            is_toxic, score = self.detect_toxicity(text)
            
            if not is_toxic:
                safe_samples.append(sample)
        
        return safe_samples

# ä½¿ç”¨ç¯„ä¾‹
detector = ToxicityDetector()
safe_data = detector.filter_toxic_samples(raw_data)
```

### 10.3 åè¦‹æª¢æ¸¬

```python
def detect_bias(text, bias_keywords):
    """ç°¡å–®çš„åè¦‹æª¢æ¸¬"""
    text_lower = text.lower()
    
    detected_biases = []
    for category, keywords in bias_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                detected_biases.append((category, keyword))
    
    return detected_biases

# ç¯„ä¾‹åè¦‹é—œéµå­—
bias_keywords = {
    'gender': ['å¥³ç”Ÿä¸é©åˆ', 'ç”·ç”Ÿæ‡‰è©²', 'å¥³æ€§å¤©ç”Ÿ'],
    'race': ['æŸæ—è£”å¤©ç”Ÿ', 'ç¨®æ—å„ªè¶Š'],
    'age': ['å¹´è¼•äººæ‰èƒ½', 'è€å¹´äººä¸æœƒ']
}
```

---

## 11. å¯¦è¸å»ºè­°

### 11.1 æ•¸æ“šè™•ç†æœ€ä½³å¯¦è¸

**DO**ï¼š
- ä¿å­˜æ‰€æœ‰ä¸­é–“çµæœ
- ä½¿ç”¨æ•¸æ“šç‰ˆæœ¬ç®¡ç† (DVC)
- è¨˜éŒ„æ‰€æœ‰è™•ç†æ­¥é©Ÿèˆ‡åƒæ•¸
- å®šæœŸå¯©æŸ¥æ•¸æ“šè³ªé‡

**DON'T**ï¼š
- ä¸è¦åˆªé™¤åŸå§‹æ•¸æ“š
- ä¸è¦è·³éå»é‡æ­¥é©Ÿ
- ä¸è¦å¿½è¦–æ•¸æ“šå€«ç†
- ä¸è¦éåº¦ä¾è³´è‡ªå‹•åŒ–è©•ä¼°

### 11.2 å¯¦é©—è¿½è¹¤

```python
import wandb

# åˆå§‹åŒ– Weights & Biases
wandb.init(project='data-filtering', name='alpaca-deita-v1')

# è¨˜éŒ„æ•¸æ“šçµ±è¨ˆ
wandb.log({
    'raw_samples': len(raw_data),
    'after_cleaning': len(cleaned_data),
    'after_dedup': len(deduped_data),
    'after_ifd': len(ifd_filtered),
    'final_samples': len(final_data),
    'avg_ifd': np.mean([s['ifd'] for s in final_data]),
    'avg_deita_score': np.mean([s['deita_score'] for s in final_data])
})
```

### 11.3 è³ªé‡ç›£æ§çœ‹æ¿

```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_data_quality_dashboard(samples_with_scores):
    """ç¹ªè£½æ•¸æ“šè³ªé‡çœ‹æ¿"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. IFD åˆ†å¸ƒ
    ifds = [s['ifd'] for s in samples_with_scores]
    axes[0, 0].hist(ifds, bins=50, edgecolor='black')
    axes[0, 0].set_title('IFD Distribution')
    axes[0, 0].set_xlabel('IFD Score')
    
    # 2. DEITA åˆ†æ•¸åˆ†å¸ƒ
    deita_scores = [s['deita_score'] for s in samples_with_scores]
    axes[0, 1].hist(deita_scores, bins=50, edgecolor='black')
    axes[0, 1].set_title('DEITA Score Distribution')
    axes[0, 1].set_xlabel('DEITA Score')
    
    # 3. è¤‡é›œåº¦ vs è³ªé‡
    complexities = [s['complexity'] for s in samples_with_scores]
    qualities = [s['quality'] for s in samples_with_scores]
    axes[1, 0].scatter(complexities, qualities, alpha=0.5)
    axes[1, 0].set_title('Complexity vs Quality')
    axes[1, 0].set_xlabel('Complexity')
    axes[1, 0].set_ylabel('Quality')
    
    # 4. æ•¸æ“šé•·åº¦åˆ†å¸ƒ
    lengths = [len(s['sample']['output'].split()) for s in samples_with_scores]
    axes[1, 1].hist(lengths, bins=50, edgecolor='black')
    axes[1, 1].set_title('Response Length Distribution')
    axes[1, 1].set_xlabel('Word Count')
    
    plt.tight_layout()
    plt.savefig('data_quality_dashboard.png', dpi=300)
    plt.show()
```

---

## ğŸ“š ç¸½çµ

### æ ¸å¿ƒè¦é»

1. **è³ªé‡å„ªæ–¼æ•¸é‡**ï¼šLIMA å’Œ Phi-1.5 è­‰æ˜å°‘é‡é«˜è³ªé‡æ•¸æ“šçš„åƒ¹å€¼
2. **ä¸‰å¤§è©•ä¼°ç¶­åº¦**ï¼šè¤‡é›œåº¦ã€å¤šæ¨£æ€§ã€æº–ç¢ºæ€§
3. **ä¸‰å¤§éæ¿¾æ–¹æ³•**ï¼š
   - IFD (å¿«é€Ÿã€ç„¡éœ€ API)
   - DEITA (å…¨é¢ã€éœ€è¦ LLM)
   - LESS (åŸºæ–¼æ¢¯åº¦ã€éœ€è¦æ¨¡å‹)
4. **å®Œæ•´ç®¡é“**ï¼šæ”¶é›† â†’ æ¸…æ´— â†’ å»é‡ â†’ è©•ä¼° â†’ éæ¿¾ â†’ ç‰ˆæœ¬ç®¡ç†
5. **å€«ç†åˆè¦**ï¼šæ•¸æ“šæˆæ¬Šã€éš±ç§ä¿è­·ã€æœ‰å®³å…§å®¹æª¢æ¸¬

### æ¨è–¦å·¥ä½œæµ

**å¿«é€Ÿå¯¦é©—** (1-2 å°æ™‚)ï¼š
```
åŸå§‹æ•¸æ“š â†’ åŸºç¤æ¸…æ´— â†’ è¦å‰‡éæ¿¾ â†’ IFD éæ¿¾ â†’ è¨“ç·´
```

**æ¨™æº–æµç¨‹** (1 å¤©)ï¼š
```
åŸå§‹æ•¸æ“š â†’ æ¸…æ´— â†’ å»é‡ â†’ IFD éæ¿¾ â†’ DEITA é¸æ“‡ top-k â†’ è¨“ç·´
```

**å®Œæ•´ç®¡é“** (3-5 å¤©)ï¼š
```
åŸå§‹æ•¸æ“š â†’ æ¸…æ´— â†’ å»é‡ â†’ å›°æƒ‘åº¦éæ¿¾ â†’ IFD è©•ä¼° â†’ 
DEITA è©•åˆ† â†’ LESS æ¢¯åº¦é¸æ“‡ â†’ å¤šæ¨£æ€§å¹³è¡¡ â†’ 
å€«ç†å¯©æŸ¥ â†’ ç‰ˆæœ¬ç®¡ç† â†’ è¨“ç·´
```

---

**å»¶ä¼¸é–±è®€**ï¼š

- [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)
- [DEITA: Data-Efficient Instruction Tuning](https://arxiv.org/abs/2312.15685)
- [Phi-1.5: Textbooks Are All You Need II](https://arxiv.org/abs/2309.05463)
- [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/abs/2402.04333)

**ä¸‹ä¸€æ­¥**ï¼šé€²å…¥ [Lab-4.2](../02-Labs/Lab-4.2-Efficient_Data_Filtering/) å¯¦éš›æ“ä½œæ•¸æ“šéæ¿¾èˆ‡è©•ä¼°ã€‚
