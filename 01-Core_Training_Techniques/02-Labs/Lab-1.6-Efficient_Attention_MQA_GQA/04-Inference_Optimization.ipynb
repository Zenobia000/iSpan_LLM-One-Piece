{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.6: 推理優化實戰\n",
    "## Inference Optimization with MQA/GQA\n",
    "\n",
    "**學習目標**:\n",
    "- 實現完整的 KV Cache 推理\n",
    "- 對比長文本生成性能\n",
    "- 優化批次推理吞吐量\n",
    "- 實際部署場景分析\n",
    "\n",
    "**預計時間**: 60-90分鐘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. 完整推理實現\n",
    "\n",
    "實現支持 KV Cache 的自回歸生成，對比 MHA/GQA/MQA。\n",
    "\n",
    "### 關鍵優化:\n",
    "1. Prefill 階段: 並行處理 prompt\n",
    "2. Decode 階段: 逐個生成, 復用 KV Cache\n",
    "3. 批次推理: 多請求並行處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. 基礎組件實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Grouped-Query Attention - 多個 Q 共享分組的 K, V\"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_groups, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads 必須能被 num_kv_groups 整除\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.heads_per_group = num_heads // num_kv_groups\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, num_kv_groups * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, num_kv_groups * self.head_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        B, N, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(B, N, self.num_kv_groups, self.head_dim)\n",
    "        V = self.v_proj(x).view(B, N, self.num_kv_groups, self.head_dim)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            K = torch.cat([past_k, K], dim=1)\n",
    "            V = torch.cat([past_v, V], dim=1)\n",
    "        \n",
    "        K_repeated = K.repeat_interleave(self.heads_per_group, dim=2)\n",
    "        V_repeated = V.repeat_interleave(self.heads_per_group, dim=2)\n",
    "        \n",
    "        Q = Q.transpose(1, 2)\n",
    "        K_repeated = K_repeated.transpose(1, 2)\n",
    "        V_repeated = V_repeated.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K_repeated.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V_repeated)\n",
    "        output = output.transpose(1, 2).contiguous().view(B, N, -1)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            return output, (K, V)\n",
    "        return output\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"模型配置\"\"\"\n",
    "    hidden_dim: int = 768\n",
    "    num_heads: int = 12\n",
    "    num_kv_groups: int = 4  # GQA-4 配置\n",
    "    vocab_size: int = 32000\n",
    "    max_seq_len: int = 2048\n",
    "    dropout: float = 0.1\n",
    "    num_layers: int = 6  # 簡化為 6 層"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. 完整的 Transformer 層實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"RoPE 位置編碼 - 簡化版本\"\"\"\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # 計算頻率\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # 預計算位置編碼\n",
    "        t = torch.arange(max_seq_len).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer('cos_cached', emb.cos()[None, None, :, :])\n",
    "        self.register_buffer('sin_cached', emb.sin()[None, None, :, :])\n",
    "    \n",
    "    def forward(self, x, seq_len):\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, :].to(x.device),\n",
    "            self.sin_cached[:, :, :seq_len, :].to(x.device)\n",
    "        )\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    \"\"\"應用 RoPE\"\"\"\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "\n",
    "class OptimizedGQALayer(nn.Module):\n",
    "    \"\"\"優化的 GQA Transformer 層\"\"\"\n",
    "    def __init__(self, config: ModelConfig, attention_type=\"gqa\"):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "        # 根據類型選擇不同的 attention\n",
    "        if attention_type == \"mha\":\n",
    "            self.attention = GroupedQueryAttention(\n",
    "                config.hidden_dim, config.num_heads, config.num_heads, config.dropout\n",
    "            )\n",
    "        elif attention_type == \"mqa\":\n",
    "            self.attention = GroupedQueryAttention(\n",
    "                config.hidden_dim, config.num_heads, 1, config.dropout\n",
    "            )\n",
    "        else:  # gqa\n",
    "            self.attention = GroupedQueryAttention(\n",
    "                config.hidden_dim, config.num_heads, config.num_kv_groups, config.dropout\n",
    "            )\n",
    "        \n",
    "        # RoPE (簡化版本)\n",
    "        self.rope = RotaryPositionalEmbedding(\n",
    "            config.hidden_dim // config.num_heads,\n",
    "            config.max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Layer Norm\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_dim)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_dim * 4, config.hidden_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False, position_ids=None):\n",
    "        # Pre-norm + Attention\n",
    "        residual = x\n",
    "        x = self.ln1(x)\n",
    "        \n",
    "        # 應用 RoPE (簡化版本 - 為了示例而跳過實際應用)\n",
    "        if position_ids is not None:\n",
    "            cos, sin = self.rope(x, x.size(1))\n",
    "            # 這裡為了簡化，直接傳遞原始 x\n",
    "        \n",
    "        attn_out, new_past_kv = self.attention(x, past_kv, use_cache)\n",
    "        x = residual + attn_out\n",
    "        \n",
    "        # Pre-norm + MLP\n",
    "        residual = x\n",
    "        x = self.ln2(x)\n",
    "        x = residual + self.mlp(x)\n",
    "        \n",
    "        if use_cache:\n",
    "            return x, new_past_kv\n",
    "        return x\n",
    "\n",
    "class OptimizedLanguageModel(nn.Module):\n",
    "    \"\"\"優化的語言模型\"\"\"\n",
    "    def __init__(self, config: ModelConfig, attention_type=\"gqa\"):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            OptimizedGQALayer(config, attention_type) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_dim)\n",
    "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, input_ids, past_kvs=None, use_cache=False):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        \n",
    "        new_past_kvs = [] if use_cache else None\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = past_kvs[i] if past_kvs else None\n",
    "            \n",
    "            if use_cache:\n",
    "                x, new_past_kv = layer(x, past_kv, use_cache)\n",
    "                new_past_kvs.append(new_past_kv)\n",
    "            else:\n",
    "                x = layer(x, past_kv, use_cache)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if use_cache:\n",
    "            return logits, new_past_kvs\n",
    "        return logits\n",
    "\n",
    "# 建立測試模型\n",
    "config = ModelConfig()\n",
    "model = OptimizedLanguageModel(config).to(device)\n",
    "\n",
    "print(f\"✅ 模型建立成功\")\n",
    "print(f\"參數量: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "print(f\"配置: {config}\")\n",
    "\n",
    "# 測試前向傳播\n",
    "test_input = torch.randint(0, config.vocab_size, (1, 10), device=device)\n",
    "logits = model(test_input)\n",
    "print(f\"測試輸入: {test_input.shape} → 輸出: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. 推理引擎實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInferenceEngine:\n",
    "    \"\"\"簡化版推理引擎 - 專注於性能對比測試\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model.eval()  # 設為評估模式\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def benchmark_model(self, seq_len=64, num_decode_steps=20):\n",
    "        \"\"\"執行模型基準測試\"\"\"\n",
    "        \n",
    "        # 🔧 確保模型精度一致性\n",
    "        model_dtype = next(self.model.parameters()).dtype\n",
    "        \n",
    "        # 清理記憶體\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        # 準備測試輸入 - 使用與模型匹配的 dtype\n",
    "        test_input = torch.randint(1, 1000, (1, seq_len), device=self.device)\n",
    "        \n",
    "        # Prefill 階段測試\n",
    "        start_time = time.time()\n",
    "        logits, past_kvs = self.model(test_input, use_cache=True)\n",
    "        torch.cuda.synchronize()\n",
    "        prefill_time = time.time() - start_time\n",
    "        \n",
    "        # Decode 階段測試\n",
    "        decode_times = []\n",
    "        for _ in range(num_decode_steps):\n",
    "            new_token = torch.randint(1, 1000, (1, 1), device=self.device)\n",
    "            start_time = time.time()\n",
    "            logits, past_kvs = self.model(new_token, past_kvs=past_kvs, use_cache=True)\n",
    "            torch.cuda.synchronize()\n",
    "            decode_times.append(time.time() - start_time)\n",
    "        \n",
    "        # 統計結果\n",
    "        avg_decode_time = np.mean(decode_times)\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "        \n",
    "        return {\n",
    "            'prefill_time_ms': prefill_time * 1000,\n",
    "            'avg_decode_time_ms': avg_decode_time * 1000,\n",
    "            'tokens_per_sec': 1.0 / avg_decode_time,\n",
    "            'peak_memory_gb': peak_memory,\n",
    "            'total_time_s': prefill_time + sum(decode_times)\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_text(self, input_ids, max_length=50, temperature=1.0):\n",
    "        \"\"\"生成文本 - 示例用途\"\"\"\n",
    "        generated = input_ids.clone()\n",
    "        past_kvs = None\n",
    "        \n",
    "        for _ in range(max_length - input_ids.size(1)):\n",
    "            if past_kvs is None:\n",
    "                # First forward pass\n",
    "                logits, past_kvs = self.model(generated, use_cache=True)\n",
    "                next_token_logits = logits[:, -1, :] / temperature\n",
    "            else:\n",
    "                # Subsequent passes with cache\n",
    "                logits, past_kvs = self.model(generated[:, -1:], past_kvs=past_kvs, use_cache=True)\n",
    "                next_token_logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            \n",
    "            # Simple stopping condition\n",
    "            if next_token.item() == 2:  # Assuming 2 is EOS token\n",
    "                break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "print(\"✅ 簡化版推理引擎準備就緒\")\n",
    "print(\"   • 支援基準測試與性能分析\")\n",
    "print(\"   • 自動記憶體管理與統計\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. 簡化版推理性能測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_benchmark_test():\n",
    "    \"\"\"執行簡化的推理性能測試\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 簡化版推理性能測試\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 簡化的測試配置\n",
    "    test_configs = [\n",
    "        (\"MHA\", \"mha\"),     # MHA: 12 KV heads\n",
    "        (\"GQA-4\", \"gqa\"),   # GQA-4: 4 KV heads\n",
    "        (\"GQA-3\", \"gqa\"),   # GQA-3: 3 KV heads\n",
    "        (\"MQA\", \"mqa\"),     # MQA: 1 KV head\n",
    "    ]\n",
    "    \n",
    "    benchmark_results = []\n",
    "    \n",
    "    for name, attention_type in test_configs:\n",
    "        print(f\"\\n🔍 測試 {name} ({attention_type})\")\n",
    "        \n",
    "        # 創建測試配置\n",
    "        if name == \"GQA-3\":\n",
    "            config = ModelConfig(num_kv_groups=3)  # 12÷3=4 heads per group\n",
    "        else:\n",
    "            config = ModelConfig()\n",
    "        \n",
    "        # 建立測試模型\n",
    "        test_model = OptimizedLanguageModel(config, attention_type).to(device).half()\n",
    "        \n",
    "        # 創建推理引擎\n",
    "        engine = SimpleInferenceEngine(test_model)\n",
    "        \n",
    "        # 執行基準測試\n",
    "        result = engine.benchmark_model(seq_len=64, num_decode_steps=30)\n",
    "        \n",
    "        # 添加配置信息\n",
    "        if attention_type == \"mha\":\n",
    "            num_kv_groups = config.num_heads\n",
    "        elif attention_type == \"mqa\":\n",
    "            num_kv_groups = 1\n",
    "        elif name == \"GQA-3\":\n",
    "            num_kv_groups = 3\n",
    "        else:  # GQA-4\n",
    "            num_kv_groups = 4\n",
    "        \n",
    "        result.update({\n",
    "            'name': name,\n",
    "            'attention_type': attention_type,\n",
    "            'num_kv_groups': num_kv_groups,\n",
    "            'num_heads': config.num_heads\n",
    "        })\n",
    "        benchmark_results.append(result)\n",
    "        \n",
    "        print(f\"   ✅ Prefill: {result['prefill_time_ms']:.2f}ms\")\n",
    "        print(f\"   ✅ Decode: {result['avg_decode_time_ms']:.3f}ms/token\")\n",
    "        print(f\"   ✅ Speed: {result['tokens_per_sec']:.1f} tok/s\")\n",
    "        print(f\"   ✅ Memory: {result['peak_memory_gb']:.3f}GB\")\n",
    "        \n",
    "        # 清理模型\n",
    "        del test_model, engine\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "# 執行簡化測試\n",
    "print(\"🚀 開始簡化版性能測試...\")\n",
    "benchmark_results = simple_benchmark_test()\n",
    "\n",
    "# 簡單的結果總結\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"📊 測試結果總結\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for result in benchmark_results:\n",
    "    print(f\"{result['name']:<8}: {result['tokens_per_sec']:.1f} tok/s, {result['peak_memory_gb']:.3f}GB\")\n",
    "\n",
    "# 計算相對效能\n",
    "mha_result = next(r for r in benchmark_results if r['name'] == 'MHA')\n",
    "print(f\"\\n🎯 相對 MHA 的性能提升:\")\n",
    "for result in benchmark_results:\n",
    "    speedup = result['tokens_per_sec'] / mha_result['tokens_per_sec']\n",
    "    memory_ratio = mha_result['peak_memory_gb'] / result['peak_memory_gb']\n",
    "    print(f\"   {result['name']:<8}: {speedup:.2f}x 速度, {memory_ratio:.2f}x 記憶體效率\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. 最終總結與部署建議"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deployment_recommendations(benchmark_results):\n",
    "    \"\"\"基於測試結果生成部署建議\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🎯 Lab-1.6 推理優化實戰總結報告\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 性能排序\n",
    "    sorted_by_speed = sorted(benchmark_results, key=lambda x: x['tokens_per_sec'], reverse=True)\n",
    "    sorted_by_memory = sorted(benchmark_results, key=lambda x: x['peak_memory_gb'])\n",
    "    \n",
    "    print(f\"\\n🚀 推理速度排名:\")\n",
    "    print(f\"{'排名':<4} {'配置':<8} {'速度 (tok/s)':<15} {'相對提升':<12}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    mha_speed = next(r['tokens_per_sec'] for r in benchmark_results if r['name'] == 'MHA')\n",
    "    for i, result in enumerate(sorted_by_speed, 1):\n",
    "        speedup = result['tokens_per_sec'] / mha_speed\n",
    "        print(f\"{i:<4} {result['name']:<8} {result['tokens_per_sec']:<15.1f} {speedup:<12.2f}x\")\n",
    "    \n",
    "    print(f\"\\n💾 記憶體效率排名:\")\n",
    "    print(f\"{'排名':<4} {'配置':<8} {'記憶體 (GB)':<15} {'相對節省':<12}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    mha_memory = next(r['peak_memory_gb'] for r in benchmark_results if r['name'] == 'MHA')\n",
    "    for i, result in enumerate(sorted_by_memory, 1):\n",
    "        efficiency = mha_memory / result['peak_memory_gb']\n",
    "        print(f\"{i:<4} {result['name']:<8} {result['peak_memory_gb']:<15.3f} {efficiency:<12.2f}x\")\n",
    "    \n",
    "    print(f\"\\n📊 部署場景建議:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 針對不同場景的建議\n",
    "    scenarios = {\n",
    "        '🚀 極致性能場景 (ChatBot, 即時應用)': {\n",
    "            'recommended': 'MQA',\n",
    "            'reason': '最高推理速度，最低記憶體占用',\n",
    "            'tradeoff': '輕微質量損失，需要評估可接受性'\n",
    "        },\n",
    "        '⚖️ 平衡場景 (生產服務, 通用部署)': {\n",
    "            'recommended': 'GQA-4',\n",
    "            'reason': '良好的性能與質量平衡',\n",
    "            'tradeoff': '適中的資源需求，易於遷移'\n",
    "        },\n",
    "        '🔬 研究場景 (基準測試, 質量優先)': {\n",
    "            'recommended': 'MHA',\n",
    "            'reason': '最高質量保證，完整表現力',\n",
    "            'tradeoff': '最高資源消耗，適合質量要求嚴格的場景'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario, info in scenarios.items():\n",
    "        print(f\"\\n{scenario}:\")\n",
    "        print(f\"   推薦: {info['recommended']}\")\n",
    "        print(f\"   原因: {info['reason']}\")\n",
    "        print(f\"   權衡: {info['tradeoff']}\")\n",
    "    \n",
    "    print(f\"\\n🛠️ 實施建議:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 實際的數據驅動建議\n",
    "    mqa_result = next(r for r in benchmark_results if r['name'] == 'MQA')\n",
    "    gqa_result = next(r for r in benchmark_results if r['name'] == 'GQA-4')\n",
    "    \n",
    "    mqa_speedup = mqa_result['tokens_per_sec'] / mha_speed\n",
    "    gqa_speedup = gqa_result['tokens_per_sec'] / mha_speed\n",
    "    \n",
    "    print(f\"1. 📈 如果追求速度提升:\")\n",
    "    if mqa_speedup > 1.5:\n",
    "        print(f\"   • MQA 提供 {mqa_speedup:.2f}x 加速，適合高吞吐量需求\")\n",
    "    else:\n",
    "        print(f\"   • MQA 提供 {mqa_speedup:.2f}x 加速，提升有限但記憶體節省顯著\")\n",
    "    \n",
    "    print(f\"\\n2. 🎯 如果需要平衡方案:\")\n",
    "    print(f\"   • GQA-4 提供 {gqa_speedup:.2f}x 加速，質量損失最小\")\n",
    "    print(f\"   • 推薦用於生產環境的首選配置\")\n",
    "    \n",
    "    print(f\"\\n3. 💾 記憶體受限環境:\")\n",
    "    mqa_memory_save = mha_memory / mqa_result['peak_memory_gb']\n",
    "    print(f\"   • MQA 節省 {mqa_memory_save:.2f}x 記憶體\")\n",
    "    print(f\"   • 特別適合邊緣設備和資源受限場景\")\n",
    "    \n",
    "    print(f\"\\n🔮 下一步學習方向:\")\n",
    "    print(\"=\" * 25)\n",
    "    print(f\"• 結合 FlashAttention 進一步優化\")\n",
    "    print(f\"• 探索 vLLM 等生產級推理框架\")\n",
    "    print(f\"• 學習模型量化技術 (INT8, FP4)\")\n",
    "    print(f\"• 了解分散式推理與服務化部署\")\n",
    "    print(f\"• 實踐 KV Cache 進階優化技術\")\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "# 生成最終建議\n",
    "print(\"🎊 根據實際測試結果生成部署建議...\")\n",
    "deployment_recommendations = generate_deployment_recommendations(benchmark_results)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Lab-1.6 推理優化實戰完成！\")\n",
    "print(\"=\"*60)\n",
    "print(f\"🎉 恭喜！您已經掌握了:\")\n",
    "print(f\"   • MHA, GQA, MQA 三種架構的實作與優化\")\n",
    "print(f\"   • KV Cache 機制與記憶體管理\")\n",
    "print(f\"   • 實際推理性能測試與分析\")\n",
    "print(f\"   • 生產部署的架構選擇策略\")\n",
    "print(f\"\\n🚀 您現在可以在實際項目中應用這些知識！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. 長文本生成示例測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🧪 長文本生成示例測試\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 選擇一個 GQA 模型進行生成測試\n",
    "config = ModelConfig(num_kv_groups=4)\n",
    "demo_model = OptimizedLanguageModel(config, \"gqa\").to(device).half()\n",
    "demo_engine = SimpleInferenceEngine(demo_model)\n",
    "\n",
    "# 示例生成\n",
    "input_ids = torch.randint(1, 1000, (1, 10), device=device)\n",
    "print(f\"輸入序列長度: {input_ids.size(1)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "generated = demo_engine.generate_text(input_ids, max_length=50)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"生成序列長度: {generated.size(1)}\")\n",
    "print(f\"生成時間: {(end_time - start_time)*1000:.2f}ms\")\n",
    "print(f\"生成速度: {(generated.size(1) - input_ids.size(1)) / (end_time - start_time):.1f} tok/s\")\n",
    "\n",
    "# 清理\n",
    "del demo_model, demo_engine\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n🎊 完整的推理優化實戰已結束！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 實驗總結\n",
    "\n",
    "### 推薦配置\n",
    "\n",
    "**生產部署**:\n",
    "- Llama-2 style: GQA-8 (32 Q heads, 8 KV groups)\n",
    "- 平衡質量與速度\n",
    "- KV Cache 減少 4x\n",
    "- 推理加速 1.3-1.5x\n",
    "\n",
    "**極致速度**:\n",
    "- Falcon style: MQA (32 Q heads, 1 KV head)\n",
    "- 最快推理速度\n",
    "- KV Cache 減少 32x\n",
    "- 推理加速 1.5-2x\n",
    "- 質量略有下降\n",
    "\n",
    "### 下一步\n",
    "- Lab-1.7: DPO Alignment\n",
    "- vLLM 部署實踐\n",
    "- 生產環境優化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
