{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 應用 LoRA 進行模型微調\n",
        "\n",
        "在本筆記本中，我們將進入最核心的環節：使用 `peft` 函式庫將 LoRA (Low-Rank Adaptation) 應用到我們已經載入的量化模型上，並設定訓練流程來進行微調。\n",
        "\n",
        "## 步驟 1: 匯入必要的函式庫\n",
        "\n",
        "首先，我們匯入 `peft` 函式庫中用於設定 LoRA 的 `LoraConfig` 和 `get_peft_model` 函式，以及 `transformers` 中用於設定訓練參數的 `TrainingArguments` 和執行訓練的 `Trainer`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# 重新執行上一筆記本的程式碼，確保模型和 Tokenizer 已載入\n",
        "# 在實際操作中，您也可以將模型和 Tokenizer 儲存後再載入，或使用其他方式在筆記本間共享變數\n",
        "%run ./02-load-model-and-dataset.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 2: 設定 LoRA (LoraConfig)\n",
        "\n",
        "為了將 LoRA 應用到我們的模型上，我們需要建立一個 `LoraConfig` 物件。這個物件會告訴 `peft` 函式庫如何設定 LoRA adapter。\n",
        "\n",
        "關鍵參數說明：\n",
        "- `r`: LoRA 的秩 (rank)。這是一個關鍵超參數，決定了低秩矩陣的大小。較小的 `r` 意味著更少的參數和更快的訓練，但可能會犧牲一些性能。常見的設定值為 8, 16, 32, 64。\n",
        "- `lora_alpha`: LoRA 的縮放因子，可以理解為學習率的縮放。公式為 `alpha / r`。通常設定為 `r` 的兩倍。\n",
        "- `target_modules`: 指定要應用 LoRA 的模組名稱。對於 Transformer 模型，通常是注意力機制中的查詢 (query) 和值 (value) 投影層，即 `\"q_proj\"` 和 `\"v_proj\"`。\n",
        "- `lora_dropout`: 在 LoRA 層中使用的 dropout 比率。\n",
        "- `bias`: 設定 bias 是否可訓練。`\"none\"` 表示所有 bias 都不訓練。\n",
        "- `task_type`: 指定任務類型。對於語言模型，我們設定為 `\"CAUSAL_LM\"`。\n",
        "\n",
        "設定好 `LoraConfig` 後，我們使用 `get_peft_model` 函式將其應用到我們的基礎模型上。該函式會凍結所有原始模型參數，並在指定模組上加上 LoRA adapter。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 建立 LoraConfig\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# 將 LoRA 應用到模型上\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# 打印出模型中可訓練的參數\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 3: 設定訓練參數與啟動訓練\n",
        "\n",
        "現在我們的模型已經準備好進行微調，最後一步是設定訓練流程。我們使用 Hugging Face `Trainer` 來處理大部分的訓練細節。\n",
        "\n",
        "首先，建立一個 `TrainingArguments` 物件來定義訓練的各項參數：\n",
        "- `output_dir`: 訓練過程中模型 checkpoint 和日誌的儲存目錄。\n",
        "- `per_device_train_batch_size`: 每個 GPU 上的訓練批次大小。\n",
        "- `gradient_accumulation_steps`: 梯度累積步數，用於在記憶體不足時模擬更大的批次大小。\n",
        "- `learning_rate`: 學習率。\n",
        "- `num_train_epochs`: 訓練的總輪數。\n",
        "- `logging_steps`: 每隔多少步記錄一次訓練日誌。\n",
        "- `fp16`: 啟用混合精度訓練以加速並節省記憶體。\n",
        "\n",
        "接著，我們實例化一個 `Trainer`，並傳入模型、訓練參數、資料集等。最後，呼叫 `trainer.train()` 即可開始微調！\n",
        "\n",
        "**注意**: 即使是 PEFT，微調 7B 模型仍然需要一些時間。在 Colab 的 T4 GPU 上，這個步驟大約需要 15-20 分鐘。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 設定訓練參數\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-llama2-7b-chat\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    save_total_limit=2,\n",
        "    overwrite_output_dir=True,\n",
        ")\n",
        "\n",
        "# 建立 Trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    # 我們需要提供一個 data_collator 來將資料整理成批次\n",
        "    # 對於語言模型，我們通常使用 DataCollatorForLanguageModeling\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# 關閉 cache 以解決一個已知的梯度檢查點問題\n",
        "peft_model.config.use_cache = False\n",
        "\n",
        "# 開始訓練\n",
        "trainer.train()\n",
        "\n",
        "print(\"\n",
        "訓練完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "模型微調已經完成！在下一個筆記本 `04-inference-and-evaluation.ipynb` 中，我們將學習如何使用這個微調後的模型進行推理。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
