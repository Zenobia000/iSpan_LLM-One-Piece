{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab-1.7: SFT (Supervised Fine-Tuning) åŸºç·šæ¨¡å‹è¨“ç·´\n",
    "\n",
    "**å¯¦é©—ç›®æ¨™**: è¨“ç·´ç›£ç£å¼å¾®èª¿åŸºç·šæ¨¡å‹ï¼Œä½œç‚º DPO çš„åƒè€ƒæ¨¡å‹\n",
    "\n",
    "åœ¨é€²è¡Œ DPO å°é½Šä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å…ˆè¨“ç·´ä¸€å€‹ SFT åŸºç·šæ¨¡å‹ã€‚é€™å€‹åŸºç·šæ¨¡å‹å°‡ï¼š\n",
    "1. **ä½œç‚º DPO çš„èµ·å§‹é»**: DPO éœ€è¦åœ¨ SFT æ¨¡å‹åŸºç¤ä¸Šé€²è¡Œåå¥½å„ªåŒ–\n",
    "2. **æä¾›æ€§èƒ½å°æ¯”åŸºæº–**: è©•ä¼° DPO å°é½Šæ•ˆæœ\n",
    "3. **ç¢ºä¿åŸºæœ¬å°è©±èƒ½åŠ›**: åœ¨åå¥½å„ªåŒ–å‰å…·å‚™åŸºæœ¬çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›\n",
    "\n",
    "## SFT vs DPO\n",
    "\n",
    "- **SFT**: ä½¿ç”¨ç›£ç£å­¸ç¿’å¾æŒ‡ä»¤-å›æ‡‰å°ä¸­å­¸ç¿’\n",
    "- **DPO**: ä½¿ç”¨åå¥½å°æ¯”å­¸ç¿’ä¾†å„ªåŒ–æ¨¡å‹è¡Œç‚º\n",
    "- **æµç¨‹**: é è¨“ç·´æ¨¡å‹ â†’ SFT â†’ DPO â†’ éƒ¨ç½²\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1: ç’°å¢ƒè¼‰å…¥èˆ‡é…ç½®\n",
    "\n",
    "é‡æ–°è¼‰å…¥ä¹‹å‰æº–å‚™çš„ç’°å¢ƒå’Œæ•¸æ“šï¼Œä¸¦é…ç½® PEFT é€²è¡Œé«˜æ•ˆå¾®èª¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_from_disk, Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# è¨­ç½®éš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('ğŸš€ é–‹å§‹ SFT åŸºç·šæ¨¡å‹è¨“ç·´')\n",
    "print(f'GPU å¯ç”¨: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'ç•¶å‰ GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2: è¼‰å…¥æ¨¡å‹èˆ‡ Tokenizer\n",
    "\n",
    "ä½¿ç”¨èˆ‡ä¹‹å‰ç›¸åŒçš„æ¨¡å‹é…ç½®ï¼Œä¸¦æº–å‚™ PEFT é…ç½®é€²è¡Œé«˜æ•ˆå¾®èª¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹é…ç½® (èˆ‡ç¬¬ä¸€å€‹ notebook ä¿æŒä¸€è‡´)\n",
    "MODEL_NAME = 'microsoft/DialoGPT-medium'  # æˆ– 'gpt2'\n",
    "\n",
    "print(f'ğŸ“¦ è¼‰å…¥æ¨¡å‹: {MODEL_NAME}')\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f'âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ')\n",
    "print(f'æ¨¡å‹åƒæ•¸é‡: {model.num_parameters():,}')\n",
    "print(f'è©å½™å¤§å°: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3: PEFT é…ç½®\n",
    "\n",
    "ä½¿ç”¨ LoRA é€²è¡Œåƒæ•¸é«˜æ•ˆå¾®èª¿ï¼Œæ¸›å°‘è¨“ç·´æ™‚é–“å’Œè¨˜æ†¶é«”éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½® LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                        # rank\n",
    "    lora_alpha=32,              # scaling parameter\n",
    "    target_modules=['c_attn', 'c_proj', 'c_fc'],  # é‡å° GPT-2 æ¶æ§‹\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "# æº–å‚™æ¨¡å‹é€²è¡Œ PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print('âœ… PEFT é…ç½®å®Œæˆ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4: æº–å‚™ SFT è¨“ç·´æ•¸æ“š\n",
    "\n",
    "å¾ä¹‹å‰çš„åå¥½æ•¸æ“šä¸­æå–æŒ‡ä»¤-å›æ‡‰å°ï¼Œç”¨æ–¼ç›£ç£å¼å¾®èª¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ä¹‹å‰è™•ç†çš„æ•¸æ“š\n",
    "try:\n",
    "    dpo_dataset = load_from_disk('./dpo_data')\n",
    "    print(f'âœ… è¼‰å…¥ DPO æ•¸æ“šé›†ï¼Œæ¨£æœ¬æ•¸: {len(dpo_dataset)}')\n",
    "except:\n",
    "    print('âš ï¸  ç„¡æ³•è¼‰å…¥å·²ä¿å­˜çš„æ•¸æ“šï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“š')\n",
    "    # å‰µå»ºæ¨¡æ“¬è¨“ç·´æ•¸æ“š\n",
    "    mock_data = [\n",
    "        {\n",
    "            'prompt': 'è«‹è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?',\n",
    "            'chosen': 'æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒè®“è¨ˆç®—æ©Ÿèƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’ä¸¦åšå‡ºæ±ºç­–ï¼Œè€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚é€šéç®—æ³•åˆ†æå¤§é‡æ•¸æ“šï¼Œç³»çµ±å¯ä»¥è­˜åˆ¥æ¨¡å¼ä¸¦æé«˜æ€§èƒ½ã€‚',\n",
    "            'rejected': 'æ©Ÿå™¨å­¸ç¿’å°±æ˜¯è®“æ©Ÿå™¨è®Šè°æ˜ã€‚'\n",
    "        },\n",
    "        {\n",
    "            'prompt': 'å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆ?',\n",
    "            'chosen': 'å­¸ç¿’ç¨‹å¼è¨­è¨ˆå»ºè­°å¾åŸºç¤èªæ³•é–‹å§‹ï¼Œé¸æ“‡ä¸€é–€é©åˆçš„èªè¨€å¦‚Pythonï¼Œå¤šåšç·´ç¿’é …ç›®ï¼Œåƒèˆ‡é–‹æºå°ˆæ¡ˆï¼Œä¸¦æŒçºŒå­¸ç¿’æ–°æŠ€è¡“ã€‚å¯¦ä½œæ˜¯æœ€é‡è¦çš„å­¸ç¿’æ–¹å¼ã€‚',\n",
    "            'rejected': 'å­¸ç¨‹å¼å°±æ˜¯å¯«codeã€‚'\n",
    "        }\n",
    "    ]\n",
    "    dpo_dataset = Dataset.from_list(mock_data)\n",
    "\n",
    "print(f'æ•¸æ“šé›†å¤§å°: {len(dpo_dataset)}')\n",
    "print(f'æ•¸æ“šæ¬„ä½: {list(dpo_dataset.features.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sft_dataset(dataset, tokenizer, max_length=512):\n",
    "    \"\"\"å°‡åå¥½æ•¸æ“šè½‰æ›ç‚º SFT è¨“ç·´æ ¼å¼\"\"\"\n",
    "    sft_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # ä½¿ç”¨ chosen å›æ‡‰ä½œç‚ºè¨“ç·´ç›®æ¨™\n",
    "        conversation = f\"Human: {sample['prompt']}\\n\\nAssistant: {sample['chosen']}\"\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = tokenizer(\n",
    "            conversation,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        sft_data.append({\n",
    "            'input_ids': tokens['input_ids'],\n",
    "            'attention_mask': tokens['attention_mask'],\n",
    "            'labels': tokens['input_ids'].copy()  # å°æ–¼èªè¨€æ¨¡å‹ï¼Œlabels ç­‰æ–¼ input_ids\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(sft_data)\n",
    "\n",
    "# å‰µå»º SFT è¨“ç·´æ•¸æ“šé›†\n",
    "print('ğŸ”„ è½‰æ›æ•¸æ“šç‚º SFT æ ¼å¼...')\n",
    "sft_dataset = create_sft_dataset(dpo_dataset, tokenizer)\n",
    "\n",
    "print(f'âœ… SFT æ•¸æ“šé›†æº–å‚™å®Œæˆï¼Œæ¨£æœ¬æ•¸: {len(sft_dataset)}')\n",
    "\n",
    "# é¡¯ç¤ºç¤ºä¾‹\n",
    "sample = sft_dataset[0]\n",
    "decoded_text = tokenizer.decode(sample['input_ids'])\n",
    "print(f'\\nğŸ“ è¨“ç·´æ¨£æœ¬ç¤ºä¾‹:')\n",
    "print(decoded_text[:200] + '...' if len(decoded_text) > 200 else decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5: è¨“ç·´é…ç½®\n",
    "\n",
    "è¨­ç½®è¨“ç·´åƒæ•¸ï¼Œä½¿ç”¨é©åˆå–® GPU ç’°å¢ƒçš„é…ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´é…ç½®\n",
    "output_dir = './sft_model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # åŸºæœ¬è¨­ç½®\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    \n",
    "    # å„ªåŒ–è¨­ç½®\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    \n",
    "    # è¨˜æ†¶é«”å„ªåŒ–\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # å…¶ä»–è¨­ç½®\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # ä¸ä½¿ç”¨ wandb\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print('âš™ï¸  è¨“ç·´é…ç½®:')\n",
    "print(f'è¼¸å‡ºç›®éŒ„: {output_dir}')\n",
    "print(f'è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}')\n",
    "print(f'æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}')\n",
    "print(f'å­¸ç¿’ç‡: {training_args.learning_rate}')\n",
    "print(f'ä½¿ç”¨ FP16: {training_args.fp16}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 6: é–‹å§‹ SFT è¨“ç·´\n",
    "\n",
    "ä½¿ç”¨ Hugging Face Trainer é€²è¡Œç›£ç£å¼å¾®èª¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•¸æ“šæ•´ç†å™¨\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # ä¸ä½¿ç”¨ masked language modeling\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# å‰µå»º Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=sft_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print('ğŸš€ é–‹å§‹ SFT è¨“ç·´...')\n",
    "\n",
    "# è¨“ç·´å‰çš„è¨˜æ†¶é«”ç‹€æ…‹\n",
    "if torch.cuda.is_available():\n",
    "    print(f'è¨“ç·´å‰ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "# é–‹å§‹è¨“ç·´\n",
    "try:\n",
    "    trainer.train()\n",
    "    print('âœ… SFT è¨“ç·´å®Œæˆï¼')\n",
    "    \n",
    "    # è¨“ç·´å¾Œçš„è¨˜æ†¶é«”ç‹€æ…‹\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'è¨“ç·´å¾Œ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'âŒ è¨“ç·´éç¨‹ä¸­å‡ºéŒ¯: {e}')\n",
    "    print('å˜—è©¦æ¸›å°‘æ‰¹æ¬¡å¤§å°æˆ–åºåˆ—é•·åº¦')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 7: ä¿å­˜æ¨¡å‹èˆ‡è©•ä¼°\n",
    "\n",
    "ä¿å­˜è¨“ç·´å¥½çš„ SFT æ¨¡å‹ï¼Œä¸¦é€²è¡Œç°¡å–®çš„ç”Ÿæˆæ¸¬è©¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "print('ğŸ’¾ ä¿å­˜ SFT æ¨¡å‹...')\n",
    "\n",
    "# ä¿å­˜ PEFT æ¨¡å‹\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f'âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}')\n",
    "\n",
    "# åˆ—å‡ºä¿å­˜çš„æª”æ¡ˆ\n",
    "saved_files = list(Path(output_dir).glob('*'))\n",
    "print(f'ä¿å­˜çš„æª”æ¡ˆ: {[f.name for f in saved_files]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ç”Ÿæˆ\n",
    "def test_generation(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"æ¸¬è©¦æ¨¡å‹ç”Ÿæˆèƒ½åŠ›\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # æº–å‚™è¼¸å…¥\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # ç”Ÿæˆå›æ‡‰\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # è§£ç¢¼è¼¸å‡º\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# æ¸¬è©¦ç”Ÿæˆ\n",
    "print('ğŸ§ª æ¸¬è©¦ SFT æ¨¡å‹ç”Ÿæˆ...')\n",
    "\n",
    "test_prompts = [\n",
    "    \"Human: ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’?\\n\\nAssistant:\",\n",
    "    \"Human: å¦‚ä½•æé«˜ç¨‹å¼è¨­è¨ˆæŠ€èƒ½?\\n\\nAssistant:\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f'\\nğŸ“ æ¸¬è©¦ {i+1}:')\n",
    "    print(f'Prompt: {prompt.split(\"Assistant:\")[0]}Assistant:')\n",
    "    \n",
    "    try:\n",
    "        response = test_generation(model, tokenizer, prompt)\n",
    "        print(f'Generated: {response}')\n",
    "    except Exception as e:\n",
    "        print(f'ç”Ÿæˆå¤±æ•—: {e}')\n",
    "    \n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 8: è¨“ç·´ç¸½çµ\n",
    "\n",
    "ç¸½çµ SFT åŸºç·šæ¨¡å‹çš„è¨“ç·´çµæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ç¸½çµ\n",
    "print('=== SFT åŸºç·šæ¨¡å‹è¨“ç·´ç¸½çµ ===')\n",
    "print(f'âœ… åŸºç¤æ¨¡å‹: {MODEL_NAME}')\n",
    "print(f'âœ… PEFT æ–¹æ³•: LoRA (r={lora_config.r}, alpha={lora_config.lora_alpha})')\n",
    "print(f'âœ… è¨“ç·´æ•¸æ“š: {len(sft_dataset)} å€‹æ¨£æœ¬')\n",
    "print(f'âœ… è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}')\n",
    "print(f'âœ… æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}')\n",
    "\n",
    "# é¡¯ç¤ºè¨“ç·´åƒæ•¸çµ±è¨ˆ\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'âœ… å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)')\n",
    "\n",
    "print('\\nğŸ¯ SFT åŸºç·šæ¨¡å‹å·²æº–å‚™å®Œæˆï¼')\n",
    "print('ä¸‹ä¸€æ­¥: åŸ·è¡Œ 03-DPO_Training.ipynb é€²è¡Œåå¥½å„ªåŒ–')\n",
    "\n",
    "# æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'GPU è¨˜æ†¶é«”å·²æ¸…ç†ï¼Œç•¶å‰ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f} GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}