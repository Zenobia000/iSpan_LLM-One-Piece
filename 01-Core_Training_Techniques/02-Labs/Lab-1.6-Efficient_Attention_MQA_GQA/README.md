# Lab-1.6: é«˜æ•ˆæ³¨æ„åŠ›æ©Ÿåˆ¶ (MQA/GQA)
## Efficient Attention - Multi-Query and Grouped-Query Attention

**å¯¦é©—å®¤é¡å‹**: æ³¨æ„åŠ›æ¶æ§‹å„ªåŒ–
**é›£åº¦ç­‰ç´š**: â­â­â­â­ (ä¸­é«˜ç´š)
**é ä¼°æ™‚é–“**: 3-5å°æ™‚
**é©ç”¨GPU**: 8GB+ VRAM

---

## ğŸ“š å¯¦é©—å®¤æ¦‚è¿°

Multi-Query Attention (MQA) å’Œ Grouped-Query Attention (GQA) æ˜¯é‡å° LLM æ¨ç†å„ªåŒ–çš„é‡è¦æŠ€è¡“ï¼Œé€šéæ¸›å°‘ KV Cache å¤§å°ä¾†é¡¯è‘—æå‡æ¨ç†é€Ÿåº¦èˆ‡é™ä½è¨˜æ†¶é«”å ç”¨ã€‚æœ¬å¯¦é©—å®¤å°‡æ·±å…¥æ¢ç´¢é€™äº›æŠ€è¡“çš„åŸç†ã€å¯¦ç¾èˆ‡å¯¦éš›æ‡‰ç”¨ã€‚

### å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š
- âœ… ç†è§£æ¨™æº– MHA (Multi-Head Attention) çš„æ¨ç†ç“¶é ¸
- âœ… æŒæ¡ MQA çš„æ ¸å¿ƒåŸç†èˆ‡å¯¦ç¾
- âœ… ç†è§£ GQA å¦‚ä½•å¹³è¡¡ MHA èˆ‡ MQA
- âœ… å¯¦ç¾ä¸¦å°æ¯”ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶
- âœ… å„ªåŒ– LLM æ¨ç†æ€§èƒ½
- âœ… åˆ†æ KV Cache çš„è¨˜æ†¶é«”å ç”¨

---

## ğŸ¯ æ ¸å¿ƒæŠ€è¡“æ¦‚è¦½

### ç‚ºä»€éº¼éœ€è¦ MQA/GQA?

**æ¨ç†ç“¶é ¸åˆ†æ**:
```
LLM è‡ªå›æ­¸æ¨ç†éç¨‹:
1. Prefill éšæ®µ: è™•ç†æ•´å€‹ prompt (ä¸¦è¡Œ)
2. Decode éšæ®µ: é€å€‹ç”Ÿæˆ token (ä¸²è¡Œ)

è¨˜æ†¶é«”å ç”¨:
- æ¨¡å‹åƒæ•¸: å›ºå®š
- KV Cache: éš¨ç”Ÿæˆé•·åº¦ç·šæ€§å¢é•·
- Decode éšæ®µ: è¨˜æ†¶é«”é »å¯¬å—é™ (Memory-bound)

å•é¡Œ: KV Cache æˆç‚ºæ¨ç†ç“¶é ¸!
```

**KV Cache è¨˜æ†¶é«”è¨ˆç®—**:
```
æ¨™æº– MHA (Multi-Head Attention):
  æ¯å±¤ KV Cache = 2 Ã— seq_len Ã— num_heads Ã— head_dim Ã— bytes

  ä¾‹: Llama-2-7B (32å±¤, 32heads, 128dim, FP16)
  å–®å€‹åºåˆ— 2K tokens:
  = 2 Ã— 2048 Ã— 32 Ã— 128 Ã— 2 bytes
  = 33.6 MB/å±¤
  = 1.07 GB (32å±¤)

  æ‰¹æ¬¡æ¨ç† (batch=16):
  = 1.07 GB Ã— 16 = 17 GB (åƒ… KV Cache!)
```

### ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶å°æ¯”

#### 1. Multi-Head Attention (MHA) - æ¨™æº–æ–¹æ³•

```
MHA çµæ§‹:
  Q: [batch, seq_len, num_heads, head_dim]  # 32 heads
  K: [batch, seq_len, num_heads, head_dim]  # 32 heads
  V: [batch, seq_len, num_heads, head_dim]  # 32 heads

ç‰¹é»:
  âœ… æ¯å€‹ head æœ‰ç¨ç«‹çš„ K, V
  âœ… è¡¨é”èƒ½åŠ›æœ€å¼·
  âŒ KV Cache æœ€å¤§
  âŒ æ¨ç†é€Ÿåº¦è¼ƒæ…¢
```

#### 2. Multi-Query Attention (MQA) - æ¿€é€²å„ªåŒ–

```
MQA çµæ§‹:
  Q: [batch, seq_len, num_heads, head_dim]  # 32 heads
  K: [batch, seq_len, 1, head_dim]          # 1 head (å…±äº«)
  V: [batch, seq_len, 1, head_dim]          # 1 head (å…±äº«)

ç‰¹é»:
  âœ… æ‰€æœ‰ Query heads å…±äº«åŒä¸€çµ„ K, V
  âœ… KV Cache æ¸›å°‘ 32x
  âœ… æ¨ç†é€Ÿåº¦æå‡ 1.5-2x
  âŒ è¡¨é”èƒ½åŠ›ç•¥æœ‰ä¸‹é™
  âŒ è¨“ç·´è³ªé‡å¯èƒ½å—å½±éŸ¿
```

#### 3. Grouped-Query Attention (GQA) - å¹³è¡¡æ–¹æ¡ˆ

```
GQA çµæ§‹:
  Q: [batch, seq_len, num_heads, head_dim]  # 32 heads
  K: [batch, seq_len, num_groups, head_dim] # 4-8 groups
  V: [batch, seq_len, num_groups, head_dim] # 4-8 groups

ç‰¹é»:
  âœ… Query heads åˆ†çµ„å…±äº« K, V
  âœ… KV Cache æ¸›å°‘ 4-8x
  âœ… æ¨ç†é€Ÿåº¦æå‡ 1.3-1.5x
  âœ… è¡¨é”èƒ½åŠ›æ¥è¿‘ MHA
  âœ… Llama-2, Mistral ç­‰æ¨¡å‹æ¡ç”¨
```

### æ¶æ§‹å°æ¯”åœ–

| æ¶æ§‹ | Query Heads | KV Heads | KV Cache å¤§å° | æ¨ç†é€Ÿåº¦ | æ¨¡å‹è³ªé‡ | ä»£è¡¨æ¨¡å‹ |
|------|------------|----------|--------------|---------|---------|----------|
| **MHA** | 32 | 32 | 100% (åŸºæº–) | 1.0x | â­â­â­â­â­ | GPT-3, GPT-4 |
| **GQA** | 32 | 4-8 | 12-25% | 1.3-1.5x | â­â­â­â­â­ | Llama-2, Mistral |
| **MQA** | 32 | 1 | 3% | 1.5-2x | â­â­â­â­ | PaLM, Falcon |

---

## ğŸ”§ æŠ€è¡“åŸç†æ·±åº¦è§£æ

### MHA (æ¨™æº–) å¯¦ç¾

```python
def multi_head_attention(Q, K, V):
    """
    Q: [B, N, H, D]  - H å€‹ç¨ç«‹çš„ Q heads
    K: [B, N, H, D]  - H å€‹ç¨ç«‹çš„ K heads
    V: [B, N, H, D]  - H å€‹ç¨ç«‹çš„ V heads
    """
    # æ¯å€‹ head ç¨ç«‹è¨ˆç®—
    scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(D)
    # scores: [B, H, N, N]

    attn_weights = softmax(scores, dim=-1)
    output = torch.matmul(attn_weights, V)
    # output: [B, N, H, D]

    return output
```

### MQA å¯¦ç¾

```python
def multi_query_attention(Q, K, V):
    """
    Q: [B, N, H, D]  - H å€‹ç¨ç«‹çš„ Q heads
    K: [B, N, 1, D]  - 1 å€‹å…±äº«çš„ K head (å»£æ’­)
    V: [B, N, 1, D]  - 1 å€‹å…±äº«çš„ V head (å»£æ’­)
    """
    # K, V æœƒè‡ªå‹•å»£æ’­åˆ°æ‰€æœ‰ Q heads
    scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(D)
    # K è‡ªå‹•å»£æ’­: [B, N, 1, D] â†’ [B, N, H, D]

    attn_weights = softmax(scores, dim=-1)
    output = torch.matmul(attn_weights, V)
    # V è‡ªå‹•å»£æ’­: [B, N, 1, D] â†’ [B, N, H, D]

    return output

# KV Cache ç¯€çœ: H â†’ 1 (32x æ¸›å°‘!)
```

### GQA å¯¦ç¾

```python
def grouped_query_attention(Q, K, V, num_groups=4):
    """
    Q: [B, N, H, D]      - H=32 å€‹ç¨ç«‹çš„ Q heads
    K: [B, N, G, D]      - G=4 å€‹ KV groups
    V: [B, N, G, D]      - G=4 å€‹ KV groups

    åˆ†çµ„ç­–ç•¥: æ¯ (H/G) å€‹ Q heads å…±äº«ä¸€çµ„ K, V
    ä¾‹: 32 Q heads, 4 KV groups â†’ æ¯ 8 å€‹ Q å…±äº« 1 çµ„ KV
    """
    B, N, H, D = Q.shape
    G = K.size(2)

    # é‡å¡‘ Q ç‚º [B, N, G, H/G, D]
    Q = Q.view(B, N, G, H // G, D)

    # K, V æ“´å±•ç¶­åº¦: [B, N, G, 1, D]
    K = K.unsqueeze(3)
    V = V.unsqueeze(3)

    # è¨ˆç®— attention (æ¯çµ„å…§ç¨ç«‹)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(D)
    attn_weights = softmax(scores, dim=-1)
    output = torch.matmul(attn_weights, V)

    # é‡å¡‘å› [B, N, H, D]
    output = output.view(B, N, H, D)

    return output

# KV Cache ç¯€çœ: H â†’ G (32/4 = 8x æ¸›å°‘!)
```

---

## ğŸ“‚ å¯¦é©—å®¤çµæ§‹

```
Lab-1.6-Efficient_Attention_MQA_GQA/
â”œâ”€â”€ README.md                          # æœ¬æ–‡æª”
â”œâ”€â”€ 01-Setup.ipynb                    # æ¨™æº– MHA åŸºæº–æ¸¬è©¦
â”œâ”€â”€ 02-MQA_Implementation.ipynb       # Multi-Query Attention å¯¦ç¾
â”œâ”€â”€ 03-GQA_Implementation.ipynb       # Grouped-Query Attention å¯¦ç¾
â””â”€â”€ 04-Inference_Optimization.ipynb   # æ¨ç†åŠ é€Ÿèˆ‡ KV Cache å„ªåŒ–
```

---

## ğŸ“Š å¯¦é©—å…§å®¹è©³è§£

### Notebook 1: æ¨™æº– MHA åŸºæº–æ¸¬è©¦ (01-Setup.ipynb)
**æ™‚é–“**: 30-45åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- å¯¦ç¾æ¨™æº– Multi-Head Attention
- æ¸¬é‡è¨“ç·´èˆ‡æ¨ç†æ€§èƒ½åŸºæº–
- åˆ†æ KV Cache è¨˜æ†¶é«”å ç”¨
- ç†è§£æ¨ç†éç¨‹çš„è¨˜æ†¶é«”ç“¶é ¸

#### å¯¦é©—å…§å®¹
1. **MHA å®Œæ•´å¯¦ç¾**
   ```python
   class MultiHeadAttention(nn.Module):
       def __init__(self, hidden_dim, num_heads):
           self.num_heads = num_heads
           self.head_dim = hidden_dim // num_heads

           self.q_proj = nn.Linear(hidden_dim, hidden_dim)
           self.k_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads çµ„ K
           self.v_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads çµ„ V
           self.out_proj = nn.Linear(hidden_dim, hidden_dim)
   ```

2. **KV Cache æ©Ÿåˆ¶å¯¦ç¾**
   ```python
   class KVCache:
       """KV Cache ç®¡ç†"""
       def __init__(self, max_batch_size, max_seq_len, num_heads, head_dim):
           self.cache_k = torch.zeros(
               max_batch_size, max_seq_len, num_heads, head_dim
           )
           self.cache_v = torch.zeros(
               max_batch_size, max_seq_len, num_heads, head_dim
           )

       def update(self, k, v, start_pos):
           # æ›´æ–° cache
           seq_len = k.size(1)
           self.cache_k[:, start_pos:start_pos+seq_len] = k
           self.cache_v[:, start_pos:start_pos+seq_len] = v
   ```

3. **æ¨ç†æ€§èƒ½åŸºæº–**
   - Prefill éšæ®µæ€§èƒ½
   - Decode éšæ®µæ€§èƒ½
   - KV Cache è¨˜æ†¶é«”å ç”¨
   - ç«¯åˆ°ç«¯æ¨ç†å»¶é²

#### é æœŸçµæœ
- ç†è§£ KV Cache çš„ä½œç”¨
- è­˜åˆ¥æ¨ç†ç“¶é ¸
- å»ºç«‹æ€§èƒ½åŸºæº–

---

### Notebook 2: Multi-Query Attention å¯¦ç¾ (02-MQA_Implementation.ipynb)
**æ™‚é–“**: 45-60åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- å¯¦ç¾ MQA æ©Ÿåˆ¶
- å°æ¯” MQA vs MHA çš„æ€§èƒ½
- åˆ†æ KV Cache è¨˜æ†¶é«”ç¯€çœ
- è©•ä¼°æ¨¡å‹è³ªé‡å½±éŸ¿

#### å¯¦é©—å…§å®¹
1. **MQA å¯¦ç¾**
   ```python
   class MultiQueryAttention(nn.Module):
       def __init__(self, hidden_dim, num_heads):
           self.num_heads = num_heads
           self.head_dim = hidden_dim // num_heads

           self.q_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads çµ„ Q
           self.k_proj = nn.Linear(hidden_dim, self.head_dim)  # 1 çµ„ K (å…±äº«)
           self.v_proj = nn.Linear(hidden_dim, self.head_dim)  # 1 çµ„ V (å…±äº«)
           self.out_proj = nn.Linear(hidden_dim, hidden_dim)

       def forward(self, x):
           Q = self.q_proj(x)  # [B, N, H*D]
           K = self.k_proj(x)  # [B, N, D] - å–®å€‹ K
           V = self.v_proj(x)  # [B, N, D] - å–®å€‹ V

           # Q é‡å¡‘ç‚ºå¤šé ­
           Q = Q.view(B, N, num_heads, head_dim)

           # K, V é‡å¡‘ä¸¦æ“´å±•åˆ°æ‰€æœ‰ heads
           K = K.view(B, N, 1, head_dim).expand(B, N, num_heads, head_dim)
           V = V.view(B, N, 1, head_dim).expand(B, N, num_heads, head_dim)
   ```

2. **KV Cache å°æ¯”**
   - MHA Cache å¤§å°: `2 Ã— L Ã— H Ã— D`
   - MQA Cache å¤§å°: `2 Ã— L Ã— 1 Ã— D`
   - ç¯€çœæ¯”ä¾‹: `(H-1)/H` (32 heads â†’ 96.9%)

3. **æ€§èƒ½æ¸¬è©¦**
   - è¨“ç·´é€Ÿåº¦å°æ¯”
   - æ¨ç†é€Ÿåº¦å°æ¯” (é—œéµ!)
   - è¨˜æ†¶é«”å ç”¨å°æ¯”

4. **è³ªé‡è©•ä¼°**
   - è¨“ç·´ Loss å°æ¯”
   - ç”Ÿæˆè³ªé‡è©•ä¼°
   - Perplexity æ¸¬è©¦

#### é æœŸçµæœ
- KV Cache æ¸›å°‘ **30-32x**
- æ¨ç†é€Ÿåº¦æå‡ **1.5-2x**
- æ¨¡å‹è³ªé‡è¼•å¾®ä¸‹é™ (**<5%**)

---

### Notebook 3: Grouped-Query Attention å¯¦ç¾ (03-GQA_Implementation.ipynb)
**æ™‚é–“**: 45-60åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- å¯¦ç¾ GQA æ©Ÿåˆ¶
- æ¸¬è©¦ä¸åŒåˆ†çµ„æ•¸çš„å½±éŸ¿
- å°æ¯” MHA/MQA/GQA ä¸‰è€…æ€§èƒ½
- ç†è§£è³ªé‡èˆ‡æ•ˆç‡çš„æ¬Šè¡¡

#### å¯¦é©—å…§å®¹
1. **GQA å¯¦ç¾**
   ```python
   class GroupedQueryAttention(nn.Module):
       def __init__(self, hidden_dim, num_heads, num_kv_groups=4):
           self.num_heads = num_heads  # 32 query heads
           self.num_kv_groups = num_kv_groups  # 4 KV groups
           self.head_dim = hidden_dim // num_heads

           assert num_heads % num_kv_groups == 0
           self.heads_per_group = num_heads // num_kv_groups  # 8

           self.q_proj = nn.Linear(hidden_dim, hidden_dim)
           self.k_proj = nn.Linear(hidden_dim, num_kv_groups * self.head_dim)
           self.v_proj = nn.Linear(hidden_dim, num_kv_groups * self.head_dim)
           self.out_proj = nn.Linear(hidden_dim, hidden_dim)

       def forward(self, x):
           Q = self.q_proj(x).view(B, N, num_heads, head_dim)
           K = self.k_proj(x).view(B, N, num_kv_groups, head_dim)
           V = self.v_proj(x).view(B, N, num_kv_groups, head_dim)

           # æ¯çµ„ K, V æœå‹™å¤šå€‹ Q heads
           # K: [B, N, 4, D] â†’ repeat â†’ [B, N, 32, D]
           K = K.repeat_interleave(self.heads_per_group, dim=2)
           V = V.repeat_interleave(self.heads_per_group, dim=2)
   ```

2. **åˆ†çµ„æ•¸å¯¦é©—**
   - num_kv_groups = [1, 2, 4, 8, 16, 32]
   - æ¸¬è©¦æ¯ç¨®é…ç½®çš„æ€§èƒ½èˆ‡è³ªé‡
   - æ‰¾å‡ºæœ€ä½³å¹³è¡¡é»

3. **ä¸‰æ–¹å°æ¯”**
   - MHA (32 KV heads)
   - GQA-8 (8 KV groups) - Llama-2 é…ç½®
   - GQA-4 (4 KV groups)
   - MQA (1 KV head)

#### é æœŸçµæœ
| é…ç½® | KV Heads | Cache å¤§å° | æ¨ç†é€Ÿåº¦ | è³ªé‡ | æ¨è–¦ |
|------|---------|-----------|---------|------|------|
| MHA | 32 | 100% | 1.0x | 100% | è¨“ç·´ |
| GQA-8 | 8 | 25% | 1.3x | 99% | â­æ¨è–¦ |
| GQA-4 | 4 | 12.5% | 1.4x | 98% | å¹³è¡¡ |
| MQA | 1 | 3% | 1.8x | 95% | æ¿€é€² |

---

### Notebook 4: æ¨ç†å„ªåŒ–å¯¦æˆ° (04-Inference_Optimization.ipynb)
**æ™‚é–“**: 45-60åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- å¯¦ç¾å®Œæ•´çš„ KV Cache æ¨ç†å„ªåŒ–
- å°æ¯”é•·æ–‡æœ¬ç”Ÿæˆæ€§èƒ½
- æ¸¬è©¦æ‰¹æ¬¡æ¨ç†åŠ é€Ÿæ•ˆæœ
- åˆ†æå¯¦éš›éƒ¨ç½²å ´æ™¯

#### å¯¦é©—å…§å®¹
1. **KV Cache æ¨ç†å¯¦ç¾**
   ```python
   def generate_with_kv_cache(model, input_ids, max_new_tokens=100):
       """ä½¿ç”¨ KV Cache çš„é«˜æ•ˆç”Ÿæˆ"""
       batch_size, seq_len = input_ids.size()

       # åˆå§‹åŒ– KV Cache
       kv_cache = KVCache(batch_size, max_seq_len, num_kv_heads, head_dim)

       # Prefill éšæ®µ: è™•ç†æ•´å€‹ prompt
       with torch.no_grad():
           outputs = model(input_ids, past_key_values=kv_cache)
           next_token = outputs.logits[:, -1, :].argmax(dim=-1)

       # Decode éšæ®µ: é€å€‹ç”Ÿæˆ token
       generated = [next_token]
       for _ in range(max_new_tokens - 1):
           outputs = model(
               next_token.unsqueeze(1),
               past_key_values=kv_cache,
               use_cache=True
           )
           next_token = outputs.logits[:, -1, :].argmax(dim=-1)
           generated.append(next_token)

       return torch.stack(generated, dim=1)
   ```

2. **é•·æ–‡æœ¬ç”Ÿæˆæ¸¬è©¦**
   - ç”Ÿæˆé•·åº¦: [100, 500, 1000, 2000] tokens
   - æ¸¬é‡æ¯ç¨®æ¶æ§‹çš„é€Ÿåº¦èˆ‡è¨˜æ†¶é«”
   - å°æ¯” tokens/sec

3. **æ‰¹æ¬¡æ¨ç†å„ªåŒ–**
   ```python
   # æ‰¹æ¬¡æ¨ç† (batch_size = 16)
   # MHA: KV Cache = 1.07 GB Ã— 16 = 17 GB
   # GQA-8: KV Cache = 0.27 GB Ã— 16 = 4.3 GB
   # MQA: KV Cache = 0.03 GB Ã— 16 = 0.5 GB
   ```

4. **å¯¦éš›å ´æ™¯æ¨¡æ“¬**
   - Chatbot å¤šè¼ªå°è©±
   - é•·æ–‡æª”æ‘˜è¦
   - ä»£ç¢¼ç”Ÿæˆ
   - æ‰¹æ¬¡ç¿»è­¯

#### é æœŸæˆæœ
- ç†è§£ KV Cache å„ªåŒ–çš„é‡è¦æ€§
- æŒæ¡ä¸åŒæ¶æ§‹çš„é©ç”¨å ´æ™¯
- èƒ½å¤ ç‚ºå¯¦éš›æ‡‰ç”¨é¸æ“‡æœ€ä½³é…ç½®

---

## ğŸš€ ç’°å¢ƒæº–å‚™

### å‰ç½®è¦æ±‚

#### ç¡¬é«”è¦æ±‚
- **GPU**: 8GB+ VRAM (æ¨è–¦ 16GB+)
- **CUDA**: 11.6+
- **æ¨ç†æ¸¬è©¦**: è¨˜æ†¶é«”è¶Šå¤§, å¯æ¸¬è©¦çš„æ‰¹æ¬¡/é•·åº¦è¶Šå¤§

#### è»Ÿé«”ä¾è³´
```bash
# å·²åœ¨ Poetry ç’°å¢ƒä¸­åŒ…å«
source .venv/bin/activate

# é©—è­‰é—œéµå¥—ä»¶
python -c "import torch; print(torch.__version__)"
python -c "import transformers; print(transformers.__version__)"
```

---

## ğŸ’¡ ç†è«–æ·±åº¦è§£æ

### KV Cache åŸç†

**è‡ªå›æ­¸ç”Ÿæˆéç¨‹**:
```
ç”Ÿæˆ "The cat is"

Step 1: Input = "The"
  â†’ Qâ‚, Kâ‚, Vâ‚
  â†’ Attention(Qâ‚, Kâ‚, Vâ‚) â†’ outputâ‚ â†’ "cat"
  â†’ Cache: Kâ‚, Vâ‚

Step 2: Input = "cat" (ä¸éœ€è¦é‡æ–°è¨ˆç®— "The")
  â†’ Qâ‚‚, Kâ‚‚, Vâ‚‚
  â†’ Attention(Qâ‚‚, [Kâ‚,Kâ‚‚], [Vâ‚,Vâ‚‚]) â†’ outputâ‚‚ â†’ "is"
  â†’ Cache: Kâ‚, Vâ‚, Kâ‚‚, Vâ‚‚

Step 3: Input = "is"
  â†’ Qâ‚ƒ, Kâ‚ƒ, Vâ‚ƒ
  â†’ Attention(Qâ‚ƒ, [Kâ‚,Kâ‚‚,Kâ‚ƒ], [Vâ‚,Vâ‚‚,Vâ‚ƒ]) â†’ outputâ‚ƒ
  â†’ Cache: Kâ‚, Vâ‚, Kâ‚‚, Vâ‚‚, Kâ‚ƒ, Vâ‚ƒ

æ¯æ­¥åªè¨ˆç®—æ–° token çš„ Q, K, V
æ­·å² K, V å¾ cache è®€å– (é¿å…é‡è¤‡è¨ˆç®—)
```

**è¨˜æ†¶é«”å¢é•·**:
```
æ¯ç”Ÿæˆä¸€å€‹ token:
  KV Cache += 2 Ã— num_kv_heads Ã— head_dim Ã— bytes

ç”Ÿæˆ N å€‹ tokens:
  Total KV Cache = 2 Ã— N Ã— num_kv_heads Ã— head_dim Ã— bytes

MQA å„ªå‹¢: num_kv_heads = 1 (æœ€å°)
GQA å¹³è¡¡: num_kv_heads = 4-8 (ä¸­ç­‰)
MHA æ¨™æº–: num_kv_heads = 32 (æœ€å¤§)
```

### ç‚ºä»€éº¼ MQA/GQA æœ‰æ•ˆ?

**é—œéµæ´å¯Ÿ**:
1. **Query éœ€è¦å¤šæ¨£æ€§** - ä¸åŒ heads é—œæ³¨ä¸åŒæ¨¡å¼
2. **Key/Value å¯ä»¥å…±äº«** - è³‡è¨Šè¡¨ç¤ºä¸éœ€è¦é‚£éº¼å¤šæ¨£åŒ–
3. **æ¨ç†æ˜¯è¨˜æ†¶é«”å—é™** - æ¸›å°‘ KV Cache æ˜¯é—œéµ

**ç†è«–æ”¯æŒ**:
- **å¯¦é©—è­‰æ“š**: PaLM, Llama-2 ç­‰å¤§æ¨¡å‹è­‰æ˜ MQA/GQA å¯è¡Œ
- **è³ªé‡ä¿æŒ**: GQA å¯ä¿æŒæ¥è¿‘ MHA çš„æ¨¡å‹è³ªé‡
- **é€Ÿåº¦æå‡**: KV Cache æ¸›å°‘ç›´æ¥è½‰åŒ–ç‚ºé€Ÿåº¦æå‡

---

## ğŸ“ˆ æ€§èƒ½é æœŸ

### è¨“ç·´æ€§èƒ½å°æ¯” (GPT-2, seq_len=1024)

| æ¶æ§‹ | å‰å‘æ™‚é–“ | åå‘æ™‚é–“ | è¨˜æ†¶é«” | ç›¸å°MHA |
|------|---------|---------|--------|---------|
| MHA | 45ms | 90ms | 8.5GB | 1.0x |
| GQA-8 | 42ms | 85ms | 8.2GB | 1.05x â¬† |
| GQA-4 | 40ms | 82ms | 8.0GB | 1.08x â¬† |
| MQA | 38ms | 78ms | 7.8GB | 1.12x â¬† |

*è¨“ç·´éšæ®µå·®ç•°è¼ƒå°*

### æ¨ç†æ€§èƒ½å°æ¯” (ç”Ÿæˆ 1000 tokens)

| æ¶æ§‹ | KV Cache | Decode æ™‚é–“ | ç¸½æ™‚é–“ | ååé‡ | ç›¸å°MHA |
|------|---------|------------|--------|--------|---------|
| MHA | 1.07GB | 15.2s | 16.0s | 62 tok/s | 1.0x |
| GQA-8 | 0.27GB | 11.8s | 12.5s | 80 tok/s | 1.29x â¬† |
| GQA-4 | 0.13GB | 10.5s | 11.2s | 89 tok/s | 1.43x â¬† |
| MQA | 0.03GB | 8.9s | 9.6s | 104 tok/s | 1.67x â¬† |

*æ¨ç†éšæ®µå·®ç•°é¡¯è‘—*

### æ‰¹æ¬¡æ¨ç†å°æ¯” (batch_size=16, ç”Ÿæˆ 500 tokens)

| æ¶æ§‹ | ç¸½ KV Cache | æ˜¯å¦ OOM (24GB GPU) | ååé‡ |
|------|------------|-------------------|--------|
| MHA | 17.1GB | âš ï¸  æ¥è¿‘æ¥µé™ | 880 tok/s |
| GQA-8 | 4.3GB | âœ… å……è¶³ | 1150 tok/s |
| GQA-4 | 2.1GB | âœ… å……è¶³ | 1280 tok/s |
| MQA | 0.5GB | âœ… å……è¶³ | 1500 tok/s |

---

## ğŸ› ï¸ å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹

### 1. Llama-2 çš„ GQA é…ç½®

```python
# Llama-2-7B ä½¿ç”¨ GQA
config = {
    'hidden_size': 4096,
    'num_attention_heads': 32,    # Query heads
    'num_key_value_heads': 8,     # KV groups (GQA-8)
    'max_position_embeddings': 4096
}

# KV Cache ç¯€çœ: 32 â†’ 8 (4x æ¸›å°‘)
```

### 2. Mistral-7B çš„ GQA é…ç½®

```python
# Mistral-7B ä½¿ç”¨ GQA + Sliding Window
config = {
    'hidden_size': 4096,
    'num_attention_heads': 32,
    'num_key_value_heads': 8,     # GQA-8
    'sliding_window': 4096,       # æ»‘å‹•çª—å£æ³¨æ„åŠ›
    'max_position_embeddings': 32768  # æ”¯æ´è¶…é•·åºåˆ—
}
```

### 3. è‡ªå®šç¾©é…ç½®æŒ‡å—

**é¸æ“‡ KV groups æ•¸é‡**:
```python
# æ¨è–¦æ¯”ä¾‹
num_query_heads = 32

# ä¿å®ˆ (è³ªé‡å„ªå…ˆ)
num_kv_groups = 16  # GQA-16, 2x æ¸›å°‘

# å¹³è¡¡ (Llama-2 æ–¹æ¡ˆ)
num_kv_groups = 8   # GQA-8, 4x æ¸›å°‘

# æ¿€é€² (é€Ÿåº¦å„ªå…ˆ)
num_kv_groups = 4   # GQA-4, 8x æ¸›å°‘

# æ¥µè‡´ (MQA)
num_kv_groups = 1   # MQA, 32x æ¸›å°‘
```

---

## ğŸ“ å­¸ç¿’æª¢æŸ¥æ¸…å–®

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œæ‚¨æ‡‰è©²èƒ½å¤ :

### ç†è«–ç†è§£
- [ ] è§£é‡‹ KV Cache çš„ä½œç”¨èˆ‡é‡è¦æ€§
- [ ] ç†è§£ MHA/MQA/GQA çš„æ¶æ§‹å·®ç•°
- [ ] èªªæ˜ç‚ºä»€éº¼æ¨ç†æ˜¯è¨˜æ†¶é«”å—é™çš„
- [ ] åˆ†æä¸åŒæ¶æ§‹çš„è³ªé‡èˆ‡æ•ˆç‡æ¬Šè¡¡
- [ ] ç†è§£åˆ†çµ„æ³¨æ„åŠ›çš„å»£æ’­æ©Ÿåˆ¶

### å¯¦ä½œæŠ€èƒ½
- [ ] å¯¦ç¾ Multi-Head Attention (MHA)
- [ ] å¯¦ç¾ Multi-Query Attention (MQA)
- [ ] å¯¦ç¾ Grouped-Query Attention (GQA)
- [ ] å¯¦ç¾ KV Cache ç®¡ç†
- [ ] å„ªåŒ–æ¨ç†æ€§èƒ½

### æ‡‰ç”¨èƒ½åŠ›
- [ ] ç‚ºé …ç›®é¸æ“‡åˆé©çš„æ³¨æ„åŠ›æ¶æ§‹
- [ ] é…ç½® KV Cache åƒæ•¸
- [ ] å„ªåŒ–æ‰¹æ¬¡æ¨ç†ååé‡
- [ ] è©•ä¼°è³ªé‡èˆ‡é€Ÿåº¦çš„æ¬Šè¡¡

---

## ğŸš€ ä¸‹ä¸€æ­¥å­¸ç¿’

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œå»ºè­°ç¹¼çºŒ:

1. **Lab-1.7: DPO Alignment**
   - ç›´æ¥åå¥½å„ªåŒ–
   - æ¨¡å‹å°é½ŠæŠ€è¡“

2. **æ¨ç†éƒ¨ç½²å„ªåŒ–**
   - vLLM èˆ‡ PagedAttention
   - Continuous Batching
   - Speculative Decoding

3. **ç”Ÿç”¢ç’°å¢ƒå¯¦è¸**
   - æ¨¡å‹æœå‹™åŒ–
   - å»¶é²å„ªåŒ–
   - æˆæœ¬æ§åˆ¶

---

**å¯¦é©—å®¤ç‹€æ…‹**: ğŸ”„ é–‹ç™¼ä¸­
**æœ€å¾Œæ›´æ–°**: 2025-10-08
**ç¶­è­·è€…**: LLM æ•™å­¸å°ˆæ¡ˆåœ˜éšŠ

**ç›¸é—œæ–‡ä»¶**:
- ç†è«–: `01-Theory/1.3-Optimization_and_Alignment.md` (MQA/GQA ç« ç¯€)
- å‰ç½®å¯¦é©—: `Lab-1.5-FlashAttention_Deep_Dive`
- å¾ŒçºŒå¯¦é©—: `Lab-1.7-DPO_Alignment`

**ç›¸é—œè«–æ–‡**:
- [Fast Transformer Decoding: One Write-Head is All You Need (MQA)](https://arxiv.org/abs/1911.02150)
- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
