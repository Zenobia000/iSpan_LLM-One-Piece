{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. 推理與評估\n",
        "\n",
        "模型經過 LoRA 微調後，本筆記本將示範如何使用這個帶有 PEFT adapter 的模型進行推理，並觀察其在特定任務上的表現。\n",
        "\n",
        "## 步驟 1: 準備模型進行推理\n",
        "\n",
        "在訓練完成後，`peft` 模型會自動儲存 adapter 的權重。要進行推理，我們需要重新載入基礎模型和 adapter。\n",
        "\n",
        "**重要提示**: 由於我們在訓練時使用了 4-bit 量化，推理時也必須以相同的量化設定載入基礎模型，否則將無法正確載入 adapter 權重。\n",
        "\n",
        "我們將從 `./lora-llama2-7b-chat` 目錄中最新的 checkpoint 載入我們訓練好的 LoRA adapter。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "# --- 重新載入基礎模型與 Tokenizer ---\n",
        "# (與 02, 03 筆記本中的設定相同)\n",
        "model_id = \"meta-llama/Llama2-7b-chat-hf\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 載入 LoRA Adapter ---\n",
        "# 找到最新的 checkpoint\n",
        "output_dir = \"./lora-llama2-7b-chat\"\n",
        "latest_checkpoint = max([os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")], key=os.path.getmtime)\n",
        "print(f\"從 {latest_checkpoint} 載入 adapter\")\n",
        "\n",
        "# 從 checkpoint 載入 PeftModel\n",
        "inference_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "print(\"\n",
        "模型準備完成，可以開始推理。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 2: 進行文本生成\n",
        "\n",
        "現在我們可以使用微調後的模型來生成文本了。我們將提供一個提示 (prompt)，並觀察模型的回答是否比原始的 Llama-2-chat 模型更符合我們的微調資料集風格。\n",
        "\n",
        "`guanaco-llama2-1k` 資料集的風格是問答式的。我們將使用一個類似的問題來測試模型。\n",
        "\n",
        "推理流程如下：\n",
        "1.  將提示文本編碼為 `input_ids`。\n",
        "2.  使用模型的 `generate()` 方法生成文本。\n",
        "3.  將生成的 `output_ids` 解碼回人類可讀的文本。\n",
        "\n",
        "`generate()` 方法的參數：\n",
        "- `max_new_tokens`: 控制生成文本的最大長度。\n",
        "- `do_sample=True`: 啟用採樣，使生成更多樣。\n",
        "- `top_k`: Top-K 採樣。\n",
        "- `num_return_sequences`: 指定生成幾個不同的序列。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 準備提示\n",
        "prompt = \"請向我介紹一下台灣的玉山國家公園\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "\n",
        "# 進行推理\n",
        "# attention_mask is not strictly necessary for generation with Llama models, \n",
        "# but it's good practice to include it.\n",
        "with torch.no_grad():\n",
        "    outputs = inference_model.generate(\n",
        "        input_ids=input_ids, \n",
        "        max_new_tokens=256, \n",
        "        do_sample=True, \n",
        "        top_k=50,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "# 解碼並打印結果\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"模型生成的回應：\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (可選) 步驟 3: 與基礎模型比較\n",
        "\n",
        "為了更好地評估 LoRA 微調的效果，您可以載入未經微調的原始 Llama-2 模型，並用相同的提示進行推理，然後比較兩者的輸出差異。\n",
        "\n",
        "---\n",
        "\n",
        "推理完成！在最後一個筆記本 `05-merge-and-save.ipynb` 中，我們將學習如何將 LoRA adapter 合併回基礎模型，以便於未來部署。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
