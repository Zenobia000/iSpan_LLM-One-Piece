# 4.1 LLM 評估基準全景圖

**更新日期**: 2025-10-17
**適用對象**: LLM 工程師、研究員、產品經理
**閱讀時間**: 30 分鐘

---

## 📋 目錄

1. [為何需要評估基準](#1-為何需要評估基準)
2. [評估基準分類體系](#2-評估基準分類體系)
3. [主流中文評估基準](#3-主流中文評估基準)
4. [主流英文評估基準](#4-主流英文評估基準)
5. [專業領域評估基準](#5-專業領域評估基準)
6. [評估方法論](#6-評估方法論)
7. [評估結果解讀](#7-評估結果解讀)
8. [基準選擇指南](#8-基準選擇指南)
9. [評估的局限性](#9-評估的局限性)
10. [未來發展趨勢](#10-未來發展趨勢)

---

## 1. 為何需要評估基準

### 1.1 評估的核心價值

在 LLM 工程化過程中，評估基準（Benchmark）扮演著不可或缺的角色：

**客觀性**：
- 避免主觀判斷的偏見
- 提供量化指標支持決策
- 確保不同團隊結果可比較

**全面性**：
- 多維度評估模型能力
- 識別優勢與劣勢領域
- 發現潛在風險點

**可重現性**：
- 標準化評估協議
- 固定測試集與評估指標
- 支持結果驗證與審計

**效率性**：
- 自動化評估流程
- 快速驗證訓練效果
- 降低人工評估成本

### 1.2 評估在 LLM 生命週期中的位置

```
預訓練 → 評估語言建模能力（Perplexity）
   ↓
微調 → 評估下游任務性能（Accuracy, F1）
   ↓
對齊 → 評估安全性與有用性（HH-RLHF）
   ↓
壓縮/優化 → 評估精度-效率權衡
   ↓
部署 → 評估推理性能（TTFT, QPS）
```

### 1.3 評估 vs 測試

| 維度 | 評估（Evaluation） | 測試（Testing） |
|:---|:---|:---|
| **目的** | 量化模型能力 | 驗證功能正確性 |
| **數據** | 固定基準數據集 | 動態測試用例 |
| **指標** | 準確率、F1、Perplexity | 通過/失敗 |
| **頻率** | 階段性（訓練後、部署前） | 持續性（CI/CD） |

---

## 2. 評估基準分類體系

### 2.1 按能力維度分類

**語言理解**：閱讀理解、情感分析、自然語言推理
- 基準：GLUE, SuperGLUE, CLUE, C-Eval

**語言生成**：文本摘要、對話生成、創意寫作
- 基準：CNN/DailyMail, XSum, PersonaChat

**知識與推理**：常識推理、數學推理、因果推理
- 基準：CommonsenseQA, GSM8K, HellaSwag

**代碼能力**：代碼生成、代碼理解、問題求解
- 基準：HumanEval, MBPP, CodeXGLUE

### 2.2 按評估範式分類

**判別式評估**：計算各選項的對數似然，選擇最高的
**生成式評估**：生成完整答案，與參考答案比對
**混合式評估**：結合判別與生成的優勢

### 2.3 按評估難度分類

| 難度 | Few-shot | 任務複雜度 | 推理步驟 | 代表基準 |
|:---|:---:|:---|:---:|:---|
| **簡單** | 0-shot | 單句理解 | 1 步 | SST-2 |
| **中等** | 5-shot | 段落理解 | 2-3 步 | MMLU |
| **困難** | 5-shot | 多文檔推理 | 5+ 步 | BBH |
| **專家級** | Few-shot CoT | 專業領域 | 10+ 步 | MATH, GSM8K |

---

## 3. 主流中文評估基準

### 3.1 C-Eval (Chinese Evaluation Suite)

**概述**：全面的中文評估基準，涵蓋 52 個學科，從中學到專業級別。

**核心特性**：
- 學科覆蓋：52 個學科（STEM、社會科學、人文、其他）
- 難度分層：初中、高中、大學、專業級別
- 題型：多選題（4 選項）
- 樣本量：13,948 道題目

**主流模型表現**（截至 2024 年）：

| 模型 | 整體 | STEM | 社會科學 | 人文 |
|:---|---:|---:|---:|---:|
| GPT-4 | 68.7% | 67.1% | 77.6% | 64.5% |
| Qwen-14B | 72.1% | 70.2% | 81.8% | 67.1% |
| Qwen-7B | 59.7% | 56.2% | 74.1% | 63.1% |
| Llama-2-7B | 45.3% | 42.1% | 52.9% | 48.7% |

**關鍵觀察**：
- Qwen 系列在中文任務上具有顯著優勢
- Llama-2 在中文基準上表現較弱
- 社會科學類別得分普遍高於 STEM

### 3.2 CMMLU (Chinese Massive Multitask Language Understanding)

**概述**：中文多任務語言理解基準，涵蓋 67 個任務，包含繁體中文支持。

**與 C-Eval 對比**：

| 維度 | C-Eval | CMMLU |
|:---|:---|:---|
| 學科數量 | 52 | 67 |
| 語言支持 | 簡體中文 | 簡繁體中文 |
| 難度分層 | 4 個級別 | 主要大學及以上 |
| 題型 | 純多選題 | 多選 + 開放式 |

---

## 4. 主流英文評估基準

### 4.1 MMLU (Massive Multitask Language Understanding)

**概述**：英文多任務語言理解基準，LLM 評估的事實標準。

**核心特性**：
- 學科覆蓋：57 個學科
- 難度：從小學到專家級別
- 題型：4 選 1 多選題
- 樣本量：15,908 道題目
- 評估模式：5-shot

**主流模型表現**：

| 模型 | 整體 | STEM | 人文 | 社會科學 |
|:---|---:|---:|---:|---:|
| GPT-4 | 86.4% | 83.2% | 85.1% | 88.6% |
| Llama-2-70B | 69.8% | 67.3% | 69.2% | 71.4% |
| Llama-2-7B | 46.8% | 43.9% | 46.1% | 48.2% |

### 4.2 GSM8K (Grade School Math 8K)

**概述**：小學數學應用題基準，測試數學推理能力。

**核心特性**：
- 題目數量：8,500 道題（訓練 7.5K，測試 1K）
- 難度：美國小學 2-8 年級水平
- 評估重點：多步驟算術推理

**主流模型表現**：

| 模型 | 準確率 | 使用 CoT |
|:---|---:|:---:|
| GPT-4 | 92.0% | ✅ |
| Llama-2-7B | 14.6% | ✅ |
| Llama-2-7B | 10.2% | ❌ |

**關鍵觀察**：
- Chain-of-Thought (CoT) 顯著提升性能（~30-40%）
- 小型模型（<10B）在數學推理上表現較弱

### 4.3 HumanEval (Code Generation)

**概述**：代碼生成評估基準，測試函數級別的編程能力。

**主流模型表現**：

| 模型 | Pass@1 | Pass@10 |
|:---|---:|---:|
| GPT-4 | 67.0% | 82.3% |
| CodeLlama-34B | 48.8% | 62.1% |
| Llama-2-7B | 12.8% | 23.4% |

---

## 5. 專業領域評估基準

### 5.1 醫學領域

**MedQA / MedMCQA**：美國醫師執照考試（USMLE）風格問題

| 模型 | MedQA |
|:---|---:|
| GPT-4 | 81.4% |
| Med-PaLM 2 | 85.4% |

### 5.2 法律領域

**LegalBench**：162 個法律推理任務
**CUAD**：合同理解與分析

---

## 6. 評估方法論

### 6.1 Zero-shot vs Few-shot

**Zero-shot**：
- 優勢：測試模型的泛化能力，評估速度快
- 劣勢：性能通常較低，對指令敏感

**Few-shot**：
- 優勢：更接近實際使用，性能更好
- 劣勢：示例選擇影響結果，需要更長的上下文

**典型配置**：

| 基準 | Few-shot 設定 | 原因 |
|:---|:---:|:---|
| MMLU | 5-shot | 標準協議 |
| C-Eval | 5-shot | 與 MMLU 一致 |
| GSM8K | 8-shot | 數學需要更多示例 |
| HumanEval | 0-shot | 代碼通常不需要示例 |

### 6.2 Chain-of-Thought (CoT) 評估

**原理**：讓模型在生成答案前，先生成中間推理步驟。

**效果提升**：

| 任務 | 標準 Prompt | CoT Prompt | 提升 |
|:---|---:|---:|---:|
| GSM8K | 10.2% | 14.6% | +43% |
| MATH | 3.5% | 6.8% | +94% |
| BBH | 34.2% | 48.9% | +43% |

---

## 7. 評估結果解讀

### 7.1 分數解讀指南

| 分數範圍 | 能力等級 | 適用場景 |
|:---|:---|:---|
| **<40%** | 基礎級 | 簡單對話、基礎分類 |
| **40-60%** | 中級 | 通用助手、信息檢索 |
| **60-80%** | 高級 | 專業領域、複雜推理 |
| **80-95%** | 專家級 | 關鍵任務、高風險場景 |

### 7.2 人類基準對照

| 基準 | 平均人類 | 專家人類 | GPT-4 | 差距 |
|:---|---:|---:|---:|---:|
| MMLU | ~65% | ~90% | 86.4% | -3.6% |
| C-Eval | ~60% | ~85% | 68.7% | -16.3% |
| GSM8K | ~85% | ~95% | 92.0% | -3.0% |

---

## 8. 基準選擇指南

### 8.1 根據應用場景選擇

| 應用場景 | 推薦基準 |
|:---|:---|
| 中文通用助手 | C-Eval, CMMLU |
| 英文通用助手 | MMLU, BBH |
| 代碼生成 | HumanEval, MBPP |
| 數學推理 | GSM8K, MATH |

### 8.2 評估預算與時間

| 預算 | 時間 | 推薦策略 |
|:---|:---|:---|
| 低 | <1h | MMLU 子集, C-Eval 子集 |
| 中 | 2-4h | MMLU, C-Eval, GSM8K |
| 高 | 6-12h | 所有主流基準 |

---

## 9. 評估的局限性

### 9.1 基準污染

**問題**：訓練數據中包含測試集數據，導致評估結果虛高。

**緩解策略**：
- 使用新發布的基準
- 動態生成測試集
- 多基準交叉驗證

### 9.2 無法評估的能力

- 創意寫作質量
- 長文本理解（>10K tokens）
- 實時信息獲取
- 安全性與價值觀對齊

---

## 10. 未來發展趨勢

### 10.1 動態評估

根據模型表現動態調整評估難度與任務。

### 10.2 多模態評估

評估模型的跨模態理解與生成能力。

### 10.3 對抗評估

測試模型的魯棒性與安全性。

---

## 📚 總結

1. **評估是 LLM 工程化的基石**
2. **選擇合適基準**：根據應用場景、語言、領域選擇
3. **多維度評估**：單一基準無法全面反映模型能力
4. **理解局限性**：評估有偏差，需結合人工評估

**推薦評估組合**：

**最小評估集**：C-Eval (子集), MMLU (子集)
**標準評估集**：MMLU, C-Eval, GSM8K, HumanEval
**全面評估集**：MMLU, C-Eval, CMMLU, GSM8K, BBH, HumanEval

---

**延伸閱讀**：
- OpenCompass 官方文檔: https://opencompass.readthedocs.io/
- HELM 評估體系: https://crfm.stanford.edu/helm/
- BIG-bench: https://github.com/google/BIG-bench

**下一步**：進入 [Lab-4.1](../02-Labs/Lab-4.1-Evaluate_with_OpenCompass/) 實際操作 OpenCompass 評估框架。
