{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Lab-1.6: é«˜æ•ˆæ³¨æ„åŠ›æ©Ÿåˆ¶æ¶æ§‹åˆ†æ\n## MHA vs MQA vs GQA å®Œæ•´å°æ¯”\n\n**å­¸ç¿’ç›®æ¨™**:\n- æ·±åº¦ç†è§£ä¸‰ç¨®æ³¨æ„åŠ›æ¶æ§‹çš„æ ¸å¿ƒå·®ç•°\n- æŒæ¡ KV Cache å„ªåŒ–åŸç†èˆ‡å¯¦ç¾\n- åˆ†æè¨˜æ†¶é«”ä½¿ç”¨èˆ‡æ¨ç†æ€§èƒ½çš„æ¬Šè¡¡\n- å¯¦ç¾å®Œæ•´çš„æ€§èƒ½åŸºæº–æ¸¬è©¦\n\n**å‰ç½®çŸ¥è­˜**: Multi-Head Attention, PyTorch åŸºç¤, CUDA è¨˜æ†¶é«”ç®¡ç†\n\n---\n\n## ğŸ” æ ¸å¿ƒæ¶æ§‹å°æ¯”\n\n### 1. Multi-Head Attention (MHA) - å‚³çµ±æ–¹æ³•\n```\nQ: [batch, seq_len, num_heads=32, head_dim=128]     â† 32 çµ„ç¨ç«‹çš„ Query\nK: [batch, seq_len, num_heads=32, head_dim=128]     â† 32 çµ„ç¨ç«‹çš„ Key  \nV: [batch, seq_len, num_heads=32, head_dim=128]     â† 32 çµ„ç¨ç«‹çš„ Value\n\nç‰¹é»: æ¯å€‹é ­éƒ½æœ‰ç¨ç«‹çš„ K, V\nå„ªé»: æœ€é«˜è¡¨ç¾åŠ›ï¼Œç†è«–ä¸Šè³ªé‡æœ€å¥½\nç¼ºé»: KV Cache æœ€å¤§ï¼Œæ¨ç†é€Ÿåº¦æœ€æ…¢\n```\n\n### 2. Multi-Query Attention (MQA) - æ¥µè‡´å„ªåŒ–\n```\nQ: [batch, seq_len, num_heads=32, head_dim=128]     â† 32 çµ„ç¨ç«‹çš„ Query\nK: [batch, seq_len, num_heads=1,  head_dim=128]     â† 1 çµ„å…±äº«çš„ Key (å»£æ’­)\nV: [batch, seq_len, num_heads=1,  head_dim=128]     â† 1 çµ„å…±äº«çš„ Value (å»£æ’­)\n\nç‰¹é»: æ‰€æœ‰é ­å…±äº«å–®ä¸€ K, V\nå„ªé»: KV Cache æœ€å° (æ¸›å°‘ 32x)ï¼Œé€Ÿåº¦æœ€å¿«\nç¼ºé»: è¡¨ç¾åŠ›å—é™ï¼Œè³ªé‡å¯èƒ½ç•¥é™\n```\n\n### 3. Grouped-Query Attention (GQA) - å¹³è¡¡ä¹‹é¸\n```\nQ: [batch, seq_len, num_heads=32, head_dim=128]     â† 32 çµ„ç¨ç«‹çš„ Query\nK: [batch, seq_len, num_heads=8,  head_dim=128]     â† 8 çµ„åˆ†çµ„çš„ Key\nV: [batch, seq_len, num_heads=8,  head_dim=128]     â† 8 çµ„åˆ†çµ„çš„ Value\n\nç‰¹é»: 4 å€‹ Q heads å…±äº« 1 çµ„ K, V (32Ã·8=4)\nå„ªé»: å¹³è¡¡è³ªé‡èˆ‡æ•ˆç‡ï¼Œå»£æ³›é©—è­‰\nç¼ºé»: è¤‡é›œåº¦ç›¸å°è¼ƒé«˜\n```\n\n---\n\n## ğŸ“Š è³‡æºæ¶ˆè€—å°æ¯” (Llama-2-7B è¦æ¨¡)\n\n| æ¶æ§‹ | KV Groups | KV Cache (GB) | ç›¸å°MHA | æ¨ç†åŠ é€Ÿ | è³ªé‡ä¿æŒ |\n|------|-----------|---------------|---------|----------|----------|\n| MHA  | 32        | 1.05          | 100%    | 1.0x     | 100%     |\n| GQA-8| 8         | 0.26          | 25%     | 1.3x     | 95-98%   |\n| GQA-4| 4         | 0.13          | 12.5%   | 1.5x     | 90-95%   |\n| MQA  | 1         | 0.03          | 3%      | 1.8x     | 85-90%   |\n\n---\n\n## ğŸ¯ é¸æ“‡å»ºè­°\n\n**ğŸš€ è¿½æ±‚æ¥µè‡´é€Ÿåº¦**: MQA\n- é©ç”¨: æ¨ç†å¯†é›†å‹æ‡‰ç”¨ã€é‚Šç·£è¨­å‚™\n- å…¸å‹: PaLM, Falcon æ¨¡å‹\n\n**âš–ï¸ å¹³è¡¡è³ªé‡èˆ‡æ•ˆç‡**: GQA-8\n- é©ç”¨: ç”Ÿç”¢éƒ¨ç½²ã€é€šç”¨æœå‹™\n- å…¸å‹: Llama-2, Mistral æ¨¡å‹\n\n**ğŸ”¬ ç ”ç©¶èˆ‡åŸºæº–**: MHA\n- é©ç”¨: è³ªé‡ä¸Šé™æ¢ç´¢ã€ç®—æ³•é©—è­‰\n- å…¸å‹: GPT, BERT ç³»åˆ—æ¨¡å‹\n\n**é è¨ˆæ™‚é–“**: 60-90åˆ†é˜"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è¨­å‚™: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"æ¨™æº– Multi-Head Attention\"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads çµ„ K\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)  # num_heads çµ„ V\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        B, N, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # è™•ç† KV Cache\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            K = torch.cat([past_k, K], dim=1)\n",
    "            V = torch.cat([past_v, V], dim=1)\n",
    "        \n",
    "        # Transpose for attention\n",
    "        Q = Q.transpose(1, 2)  # [B, H, N, D]\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(B, N, self.hidden_dim)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            return output, (K.transpose(1, 2), V.transpose(1, 2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. KV Cache æ€§èƒ½æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KV Cache å¤§å°åˆ†æ (Llama-2-7B é…ç½®)\n",
      "======================================================================\n",
      "\n",
      "é…ç½®:\n",
      "  num_layers: 32\n",
      "  num_query_heads: 32\n",
      "  head_dim: 128\n",
      "  seq_len: 2048\n",
      "\n",
      "æ¶æ§‹                   KV Heads     æ¯å±¤(MB)          ç¸½è¨ˆ(GB)          ç›¸å°MHA       \n",
      "----------------------------------------------------------------------\n",
      "MHA (æ¨™æº–)             32           33.55           1.05            100%        \n",
      "GQA-8                8            8.39            0.26            25.0%       \n",
      "GQA-4                4            4.19            0.13            12.5%       \n",
      "MQA                  1            1.05            0.03            3.1%        \n",
      "\n",
      "ğŸ’¡ è§€å¯Ÿ: MQA å°‡ KV Cache æ¸›å°‘è‡³ MHA çš„ 3%, GQA-8 æ¸›å°‘è‡³ 25%\n"
     ]
    }
   ],
   "source": [
    "def measure_kv_cache_size(num_layers, seq_len, num_kv_heads, head_dim, dtype=torch.float16):\n",
    "    \"\"\"è¨ˆç®— KV Cache å¤§å°\"\"\"\n",
    "    bytes_per_element = 2 if dtype == torch.float16 else 4\n",
    "    \n",
    "    # æ¯å±¤: 2 (K+V) Ã— seq_len Ã— num_kv_heads Ã— head_dim Ã— bytes\n",
    "    per_layer_mb = 2 * seq_len * num_kv_heads * head_dim * bytes_per_element / 1e6\n",
    "    total_mb = per_layer_mb * num_layers\n",
    "    \n",
    "    return {\n",
    "        'per_layer_mb': per_layer_mb,\n",
    "        'total_mb': total_mb,\n",
    "        'total_gb': total_mb / 1024\n",
    "    }\n",
    "\n",
    "# Llama-2-7B é…ç½®\n",
    "config = {\n",
    "    'num_layers': 32,\n",
    "    'num_query_heads': 32,\n",
    "    'head_dim': 128,\n",
    "    'seq_len': 2048\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KV Cache å¤§å°åˆ†æ (Llama-2-7B é…ç½®)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\né…ç½®:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# å°æ¯”ä¸åŒæ¶æ§‹\n",
    "architectures = [\n",
    "    ('MHA (æ¨™æº–)', config['num_query_heads']),\n",
    "    ('GQA-8', 8),\n",
    "    ('GQA-4', 4),\n",
    "    ('MQA', 1)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'æ¶æ§‹':<20} {'KV Heads':<12} {'æ¯å±¤(MB)':<15} {'ç¸½è¨ˆ(GB)':<15} {'ç›¸å°MHA':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "mha_total = None\n",
    "for name, num_kv_heads in architectures:\n",
    "    cache_size = measure_kv_cache_size(\n",
    "        config['num_layers'],\n",
    "        config['seq_len'],\n",
    "        num_kv_heads,\n",
    "        config['head_dim']\n",
    "    )\n",
    "    \n",
    "    if mha_total is None:\n",
    "        mha_total = cache_size['total_gb']\n",
    "        relative = \"100%\"\n",
    "    else:\n",
    "        relative = f\"{cache_size['total_gb']/mha_total*100:.1f}%\"\n",
    "    \n",
    "    print(f\"{name:<20} {num_kv_heads:<12} {cache_size['per_layer_mb']:<15.2f} {cache_size['total_gb']:<15.2f} {relative:<12}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ è§€å¯Ÿ: MQA å°‡ KV Cache æ¸›å°‘è‡³ MHA çš„ 3%, GQA-8 æ¸›å°‘è‡³ 25%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "px8hs0hmo5h",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(attn_module, seq_len, num_tokens=100):\n",
    "    \"\"\"æ¸¬è©¦ç”Ÿæˆæ€§èƒ½\"\"\"\n",
    "    attn_module.eval()\n",
    "    \n",
    "    # æ¨¡æ“¬æ¨ç†\n",
    "    x = torch.randn(1, seq_len, 768, device=device, dtype=torch.float16)\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        # Prefill\n",
    "        start = time.time()\n",
    "        out, past_kv = attn_module(x, use_cache=True)\n",
    "        torch.cuda.synchronize()\n",
    "        prefill_time = time.time() - start\n",
    "        \n",
    "        # Decode\n",
    "        for _ in range(num_tokens):\n",
    "            new_token = torch.randn(1, 1, 768, device=device, dtype=torch.float16)\n",
    "            start = time.time()\n",
    "            out, past_kv = attn_module(new_token, past_kv=past_kv, use_cache=True)\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    return {\n",
    "        'prefill_time': prefill_time,\n",
    "        'avg_decode_time': np.mean(times),\n",
    "        'total_time': prefill_time + sum(times),\n",
    "        'peak_memory_gb': peak_mem\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MHA æ¸¬è©¦é€šé: torch.Size([2, 128, 768]) â†’ torch.Size([2, 128, 768])\n",
      "MHA æ¨ç†åŸºæº–æ¸¬è©¦...\n",
      "Prefill: 0.65ms\n",
      "Decode: 0.49ms/token\n",
      "è¨˜æ†¶é«”: 0.021GB\n"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦\n",
    "mha = MultiHeadAttention(768, 12).to(device).half()  # ä½¿ç”¨ fp16\n",
    "x = torch.randn(2, 128, 768, device=device, dtype=torch.float16)\n",
    "out = mha(x)\n",
    "print(f\"âœ… MHA æ¸¬è©¦é€šé: {x.shape} â†’ {out.shape}\")\n",
    "\n",
    "# æ¸¬è©¦\n",
    "print(\"MHA æ¨ç†åŸºæº–æ¸¬è©¦...\")\n",
    "result = benchmark_generation(mha, seq_len=128, num_tokens=50)\n",
    "print(f\"Prefill: {result['prefill_time']*1000:.2f}ms\")\n",
    "print(f\"Decode: {result['avg_decode_time']*1000:.2f}ms/token\")\n",
    "print(f\"è¨˜æ†¶é«”: {result['peak_memory_gb']:.3f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}