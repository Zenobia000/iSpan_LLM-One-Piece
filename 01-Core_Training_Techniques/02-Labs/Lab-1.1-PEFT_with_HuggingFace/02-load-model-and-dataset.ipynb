{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. 載入預訓練模型與資料集\n",
        "\n",
        "在本筆記本中，我們將學習如何從 Hugging Face Hub 載入一個預訓練的大型語言模型 (LLM) 及其對應的 Tokenizer。同時，我們也會載入一個用於後續微調任務的資料集。\n",
        "\n",
        "## 步驟 1: 匯入必要的函式庫\n",
        "\n",
        "首先，我們從 `transformers` 和 `torch` 匯入本次操作需要的模組。\n",
        "\n",
        "- `AutoTokenizer`: 一個方便的類別，可以自動根據模型名稱實例化對應的 Tokenizer。\n",
        "- `AutoModelForCausalLM`: 一個用於載入自回歸語言模型（如 Llama-2）的類別。\n",
        "- `BitsAndBytesConfig`: 用於設定模型量化，這是實現 QLoRa 的關鍵。\n",
        "- `torch`: PyTorch 函式庫。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 2: 設定模型與 Tokenizer\n",
        "\n",
        "我們將使用 Meta 開源的 `Llama-2-7b-chat-hf` 模型作為我們的基礎模型。\n",
        "\n",
        "為了在記憶體有限的環境（如單張消費級 GPU）中載入這個 70 億參數的模型，我們將採用 **4-bit 量化**技術。`BitsAndBytesConfig` 讓我們可以精確地設定量化細節：\n",
        "\n",
        "- `load_in_4bit=True`: 啟用 4-bit 量化。\n",
        "- `bnb_4bit_quant_type=\"nf4\"`: 使用 NF4 (NormalFloat 4) 量化類型，這是一種專為常態分佈權重設計的先進量化方法。\n",
        "- `bnb_4bit_compute_dtype=torch.bfloat16`: 在計算過程中（如矩陣乘法），使用 bfloat16 類型以保持精度和穩定性。\n",
        "\n",
        "**注意**: 第一次下載模型權重時，會需要一些時間。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Hugging Face 登入成功\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# 建議使用環境變數或 getpass 來管理您的 Hugging Face Token，避免將密鑰直接寫在程式碼中。\n",
        "# 方式一：從環境變數讀取 (推薦用於自動化環境)\n",
        "# hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "# 方式二：使用 getpass 在執行時手動輸入 (推薦用於互動式環境)\n",
        "# 如果 token 尚未定義，則提示使用者輸入\n",
        "if \"hf_token\" not in locals() or not hf_token:\n",
        "    try:\n",
        "        # 在 .ipynb 中，getpass 可能無法正常運作，這裡提供一個備用方案\n",
        "        hf_token = getpass.getpass(\"請貼上您的 Hugging Face Access Token: \")\n",
        "    except Exception:\n",
        "        print(\"無法使用 getpass。請手動在下方設定您的 token。\")\n",
        "        # 如果 getpass 失敗（例如在某些非互動式環境中），可以在此處手動貼上 token\n",
        "        hf_token = \"YOUR_HF_TOKEN_HERE\" \n",
        "\n",
        "# 登入 Hugging Face\n",
        "if hf_token and hf_token != \"YOUR_HF_TOKEN_HERE\":\n",
        "    login(token=hf_token)\n",
        "    print(\"✅ Hugging Face 登入成功\")\n",
        "else:\n",
        "    print(\"⚠️ 未提供 Hugging Face token。請執行此儲存格並手動輸入，或將 'YOUR_HF_TOKEN_HERE' 替換為您的 token。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e23754bcbda40a0baac61efc426d28f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dc6f002bb4747478cf17644f409201d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ac6d847aaa24b6bb43ac07f12099a49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c967d18f54c40aa9efafcc61569347a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07fc84e224e84a5a98bd2ad1b5b7052a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb53a58b0bb646a8bea056aef788b163",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f9e5eab00d141789d7b3d187df37ea6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fce9a9c966074314a5e723f8fe747a17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0c9343a43c846f3980ac831c6aee82d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0c0985ee6264277ae38c1ccc2283543",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1f870caf8cd492faf547ae7fd992e2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型與 Tokenizer 載入成功！\n",
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# 設定量化參數\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 載入 Tokenizer\n",
        "# 注意：你需要先到 Hugging Face 網站申請 Llama-2 的使用權限，並使用 `huggingface-cli login` 登入\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token # 設定 pad_token\n",
        "\n",
        "# 載入量化後的模型\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\", # 自動將模型分配到可用硬體\n",
        ")\n",
        "\n",
        "print(\"模型與 Tokenizer 載入成功！\")\n",
        "print(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 3: 載入資料集\n",
        "\n",
        "現在模型已經準備好，我們需要一個資料集來進行微調。這裡我們使用 `mlabonne/guanaco-llama2-1k`，這是一個已經格式化好的、適合 Llama-2 指令微調的小型資料集。\n",
        "\n",
        "我們使用 `datasets` 函式庫中的 `load_dataset` 來載入它。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "資料集載入成功！\n",
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "範例資料：\n",
            "{'text': '<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "print(\"資料集載入成功！\")\n",
        "print(dataset)\n",
        "print(\"範例資料：\")\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "模型和資料都已備妥！在下一個筆記本 `03-apply-lora.ipynb` 中，我們將正式開始進行 LoRA 微調。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM Engineering Course (Poetry)",
      "language": "python",
      "name": "llm-engineering-course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
