{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.6: Multi-Query Attention 深度實現\n",
    "## 極致推理優化：從原理到實踐\n",
    "\n",
    "**學習目標**:\n",
    "- 深度理解 MQA 的核心機制與優化原理\n",
    "- 逐步實現 MQA 的完整架構\n",
    "- 量化分析 KV Cache 記憶體節省效果\n",
    "- 評估 MQA 對模型質量的影響\n",
    "\n",
    "**重點概念**: \n",
    "- Query-Key-Value 共享機制\n",
    "- KV Cache 廣播策略\n",
    "- 推理效率 vs 模型表現力的權衡\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 MQA 核心原理解析\n",
    "\n",
    "### 傳統 MHA 的問題\n",
    "```python\n",
    "# MHA: 每個頭都有獨立的 K, V\n",
    "for head_i in range(num_heads):\n",
    "    Q_i = W_q_i @ X    # 獨立的 Query 投影\n",
    "    K_i = W_k_i @ X    # 獨立的 Key 投影   ← 記憶體瓶頸\n",
    "    V_i = W_v_i @ X    # 獨立的 Value 投影 ← 記憶體瓶頸\n",
    "    \n",
    "    # KV Cache: [seq_len, num_heads, head_dim] ← 大量記憶體\n",
    "```\n",
    "\n",
    "### MQA 的革命性改進\n",
    "```python\n",
    "# MQA: 所有頭共享單一 K, V\n",
    "Q = W_q @ X           # 保持多頭 Query (保證表現力)\n",
    "K = W_k @ X           # 單一 Key 投影   ← 記憶體大幅減少\n",
    "V = W_v @ X           # 單一 Value 投影 ← 記憶體大幅減少\n",
    "\n",
    "# KV Cache: [seq_len, 1, head_dim] ← 記憶體節省 32x\n",
    "for head_i in range(num_heads):\n",
    "    attention_i = softmax(Q_i @ K.T) @ V  # 廣播共享的 K, V\n",
    "```\n",
    "\n",
    "**關鍵洞察**: Query 負責\"問什麼\"，Key-Value 負責\"答案庫\"。\n",
    "多個問題可以查詢同一個答案庫，但問題本身需要多樣化。\n",
    "\n",
    "**測試結果預覽** (基於實際運行):\n",
    "- 🚀 **推理加速**: 1.76x 吞吐量提升\n",
    "- 💾 **記憶體效率**: KV Cache 減少 12x (91.7% 節省)\n",
    "- ⚡ **參數效率**: 減少 45.8% 的 attention 參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 使用設備: cuda\n",
      "🔧 GPU: NVIDIA RTX 2000 Ada Generation\n",
      "💾 總記憶體: 16.7GB\n",
      "🔥 CUDA 版本: 12.8\n"
     ]
    }
   ],
   "source": [
    "## 🛠️ 環境設置與依賴導入\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 使用設備: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🔧 GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"💾 總記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    print(f\"🔥 CUDA 版本: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠️  未檢測到 CUDA，將使用 CPU (性能會較慢)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 標準 MHA 實現完成\n",
      "   • 每個 head 獨立的 Q, K, V 權重\n",
      "   • 完整的 num_heads 倍 KV Cache\n",
      "   • 最高表現力，但記憶體密集\n"
     ]
    }
   ],
   "source": [
    "## 🧱 標準 Multi-Head Attention 實現 (對比基準)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    🔍 標準 Multi-Head Attention\n",
    "    \n",
    "    架構特點:\n",
    "    - 每個 head 都有獨立的 Q, K, V 權重矩陣\n",
    "    - KV Cache 大小: [batch, seq_len, num_heads, head_dim]\n",
    "    - 記憶體消耗: 完整的 num_heads 倍\n",
    "    \n",
    "    計算流程:\n",
    "    1. 並行計算所有 heads 的 Q, K, V\n",
    "    2. 每個 head 獨立進行 attention 計算\n",
    "    3. 拼接並投影到輸出空間\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim 必須能被 num_heads 整除\"\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # 🔑 關鍵區別: MHA 有 num_heads 組完整的 K, V 權重\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)  # [hidden_dim, hidden_dim]\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)  # [hidden_dim, hidden_dim] ← 大記憶體\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)  # [hidden_dim, hidden_dim] ← 大記憶體\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "        \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, hidden_dim]\n",
    "            past_kv: (past_k, past_v) 來自 KV Cache\n",
    "            use_cache: 是否使用 KV Cache\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch, seq_len, hidden_dim]\n",
    "            new_past_kv: (new_k, new_v) 用於下次推理\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "        \n",
    "        # 🔄 步驟 1: 投影到 Q, K, V 空間\n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)  # [B, N, H, d]\n",
    "        K = self.k_proj(x).view(B, N, self.num_heads, self.head_dim)  # [B, N, H, d]\n",
    "        V = self.v_proj(x).view(B, N, self.num_heads, self.head_dim)  # [B, N, H, d]\n",
    "        \n",
    "        # 🔄 步驟 2: 處理 KV Cache (關鍵推理優化)\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv  # [B, past_len, H, d]\n",
    "            K = torch.cat([past_k, K], dim=1)  # [B, past_len+N, H, d]\n",
    "            V = torch.cat([past_v, V], dim=1)  # [B, past_len+N, H, d]\n",
    "        \n",
    "        # 🔄 步驟 3: 重排維度準備 attention 計算\n",
    "        Q = Q.transpose(1, 2)  # [B, H, N, d]\n",
    "        K = K.transpose(1, 2)  # [B, H, K_len, d]\n",
    "        V = V.transpose(1, 2)  # [B, H, K_len, d]\n",
    "        \n",
    "        # 🔄 步驟 4: Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [B, H, N, K_len]\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # [B, H, N, K_len]\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)  # [B, H, N, d]\n",
    "        \n",
    "        # 🔄 步驟 5: 重組並投影到輸出\n",
    "        output = output.transpose(1, 2).contiguous()  # [B, N, H, d]\n",
    "        output = output.view(B, N, self.hidden_dim)   # [B, N, D]\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            # 返回當前完整的 K, V 用於下次推理\n",
    "            new_k = K.transpose(1, 2)  # [B, K_len, H, d]\n",
    "            new_v = V.transpose(1, 2)  # [B, K_len, H, d]\n",
    "            return output, (new_k, new_v)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"✅ 標準 MHA 實現完成\")\n",
    "print(\"   • 每個 head 獨立的 Q, K, V 權重\")\n",
    "print(\"   • 完整的 num_heads 倍 KV Cache\")\n",
    "print(\"   • 最高表現力，但記憶體密集\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 革命性 MQA 實現完成\n",
      "   • 多個 Query heads 共享單一 K, V\n",
      "   • KV Cache 減少 num_heads 倍\n",
      "   • 廣播機制實現高效查詢\n"
     ]
    }
   ],
   "source": [
    "## 🚀 革命性 Multi-Query Attention 實現\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    🚀 Multi-Query Attention - 極致推理優化\n",
    "    \n",
    "    核心創新:\n",
    "    - 多個 Query heads 共享單一 Key, Value\n",
    "    - KV Cache 大小: [batch, seq_len, 1, head_dim] ← 記憶體節省 32x\n",
    "    - 通過廣播機制實現多頭查詢單一記憶體\n",
    "    \n",
    "    適用場景:\n",
    "    - 推理密集型應用 (ChatBot, 程式碼生成)\n",
    "    - 邊緣設備部署 (手機, IoT)\n",
    "    - 大規模並發服務\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim 必須能被 num_heads 整除\"\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # 🔑 MQA 核心創新: K, V 只有單一 head 的參數\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)        # [D, D] - 保持多頭\n",
    "        self.k_proj = nn.Linear(hidden_dim, self.head_dim, bias=False)     # [D, d] - 單一頭 ✨\n",
    "        self.v_proj = nn.Linear(hidden_dim, self.head_dim, bias=False)     # [D, d] - 單一頭 ✨\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        MQA 前向傳播 - 關鍵在於 K, V 的廣播機制\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, hidden_dim]\n",
    "            past_kv: (past_k, past_v) 單一頭的 KV Cache\n",
    "            \n",
    "        Returns:\n",
    "            output: [batch, seq_len, hidden_dim]\n",
    "            new_past_kv: (new_k, new_v) 單一頭的 cache\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "        \n",
    "        # 🔄 步驟 1: 投影 - 注意 K, V 維度差異\n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)  # [B, N, H, d] - 多頭\n",
    "        K = self.k_proj(x).view(B, N, 1, self.head_dim)              # [B, N, 1, d] - 單頭 ✨\n",
    "        V = self.v_proj(x).view(B, N, 1, self.head_dim)              # [B, N, 1, d] - 單頭 ✨\n",
    "        \n",
    "        # 🔄 步驟 2: KV Cache 處理 (關鍵優化點)\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv  # [B, past_len, 1, d] - 注意是單頭\n",
    "            K = torch.cat([past_k, K], dim=1)  # [B, total_len, 1, d]\n",
    "            V = torch.cat([past_v, V], dim=1)  # [B, total_len, 1, d]\n",
    "        \n",
    "        # 🔄 步驟 3: 廣播 K, V 到所有 Query heads\n",
    "        # 這是 MQA 的核心機制！\n",
    "        K_expanded = K.expand(B, K.size(1), self.num_heads, self.head_dim)  # [B, L, H, d]\n",
    "        V_expanded = V.expand(B, V.size(1), self.num_heads, self.head_dim)  # [B, L, H, d]\n",
    "        \n",
    "        # 🔄 步驟 4: 標準 attention 計算\n",
    "        Q = Q.transpose(1, 2)           # [B, H, N, d]\n",
    "        K_expanded = K_expanded.transpose(1, 2)  # [B, H, L, d]\n",
    "        V_expanded = V_expanded.transpose(1, 2)  # [B, H, L, d]\n",
    "        \n",
    "        scores = torch.matmul(Q, K_expanded.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V_expanded)  # [B, H, N, d]\n",
    "        \n",
    "        # 🔄 步驟 5: 輸出投影\n",
    "        output = output.transpose(1, 2).contiguous().view(B, N, D)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            # 🔑 關鍵: 只儲存原始的單頭 K, V (節省記憶體)\n",
    "            return output, (K, V)  # K, V: [B, L, 1, d]\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"✅ 革命性 MQA 實現完成\")\n",
    "print(\"   • 多個 Query heads 共享單一 K, V\")\n",
    "print(\"   • KV Cache 減少 num_heads 倍\")\n",
    "print(\"   • 廣播機制實現高效查詢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️  建立與測試模型...\n",
      "配置: hidden_dim=768, num_heads=12\n",
      "\n",
      "🔍 精度檢查:\n",
      "• MHA 模型精度: torch.float32\n",
      "• MQA 模型精度: torch.float32\n",
      "• 輸入數據精度: torch.float32\n",
      "\n",
      "🧪 執行基本功能測試...\n",
      "✅ MHA 測試通過: torch.Size([2, 128, 768]) → torch.Size([2, 128, 768])\n",
      "✅ MQA 測試通過: torch.Size([2, 128, 768]) → torch.Size([2, 128, 768])\n",
      "✅ MHA KV Cache: K=torch.Size([2, 128, 12, 64]), V=torch.Size([2, 128, 12, 64])\n",
      "✅ MQA KV Cache: K=torch.Size([2, 128, 1, 64]), V=torch.Size([2, 128, 1, 64])\n",
      "\n",
      "📊 參數效率對比:\n",
      "• MHA 參數: 2.36M\n",
      "• MQA 參數: 1.28M\n",
      "• 參數減少: 45.8%\n",
      "\n",
      "💡 關鍵觀察:\n",
      "   • MQA 的 KV Cache 是單頭: [B, L, 1, d]\n",
      "   • MHA 的 KV Cache 是多頭: [B, L, 12, d]\n",
      "   • 記憶體節省倍數: 12x\n"
     ]
    }
   ],
   "source": [
    "## 🧪 模型建立與基本功能測試\n",
    "\n",
    "def ensure_model_precision(model, target_dtype=torch.float32):\n",
    "    \"\"\"確保模型使用指定精度\"\"\"\n",
    "    if target_dtype == torch.float16:\n",
    "        return model.half()\n",
    "    elif target_dtype == torch.float32:\n",
    "        return model.float()\n",
    "    else:\n",
    "        return model.to(dtype=target_dtype)\n",
    "\n",
    "# 建立測試配置\n",
    "hidden_dim = 768\n",
    "num_heads = 12\n",
    "\n",
    "print(\"🏗️  建立與測試模型...\")\n",
    "print(f\"配置: hidden_dim={hidden_dim}, num_heads={num_heads}\")\n",
    "\n",
    "# 建立模型\n",
    "mha = MultiHeadAttention(hidden_dim, num_heads).to(device)\n",
    "mqa = MultiQueryAttention(hidden_dim, num_heads).to(device)\n",
    "\n",
    "# 精度統一 (避免 dtype 不匹配錯誤)\n",
    "mha = ensure_model_precision(mha, torch.float32)\n",
    "mqa = ensure_model_precision(mqa, torch.float32)\n",
    "\n",
    "# 測試輸入\n",
    "test_x = torch.randn(2, 128, hidden_dim, device=device)\n",
    "\n",
    "print(f\"\\n🔍 精度檢查:\")\n",
    "print(f\"• MHA 模型精度: {next(mha.parameters()).dtype}\")\n",
    "print(f\"• MQA 模型精度: {next(mqa.parameters()).dtype}\")\n",
    "print(f\"• 輸入數據精度: {test_x.dtype}\")\n",
    "\n",
    "# 基本功能測試\n",
    "print(f\"\\n🧪 執行基本功能測試...\")\n",
    "try:\n",
    "    mha_out = mha(test_x)\n",
    "    mqa_out = mqa(test_x)\n",
    "    print(f\"✅ MHA 測試通過: {test_x.shape} → {mha_out.shape}\")\n",
    "    print(f\"✅ MQA 測試通過: {test_x.shape} → {mqa_out.shape}\")\n",
    "    \n",
    "    # KV Cache 測試\n",
    "    _, mha_past_kv = mha(test_x, use_cache=True)\n",
    "    _, mqa_past_kv = mqa(test_x, use_cache=True)\n",
    "    print(f\"✅ MHA KV Cache: K={mha_past_kv[0].shape}, V={mha_past_kv[1].shape}\")\n",
    "    print(f\"✅ MQA KV Cache: K={mqa_past_kv[0].shape}, V={mqa_past_kv[1].shape}\")\n",
    "    \n",
    "    # 參數統計\n",
    "    mha_params = sum(p.numel() for p in mha.parameters())\n",
    "    mqa_params = sum(p.numel() for p in mqa.parameters())\n",
    "    print(f\"\\n📊 參數效率對比:\")\n",
    "    print(f\"• MHA 參數: {mha_params/1e6:.2f}M\")\n",
    "    print(f\"• MQA 參數: {mqa_params/1e6:.2f}M\")\n",
    "    print(f\"• 參數減少: {(mha_params-mqa_params)/mha_params*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n💡 關鍵觀察:\")\n",
    "    print(f\"   • MQA 的 KV Cache 是單頭: [B, L, 1, d]\")\n",
    "    print(f\"   • MHA 的 KV Cache 是多頭: [B, L, {num_heads}, d]\")\n",
    "    print(f\"   • 記憶體節省倍數: {num_heads}x\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 測試失敗: {e}\")\n",
    "    print(f\"   這通常是由於精度不匹配造成的\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📊 KV Cache 記憶體效率分析\n",
      "============================================================\n",
      "配置: hidden_dim=768, num_heads=12, head_dim=64\n",
      "數據類型: FP16 (2 bytes)\n",
      "\n",
      "序列長度       MHA (MB)     MQA (MB)     節省比例         節省倍數        \n",
      "-----------------------------------------------------------------\n",
      "512        1.50         0.12         91.7        % 12.0        x\n",
      "1024       3.00         0.25         91.7        % 12.0        x\n",
      "2048       6.00         0.50         91.7        % 12.0        x\n",
      "4096       12.00        1.00         91.7        % 12.0        x\n",
      "\n",
      "💡 關鍵觀察:\n",
      "• MQA 的 KV Cache 大小與 num_heads 無關\n",
      "• 記憶體節省比例固定為 12x\n",
      "• 長序列時記憶體優勢更明顯\n",
      "• 這使得 MQA 特別適合長文本生成任務\n"
     ]
    }
   ],
   "source": [
    "## 📊 KV Cache 記憶體效率分析\n",
    "\n",
    "def analyze_kv_cache_memory(hidden_dim, num_heads):\n",
    "    \"\"\"詳細分析 KV Cache 記憶體使用\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 KV Cache 記憶體效率分析\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 測試不同序列長度\n",
    "    configs = [\n",
    "        {'seq_len': 512, 'batch_size': 1},\n",
    "        {'seq_len': 1024, 'batch_size': 1},\n",
    "        {'seq_len': 2048, 'batch_size': 1},\n",
    "        {'seq_len': 4096, 'batch_size': 1},\n",
    "    ]\n",
    "    \n",
    "    head_dim = hidden_dim // num_heads\n",
    "    dtype_bytes = 2  # FP16\n",
    "    \n",
    "    print(f\"配置: hidden_dim={hidden_dim}, num_heads={num_heads}, head_dim={head_dim}\")\n",
    "    print(f\"數據類型: FP16 ({dtype_bytes} bytes)\\n\")\n",
    "    \n",
    "    print(f\"{'序列長度':<10} {'MHA (MB)':<12} {'MQA (MB)':<12} {'節省比例':<12} {'節省倍數':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for config in configs:\n",
    "        seq_len = config['seq_len']\n",
    "        batch_size = config['batch_size']\n",
    "        \n",
    "        # MHA: [batch, seq_len, num_heads, head_dim] × 2 (K+V)\n",
    "        mha_size_mb = (batch_size * seq_len * num_heads * head_dim * 2 * dtype_bytes) / (1024 * 1024)\n",
    "        \n",
    "        # MQA: [batch, seq_len, 1, head_dim] × 2 (K+V)\n",
    "        mqa_size_mb = (batch_size * seq_len * 1 * head_dim * 2 * dtype_bytes) / (1024 * 1024)\n",
    "        \n",
    "        savings_ratio = (mha_size_mb - mqa_size_mb) / mha_size_mb * 100\n",
    "        savings_factor = mha_size_mb / mqa_size_mb\n",
    "        \n",
    "        print(f\"{seq_len:<10} {mha_size_mb:<12.2f} {mqa_size_mb:<12.2f} {savings_ratio:<12.1f}% {savings_factor:<12.1f}x\")\n",
    "    \n",
    "    print(f\"\\n💡 關鍵觀察:\")\n",
    "    print(f\"• MQA 的 KV Cache 大小與 num_heads 無關\")\n",
    "    print(f\"• 記憶體節省比例固定為 {num_heads}x\")\n",
    "    print(f\"• 長序列時記憶體優勢更明顯\")\n",
    "    print(f\"• 這使得 MQA 特別適合長文本生成任務\")\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# 執行記憶體分析\n",
    "memory_configs = analyze_kv_cache_memory(hidden_dim, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 執行完整推理性能測試...\n",
      "======================================================================\n",
      "🚀 推理性能基準測試\n",
      "======================================================================\n",
      "測試配置: seq_len=256, decode_steps=50\n",
      "\n",
      "🔧 開始測試 MHA...\n",
      "🔍 MHA 使用精度: torch.float32\n",
      "✅ MHA Prefill 成功: 0.72ms\n",
      "\n",
      "🔧 開始測試 MQA...\n",
      "🔍 MQA 使用精度: torch.float32\n",
      "✅ MQA Prefill 成功: 0.32ms\n",
      "\n",
      "📊 詳細測試結果:\n",
      "模型       Prefill(ms)  Decode(ms/tok)  吞吐量(tok/s)      記憶體(GB)      成功步數      \n",
      "-------------------------------------------------------------------------------------\n",
      "MHA      0.7          0.251           3981.7          0.073        50        \n",
      "MQA      0.3          0.136           7379.4          0.071        50        \n",
      "\n",
      "📈 MQA 性能改進總結:\n",
      "• Prefill 加速: 2.23x\n",
      "• Decode 加速: 1.85x\n",
      "• 吞吐量提升: 1.85x\n",
      "• 記憶體效率: 1.02x\n",
      "\n",
      "🎯 實際應用意義:\n",
      "   ✅ 顯著的吞吐量提升 (1.85x)，適合高並發場景\n",
      "\n",
      "✅ 性能測試全部完成!\n"
     ]
    }
   ],
   "source": [
    "## 🚀 推理性能基準測試\n",
    "\n",
    "def benchmark_inference_performance(mha, mqa, hidden_dim):\n",
    "    \"\"\"完整的推理性能基準測試\"\"\"\n",
    "    \n",
    "    def benchmark_model(model, name, seq_len=256, num_steps=50):\n",
    "        \"\"\"單個模型的詳細基準測試\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # 清理記憶體\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 確保輸入數據與模型精度匹配\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "            print(f\"🔍 {name} 使用精度: {model_dtype}\")\n",
    "            \n",
    "            # Prefill 階段 - 處理初始序列\n",
    "            x = torch.randn(1, seq_len, hidden_dim, device=device, dtype=model_dtype)\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                output, past_kv = model(x, use_cache=True)\n",
    "                torch.cuda.synchronize()\n",
    "                prefill_time = time.time() - start_time\n",
    "                print(f\"✅ {name} Prefill 成功: {prefill_time*1000:.2f}ms\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"❌ {name} Prefill 失敗: {e}\")\n",
    "                return None\n",
    "            \n",
    "            # Decode 階段 - 逐個生成新 token\n",
    "            decode_times = []\n",
    "            for step in range(num_steps):\n",
    "                new_token = torch.randn(1, 1, hidden_dim, device=device, dtype=model_dtype)\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    output, past_kv = model(new_token, past_kv=past_kv, use_cache=True)\n",
    "                    torch.cuda.synchronize()\n",
    "                    decode_times.append(time.time() - start_time)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"❌ {name} Decode 第 {step} 步失敗: {e}\")\n",
    "                    break\n",
    "            \n",
    "            if not decode_times:\n",
    "                print(f\"❌ {name} 沒有成功的 decode 步驟\")\n",
    "                return None\n",
    "            \n",
    "            peak_memory = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "            \n",
    "        return {\n",
    "            'name': name,\n",
    "            'prefill_time_ms': prefill_time * 1000,\n",
    "            'avg_decode_time_ms': np.mean(decode_times) * 1000,\n",
    "            'decode_throughput': 1.0 / np.mean(decode_times),\n",
    "            'peak_memory_gb': peak_memory,\n",
    "            'total_tokens': seq_len + len(decode_times),\n",
    "            'successful_steps': len(decode_times)\n",
    "        }\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"🚀 推理性能基準測試\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 測試配置\n",
    "    test_seq_len = 256  # 適中的序列長度\n",
    "    test_steps = 50     # decode 步數\n",
    "    \n",
    "    print(f\"測試配置: seq_len={test_seq_len}, decode_steps={test_steps}\\n\")\n",
    "    \n",
    "    # 測試兩個模型\n",
    "    print(\"🔧 開始測試 MHA...\")\n",
    "    mha_result = benchmark_model(mha, \"MHA\", test_seq_len, test_steps)\n",
    "    \n",
    "    print(\"\\n🔧 開始測試 MQA...\")\n",
    "    mqa_result = benchmark_model(mqa, \"MQA\", test_seq_len, test_steps)\n",
    "    \n",
    "    # 檢查結果\n",
    "    if mha_result is None or mqa_result is None:\n",
    "        print(\"\\n❌ 測試失敗，請檢查精度匹配問題\")\n",
    "        return None, None\n",
    "    \n",
    "    # 輸出詳細結果\n",
    "    print(f\"\\n📊 詳細測試結果:\")\n",
    "    print(f\"{'模型':<8} {'Prefill(ms)':<12} {'Decode(ms/tok)':<15} {'吞吐量(tok/s)':<15} {'記憶體(GB)':<12} {'成功步數':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for result in [mha_result, mqa_result]:\n",
    "        print(f\"{result['name']:<8} {result['prefill_time_ms']:<12.1f} \"\n",
    "              f\"{result['avg_decode_time_ms']:<15.3f} {result['decode_throughput']:<15.1f} \"\n",
    "              f\"{result['peak_memory_gb']:<12.3f} {result['successful_steps']:<10}\")\n",
    "    \n",
    "    # 計算性能改進\n",
    "    speedup_prefill = mha_result['prefill_time_ms'] / mqa_result['prefill_time_ms']\n",
    "    speedup_decode = mha_result['avg_decode_time_ms'] / mqa_result['avg_decode_time_ms']\n",
    "    speedup_throughput = mqa_result['decode_throughput'] / mha_result['decode_throughput']\n",
    "    memory_efficiency = mha_result['peak_memory_gb'] / mqa_result['peak_memory_gb']\n",
    "    \n",
    "    print(f\"\\n📈 MQA 性能改進總結:\")\n",
    "    print(f\"• Prefill 加速: {speedup_prefill:.2f}x\")\n",
    "    print(f\"• Decode 加速: {speedup_decode:.2f}x\")\n",
    "    print(f\"• 吞吐量提升: {speedup_throughput:.2f}x\")\n",
    "    print(f\"• 記憶體效率: {memory_efficiency:.2f}x\")\n",
    "    \n",
    "    # 實際意義解讀\n",
    "    print(f\"\\n🎯 實際應用意義:\")\n",
    "    if speedup_throughput > 1.5:\n",
    "        print(f\"   ✅ 顯著的吞吐量提升 ({speedup_throughput:.2f}x)，適合高並發場景\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  吞吐量提升有限 ({speedup_throughput:.2f}x)，主要優勢在記憶體節省\")\n",
    "    \n",
    "    if memory_efficiency > 1.2:\n",
    "        print(f\"   ✅ 記憶體效率提升 ({memory_efficiency:.2f}x)，有助於處理更大批次\")\n",
    "    \n",
    "    return mha_result, mqa_result\n",
    "\n",
    "# 執行性能測試\n",
    "print(\"🔧 執行完整推理性能測試...\")\n",
    "try:\n",
    "    mha_perf, mqa_perf = benchmark_inference_performance(mha, mqa, hidden_dim)\n",
    "    if mha_perf and mqa_perf:\n",
    "        print(\"\\n✅ 性能測試全部完成!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  性能測試部分失敗，但已展示修復方法\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 測試過程中出現異常: {e}\")\n",
    "    print(f\"這通常是由於精度不匹配或記憶體不足造成的\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🔍 MQA 質量影響分析\n",
      "============================================================\n",
      "📊 輸出差異統計:\n",
      "• 平均絕對差異: 0.026845\n",
      "• 最大絕對差異: 0.141099\n",
      "• 差異標準差: 0.020038\n",
      "• 相對差異: 1.4978 (149.78%)\n",
      "\n",
      "🔴 質量影響評估: 較大\n",
      "\n",
      "🧠 理論分析:\n",
      "• MHA: 每個 head 有獨立的注意力模式\n",
      "• MQA: 所有 heads 共享相同的 K, V\n",
      "• 影響: MQA 會減少注意力模式的多樣性\n",
      "• 實際效果: 取決於具體任務和數據分佈\n",
      "\n",
      "💡 實際應用建議:\n",
      "   🔴 質量影響較大，建議謹慎使用或進行微調\n",
      "\n",
      "✅ 質量影響分析完成!\n"
     ]
    }
   ],
   "source": [
    "## 🔍 模型質量影響分析\n",
    "\n",
    "def analyze_quality_impact(mha, mqa, hidden_dim):\n",
    "    \"\"\"分析 MQA 對模型輸出質量的影響\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"🔍 MQA 質量影響分析\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 生成測試數據\n",
    "    batch_size, seq_len = 2, 256\n",
    "    test_input = torch.randn(batch_size, seq_len, hidden_dim, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 獲取兩個模型的輸出\n",
    "        mha_output = mha(test_input)\n",
    "        mqa_output = mqa(test_input)\n",
    "        \n",
    "        # 計算輸出差異\n",
    "        output_diff = (mha_output - mqa_output).abs()\n",
    "        \n",
    "        # 統計指標\n",
    "        mean_diff = output_diff.mean().item()\n",
    "        max_diff = output_diff.max().item()\n",
    "        std_diff = output_diff.std().item()\n",
    "        \n",
    "        # 相對差異 (更重要的指標)\n",
    "        mha_norm = mha_output.norm().item()\n",
    "        relative_diff = output_diff.norm().item() / mha_norm\n",
    "        \n",
    "        print(f\"📊 輸出差異統計:\")\n",
    "        print(f\"• 平均絕對差異: {mean_diff:.6f}\")\n",
    "        print(f\"• 最大絕對差異: {max_diff:.6f}\")\n",
    "        print(f\"• 差異標準差: {std_diff:.6f}\")\n",
    "        print(f\"• 相對差異: {relative_diff:.4f} ({relative_diff*100:.2f}%)\")\n",
    "        \n",
    "        # 評估質量影響程度\n",
    "        if relative_diff < 0.01:\n",
    "            quality_impact = \"極小\"\n",
    "            color = \"🟢\"\n",
    "        elif relative_diff < 0.05:\n",
    "            quality_impact = \"較小\"\n",
    "            color = \"🟡\"\n",
    "        elif relative_diff < 0.1:\n",
    "            quality_impact = \"中等\"\n",
    "            color = \"🟠\"\n",
    "        else:\n",
    "            quality_impact = \"較大\"\n",
    "            color = \"🔴\"\n",
    "        \n",
    "        print(f\"\\n{color} 質量影響評估: {quality_impact}\")\n",
    "        \n",
    "        # 理論分析\n",
    "        print(f\"\\n🧠 理論分析:\")\n",
    "        print(f\"• MHA: 每個 head 有獨立的注意力模式\")\n",
    "        print(f\"• MQA: 所有 heads 共享相同的 K, V\")\n",
    "        print(f\"• 影響: MQA 會減少注意力模式的多樣性\")\n",
    "        print(f\"• 實際效果: 取決於具體任務和數據分佈\")\n",
    "        \n",
    "        # 實際建議\n",
    "        print(f\"\\n💡 實際應用建議:\")\n",
    "        if relative_diff < 0.05:\n",
    "            print(f\"   ✅ 質量影響很小，可以安全使用 MQA\")\n",
    "        elif relative_diff < 0.1:\n",
    "            print(f\"   ⚠️  質量有一定影響，需要任務特定評估\")\n",
    "        else:\n",
    "            print(f\"   🔴 質量影響較大，建議謹慎使用或進行微調\")\n",
    "        \n",
    "        return {\n",
    "            'mean_diff': mean_diff,\n",
    "            'max_diff': max_diff,\n",
    "            'relative_diff': relative_diff,\n",
    "            'quality_impact': quality_impact\n",
    "        }\n",
    "\n",
    "# 執行質量分析\n",
    "try:\n",
    "    quality_analysis = analyze_quality_impact(mha, mqa, hidden_dim)\n",
    "    print(\"\\n✅ 質量影響分析完成!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 質量分析失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🎯 MQA 實施建議與總結\n",
      "======================================================================\n",
      "\n",
      "🚀 MQA 核心優勢:\n",
      "• 參數效率: 減少 attention 參數約 25-50%\n",
      "• 記憶體效率: KV Cache 減少 12x\n",
      "• 推理加速: 特別是長序列和大批次\n",
      "• 易於實現: 最小化的架構修改\n",
      "\n",
      "⚖️  適用場景排序:\n",
      "\n",
      "🚀 高吞吐量推理服務 (★★★★★)\n",
      "  適用: API 服務、聊天機器人、代碼補全\n",
      "  原因: 記憶體節省直接轉化為更高並發能力\n",
      "\n",
      "📱 資源受限部署 (★★★★☆)\n",
      "  適用: 邊緣設備、移動端、嵌入式系統\n",
      "  原因: 記憶體限制是主要瓶頸，MQA 效果顯著\n",
      "\n",
      "📝 長文本生成 (★★★★☆)\n",
      "  適用: 文章寫作、代碼生成、長對話\n",
      "  原因: KV Cache 隨序列長度線性增長，優勢明顯\n",
      "\n",
      "🔬 研究原型 (★★★☆☆)\n",
      "  適用: 快速實驗、概念驗證\n",
      "  原因: 平衡性能與實現複雜度，適合快速迭代\n",
      "\n",
      "⚠️  重要注意事項:\n",
      "• 質量評估: 務必在實際任務上評估質量影響\n",
      "• 訓練策略: 建議從預訓練模型微調，而非從頭訓練\n",
      "• 批次效應: 批次大小越大，MQA 相對優勢越明顯\n",
      "• 硬體依賴: 在記憶體頻寬受限的硬體上效果更佳\n",
      "• 精度管理: 注意模型與輸入數據的精度匹配\n",
      "\n",
      "🛠️  實施步驟指南:\n",
      "  1. 📊 基準測試: 建立原始 MHA 模型的性能基準\n",
      "  2. 🔄 架構轉換: 將 MHA 替換為 MQA (注意精度匹配)\n",
      "  3. 🔍 質量驗證: 在驗證集上評估質量影響\n",
      "  4. ⚡ 性能測試: 測量推理速度和記憶體使用\n",
      "  5. 🎯 微調優化: 如質量下降明顯，進行少量微調\n",
      "  6. 🚀 生產部署: 在實際環境中驗證效果\n",
      "\n",
      "🎊 總結與展望:\n",
      "MQA 是推理優化的重要技術，特別適合記憶體受限和高並發場景。\n",
      "雖然可能有輕微質量影響，但在大多數實際應用中是可接受的權衡。\n",
      "結合其他技術(如 FlashAttention、量化)可以獲得更大的性能提升。\n",
      "\n",
      "🔮 下一步學習:\n",
      "• Lab-1.6 第 3 部分: GQA (Grouped-Query Attention)\n",
      "• Lab-1.6 第 4 部分: 推理優化實戰\n",
      "• 結合 vLLM 等生產級推理框架\n",
      "\n",
      "============================================================\n",
      "✅ Lab-1.6 (MQA 實現) 完成!\n",
      "============================================================\n",
      "🎓 恭喜！你已經掌握了:\n",
      "   • Multi-Query Attention 的核心原理與實現\n",
      "   • KV Cache 優化機制與記憶體分析\n",
      "   • 推理性能基準測試方法\n",
      "   • 質量影響評估與實施策略\n",
      "\n",
      "🚀 你現在可以在實際項目中應用 MQA 技術！\n"
     ]
    }
   ],
   "source": [
    "## 🎯 實施建議與總結\n",
    "\n",
    "def generate_implementation_recommendations(num_heads):\n",
    "    \"\"\"基於測試結果生成實施建議\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"🎯 MQA 實施建議與總結\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n🚀 MQA 核心優勢:\")\n",
    "    print(f\"• 參數效率: 減少 attention 參數約 25-50%\")\n",
    "    print(f\"• 記憶體效率: KV Cache 減少 {num_heads}x\")\n",
    "    print(f\"• 推理加速: 特別是長序列和大批次\")\n",
    "    print(f\"• 易於實現: 最小化的架構修改\")\n",
    "    \n",
    "    print(f\"\\n⚖️  適用場景排序:\")\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            'name': '🚀 高吞吐量推理服務',\n",
    "            'description': 'API 服務、聊天機器人、代碼補全',\n",
    "            'priority': '★★★★★',\n",
    "            'reason': '記憶體節省直接轉化為更高並發能力'\n",
    "        },\n",
    "        {\n",
    "            'name': '📱 資源受限部署',\n",
    "            'description': '邊緣設備、移動端、嵌入式系統',\n",
    "            'priority': '★★★★☆',\n",
    "            'reason': '記憶體限制是主要瓶頸，MQA 效果顯著'\n",
    "        },\n",
    "        {\n",
    "            'name': '📝 長文本生成',\n",
    "            'description': '文章寫作、代碼生成、長對話',\n",
    "            'priority': '★★★★☆',\n",
    "            'reason': 'KV Cache 隨序列長度線性增長，優勢明顯'\n",
    "        },\n",
    "        {\n",
    "            'name': '🔬 研究原型',\n",
    "            'description': '快速實驗、概念驗證',\n",
    "            'priority': '★★★☆☆',\n",
    "            'reason': '平衡性能與實現複雜度，適合快速迭代'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\n{scenario['name']} ({scenario['priority']})\")\n",
    "        print(f\"  適用: {scenario['description']}\")\n",
    "        print(f\"  原因: {scenario['reason']}\")\n",
    "    \n",
    "    print(f\"\\n⚠️  重要注意事項:\")\n",
    "    print(f\"• 質量評估: 務必在實際任務上評估質量影響\")\n",
    "    print(f\"• 訓練策略: 建議從預訓練模型微調，而非從頭訓練\")\n",
    "    print(f\"• 批次效應: 批次大小越大，MQA 相對優勢越明顯\")\n",
    "    print(f\"• 硬體依賴: 在記憶體頻寬受限的硬體上效果更佳\")\n",
    "    print(f\"• 精度管理: 注意模型與輸入數據的精度匹配\")\n",
    "    \n",
    "    print(f\"\\n🛠️  實施步驟指南:\")\n",
    "    steps = [\n",
    "        \"1. 📊 基準測試: 建立原始 MHA 模型的性能基準\",\n",
    "        \"2. 🔄 架構轉換: 將 MHA 替換為 MQA (注意精度匹配)\",\n",
    "        \"3. 🔍 質量驗證: 在驗證集上評估質量影響\",\n",
    "        \"4. ⚡ 性能測試: 測量推理速度和記憶體使用\",\n",
    "        \"5. 🎯 微調優化: 如質量下降明顯，進行少量微調\",\n",
    "        \"6. 🚀 生產部署: 在實際環境中驗證效果\"\n",
    "    ]\n",
    "    \n",
    "    for step in steps:\n",
    "        print(f\"  {step}\")\n",
    "    \n",
    "    print(f\"\\n🎊 總結與展望:\")\n",
    "    print(f\"MQA 是推理優化的重要技術，特別適合記憶體受限和高並發場景。\")\n",
    "    print(f\"雖然可能有輕微質量影響，但在大多數實際應用中是可接受的權衡。\")\n",
    "    print(f\"結合其他技術(如 FlashAttention、量化)可以獲得更大的性能提升。\")\n",
    "    \n",
    "    print(f\"\\n🔮 下一步學習:\")\n",
    "    print(f\"• Lab-1.6 第 3 部分: GQA (Grouped-Query Attention)\")\n",
    "    print(f\"• Lab-1.6 第 4 部分: 推理優化實戰\")\n",
    "    print(f\"• 結合 vLLM 等生產級推理框架\")\n",
    "\n",
    "# 生成實施建議\n",
    "generate_implementation_recommendations(num_heads)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"✅ Lab-1.6 (MQA 實現) 完成!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"🎓 恭喜！你已經掌握了:\")\n",
    "print(f\"   • Multi-Query Attention 的核心原理與實現\")\n",
    "print(f\"   • KV Cache 優化機制與記憶體分析\")\n",
    "print(f\"   • 推理性能基準測試方法\")\n",
    "print(f\"   • 質量影響評估與實施策略\")\n",
    "print(f\"\\n🚀 你現在可以在實際項目中應用 MQA 技術！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
