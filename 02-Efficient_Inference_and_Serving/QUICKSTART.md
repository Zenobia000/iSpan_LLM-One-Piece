# Chapter 2: Efficient Inference & Serving - Quick Start Guide
## é›™è»Œé“æ¶æ§‹å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒæº–å‚™

```bash
# æ¿€æ´» Poetry ç’°å¢ƒ
cd 00-Course_Setup
source .venv/bin/activate

# å®‰è£ç¬¬äºŒç« ä¾è³´
poetry install --all-extras

# å¯é¸: Flash Attention
pip install flash-attn --no-build-isolation

# Triton Server (éœ€è¦ Docker)
docker pull nvcr.io/nvidia/tritonserver:24.08-py3
```

### 2. é¸æ“‡å­¸ç¿’è·¯å¾‘

#### ğŸ¯ åŸºç¤è·¯å¾‘ (vLLM Track) - æ¨è–¦æ–°æ‰‹
é©åˆå¿«é€ŸåŸå‹é–‹ç™¼å’Œå€‹äººé …ç›®

```bash
# 1. ç†è«–åŸºç¤
cat 01-Theory/2.1-Inference_Engines.md
cat 01-Theory/2.2-Serving_and_Optimization.md

# 2. vLLM è»Œé“ (ä¾åºåŸ·è¡Œ)
cd 02-Labs/vLLM_Track/Lab-2.1-vLLM_Deployment
cd 02-Labs/vLLM_Track/Lab-2.2-Inference_Optimization
cd 02-Labs/vLLM_Track/Lab-2.3-FastAPI_Service
cd 02-Labs/vLLM_Track/Lab-2.4-Production_Deployment
cd 02-Labs/vLLM_Track/Lab-2.5-Performance_Monitoring
```

#### ğŸ¢ ä¼æ¥­è·¯å¾‘ (Triton Track) - æ¨è–¦æœ‰ç¶“é©—è€…
é©åˆä¼æ¥­ç´šå¹³å°é–‹ç™¼å’Œ MLOps å·¥ç¨‹å¸«

```bash
# 1. ç†è«–åŸºç¤ (é‡é»ä¼æ¥­ç´šæ¶æ§‹)
cat 01-Theory/2.2-Serving_and_Optimization.md

# 2. Triton è»Œé“ (ä¾åºåŸ·è¡Œ)
cd 02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics
cd 02-Labs/Triton_Track/Lab-2.2-Multi_Model_Management
cd 02-Labs/Triton_Track/Lab-2.3-Backend_Integration
cd 02-Labs/Triton_Track/Lab-2.4-Enterprise_Features
cd 02-Labs/Triton_Track/Lab-2.5-Production_Operations
```

#### ğŸš€ å®Œæ•´è·¯å¾‘ (Both Tracks) - æ¨è–¦å°ˆæ¥­é€²éš
é©åˆ AI Infrastructure Engineer å’ŒæŠ€è¡“ä¸»ç®¡

```bash
# Phase 1: vLLM åŸºç¤ (4é€±)
# Phase 2: Triton ä¼æ¥­ç´š (4é€±)
# Phase 3: æ•´åˆèˆ‡å°æ¯” (2é€±)
```

### 3. æ ¸å¿ƒæŠ€èƒ½æª¢æŸ¥è¡¨

#### vLLM Track æŠ€èƒ½
- âœ… éƒ¨ç½²ä¸¦å„ªåŒ– vLLM æ¨ç†å¼•æ“
- âœ… ç†è§£ PagedAttention åŸç†
- âœ… å¯¦ç¾ Speculative Decoding (1.5-3x åŠ é€Ÿ)
- âœ… æ‡‰ç”¨é‡åŒ–æŠ€è¡“ (INT8/INT4)
- âœ… æ§‹å»º FastAPI æœå‹™
- âœ… é›†æˆ Prometheus ç›£æ§
- âœ… ä½¿ç”¨ Docker éƒ¨ç½²

#### Triton Track æŠ€èƒ½
- âœ… éƒ¨ç½²å’Œé…ç½® Triton Inference Server
- âœ… è¨­è¨ˆå¤šæ¨¡å‹å€‰åº«æ¶æ§‹
- âœ… å¯¦ç¾ A/B æ¸¬è©¦å’Œç‰ˆæœ¬æ§åˆ¶
- âœ… æ•´åˆå¤šç¨® Backend (PyTorch/TensorRT/vLLM)
- âœ… é–‹ç™¼æ¨¡å‹çµ„åˆ (Ensemble)
- âœ… å¯¦æ–½ä¼æ¥­ç´šç›£æ§å’Œé‹ç¶­
- âœ… ä½¿ç”¨ Kubernetes é€²è¡Œç”Ÿç”¢éƒ¨ç½²

### 4. å¸¸è¦‹å•é¡Œ

**Q: GPU è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦?**
A: ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (å¦‚ `gpt2`, `opt-125m`) æˆ–é™ä½ `gpu_memory_utilization`

**Q: vLLM å®‰è£å¤±æ•—?**
A: ç¢ºèª CUDA ç‰ˆæœ¬åŒ¹é…ï¼Œå¯èƒ½éœ€è¦: `pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121`

**Q: Notebooks ç„¡æ³•åŸ·è¡Œ?**
A: ç¢ºèªå·²æ¿€æ´»æ­£ç¢ºçš„è™›æ“¬ç’°å¢ƒï¼Œä¸¦å®‰è£æ‰€æœ‰ä¾è³´

### 5. å¿«é€Ÿé©—è­‰

```python
# é©—è­‰ vLLM å®‰è£
from vllm import LLM, SamplingParams
llm = LLM(model="gpt2")
outputs = llm.generate(["Hello"], SamplingParams(max_tokens=10))
print(outputs[0].outputs[0].text)

# é©—è­‰ FastAPI
from fastapi import FastAPI
app = FastAPI()
@app.get("/")
def root():
    return {"status": "ok"}
```

---

### 6. è»Œé“é¸æ“‡å»ºè­°

| èƒŒæ™¯ | æ¨è–¦è»Œé“ | ç†ç”± |
|------|----------|------|
| åˆå­¸è€…/å€‹äººé …ç›® | vLLM Track | å­¸ç¿’æ›²ç·šå¹³ç·©ï¼Œå¿«é€Ÿä¸Šæ‰‹ |
| æœ‰MLç¶“é©— | å…ˆ vLLM å¾Œ Triton | å¾ªåºæ¼¸é€²ï¼ŒæŠ€èƒ½äº’è£œ |
| ä¼æ¥­ç’°å¢ƒ/MLOps | Triton Track | ç›´æ¥å°æ‡‰å·¥ä½œéœ€æ±‚ |
| æŠ€è¡“ä¸»ç®¡/æ¶æ§‹å¸« | Both Tracks | å®Œæ•´æŠ€è¡“è¦–é‡ |

---

**æœ€å¾Œæ›´æ–°**: 2025-10-16
**é©ç”¨ç‰ˆæœ¬**: Chapter 2 v4.0 (é›™è»Œé“æ¶æ§‹)
