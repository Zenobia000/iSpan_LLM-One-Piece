{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.2 Part 1: KV Cache Optimization\n",
    "\n",
    "## Objectives\n",
    "- Understand KV Cache structure and memory usage\n",
    "- Calculate cache size for different scenarios\n",
    "- Implement cache management strategies\n",
    "- Optimize for long conversations\n",
    "\n",
    "## Estimated Time: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. KV Cache Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding KV Cache\n",
    "\n",
    "In transformer models, the attention mechanism requires:\n",
    "- **Query (Q)**: Current token\n",
    "- **Key (K)**: All previous tokens\n",
    "- **Value (V)**: All previous tokens\n",
    "\n",
    "To avoid recomputing K and V for every token, we cache them - this is the **KV Cache**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KV Cache calculation function\n",
    "def calculate_kv_cache_size(\n",
    "    num_layers: int,\n",
    "    num_heads: int,\n",
    "    head_dim: int,\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "    precision: int = 2,  # FP16 = 2 bytes\n",
    ") -> Tuple[float, dict]:\n",
    "    \"\"\"\n",
    "    Calculate KV Cache size.\n",
    "    \n",
    "    Formula:\n",
    "    size = 2 (K+V) * batch * layers * heads * seq_len * head_dim * precision\n",
    "    \"\"\"\n",
    "    size_bytes = (\n",
    "        2  # K and V\n",
    "        * batch_size\n",
    "        * num_layers\n",
    "        * num_heads\n",
    "        * seq_len\n",
    "        * head_dim\n",
    "        * precision\n",
    "    )\n",
    "    \n",
    "    size_gb = size_bytes / (1024 ** 3)\n",
    "    \n",
    "    breakdown = {\n",
    "        'size_bytes': size_bytes,\n",
    "        'size_mb': size_bytes / (1024 ** 2),\n",
    "        'size_gb': size_gb,\n",
    "        'per_layer_mb': size_bytes / num_layers / (1024 ** 2),\n",
    "    }\n",
    "    \n",
    "    return size_gb, breakdown\n",
    "\n",
    "print(\"KV Cache calculation function defined ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Llama-2-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2-7B configuration\n",
    "llama2_7b_config = {\n",
    "    'num_layers': 32,\n",
    "    'num_heads': 32,\n",
    "    'head_dim': 128,\n",
    "}\n",
    "\n",
    "# Calculate for different scenarios\n",
    "scenarios = [\n",
    "    ('Single request, short', 1, 512),\n",
    "    ('Single request, long', 1, 2048),\n",
    "    ('Batch 8, medium', 8, 1024),\n",
    "    ('Batch 16, medium', 16, 1024),\n",
    "    ('Batch 32, short', 32, 512),\n",
    "]\n",
    "\n",
    "print(\"KV Cache Size Analysis (Llama-2-7B, FP16)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Scenario':<30} {'Batch':<8} {'Seq Len':<10} {'Cache Size'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "for name, batch_size, seq_len in scenarios:\n",
    "    size_gb, _ = calculate_kv_cache_size(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        **llama2_7b_config\n",
    "    )\n",
    "    results.append((name, batch_size, seq_len, size_gb))\n",
    "    print(f\"{name:<30} {batch_size:<8} {seq_len:<10} {size_gb:>6.2f} GB\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cache size scaling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scaling with sequence length\n",
    "seq_lens = [128, 256, 512, 1024, 2048, 4096]\n",
    "cache_sizes_seq = [\n",
    "    calculate_kv_cache_size(batch_size=1, seq_len=s, **llama2_7b_config)[0]\n",
    "    for s in seq_lens\n",
    "]\n",
    "\n",
    "ax1.plot(seq_lens, cache_sizes_seq, marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Sequence Length')\n",
    "ax1.set_ylabel('KV Cache Size (GB)')\n",
    "ax1.set_title('Cache Size vs Sequence Length\\n(Batch=1)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log', base=2)\n",
    "\n",
    "# Scaling with batch size\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64]\n",
    "cache_sizes_batch = [\n",
    "    calculate_kv_cache_size(batch_size=b, seq_len=1024, **llama2_7b_config)[0]\n",
    "    for b in batch_sizes\n",
    "]\n",
    "\n",
    "ax2.plot(batch_sizes, cache_sizes_batch, marker='s', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('KV Cache Size (GB)')\n",
    "ax2.set_title('Cache Size vs Batch Size\\n(Seq Len=1024)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Observations:\")\n",
    "print(\"- KV Cache size scales linearly with sequence length\")\n",
    "print(\"- KV Cache size scales linearly with batch size\")\n",
    "print(\"- Doubling either parameter doubles memory usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Memory Bottleneck Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze total memory usage\n",
    "def analyze_total_memory(\n",
    "    model_size_gb: float,\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "    model_config: dict,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Analyze total GPU memory usage.\n",
    "    \"\"\"\n",
    "    # Model weights\n",
    "    model_memory = model_size_gb\n",
    "    \n",
    "    # KV Cache\n",
    "    kv_cache_gb, _ = calculate_kv_cache_size(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    # Activations (rough estimate)\n",
    "    hidden_size = model_config['num_heads'] * model_config['head_dim']\n",
    "    activation_bytes = batch_size * seq_len * hidden_size * 2  # FP16\n",
    "    activation_gb = activation_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Total\n",
    "    total_gb = model_memory + kv_cache_gb + activation_gb\n",
    "    \n",
    "    return {\n",
    "        'model': model_memory,\n",
    "        'kv_cache': kv_cache_gb,\n",
    "        'activations': activation_gb,\n",
    "        'total': total_gb,\n",
    "    }\n",
    "\n",
    "# Llama-2-7B analysis\n",
    "model_size = 14.0  # GB (FP16)\n",
    "\n",
    "print(\"Memory Usage Analysis (Llama-2-7B)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_configs = [\n",
    "    (1, 512),\n",
    "    (8, 1024),\n",
    "    (16, 1024),\n",
    "    (32, 512),\n",
    "]\n",
    "\n",
    "for batch, seq_len in test_configs:\n",
    "    mem = analyze_total_memory(model_size, batch, seq_len, llama2_7b_config)\n",
    "    \n",
    "    print(f\"\\nBatch={batch}, Seq Len={seq_len}:\")\n",
    "    print(f\"  Model weights:  {mem['model']:6.2f} GB ({mem['model']/mem['total']*100:.1f}%)\")\n",
    "    print(f\"  KV Cache:       {mem['kv_cache']:6.2f} GB ({mem['kv_cache']/mem['total']*100:.1f}%)\")\n",
    "    print(f\"  Activations:    {mem['activations']:6.2f} GB ({mem['activations']/mem['total']*100:.1f}%)\")\n",
    "    print(f\"  Total:          {mem['total']:6.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n💡 Key Insight: KV Cache becomes dominant as batch/seq_len increases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cache Management Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Static vs Dynamic Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different allocation strategies\n",
    "class StaticCacheManager:\n",
    "    \"\"\"Pre-allocate maximum size for all requests.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len: int, batch_size: int):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.allocated_memory = self._calculate_allocation()\n",
    "        \n",
    "    def _calculate_allocation(self):\n",
    "        \"\"\"Allocate for max length.\"\"\"\n",
    "        return calculate_kv_cache_size(\n",
    "            batch_size=self.batch_size,\n",
    "            seq_len=self.max_seq_len,\n",
    "            **llama2_7b_config\n",
    "        )[0]\n",
    "    \n",
    "    def get_utilization(self, actual_seq_lens: list) -> float:\n",
    "        \"\"\"Calculate memory utilization.\"\"\"\n",
    "        used = sum(\n",
    "            calculate_kv_cache_size(\n",
    "                batch_size=1,\n",
    "                seq_len=s,\n",
    "                **llama2_7b_config\n",
    "            )[0]\n",
    "            for s in actual_seq_lens\n",
    "        )\n",
    "        return used / self.allocated_memory\n",
    "\n",
    "class DynamicCacheManager:\n",
    "    \"\"\"Allocate based on actual needs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.allocated_memory = 0\n",
    "        \n",
    "    def allocate(self, seq_len: int):\n",
    "        \"\"\"Allocate for specific sequence length.\"\"\"\n",
    "        cache_size = calculate_kv_cache_size(\n",
    "            batch_size=1,\n",
    "            seq_len=seq_len,\n",
    "            **llama2_7b_config\n",
    "        )[0]\n",
    "        self.allocated_memory += cache_size\n",
    "        return cache_size\n",
    "    \n",
    "    def get_total_allocation(self):\n",
    "        return self.allocated_memory\n",
    "\n",
    "# Compare strategies\n",
    "max_len = 2048\n",
    "actual_lens = [512, 256, 1024, 128, 768]  # Varied lengths\n",
    "\n",
    "static_mgr = StaticCacheManager(max_len, len(actual_lens))\n",
    "dynamic_mgr = DynamicCacheManager()\n",
    "\n",
    "for length in actual_lens:\n",
    "    dynamic_mgr.allocate(length)\n",
    "\n",
    "static_util = static_mgr.get_utilization(actual_lens)\n",
    "\n",
    "print(\"Cache Allocation Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Actual sequence lengths: {actual_lens}\")\n",
    "print(f\"\\nStatic Allocation:\")\n",
    "print(f\"  Allocated:    {static_mgr.allocated_memory:.2f} GB\")\n",
    "print(f\"  Utilization:  {static_util*100:.1f}%\")\n",
    "print(f\"  Wasted:       {(1-static_util)*100:.1f}%\")\n",
    "print(f\"\\nDynamic Allocation:\")\n",
    "print(f\"  Allocated:    {dynamic_mgr.get_total_allocation():.2f} GB\")\n",
    "print(f\"  Utilization:  100.0%\")\n",
    "print(f\"  Wasted:       0.0%\")\n",
    "print(f\"\\n💾 Memory Saved: {static_mgr.allocated_memory - dynamic_mgr.get_total_allocation():.2f} GB\")\n",
    "print(f\"📊 Efficiency Gain: {(1 - static_util)*100:.1f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PagedAttention (vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate PagedAttention block management\n",
    "class PagedCacheManager:\n",
    "    \"\"\"Simulate vLLM's PagedAttention.\"\"\"\n",
    "    \n",
    "    def __init__(self, block_size: int = 16, total_blocks: int = 1000):\n",
    "        self.block_size = block_size  # tokens per block\n",
    "        self.total_blocks = total_blocks\n",
    "        self.free_blocks = list(range(total_blocks))\n",
    "        self.allocations = {}  # {request_id: [block_ids]}\n",
    "        \n",
    "    def allocate(self, request_id: int, seq_len: int) -> list:\n",
    "        \"\"\"Allocate blocks for a request.\"\"\"\n",
    "        num_blocks_needed = (seq_len + self.block_size - 1) // self.block_size\n",
    "        \n",
    "        if len(self.free_blocks) < num_blocks_needed:\n",
    "            raise MemoryError(\"Not enough blocks available\")\n",
    "        \n",
    "        allocated = []\n",
    "        for _ in range(num_blocks_needed):\n",
    "            block = self.free_blocks.pop(0)\n",
    "            allocated.append(block)\n",
    "        \n",
    "        self.allocations[request_id] = allocated\n",
    "        return allocated\n",
    "    \n",
    "    def free(self, request_id: int):\n",
    "        \"\"\"Free blocks for a request.\"\"\"\n",
    "        if request_id in self.allocations:\n",
    "            blocks = self.allocations.pop(request_id)\n",
    "            self.free_blocks.extend(blocks)\n",
    "    \n",
    "    def get_utilization(self) -> float:\n",
    "        \"\"\"Get memory utilization.\"\"\"\n",
    "        used = self.total_blocks - len(self.free_blocks)\n",
    "        return used / self.total_blocks\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        return {\n",
    "            'total_blocks': self.total_blocks,\n",
    "            'free_blocks': len(self.free_blocks),\n",
    "            'used_blocks': self.total_blocks - len(self.free_blocks),\n",
    "            'utilization': self.get_utilization(),\n",
    "            'active_requests': len(self.allocations),\n",
    "        }\n",
    "\n",
    "# Test PagedAttention\n",
    "paged_mgr = PagedCacheManager(block_size=16, total_blocks=1000)\n",
    "\n",
    "print(\"PagedAttention Simulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate requests\n",
    "requests = [\n",
    "    (1, 512),\n",
    "    (2, 256),\n",
    "    (3, 1024),\n",
    "    (4, 128),\n",
    "]\n",
    "\n",
    "for req_id, seq_len in requests:\n",
    "    blocks = paged_mgr.allocate(req_id, seq_len)\n",
    "    print(f\"Request {req_id}: {seq_len} tokens → {len(blocks)} blocks\")\n",
    "\n",
    "stats = paged_mgr.get_stats()\n",
    "print(f\"\\nMemory Status:\")\n",
    "print(f\"  Used blocks:   {stats['used_blocks']}/{stats['total_blocks']}\")\n",
    "print(f\"  Utilization:   {stats['utilization']*100:.1f}%\")\n",
    "print(f\"  Active reqs:   {stats['active_requests']}\")\n",
    "\n",
    "# Free some requests\n",
    "print(f\"\\nFreeing requests 1 and 2...\")\n",
    "paged_mgr.free(1)\n",
    "paged_mgr.free(2)\n",
    "\n",
    "stats = paged_mgr.get_stats()\n",
    "print(f\"  Used blocks:   {stats['used_blocks']}/{stats['total_blocks']}\")\n",
    "print(f\"  Utilization:   {stats['utilization']*100:.1f}%\")\n",
    "print(f\"  Active reqs:   {stats['active_requests']}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. MQA and GQA Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Query Attention (MQA) vs Grouped-Query Attention (GQA)\n",
    "\n",
    "**Standard Multi-Head Attention (MHA)**:\n",
    "- Each head has its own K and V\n",
    "- Memory: num_heads × head_dim\n",
    "\n",
    "**Multi-Query Attention (MQA)**:\n",
    "- All heads share single K and V\n",
    "- Memory: 1 × head_dim\n",
    "- Reduction: num_heads × smaller\n",
    "\n",
    "**Grouped-Query Attention (GQA)**:\n",
    "- Groups of heads share K and V\n",
    "- Memory: num_groups × head_dim\n",
    "- Balance between MHA and MQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare KV Cache sizes for MHA, MQA, GQA\n",
    "def calculate_kv_cache_variants(\n",
    "    num_layers: int,\n",
    "    num_heads: int,\n",
    "    head_dim: int,\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "    num_kv_heads: int = None,  # For GQA\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate KV Cache for MHA, MQA, and GQA.\n",
    "    \"\"\"\n",
    "    precision = 2  # FP16\n",
    "    \n",
    "    # MHA (standard)\n",
    "    mha_size = (\n",
    "        2 * batch_size * num_layers * num_heads * seq_len * head_dim * precision\n",
    "    ) / (1024 ** 3)\n",
    "    \n",
    "    # MQA (single KV head)\n",
    "    mqa_size = (\n",
    "        2 * batch_size * num_layers * 1 * seq_len * head_dim * precision\n",
    "    ) / (1024 ** 3)\n",
    "    \n",
    "    # GQA (grouped KV heads)\n",
    "    if num_kv_heads is None:\n",
    "        num_kv_heads = num_heads // 4  # Default: 4 query heads per KV head\n",
    "    \n",
    "    gqa_size = (\n",
    "        2 * batch_size * num_layers * num_kv_heads * seq_len * head_dim * precision\n",
    "    ) / (1024 ** 3)\n",
    "    \n",
    "    return {\n",
    "        'MHA': mha_size,\n",
    "        'MQA': mqa_size,\n",
    "        'GQA': gqa_size,\n",
    "        'MQA_reduction': mha_size / mqa_size,\n",
    "        'GQA_reduction': mha_size / gqa_size,\n",
    "    }\n",
    "\n",
    "# Compare for Llama-2-7B config\n",
    "variants = calculate_kv_cache_variants(\n",
    "    num_layers=32,\n",
    "    num_heads=32,\n",
    "    head_dim=128,\n",
    "    batch_size=16,\n",
    "    seq_len=2048,\n",
    "    num_kv_heads=8,  # 4:1 ratio\n",
    ")\n",
    "\n",
    "print(\"Attention Variants Comparison (Batch=16, Seq=2048)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"MHA (Multi-Head):        {variants['MHA']:6.2f} GB\")\n",
    "print(f\"GQA (Grouped, 8 KV):     {variants['GQA']:6.2f} GB ({variants['GQA_reduction']:.1f}x reduction)\")\n",
    "print(f\"MQA (Multi-Query, 1 KV): {variants['MQA']:6.2f} GB ({variants['MQA_reduction']:.1f}x reduction)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods = ['MHA\\n(32 KV heads)', 'GQA\\n(8 KV heads)', 'MQA\\n(1 KV head)']\n",
    "sizes = [variants['MHA'], variants['GQA'], variants['MQA']]\n",
    "colors = ['#ff6b6b', '#ffd93d', '#51cf66']\n",
    "\n",
    "bars = ax.bar(methods, sizes, color=colors, width=0.6)\n",
    "ax.set_ylabel('KV Cache Size (GB)', fontsize=12)\n",
    "ax.set_title('KV Cache Size: MHA vs GQA vs MQA', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(sizes) * 1.2)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, size in zip(bars, sizes):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{size:.2f} GB',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 GQA provides excellent balance: ~4x memory reduction with minimal quality loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Long Conversation Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate long conversation scenario\n",
    "class ConversationCacheManager:\n",
    "    \"\"\"Manage KV Cache for long conversations.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_context: int = 4096):\n",
    "        self.max_context = max_context\n",
    "        self.messages = []\n",
    "        self.total_tokens = 0\n",
    "        \n",
    "    def add_turn(self, user_tokens: int, assistant_tokens: int):\n",
    "        \"\"\"Add a conversation turn.\"\"\"\n",
    "        self.messages.append({\n",
    "            'user': user_tokens,\n",
    "            'assistant': assistant_tokens,\n",
    "            'total': user_tokens + assistant_tokens,\n",
    "        })\n",
    "        self.total_tokens += user_tokens + assistant_tokens\n",
    "        \n",
    "        # Truncate if exceeds max context\n",
    "        while self.total_tokens > self.max_context and len(self.messages) > 1:\n",
    "            removed = self.messages.pop(0)\n",
    "            self.total_tokens -= removed['total']\n",
    "    \n",
    "    def get_cache_size(self) -> float:\n",
    "        \"\"\"Calculate current KV Cache size.\"\"\"\n",
    "        return calculate_kv_cache_size(\n",
    "            batch_size=1,\n",
    "            seq_len=self.total_tokens,\n",
    "            **llama2_7b_config\n",
    "        )[0]\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        return {\n",
    "            'num_turns': len(self.messages),\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'cache_size_gb': self.get_cache_size(),\n",
    "            'utilization': self.total_tokens / self.max_context,\n",
    "        }\n",
    "\n",
    "# Simulate conversation\n",
    "conv_mgr = ConversationCacheManager(max_context=4096)\n",
    "\n",
    "print(\"Long Conversation Simulation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate 10 turns\n",
    "conversation_turns = [\n",
    "    (50, 100),   # Short Q, medium A\n",
    "    (30, 150),   # Short Q, long A\n",
    "    (100, 200),  # Long Q, long A\n",
    "    (40, 80),\n",
    "    (60, 120),\n",
    "    (50, 100),\n",
    "    (70, 140),\n",
    "    (45, 90),\n",
    "    (80, 160),\n",
    "    (55, 110),\n",
    "]\n",
    "\n",
    "cache_sizes = []\n",
    "for i, (user, assistant) in enumerate(conversation_turns, 1):\n",
    "    conv_mgr.add_turn(user, assistant)\n",
    "    stats = conv_mgr.get_stats()\n",
    "    cache_sizes.append(stats['cache_size_gb'])\n",
    "    \n",
    "    print(f\"Turn {i:2d}: {stats['total_tokens']:4d} tokens, \"\n",
    "          f\"{stats['cache_size_gb']:.3f} GB cache, \"\n",
    "          f\"{stats['utilization']*100:.1f}% utilized\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFinal: {conv_mgr.get_stats()['num_turns']} turns active in memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cache growth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cache_sizes)+1), cache_sizes, marker='o', linewidth=2)\n",
    "plt.axhline(y=conv_mgr.get_cache_size(), color='r', linestyle='--', \n",
    "            label=f'Current: {conv_mgr.get_cache_size():.3f} GB')\n",
    "plt.xlabel('Conversation Turn')\n",
    "plt.ylabel('KV Cache Size (GB)')\n",
    "plt.title('KV Cache Growth During Long Conversation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Cache management is crucial for long conversations!\")\n",
    "print(\"   Strategies: truncation, summarization, or prefix caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ **Completed**:\n",
    "1. Calculated KV Cache sizes for various scenarios\n",
    "2. Analyzed memory bottlenecks\n",
    "3. Compared static vs dynamic allocation\n",
    "4. Simulated PagedAttention\n",
    "5. Evaluated MQA/GQA benefits\n",
    "6. Optimized for long conversations\n",
    "\n",
    "📊 **Key Findings**:\n",
    "- KV Cache scales linearly with batch and sequence length\n",
    "- Dynamic allocation (PagedAttention) saves 40-60% memory\n",
    "- GQA reduces cache by 4x with minimal quality loss\n",
    "- Long conversations require cache management strategies\n",
    "\n",
    "➡️ **Next**: In `02-Speculative_Decoding.ipynb`, we'll learn:\n",
    "- Speculative Decoding algorithm\n",
    "- Draft model selection\n",
    "- 1.5-3x speedup techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Lab 2.2 Part 1 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
