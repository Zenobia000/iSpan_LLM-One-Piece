{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.2: PyTorch DDP 分散式訓練基礎 - 01-Setup\n",
    "## 環境設置與基礎概念\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 注意事項\n",
    "\n",
    "本實驗室在**單GPU環境**中提供**概念演示**和**配置學習**。\n",
    "- ✅ **可學習**: DDP 原理、配置方法、代碼結構\n",
    "- ⚠️ **限制**: 無法展示真正的多GPU通訊和加速效果\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 學習目標\n",
    "\n",
    "1. 理解 PyTorch DDP 的基本概念\n",
    "2. 掌握分散式訓練的環境配置\n",
    "3. 學習進程初始化和通訊設置\n",
    "4. 準備訓練數據和模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU 數量: {torch.cuda.device_count()}\")\n",
    "print(f\"分散式訓練支援: {torch.distributed.is_available()}\")\n",
    "print(f\"NCCL 後端支援: {torch.distributed.is_nccl_available()}\")\n",
    "print(f\"Gloo 後端支援: {torch.distributed.is_gloo_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  記憶體: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DDP 核心概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP 關鍵概念演示\n",
    "print(\"=== PyTorch DDP 核心概念 ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. 基本術語:\")\n",
    "print(\"   - World Size: 總的進程數量 (通常等於GPU數量)\")\n",
    "print(\"   - Rank: 每個進程的唯一標識符 (0 到 world_size-1)\")\n",
    "print(\"   - Local Rank: 節點內的GPU編號\")\n",
    "print(\"   - Backend: 通訊後端 (NCCL for GPU, Gloo for CPU)\")\n",
    "print()\n",
    "\n",
    "print(\"2. DDP 工作流程:\")\n",
    "print(\"   Step 1: 初始化進程組 (init_process_group)\")\n",
    "print(\"   Step 2: 設置本地設備 (cuda.set_device)\")\n",
    "print(\"   Step 3: 包裝模型為DDP (DistributedDataParallel)\")\n",
    "print(\"   Step 4: 使用DistributedSampler分割數據\")\n",
    "print(\"   Step 5: 訓練 (自動梯度同步)\")\n",
    "print(\"   Step 6: 清理 (destroy_process_group)\")\n",
    "print()\n",
    "\n",
    "print(\"3. 關鍵優勢:\")\n",
    "print(\"   - 梯度自動同步 (All-Reduce)\")\n",
    "print(\"   - 參數一致性保證\")\n",
    "print(\"   - 高效的通訊優化\")\n",
    "print(\"   - 容錯和故障恢復\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 進程初始化函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ddp(rank, world_size, backend='nccl'):\n",
    "    \"\"\"\n",
    "    初始化分散式訓練環境\n",
    "    \n",
    "    Args:\n",
    "        rank: 當前進程編號\n",
    "        world_size: 總進程數\n",
    "        backend: 通訊後端 ('nccl' for GPU, 'gloo' for CPU)\n",
    "    \"\"\"\n",
    "    print(f\"[Rank {rank}] 初始化進程組...\")\n",
    "    \n",
    "    # 設置環境變數\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # 初始化進程組\n",
    "    dist.init_process_group(\n",
    "        backend=backend,\n",
    "        rank=rank,\n",
    "        world_size=world_size\n",
    "    )\n",
    "    \n",
    "    # 設置CUDA設備\n",
    "    if torch.cuda.is_available() and backend == 'nccl':\n",
    "        torch.cuda.set_device(rank)\n",
    "        print(f\"[Rank {rank}] 使用 GPU {rank}\")\n",
    "    else:\n",
    "        print(f\"[Rank {rank}] 使用 CPU\")\n",
    "    \n",
    "    print(f\"[Rank {rank}] 進程組初始化完成\")\n",
    "\n",
    "def cleanup_ddp():\n",
    "    \"\"\"\n",
    "    清理分散式訓練環境\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "    print(\"進程組已清理\")\n",
    "\n",
    "# 單GPU環境的模擬設置\n",
    "def setup_single_gpu_demo():\n",
    "    \"\"\"\n",
    "    單GPU環境的DDP概念演示\n",
    "    \"\"\"\n",
    "    print(\"=== 單GPU環境 DDP 概念演示 ===\")\n",
    "    print(\"注意: 這是概念演示，不會啟動真正的多進程\")\n",
    "    print()\n",
    "    \n",
    "    # 模擬參數\n",
    "    world_size = 1  # 在多GPU環境中，這會是GPU數量\n",
    "    rank = 0        # 在多GPU環境中，每個進程有不同的rank\n",
    "    \n",
    "    print(f\"模擬配置:\")\n",
    "    print(f\"  World Size: {world_size}\")\n",
    "    print(f\"  Rank: {rank}\")\n",
    "    print(f\"  Backend: {'nccl' if torch.cuda.is_available() else 'gloo'}\")\n",
    "    print(f\"  Device: {'cuda:0' if torch.cuda.is_available() else 'cpu'}\")\n",
    "    \n",
    "    return world_size, rank\n",
    "\n",
    "world_size, rank = setup_single_gpu_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 示例模型定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    簡化的Transformer模型，用於DDP演示\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=10000, d_model=512, nhead=8, num_layers=6, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.embedding(input_ids) + self.pos_encoding(positions)\n",
    "        \n",
    "        # 處理attention mask\n",
    "        if attention_mask is not None:\n",
    "            # 轉換為transformer期望的格式\n",
    "            attention_mask = attention_mask.bool()\n",
    "            attention_mask = ~attention_mask  # 反轉：True表示masked\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# 創建模型實例\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleTransformer(\n",
    "    vocab_size=8000,\n",
    "    d_model=256,  # 較小的模型以適應單GPU\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    max_seq_len=128\n",
    ").to(device)\n",
    "\n",
    "print(f\"模型已創建，設備: {device}\")\n",
    "print(f\"模型參數量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"可訓練參數量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 數據集準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "class DummyTextDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    模擬文本數據集，用於DDP訓練演示\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples=1000, seq_len=128, vocab_size=8000):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # 生成隨機數據（實際使用中會載入真實數據）\n",
    "        np.random.seed(42)  # 確保可重現\n",
    "        self.data = np.random.randint(1, vocab_size, (num_samples, seq_len))\n",
    "        \n",
    "        # 生成attention mask (隨機mask一些token)\n",
    "        self.attention_masks = np.ones((num_samples, seq_len))\n",
    "        for i in range(num_samples):\n",
    "            # 隨機選擇序列的實際長度\n",
    "            actual_len = np.random.randint(seq_len // 2, seq_len + 1)\n",
    "            self.attention_masks[i, actual_len:] = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.attention_masks[idx], dtype=torch.long)\n",
    "        \n",
    "        # 對於語言模型，labels通常是input_ids向右移動一位\n",
    "        labels = torch.cat([input_ids[1:], torch.tensor([0])], dim=0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def create_dataloader(dataset, batch_size=8, shuffle=True, num_workers=2, is_distributed=False, rank=0, world_size=1):\n",
    "    \"\"\"\n",
    "    創建DataLoader，支援分散式訓練\n",
    "    \"\"\"\n",
    "    if is_distributed:\n",
    "        # 分散式訓練使用DistributedSampler\n",
    "        sampler = DistributedSampler(\n",
    "            dataset,\n",
    "            num_replicas=world_size,\n",
    "            rank=rank,\n",
    "            shuffle=shuffle\n",
    "        )\n",
    "        dataloader = data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        print(f\"[Rank {rank}] 使用DistributedSampler\")\n",
    "    else:\n",
    "        # 標準訓練\n",
    "        dataloader = data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        print(\"使用標準DataLoader\")\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# 創建數據集\n",
    "train_dataset = DummyTextDataset(num_samples=2000, seq_len=128)\n",
    "val_dataset = DummyTextDataset(num_samples=400, seq_len=128)\n",
    "\n",
    "print(f\"訓練數據集大小: {len(train_dataset)}\")\n",
    "print(f\"驗證數據集大小: {len(val_dataset)}\")\n",
    "\n",
    "# 檢查數據格式\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\n數據格式檢查:\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    print(f\"    範例: {value[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DDP 模型包裝演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_model_for_ddp(model, device_id=None, find_unused_parameters=False):\n",
    "    \"\"\"\n",
    "    將模型包裝為DDP模型\n",
    "    \n",
    "    Args:\n",
    "        model: 要包裝的模型\n",
    "        device_id: GPU設備ID\n",
    "        find_unused_parameters: 是否查找未使用的參數\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() > 1 and dist.is_initialized():\n",
    "        # 真正的多GPU DDP包裝\n",
    "        ddp_model = DDP(\n",
    "            model,\n",
    "            device_ids=[device_id] if device_id is not None else None,\n",
    "            find_unused_parameters=find_unused_parameters\n",
    "        )\n",
    "        print(f\"模型已包裝為DDP，使用GPU {device_id}\")\n",
    "    else:\n",
    "        # 單GPU環境，使用原始模型\n",
    "        ddp_model = model\n",
    "        print(\"單GPU環境，使用原始模型（非DDP）\")\n",
    "    \n",
    "    return ddp_model\n",
    "\n",
    "# 演示DDP包裝\n",
    "print(\"=== DDP 模型包裝演示 ===\")\n",
    "print()\n",
    "\n",
    "# 在單GPU環境中的演示\n",
    "ddp_model = wrap_model_for_ddp(model, device_id=0 if torch.cuda.is_available() else None)\n",
    "\n",
    "print(f\"\\n原始模型類型: {type(model).__name__}\")\n",
    "print(f\"DDP模型類型: {type(ddp_model).__name__}\")\n",
    "\n",
    "# 顯示DDP的關鍵特性\n",
    "print(\"\\n=== DDP 關鍵特性 ===\")\n",
    "print(\"1. 參數同步: DDP會自動同步所有GPU上的模型參數\")\n",
    "print(\"2. 梯度聚合: 使用All-Reduce算法聚合梯度\")\n",
    "print(\"3. 廣播: 在訓練開始時廣播模型參數\")\n",
    "print(\"4. 錯誤檢測: 檢測未使用的參數和梯度異常\")\n",
    "\n",
    "if hasattr(ddp_model, 'module'):\n",
    "    print(\"\\n注意: 在DDP模型中，原始模型可通過 .module 屬性訪問\")\n",
    "    print(f\"ddp_model.module 類型: {type(ddp_model.module).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 訓練配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練配置\n",
    "config = {\n",
    "    # 模型配置\n",
    "    'model_name': 'SimpleTransformer',\n",
    "    'vocab_size': 8000,\n",
    "    'd_model': 256,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 4,\n",
    "    'max_seq_len': 128,\n",
    "    \n",
    "    # 訓練配置\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 5e-4,\n",
    "    'num_epochs': 3,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "    \n",
    "    # DDP配置\n",
    "    'backend': 'nccl' if torch.cuda.is_available() else 'gloo',\n",
    "    'find_unused_parameters': False,\n",
    "    'gradient_clipping': 1.0,\n",
    "    \n",
    "    # 數據配置\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # 日誌配置\n",
    "    'log_interval': 50,\n",
    "    'save_interval': 500,\n",
    "    'eval_interval': 200,\n",
    "}\n",
    "\n",
    "print(\"=== 訓練配置 ===\")\n",
    "for category in ['模型配置', '訓練配置', 'DDP配置', '數據配置', '日誌配置']:\n",
    "    print(f\"\\n{category}:\")\n",
    "    if category == '模型配置':\n",
    "        keys = ['model_name', 'vocab_size', 'd_model', 'nhead', 'num_layers', 'max_seq_len']\n",
    "    elif category == '訓練配置':\n",
    "        keys = ['batch_size', 'learning_rate', 'num_epochs', 'warmup_steps', 'weight_decay']\n",
    "    elif category == 'DDP配置':\n",
    "        keys = ['backend', 'find_unused_parameters', 'gradient_clipping']\n",
    "    elif category == '數據配置':\n",
    "        keys = ['num_workers', 'pin_memory']\n",
    "    else:\n",
    "        keys = ['log_interval', 'save_interval', 'eval_interval']\n",
    "    \n",
    "    for key in keys:\n",
    "        if key in config:\n",
    "            print(f\"  {key}: {config[key]}\")\n",
    "\n",
    "# 創建優化器\n",
    "optimizer = torch.optim.AdamW(\n",
    "    ddp_model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# 創建學習率調度器\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "# Warmup + Cosine調度\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    total_iters=config['warmup_steps']\n",
    ")\n",
    "\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['num_epochs'] * len(train_dataset) // config['batch_size'] - config['warmup_steps']\n",
    ")\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "    milestones=[config['warmup_steps']]\n",
    ")\n",
    "\n",
    "print(f\"\\n優化器: {type(optimizer).__name__}\")\n",
    "print(f\"調度器: Warmup + CosineAnnealing\")\n",
    "print(f\"初始學習率: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 多GPU訓練腳本範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建多GPU訓練腳本範例\n",
    "multi_gpu_script = '''\n",
    "#!/usr/bin/env python3\n",
    "# train_ddp.py - 多GPU DDP訓練腳本\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import argparse\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"初始化分散式訓練環境\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # 初始化進程組\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"清理分散式訓練環境\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train_ddp(rank, world_size, args):\n",
    "    \"\"\"DDP訓練主函數\"\"\"\n",
    "    print(f\"[Rank {rank}] 開始訓練\")\n",
    "    \n",
    "    # 設置分散式環境\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # 創建模型\n",
    "    model = YourModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # 創建數據集和DataLoader\n",
    "    dataset = YourDataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, sampler=sampler)\n",
    "    \n",
    "    # 訓練循環\n",
    "    for epoch in range(args.epochs):\n",
    "        sampler.set_epoch(epoch)  # 確保每個epoch的數據分割不同\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(rank), target.to(rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if rank == 0 and batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=32)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train_ddp, args=(world_size, args), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# 顯示腳本\n",
    "print(\"=== 多GPU DDP訓練腳本範例 ===\")\n",
    "print(\"以下是完整的多GPU DDP訓練腳本結構:\")\n",
    "print(multi_gpu_script)\n",
    "\n",
    "print(\"\\n=== 執行命令 ===\")\n",
    "print(\"# 使用 torchrun (推薦)\")\n",
    "print(\"torchrun --nproc_per_node=4 train_ddp.py --epochs 10 --batch-size 32\")\n",
    "print()\n",
    "print(\"# 使用 mp.spawn\")\n",
    "print(\"python train_ddp.py --epochs 10 --batch-size 32\")\n",
    "print()\n",
    "print(\"# 多節點訓練\")\n",
    "print(\"torchrun --nnodes=2 --nproc_per_node=4 --node_rank=0 \\\\\")\n",
    "print(\"         --master_addr=192.168.1.1 --master_port=29500 \\\\\")\n",
    "print(\"         train_ddp.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Lab-1.2 Setup 完成 ===\")\n",
    "print()\n",
    "print(\"✅ 已完成:\")\n",
    "print(\"  1. 環境檢查和依賴驗證\")\n",
    "print(\"  2. DDP核心概念理解\")\n",
    "print(\"  3. 進程初始化函數定義\")\n",
    "print(\"  4. 示例模型創建\")\n",
    "print(\"  5. 數據集和DataLoader準備\")\n",
    "print(\"  6. DDP模型包裝演示\")\n",
    "print(\"  7. 訓練配置設定\")\n",
    "print(\"  8. 多GPU訓練腳本範例\")\n",
    "print()\n",
    "print(\"📝 下一步:\")\n",
    "print(\"  - 02-Train.ipynb: 實際訓練過程\")\n",
    "print(\"  - 03-Optimization.ipynb: 通訊優化\")\n",
    "print(\"  - 04-Advanced.ipynb: 進階技術\")\n",
    "print()\n",
    "print(\"💡 重要提醒:\")\n",
    "print(\"  - 當前為單GPU環境演示\")\n",
    "print(\"  - 多GPU環境中將看到真正的加速效果\")\n",
    "print(\"  - 重點理解DDP的設計理念和配置方法\")\n",
    "\n",
    "# 保存配置供下一個notebook使用\n",
    "torch.save({\n",
    "    'config': config,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': 8000,\n",
    "    'device': str(device)\n",
    "}, 'ddp_setup.pth')\n",
    "\n",
    "print(\"\\n💾 配置已保存到 ddp_setup.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}