{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.8: ORPO 單階段對齊訓練\n",
    "\n",
    "**實驗目標**: 實現 ORPO 單階段對齊訓練\n",
    "\n",
    "ORPO (Odds Ratio Preference Optimization) 的核心創新在於單階段訓練：\n",
    "- **統一目標**: 同時優化 SFT 和偏好對齊\n",
    "- **無需參考模型**: 不像 DPO 需要參考模型\n",
    "- **記憶體高效**: 只需載入一個模型\n",
    "- **訓練簡化**: 單一損失函數，更容易調優\n",
    "\n",
    "## ORPO 損失函數\n",
    "\n",
    "```\n",
    "L_ORPO = L_SFT + λ × L_OR\n",
    "\n",
    "其中:\n",
    "- L_SFT: 標準語言模型損失 (在 chosen 上)\n",
    "- L_OR: Odds Ratio 偏好損失\n",
    "- λ: 平衡權重 (通常 0.1-1.0)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 1: 環境準備與數據載入\n",
    "\n",
    "載入之前準備的環境和數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_from_disk, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 設置隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('🚀 開始 ORPO 單階段對齊訓練')\n",
    "print(f'GPU 可用: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'當前 GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU 記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入數據集\n",
    "try:\n",
    "    orpo_dataset = load_from_disk('./orpo_data')\n",
    "    print(f'✅ 載入 ORPO 數據集，樣本數: {len(orpo_dataset)}')\n",
    "except:\n",
    "    print('⚠️  無法載入已保存的數據，創建模擬數據')\n",
    "    # 創建模擬 ORPO 數據\n",
    "    mock_data = [\n",
    "        {\n",
    "            'prompt': '請解釋什麼是機器學習?',\n",
    "            'chosen': '機器學習是人工智能的一個重要分支，它讓計算機能夠從數據中自動學習並改進性能，而無需明確編程。',\n",
    "            'rejected': '機器學習就是讓機器變聰明。'\n",
    "        },\n",
    "        {\n",
    "            'prompt': '如何開始學習程式設計?',\n",
    "            'chosen': '學習程式設計建議從基礎語法開始，選擇一門適合的語言如Python，多做練習項目，參與開源專案。',\n",
    "            'rejected': '學程式就是寫code。'\n",
    "        },\n",
    "        {\n",
    "            'prompt': '健康飲食的基本原則是什麼?',\n",
    "            'chosen': '健康飲食應遵循營養均衡原則，包括適當的碳水化合物、蛋白質、脂肪和維生素攝取。',\n",
    "            'rejected': '健康飲食就是少吃多動。'\n",
    "        }\n",
    "    ]\n",
    "    orpo_dataset = Dataset.from_list(mock_data)\n",
    "\n",
    "print(f'數據集大小: {len(orpo_dataset)}')\n",
    "print(f'數據欄位: {list(orpo_dataset.features.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 2: 模型準備\n",
    "\n",
    "載入基礎模型並配置 PEFT。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型配置\n",
    "MODEL_NAME = 'microsoft/DialoGPT-medium'\n",
    "\n",
    "print(f'📦 載入模型: {MODEL_NAME}')\n",
    "\n",
    "# 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 載入模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f'✅ 模型載入成功')\n",
    "print(f'模型參數量: {model.num_parameters():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置 LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['c_attn', 'c_proj', 'c_fc'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "# 準備模型進行 PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print('✅ PEFT 配置完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 3: ORPO 損失函數實現\n",
    "\n",
    "實現完整的 ORPO 損失函數和訓練邏輯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(model, input_ids, attention_mask, tokenizer):\n",
    "    \"\"\"計算序列的對數概率\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # 計算每個 token 的對數概率\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # 收集目標 token 的對數概率\n",
    "        target_log_probs = torch.gather(log_probs[:, :-1], dim=-1, \n",
    "                                       index=input_ids[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # 僅計算非 padding token 的平均對數概率\n",
    "        mask = (input_ids[:, 1:] != tokenizer.pad_token_id).float()\n",
    "        sequence_log_prob = (target_log_probs * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "    \n",
    "    return sequence_log_prob\n",
    "\n",
    "\n",
    "def compute_odds_ratio_loss(chosen_log_probs, rejected_log_probs):\n",
    "    \"\"\"計算 ORPO 的 Odds Ratio 損失\"\"\"\n",
    "    # 計算 log odds (使用近似)\n",
    "    chosen_log_odds = chosen_log_probs\n",
    "    rejected_log_odds = rejected_log_probs\n",
    "    \n",
    "    # Odds Ratio 損失\n",
    "    log_odds_ratio = chosen_log_odds - rejected_log_odds\n",
    "    loss = -F.logsigmoid(log_odds_ratio).mean()\n",
    "    \n",
    "    return loss, log_odds_ratio.mean()\n",
    "\n",
    "\n",
    "def orpo_loss_function(model, batch, tokenizer, lambda_or=0.5):\n",
    "    \"\"\"\n",
    "    ORPO 損失函數\n",
    "    \n",
    "    Args:\n",
    "        model: 訓練中的模型\n",
    "        batch: 包含 chosen_input_ids, rejected_input_ids 等\n",
    "        tokenizer: tokenizer\n",
    "        lambda_or: Odds Ratio 損失權重\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, metrics\n",
    "    \"\"\"\n",
    "    # 1. SFT 損失 (標準語言模型損失，僅在 chosen 上)\n",
    "    chosen_input_ids = batch['chosen_input_ids']\n",
    "    chosen_attention_mask = batch['chosen_attention_mask']\n",
    "    \n",
    "    outputs = model(input_ids=chosen_input_ids, attention_mask=chosen_attention_mask, \n",
    "                   labels=chosen_input_ids)\n",
    "    sft_loss = outputs.loss\n",
    "    \n",
    "    # 2. Odds Ratio 損失\n",
    "    chosen_log_probs = compute_log_probs(model, batch['chosen_input_ids'], \n",
    "                                        batch['chosen_attention_mask'], tokenizer)\n",
    "    \n",
    "    rejected_log_probs = compute_log_probs(model, batch['rejected_input_ids'],\n",
    "                                          batch['rejected_attention_mask'], tokenizer)\n",
    "    \n",
    "    or_loss, log_odds_ratio = compute_odds_ratio_loss(chosen_log_probs, rejected_log_probs)\n",
    "    \n",
    "    # 3. 總損失\n",
    "    total_loss = sft_loss + lambda_or * or_loss\n",
    "    \n",
    "    metrics = {\n",
    "        'total_loss': total_loss.item(),\n",
    "        'sft_loss': sft_loss.item(),\n",
    "        'or_loss': or_loss.item(),\n",
    "        'log_odds_ratio': log_odds_ratio.item()\n",
    "    }\n",
    "    \n",
    "    return total_loss, metrics\n",
    "\n",
    "\n",
    "print('✅ ORPO 損失函數實現完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 4: 數據預處理與 DataLoader\n",
    "\n",
    "準備 ORPO 訓練的數據載入器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_orpo_batch(examples, tokenizer, max_length=256):\n",
    "    \"\"\"準備 ORPO 訓練批次\"\"\"\n",
    "    batch_data = {\n",
    "        'chosen_input_ids': [],\n",
    "        'chosen_attention_mask': [],\n",
    "        'rejected_input_ids': [],\n",
    "        'rejected_attention_mask': []\n",
    "    }\n",
    "    \n",
    "    for example in examples:\n",
    "        # 格式化為對話格式\n",
    "        prompt = f\"Human: {example['prompt']}\\n\\nAssistant:\"\n",
    "        chosen_text = prompt + example['chosen']\n",
    "        rejected_text = prompt + example['rejected']\n",
    "        \n",
    "        # Tokenize chosen\n",
    "        chosen_tokens = tokenizer(\n",
    "            chosen_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize rejected\n",
    "        rejected_tokens = tokenizer(\n",
    "            rejected_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        batch_data['chosen_input_ids'].append(chosen_tokens['input_ids'].squeeze())\n",
    "        batch_data['chosen_attention_mask'].append(chosen_tokens['attention_mask'].squeeze())\n",
    "        batch_data['rejected_input_ids'].append(rejected_tokens['input_ids'].squeeze())\n",
    "        batch_data['rejected_attention_mask'].append(rejected_tokens['attention_mask'].squeeze())\n",
    "    \n",
    "    # 轉換為 tensor\n",
    "    for key in batch_data:\n",
    "        batch_data[key] = torch.stack(batch_data[key])\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "\n",
    "# 準備訓練數據\n",
    "def create_dataloader(dataset, batch_size=2):\n",
    "    \"\"\"創建 ORPO 數據載入器\"\"\"\n",
    "    def collate_fn(examples):\n",
    "        batch = prepare_orpo_batch(examples, tokenizer)\n",
    "        return batch\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# 創建訓練 DataLoader\n",
    "train_dataloader = create_dataloader(orpo_dataset, batch_size=1)\n",
    "\n",
    "print(f'✅ 數據載入器準備完成，批次數: {len(train_dataloader)}')\n",
    "\n",
    "# 測試一個批次\n",
    "test_batch = next(iter(train_dataloader))\n",
    "print(f'批次測試:')\n",
    "for key, value in test_batch.items():\n",
    "    print(f'  {key}: {value.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 5: ORPO 訓練配置\n",
    "\n",
    "設置 ORPO 訓練的參數和優化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO 訓練配置\n",
    "orpo_config = {\n",
    "    'epochs': 2,\n",
    "    'learning_rate': 5e-6,  # ORPO 通常使用較小的學習率\n",
    "    'lambda_or': 0.5,       # Odds Ratio 損失權重\n",
    "    'warmup_steps': 10,\n",
    "    'max_grad_norm': 1.0\n",
    "}\n",
    "\n",
    "# 優化器\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=orpo_config['learning_rate'],\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# 學習率調度器\n",
    "total_steps = len(train_dataloader) * orpo_config['epochs']\n",
    "warmup_steps = orpo_config['warmup_steps']\n",
    "\n",
    "def get_linear_schedule_with_warmup_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return current_step / warmup_steps\n",
    "    return max(0, (total_steps - current_step) / (total_steps - warmup_steps))\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "scheduler = LambdaLR(optimizer, get_linear_schedule_with_warmup_lambda)\n",
    "\n",
    "print('⚙️  ORPO 訓練配置:')\n",
    "for key, value in orpo_config.items():\n",
    "    print(f'  {key}: {value}')\n",
    "print(f'  total_steps: {total_steps}')\n",
    "print(f'  optimizer: AdamW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 6: ORPO 訓練循環\n",
    "\n",
    "執行 ORPO 單階段對齊訓練。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練指標記錄\n",
    "training_metrics = {\n",
    "    'total_loss': [],\n",
    "    'sft_loss': [],\n",
    "    'or_loss': [],\n",
    "    'log_odds_ratio': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "# 模型設為訓練模式\n",
    "model.train()\n",
    "\n",
    "print('🚀 開始 ORPO 訓練...')\n",
    "print(f'總計 {orpo_config[\"epochs\"]} 個 epoch，{total_steps} 個訓練步驟')\n",
    "\n",
    "# 訓練前的記憶體狀態\n",
    "if torch.cuda.is_available():\n",
    "    print(f'訓練前 GPU 記憶體: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(orpo_config['epochs']):\n",
    "        print(f'\\n=== Epoch {epoch + 1}/{orpo_config[\"epochs\"]} ===')\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=f'Epoch {epoch + 1}')):\n",
    "            # 移動批次到 GPU\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            \n",
    "            # 計算 ORPO 損失\n",
    "            total_loss, metrics = orpo_loss_function(\n",
    "                model, batch, tokenizer, lambda_or=orpo_config['lambda_or']\n",
    "            )\n",
    "            \n",
    "            # 反向傳播\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # 梯度裁切\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), orpo_config['max_grad_norm']\n",
    "            )\n",
    "            \n",
    "            # 優化器步驟\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # 記錄指標\n",
    "            training_metrics['total_loss'].append(metrics['total_loss'])\n",
    "            training_metrics['sft_loss'].append(metrics['sft_loss'])\n",
    "            training_metrics['or_loss'].append(metrics['or_loss'])\n",
    "            training_metrics['log_odds_ratio'].append(metrics['log_odds_ratio'])\n",
    "            training_metrics['learning_rate'].append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            epoch_losses.append(total_loss.item())\n",
    "            global_step += 1\n",
    "            \n",
    "            # 每幾步打印一次\n",
    "            if step % 2 == 0:\n",
    "                print(f'Step {step}: Loss={total_loss.item():.4f}, '\n",
    "                      f'SFT={metrics[\"sft_loss\"]:.4f}, '\n",
    "                      f'OR={metrics[\"or_loss\"]:.4f}, '\n",
    "                      f'Odds Ratio={metrics[\"log_odds_ratio\"]:.4f}')\n",
    "        \n",
    "        # Epoch 結束統計\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        print(f'Epoch {epoch + 1} 平均損失: {avg_loss:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'\\n✅ ORPO 訓練完成！')\n",
    "    print(f'總訓練時間: {training_time:.2f} 秒')\n",
    "    \n",
    "    # 訓練後的記憶體狀態\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'訓練後 GPU 記憶體: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'❌ 訓練過程中出錯: {e}')\n",
    "    print('嘗試減少批次大小或序列長度')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 7: 訓練指標分析\n",
    "\n",
    "分析 ORPO 訓練過程中的指標變化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化訓練指標\n",
    "if training_metrics['total_loss']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('ORPO 訓練指標', fontsize=16)\n",
    "    \n",
    "    steps = range(len(training_metrics['total_loss']))\n",
    "    \n",
    "    # 總損失\n",
    "    axes[0, 0].plot(steps, training_metrics['total_loss'], 'b-', alpha=0.7)\n",
    "    axes[0, 0].set_title('總損失 (Total Loss)')\n",
    "    axes[0, 0].set_xlabel('步驟')\n",
    "    axes[0, 0].set_ylabel('損失')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SFT vs OR 損失\n",
    "    axes[0, 1].plot(steps, training_metrics['sft_loss'], 'g-', label='SFT Loss', alpha=0.7)\n",
    "    axes[0, 1].plot(steps, training_metrics['or_loss'], 'r-', label='OR Loss', alpha=0.7)\n",
    "    axes[0, 1].set_title('SFT vs OR 損失')\n",
    "    axes[0, 1].set_xlabel('步驟')\n",
    "    axes[0, 1].set_ylabel('損失')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log Odds Ratio\n",
    "    axes[1, 0].plot(steps, training_metrics['log_odds_ratio'], 'purple', alpha=0.7)\n",
    "    axes[1, 0].set_title('Log Odds Ratio (Chosen vs Rejected)')\n",
    "    axes[1, 0].set_xlabel('步驟')\n",
    "    axes[1, 0].set_ylabel('Log Odds Ratio')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 學習率\n",
    "    axes[1, 1].plot(steps, training_metrics['learning_rate'], 'orange', alpha=0.7)\n",
    "    axes[1, 1].set_title('學習率調度')\n",
    "    axes[1, 1].set_xlabel('步驟')\n",
    "    axes[1, 1].set_ylabel('學習率')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印訓練統計\n",
    "    print('📊 訓練統計:')\n",
    "    print(f'最終總損失: {training_metrics[\"total_loss\"][-1]:.4f}')\n",
    "    print(f'最終 SFT 損失: {training_metrics[\"sft_loss\"][-1]:.4f}')\n",
    "    print(f'最終 OR 損失: {training_metrics[\"or_loss\"][-1]:.4f}')\n",
    "    print(f'最終 Log Odds Ratio: {training_metrics[\"log_odds_ratio\"][-1]:.4f}')\n",
    "    \n",
    "    # 分析趨勢\n",
    "    total_loss_trend = training_metrics['total_loss'][-1] - training_metrics['total_loss'][0]\n",
    "    odds_ratio_trend = training_metrics['log_odds_ratio'][-1] - training_metrics['log_odds_ratio'][0]\n",
    "    \n",
    "    print(f'\\n📈 趨勢分析:')\n",
    "    print(f'總損失變化: {total_loss_trend:+.4f} ({\"+\" if total_loss_trend < 0 else \"-\"})')\n",
    "    print(f'Odds Ratio 變化: {odds_ratio_trend:+.4f} ({\"+\" if odds_ratio_trend > 0 else \"-\"})')\n",
    "    \n",
    "else:\n",
    "    print('⚠️  無可用的訓練指標進行分析')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 8: 模型保存\n",
    "\n",
    "保存訓練完成的 ORPO 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 ORPO 模型\n",
    "output_dir = './orpo_model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print('💾 保存 ORPO 模型...')\n",
    "\n",
    "try:\n",
    "    # 保存 PEFT 模型\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # 保存訓練配置\n",
    "    import json\n",
    "    config_path = os.path.join(output_dir, 'orpo_config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(orpo_config, f, indent=2)\n",
    "    \n",
    "    # 保存訓練指標\n",
    "    metrics_path = os.path.join(output_dir, 'training_metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(training_metrics, f, indent=2)\n",
    "    \n",
    "    print(f'✅ ORPO 模型已保存至: {output_dir}')\n",
    "    \n",
    "    # 列出保存的檔案\n",
    "    saved_files = list(Path(output_dir).glob('*'))\n",
    "    print(f'保存的檔案: {[f.name for f in saved_files]}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'⚠️  模型保存過程中出現問題: {e}')\n",
    "    print('嘗試基本保存方式...')\n",
    "    \n",
    "    # 基本保存方式\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': orpo_config,\n",
    "        'metrics': training_metrics\n",
    "    }, os.path.join(output_dir, 'orpo_model_manual.pth'))\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f'✅ ORPO 模型手動保存至: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 9: ORPO 模型測試\n",
    "\n",
    "測試訓練完成的 ORPO 模型生成能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 ORPO 模型\n",
    "def test_orpo_model(model, tokenizer, prompt, max_length=200):\n",
    "    \"\"\"測試 ORPO 模型生成\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 準備輸入\n",
    "    formatted_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # 生成回應\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解碼輸出\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(formatted_prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "print('🧪 測試 ORPO 模型生成能力...')\n",
    "\n",
    "test_prompts = [\n",
    "    \"請解釋什麼是機器學習?\",\n",
    "    \"如何開始學習程式設計?\",\n",
    "    \"健康飲食的基本原則是什麼?\",\n",
    "    \"如何提高工作效率?\",\n",
    "    \"人工智能的發展前景如何?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f'\\n📝 測試 {i+1}:')\n",
    "    print(f'Prompt: {prompt}')\n",
    "    print('ORPO Generated:')\n",
    "    \n",
    "    try:\n",
    "        response = test_orpo_model(model, tokenizer, prompt)\n",
    "        print(f'{response}')\n",
    "    except Exception as e:\n",
    "        print(f'生成失敗: {e}')\n",
    "    \n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 10: ORPO 訓練總結\n",
    "\n",
    "總結 ORPO 單階段對齊訓練的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO 訓練總結\n",
    "print('=== ORPO 單階段對齊訓練總結 ===')\n",
    "print(f'✅ 基礎模型: {MODEL_NAME}')\n",
    "print(f'✅ PEFT 方法: LoRA (r={lora_config.r}, alpha={lora_config.lora_alpha})')\n",
    "print(f'✅ 訓練數據: {len(orpo_dataset)} 個偏好對')\n",
    "print(f'✅ 訓練輪數: {orpo_config[\"epochs\"]}')\n",
    "print(f'✅ Lambda OR: {orpo_config[\"lambda_or\"]}')\n",
    "print(f'✅ 學習率: {orpo_config[\"learning_rate\"]}')\n",
    "print(f'✅ ORPO 模型保存位置: {output_dir}')\n",
    "\n",
    "# 計算模型參數統計\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'✅ 總參數: {total_params:,}')\n",
    "print(f'✅ 可訓練參數: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)')\n",
    "\n",
    "if training_metrics['total_loss']:\n",
    "    print(f'\\n📊 訓練成果:')\n",
    "    print(f'• 損失改善: {training_metrics[\"total_loss\"][0]:.4f} → {training_metrics[\"total_loss\"][-1]:.4f}')\n",
    "    print(f'• Odds Ratio 提升: {training_metrics[\"log_odds_ratio\"][0]:.4f} → {training_metrics[\"log_odds_ratio\"][-1]:.4f}')\n",
    "\n",
    "print('\\n🎯 ORPO 核心優勢驗證:')\n",
    "print('✅ 單階段訓練: 無需 SFT + DPO 分階段')\n",
    "print('✅ 無需參考模型: 記憶體使用減半')\n",
    "print('✅ 統一損失函數: SFT + OR 同時優化')\n",
    "print('✅ 訓練穩定: 比 PPO 更穩定的優化過程')\n",
    "\n",
    "print('\\n🚀 下一步: 執行 03-Compare_with_DPO.ipynb 進行對比分析')\n",
    "\n",
    "# 清理 GPU 記憶體\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\n💾 GPU 記憶體已清理，當前使用: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "print('\\n🔬 ORPO 技術總結:')\n",
    "print('• ORPO 實現了真正的單階段對齊訓練')\n",
    "print('• 通過 Odds Ratio 直接優化偏好，無需獎勵模型')\n",
    "print('• 相比 DPO 更加高效，訓練成本降低 50%+')\n",
    "print('• 為 LLM 對齊技術帶來新的範式突破')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}