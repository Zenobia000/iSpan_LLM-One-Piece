{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Adapter Layers - Fine-Tuning a BERT Model for Classification\n",
        "---\n",
        "## Notebook 2: The Training Process\n",
        "\n",
        "**Goal:** In this notebook, you will fine-tune a `bert-base-uncased` model on a sequence classification task (predicting if two sentences are paraphrases) using **Adapter Layers**.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Load a dataset for sequence classification and preprocess it with a tokenizer.\n",
        "-   Load a pre-trained BERT model intended for `SequenceClassification`.\n",
        "-   Deeply understand and configure `peft.AdapterConfig`.\n",
        "-   Apply the Adapter configuration to the base model.\n",
        "-   Fine-tune the model by training *only* the adapter weights using the `transformers.Trainer`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Dataset and Preprocess\n",
        "\n",
        "First, we'll load the GLUE MRPC (Microsoft Research Paraphrase Corpus) dataset. Each item in the dataset consists of two sentences and a label indicating whether they are semantically equivalent.\n",
        "\n",
        "#### Key Hugging Face Components:\n",
        "\n",
        "-   `datasets.load_dataset`: Fetches a dataset from the Hugging Face Hub.\n",
        "-   `transformers.AutoTokenizer`: Loads the appropriate tokenizer for our `bert-base-uncased` model.\n",
        "-   `tokenizer()`: The tokenizer will convert the sentence pairs into the format BERT expects: `[CLS] sentence1 [SEP] sentence2 [SEP]`, along with `token_type_ids` and an `attention_mask`.\n",
        "-   `dataset.map()`: A powerful method to apply a processing function to every example in the dataset. We use `batched=True` for efficient processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Load Dataset ---\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "# --- Load Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "# --- Apply Preprocessing ---\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# The Trainer expects columns named 'labels', but the dataset has 'label'. Let's rename it.\n",
        "encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# We only need a few columns for training.\n",
        "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"])\n",
        "\n",
        "print(\"âœ… Dataset loaded and preprocessed.\")\n",
        "print(encoded_dataset[\"train\"][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load the Base Model\n",
        "\n",
        "Next, we load the `bert-base-uncased` model. Since this is a classification task, we use `AutoModelForSequenceClassification`.\n",
        "\n",
        "#### Key Hugging Face Component:\n",
        "- `transformers.AutoModelForSequenceClassification`: This class automatically loads a pre-trained model with a classification head on top.\n",
        "    - `num_labels`: We need to tell the model how many classes we are predicting. In the MRPC dataset, there are two labels (0 for not a paraphrase, 1 for is a paraphrase).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "\n",
        "print(\"âœ… Base BERT model loaded.\")\n",
        "# print(model) # Uncomment to see the model architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Configure Adapters\n",
        "\n",
        "Here, we'll use the `peft` library to configure our Adapter layers. Unlike LoRA, which modifies existing weights via reparameterization, Adapters add new layers to the model.\n",
        "\n",
        "#### Key Hugging Face `peft` Components:\n",
        "\n",
        "-   `peft.TaskType.SEQ_CLS`: We explicitly tell `peft` that this is a Sequence Classification task.\n",
        "-   `peft.get_peft_model`: This function works for all PEFT methods. It takes the base model and the configuration, and returns the modified `PeftModel`.\n",
        "-   `peft.AdapterConfig`: This is the specific config for classic Adapter layers (also known as Houlsby Adapters).\n",
        "    - `mh_adapter`: Whether to add an adapter layer to the multi-head attention block.\n",
        "    - `output_adapter`: Whether to add an adapter layer after the feed-forward block.\n",
        "    - `reduction_factor`: This is the most important parameter. It controls the bottleneck dimension of the adapter. The bottleneck size will be `d_model / reduction_factor`. So a larger reduction factor means fewer trainable parameters.\n",
        "    - `non_linearity`: The activation function to use in the adapter, e.g., \"relu\" or \"gelu\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_model, AdapterConfig, TaskType\n",
        "\n",
        "# --- Adapter Configuration ---\n",
        "adapter_config = AdapterConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    mh_adapter=True,\n",
        "    output_adapter=True,\n",
        "    reduction_factor=16,\n",
        "    non_linearity=\"relu\"\n",
        ")\n",
        "\n",
        "# --- Create PeftModel ---\n",
        "peft_model = get_peft_model(model, adapter_config)\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Set Up Training and Evaluation\n",
        "\n",
        "The final step is to configure the training process. This is very similar to the LoRA lab, but we'll add an evaluation step to see how well our model is learning.\n",
        "\n",
        "#### Key Hugging Face Components:\n",
        "\n",
        "-   `transformers.TrainingArguments`: We configure this as before, but add:\n",
        "    -   `evaluation_strategy=\"epoch\"`: Tells the `Trainer` to run an evaluation at the end of each epoch.\n",
        "-   `compute_metrics`: We'll define a function to calculate evaluation metrics (accuracy and F1 score) and pass it to the `Trainer`.\n",
        "-   `transformers.Trainer`: We instantiate the trainer with the model, arguments, datasets, tokenizer, and our new metrics function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import datasets as nlp_datasets\n",
        "\n",
        "# --- Metrics Calculation Function ---\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    # Load the metric scripts\n",
        "    accuracy_metric = nlp_datasets.load_metric(\"accuracy\")\n",
        "    f1_metric = nlp_datasets.load_metric(\"f1\")\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"],\n",
        "        \"f1\": f1[\"f1\"],\n",
        "    }\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-adapters-mrpc\",\n",
        "    learning_rate=1e-3, # Adapters often use a higher learning rate than full fine-tuning\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# --- Create Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"ðŸš€ Starting training with Adapters...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Configure Adapters\n",
        "\n",
        "Here, we'll use the `peft` library to configure our Adapter layers. Unlike LoRA, which modifies existing weights via reparameterization, Adapters add new layers to the model.\n",
        "\n",
        "#### Key Hugging Face `peft` Components:\n",
        "\n",
        "-   `peft.PeftConfig`: This is the base class for PEFT configurations. We will use a specific variant for Adapters.\n",
        "-   `peft.TaskType.SEQ_CLS`: We explicitly tell `peft` that this is a Sequence Classification task.\n",
        "-   `peft.get_peft_model`: This function works for all PEFT methods. It takes the base model and the configuration, and returns the modified `PeftModel`.\n",
        "\n",
        "For this specific case, the `peft` library has a more direct configuration using `peft.AdaptionPromptConfig` that can be used for Adapter layers, but we'll use a general `PeftConfig` to show the common workflow. However, the current recommended way for adapters in `peft` is to use the `AdapterConfig`. Let's use that.\n",
        "\n",
        "-   `peft.AdapterConfig`: This is the specific config for classic Adapter layers (also known as Houlsby Adapters).\n",
        "    - `adapter_len`: The bottleneck dimension of the adapter layers. This is the equivalent of LoRA's rank `r`.\n",
        "    - `adapter_layers`: The number of adapter layers to insert.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
