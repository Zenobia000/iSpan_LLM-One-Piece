{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2: Wanda Pruning - Performance Benchmarking\n",
    "\n",
    "**Goal:** Comprehensive performance analysis of pruned sparse models.\n",
    "\n",
    "**You will learn to:**\n",
    "- Measure perplexity on WikiText-2 dataset\n",
    "- Analyze latency distribution (P50/P95/P99)\n",
    "- Profile memory usage across different batch sizes\n",
    "- Evaluate throughput scaling\n",
    "- Understand hardware requirements for sparse acceleration\n",
    "- Make deployment decisions based on metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Why Comprehensive Benchmarking?\n",
    "\n",
    "**Simple inference tests are not enough because**:\n",
    "- **Perplexity**: Quantitative measure of model quality\n",
    "- **Latency Distribution**: P99 latency matters for production SLAs\n",
    "- **Memory Profiling**: Understand deployment requirements\n",
    "- **Throughput Scaling**: Optimize for different workloads\n",
    "\n",
    "**Expected Results** (50% Wanda pruning, Llama-2-7B):\n",
    "- **Perplexity**: 5.68 ‚Üí 6.12 (+7.7% degradation)\n",
    "- **Latency**: Similar without hardware acceleration\n",
    "- **Memory**: Same (dense format storage)\n",
    "- **With 2:4 Sparse + A100**: 2x speedup possible\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have completed:\n",
    "- **01-Setup.ipynb**: Environment setup\n",
    "- **02-Prune.ipynb**: Applied Wanda pruning\n",
    "- **03-Inference.ipynb**: Initial quality comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Models\n",
    "\n",
    "Load both dense and pruned models for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "PRUNED_MODEL_DIR = \"./pruned_model\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Models for Benchmarking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load tokenizer (shared)\n",
    "print(\"‚è≥ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"‚úÖ Tokenizer loaded\\n\")\n",
    "\n",
    "# Load dense model\n",
    "print(\"‚è≥ Loading dense baseline model...\")\n",
    "dense_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"‚úÖ Dense model loaded\\n\")\n",
    "\n",
    "# Load sparse model\n",
    "print(\"‚è≥ Loading pruned sparse model...\")\n",
    "sparse_model = AutoModelForCausalLM.from_pretrained(\n",
    "    PRUNED_MODEL_DIR,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load pruning config\n",
    "config_path = os.path.join(PRUNED_MODEL_DIR, \"pruning_config.json\")\n",
    "with open(config_path, 'r') as f:\n",
    "    pruning_config = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Sparse model loaded (sparsity: {pruning_config['achieved_sparsity']:.2%})\\n\")\n",
    "\n",
    "# GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"üñ•Ô∏è  GPU Memory: {memory_allocated:.2f} GB\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load WikiText-2 Test Dataset\n",
    "\n",
    "Use WikiText-2 test set for perplexity evaluation (standard benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading WikiText-2 Test Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load test split\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "print(f\"‚úÖ Dataset loaded: {len(dataset)} samples\\n\")\n",
    "\n",
    "# Filter empty texts\n",
    "dataset = dataset.filter(lambda x: len(x['text'].strip()) > 0)\n",
    "print(f\"‚úÖ Filtered dataset: {len(dataset)} non-empty samples\\n\")\n",
    "\n",
    "# Concatenate all texts\n",
    "all_text = \"\\n\\n\".join(dataset['text'])\n",
    "print(f\"üìä Total characters: {len(all_text):,}\")\n",
    "\n",
    "# Tokenize\n",
    "print(\"‚è≥ Tokenizing dataset...\")\n",
    "encodings = tokenizer(all_text, return_tensors=\"pt\")\n",
    "input_ids = encodings['input_ids'][0]\n",
    "print(f\"‚úÖ Tokenized: {len(input_ids):,} tokens\\n\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Calculate Perplexity (Dense Model)\n",
    "\n",
    "Measure baseline perplexity on WikiText-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def calculate_perplexity(model, input_ids, max_length=2048, stride=512):\n",
    "    \"\"\"\n",
    "    Calculate perplexity using sliding window.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        input_ids: Tokenized input\n",
    "        max_length: Maximum context length\n",
    "        stride: Stride for sliding window\n",
    "    \n",
    "    Returns:\n",
    "        perplexity, avg_loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    nlls = []  # Negative log-likelihoods\n",
    "    \n",
    "    # Sliding window over the dataset\n",
    "    for i in tqdm(range(0, len(input_ids), stride), desc=\"Computing perplexity\"):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, len(input_ids))\n",
    "        trg_len = end_loc - i  # Target length (labels)\n",
    "        \n",
    "        input_chunk = input_ids[begin_loc:end_loc].unsqueeze(0).to(model.device)\n",
    "        target_ids = input_chunk.clone()\n",
    "        target_ids[:, :-trg_len] = -100  # Ignore context tokens\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_chunk, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "        \n",
    "        nlls.append(neg_log_likelihood)\n",
    "        \n",
    "        if i + stride >= len(input_ids):\n",
    "            break\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_nll = torch.stack(nlls).sum() / end_loc\n",
    "    perplexity = torch.exp(avg_nll)\n",
    "    \n",
    "    return perplexity.item(), avg_nll.item()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Calculating Perplexity: Dense Model\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚è≥ This may take 5-10 minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "dense_ppl, dense_loss = calculate_perplexity(dense_model, input_ids)\n",
    "dense_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüìä Dense Model Results:\")\n",
    "print(f\"   Perplexity: {dense_ppl:.2f}\")\n",
    "print(f\"   Avg Loss: {dense_loss:.4f}\")\n",
    "print(f\"   Time: {dense_time:.2f}s\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Calculate Perplexity (Sparse Model)\n",
    "\n",
    "Measure perplexity of pruned model to quantify quality loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Calculating Perplexity: Sparse Model (50% pruned)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚è≥ This may take 5-10 minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "sparse_ppl, sparse_loss = calculate_perplexity(sparse_model, input_ids)\n",
    "sparse_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüìä Sparse Model Results:\")\n",
    "print(f\"   Perplexity: {sparse_ppl:.2f}\")\n",
    "print(f\"   Avg Loss: {sparse_loss:.4f}\")\n",
    "print(f\"   Time: {sparse_time:.2f}s\")\n",
    "\n",
    "# Comparison\n",
    "ppl_increase = (sparse_ppl - dense_ppl) / dense_ppl * 100\n",
    "\n",
    "print(f\"\\nüìä Perplexity Comparison:\")\n",
    "print(f\"   Dense:  {dense_ppl:.2f}\")\n",
    "print(f\"   Sparse: {sparse_ppl:.2f}\")\n",
    "print(f\"   Increase: {ppl_increase:+.2f}%\")\n",
    "\n",
    "if ppl_increase < 10:\n",
    "    print(f\"   ‚úÖ Quality well preserved (<10% degradation)\")\n",
    "elif ppl_increase < 20:\n",
    "    print(f\"   üü° Moderate quality loss (10-20% degradation)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Significant quality loss (>20% degradation)\")\n",
    "\n",
    "print(f\"\\nüìñ Reference (Wanda paper, Llama-2-7B, 50% sparsity):\")\n",
    "print(f\"   Expected PPL: 5.68 ‚Üí 6.12 (+7.7%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Latency Distribution Analysis\n",
    "\n",
    "Measure latency across multiple runs to understand variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def measure_latency_distribution(model, tokenizer, prompt, num_runs=50, max_tokens=50):\n",
    "    \"\"\"\n",
    "    Measure latency distribution over multiple runs.\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Warmup runs\n",
    "    for _ in range(5):\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "    \n",
    "    # Measure latency\n",
    "    for _ in tqdm(range(num_runs), desc=\"Measuring latency\"):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "        latency = time.time() - start\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    return np.array(latencies)\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Latency Distribution Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Runs: 50\")\n",
    "print(f\"Output tokens: 50\\n\")\n",
    "\n",
    "# Dense model\n",
    "print(\"[Dense Model]\")\n",
    "dense_latencies = measure_latency_distribution(dense_model, tokenizer, prompt)\n",
    "print(f\"‚úÖ Dense latencies collected\\n\")\n",
    "\n",
    "# Sparse model\n",
    "print(\"[Sparse Model]\")\n",
    "sparse_latencies = measure_latency_distribution(sparse_model, tokenizer, prompt)\n",
    "print(f\"‚úÖ Sparse latencies collected\\n\")\n",
    "\n",
    "# Statistics\n",
    "print(\"üìä Latency Statistics:\")\n",
    "print(\"\\nDense Model:\")\n",
    "print(f\"   Mean: {np.mean(dense_latencies)*1000:.2f}ms\")\n",
    "print(f\"   Std:  {np.std(dense_latencies)*1000:.2f}ms\")\n",
    "print(f\"   P50:  {np.percentile(dense_latencies, 50)*1000:.2f}ms\")\n",
    "print(f\"   P95:  {np.percentile(dense_latencies, 95)*1000:.2f}ms\")\n",
    "print(f\"   P99:  {np.percentile(dense_latencies, 99)*1000:.2f}ms\")\n",
    "\n",
    "print(\"\\nSparse Model (50% pruned):\")\n",
    "print(f\"   Mean: {np.mean(sparse_latencies)*1000:.2f}ms\")\n",
    "print(f\"   Std:  {np.std(sparse_latencies)*1000:.2f}ms\")\n",
    "print(f\"   P50:  {np.percentile(sparse_latencies, 50)*1000:.2f}ms\")\n",
    "print(f\"   P95:  {np.percentile(sparse_latencies, 95)*1000:.2f}ms\")\n",
    "print(f\"   P99:  {np.percentile(sparse_latencies, 99)*1000:.2f}ms\")\n",
    "\n",
    "# Speedup\n",
    "p50_speedup = np.percentile(dense_latencies, 50) / np.percentile(sparse_latencies, 50)\n",
    "p95_speedup = np.percentile(dense_latencies, 95) / np.percentile(sparse_latencies, 95)\n",
    "p99_speedup = np.percentile(dense_latencies, 99) / np.percentile(sparse_latencies, 99)\n",
    "\n",
    "print(\"\\nüìä Speedup Analysis:\")\n",
    "print(f\"   P50 speedup: {p50_speedup:.2f}x\")\n",
    "print(f\"   P95 speedup: {p95_speedup:.2f}x\")\n",
    "print(f\"   P99 speedup: {p99_speedup:.2f}x\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Throughput Benchmarking\n",
    "\n",
    "Measure tokens/second at different output lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_throughput(model, tokenizer, prompt, output_lengths=[50, 100, 200]):\n",
    "    \"\"\"\n",
    "    Measure throughput at different output lengths.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    for length in output_lengths:\n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=length, do_sample=False)\n",
    "        \n",
    "        # Measure\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=length, do_sample=False)\n",
    "        latency = time.time() - start\n",
    "        \n",
    "        num_tokens = len(outputs[0])\n",
    "        throughput = num_tokens / latency\n",
    "        \n",
    "        results.append({\n",
    "            'output_length': length,\n",
    "            'latency': latency,\n",
    "            'tokens': num_tokens,\n",
    "            'throughput': throughput\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Throughput Benchmarking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output_lengths = [50, 100, 200]\n",
    "prompt = \"Artificial intelligence is transforming\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Output lengths: {output_lengths}\\n\")\n",
    "\n",
    "# Dense model\n",
    "print(\"[Dense Model]\")\n",
    "dense_throughput = measure_throughput(dense_model, tokenizer, prompt, output_lengths)\n",
    "print(\"‚úÖ Dense throughput measured\\n\")\n",
    "\n",
    "# Sparse model\n",
    "print(\"[Sparse Model]\")\n",
    "sparse_throughput = measure_throughput(sparse_model, tokenizer, prompt, output_lengths)\n",
    "print(\"‚úÖ Sparse throughput measured\\n\")\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Throughput Results:\\n\")\n",
    "print(f\"{'Length':<10} {'Dense (tok/s)':<15} {'Sparse (tok/s)':<15} {'Speedup':<10}\")\n",
    "print(\"‚îÄ\" * 60)\n",
    "\n",
    "for d, s in zip(dense_throughput, sparse_throughput):\n",
    "    speedup = s['throughput'] / d['throughput']\n",
    "    print(f\"{d['output_length']:<10} {d['throughput']:<15.2f} {s['throughput']:<15.2f} {speedup:<10.2f}x\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Memory Profiling\n",
    "\n",
    "Analyze GPU memory usage during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_memory(model, tokenizer, prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Profile GPU memory usage during inference.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Measure baseline\n",
    "    baseline = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Run inference\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "    \n",
    "    # Measure peak\n",
    "    peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "    current = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    return {\n",
    "        'baseline': baseline,\n",
    "        'peak': peak,\n",
    "        'current': current,\n",
    "        'inference_overhead': peak - baseline\n",
    "    }\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Memory Profiling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Dense model\n",
    "    print(\"\\n[Dense Model]\")\n",
    "    dense_mem = profile_memory(dense_model, tokenizer, \"The impact of AI on society\")\n",
    "    print(f\"   Baseline:  {dense_mem['baseline']:.2f} GB\")\n",
    "    print(f\"   Peak:      {dense_mem['peak']:.2f} GB\")\n",
    "    print(f\"   Current:   {dense_mem['current']:.2f} GB\")\n",
    "    print(f\"   Inference overhead: {dense_mem['inference_overhead']:.2f} GB\")\n",
    "    \n",
    "    # Sparse model\n",
    "    print(\"\\n[Sparse Model (50% pruned)]\")\n",
    "    sparse_mem = profile_memory(sparse_model, tokenizer, \"The impact of AI on society\")\n",
    "    print(f\"   Baseline:  {sparse_mem['baseline']:.2f} GB\")\n",
    "    print(f\"   Peak:      {sparse_mem['peak']:.2f} GB\")\n",
    "    print(f\"   Current:   {sparse_mem['current']:.2f} GB\")\n",
    "    print(f\"   Inference overhead: {sparse_mem['inference_overhead']:.2f} GB\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\nüìä Memory Comparison:\")\n",
    "    print(f\"   Baseline reduction: {(dense_mem['baseline'] - sparse_mem['baseline']) / dense_mem['baseline']:.1%}\")\n",
    "    print(f\"   Peak reduction: {(dense_mem['peak'] - sparse_mem['peak']) / dense_mem['peak']:.1%}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Note:\")\n",
    "    print(\"   Memory is similar because sparse model is stored in dense format.\")\n",
    "    print(\"   For actual memory reduction, export to sparse format (CSR/COO).\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available. Skipping memory profiling.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Comprehensive Visualization\n",
    "\n",
    "Create publication-quality visualizations of all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating Comprehensive Visualizations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Perplexity Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "models = ['Dense', 'Sparse\\n(50%)']\n",
    "ppls = [dense_ppl, sparse_ppl]\n",
    "colors = ['green', 'blue']\n",
    "bars = ax1.bar(models, ppls, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Perplexity', fontsize=11)\n",
    "ax1.set_title('Perplexity on WikiText-2', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for bar, ppl in zip(bars, ppls):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{ppl:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 2: Perplexity Increase\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.bar(['Increase'], [ppl_increase], color='coral', alpha=0.7)\n",
    "ax2.axhline(y=10, color='orange', linestyle='--', label='10% threshold', linewidth=2)\n",
    "ax2.set_ylabel('Increase (%)', fontsize=11)\n",
    "ax2.set_title('Perplexity Degradation', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Latency Distribution (Box plot)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "box_data = [dense_latencies * 1000, sparse_latencies * 1000]\n",
    "bp = ax3.boxplot(box_data, labels=['Dense', 'Sparse'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('green')\n",
    "bp['boxes'][1].set_facecolor('blue')\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_alpha(0.6)\n",
    "ax3.set_ylabel('Latency (ms)', fontsize=11)\n",
    "ax3.set_title('Latency Distribution (50 tokens)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: P50/P95/P99 Latency Comparison\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "percentiles = ['P50', 'P95', 'P99']\n",
    "dense_p = [np.percentile(dense_latencies, p)*1000 for p in [50, 95, 99]]\n",
    "sparse_p = [np.percentile(sparse_latencies, p)*1000 for p in [50, 95, 99]]\n",
    "x = np.arange(len(percentiles))\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, dense_p, width, label='Dense', color='green', alpha=0.7)\n",
    "ax4.bar(x + width/2, sparse_p, width, label='Sparse', color='blue', alpha=0.7)\n",
    "ax4.set_ylabel('Latency (ms)', fontsize=11)\n",
    "ax4.set_title('Percentile Latency Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(percentiles)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 5: Throughput vs Output Length\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "lengths = [d['output_length'] for d in dense_throughput]\n",
    "dense_thr = [d['throughput'] for d in dense_throughput]\n",
    "sparse_thr = [s['throughput'] for s in sparse_throughput]\n",
    "ax5.plot(lengths, dense_thr, 'o-', label='Dense', color='green', linewidth=2, markersize=8)\n",
    "ax5.plot(lengths, sparse_thr, 's-', label='Sparse', color='blue', linewidth=2, markersize=8)\n",
    "ax5.set_xlabel('Output Length (tokens)', fontsize=11)\n",
    "ax5.set_ylabel('Throughput (tokens/sec)', fontsize=11)\n",
    "ax5.set_title('Throughput Scaling', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Speedup by Output Length\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "speedups_by_length = [s['throughput'] / d['throughput'] \n",
    "                      for d, s in zip(dense_throughput, sparse_throughput)]\n",
    "bars = ax6.bar(lengths, speedups_by_length, color=['green' if s > 1 else 'orange' \n",
    "                                                     for s in speedups_by_length], alpha=0.7)\n",
    "ax6.axhline(y=1.0, color='red', linestyle='--', label='Baseline', linewidth=2)\n",
    "ax6.set_xlabel('Output Length (tokens)', fontsize=11)\n",
    "ax6.set_ylabel('Speedup (sparse/dense)', fontsize=11)\n",
    "ax6.set_title('Speedup by Output Length', fontsize=12, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 7: Memory Usage (if available)\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "if torch.cuda.is_available() and dense_mem and sparse_mem:\n",
    "    memory_metrics = ['Baseline', 'Peak', 'Current']\n",
    "    dense_mems = [dense_mem['baseline'], dense_mem['peak'], dense_mem['current']]\n",
    "    sparse_mems = [sparse_mem['baseline'], sparse_mem['peak'], sparse_mem['current']]\n",
    "    x = np.arange(len(memory_metrics))\n",
    "    ax7.bar(x - width/2, dense_mems, width, label='Dense', color='green', alpha=0.7)\n",
    "    ax7.bar(x + width/2, sparse_mems, width, label='Sparse', color='blue', alpha=0.7)\n",
    "    ax7.set_ylabel('Memory (GB)', fontsize=11)\n",
    "    ax7.set_title('GPU Memory Usage', fontsize=12, fontweight='bold')\n",
    "    ax7.set_xticks(x)\n",
    "    ax7.set_xticklabels(memory_metrics)\n",
    "    ax7.legend()\n",
    "    ax7.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax7.text(0.5, 0.5, 'CUDA not available', ha='center', va='center', fontsize=12)\n",
    "    ax7.axis('off')\n",
    "\n",
    "# Plot 8: Latency Histogram\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "ax8.hist(dense_latencies * 1000, bins=20, alpha=0.5, label='Dense', color='green')\n",
    "ax8.hist(sparse_latencies * 1000, bins=20, alpha=0.5, label='Sparse', color='blue')\n",
    "ax8.axvline(np.mean(dense_latencies) * 1000, color='green', linestyle='--', linewidth=2)\n",
    "ax8.axvline(np.mean(sparse_latencies) * 1000, color='blue', linestyle='--', linewidth=2)\n",
    "ax8.set_xlabel('Latency (ms)', fontsize=11)\n",
    "ax8.set_ylabel('Frequency', fontsize=11)\n",
    "ax8.set_title('Latency Distribution', fontsize=12, fontweight='bold')\n",
    "ax8.legend()\n",
    "ax8.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 9: Summary Table\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.axis('off')\n",
    "summary_data = [\n",
    "    ['Metric', 'Dense', 'Sparse', 'Change'],\n",
    "    ['Perplexity', f'{dense_ppl:.2f}', f'{sparse_ppl:.2f}', f'+{ppl_increase:.1f}%'],\n",
    "    ['P50 Latency', f'{np.percentile(dense_latencies, 50)*1000:.0f}ms', \n",
    "     f'{np.percentile(sparse_latencies, 50)*1000:.0f}ms', f'{p50_speedup:.2f}x'],\n",
    "    ['Throughput', f'{dense_thr[0]:.1f}', f'{sparse_thr[0]:.1f}', \n",
    "     f'{sparse_thr[0]/dense_thr[0]:.2f}x'],\n",
    "    ['Parameters', '7.0B', '7.0B', '50% sparse']\n",
    "]\n",
    "table = ax9.table(cellText=summary_data, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "for i in range(len(summary_data[0])):\n",
    "    table[(0, i)].set_facecolor('#40466e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "ax9.set_title('Performance Summary', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "plt.savefig(os.path.join(PRUNED_MODEL_DIR, 'comprehensive_benchmark.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization saved to {PRUNED_MODEL_DIR}/comprehensive_benchmark.png\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Production Deployment Recommendations\n",
    "\n",
    "Provide actionable insights based on benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä Benchmark Summary:\")\n",
    "print(f\"   Sparsity: {pruning_config['achieved_sparsity']:.1%}\")\n",
    "print(f\"   Perplexity: {dense_ppl:.2f} ‚Üí {sparse_ppl:.2f} (+{ppl_increase:.1f}%)\")\n",
    "print(f\"   P50 Latency: {np.percentile(dense_latencies, 50)*1000:.0f}ms ‚Üí {np.percentile(sparse_latencies, 50)*1000:.0f}ms ({p50_speedup:.2f}x)\")\n",
    "print(f\"   Throughput: {dense_thr[0]:.1f} ‚Üí {sparse_thr[0]:.1f} tok/s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ WHEN TO USE SPARSE MODEL (50% Wanda Pruning):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DEPLOYMENT WITH 2:4 SPARSE HARDWARE (NVIDIA A100+):\")\n",
    "print(\"   ‚úÖ Use Case: High-throughput inference services\")\n",
    "print(\"   ‚úÖ Benefits:\")\n",
    "print(\"      - 2x speedup with Sparse Tensor Cores\")\n",
    "print(\"      - Same memory footprint (in CSR format)\")\n",
    "print(\"      - <10% quality loss (acceptable for most tasks)\")\n",
    "print(\"   ‚úÖ Requirements:\")\n",
    "print(\"      - NVIDIA A100 or newer GPU\")\n",
    "print(\"      - PyTorch with sparse kernel support\")\n",
    "print(\"      - Export model to 2:4 semi-structured sparse format\")\n",
    "\n",
    "print(\"\\n2. DEPLOYMENT WITHOUT SPARSE ACCELERATION:\")\n",
    "if abs(p50_speedup - 1.0) < 0.1:\n",
    "    print(\"   ‚ö†Ô∏è  Performance: Similar to dense model\")\n",
    "    print(\"   üí° Recommendation: Use dense model for now\")\n",
    "    print(\"   üìå Future: Wait for hardware acceleration support\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Performance: Still provides speedup\")\n",
    "    print(\"   üí° Recommendation: Deploy if quality acceptable\")\n",
    "\n",
    "print(\"\\n3. MEMORY-CONSTRAINED DEPLOYMENT:\")\n",
    "print(\"   ‚ö†Ô∏è  Current: Dense format (no memory reduction)\")\n",
    "print(\"   üí° Action Required: Export to sparse format (CSR/COO)\")\n",
    "print(\"   ‚úÖ Expected Savings: ~50% memory reduction\")\n",
    "print(\"   üìå Trade-off: Need sparse-aware inference engine\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ö†Ô∏è  WHEN NOT TO USE SPARSE MODEL:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if ppl_increase > 10:\n",
    "    print(\"\\n‚ùå QUALITY-CRITICAL APPLICATIONS:\")\n",
    "    print(f\"   Perplexity increase: {ppl_increase:.1f}% (>10% threshold)\")\n",
    "    print(\"   Use cases to avoid: Medical diagnosis, legal advice, financial analysis\")\n",
    "    print(\"   üí° Consider: Lower sparsity (30-40%) or dense model\")\n",
    "\n",
    "if p50_speedup < 1.0:\n",
    "    print(\"\\n‚ùå LATENCY-CRITICAL APPLICATIONS (without sparse hardware):\")\n",
    "    print(f\"   Sparse model is {1/p50_speedup:.2f}x slower\")\n",
    "    print(\"   Use cases to avoid: Real-time chat, low-latency APIs\")\n",
    "    print(\"   üí° Recommendation: Use dense model or upgrade to A100+\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã DEPLOYMENT CHECKLIST:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚òëÔ∏è  Model Validation:\")\n",
    "print(\"   [ ] Perplexity within acceptable range\")\n",
    "print(\"   [ ] Side-by-side quality testing on production data\")\n",
    "print(\"   [ ] A/B testing with 5% traffic\")\n",
    "\n",
    "print(\"\\n‚òëÔ∏è  Infrastructure:\")\n",
    "print(\"   [ ] GPU supports sparse operations (check SM compute capability)\")\n",
    "print(\"   [ ] PyTorch/TensorRT sparse kernels installed\")\n",
    "print(\"   [ ] Export model to sparse format (CSR for 2:4, COO for unstructured)\")\n",
    "\n",
    "print(\"\\n‚òëÔ∏è  Performance Monitoring:\")\n",
    "print(\"   [ ] Set up P95/P99 latency monitoring\")\n",
    "print(\"   [ ] Track quality metrics (perplexity, task accuracy)\")\n",
    "print(\"   [ ] Compare dense vs sparse in production\")\n",
    "\n",
    "print(\"\\n‚òëÔ∏è  Rollback Plan:\")\n",
    "print(\"   [ ] Keep dense model as fallback\")\n",
    "print(\"   [ ] Define quality degradation thresholds\")\n",
    "print(\"   [ ] Automated rollback if metrics degrade\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ FINAL RECOMMENDATION:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if ppl_increase < 10 and torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    if gpu_props.major >= 8:  # Ampere or newer\n",
    "        print(\"\\n‚úÖ DEPLOY SPARSE MODEL\")\n",
    "        print(f\"   Your GPU (SM {gpu_props.major}.{gpu_props.minor}) supports sparse acceleration\")\n",
    "        print(f\"   Quality loss is acceptable ({ppl_increase:.1f}%)\")\n",
    "        print(\"   Expected benefits: 2x speedup with proper sparse kernel setup\")\n",
    "        print(\"\\nüìå Next Steps:\")\n",
    "        print(\"   1. Export model to 2:4 semi-structured sparse format\")\n",
    "        print(\"   2. Integrate with TensorRT or vLLM sparse backend\")\n",
    "        print(\"   3. Run production load testing\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  WAIT FOR HARDWARE UPGRADE\")\n",
    "        print(f\"   Your GPU (SM {gpu_props.major}.{gpu_props.minor}) lacks sparse acceleration\")\n",
    "        print(\"   Current performance gain is minimal\")\n",
    "        print(\"   üí° Recommendation: Stick with dense model until A100+ available\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  FURTHER TUNING NEEDED\")\n",
    "    if ppl_increase >= 10:\n",
    "        print(f\"   Quality loss ({ppl_increase:.1f}%) exceeds acceptable threshold\")\n",
    "        print(\"   üí° Options:\")\n",
    "        print(\"      - Reduce sparsity to 30-40%\")\n",
    "        print(\"      - Use better calibration data\")\n",
    "        print(\"      - Try structured pruning (2:4) instead of unstructured\")\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"   CUDA not available - need GPU for deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Benchmarking Complete!\n",
    "\n",
    "**Summary**:\n",
    "- ‚úÖ Measured perplexity on WikiText-2 (quantitative quality metric)\n",
    "- ‚úÖ Analyzed latency distribution (P50/P95/P99)\n",
    "- ‚úÖ Benchmarked throughput at different output lengths\n",
    "- ‚úÖ Profiled GPU memory usage\n",
    "- ‚úÖ Created comprehensive visualizations\n",
    "- ‚úÖ Provided production deployment recommendations\n",
    "\n",
    "**Key Findings**:\n",
    "- **Quality**: Perplexity increased by ~{ppl_increase:.1f}%\n",
    "- **Performance**: Speedup depends on hardware (2x with A100 sparse support)\n",
    "- **Memory**: Similar in dense format, ~50% reduction with sparse export\n",
    "- **Production**: Viable for non-critical tasks with sparse hardware\n",
    "\n",
    "**Files Generated**:\n",
    "- `pruned_model/comprehensive_benchmark.png`: All visualizations\n",
    "- Performance metrics logged above\n",
    "\n",
    "**Next Steps**:\n",
    "1. Export model to sparse format (CSR/COO) for size reduction\n",
    "2. Integrate with sparse inference engine (TensorRT, vLLM)\n",
    "3. Deploy with A/B testing on 5% production traffic\n",
    "4. Monitor quality and performance metrics closely\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You have completed Lab-3.2: Wanda Pruning!\n",
    "\n",
    "You now understand:\n",
    "- How activation-aware pruning works (weight √ó activation importance)\n",
    "- Trade-offs between sparsity and quality\n",
    "- Hardware requirements for sparse acceleration\n",
    "- When to use sparse models in production\n",
    "\n",
    "**Next Lab**: Lab-3.3 Knowledge Distillation (coming soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
