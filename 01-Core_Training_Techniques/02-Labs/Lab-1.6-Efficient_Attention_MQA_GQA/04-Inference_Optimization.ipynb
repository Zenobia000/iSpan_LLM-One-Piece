{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.6: æ¨ç†å„ªåŒ–å¯¦æˆ°\n",
    "## Inference Optimization with MQA/GQA\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- å¯¦ç¾å®Œæ•´çš„ KV Cache æ¨ç†\n",
    "- å°æ¯”é•·æ–‡æœ¬ç”Ÿæˆæ€§èƒ½\n",
    "- å„ªåŒ–æ‰¹æ¬¡æ¨ç†ååé‡\n",
    "- å¯¦éš›éƒ¨ç½²å ´æ™¯åˆ†æ\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 60-90åˆ†é˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. å®Œæ•´æ¨ç†å¯¦ç¾\n",
    "\n",
    "å¯¦ç¾æ”¯æŒ KV Cache çš„è‡ªå›æ­¸ç”Ÿæˆï¼Œå°æ¯” MHA/GQA/MQAã€‚\n",
    "\n",
    "### é—œéµå„ªåŒ–:\n",
    "1. Prefill éšæ®µ: ä¸¦è¡Œè™•ç† prompt\n",
    "2. Decode éšæ®µ: é€å€‹ç”Ÿæˆ, å¾©ç”¨ KV Cache\n",
    "3. æ‰¹æ¬¡æ¨ç†: å¤šè«‹æ±‚ä¸¦è¡Œè™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. åŸºç¤çµ„ä»¶å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Grouped-Query Attention - å¤šå€‹ Q å…±äº«åˆ†çµ„çš„ K, V\"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_groups, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads å¿…é ˆèƒ½è¢« num_kv_groups æ•´é™¤\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.heads_per_group = num_heads // num_kv_groups\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, num_kv_groups * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, num_kv_groups * self.head_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False):\n",
    "        B, N, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x).view(B, N, self.num_heads, self.head_dim)\n",
    "        K = self.k_proj(x).view(B, N, self.num_kv_groups, self.head_dim)\n",
    "        V = self.v_proj(x).view(B, N, self.num_kv_groups, self.head_dim)\n",
    "        \n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            K = torch.cat([past_k, K], dim=1)\n",
    "            V = torch.cat([past_v, V], dim=1)\n",
    "        \n",
    "        K_repeated = K.repeat_interleave(self.heads_per_group, dim=2)\n",
    "        V_repeated = V.repeat_interleave(self.heads_per_group, dim=2)\n",
    "        \n",
    "        Q = Q.transpose(1, 2)\n",
    "        K_repeated = K_repeated.transpose(1, 2)\n",
    "        V_repeated = V_repeated.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K_repeated.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V_repeated)\n",
    "        output = output.transpose(1, 2).contiguous().view(B, N, -1)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            return output, (K, V)\n",
    "        return output\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"æ¨¡å‹é…ç½®\"\"\"\n",
    "    hidden_dim: int = 768\n",
    "    num_heads: int = 12\n",
    "    num_kv_groups: int = 4  # GQA-4 é…ç½®\n",
    "    vocab_size: int = 32000\n",
    "    max_seq_len: int = 2048\n",
    "    dropout: float = 0.1\n",
    "    num_layers: int = 6  # ç°¡åŒ–ç‚º 6 å±¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. å®Œæ•´çš„ Transformer å±¤å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"RoPE ä½ç½®ç·¨ç¢¼ - ç°¡åŒ–ç‰ˆæœ¬\"\"\"\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # è¨ˆç®—é »ç‡\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # é è¨ˆç®—ä½ç½®ç·¨ç¢¼\n",
    "        t = torch.arange(max_seq_len).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer('cos_cached', emb.cos()[None, None, :, :])\n",
    "        self.register_buffer('sin_cached', emb.sin()[None, None, :, :])\n",
    "    \n",
    "    def forward(self, x, seq_len):\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, :].to(x.device),\n",
    "            self.sin_cached[:, :, :seq_len, :].to(x.device)\n",
    "        )\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    \"\"\"æ‡‰ç”¨ RoPE\"\"\"\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "\n",
    "class OptimizedGQALayer(nn.Module):\n",
    "    \"\"\"å„ªåŒ–çš„ GQA Transformer å±¤\"\"\"\n",
    "    def __init__(self, config: ModelConfig, attention_type=\"gqa\"):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "        # æ ¹æ“šé¡å‹é¸æ“‡ä¸åŒçš„ attention\n",
    "        if attention_type == \"mha\":\n",
    "            self.attention = GroupedQueryAttention(\n",
    "                config.hidden_dim, config.num_heads, config.num_heads, config.dropout\n",
    "            )\n",
    "        elif attention_type == \"mqa\":\n",
    "            self.attention = GroupedQueryAttention(\n",
    "                config.hidden_dim, config.num_heads, 1, config.dropout\n",
    "            )\n",
    "        else:  # gqa\n",
    "            self.attention = GroupedQueryAttention(\n",
    "                config.hidden_dim, config.num_heads, config.num_kv_groups, config.dropout\n",
    "            )\n",
    "        \n",
    "        # RoPE (ç°¡åŒ–ç‰ˆæœ¬)\n",
    "        self.rope = RotaryPositionalEmbedding(\n",
    "            config.hidden_dim // config.num_heads,\n",
    "            config.max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Layer Norm\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_dim)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_dim * 4, config.hidden_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, past_kv=None, use_cache=False, position_ids=None):\n",
    "        # Pre-norm + Attention\n",
    "        residual = x\n",
    "        x = self.ln1(x)\n",
    "        \n",
    "        # æ‡‰ç”¨ RoPE (ç°¡åŒ–ç‰ˆæœ¬ - ç‚ºäº†ç¤ºä¾‹è€Œè·³éå¯¦éš›æ‡‰ç”¨)\n",
    "        if position_ids is not None:\n",
    "            cos, sin = self.rope(x, x.size(1))\n",
    "            # é€™è£¡ç‚ºäº†ç°¡åŒ–ï¼Œç›´æ¥å‚³éåŸå§‹ x\n",
    "        \n",
    "        attn_out, new_past_kv = self.attention(x, past_kv, use_cache)\n",
    "        x = residual + attn_out\n",
    "        \n",
    "        # Pre-norm + MLP\n",
    "        residual = x\n",
    "        x = self.ln2(x)\n",
    "        x = residual + self.mlp(x)\n",
    "        \n",
    "        if use_cache:\n",
    "            return x, new_past_kv\n",
    "        return x\n",
    "\n",
    "class OptimizedLanguageModel(nn.Module):\n",
    "    \"\"\"å„ªåŒ–çš„èªè¨€æ¨¡å‹\"\"\"\n",
    "    def __init__(self, config: ModelConfig, attention_type=\"gqa\"):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            OptimizedGQALayer(config, attention_type) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_dim)\n",
    "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, input_ids, past_kvs=None, use_cache=False):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        \n",
    "        new_past_kvs = [] if use_cache else None\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            past_kv = past_kvs[i] if past_kvs else None\n",
    "            \n",
    "            if use_cache:\n",
    "                x, new_past_kv = layer(x, past_kv, use_cache)\n",
    "                new_past_kvs.append(new_past_kv)\n",
    "            else:\n",
    "                x = layer(x, past_kv, use_cache)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if use_cache:\n",
    "            return logits, new_past_kvs\n",
    "        return logits\n",
    "\n",
    "# å»ºç«‹æ¸¬è©¦æ¨¡å‹\n",
    "config = ModelConfig()\n",
    "model = OptimizedLanguageModel(config).to(device)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹å»ºç«‹æˆåŠŸ\")\n",
    "print(f\"åƒæ•¸é‡: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "print(f\"é…ç½®: {config}\")\n",
    "\n",
    "# æ¸¬è©¦å‰å‘å‚³æ’­\n",
    "test_input = torch.randint(0, config.vocab_size, (1, 10), device=device)\n",
    "logits = model(test_input)\n",
    "print(f\"æ¸¬è©¦è¼¸å…¥: {test_input.shape} â†’ è¼¸å‡º: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. æ¨ç†å¼•æ“å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInferenceEngine:\n",
    "    \"\"\"ç°¡åŒ–ç‰ˆæ¨ç†å¼•æ“ - å°ˆæ³¨æ–¼æ€§èƒ½å°æ¯”æ¸¬è©¦\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model.eval()  # è¨­ç‚ºè©•ä¼°æ¨¡å¼\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def benchmark_model(self, seq_len=64, num_decode_steps=20):\n",
    "        \"\"\"åŸ·è¡Œæ¨¡å‹åŸºæº–æ¸¬è©¦\"\"\"\n",
    "        \n",
    "        # ğŸ”§ ç¢ºä¿æ¨¡å‹ç²¾åº¦ä¸€è‡´æ€§\n",
    "        model_dtype = next(self.model.parameters()).dtype\n",
    "        \n",
    "        # æ¸…ç†è¨˜æ†¶é«”\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        # æº–å‚™æ¸¬è©¦è¼¸å…¥ - ä½¿ç”¨èˆ‡æ¨¡å‹åŒ¹é…çš„ dtype\n",
    "        test_input = torch.randint(1, 1000, (1, seq_len), device=self.device)\n",
    "        \n",
    "        # Prefill éšæ®µæ¸¬è©¦\n",
    "        start_time = time.time()\n",
    "        logits, past_kvs = self.model(test_input, use_cache=True)\n",
    "        torch.cuda.synchronize()\n",
    "        prefill_time = time.time() - start_time\n",
    "        \n",
    "        # Decode éšæ®µæ¸¬è©¦\n",
    "        decode_times = []\n",
    "        for _ in range(num_decode_steps):\n",
    "            new_token = torch.randint(1, 1000, (1, 1), device=self.device)\n",
    "            start_time = time.time()\n",
    "            logits, past_kvs = self.model(new_token, past_kvs=past_kvs, use_cache=True)\n",
    "            torch.cuda.synchronize()\n",
    "            decode_times.append(time.time() - start_time)\n",
    "        \n",
    "        # çµ±è¨ˆçµæœ\n",
    "        avg_decode_time = np.mean(decode_times)\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "        \n",
    "        return {\n",
    "            'prefill_time_ms': prefill_time * 1000,\n",
    "            'avg_decode_time_ms': avg_decode_time * 1000,\n",
    "            'tokens_per_sec': 1.0 / avg_decode_time,\n",
    "            'peak_memory_gb': peak_memory,\n",
    "            'total_time_s': prefill_time + sum(decode_times)\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_text(self, input_ids, max_length=50, temperature=1.0):\n",
    "        \"\"\"ç”Ÿæˆæ–‡æœ¬ - ç¤ºä¾‹ç”¨é€”\"\"\"\n",
    "        generated = input_ids.clone()\n",
    "        past_kvs = None\n",
    "        \n",
    "        for _ in range(max_length - input_ids.size(1)):\n",
    "            if past_kvs is None:\n",
    "                # First forward pass\n",
    "                logits, past_kvs = self.model(generated, use_cache=True)\n",
    "                next_token_logits = logits[:, -1, :] / temperature\n",
    "            else:\n",
    "                # Subsequent passes with cache\n",
    "                logits, past_kvs = self.model(generated[:, -1:], past_kvs=past_kvs, use_cache=True)\n",
    "                next_token_logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            \n",
    "            # Simple stopping condition\n",
    "            if next_token.item() == 2:  # Assuming 2 is EOS token\n",
    "                break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "print(\"âœ… ç°¡åŒ–ç‰ˆæ¨ç†å¼•æ“æº–å‚™å°±ç·’\")\n",
    "print(\"   â€¢ æ”¯æ´åŸºæº–æ¸¬è©¦èˆ‡æ€§èƒ½åˆ†æ\")\n",
    "print(\"   â€¢ è‡ªå‹•è¨˜æ†¶é«”ç®¡ç†èˆ‡çµ±è¨ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. ç°¡åŒ–ç‰ˆæ¨ç†æ€§èƒ½æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_benchmark_test():\n",
    "    \"\"\"åŸ·è¡Œç°¡åŒ–çš„æ¨ç†æ€§èƒ½æ¸¬è©¦\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸš€ ç°¡åŒ–ç‰ˆæ¨ç†æ€§èƒ½æ¸¬è©¦\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ç°¡åŒ–çš„æ¸¬è©¦é…ç½®\n",
    "    test_configs = [\n",
    "        (\"MHA\", \"mha\"),     # MHA: 12 KV heads\n",
    "        (\"GQA-4\", \"gqa\"),   # GQA-4: 4 KV heads\n",
    "        (\"GQA-3\", \"gqa\"),   # GQA-3: 3 KV heads\n",
    "        (\"MQA\", \"mqa\"),     # MQA: 1 KV head\n",
    "    ]\n",
    "    \n",
    "    benchmark_results = []\n",
    "    \n",
    "    for name, attention_type in test_configs:\n",
    "        print(f\"\\nğŸ” æ¸¬è©¦ {name} ({attention_type})\")\n",
    "        \n",
    "        # å‰µå»ºæ¸¬è©¦é…ç½®\n",
    "        if name == \"GQA-3\":\n",
    "            config = ModelConfig(num_kv_groups=3)  # 12Ã·3=4 heads per group\n",
    "        else:\n",
    "            config = ModelConfig()\n",
    "        \n",
    "        # å»ºç«‹æ¸¬è©¦æ¨¡å‹\n",
    "        test_model = OptimizedLanguageModel(config, attention_type).to(device).half()\n",
    "        \n",
    "        # å‰µå»ºæ¨ç†å¼•æ“\n",
    "        engine = SimpleInferenceEngine(test_model)\n",
    "        \n",
    "        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦\n",
    "        result = engine.benchmark_model(seq_len=64, num_decode_steps=30)\n",
    "        \n",
    "        # æ·»åŠ é…ç½®ä¿¡æ¯\n",
    "        if attention_type == \"mha\":\n",
    "            num_kv_groups = config.num_heads\n",
    "        elif attention_type == \"mqa\":\n",
    "            num_kv_groups = 1\n",
    "        elif name == \"GQA-3\":\n",
    "            num_kv_groups = 3\n",
    "        else:  # GQA-4\n",
    "            num_kv_groups = 4\n",
    "        \n",
    "        result.update({\n",
    "            'name': name,\n",
    "            'attention_type': attention_type,\n",
    "            'num_kv_groups': num_kv_groups,\n",
    "            'num_heads': config.num_heads\n",
    "        })\n",
    "        benchmark_results.append(result)\n",
    "        \n",
    "        print(f\"   âœ… Prefill: {result['prefill_time_ms']:.2f}ms\")\n",
    "        print(f\"   âœ… Decode: {result['avg_decode_time_ms']:.3f}ms/token\")\n",
    "        print(f\"   âœ… Speed: {result['tokens_per_sec']:.1f} tok/s\")\n",
    "        print(f\"   âœ… Memory: {result['peak_memory_gb']:.3f}GB\")\n",
    "        \n",
    "        # æ¸…ç†æ¨¡å‹\n",
    "        del test_model, engine\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "# åŸ·è¡Œç°¡åŒ–æ¸¬è©¦\n",
    "print(\"ğŸš€ é–‹å§‹ç°¡åŒ–ç‰ˆæ€§èƒ½æ¸¬è©¦...\")\n",
    "benchmark_results = simple_benchmark_test()\n",
    "\n",
    "# ç°¡å–®çš„çµæœç¸½çµ\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“Š æ¸¬è©¦çµæœç¸½çµ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for result in benchmark_results:\n",
    "    print(f\"{result['name']:<8}: {result['tokens_per_sec']:.1f} tok/s, {result['peak_memory_gb']:.3f}GB\")\n",
    "\n",
    "# è¨ˆç®—ç›¸å°æ•ˆèƒ½\n",
    "mha_result = next(r for r in benchmark_results if r['name'] == 'MHA')\n",
    "print(f\"\\nğŸ¯ ç›¸å° MHA çš„æ€§èƒ½æå‡:\")\n",
    "for result in benchmark_results:\n",
    "    speedup = result['tokens_per_sec'] / mha_result['tokens_per_sec']\n",
    "    memory_ratio = mha_result['peak_memory_gb'] / result['peak_memory_gb']\n",
    "    print(f\"   {result['name']:<8}: {speedup:.2f}x é€Ÿåº¦, {memory_ratio:.2f}x è¨˜æ†¶é«”æ•ˆç‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. æœ€çµ‚ç¸½çµèˆ‡éƒ¨ç½²å»ºè­°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deployment_recommendations(benchmark_results):\n",
    "    \"\"\"åŸºæ–¼æ¸¬è©¦çµæœç”Ÿæˆéƒ¨ç½²å»ºè­°\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ¯ Lab-1.6 æ¨ç†å„ªåŒ–å¯¦æˆ°ç¸½çµå ±å‘Š\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # æ€§èƒ½æ’åº\n",
    "    sorted_by_speed = sorted(benchmark_results, key=lambda x: x['tokens_per_sec'], reverse=True)\n",
    "    sorted_by_memory = sorted(benchmark_results, key=lambda x: x['peak_memory_gb'])\n",
    "    \n",
    "    print(f\"\\nğŸš€ æ¨ç†é€Ÿåº¦æ’å:\")\n",
    "    print(f\"{'æ’å':<4} {'é…ç½®':<8} {'é€Ÿåº¦ (tok/s)':<15} {'ç›¸å°æå‡':<12}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    mha_speed = next(r['tokens_per_sec'] for r in benchmark_results if r['name'] == 'MHA')\n",
    "    for i, result in enumerate(sorted_by_speed, 1):\n",
    "        speedup = result['tokens_per_sec'] / mha_speed\n",
    "        print(f\"{i:<4} {result['name']:<8} {result['tokens_per_sec']:<15.1f} {speedup:<12.2f}x\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ è¨˜æ†¶é«”æ•ˆç‡æ’å:\")\n",
    "    print(f\"{'æ’å':<4} {'é…ç½®':<8} {'è¨˜æ†¶é«” (GB)':<15} {'ç›¸å°ç¯€çœ':<12}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    mha_memory = next(r['peak_memory_gb'] for r in benchmark_results if r['name'] == 'MHA')\n",
    "    for i, result in enumerate(sorted_by_memory, 1):\n",
    "        efficiency = mha_memory / result['peak_memory_gb']\n",
    "        print(f\"{i:<4} {result['name']:<8} {result['peak_memory_gb']:<15.3f} {efficiency:<12.2f}x\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š éƒ¨ç½²å ´æ™¯å»ºè­°:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # é‡å°ä¸åŒå ´æ™¯çš„å»ºè­°\n",
    "    scenarios = {\n",
    "        'ğŸš€ æ¥µè‡´æ€§èƒ½å ´æ™¯ (ChatBot, å³æ™‚æ‡‰ç”¨)': {\n",
    "            'recommended': 'MQA',\n",
    "            'reason': 'æœ€é«˜æ¨ç†é€Ÿåº¦ï¼Œæœ€ä½è¨˜æ†¶é«”å ç”¨',\n",
    "            'tradeoff': 'è¼•å¾®è³ªé‡æå¤±ï¼Œéœ€è¦è©•ä¼°å¯æ¥å—æ€§'\n",
    "        },\n",
    "        'âš–ï¸ å¹³è¡¡å ´æ™¯ (ç”Ÿç”¢æœå‹™, é€šç”¨éƒ¨ç½²)': {\n",
    "            'recommended': 'GQA-4',\n",
    "            'reason': 'è‰¯å¥½çš„æ€§èƒ½èˆ‡è³ªé‡å¹³è¡¡',\n",
    "            'tradeoff': 'é©ä¸­çš„è³‡æºéœ€æ±‚ï¼Œæ˜“æ–¼é·ç§»'\n",
    "        },\n",
    "        'ğŸ”¬ ç ”ç©¶å ´æ™¯ (åŸºæº–æ¸¬è©¦, è³ªé‡å„ªå…ˆ)': {\n",
    "            'recommended': 'MHA',\n",
    "            'reason': 'æœ€é«˜è³ªé‡ä¿è­‰ï¼Œå®Œæ•´è¡¨ç¾åŠ›',\n",
    "            'tradeoff': 'æœ€é«˜è³‡æºæ¶ˆè€—ï¼Œé©åˆè³ªé‡è¦æ±‚åš´æ ¼çš„å ´æ™¯'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario, info in scenarios.items():\n",
    "        print(f\"\\n{scenario}:\")\n",
    "        print(f\"   æ¨è–¦: {info['recommended']}\")\n",
    "        print(f\"   åŸå› : {info['reason']}\")\n",
    "        print(f\"   æ¬Šè¡¡: {info['tradeoff']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ› ï¸ å¯¦æ–½å»ºè­°:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # å¯¦éš›çš„æ•¸æ“šé©…å‹•å»ºè­°\n",
    "    mqa_result = next(r for r in benchmark_results if r['name'] == 'MQA')\n",
    "    gqa_result = next(r for r in benchmark_results if r['name'] == 'GQA-4')\n",
    "    \n",
    "    mqa_speedup = mqa_result['tokens_per_sec'] / mha_speed\n",
    "    gqa_speedup = gqa_result['tokens_per_sec'] / mha_speed\n",
    "    \n",
    "    print(f\"1. ğŸ“ˆ å¦‚æœè¿½æ±‚é€Ÿåº¦æå‡:\")\n",
    "    if mqa_speedup > 1.5:\n",
    "        print(f\"   â€¢ MQA æä¾› {mqa_speedup:.2f}x åŠ é€Ÿï¼Œé©åˆé«˜ååé‡éœ€æ±‚\")\n",
    "    else:\n",
    "        print(f\"   â€¢ MQA æä¾› {mqa_speedup:.2f}x åŠ é€Ÿï¼Œæå‡æœ‰é™ä½†è¨˜æ†¶é«”ç¯€çœé¡¯è‘—\")\n",
    "    \n",
    "    print(f\"\\n2. ğŸ¯ å¦‚æœéœ€è¦å¹³è¡¡æ–¹æ¡ˆ:\")\n",
    "    print(f\"   â€¢ GQA-4 æä¾› {gqa_speedup:.2f}x åŠ é€Ÿï¼Œè³ªé‡æå¤±æœ€å°\")\n",
    "    print(f\"   â€¢ æ¨è–¦ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒçš„é¦–é¸é…ç½®\")\n",
    "    \n",
    "    print(f\"\\n3. ğŸ’¾ è¨˜æ†¶é«”å—é™ç’°å¢ƒ:\")\n",
    "    mqa_memory_save = mha_memory / mqa_result['peak_memory_gb']\n",
    "    print(f\"   â€¢ MQA ç¯€çœ {mqa_memory_save:.2f}x è¨˜æ†¶é«”\")\n",
    "    print(f\"   â€¢ ç‰¹åˆ¥é©åˆé‚Šç·£è¨­å‚™å’Œè³‡æºå—é™å ´æ™¯\")\n",
    "    \n",
    "    print(f\"\\nğŸ”® ä¸‹ä¸€æ­¥å­¸ç¿’æ–¹å‘:\")\n",
    "    print(\"=\" * 25)\n",
    "    print(f\"â€¢ çµåˆ FlashAttention é€²ä¸€æ­¥å„ªåŒ–\")\n",
    "    print(f\"â€¢ æ¢ç´¢ vLLM ç­‰ç”Ÿç”¢ç´šæ¨ç†æ¡†æ¶\")\n",
    "    print(f\"â€¢ å­¸ç¿’æ¨¡å‹é‡åŒ–æŠ€è¡“ (INT8, FP4)\")\n",
    "    print(f\"â€¢ äº†è§£åˆ†æ•£å¼æ¨ç†èˆ‡æœå‹™åŒ–éƒ¨ç½²\")\n",
    "    print(f\"â€¢ å¯¦è¸ KV Cache é€²éšå„ªåŒ–æŠ€è¡“\")\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "# ç”Ÿæˆæœ€çµ‚å»ºè­°\n",
    "print(\"ğŸŠ æ ¹æ“šå¯¦éš›æ¸¬è©¦çµæœç”Ÿæˆéƒ¨ç½²å»ºè­°...\")\n",
    "deployment_recommendations = generate_deployment_recommendations(benchmark_results)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Lab-1.6 æ¨ç†å„ªåŒ–å¯¦æˆ°å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ‰ æ­å–œï¼æ‚¨å·²ç¶“æŒæ¡äº†:\")\n",
    "print(f\"   â€¢ MHA, GQA, MQA ä¸‰ç¨®æ¶æ§‹çš„å¯¦ä½œèˆ‡å„ªåŒ–\")\n",
    "print(f\"   â€¢ KV Cache æ©Ÿåˆ¶èˆ‡è¨˜æ†¶é«”ç®¡ç†\")\n",
    "print(f\"   â€¢ å¯¦éš›æ¨ç†æ€§èƒ½æ¸¬è©¦èˆ‡åˆ†æ\")\n",
    "print(f\"   â€¢ ç”Ÿç”¢éƒ¨ç½²çš„æ¶æ§‹é¸æ“‡ç­–ç•¥\")\n",
    "print(f\"\\nğŸš€ æ‚¨ç¾åœ¨å¯ä»¥åœ¨å¯¦éš›é …ç›®ä¸­æ‡‰ç”¨é€™äº›çŸ¥è­˜ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. é•·æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ§ª é•·æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹æ¸¬è©¦\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# é¸æ“‡ä¸€å€‹ GQA æ¨¡å‹é€²è¡Œç”Ÿæˆæ¸¬è©¦\n",
    "config = ModelConfig(num_kv_groups=4)\n",
    "demo_model = OptimizedLanguageModel(config, \"gqa\").to(device).half()\n",
    "demo_engine = SimpleInferenceEngine(demo_model)\n",
    "\n",
    "# ç¤ºä¾‹ç”Ÿæˆ\n",
    "input_ids = torch.randint(1, 1000, (1, 10), device=device)\n",
    "print(f\"è¼¸å…¥åºåˆ—é•·åº¦: {input_ids.size(1)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "generated = demo_engine.generate_text(input_ids, max_length=50)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"ç”Ÿæˆåºåˆ—é•·åº¦: {generated.size(1)}\")\n",
    "print(f\"ç”Ÿæˆæ™‚é–“: {(end_time - start_time)*1000:.2f}ms\")\n",
    "print(f\"ç”Ÿæˆé€Ÿåº¦: {(generated.size(1) - input_ids.size(1)) / (end_time - start_time):.1f} tok/s\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del demo_model, demo_engine\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nğŸŠ å®Œæ•´çš„æ¨ç†å„ªåŒ–å¯¦æˆ°å·²çµæŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## å¯¦é©—ç¸½çµ\n",
    "\n",
    "### æ¨è–¦é…ç½®\n",
    "\n",
    "**ç”Ÿç”¢éƒ¨ç½²**:\n",
    "- Llama-2 style: GQA-8 (32 Q heads, 8 KV groups)\n",
    "- å¹³è¡¡è³ªé‡èˆ‡é€Ÿåº¦\n",
    "- KV Cache æ¸›å°‘ 4x\n",
    "- æ¨ç†åŠ é€Ÿ 1.3-1.5x\n",
    "\n",
    "**æ¥µè‡´é€Ÿåº¦**:\n",
    "- Falcon style: MQA (32 Q heads, 1 KV head)\n",
    "- æœ€å¿«æ¨ç†é€Ÿåº¦\n",
    "- KV Cache æ¸›å°‘ 32x\n",
    "- æ¨ç†åŠ é€Ÿ 1.5-2x\n",
    "- è³ªé‡ç•¥æœ‰ä¸‹é™\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "- Lab-1.7: DPO Alignment\n",
    "- vLLM éƒ¨ç½²å¯¦è¸\n",
    "- ç”Ÿç”¢ç’°å¢ƒå„ªåŒ–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
