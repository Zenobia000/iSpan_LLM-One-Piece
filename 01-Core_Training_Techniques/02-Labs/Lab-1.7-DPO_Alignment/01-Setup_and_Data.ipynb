{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab-1.7: DPO (Direct Preference Optimization) 環境與數據準備\n",
    "\n",
    "**實驗目標**: 準備 DPO 訓練環境與偏好數據集\n",
    "\n",
    "**Direct Preference Optimization (DPO)** 是一種新穎的 RLHF 替代方案，由 Stanford 提出，能夠直接從偏好數據中優化語言模型，無需訓練獎勵模型或執行強化學習。\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "DPO 的主要優勢：\n",
    "- **無需獎勵模型**: 直接從偏好數據訓練，簡化 RLHF 流程\n",
    "- **穩定訓練**: 避免強化學習中的不穩定性問題\n",
    "- **高效實現**: 結合 PEFT 技術，在單 GPU 上實現大模型對齊\n",
    "- **理論保證**: 基於 Bradley-Terry 模型的理論基礎\n",
    "\n",
    "## 實驗流程\n",
    "\n",
    "1. **環境準備**: 安裝 TRL、PEFT 等必要庫\n",
    "2. **數據準備**: 載入偏好數據集並進行預處理\n",
    "3. **模型載入**: 準備基礎模型和 tokenizer\n",
    "4. **數據格式化**: 將偏好對轉換為 DPO 訓練格式\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 步驟 1: 環境檢查與安裝\n",
    "\n",
    "首先檢查 GPU 環境並安裝必要的套件。DPO 訓練需要：\n",
    "- `trl`: Transformer Reinforcement Learning 庫\n",
    "- `peft`: 參數高效微調\n",
    "- `bitsandbytes`: 量化支持\n",
    "- `accelerate`: 分散式訓練支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 GPU 環境\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(f'Python 版本: {sys.version}')\n",
    "print(f'PyTorch 版本: {torch.__version__}')\n",
    "print(f'CUDA 可用: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU 設備: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU 記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "    print(f'當前記憶體使用: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "# 設置隨機種子確保實驗可重現\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件 (如果尚未安裝)\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def install_and_import(package, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = package\n",
    "    \n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f'✅ {package} 已安裝')\n",
    "    except ImportError:\n",
    "        print(f'⚠️  安裝 {package}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f'✅ {package} 安裝完成')\n",
    "\n",
    "# 檢查必要套件\n",
    "install_and_import('trl')\n",
    "install_and_import('peft')\n",
    "install_and_import('bitsandbytes')\n",
    "install_and_import('accelerate')\n",
    "install_and_import('transformers')\n",
    "install_and_import('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要庫並檢查版本\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import datasets\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "\n",
    "print('=== 套件版本資訊 ===')\n",
    "print(f'🤗 Transformers: {transformers.__version__}')\n",
    "print(f'🎯 PEFT: {peft.__version__}')\n",
    "print(f'🚀 TRL: {trl.__version__}')\n",
    "print(f'📊 Datasets: {datasets.__version__}')\n",
    "print(f'⚡ Accelerate: {accelerate.__version__}')\n",
    "\n",
    "print('✅ DPO 環境準備完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 步驟 2: 偏好數據集載入與探索\n",
    "\n",
    "DPO 需要偏好數據，格式為 (prompt, chosen, rejected) 三元組：\n",
    "- **prompt**: 輸入提示\n",
    "- **chosen**: 偏好的回應 (高品質)\n",
    "- **rejected**: 非偏好的回應 (低品質)\n",
    "\n",
    "我們使用 Anthropic 的 HH-RLHF 數據集，這是人類偏好數據的經典數據集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 載入偏好數據集\n",
    "print('📥 載入 Anthropic HH-RLHF 數據集...')\n",
    "\n",
    "try:\n",
    "    # 先載入小樣本進行測試\n",
    "    dataset = load_dataset('Anthropic/hh-rlhf', split='train[:100]')\n",
    "    print(f'✅ 成功載入數據集，樣本數: {len(dataset)}')\n",
    "    \n",
    "    # 檢查數據格式\n",
    "    print('\\n=== 數據集結構 ===')\n",
    "    print(f'欄位: {list(dataset.features.keys())}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ 數據集載入失敗: {e}')\n",
    "    print('使用本地模擬數據...')\n",
    "    \n",
    "    # 創建模擬偏好數據\n",
    "    mock_data = {\n",
    "        'chosen': [\n",
    "            'Human: 請解釋什麼是機器學習?\\n\\nAssistant: 機器學習是人工智能的一個分支，它讓計算機能夠從數據中學習並做出決策，而無需明確編程。通過算法分析大量數據，系統可以識別模式並提高性能。',\n",
    "            'Human: 如何學習程式設計?\\n\\nAssistant: 學習程式設計建議從基礎語法開始，選擇一門適合的語言如Python，多做練習項目，參與開源專案，並持續學習新技術。實作是最重要的學習方式。'\n",
    "        ],\n",
    "        'rejected': [\n",
    "            'Human: 請解釋什麼是機器學習?\\n\\nAssistant: 機器學習就是讓機器變聰明。',\n",
    "            'Human: 如何學習程式設計?\\n\\nAssistant: 學程式就是寫code。'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_dict(mock_data)\n",
    "    print(f'✅ 創建模擬數據集，樣本數: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索數據集內容\n",
    "print('=== 數據樣本示例 ===')\n",
    "\n",
    "for i in range(min(2, len(dataset))):\n",
    "    sample = dataset[i]\n",
    "    print(f'\\n📝 樣本 {i+1}:')\n",
    "    \n",
    "    if 'chosen' in sample:\n",
    "        print('✅ 偏好回應 (chosen):')\n",
    "        print(sample['chosen'][:200] + '...' if len(sample['chosen']) > 200 else sample['chosen'])\n",
    "        print()\n",
    "        \n",
    "    if 'rejected' in sample:\n",
    "        print('❌ 非偏好回應 (rejected):')\n",
    "        print(sample['rejected'][:200] + '...' if len(sample['rejected']) > 200 else sample['rejected'])\n",
    "        print()\n",
    "    \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 步驟 3: 模型與 Tokenizer 準備\n",
    "\n",
    "DPO 訓練需要一個預訓練的基礎模型。我們使用較小的模型進行實驗，並配置量化以節省記憶體。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# 選擇模型 (使用較小的模型進行實驗)\n",
    "MODEL_NAME = 'microsoft/DialoGPT-medium'  # 或者使用 'gpt2' 進行快速測試\n",
    "\n",
    "print(f'📦 載入模型: {MODEL_NAME}')\n",
    "\n",
    "# 配置量化 (可選，用於大模型)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 載入 tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # 設置 pad token (DPO 需要)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f'✅ Tokenizer 載入成功')\n",
    "    print(f'詞彙大小: {len(tokenizer)}')\n",
    "    print(f'特殊 tokens: EOS={tokenizer.eos_token}, PAD={tokenizer.pad_token}')\n",
    "    \n",
    "    # 載入模型 (不量化進行快速測試)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    "        # quantization_config=quantization_config  # 如需量化請取消註解\n",
    "    )\n",
    "    \n",
    "    print(f'✅ 模型載入成功')\n",
    "    print(f'模型參數量: {model.num_parameters():,}')\n",
    "    print(f'模型設備: {next(model.parameters()).device}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ 模型載入失敗: {e}')\n",
    "    print('請檢查網路連接或嘗試其他模型')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 步驟 4: 數據預處理與格式化\n",
    "\n",
    "將偏好數據轉換為 DPO 訓練所需的格式。DPO 需要 prompt、chosen、rejected 三個欄位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_and_response(text):\n",
    "    \"\"\"從對話文本中提取提示和回應\"\"\"\n",
    "    if 'Human:' in text and 'Assistant:' in text:\n",
    "        parts = text.split('Assistant:')\n",
    "        if len(parts) >= 2:\n",
    "            prompt = parts[0].replace('Human:', '').strip()\n",
    "            response = parts[1].strip()\n",
    "            return prompt, response\n",
    "    return None, None\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"預處理數據集為 DPO 格式\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # 提取 chosen 和 rejected 的 prompt 和 response\n",
    "        chosen_prompt, chosen_response = extract_prompt_and_response(sample['chosen'])\n",
    "        rejected_prompt, rejected_response = extract_prompt_and_response(sample['rejected'])\n",
    "        \n",
    "        if chosen_prompt and chosen_response and rejected_response:\n",
    "            processed_data.append({\n",
    "                'prompt': chosen_prompt,\n",
    "                'chosen': chosen_response,\n",
    "                'rejected': rejected_response\n",
    "            })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# 預處理數據\n",
    "print('🔄 預處理偏好數據...')\n",
    "processed_data = preprocess_dataset(dataset)\n",
    "\n",
    "print(f'✅ 處理完成，有效樣本數: {len(processed_data)}')\n",
    "\n",
    "# 顯示處理後的樣本\n",
    "if processed_data:\n",
    "    print('\\n=== 處理後樣本示例 ===')\n",
    "    sample = processed_data[0]\n",
    "    print(f'📝 Prompt: {sample[\"prompt\"]}')\n",
    "    print(f'✅ Chosen: {sample[\"chosen\"]}')\n",
    "    print(f'❌ Rejected: {sample[\"rejected\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建最終的 DPO 數據集\n",
    "from datasets import Dataset\n",
    "\n",
    "if processed_data:\n",
    "    dpo_dataset = Dataset.from_list(processed_data)\n",
    "    print(f'📊 DPO 數據集創建完成')\n",
    "    print(f'樣本數: {len(dpo_dataset)}')\n",
    "    print(f'欄位: {list(dpo_dataset.features.keys())}')\n",
    "    \n",
    "    # 保存預處理數據 (可選)\n",
    "    output_dir = './dpo_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    dpo_dataset.save_to_disk(output_dir)\n",
    "    print(f'💾 數據集已保存至: {output_dir}')\n",
    "    \n",
    "else:\n",
    "    print('❌ 無有效的處理數據，請檢查數據格式')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 步驟 5: 實驗設置檢查\n",
    "\n",
    "最後確認所有組件準備就緒，為下一個 notebook 的 SFT 基線訓練做準備。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實驗設置總結\n",
    "print('=== DPO 實驗環境設置總結 ===')\n",
    "print(f'✅ PyTorch 版本: {torch.__version__}')\n",
    "print(f'✅ GPU 可用: {torch.cuda.is_available()}')\n",
    "print(f'✅ 模型: {MODEL_NAME}')\n",
    "print(f'✅ 數據集樣本數: {len(processed_data) if processed_data else 0}')\n",
    "print(f'✅ Tokenizer 詞彙大小: {len(tokenizer)}')\n",
    "\n",
    "# 測試 tokenization\n",
    "test_text = 'Hello, how are you?'\n",
    "tokens = tokenizer(test_text, return_tensors='pt')\n",
    "print(f'✅ Tokenization 測試成功: {test_text} -> {tokens[\"input_ids\"].shape[1]} tokens')\n",
    "\n",
    "print('🚀 準備進入下一步: SFT 基線訓練')\n",
    "print('請執行 02-SFT_Baseline.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}