{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: LoRA & QLoRA - Fine-Tuning a Llama-2 Model\n",
    "---\n",
    "## Notebook 2: The Training Process\n",
    "\n",
    "**Goal:** In this notebook, you will use QLoRA to fine-tune a `Llama-2-7B` model on an instruction-following dataset.\n",
    "\n",
    "**You will learn to:**\n",
    "-   Load a dataset for supervised fine-tuning and preprocess it with a tokenizer.\n",
    "-   Configure `BitsAndBytesConfig` to load a model in 4-bit precision (QLoRA).\n",
    "-   Load a pre-trained Llama-2 model for causal language modeling.\n",
    "-   Configure `peft.LoraConfig` to apply LoRA adapters to the model.\n",
    "-   Use the `transformers.Trainer` to efficiently fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Dataset and Preprocess\n",
    "\n",
    "First, we'll load our instruction-tuning dataset. We will use the `guanaco-llama2-1k` dataset, which is a small, high-quality dataset of prompts and responses formatted for Llama-2.\n",
    "\n",
    "#### Key Hugging Face Components:\n",
    "\n",
    "-   `datasets.load_dataset`: Fetches a dataset from the Hugging Face Hub.\n",
    "-   `transformers.AutoTokenizer`: Loads the appropriate tokenizer for our model.\n",
    "-   `dataset.map()`: A powerful method to apply a processing function to every example in the dataset. We use `batched=True` for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0137246b0b043cf9527d7f191b0d58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d30c35c9f54f969a7ff0f6d4144936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded and tokenized.\n",
      "Train samples: 900\n",
      "Test samples: 100\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"NousResearch/Llama-2-7b-hf\"\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Important for Causal LM\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# --- Preprocess and Tokenize ---\n",
    "def preprocess_function(examples):\n",
    "    # The 'text' field contains the full formatted prompt.\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
    "\n",
    "# Split the dataset\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Tokenize both splits\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"]) # Remove original text column\n",
    "\n",
    "print(\"✅ Dataset loaded and tokenized.\")\n",
    "print(f\"Train samples: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Test samples: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Base Model\n",
    "\n",
    "Now, we load the base model. The key to QLoRA is loading the base model in a quantized format.\n",
    "\n",
    "#### Key Hugging Face `transformers` Components:\n",
    "\n",
    "-   `transformers.BitsAndBytesConfig`: This configuration class is used to specify all the parameters for quantization.\n",
    "    -   `load_in_4bit=True`: This is the master switch to enable 4-bit loading.\n",
    "    -   `bnb_4bit_quant_type=\"nf4\"`: We use the \"nf4\" (Normalized Float 4) quantization type, which is recommended for QLoRA.\n",
    "    -   `bnb_4bit_compute_dtype=torch.bfloat16`: This sets the compute data type during the forward and backward passes. `bfloat16` is a good choice for modern GPUs.\n",
    "-   `transformers.AutoModelForCausalLM`: We use this to load our Llama-2 model, passing the `quantization_config` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20dbb420ae044eba0b943f053d5447f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# --- Quantization Configuration ---\n",
    "# Load model in 4-bit\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# --- Load Base Model ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"✅ Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure LoRA\n",
    "\n",
    "Now we configure LoRA using the `peft` library.\n",
    "\n",
    "#### Key Hugging Face `peft` Components:\n",
    "\n",
    "-   `peft.LoraConfig`: The main configuration class for LoRA.\n",
    "    -   `r`: The rank of the update matrices. A lower rank means fewer trainable parameters. A common range is 8-64.\n",
    "    -   `lora_alpha`: The scaling factor for the LoRA matrices. It's often set to twice the rank (`2*r`).\n",
    "    -   `target_modules`: A list of the names of the modules (e.g., attention layers) to apply LoRA to. For Llama models, this is typically `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`.\n",
    "    -   `lora_dropout`: Dropout probability for the LoRA layers to reduce overfitting.\n",
    "    -   `bias=\"none\"`: Specifies which biases to train. \"none\" is common.\n",
    "    -   `task_type=\"CAUSAL_LM\"`: Specifies the task type.\n",
    "-   `peft.get_peft_model`: This function takes the base model and the LoRA config and returns a PEFT model ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --- LoRA Configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# --- Create PEFT Model ---\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- Print Trainable Parameters ---\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set Up Training\n",
    "\n",
    "The final step is to configure and run the training process using the `transformers.Trainer`.\n",
    "\n",
    "#### Key Hugging Face Components:\n",
    "\n",
    "-   `transformers.TrainingArguments`: This class holds all the hyperparameters for the training run, such as learning rate, number of epochs, batch size, and logging settings.\n",
    "    -   `metric_for_best_model=\"perplexity\"`: We specify perplexity as the metric to determine the best model.\n",
    "    -   `greater_is_better=False`: Lower perplexity indicates better performance.\n",
    "-   `transformers.Trainer`: The standard trainer class from the `transformers` library. It requires a model, training arguments, datasets, a tokenizer, and a data collator.\n",
    "-   `transformers.DataCollatorForLanguageModeling`: This data collator will be used to form batches of tokenized data. It also handles the creation of the `labels` for causal language modeling, where the model predicts the next token.\n",
    "-   `compute_metrics`: A custom function to calculate **perplexity**, which is the standard evaluation metric for language modeling tasks. Perplexity measures how well the model predicts the next token - lower values indicate better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# --- Custom Evaluation Metrics ---\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute perplexity from the model's predictions.\n",
    "    Perplexity is the standard metric for language modeling tasks.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Convert to tensors\n",
    "    predictions = torch.from_numpy(predictions).float()\n",
    "    labels = torch.from_numpy(labels).long()\n",
    "    \n",
    "    # Shift for causal language modeling\n",
    "    shift_logits = predictions[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    # Calculate cross entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), \n",
    "        shift_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    # Calculate perplexity\n",
    "    try:\n",
    "        perplexity = math.exp(loss.item())\n",
    "    except OverflowError:\n",
    "        perplexity = float('inf')\n",
    "    \n",
    "    return {\n",
    "        \"perplexity\": perplexity,\n",
    "        \"eval_loss\": loss.item()\n",
    "    }\n",
    "\n",
    "# --- Data Collator ---\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-llama2-7b-guanaco\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=2,\n",
    "    eval_strategy=\"steps\",  # 改為 steps 以便更頻繁顯示指標\n",
    "    eval_steps=50,          # 每50步評估一次\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"perplexity\",\n",
    "    greater_is_better=False,  # Lower perplexity is better\n",
    "    logging_first_step=True,  # 記錄第一步\n",
    "    report_to=None,          # 避免外部報告干擾\n",
    ")\n",
    "\n",
    "# --- Create Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "print(\"🚀 Starting training with LoRA...\")\n",
    "trainer.train()\n",
    "print(\"✅ Training complete!\")\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "print(\"\\n📊 Final Evaluation Results:\")\n",
    "final_metrics = trainer.evaluate()\n",
    "print(f\"Final Perplexity: {final_metrics['eval_perplexity']:.4f}\")\n",
    "print(f\"Final Loss: {final_metrics['eval_loss']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Course (Poetry)",
   "language": "python",
   "name": "llm-course-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
