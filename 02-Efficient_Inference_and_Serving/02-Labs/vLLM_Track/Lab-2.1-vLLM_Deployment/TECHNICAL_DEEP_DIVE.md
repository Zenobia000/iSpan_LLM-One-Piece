# 技術深度剖析：vLLM、PagedAttention 與 Continuous Batching

本文檔旨在從工程角度，深入剖析 vLLM 的核心技術，特別是 PagedAttention 和 Continuous Batching。

---

### 1. 源起 (Origin)

vLLM 源於加州大學柏克萊分校 SkyLab 實驗室的一項研究。研究人員發現，當時主流的 LLM 服務框架（如 HuggingFace Transformers）在處理高併發推理請求時效率極低。問題的根源不在於計算本身，而在於對 GPU 記憶體，尤其是 KV Cache 的管理不善。這導致了大量的記憶體浪費和 GPU 閒置，從而限制了系統的整體吞吐量。

### 2. 解決痛點 (Pain Points Solved)

傳統 LLM 服務框架主要存在以下痛點：

1.  **嚴重的記憶體浪費與碎片化 (Memory Fragmentation & Waste)**：
    *   **問題**：傳統方法為每個推理請求預先分配一塊**連續**且**固定大小**的記憶體塊來儲存 KV Cache。這個大小必須等於模型的最大上下文長度（如 2048 或 4096）。如果一個請求的實際長度只有 100 tokens，那麼剩餘 1948 tokens 的空間就被浪費了。這種內部碎片化導致記憶體利用率極低，通常只有 20-40%。
    *   **後果**：由於記憶體被大量浪費，單個 GPU 能同時處理的請求數量（即批次大小）非常有限。

2.  **低吞吐量與 GPU 閒置 (Low Throughput & GPU Idleness)**：
    *   **問題**：由於上述記憶體限制，GPU 無法以足夠大的批次大小進行運算，導致其計算核心在大部分時間處於閒置狀態，等待記憶體 I/O。
    *   **後果**：硬體成本高昂的 GPU 未被充分利用，單位時間內處理的請求數和生成的 token 數遠低於其理論上限。

3.  **靜態批處理的低效 (Static Batching Inefficiency)**：
    *   **問題**：在批處理模式下，系統必須等待批次中**所有**請求都生成完畢，才能進行下一步。即使某些請求已經提前完成，它們佔用的資源也無法被釋放，必須等待最慢的那個請求。
    *   **後果**：進一步加劇了 GPU 的閒置，降低了系統的反應速度和整體效率。

### 3. 技術疊代 (Technical Iterations)

vLLM 的核心創新是借鑒了作業系統中虛擬記憶體和分頁的經典思想：

1.  **初始狀態 (HuggingFace Transformers)**：採用簡單的**連續記憶體分配**策略。每個請求都獲得一個 `[batch_size, num_heads, max_seq_len, head_dim]` 的大塊 VRAM。這種方式實現簡單，但效率極低，是 vLLM 要解決的起點。

2.  **核心創新 (PagedAttention)**：
    *   **概念**：PagedAttention 將 KV Cache 的記憶體空間**虛擬化**。它不再分配連續的大塊記憶體，而是將 VRAM 劃分成許多個固定大小（例如 16 tokens）的**物理塊 (Physical Blocks)**。一個請求的 KV Cache 則由一系列**邏輯塊 (Logical Blocks)** 組成，這些邏輯塊通過一個「頁表」對應到 VRAM 中任意位置的物理塊。
    *   **優勢**：
        *   **消除內部碎片化**：記憶體按需以塊為單位分配，浪費最多只會是一個塊內的幾個 token 空間，實現了近乎 100% 的記憶體利用率。
        *   **靈活的記憶體管理**：可以輕鬆實現複雜的共享機制，例如在多路徑搜索（Beam Search）或並行採樣時，不同的序列可以共享相同的邏輯塊，只需複製頁表即可，避免了 KV Cache 的重複儲存。

3.  **衍生優化 (Continuous Batching / In-flight Batching)**：
    *   **概念**：在 PagedAttention 的基礎上，vLLM 實現了更先進的**持續批處理**。傳統的靜態批處理是「同步」的，而持續批處理是「非同步」的。
    *   **流程**：
        1.  請求不斷進入一個隊列。
        2.  vLLM 的調度器在每個迭代步驟中，檢查是否有請求已完成。
        3.  一旦有請求完成，其佔用的物理塊會被**立即釋放**。
        4.  調度器會立刻從隊列中取出新的請求，分配釋放出的物理塊，並將其加入到正在運行的批次中。
    *   **優勢**：確保 GPU 在任何時候都處於滿負荷工作狀態，極大地提升了吞吐量，並顯著降低了請求的平均延遲。

### 4. 適用場域 (Applicable Scenarios)

*   **高吞吐量 API 服務**：如大型聊天機器人、內容生成平台等需要同時服務大量用戶的場景。
*   **請求長度變化大的場景**：當用戶輸入和模型輸出的長度不可預測時，PagedAttention 的優勢最為明顯。
*   **成本敏感型應用**：在相同硬體上實現數倍的性能提升，直接轉化為顯著的成本節省。
*   **低延遲要求的應用**：持續批處理能夠更快地處理新來的請求，降低了排隊等待時間。

### 5. 效益 (Benefits)

*   **極高的吞吐量**：根據官方數據，相比 HuggingFace Transformers (HF) 和 FasterTransformer (FT)，vLLM 可將吞吐量提升 **2-24倍**。
*   **高效的記憶體利用**：將 KV Cache 的記憶體浪費從 60-80% 降低到幾乎為零。
*   **靈活的架構**：易於集成，並提供與 OpenAI API 兼容的服務器，方便遷移。
*   **支援進階功能**：高效地支援複雜的採樣算法、長文本處理和多模型服務。

### 6. 先備知識 (Prerequisites)

*   **Transformer 架構**：必須深入理解 Self-Attention 機制，特別是 Query, Key, Value 的角色。
*   **KV Cache 機制**：清楚了解在自迴歸解碼過程中，KV Cache 為何產生、如何運作，以及其對性能的影響。
*   **作業系統 (OS) 核心概念**：對**虛擬記憶體 (Virtual Memory)**、**分頁 (Paging)** 和 **頁表 (Page Table)** 的理解是掌握 PagedAttention 的關鍵。
*   **GPU 硬體架構**：對 CUDA、VRAM (顯存)、記憶體頻寬等概念有基本認識。
