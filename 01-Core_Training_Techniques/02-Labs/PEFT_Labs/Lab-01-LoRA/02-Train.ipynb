{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1: LoRA & QLoRA - Fine-Tuning a Llama-2 Model\n",
        "---\n",
        "## Notebook 2: The Training Process\n",
        "\n",
        "**Goal:** This is the core of the lab. You will load a quantized base model and a dataset, configure LoRA, and then launch the training job using the `transformers` Trainer.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Use `BitsAndBytesConfig` to load a model in 4-bit precision (QLoRA).\n",
        "-   Load a dataset and prepare it for training.\n",
        "-   Deeply understand the parameters of `peft.LoraConfig` to control how LoRA is applied.\n",
        "-   Use `peft.get_peft_model` to create a trainable PEFT model.\n",
        "-   Configure `transformers.TrainingArguments` for the fine-tuning task.\n",
        "-   Use the `transformers.Trainer` to run the fine-tuning process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Model and Tokenizer\n",
        "\n",
        "First, we need to load our base model. We'll use `meta-llama/Llama-2-7b-chat-hf`. To make this large model fit into memory on a single consumer GPU, we'll use **4-bit quantization** via the `bitsandbytes` library.\n",
        "\n",
        "#### Key Hugging Face Components:\n",
        "\n",
        "-   `torch`: We set the default tensor type to `bfloat16`, a datatype optimized for deep learning that offers a good balance between range and precision.\n",
        "-   `transformers.BitsAndBytesConfig`: This is the configuration class that enables quantization.\n",
        "    -   `load_in_4bit=True`: Activates 4-bit loading.\n",
        "    -   `bnb_4bit_quant_type=\"nf4\"`: Specifies the \"NormalFloat4\" (nf4) quantization type, which is particularly effective for models with normally distributed weights, like LLMs.\n",
        "    -   `bnb_4bit_compute_dtype=torch.bfloat16`: While the model weights are stored in 4-bit, computations (like matrix multiplications during the forward and backward passes) are performed in 16-bit `bfloat16` for accuracy and stability. This is the essence of **QLoRA**.\n",
        "-   `transformers.AutoModelForCausalLM`: A generic model class that automatically loads the correct architecture for causal language models (like Llama-2).\n",
        "    -   `from_pretrained()`: The primary method for loading models from the Hugging Face Hub.\n",
        "    -   `quantization_config`: We pass our `BitsAndBytesConfig` here to apply quantization on-the-fly during model loading.\n",
        "    -   `device_map=\"auto\"`: This tells `accelerate` to automatically figure out the best way to distribute the model across available hardware (e.g., placing all of it on the GPU if it fits).\n",
        "-   `transformers.AutoTokenizer`: Loads the correct tokenizer for a given model.\n",
        "    -   `tokenizer.pad_token = tokenizer.eos_token`: For autoregressive models like Llama, there's often no dedicated padding token. A common practice is to use the End-Of-Sentence (`eos`) token for padding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14921f283f9640098b549147049ef48e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model and Tokenizer loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "# --- Set Default Tensor Type ---\n",
        "# ä½¿ç”¨ bfloat16 ä½œç‚ºé è¨­å¼µé‡é¡žåž‹ï¼Œé€™æ˜¯ä¸€ç¨®é‡å°æ·±åº¦å­¸ç¿’å„ªåŒ–çš„è³‡æ–™é¡žåž‹\n",
        "# åœ¨ä¿æŒæ•¸å€¼ç¯„åœçš„åŒæ™‚æä¾›é©ç•¶çš„ç²¾åº¦ï¼Œç‰¹åˆ¥é©åˆå¤§åž‹èªžè¨€æ¨¡åž‹\n",
        "torch.set_default_dtype(torch.bfloat16)\n",
        "\n",
        "# --- Quantization Configuration ---\n",
        "# This is the core of QLoRA\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# --- Load Tokenizer ---\n",
        "# Note: You'll need to have logged in to your Hugging Face account\n",
        "# using `huggingface-cli login` and have access to the Llama-2 model.\n",
        "\n",
        "# --- Set Cache Directory ---\n",
        "# Specify the download folder for pretrained models\n",
        "# The path r'D:\\Pretrained_models\\modelscope' is a Windows path.\n",
        "# Since you are running in a WSL environment, you need to use the Linux path format.\n",
        "# Windows drives are typically mounted under /mnt/ in WSL.\n",
        "# For example, D:\\ would become /mnt/d/.\n",
        "# Please modify the path below to match your actual directory in WSL.\n",
        "\n",
        "cache_dir = \"/mnt/d/Pretrained_models/modelscope\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, cache_dir=cache_dir)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Load Model ---\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(\"âœ… Model and Tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Configure LoRA with `peft`\n",
        "\n",
        "Now we apply the LoRA adapter to our quantized model.\n",
        "\n",
        "#### Key Hugging Face `peft` Components:\n",
        "\n",
        "-   `peft.LoraConfig`: This class holds all the configuration parameters for LoRA.\n",
        "    -   `r`: The **rank** of the update matrices. This is the most critical LoRA hyperparameter. It determines the number of trainable parameters. A higher `r` means more expressive power but also more parameters. A common range is 8-64. We'll use 16.\n",
        "    -   `lora_alpha`: The scaling factor for the LoRA updates. It's related to the learning rate. A common practice is to set `lora_alpha` to be double the `r` value.\n",
        "    -   `target_modules`: This is a list of the names of the modules (layers) in the base model that we want to apply LoRA to. For Llama models, this is typically the query (`q_proj`) and value (`v_proj`) projection matrices in the self-attention layers. Applying it to more layers can improve performance but increases trainable parameters.\n",
        "    -   `lora_dropout`: A standard dropout probability applied to the LoRA layers to prevent overfitting.\n",
        "    -   `bias=\"none\"`: Specifies which biases to train. `\"none\"` means no biases are trained, which is standard practice for LoRA.\n",
        "    -   `task_type=\"CAUSAL_LM\"`: Informs `peft` that we are performing a Causal Language Modeling task.\n",
        "-   `peft.get_peft_model`: This function takes the base model and the `LoraConfig` and returns a new `PeftModel`. This new model wraps the original, freezing all its weights and injecting the trainable LoRA adapters into the specified `target_modules`.\n",
        "\n",
        "After creating the `PeftModel`, we can use `print_trainable_parameters()` to see just how effective LoRA is at reducing the number of parameters we need to update.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# --- LoRA Configuration ---\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# --- Create PeftModel ---\n",
        "# This freezes the base model and adds the LoRA adapters\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "# See how few parameters we'll actually be training!\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Load Dataset and Set Up Training\n",
        "\n",
        "The final step is to configure and run the training process using the `transformers.Trainer`.\n",
        "\n",
        "#### Key Hugging Face Components:\n",
        "\n",
        "-   `datasets.load_dataset`: Loads the `mlabonne/guanaco-llama2-1k` dataset, which contains 1,000 instruction-response pairs formatted for Llama-2.\n",
        "-   `transformers.TrainingArguments`: A comprehensive class to define all training hyperparameters.\n",
        "    -   `output_dir`: Where to save model checkpoints and logs.\n",
        "    -   `per_device_train_batch_size`: The number of samples per batch on each GPU.\n",
        "    -   `gradient_accumulation_steps`: Simulates a larger batch size. The effective batch size is `batch_size * grad_accumulation_steps`. This is useful for fitting larger batches into limited GPU memory.\n",
        "    -   `learning_rate`: The learning rate for the optimizer (AdamW by default).\n",
        "    -   `num_train_epochs`: The total number of times to iterate over the dataset.\n",
        "    -   `logging_steps`: How often to log training progress.\n",
        "    -   `fp16=True`: Enables automatic mixed-precision training, which speeds up training and reduces memory usage without significant loss in precision.\n",
        "-   `transformers.Trainer`: This class orchestrates the entire training loop. It handles batching, optimization, logging, and saving checkpoints.\n",
        "    -   We provide it with our `peft_model`, the `training_args`, the `train_dataset`, and a `data_collator`.\n",
        "    -   `data_collator`: A function that takes a list of samples from the dataset and collates them into a single batch tensor. `DataCollatorForLanguageModeling` is used for language modeling tasks.\n",
        "\n",
        "With everything configured, calling `trainer.train()` kicks off the fine-tuning process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing dataset...\n",
            "âœ… Dataset preprocessed.\n",
            "[2025-08-22 00:50:29,228] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n",
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-08-22 00:50:31,127] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "ðŸš€ Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 05:38, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.431200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.329600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.311800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.214000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.229400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.186600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Training complete!\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Load Dataset ---\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "# The dataset is small, so we'll use the whole thing for training.\n",
        "dataset = load_dataset(dataset_name, split=\"train\", cache_dir=\"/mnt/d/Pretrained_models/modelscope\")\n",
        "\n",
        "# --- Preprocessing ---\n",
        "# We need to tokenize our dataset so the model can understand it.\n",
        "def preprocess_function(examples):\n",
        "    # The guanaco dataset format is a single 'text' field.\n",
        "    # We will tokenize it and set the labels to be the same as the input_ids\n",
        "    # for the causal language modeling task.\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"text\"], \n",
        "        truncation=True, \n",
        "        padding=\"max_length\", \n",
        "        max_length=512 # You can adjust this based on your VRAM and typical sequence length\n",
        "    )\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"Preprocessing dataset...\")\n",
        "# We use remove_columns to get rid of the original 'text' column, which the Trainer doesn't need.\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n",
        "print(\"âœ… Dataset preprocessed.\")\n",
        "\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=\"./lora-llama2-7b-guanaco\",\n",
        "    per_device_train_batch_size=4, # Reduced from 16 to 4 to prevent potential OOM errors\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    fp16=True,  # Use mixed precision\n",
        "    save_total_limit=2,\n",
        "    overwrite_output_dir=True,\n",
        ")\n",
        "\n",
        "# --- Create Trainer ---\n",
        "# We pass the tokenized_dataset to the trainer.\n",
        "trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# A known issue with gradient checkpointing and PEFT requires this\n",
        "peft_model.config.use_cache = False\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM Course (Poetry)",
      "language": "python",
      "name": "llm-course-poetry"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
