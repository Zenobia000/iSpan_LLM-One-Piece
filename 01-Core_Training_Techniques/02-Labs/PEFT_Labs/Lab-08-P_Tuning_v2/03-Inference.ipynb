{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 8: P-Tuning v2 - Deep Prompt Tuning for Diverse Tasks\n",
        "---\n",
        "## Notebook 3: Inference\n",
        "\n",
        "**Goal:** In this notebook, you will load the trained P-Tuning v2 adapter and use the fine-tuned model to perform various tasks, demonstrating the universal effectiveness of deep prompt tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Reload Model and Adapter\n",
        "\n",
        "We will load the base BERT model and then apply our trained P-Tuning v2 weights on top of it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- Load Base Model and Tokenizer ---\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, \n",
        "    num_labels=2\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# --- Load PEFT Adapter ---\n",
        "output_dir = \"./bert-ptuning-v2-cola\"\n",
        "latest_checkpoint = max(\n",
        "    [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")],\n",
        "    key=os.path.getmtime\n",
        ")\n",
        "print(f\"Loading adapter from: {latest_checkpoint}\")\n",
        "\n",
        "inference_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inference_model.to(device)\n",
        "inference_model.eval()\n",
        "\n",
        "print(\"âœ… P-Tuning v2 fine-tuned model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Perform Linguistic Acceptability Classification\n",
        "\n",
        "Now, let's test the model by performing linguistic acceptability classification on new sentences. We'll test sentences that are grammatically correct vs. incorrect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the labels\n",
        "id2label = {0: \"Unacceptable\", 1: \"Acceptable\"}\n",
        "\n",
        "def predict_acceptability(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = inference_model(**inputs)\n",
        "    \n",
        "    logits = outputs.logits\n",
        "    probabilities = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "    prediction = torch.argmax(logits, dim=-1).cpu().item()\n",
        "    \n",
        "    print(f\"Sentence: '{text}'\")\n",
        "    print(f\"Prediction: {id2label[prediction]}\")\n",
        "    print(f\"Confidence: {probabilities[prediction]:.4f}\")\n",
        "    print(f\"Probabilities:\")\n",
        "    print(f\"  - {id2label[0]}: {probabilities[0]:.4f}\")\n",
        "    print(f\"  - {id2label[1]}: {probabilities[1]:.4f}\")\n",
        "    print()\n",
        "\n",
        "# --- Test Cases ---\n",
        "# Mix of grammatically correct and incorrect sentences\n",
        "test_sentences = [\n",
        "    \"The cat sat on the mat.\",                           # Correct\n",
        "    \"The cat sat on mat the.\",                           # Incorrect (word order)\n",
        "    \"She enjoys reading books in the library.\",          # Correct\n",
        "    \"Books enjoys reading she in library the.\",          # Incorrect (scrambled)\n",
        "    \"The students are studying for their exams.\",        # Correct\n",
        "    \"Students the are studying for exams their.\",        # Incorrect (word order)\n",
        "    \"I think that this sentence is grammatically correct.\", # Correct\n",
        "    \"Think I that sentence this is correct grammatically.\", # Incorrect (word order)\n",
        "    \"The weather is beautiful today.\",                   # Correct\n",
        "    \"Weather the is today beautiful.\",                   # Incorrect (word order)\n",
        "]\n",
        "\n",
        "print(\"--- P-Tuning v2 Linguistic Acceptability Results ---\")\n",
        "print(\"Testing the model's ability to distinguish grammatically correct from incorrect sentences:\\n\")\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    predict_acceptability(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Lab Conclusion\n",
        "\n",
        "This concludes the P-Tuning v2 lab. You have successfully implemented deep prompt tuning that applies to every transformer layer, achieving:\n",
        "\n",
        "- **Universal Effectiveness**: P-Tuning v2 works across various tasks and model scales\n",
        "- **Extreme Parameter Efficiency**: Only ~0.1% of parameters while matching full fine-tuning performance\n",
        "- **Simplified Architecture**: No need for complex MLP encoders like P-Tuning v1\n",
        "- **Strong Performance**: Comparable to full fine-tuning with minimal overhead\n",
        "\n",
        "P-Tuning v2 represents the evolution of prompt-based methods toward universal and efficient fine-tuning!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
