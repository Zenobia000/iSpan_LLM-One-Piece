{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 - é«˜æ•ˆæ•¸æ“šç¯©é¸\n",
    "## Notebook 01: æ•¸æ“šæº–å‚™èˆ‡ç’°å¢ƒé…ç½®\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "1. åŠ è¼‰ Alpaca æŒ‡ä»¤æ•¸æ“šé›† (52K æ¨£æœ¬)\n",
    "2. é€²è¡Œçµ±è¨ˆåˆ†æ (é•·åº¦ã€é ˜åŸŸåˆ†å¸ƒ)\n",
    "3. æº–å‚™ Sentence-BERT æ¨¡å‹\n",
    "4. è¨­å®šç¯©é¸ç›®æ¨™ (30% ä¿ç•™ç‡)\n",
    "5. å¯è¦–åŒ–æ•¸æ“šåˆ†å¸ƒ\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 30-45 åˆ†é˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæª¢æŸ¥\n",
    "\n",
    "é¦–å…ˆé©—è­‰å¿…è¦çš„ä¾è³´æ˜¯å¦å®‰è£æ­£ç¢ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ç’°å¢ƒæª¢æŸ¥\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Python ç‰ˆæœ¬\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "# æª¢æŸ¥æ ¸å¿ƒä¾è³´\n",
    "dependencies = {\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn',\n",
    "    'sentence-transformers': 'sentence_transformers',\n",
    "    'scikit-learn': 'sklearn',\n",
    "    'datasets': 'datasets',\n",
    "    'tqdm': 'tqdm'\n",
    "}\n",
    "\n",
    "missing = []\n",
    "for name, module in dependencies.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"âœ… {name}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {name} (æœªå®‰è£)\")\n",
    "        missing.append(name)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nâš ï¸  ç¼ºå°‘ä¾è³´: {', '.join(missing)}\")\n",
    "    print(f\"å®‰è£æŒ‡ä»¤: pip install {' '.join(missing)}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… æ‰€æœ‰ä¾è³´å·²å®‰è£\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å°å…¥ä¾è³´\n",
    "\n",
    "å°å…¥æ‰€éœ€çš„ Python å¥—ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# è¨­å®šå¯è¦–åŒ–é¢¨æ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ä¾è³´å°å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å»ºç«‹ç›®éŒ„çµæ§‹\n",
    "\n",
    "å»ºç«‹ç”¨æ–¼å­˜æ”¾æ•¸æ“šã€åˆ†æçµæœå’Œè¼¸å‡ºçš„ç›®éŒ„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ç›®éŒ„çµæ§‹\n",
    "directories = {\n",
    "    'data': Path('./data'),\n",
    "    'analysis': Path('./analysis'),\n",
    "    'results': Path('./results'),\n",
    "    'validation': Path('./validation'),\n",
    "    'pipeline': Path('./pipeline')\n",
    "}\n",
    "\n",
    "for name, path in directories.items():\n",
    "    path.mkdir(exist_ok=True)\n",
    "    print(f\"âœ… å»ºç«‹ç›®éŒ„: {path}\")\n",
    "\n",
    "print(f\"\\nç›®éŒ„çµæ§‹å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åŠ è¼‰ Alpaca æ•¸æ“šé›†\n",
    "\n",
    "åŠ è¼‰ Stanford Alpaca æ•¸æ“šé›† (52K æ¨£æœ¬)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ åŠ è¼‰ Alpaca æ•¸æ“šé›†...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # å¾ HuggingFace åŠ è¼‰ Alpaca æ•¸æ“šé›†\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "    \n",
    "    # è½‰æ›ç‚º list of dicts\n",
    "    raw_data = []\n",
    "    for item in dataset:\n",
    "        sample = {\n",
    "            'instruction': item['instruction'],\n",
    "            'input': item.get('input', ''),\n",
    "            'output': item['output']\n",
    "        }\n",
    "        raw_data.append(sample)\n",
    "    \n",
    "    print(f\"âœ… æ•¸æ“šé›†åŠ è¼‰æˆåŠŸ\")\n",
    "    print(f\"   ç¸½æ¨£æœ¬æ•¸: {len(raw_data):,}\")\n",
    "    \n",
    "    # ä¿å­˜åŸå§‹æ•¸æ“š\n",
    "    raw_data_path = directories['data'] / 'alpaca_raw.json'\n",
    "    with open(raw_data_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(raw_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"   å·²ä¿å­˜è‡³: {raw_data_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ éŒ¯èª¤: {e}\")\n",
    "    print(\"\\nğŸ’¡ æç¤º: å˜—è©¦ä½¿ç”¨æœ¬åœ°æ•¸æ“šæˆ–å…¶ä»–æ•¸æ“šé›†\")\n",
    "    raw_data = []\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æŸ¥çœ‹æ¨£æœ¬ç¯„ä¾‹\n",
    "\n",
    "é¡¯ç¤ºå¹¾å€‹æ•¸æ“šæ¨£æœ¬ä»¥äº†è§£æ•¸æ“šæ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data:\n",
    "    print(\"\\nğŸ“„ æ•¸æ“šæ¨£æœ¬:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i in range(min(3, len(raw_data))):\n",
    "        sample = raw_data[i]\n",
    "        print(f\"\\næ¨£æœ¬ {i+1}:\")\n",
    "        print(f\"  æŒ‡ä»¤: {sample['instruction']}\")\n",
    "        if sample['input']:\n",
    "            print(f\"  è¼¸å…¥: {sample['input']}\")\n",
    "        print(f\"  è¼¸å‡º: {sample['output'][:100]}...\" if len(sample['output']) > 100 else f\"  è¼¸å‡º: {sample['output']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸  æ•¸æ“šæœªåŠ è¼‰,è·³éæ¨£æœ¬é¡¯ç¤º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ•¸æ“šçµ±è¨ˆåˆ†æ\n",
    "\n",
    "åˆ†ææ•¸æ“šé›†çš„åŸºæœ¬çµ±è¨ˆç‰¹å¾µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data:\n",
    "    print(\"ğŸ“Š æ•¸æ“šçµ±è¨ˆåˆ†æ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # è¨ˆç®—æ–‡æœ¬é•·åº¦ (å­—ç¬¦æ•¸)\n",
    "    instruction_lengths = [len(s['instruction']) for s in raw_data]\n",
    "    input_lengths = [len(s['input']) for s in raw_data]\n",
    "    output_lengths = [len(s['output']) for s in raw_data]\n",
    "    \n",
    "    # è¨ˆç®—è©æ•¸ (ç°¡å–®åˆ†å‰²)\n",
    "    instruction_words = [len(s['instruction'].split()) for s in raw_data]\n",
    "    output_words = [len(s['output'].split()) for s in raw_data]\n",
    "    \n",
    "    stats = {\n",
    "        'ç¸½æ¨£æœ¬æ•¸': len(raw_data),\n",
    "        'å¹³å‡æŒ‡ä»¤é•·åº¦ (å­—ç¬¦)': np.mean(instruction_lengths),\n",
    "        'å¹³å‡è¼¸å‡ºé•·åº¦ (å­—ç¬¦)': np.mean(output_lengths),\n",
    "        'å¹³å‡æŒ‡ä»¤è©æ•¸': np.mean(instruction_words),\n",
    "        'å¹³å‡è¼¸å‡ºè©æ•¸': np.mean(output_words),\n",
    "        'æœ‰è¼¸å…¥çš„æ¨£æœ¬æ•¸': sum(1 for s in raw_data if s['input']),\n",
    "        'ç„¡è¼¸å…¥çš„æ¨£æœ¬æ•¸': sum(1 for s in raw_data if not s['input'])\n",
    "    }\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value:,}\")\n",
    "    \n",
    "    # ä¿å­˜çµ±è¨ˆä¿¡æ¯\n",
    "    stats_path = directories['analysis'] / 'data_statistics.json'\n",
    "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nâœ… çµ±è¨ˆä¿¡æ¯å·²ä¿å­˜è‡³: {stats_path}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸  æ•¸æ“šæœªåŠ è¼‰,è·³éçµ±è¨ˆåˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å¯è¦–åŒ–æ•¸æ“šåˆ†å¸ƒ\n",
    "\n",
    "ç”Ÿæˆæ•¸æ“šåˆ†å¸ƒçš„å¯è¦–åŒ–åœ–è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data:\n",
    "    print(\"ğŸ“ˆ ç”Ÿæˆæ•¸æ“šåˆ†å¸ƒåœ–è¡¨...\")\n",
    "    \n",
    "    # å»ºç«‹å­åœ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Alpaca æ•¸æ“šé›†åˆ†å¸ƒåˆ†æ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. æŒ‡ä»¤é•·åº¦åˆ†å¸ƒ\n",
    "    axes[0, 0].hist(instruction_words, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(np.mean(instruction_words), color='red', linestyle='--', \n",
    "                       label=f'å‡å€¼: {np.mean(instruction_words):.1f}')\n",
    "    axes[0, 0].set_xlabel('è©æ•¸')\n",
    "    axes[0, 0].set_ylabel('æ¨£æœ¬æ•¸')\n",
    "    axes[0, 0].set_title('æŒ‡ä»¤é•·åº¦åˆ†å¸ƒ')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. è¼¸å‡ºé•·åº¦åˆ†å¸ƒ\n",
    "    axes[0, 1].hist(output_words, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[0, 1].axvline(np.mean(output_words), color='red', linestyle='--', \n",
    "                       label=f'å‡å€¼: {np.mean(output_words):.1f}')\n",
    "    axes[0, 1].set_xlabel('è©æ•¸')\n",
    "    axes[0, 1].set_ylabel('æ¨£æœ¬æ•¸')\n",
    "    axes[0, 1].set_title('è¼¸å‡ºé•·åº¦åˆ†å¸ƒ')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. æŒ‡ä»¤ vs è¼¸å‡ºé•·åº¦æ•£é»åœ–\n",
    "    sample_size = min(5000, len(raw_data))\n",
    "    sample_indices = np.random.choice(len(raw_data), sample_size, replace=False)\n",
    "    sample_instr = [instruction_words[i] for i in sample_indices]\n",
    "    sample_out = [output_words[i] for i in sample_indices]\n",
    "    \n",
    "    axes[1, 0].scatter(sample_instr, sample_out, alpha=0.3, s=10)\n",
    "    axes[1, 0].set_xlabel('æŒ‡ä»¤é•·åº¦ (è©æ•¸)')\n",
    "    axes[1, 0].set_ylabel('è¼¸å‡ºé•·åº¦ (è©æ•¸)')\n",
    "    axes[1, 0].set_title(f'æŒ‡ä»¤èˆ‡è¼¸å‡ºé•·åº¦é—œä¿‚ (æ¨£æœ¬: {sample_size:,})')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. è¼¸å…¥æ•¸æ“šçµ±è¨ˆ\n",
    "    has_input = sum(1 for s in raw_data if s['input'])\n",
    "    no_input = len(raw_data) - has_input\n",
    "    \n",
    "    axes[1, 1].pie([has_input, no_input], \n",
    "                   labels=[f'æœ‰è¼¸å…¥\\n({has_input:,})', f'ç„¡è¼¸å…¥\\n({no_input:,})'],\n",
    "                   autopct='%1.1f%%',\n",
    "                   startangle=90,\n",
    "                   colors=['#66b3ff', '#ff9999'])\n",
    "    axes[1, 1].set_title('æ¨£æœ¬è¼¸å…¥åˆ†å¸ƒ')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜åœ–è¡¨\n",
    "    fig_path = directories['analysis'] / 'data_distribution.png'\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ… åœ–è¡¨å·²ä¿å­˜è‡³: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  æ•¸æ“šæœªåŠ è¼‰,è·³éå¯è¦–åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æº–å‚™ Sentence-BERT æ¨¡å‹\n",
    "\n",
    "åŠ è¼‰ Sentence-BERT æ¨¡å‹ç”¨æ–¼å¾ŒçºŒçš„ IFD è¨ˆç®—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ åŠ è¼‰ Sentence-BERT æ¨¡å‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ä½¿ç”¨å¹³è¡¡è³ªé‡èˆ‡é€Ÿåº¦çš„æ¨¡å‹\n",
    "MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(MODEL_NAME)\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ: {MODEL_NAME}\")\n",
    "    print(f\"   æœ€å¤§åºåˆ—é•·åº¦: {embedding_model.max_seq_length}\")\n",
    "    print(f\"   åµŒå…¥ç¶­åº¦: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    # æ¸¬è©¦ç·¨ç¢¼\n",
    "    test_text = \"This is a test sentence.\"\n",
    "    test_embedding = embedding_model.encode([test_text])\n",
    "    print(f\"   æ¸¬è©¦ç·¨ç¢¼: {test_embedding.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ éŒ¯èª¤: {e}\")\n",
    "    print(\"\\nğŸ’¡ æç¤º: å¯èƒ½éœ€è¦ç¶²è·¯é€£æ¥ä¸‹è¼‰æ¨¡å‹\")\n",
    "    embedding_model = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è¨­å®šç¯©é¸ç›®æ¨™\n",
    "\n",
    "å®šç¾©æ•¸æ“šç¯©é¸çš„ç›®æ¨™å’Œåƒæ•¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯©é¸é…ç½®\n",
    "filtering_config = {\n",
    "    'source_dataset': 'Alpaca',\n",
    "    'total_samples': len(raw_data) if raw_data else 0,\n",
    "    'target_retention_rate': 0.30,\n",
    "    'target_samples': int(len(raw_data) * 0.30) if raw_data else 0,\n",
    "    \n",
    "    # IFD åƒæ•¸\n",
    "    'ifd_min_threshold': 0.3,\n",
    "    'ifd_max_threshold': 0.9,\n",
    "    \n",
    "    # DEITA æ¬Šé‡\n",
    "    'deita_alpha': 0.4,  # è¤‡é›œåº¦æ¬Šé‡\n",
    "    'deita_beta': 0.4,   # è³ªé‡æ¬Šé‡\n",
    "    'deita_gamma': 0.2,  # å¤šæ¨£æ€§æ¬Šé‡\n",
    "    \n",
    "    # å…¶ä»–åƒæ•¸\n",
    "    'embedding_model': MODEL_NAME,\n",
    "    'batch_size': 64,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ ç¯©é¸é…ç½®\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(filtering_config, indent=2, ensure_ascii=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ä¿å­˜é…ç½®\n",
    "config_path = directories['data'] / 'filtering_config.json'\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(filtering_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nâœ… é…ç½®å·²ä¿å­˜è‡³: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç’°å¢ƒé©—è­‰ç¸½çµ\n",
    "\n",
    "æª¢æŸ¥æ‰€æœ‰è¨­ç½®æ˜¯å¦æ­£ç¢ºå®Œæˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ç’°å¢ƒé©—è­‰ç¸½çµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = [\n",
    "    (\"æ•¸æ“šé›†åŠ è¼‰\", len(raw_data) > 0 if raw_data else False),\n",
    "    (\"çµ±è¨ˆåˆ†æå®Œæˆ\", (directories['analysis'] / 'data_statistics.json').exists()),\n",
    "    (\"å¯è¦–åŒ–ç”Ÿæˆ\", (directories['analysis'] / 'data_distribution.png').exists()),\n",
    "    (\"Sentence-BERT æ¨¡å‹\", embedding_model is not None),\n",
    "    (\"ç¯©é¸é…ç½®å»ºç«‹\", (directories['data'] / 'filtering_config.json').exists())\n",
    "]\n",
    "\n",
    "for check_name, status in checks:\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"{status_icon} {check_name}\")\n",
    "\n",
    "all_passed = all(status for _, status in checks)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_passed:\n",
    "    print(\"âœ… æ‰€æœ‰æª¢æŸ¥é€šé! å¯ä»¥ç¹¼çºŒé€²è¡Œç¯©é¸ (Notebook 02)\")\n",
    "    print(f\"\\né æœŸç¯©é¸çµæœ: {filtering_config['total_samples']:,} â†’ {filtering_config['target_samples']:,} æ¨£æœ¬ (ä¿ç•™ 30%)\")\n",
    "else:\n",
    "    print(\"âš ï¸  éƒ¨åˆ†æª¢æŸ¥æœªé€šé,è«‹è§£æ±ºä¸Šè¿°å•é¡Œå¾Œå†ç¹¼çºŒ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ç¸½çµ\n",
    "\n",
    "åœ¨æœ¬ notebook ä¸­,æˆ‘å€‘å®Œæˆäº†:\n",
    "\n",
    "1. âœ… ç’°å¢ƒæª¢æŸ¥èˆ‡ä¾è³´é©—è­‰\n",
    "2. âœ… åŠ è¼‰ Alpaca æ•¸æ“šé›† (52K æ¨£æœ¬)\n",
    "3. âœ… æ•¸æ“šçµ±è¨ˆåˆ†æ (é•·åº¦ã€åˆ†å¸ƒ)\n",
    "4. âœ… ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨\n",
    "5. âœ… æº–å‚™ Sentence-BERT æ¨¡å‹\n",
    "6. âœ… è¨­å®šç¯©é¸é…ç½® (30% ä¿ç•™ç‡)\n",
    "\n",
    "### é—œéµè§€å¯Ÿ\n",
    "\n",
    "- å¹³å‡æŒ‡ä»¤é•·åº¦: ~15 è©\n",
    "- å¹³å‡è¼¸å‡ºé•·åº¦: ~78 è©\n",
    "- è¼¸å…¥åˆ†å¸ƒ: ç´„ 40% æ¨£æœ¬æœ‰é¡å¤–è¼¸å…¥\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ **02-Filter.ipynb** åŸ·è¡Œ IFD å’Œ DEITA ç¯©é¸æµç¨‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**è¨˜å¾—**:\n",
    "- ç¯©é¸ç›®æ¨™: å¾ 52K æ¨£æœ¬ä¸­ç¯©é¸å‡º 15.6K é«˜è³ªé‡æ¨£æœ¬\n",
    "- IFD é–¾å€¼: 0.3 â‰¤ IFD â‰¤ 0.9\n",
    "- DEITA æ¬Šé‡: è¤‡é›œåº¦ 40%, è³ªé‡ 40%, å¤šæ¨£æ€§ 20%\n",
    "- é æœŸæ•ˆæœ: è³ªé‡æå‡,è¨“ç·´æ™‚é–“æ¸›å°‘ 70%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
