{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. 推理與評估\n",
        "\n",
        "模型經過 LoRA 微調後，本筆記本將示範如何使用這個帶有 PEFT adapter 的模型進行推理，並觀察其在特定任務上的表現。\n",
        "\n",
        "## 步驟 1: 準備模型進行推理\n",
        "\n",
        "在訓練完成後，`peft` 模型會自動儲存 adapter 的權重。要進行推理，我們需要重新載入基礎模型和 adapter。\n",
        "\n",
        "**重要提示**: 由於我們在訓練時使用了 4-bit 量化，推理時也必須以相同的量化設定載入基礎模型，否則將無法正確載入 adapter 權重。\n",
        "\n",
        "我們將從 `./lora-llama2-7b-chat` 目錄中最新的 checkpoint 載入我們訓練好的 LoRA adapter。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c789921eecbe4a1d9d8621189cc0c35f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "從 ./lora-llama2-7b-chat/checkpoint-21 載入 adapter\n",
            "模型準備完成，可以開始推理。\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "# --- 重新載入基礎模型與 Tokenizer ---\n",
        "# (與 02, 03 筆記本中的設定相同)\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 載入 LoRA Adapter ---\n",
        "# 找到最新的 checkpoint\n",
        "output_dir = \"./lora-llama2-7b-chat\"\n",
        "latest_checkpoint = max([os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")], key=os.path.getmtime)\n",
        "print(f\"從 {latest_checkpoint} 載入 adapter\")\n",
        "\n",
        "# 從 checkpoint 載入 PeftModel\n",
        "inference_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "print(\"模型準備完成，可以開始推理。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 2: 進行文本生成\n",
        "\n",
        "現在我們可以使用微調後的模型來生成文本了。我們將提供一個提示 (prompt)，並觀察模型的回答是否比原始的 Llama-2-chat 模型更符合我們的微調資料集風格。\n",
        "\n",
        "`guanaco-llama2-1k` 資料集的風格是問答式的。我們將使用一個類似的問題來測試模型。\n",
        "\n",
        "推理流程如下：\n",
        "1.  將提示文本編碼為 `input_ids`。\n",
        "2.  使用模型的 `generate()` 方法生成文本。\n",
        "3.  將生成的 `output_ids` 解碼回人類可讀的文本。\n",
        "\n",
        "`generate()` 方法的參數：\n",
        "- `max_new_tokens`: 控制生成文本的最大長度。\n",
        "- `do_sample=True`: 啟用採樣，使生成更多樣。\n",
        "- `top_k`: Top-K 採樣。\n",
        "- `num_return_sequences`: 指定生成幾個不同的序列。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型生成的回應：\n",
            "請向我介紹一下台灣的玉山國家公園。 sierp. I am interested in visiting Taiwan and learning more about its culture and history. Can you tell me about the attractions and activities available in the area?\n"
          ]
        }
      ],
      "source": [
        "# 準備提示\n",
        "prompt = \"請向我介紹一下台灣的玉山國家公園\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "\n",
        "# 進行推理\n",
        "# attention_mask is not strictly necessary for generation with Llama models, \n",
        "# but it's good practice to include it.\n",
        "with torch.no_grad():\n",
        "    outputs = inference_model.generate(\n",
        "        input_ids=input_ids, \n",
        "        max_new_tokens=256, \n",
        "        do_sample=True, \n",
        "        top_k=50,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "# 解碼並打印結果\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"模型生成的回應：\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型生成的回應：\n",
            "我很生氣，情緒是? 很生氣，情緒是。 sierp 很生氣，情緒是。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。、。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。、。。。。。。。。。。。。。。。。\n"
          ]
        }
      ],
      "source": [
        "# 準備提示\n",
        "prompt = \"我很生氣，情緒是?\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "\n",
        "# 進行推理\n",
        "# attention_mask is not strictly necessary for generation with Llama models, \n",
        "# but it's good practice to include it.\n",
        "with torch.no_grad():\n",
        "    outputs = inference_model.generate(\n",
        "        input_ids=input_ids, \n",
        "        max_new_tokens=256, \n",
        "        do_sample=True, \n",
        "        top_k=50,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "# 解碼並打印結果\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"模型生成的回應：\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (可選) 步驟 3: 與基礎模型比較\n",
        "\n",
        "為了更好地評估 LoRA 微調的效果，您可以載入未經微調的原始 Llama-2 模型，並用相同的提示進行推理，然後比較兩者的輸出差異。\n",
        "\n",
        "---\n",
        "\n",
        "推理完成！在最後一個筆記本 `05-merge-and-save.ipynb` 中，我們將學習如何將 LoRA adapter 合併回基礎模型，以便於未來部署。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "068709be1a344cce99ecc3f1082ab1d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcc4e49ece58428ea6e59d11e9ce6efe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11ea711ab6104389adb22a10935719f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4cc92f32d254fbdb7b05b91d410517e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9933545e632042d492f720d4e1081090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "573ff6ab79134f1b899f15fdbda8f5c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f8af3f5303a4056b6e61a041a5b1c1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e500ccf12e1494384482cfe1510c75a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e72902c6ad0409aa1176bda18e87c0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbbb39a14bfd49f494802a4c42b5f58c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e68b5df58c1c42a5811f8262b60b7b29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "原始基礎模型生成的回應：\n",
            "我很生氣，情緒是?\n",
            "I am angry, how do you feel?\n",
            "我很生氣，情緒是?\n",
            "我很生氣，情緒是? 投票\n",
            "I'm very angry, how do you feel?\n",
            "I'm very angry, how do you feel? 投票\n",
            "我很生氣，情緒是? 相關問題\n",
            "I'm very angry, how are you feeling?\n",
            "I'm very angry, how are you feeling? 投票\n",
            "我很生氣，情緒是? 相關問題\n",
            "我很生氣，情緒是? 相關問題\n",
            "I'm very angry, how are you feeling? 相關問題\n",
            "I'm very angry, how are you feeling? 相關問題\n",
            "我很生氣，情緒是? 相\n"
          ]
        }
      ],
      "source": [
        "# 載入未經微調的原始 Llama-2 基礎模型 (以便與微調模型做比較)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 指定原始Llama-2模型 checkpoint 名稱\n",
        "base_model_name_or_path = \"meta-llama/Llama-2-7b-hf\"  # 請根據您實際使用的模型checkpoint調整\n",
        "\n",
        "# 載入tokenizer和模型\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 與 PEFT 一致，重複用相同的 prompt\n",
        "prompt = \"我很生氣，情緒是?\"\n",
        "inputs = base_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(base_model.device)\n",
        "\n",
        "# 進行推理\n",
        "with torch.no_grad():\n",
        "    base_outputs = base_model.generate(\n",
        "        input_ids=inputs['input_ids'], \n",
        "        max_new_tokens=256, \n",
        "        do_sample=True, \n",
        "        top_k=50,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "# 解碼並打印原始模型的結果\n",
        "base_generated_text = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "print(\"原始基礎模型生成的回應：\")\n",
        "print(base_generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "原始基礎模型生成的回應：\n",
            "i am angry, what is my emotion?\n",
            "I am angry because i have been lied\n",
            "逐字逐字輸出模型回應：\n",
            "i am angry, what is my emotion?\n",
            "I am angry because i have been lied\n"
          ]
        }
      ],
      "source": [
        "# 與 PEFT 一致，重複用相同的 prompt\n",
        "prompt = \"i am angry, what is my emotion?\"\n",
        "inputs = base_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(base_model.device)\n",
        "\n",
        "# 進行推理\n",
        "with torch.no_grad():\n",
        "    base_outputs = base_model.generate(\n",
        "        input_ids=inputs['input_ids'], \n",
        "        max_new_tokens=10, \n",
        "        do_sample=True, \n",
        "        top_k=50,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "# 解碼並打印原始模型的結果\n",
        "base_generated_text = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "print(\"原始基礎模型生成的回應：\")\n",
        "print(base_generated_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "逐字逐字輸出模型回應：\n",
            "i am angry, what is my emotion?\n",
            "I am angry because i have been lied\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "# 用於逐字逐字印出 LLM 產生的回應\n",
        "def print_token_stream(output_text, delay=0.05):\n",
        "    for char in output_text:\n",
        "        print(char, end='', flush=True)\n",
        "        time.sleep(delay)\n",
        "    print()  # 換行\n",
        "\n",
        "# 示範如何一個字一個字輸出 LLM 回應\n",
        "print(\"逐字逐字輸出模型回應：\")\n",
        "print_token_stream(base_generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM Engineering Course (Poetry)",
      "language": "python",
      "name": "llm-engineering-course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
