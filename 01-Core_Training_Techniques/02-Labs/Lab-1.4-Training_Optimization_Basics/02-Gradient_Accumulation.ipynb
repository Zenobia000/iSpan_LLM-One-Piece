{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.4: æ¢¯åº¦ç´¯ç© (Gradient Accumulation)\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- ç†è§£æ¢¯åº¦ç´¯ç©å¦‚ä½•å¯¦ç¾å¤§æ‰¹æ¬¡è¨“ç·´\n",
    "- æŒæ¡æœ‰æ•ˆæ‰¹æ¬¡å¤§å°çš„è¨ˆç®—æ–¹æ³•\n",
    "- å°æ¯”ä¸åŒç´¯ç©ç­–ç•¥çš„æ•ˆæœ\n",
    "- å­¸ç¿’çµåˆæ··åˆç²¾åº¦çš„å„ªåŒ–æŠ€å·§\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 45-60åˆ†é˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç†è«–èƒŒæ™¯\n",
    "\n",
    "### 1.1 ä»€éº¼æ˜¯æ¢¯åº¦ç´¯ç©ï¼Ÿ\n",
    "\n",
    "**å•é¡Œ**: GPU è¨˜æ†¶é«”ä¸è¶³ä»¥æ”¯æŒå¤§æ‰¹æ¬¡è¨“ç·´\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ**: å°‡å¤§æ‰¹æ¬¡åˆ†å‰²æˆå¤šå€‹å°æ‰¹æ¬¡ (micro-batch)ï¼Œç´¯ç©æ¢¯åº¦å¾Œå†æ›´æ–°\n",
    "\n",
    "### 1.2 å·¥ä½œåŸç†\n",
    "\n",
    "```\n",
    "å‚³çµ±è¨“ç·´ (batch_size=32):\n",
    "  å‰å‘å‚³æ’­(32) â†’ åå‘å‚³æ’­(32) â†’ æ›´æ–°åƒæ•¸ â†’ æ¸…é›¶æ¢¯åº¦\n",
    "\n",
    "æ¢¯åº¦ç´¯ç© (micro_batch=8, accumulation=4):\n",
    "  å‰å‘å‚³æ’­(8) â†’ åå‘å‚³æ’­(8) â†’ ç´¯ç©æ¢¯åº¦\n",
    "  å‰å‘å‚³æ’­(8) â†’ åå‘å‚³æ’­(8) â†’ ç´¯ç©æ¢¯åº¦  \n",
    "  å‰å‘å‚³æ’­(8) â†’ åå‘å‚³æ’­(8) â†’ ç´¯ç©æ¢¯åº¦\n",
    "  å‰å‘å‚³æ’­(8) â†’ åå‘å‚³æ’­(8) â†’ ç´¯ç©æ¢¯åº¦\n",
    "  â†’ æ›´æ–°åƒæ•¸ â†’ æ¸…é›¶æ¢¯åº¦\n",
    "  \n",
    "æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 8 Ã— 4 = 32 (èˆ‡å‚³çµ±è¨“ç·´ç­‰æ•ˆ)\n",
    "```\n",
    "\n",
    "### 1.3 é—œéµå…¬å¼\n",
    "\n",
    "$$\\text{Effective Batch Size} = \\text{Micro Batch Size} \\times \\text{Accumulation Steps}$$\n",
    "\n",
    "$$\\text{æ¢¯åº¦ç´¯ç©} = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla L_i$$\n",
    "\n",
    "å…¶ä¸­ $N$ ç‚ºç´¯ç©æ­¥æ•¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ•¸æ“šæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextDataset(Dataset):\n",
    "    \"\"\"ç°¡å–®çš„æ–‡æœ¬æ•¸æ“šé›†\"\"\"\n",
    "    def __init__(self, tokenizer, num_samples=1000, seq_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # ç”Ÿæˆè¨“ç·´æ–‡æœ¬\n",
    "        self.texts = [\n",
    "            f\"The quick brown fox jumps over the lazy dog. \" * 10\n",
    "            for _ in range(num_samples)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "print(\"è¼‰å…¥ GPT-2 Tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# å‰µå»ºæ•¸æ“šé›†\n",
    "dataset = SimpleTextDataset(tokenizer, num_samples=800, seq_length=128)\n",
    "print(f\"æ•¸æ“šé›†å¤§å°: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è¨˜æ†¶é«”è¿½è¹¤å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTracker:\n",
    "    \"\"\"è¨˜æ†¶é«”è¿½è¹¤å™¨\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    def get_memory_mb(self):\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\"allocated\": 0, \"peak\": 0}\n",
    "        return {\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e6,\n",
    "            \"peak\": torch.cuda.max_memory_allocated() / 1e6\n",
    "        }\n",
    "    \n",
    "    def print_memory(self, prefix=\"\"):\n",
    "        stats = self.get_memory_mb()\n",
    "        print(f\"{prefix}è¨˜æ†¶é«” - ç•¶å‰: {stats['allocated']:.0f}MB, å³°å€¼: {stats['peak']:.0f}MB\")\n",
    "        return stats\n",
    "\n",
    "memory_tracker = MemoryTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¢¯åº¦ç´¯ç©è¨“ç·´å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_accumulation(\n",
    "    model,\n",
    "    dataset,\n",
    "    micro_batch_size,\n",
    "    accumulation_steps,\n",
    "    num_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    use_amp=True\n",
    "):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ¢¯åº¦ç´¯ç©è¨“ç·´æ¨¡å‹\n",
    "    \n",
    "    Args:\n",
    "        model: å¾…è¨“ç·´æ¨¡å‹\n",
    "        dataset: è¨“ç·´æ•¸æ“šé›†\n",
    "        micro_batch_size: æ¯æ¬¡å‰å‘å‚³æ’­çš„æ‰¹æ¬¡å¤§å°\n",
    "        accumulation_steps: æ¢¯åº¦ç´¯ç©æ­¥æ•¸\n",
    "        num_steps: ç¸½è¨“ç·´æ­¥æ•¸\n",
    "        learning_rate: å­¸ç¿’ç‡\n",
    "        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    # å‰µå»º DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=micro_batch_size, shuffle=True)\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    # é‡ç½®è¨˜æ†¶é«”è¿½è¹¤\n",
    "    memory_tracker.reset()\n",
    "    \n",
    "    # è¨“ç·´çµ±è¨ˆ\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # æœ‰æ•ˆæ‰¹æ¬¡å¤§å°\n",
    "    effective_batch_size = micro_batch_size * accumulation_steps\n",
    "    \n",
    "    pbar = tqdm(range(num_steps), desc=f\"Training (bs={effective_batch_size})\")\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for step in pbar:\n",
    "        step_loss = 0.0\n",
    "        \n",
    "        # æ¢¯åº¦ç´¯ç©å¾ªç’°\n",
    "        for accum_step in range(accumulation_steps):\n",
    "            try:\n",
    "                batch = next(dataloader_iter)\n",
    "            except StopIteration:\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                batch = next(dataloader_iter)\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # å‰å‘å‚³æ’­\n",
    "            if use_amp:\n",
    "                with autocast(dtype=torch.float16):\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss / accumulation_steps  # å¹³å‡æå¤±\n",
    "                \n",
    "                # åå‘å‚³æ’­ (ç´¯ç©æ¢¯åº¦)\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "                loss.backward()\n",
    "            \n",
    "            step_loss += loss.item()\n",
    "        \n",
    "        # ç´¯ç©å®Œæˆ, æ›´æ–°åƒæ•¸\n",
    "        if use_amp:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses.append(step_loss)\n",
    "        pbar.set_postfix({\"loss\": f\"{step_loss:.4f}\"})\n",
    "    \n",
    "    # çµ±è¨ˆçµæœ\n",
    "    training_time = time.time() - start_time\n",
    "    memory_stats = memory_tracker.get_memory_mb()\n",
    "    \n",
    "    return {\n",
    "        \"losses\": losses,\n",
    "        \"avg_loss\": np.mean(losses),\n",
    "        \"final_loss\": losses[-1],\n",
    "        \"training_time\": training_time,\n",
    "        \"steps_per_sec\": num_steps / training_time,\n",
    "        \"peak_memory_mb\": memory_stats[\"peak\"],\n",
    "        \"effective_batch_size\": effective_batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å¯¦é©— 1: å°æ‰¹æ¬¡åŸºæº– (ç„¡æ¢¯åº¦ç´¯ç©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 1: å°æ‰¹æ¬¡è¨“ç·´ (ç„¡æ¢¯åº¦ç´¯ç©)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "model_baseline = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_baseline = model_baseline.to(device)\n",
    "\n",
    "print(f\"\\né…ç½®: micro_batch_size=2, accumulation_steps=1\")\n",
    "print(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 2 Ã— 1 = 2\")\n",
    "\n",
    "# è¨“ç·´\n",
    "results_baseline = train_with_accumulation(\n",
    "    model=model_baseline,\n",
    "    dataset=dataset,\n",
    "    micro_batch_size=2,\n",
    "    accumulation_steps=1,  # ç„¡ç´¯ç©\n",
    "    num_steps=100,\n",
    "    use_amp=True\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"åŸºæº–çµæœ (batch_size=2)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"å¹³å‡ Loss: {results_baseline['avg_loss']:.4f}\")\n",
    "print(f\"æœ€çµ‚ Loss: {results_baseline['final_loss']:.4f}\")\n",
    "print(f\"è¨“ç·´æ™‚é–“: {results_baseline['training_time']:.2f} ç§’\")\n",
    "print(f\"å³°å€¼è¨˜æ†¶é«”: {results_baseline['peak_memory_mb']:.0f} MB\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del model_baseline\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¯¦é©— 2: æ¢¯åº¦ç´¯ç© (accumulation_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 2: æ¢¯åº¦ç´¯ç© (accumulation_steps=4)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "model_acc4 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_acc4 = model_acc4.to(device)\n",
    "\n",
    "print(f\"\\né…ç½®: micro_batch_size=2, accumulation_steps=4\")\n",
    "print(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 2 Ã— 4 = 8\")\n",
    "\n",
    "# è¨“ç·´\n",
    "results_acc4 = train_with_accumulation(\n",
    "    model=model_acc4,\n",
    "    dataset=dataset,\n",
    "    micro_batch_size=2,\n",
    "    accumulation_steps=4,\n",
    "    num_steps=100,\n",
    "    use_amp=True\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ¢¯åº¦ç´¯ç©çµæœ (effective_batch_size=8)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"å¹³å‡ Loss: {results_acc4['avg_loss']:.4f}\")\n",
    "print(f\"æœ€çµ‚ Loss: {results_acc4['final_loss']:.4f}\")\n",
    "print(f\"è¨“ç·´æ™‚é–“: {results_acc4['training_time']:.2f} ç§’\")\n",
    "print(f\"å³°å€¼è¨˜æ†¶é«”: {results_acc4['peak_memory_mb']:.0f} MB\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del model_acc4\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å¯¦é©— 3: æ›´å¤§çš„æ¢¯åº¦ç´¯ç© (accumulation_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 3: æ¢¯åº¦ç´¯ç© (accumulation_steps=8)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "model_acc8 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_acc8 = model_acc8.to(device)\n",
    "\n",
    "print(f\"\\né…ç½®: micro_batch_size=2, accumulation_steps=8\")\n",
    "print(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 2 Ã— 8 = 16\")\n",
    "\n",
    "# è¨“ç·´\n",
    "results_acc8 = train_with_accumulation(\n",
    "    model=model_acc8,\n",
    "    dataset=dataset,\n",
    "    micro_batch_size=2,\n",
    "    accumulation_steps=8,\n",
    "    num_steps=100,\n",
    "    use_amp=True\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ¢¯åº¦ç´¯ç©çµæœ (effective_batch_size=16)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"å¹³å‡ Loss: {results_acc8['avg_loss']:.4f}\")\n",
    "print(f\"æœ€çµ‚ Loss: {results_acc8['final_loss']:.4f}\")\n",
    "print(f\"è¨“ç·´æ™‚é–“: {results_acc8['training_time']:.2f} ç§’\")\n",
    "print(f\"å³°å€¼è¨˜æ†¶é«”: {results_acc8['peak_memory_mb']:.0f} MB\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del model_acc8\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. å¯¦é©— 4: æ¥µå¤§æ‰¹æ¬¡æ¢¯åº¦ç´¯ç© (accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"å¯¦é©— 4: æ¢¯åº¦ç´¯ç© (accumulation_steps=16)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "model_acc16 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_acc16 = model_acc16.to(device)\n",
    "\n",
    "print(f\"\\né…ç½®: micro_batch_size=2, accumulation_steps=16\")\n",
    "print(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 2 Ã— 16 = 32\")\n",
    "\n",
    "# è¨“ç·´\n",
    "results_acc16 = train_with_accumulation(\n",
    "    model=model_acc16,\n",
    "    dataset=dataset,\n",
    "    micro_batch_size=2,\n",
    "    accumulation_steps=16,\n",
    "    num_steps=100,\n",
    "    use_amp=True\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ¢¯åº¦ç´¯ç©çµæœ (effective_batch_size=32)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"å¹³å‡ Loss: {results_acc16['avg_loss']:.4f}\")\n",
    "print(f\"æœ€çµ‚ Loss: {results_acc16['final_loss']:.4f}\")\n",
    "print(f\"è¨“ç·´æ™‚é–“: {results_acc16['training_time']:.2f} ç§’\")\n",
    "print(f\"å³°å€¼è¨˜æ†¶é«”: {results_acc16['peak_memory_mb']:.0f} MB\")\n",
    "\n",
    "# æ¸…ç†\n",
    "del model_acc16\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ€§èƒ½å°æ¯”åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•´ç†çµæœ\n",
    "all_results = [\n",
    "    (\"Baseline (BS=2)\", results_baseline),\n",
    "    (\"AccumÃ—4 (BS=8)\", results_acc4),\n",
    "    (\"AccumÃ—8 (BS=16)\", results_acc8),\n",
    "    (\"AccumÃ—16 (BS=32)\", results_acc16)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"æ¢¯åº¦ç´¯ç©æ€§èƒ½å°æ¯”\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'é…ç½®':<20} {'æœ‰æ•ˆBS':<10} {'å¹³å‡Loss':<12} {'æœ€çµ‚Loss':<12} {'æ™‚é–“(s)':<10} {'è¨˜æ†¶é«”(MB)':<12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name, result in all_results:\n",
    "    print(f\"{name:<20} \"\n",
    "          f\"{result['effective_batch_size']:<10} \"\n",
    "          f\"{result['avg_loss']:<12.4f} \"\n",
    "          f\"{result['final_loss']:<12.4f} \"\n",
    "          f\"{result['training_time']:<10.2f} \"\n",
    "          f\"{result['peak_memory_mb']:<12.0f}\")\n",
    "\n",
    "# åˆ†æè¨˜æ†¶é«”ä½¿ç”¨\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"é—œéµç™¼ç¾\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"1. è¨˜æ†¶é«”ä½¿ç”¨: æ‰€æœ‰é…ç½®è¨˜æ†¶é«”å ç”¨ç›¸è¿‘ (~{results_baseline['peak_memory_mb']:.0f}MB)\")\n",
    "print(f\"   â†’ æ¢¯åº¦ç´¯ç©ä¸æœƒå¢åŠ è¨˜æ†¶é«”å³°å€¼ âœ…\")\n",
    "print(f\"\\n2. è¨“ç·´æ™‚é–“: ç´¯ç©æ­¥æ•¸è¶Šå¤§, æ™‚é–“ç•¥å¾®å¢åŠ \")\n",
    "print(f\"   â†’ Baseline: {results_baseline['training_time']:.1f}s\")\n",
    "print(f\"   â†’ AccumÃ—16: {results_acc16['training_time']:.1f}s (+{(results_acc16['training_time']/results_baseline['training_time']-1)*100:.1f}%)\")\n",
    "print(f\"\\n3. è¨“ç·´æ•ˆæœ: æ›´å¤§çš„æ‰¹æ¬¡é€šå¸¸å¸¶ä¾†æ›´ç©©å®šçš„è¨“ç·´\")\n",
    "print(f\"   â†’ è§€å¯Ÿ Loss æ›²ç·šçš„å¹³æ»‘åº¦å·®ç•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. è¦–è¦ºåŒ–å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºå°æ¯”åœ–è¡¨\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"æ¢¯åº¦ç´¯ç©æ•ˆæœå°æ¯”\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "configs = [\"BS=2\", \"BS=8\", \"BS=16\", \"BS=32\"]\n",
    "colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\"]\n",
    "\n",
    "# 1. Loss æ›²ç·šå°æ¯”\n",
    "axes[0, 0].plot(results_baseline[\"losses\"], label=configs[0], linewidth=2, color=colors[0], alpha=0.8)\n",
    "axes[0, 0].plot(results_acc4[\"losses\"], label=configs[1], linewidth=2, color=colors[1], alpha=0.8)\n",
    "axes[0, 0].plot(results_acc8[\"losses\"], label=configs[2], linewidth=2, color=colors[2], alpha=0.8)\n",
    "axes[0, 0].plot(results_acc16[\"losses\"], label=configs[3], linewidth=2, color=colors[3], alpha=0.8)\n",
    "axes[0, 0].set_title(\"è¨“ç·´ Loss æ›²ç·š\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"Step\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. å¹³å‡ Loss å°æ¯”\n",
    "avg_losses = [r[\"avg_loss\"] for _, r in all_results]\n",
    "axes[0, 1].bar(configs, avg_losses, color=colors)\n",
    "axes[0, 1].set_title(\"å¹³å‡ Loss å°æ¯”\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 1].set_ylabel(\"Average Loss\")\n",
    "axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 3. è¨“ç·´æ™‚é–“å°æ¯”\n",
    "times = [r[\"training_time\"] for _, r in all_results]\n",
    "axes[1, 0].bar(configs, times, color=colors)\n",
    "axes[1, 0].set_title(\"è¨“ç·´æ™‚é–“å°æ¯”\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"æ™‚é–“ (ç§’)\")\n",
    "axes[1, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 4. è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”\n",
    "memories = [r[\"peak_memory_mb\"] for _, r in all_results]\n",
    "axes[1, 1].bar(configs, memories, color=colors)\n",
    "axes[1, 1].set_title(\"å³°å€¼è¨˜æ†¶é«”ä½¿ç”¨\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"è¨˜æ†¶é«” (MB)\")\n",
    "axes[1, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Loss ç©©å®šæ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®— Loss çš„æ¨™æº–å·® (è¡¡é‡ç©©å®šæ€§)\n",
    "print(\"=\" * 70)\n",
    "print(\"è¨“ç·´ç©©å®šæ€§åˆ†æ (Loss æ¨™æº–å·®)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, result in all_results:\n",
    "    losses = result[\"losses\"]\n",
    "    loss_std = np.std(losses)\n",
    "    loss_mean = np.mean(losses)\n",
    "    cv = loss_std / loss_mean  # è®Šç•°ä¿‚æ•¸\n",
    "    \n",
    "    print(f\"{name:<20} Lossæ¨™æº–å·®: {loss_std:.4f}, è®Šç•°ä¿‚æ•¸: {cv:.4f}\")\n",
    "\n",
    "print(\"\\nèªªæ˜: è®Šç•°ä¿‚æ•¸è¶Šå°, è¨“ç·´è¶Šç©©å®š\")\n",
    "print(\"ä¸€èˆ¬è€Œè¨€, æ›´å¤§çš„æ‰¹æ¬¡å¤§å°æœƒå¸¶ä¾†æ›´ç©©å®šçš„è¨“ç·´\")\n",
    "\n",
    "# ç¹ªè£½ Loss è®ŠåŒ–ç‡\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, (name, result) in enumerate(all_results):\n",
    "    losses = result[\"losses\"]\n",
    "    # è¨ˆç®—ç§»å‹•å¹³å‡\n",
    "    window_size = 10\n",
    "    smoothed = np.convolve(losses, np.ones(window_size)/window_size, mode=\"valid\")\n",
    "    plt.plot(smoothed, label=name, linewidth=2, color=colors[i])\n",
    "\n",
    "plt.title(\"Loss å¹³æ»‘æ›²ç·šå°æ¯” (ç§»å‹•å¹³å‡çª—å£=10)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Smoothed Loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. æ¢¯åº¦ç¯„æ•¸åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"æ¢¯åº¦ç¯„æ•¸åˆ†æå¯¦é©—\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def analyze_gradient_norms(model, dataloader, accumulation_steps=1, num_steps=20):\n",
    "    \"\"\"åˆ†ææ¢¯åº¦ç¯„æ•¸\"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    grad_norms = []\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # æ¢¯åº¦ç´¯ç©\n",
    "        for _ in range(accumulation_steps):\n",
    "            try:\n",
    "                batch = next(dataloader_iter)\n",
    "            except StopIteration:\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                batch = next(dataloader_iter)\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with autocast(dtype=torch.float16):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "        \n",
    "        # è¨ˆç®—æ¢¯åº¦ç¯„æ•¸\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        grad_norms.append(total_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# å‰µå»ºæ¸¬è©¦ DataLoader\n",
    "test_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# æ¸¬è©¦ä¸åŒç´¯ç©æ­¥æ•¸\n",
    "print(\"\\nåˆ†æä¸åŒæ¢¯åº¦ç´¯ç©é…ç½®çš„æ¢¯åº¦ç¯„æ•¸...\")\n",
    "grad_results = {}\n",
    "\n",
    "for accum in [1, 4, 8]:\n",
    "    print(f\"\\næ¸¬è©¦ accumulation_steps={accum}...\")\n",
    "    test_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "    \n",
    "    norms = analyze_gradient_norms(test_model, test_loader, accumulation_steps=accum, num_steps=20)\n",
    "    grad_results[f\"AccumÃ—{accum}\"] = norms\n",
    "    \n",
    "    print(f\"å¹³å‡æ¢¯åº¦ç¯„æ•¸: {np.mean(norms):.4f}\")\n",
    "    print(f\"æ¢¯åº¦ç¯„æ•¸æ¨™æº–å·®: {np.std(norms):.4f}\")\n",
    "    \n",
    "    del test_model\n",
    "    memory_tracker.reset()\n",
    "\n",
    "# ç¹ªè£½æ¢¯åº¦ç¯„æ•¸è®ŠåŒ–\n",
    "plt.figure(figsize=(10, 5))\n",
    "for name, norms in grad_results.items():\n",
    "    plt.plot(norms, marker=\"o\", label=name, linewidth=2)\n",
    "\n",
    "plt.title(\"æ¢¯åº¦ç¯„æ•¸è®ŠåŒ– (ä¸åŒç´¯ç©æ­¥æ•¸)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Gradient Norm\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nèªªæ˜: æ›´å¤§çš„ç´¯ç©æ­¥æ•¸é€šå¸¸å¸¶ä¾†æ›´ç©©å®šçš„æ¢¯åº¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. å¯¦é©—ç¸½çµèˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### å¯¦é©—çµè«–\n",
    "\n",
    "1. **è¨˜æ†¶é«”å ç”¨**: æ¢¯åº¦ç´¯ç©**ä¸æœƒå¢åŠ **è¨˜æ†¶é«”å³°å€¼ âœ…\n",
    "   - æ‰€æœ‰é…ç½®çš„è¨˜æ†¶é«”ä½¿ç”¨åŸºæœ¬ç›¸åŒ\n",
    "   - å¯ä»¥åœ¨å°GPUä¸Šå¯¦ç¾å¤§æ‰¹æ¬¡è¨“ç·´æ•ˆæœ\n",
    "\n",
    "2. **è¨“ç·´æ™‚é–“**: ç´¯ç©æ­¥æ•¸è¶Šå¤§, æ™‚é–“ç•¥å¾®å¢åŠ  (5-10%)\n",
    "   - é¡å¤–é–‹éŠ·ä¸»è¦ä¾†è‡ªæ•¸æ“šè¼‰å…¥å’Œå‰å‘å‚³æ’­\n",
    "   - ç›¸å°æ–¼è¨˜æ†¶é«”ç¯€çœ, ä»£åƒ¹å¯æ¥å—\n",
    "\n",
    "3. **è¨“ç·´ç©©å®šæ€§**: æ›´å¤§çš„æœ‰æ•ˆæ‰¹æ¬¡é€šå¸¸æ›´ç©©å®š\n",
    "   - Loss æ›²ç·šæ›´å¹³æ»‘\n",
    "   - æ¢¯åº¦ç¯„æ•¸è®ŠåŒ–æ›´å°\n",
    "   - æ”¶æ–‚æ›´åŠ ç©©å®š\n",
    "\n",
    "4. **æœ€çµ‚æ•ˆæœ**: ç›¸åŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°, æ•ˆæœåŸºæœ¬ç­‰åƒ¹\n",
    "   - æ¢¯åº¦ç´¯ç© vs ç›´æ¥å¤§æ‰¹æ¬¡: æ•¸å­¸ç­‰åƒ¹\n",
    "   - å¯¦éš›æ•ˆæœå·®ç•°ä¸»è¦ä¾†è‡ªæ•¸å€¼ç²¾åº¦\n",
    "\n",
    "### æœ€ä½³å¯¦è¸\n",
    "\n",
    "#### å¦‚ä½•é¸æ“‡ç´¯ç©æ­¥æ•¸?\n",
    "\n",
    "```python\n",
    "# è¨ˆç®—å…¬å¼\n",
    "target_batch_size = 32  # ç›®æ¨™æœ‰æ•ˆæ‰¹æ¬¡å¤§å°\n",
    "max_micro_batch = 4     # GPU èƒ½å®¹ç´çš„æœ€å¤§ micro batch\n",
    "\n",
    "accumulation_steps = target_batch_size // max_micro_batch\n",
    "# 32 // 4 = 8 æ­¥ç´¯ç©\n",
    "```\n",
    "\n",
    "#### æ¨è–¦é…ç½®\n",
    "\n",
    "| GPU è¨˜æ†¶é«” | Micro Batch | ç´¯ç©æ­¥æ•¸ | æœ‰æ•ˆ Batch | é©ç”¨æ¨¡å‹ |\n",
    "|-----------|------------|---------|-----------|----------|\n",
    "| 8GB | 1-2 | 16-32 | 16-64 | GPT-2 Small |\n",
    "| 16GB | 2-4 | 8-16 | 16-64 | GPT-2 Medium |\n",
    "| 24GB | 4-8 | 4-8 | 16-64 | GPT-2 Large |\n",
    "| 40GB+ | 8-16 | 2-4 | 16-64 | GPT-2 XL |\n",
    "\n",
    "#### å­¸ç¿’ç‡èª¿æ•´\n",
    "\n",
    "**é‡è¦**: æ”¹è®Šæœ‰æ•ˆæ‰¹æ¬¡å¤§å°æ™‚, éœ€è¦èª¿æ•´å­¸ç¿’ç‡!\n",
    "\n",
    "```python\n",
    "# ç·šæ€§ç¸®æ”¾è¦å‰‡ (Linear Scaling Rule)\n",
    "base_lr = 5e-5\n",
    "base_batch_size = 8\n",
    "new_batch_size = 32\n",
    "\n",
    "new_lr = base_lr * (new_batch_size / base_batch_size)\n",
    "# 5e-5 * (32 / 8) = 2e-4\n",
    "\n",
    "# æˆ–ä½¿ç”¨ sqrt ç¸®æ”¾ (æ›´ä¿å®ˆ)\n",
    "new_lr = base_lr * np.sqrt(new_batch_size / base_batch_size)\n",
    "```\n",
    "\n",
    "#### å®Œæ•´å„ªåŒ–é…ç½®\n",
    "\n",
    "```python\n",
    "# æ¨è–¦çš„ç”Ÿç”¢ç’°å¢ƒé…ç½®\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# é…ç½®\n",
    "micro_batch_size = 2\n",
    "accumulation_steps = 16\n",
    "effective_batch_size = micro_batch_size * accumulation_steps  # 32\n",
    "\n",
    "# å­¸ç¿’ç‡ç¸®æ”¾\n",
    "base_lr = 5e-5\n",
    "learning_rate = base_lr * (effective_batch_size / 8)\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# è¨“ç·´å¾ªç’°\n",
    "model.zero_grad()\n",
    "for step, batch in enumerate(dataloader):\n",
    "    # ç´¯ç©æ¢¯åº¦\n",
    "    with autocast(dtype=torch.float16):\n",
    "        loss = model(**batch).loss / accumulation_steps\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # ç´¯ç©å®Œæˆ, æ›´æ–°åƒæ•¸\n",
    "    if (step + 1) % accumulation_steps == 0:\n",
    "        # æ¢¯åº¦è£å‰ª\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # æ›´æ–°\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        model.zero_grad()\n",
    "```\n",
    "\n",
    "### å¸¸è¦‹å•é¡Œ\n",
    "\n",
    "#### Q1: Loss éœ€è¦é™¤ä»¥ accumulation_steps å—?\n",
    "**A**: æ˜¯çš„! é€™æ¨£å¯ä»¥ä¿è­‰ç´¯ç©çš„æ¢¯åº¦èˆ‡ç›´æ¥å¤§æ‰¹æ¬¡è¨“ç·´ä¸€è‡´ã€‚\n",
    "\n",
    "#### Q2: ä»€éº¼æ™‚å€™æ¸…é›¶æ¢¯åº¦?\n",
    "**A**: åœ¨ç´¯ç©å®Œæˆä¸¦æ›´æ–°åƒæ•¸**ä¹‹å¾Œ**, è€Œä¸æ˜¯æ¯å€‹ micro-batch ä¹‹å¾Œã€‚\n",
    "\n",
    "#### Q3: æ¢¯åº¦ç´¯ç©æœƒé™ä½è¨“ç·´é€Ÿåº¦å—?\n",
    "**A**: æœƒç•¥å¾®é™ä½ (5-10%), ä½†æ›ä¾†äº†è¨˜æ†¶é«”ç¯€çœ, å€¼å¾—!\n",
    "\n",
    "#### Q4: å¯ä»¥å’Œå…¶ä»–å„ªåŒ–æŠ€è¡“çµ„åˆå—?\n",
    "**A**: å®Œå…¨å¯ä»¥! æ¨è–¦çµ„åˆ:\n",
    "- âœ… æ··åˆç²¾åº¦è¨“ç·´ (AMP)\n",
    "- âœ… æ¢¯åº¦æª¢æŸ¥é» (Gradient Checkpointing)\n",
    "- âœ… æ¢¯åº¦ç´¯ç©\n",
    "\n",
    "é€™ä¸‰è€…çµ„åˆå¯ä»¥åœ¨å°GPUä¸Šè¨“ç·´å¤§æ¨¡å‹!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ä¸‹ä¸€æ­¥å­¸ç¿’\n",
    "\n",
    "å®Œæˆæœ¬ Notebook å¾Œ, å»ºè­°ç¹¼çºŒ:\n",
    "\n",
    "1. **03-Gradient_Checkpointing.ipynb** - é€²ä¸€æ­¥é™ä½è¨˜æ†¶é«”\n",
    "2. **04-Memory_Profiling.ipynb** - æ·±å…¥åˆ†æè¨˜æ†¶é«”ä½¿ç”¨\n",
    "3. **çµ„åˆå„ªåŒ–** - å°‡æ‰€æœ‰æŠ€è¡“çµ„åˆæ‡‰ç”¨æ–¼å¯¦éš›é …ç›®\n",
    "\n",
    "æ­å–œå®Œæˆæ¢¯åº¦ç´¯ç©å¯¦é©—! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
