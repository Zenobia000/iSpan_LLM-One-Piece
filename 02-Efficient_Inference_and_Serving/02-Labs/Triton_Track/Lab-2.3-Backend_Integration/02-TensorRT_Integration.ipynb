{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Backend 整合與極致優化\n",
    "\n",
    "## 🎯 學習目標\n",
    "\n",
    "本實驗將深入探討 Triton TensorRT Backend 的整合與優化技術，學習如何將 PyTorch 模型轉換為高性能的 TensorRT 引擎，並實現極致的推理性能。\n",
    "\n",
    "### 核心知識點\n",
    "- ✅ TensorRT 引擎構建與優化\n",
    "- ✅ 模型轉換流程 (PyTorch → ONNX → TensorRT)\n",
    "- ✅ 精度優化 (FP32/FP16/INT8)\n",
    "- ✅ 動態形狀支援\n",
    "- ✅ 性能基準測試與對比\n",
    "- ✅ 企業級部署配置\n",
    "\n",
    "### 技術架構\n",
    "```\n",
    "PyTorch 模型\n",
    "     ↓\n",
    "ONNX 轉換 (torch.onnx.export)\n",
    "     ↓\n",
    "TensorRT 優化 (trtexec/TensorRT Python API)\n",
    "     ↓\n",
    "Triton TensorRT Backend\n",
    "     ↓\n",
    "高性能推理服務\n",
    "```\n",
    "\n",
    "### 性能提升預期\n",
    "- 🚀 **推理速度**: 2-10x 加速\n",
    "- 💾 **記憶體使用**: 30-50% 減少\n",
    "- ⚡ **延遲優化**: 毫秒級響應\n",
    "- 🔋 **能耗效率**: 顯著降低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境準備與 TensorRT 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "import tritonclient.http as httpclient\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import logging\n",
    "import subprocess\n",
    "import shutil\n",
    "import pickle\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from dataclasses import dataclass\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# TensorRT 相關導入\n",
    "try:\n",
    "    import tensorrt as trt\n",
    "    import pycuda.driver as cuda\n",
    "    import pycuda.autoinit\n",
    "    TRT_AVAILABLE = True\n",
    "    print(f\"✅ TensorRT 版本: {trt.__version__}\")\n",
    "except ImportError as e:\n",
    "    TRT_AVAILABLE = False\n",
    "    print(f\"⚠️  TensorRT 未安裝: {e}\")\n",
    "    print(\"   請安裝 TensorRT 以使用完整功能\")\n",
    "\n",
    "# 設定日誌\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"🔧 環境資訊檢查\")\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"ONNX 版本: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime 版本: {ort.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 數量: {torch.cuda.device_count()}\")\n",
    "    print(f\"當前 GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "    print(f\"計算能力: {torch.cuda.get_device_capability()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定工作目錄\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_REPO = BASE_DIR / \"model_repository_tensorrt\"\n",
    "ONNX_DIR = BASE_DIR / \"onnx_models\"\n",
    "TRT_DIR = BASE_DIR / \"tensorrt_engines\"\n",
    "BENCHMARK_DIR = BASE_DIR / \"benchmarks\"\n",
    "SCRIPTS_DIR = BASE_DIR / \"scripts\"\n",
    "\n",
    "# 創建必要目錄\n",
    "for dir_path in [MODEL_REPO, ONNX_DIR, TRT_DIR, BENCHMARK_DIR, SCRIPTS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "print(f\"📁 工作目錄: {BASE_DIR}\")\n",
    "print(f\"📁 模型倉庫: {MODEL_REPO}\")\n",
    "print(f\"📁 ONNX 模型: {ONNX_DIR}\")\n",
    "print(f\"📁 TensorRT 引擎: {TRT_DIR}\")\n",
    "print(f\"📁 基準測試: {BENCHMARK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 建立參考 PyTorch 模型\n",
    "\n",
    "創建一個適合 TensorRT 優化的模型，展示轉換流程和性能對比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientTextClassifier(nn.Module):\n",
    "    \"\"\"為 TensorRT 優化設計的文本分類模型\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000, embed_dim=128, hidden_dim=256, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # 嵌入層\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # 1D 卷積層 (對 TensorRT 友好)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # 全局最大池化\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # 分類頭\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 權重初始化\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"權重初始化\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, 0, 0.1)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # 輸入 shape: [batch_size, sequence_length]\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # 嵌入: [batch_size, sequence_length, embed_dim]\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # 轉置用於卷積: [batch_size, embed_dim, sequence_length]\n",
    "        embedded = embedded.transpose(1, 2)\n",
    "        \n",
    "        # 卷積特徵提取\n",
    "        conv_output = self.conv_layers(embedded)\n",
    "        \n",
    "        # 全局池化: [batch_size, hidden_dim, 1]\n",
    "        pooled = self.global_pool(conv_output)\n",
    "        \n",
    "        # 展平: [batch_size, hidden_dim]\n",
    "        flattened = pooled.squeeze(2)\n",
    "        \n",
    "        # 分類\n",
    "        logits = self.classifier(flattened)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# 創建和初始化模型\n",
    "model = EfficientTextClassifier(\n",
    "    vocab_size=10000,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_classes=5\n",
    ")\n",
    "\n",
    "# 模型設定\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"🎯 EfficientTextClassifier 已創建\")\n",
    "print(f\"📊 模型參數量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"🔧 模型設備: {device}\")\n",
    "\n",
    "# 計算模型大小\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "model_size_mb = (param_size + buffer_size) / 1024**2\n",
    "print(f\"💾 模型大小: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch 到 ONNX 轉換\n",
    "\n",
    "### 轉換流程\n",
    "PyTorch → ONNX → TensorRT 是標準的優化路徑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ONNXExportConfig:\n",
    "    \"\"\"ONNX 導出配置\"\"\"\n",
    "    input_names: List[str]\n",
    "    output_names: List[str]\n",
    "    dynamic_axes: Dict[str, Any]\n",
    "    opset_version: int = 11\n",
    "    do_constant_folding: bool = True\n",
    "    verbose: bool = False\n",
    "\n",
    "class PyTorchToONNXConverter:\n",
    "    \"\"\"PyTorch 到 ONNX 轉換器\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "    def export_onnx(self, \n",
    "                   dummy_input: torch.Tensor,\n",
    "                   onnx_path: Path,\n",
    "                   config: ONNXExportConfig) -> bool:\n",
    "        \"\"\"導出 ONNX 模型\"\"\"\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"開始導出 ONNX 模型到: {onnx_path}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                torch.onnx.export(\n",
    "                    self.model,\n",
    "                    dummy_input,\n",
    "                    str(onnx_path),\n",
    "                    export_params=True,\n",
    "                    opset_version=config.opset_version,\n",
    "                    do_constant_folding=config.do_constant_folding,\n",
    "                    input_names=config.input_names,\n",
    "                    output_names=config.output_names,\n",
    "                    dynamic_axes=config.dynamic_axes,\n",
    "                    verbose=config.verbose\n",
    "                )\n",
    "            \n",
    "            # 驗證 ONNX 模型\n",
    "            self._validate_onnx_model(onnx_path, dummy_input)\n",
    "            \n",
    "            logger.info(\"ONNX 導出成功\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ONNX 導出失敗: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _validate_onnx_model(self, onnx_path: Path, dummy_input: torch.Tensor):\n",
    "        \"\"\"驗證 ONNX 模型\"\"\"\n",
    "        \n",
    "        # 載入和檢查 ONNX 模型\n",
    "        onnx_model = onnx.load(str(onnx_path))\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        \n",
    "        # 比較 PyTorch 和 ONNX 輸出\n",
    "        with torch.no_grad():\n",
    "            pytorch_output = self.model(dummy_input).cpu().numpy()\n",
    "        \n",
    "        # ONNX Runtime 推理\n",
    "        ort_session = ort.InferenceSession(str(onnx_path))\n",
    "        ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.cpu().numpy()}\n",
    "        onnx_output = ort_session.run(None, ort_inputs)[0]\n",
    "        \n",
    "        # 檢查輸出一致性\n",
    "        max_diff = np.max(np.abs(pytorch_output - onnx_output))\n",
    "        logger.info(f\"PyTorch vs ONNX 最大差異: {max_diff:.6f}\")\n",
    "        \n",
    "        if max_diff > 1e-3:\n",
    "            logger.warning(f\"輸出差異較大: {max_diff}\")\n",
    "        else:\n",
    "            logger.info(\"輸出一致性驗證通過\")\n",
    "\n",
    "# 準備導出配置\n",
    "export_configs = {\n",
    "    # 固定形狀版本 (用於最大優化)\n",
    "    \"fixed\": ONNXExportConfig(\n",
    "        input_names=[\"input_ids\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={},  # 無動態軸\n",
    "        opset_version=11\n",
    "    ),\n",
    "    \n",
    "    # 動態形狀版本 (用於靈活性)\n",
    "    \"dynamic\": ONNXExportConfig(\n",
    "        input_names=[\"input_ids\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"logits\": {0: \"batch_size\"}\n",
    "        },\n",
    "        opset_version=11\n",
    "    )\n",
    "}\n",
    "\n",
    "# 創建轉換器\n",
    "converter = PyTorchToONNXConverter(model, device)\n",
    "\n",
    "# 導出不同版本的 ONNX 模型\n",
    "export_results = {}\n",
    "\n",
    "for config_name, config in export_configs.items():\n",
    "    print(f\"\\n🔄 導出 {config_name} ONNX 模型\")\n",
    "    \n",
    "    # 創建示例輸入\n",
    "    if config_name == \"fixed\":\n",
    "        # 固定形狀: batch=4, seq_len=128\n",
    "        dummy_input = torch.randint(1, 1000, (4, 128), dtype=torch.long, device=device)\n",
    "        onnx_path = ONNX_DIR / f\"text_classifier_{config_name}_b4_s128.onnx\"\n",
    "    else:\n",
    "        # 動態形狀: batch=1, seq_len=64 (示例)\n",
    "        dummy_input = torch.randint(1, 1000, (1, 64), dtype=torch.long, device=device)\n",
    "        onnx_path = ONNX_DIR / f\"text_classifier_{config_name}.onnx\"\n",
    "    \n",
    "    # 執行導出\n",
    "    success = converter.export_onnx(dummy_input, onnx_path, config)\n",
    "    export_results[config_name] = {\n",
    "        \"path\": onnx_path,\n",
    "        \"success\": success,\n",
    "        \"dummy_input_shape\": dummy_input.shape\n",
    "    }\n",
    "    \n",
    "    if success:\n",
    "        # 檢查文件大小\n",
    "        file_size_mb = onnx_path.stat().st_size / 1024**2\n",
    "        print(f\"  ✅ 導出成功，文件大小: {file_size_mb:.2f} MB\")\n",
    "        print(f\"  📄 文件路徑: {onnx_path}\")\n",
    "    else:\n",
    "        print(f\"  ❌ 導出失敗\")\n",
    "\n",
    "print(f\"\\n📊 ONNX 導出總結: {sum(1 for r in export_results.values() if r['success'])}/{len(export_results)} 成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorRT 引擎構建\n",
    "\n",
    "### 引擎優化策略\n",
    "- **FP16 精度**: 2x 速度提升，最小精度損失\n",
    "- **INT8 量化**: 4x 速度提升，需要校準數據\n",
    "- **動態形狀**: 支援不同輸入尺寸\n",
    "- **層融合**: 自動優化運算圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRT_AVAILABLE:\n",
    "    \n",
    "    @dataclass\n",
    "    class TensorRTBuildConfig:\n",
    "        \"\"\"TensorRT 構建配置\"\"\"\n",
    "        precision: str = \"fp16\"  # fp32, fp16, int8\n",
    "        max_batch_size: int = 16\n",
    "        max_workspace_size: int = 1 << 30  # 1GB\n",
    "        optimization_level: int = 3\n",
    "        enable_dynamic_shapes: bool = True\n",
    "        min_shapes: Dict[str, Tuple] = None\n",
    "        opt_shapes: Dict[str, Tuple] = None\n",
    "        max_shapes: Dict[str, Tuple] = None\n",
    "    \n",
    "    class TensorRTEngineBuilder:\n",
    "        \"\"\"TensorRT 引擎構建器\"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.logger = trt.Logger(trt.Logger.INFO)\n",
    "            self.builder = trt.Builder(self.logger)\n",
    "            \n",
    "        def build_engine_from_onnx(self, \n",
    "                                  onnx_path: Path, \n",
    "                                  engine_path: Path,\n",
    "                                  config: TensorRTBuildConfig) -> bool:\n",
    "            \"\"\"從 ONNX 構建 TensorRT 引擎\"\"\"\n",
    "            \n",
    "            try:\n",
    "                logger.info(f\"開始構建 TensorRT 引擎: {engine_path}\")\n",
    "                logger.info(f\"精度模式: {config.precision}\")\n",
    "                \n",
    "                # 創建網絡\n",
    "                network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "                network = self.builder.create_network(network_flags)\n",
    "                parser = trt.OnnxParser(network, self.logger)\n",
    "                \n",
    "                # 解析 ONNX\n",
    "                with open(onnx_path, 'rb') as model:\n",
    "                    if not parser.parse(model.read()):\n",
    "                        logger.error(\"ONNX 解析失敗\")\n",
    "                        for error in range(parser.num_errors):\n",
    "                            logger.error(parser.get_error(error))\n",
    "                        return False\n",
    "                \n",
    "                # 創建構建配置\n",
    "                build_config = self.builder.create_builder_config()\n",
    "                build_config.max_workspace_size = config.max_workspace_size\n",
    "                \n",
    "                # 設定精度\n",
    "                if config.precision == \"fp16\":\n",
    "                    if self.builder.platform_has_fast_fp16:\n",
    "                        build_config.set_flag(trt.BuilderFlag.FP16)\n",
    "                        logger.info(\"啟用 FP16 精度\")\n",
    "                    else:\n",
    "                        logger.warning(\"GPU 不支援 FP16，使用 FP32\")\n",
    "                elif config.precision == \"int8\":\n",
    "                    if self.builder.platform_has_fast_int8:\n",
    "                        build_config.set_flag(trt.BuilderFlag.INT8)\n",
    "                        # 這裡需要校準器，簡化示例跳過\n",
    "                        logger.info(\"啟用 INT8 精度 (需要校準數據)\")\n",
    "                    else:\n",
    "                        logger.warning(\"GPU 不支援 INT8，使用 FP32\")\n",
    "                \n",
    "                # 動態形狀配置\n",
    "                if config.enable_dynamic_shapes and config.min_shapes:\n",
    "                    profile = self.builder.create_optimization_profile()\n",
    "                    \n",
    "                    for input_name in config.min_shapes:\n",
    "                        min_shape = config.min_shapes[input_name]\n",
    "                        opt_shape = config.opt_shapes.get(input_name, min_shape)\n",
    "                        max_shape = config.max_shapes.get(input_name, min_shape)\n",
    "                        \n",
    "                        profile.set_shape(input_name, min_shape, opt_shape, max_shape)\n",
    "                        logger.info(f\"動態形狀 {input_name}: min={min_shape}, opt={opt_shape}, max={max_shape}\")\n",
    "                    \n",
    "                    build_config.add_optimization_profile(profile)\n",
    "                \n",
    "                # 構建引擎\n",
    "                logger.info(\"開始構建引擎... (這可能需要幾分鐘)\")\n",
    "                engine = self.builder.build_engine(network, build_config)\n",
    "                \n",
    "                if engine is None:\n",
    "                    logger.error(\"引擎構建失敗\")\n",
    "                    return False\n",
    "                \n",
    "                # 序列化引擎\n",
    "                with open(engine_path, 'wb') as f:\n",
    "                    f.write(engine.serialize())\n",
    "                \n",
    "                logger.info(f\"TensorRT 引擎已儲存: {engine_path}\")\n",
    "                \n",
    "                # 顯示引擎資訊\n",
    "                self._print_engine_info(engine, engine_path)\n",
    "                \n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"TensorRT 引擎構建錯誤: {e}\")\n",
    "                return False\n",
    "        \n",
    "        def _print_engine_info(self, engine, engine_path: Path):\n",
    "            \"\"\"顯示引擎資訊\"\"\"\n",
    "            file_size_mb = engine_path.stat().st_size / 1024**2\n",
    "            logger.info(f\"引擎文件大小: {file_size_mb:.2f} MB\")\n",
    "            logger.info(f\"輸入數量: {engine.num_bindings // 2}\")\n",
    "            logger.info(f\"最大批次大小: {engine.max_batch_size}\")\n",
    "            \n",
    "            for i in range(engine.num_bindings):\n",
    "                name = engine.get_binding_name(i)\n",
    "                shape = engine.get_binding_shape(i)\n",
    "                dtype = engine.get_binding_dtype(i)\n",
    "                is_input = engine.binding_is_input(i)\n",
    "                logger.info(f\"  {'輸入' if is_input else '輸出'} {name}: {shape} ({dtype})\")\n",
    "    \n",
    "    # 創建 TensorRT 引擎構建器\n",
    "    builder = TensorRTEngineBuilder()\n",
    "    \n",
    "    # 構建不同精度的引擎\n",
    "    build_configs = {\n",
    "        \"fp32\": TensorRTBuildConfig(\n",
    "            precision=\"fp32\",\n",
    "            max_batch_size=16,\n",
    "            enable_dynamic_shapes=False\n",
    "        ),\n",
    "        \"fp16\": TensorRTBuildConfig(\n",
    "            precision=\"fp16\",\n",
    "            max_batch_size=16,\n",
    "            enable_dynamic_shapes=False\n",
    "        ),\n",
    "        \"dynamic_fp16\": TensorRTBuildConfig(\n",
    "            precision=\"fp16\",\n",
    "            max_batch_size=0,  # 動態批次\n",
    "            enable_dynamic_shapes=True,\n",
    "            min_shapes={\"input_ids\": (1, 16)},\n",
    "            opt_shapes={\"input_ids\": (4, 128)},\n",
    "            max_shapes={\"input_ids\": (16, 512)}\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    build_results = {}\n",
    "    \n",
    "    for config_name, build_config in build_configs.items():\n",
    "        print(f\"\\n🔧 構建 {config_name} TensorRT 引擎\")\n",
    "        \n",
    "        # 選擇對應的 ONNX 模型\n",
    "        if \"dynamic\" in config_name:\n",
    "            onnx_path = export_results[\"dynamic\"][\"path\"]\n",
    "        else:\n",
    "            onnx_path = export_results[\"fixed\"][\"path\"]\n",
    "        \n",
    "        if not export_results[\"dynamic\" if \"dynamic\" in config_name else \"fixed\"][\"success\"]:\n",
    "            print(f\"  ⚠️  跳過 {config_name}，ONNX 模型不可用\")\n",
    "            continue\n",
    "        \n",
    "        engine_path = TRT_DIR / f\"text_classifier_{config_name}.engine\"\n",
    "        \n",
    "        # 構建引擎\n",
    "        success = builder.build_engine_from_onnx(onnx_path, engine_path, build_config)\n",
    "        build_results[config_name] = {\n",
    "            \"path\": engine_path,\n",
    "            \"success\": success,\n",
    "            \"config\": build_config\n",
    "        }\n",
    "        \n",
    "        if success:\n",
    "            print(f\"  ✅ 引擎構建成功\")\n",
    "        else:\n",
    "            print(f\"  ❌ 引擎構建失敗\")\n",
    "    \n",
    "    print(f\"\\n📊 TensorRT 引擎構建總結: {sum(1 for r in build_results.values() if r['success'])}/{len(build_results)} 成功\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  TensorRT 未安裝，跳過引擎構建\")\n",
    "    print(\"   可以使用 trtexec 命令行工具作為替代\")\n",
    "    \n",
    "    # 生成 trtexec 命令示例\n",
    "    trtexec_commands = []\n",
    "    \n",
    "    if export_results[\"fixed\"][\"success\"]:\n",
    "        onnx_path = export_results[\"fixed\"][\"path\"]\n",
    "        engine_path = TRT_DIR / \"text_classifier_fp16.engine\"\n",
    "        \n",
    "        cmd = f\"\"\"trtexec --onnx={onnx_path} \\\n",
    "  --saveEngine={engine_path} \\\n",
    "  --fp16 \\\n",
    "  --workspace=1024 \\\n",
    "  --verbose\"\"\"\n",
    "        \n",
    "        trtexec_commands.append((\"FP16 引擎\", cmd))\n",
    "    \n",
    "    if trtexec_commands:\n",
    "        print(\"\\n📜 trtexec 命令參考:\")\n",
    "        for name, cmd in trtexec_commands:\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(cmd)\n",
    "    \n",
    "    build_results = {}  # 空字典用於後續代碼兼容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Triton TensorRT Backend 配置\n",
    "\n",
    "### 建立 Triton 模型倉庫結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonTensorRTModelGenerator:\n",
    "    \"\"\"Triton TensorRT 模型生成器\"\"\"\n",
    "    \n",
    "    def __init__(self, model_repo_path: Path):\n",
    "        self.model_repo = model_repo_path\n",
    "        \n",
    "    def create_tensorrt_model(self, \n",
    "                             model_name: str,\n",
    "                             engine_path: Path,\n",
    "                             input_spec: Dict[str, Any],\n",
    "                             output_spec: Dict[str, Any],\n",
    "                             config_params: Dict[str, Any] = None) -> bool:\n",
    "        \"\"\"創建 TensorRT 模型配置\"\"\"\n",
    "        \n",
    "        model_dir = self.model_repo / model_name\n",
    "        version_dir = model_dir / \"1\"\n",
    "        version_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 複製引擎文件\n",
    "        if engine_path.exists():\n",
    "            target_engine = version_dir / \"model.plan\"\n",
    "            shutil.copy2(engine_path, target_engine)\n",
    "            logger.info(f\"TensorRT 引擎已複製到: {target_engine}\")\n",
    "        else:\n",
    "            logger.error(f\"引擎文件不存在: {engine_path}\")\n",
    "            return False\n",
    "        \n",
    "        # 生成配置文件\n",
    "        config = self._generate_config(model_name, input_spec, output_spec, config_params)\n",
    "        config_path = model_dir / \"config.pbtxt\"\n",
    "        \n",
    "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(config)\n",
    "        \n",
    "        logger.info(f\"配置文件已創建: {config_path}\")\n",
    "        return True\n",
    "    \n",
    "    def _generate_config(self, \n",
    "                        model_name: str,\n",
    "                        input_spec: Dict[str, Any],\n",
    "                        output_spec: Dict[str, Any],\n",
    "                        config_params: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"生成 TensorRT 模型配置\"\"\"\n",
    "        \n",
    "        config_params = config_params or {}\n",
    "        \n",
    "        # 基本配置\n",
    "        config_lines = [\n",
    "            f'name: \"{model_name}\"',\n",
    "            'platform: \"tensorrt_plan\"',\n",
    "            f'max_batch_size: {config_params.get(\"max_batch_size\", 16)}',\n",
    "            ''\n",
    "        ]\n",
    "        \n",
    "        # 輸入配置\n",
    "        for input_name, spec in input_spec.items():\n",
    "            config_lines.extend([\n",
    "                'input {',\n",
    "                f'  name: \"{input_name}\"',\n",
    "                f'  data_type: {spec[\"data_type\"]}',\n",
    "                f'  dims: {spec[\"dims\"]}',\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # 輸出配置\n",
    "        for output_name, spec in output_spec.items():\n",
    "            config_lines.extend([\n",
    "                'output {',\n",
    "                f'  name: \"{output_name}\"',\n",
    "                f'  data_type: {spec[\"data_type\"]}',\n",
    "                f'  dims: {spec[\"dims\"]}',\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # 實例組配置\n",
    "        instance_count = config_params.get(\"instance_count\", 1)\n",
    "        config_lines.extend([\n",
    "            'instance_group {',\n",
    "            f'  count: {instance_count}',\n",
    "            '  kind: KIND_GPU',\n",
    "            '}',\n",
    "            ''\n",
    "        ])\n",
    "        \n",
    "        # 動態批次配置\n",
    "        if config_params.get(\"enable_dynamic_batching\", True):\n",
    "            preferred_batch_sizes = config_params.get(\"preferred_batch_sizes\", [1, 4, 8])\n",
    "            config_lines.extend([\n",
    "                'dynamic_batching {',\n",
    "                f'  preferred_batch_size: {preferred_batch_sizes}',\n",
    "                f'  max_queue_delay_microseconds: {config_params.get(\"max_queue_delay\", 100)}',\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # TensorRT 特定優化\n",
    "        config_lines.extend([\n",
    "            '# TensorRT 優化參數',\n",
    "            'optimization {',\n",
    "            '  cuda {',\n",
    "            '    graphs: true',\n",
    "            '    graph_spec {',\n",
    "            '      batch_size: 1',\n",
    "            '      input {',\n",
    "            f'        key: \"{list(input_spec.keys())[0]}\"',\n",
    "            '        value {',\n",
    "            f'          dim: [ {config_params.get(\"opt_input_dims\", \"128\")} ]',\n",
    "            '        }',\n",
    "            '      }',\n",
    "            '    }',\n",
    "            '  }',\n",
    "            '}',\n",
    "            '',\n",
    "            '# 模型預熱',\n",
    "            'model_warmup {',\n",
    "            '  name: \"warmup_sample\"',\n",
    "            '  batch_size: 1',\n",
    "            '  inputs {',\n",
    "            f'    key: \"{list(input_spec.keys())[0]}\"',\n",
    "            '    value {',\n",
    "            f'      data_type: {list(input_spec.values())[0][\"data_type\"]}',\n",
    "            f'      dims: [ {config_params.get(\"warmup_dims\", \"128\")} ]',\n",
    "            '      zero_data: true',\n",
    "            '    }',\n",
    "            '  }',\n",
    "            '}'\n",
    "        ])\n",
    "        \n",
    "        return '\\n'.join(config_lines)\n",
    "\n",
    "# 創建 Triton 模型生成器\n",
    "model_generator = TritonTensorRTModelGenerator(MODEL_REPO)\n",
    "\n",
    "# 為每個成功構建的引擎創建 Triton 模型\n",
    "triton_models = {}\n",
    "\n",
    "if TRT_AVAILABLE and build_results:\n",
    "    for config_name, result in build_results.items():\n",
    "        if result[\"success\"]:\n",
    "            model_name = f\"text_classifier_trt_{config_name}\"\n",
    "            \n",
    "            # 輸入輸出規格\n",
    "            if \"dynamic\" in config_name:\n",
    "                input_spec = {\n",
    "                    \"input_ids\": {\n",
    "                        \"data_type\": \"TYPE_INT64\",\n",
    "                        \"dims\": [-1]  # 動態序列長度\n",
    "                    }\n",
    "                }\n",
    "                max_batch_size = 0  # 動態批次\n",
    "            else:\n",
    "                input_spec = {\n",
    "                    \"input_ids\": {\n",
    "                        \"data_type\": \"TYPE_INT64\",\n",
    "                        \"dims\": [128]  # 固定序列長度\n",
    "                    }\n",
    "                }\n",
    "                max_batch_size = 16\n",
    "            \n",
    "            output_spec = {\n",
    "                \"logits\": {\n",
    "                    \"data_type\": \"TYPE_FP32\",\n",
    "                    \"dims\": [5]  # 5 個分類\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 配置參數\n",
    "            config_params = {\n",
    "                \"max_batch_size\": max_batch_size,\n",
    "                \"instance_count\": 1,\n",
    "                \"enable_dynamic_batching\": max_batch_size > 0,\n",
    "                \"preferred_batch_sizes\": [1, 4, 8] if max_batch_size > 0 else [],\n",
    "                \"opt_input_dims\": \"128\",\n",
    "                \"warmup_dims\": \"128\"\n",
    "            }\n",
    "            \n",
    "            # 創建模型\n",
    "            success = model_generator.create_tensorrt_model(\n",
    "                model_name,\n",
    "                result[\"path\"],\n",
    "                input_spec,\n",
    "                output_spec,\n",
    "                config_params\n",
    "            )\n",
    "            \n",
    "            triton_models[model_name] = {\n",
    "                \"engine_path\": result[\"path\"],\n",
    "                \"success\": success,\n",
    "                \"config_name\": config_name\n",
    "            }\n",
    "            \n",
    "            if success:\n",
    "                print(f\"✅ Triton 模型已創建: {model_name}\")\n",
    "            else:\n",
    "                print(f\"❌ Triton 模型創建失敗: {model_name}\")\n",
    "\n",
    "# 總結\n",
    "print(f\"\\n📊 Triton TensorRT 模型創建總結:\")\n",
    "print(f\"   成功: {sum(1 for m in triton_models.values() if m['success'])}\")\n",
    "print(f\"   總計: {len(triton_models)}\")\n",
    "\n",
    "if not TRT_AVAILABLE:\n",
    "    print(\"\\n⚠️  TensorRT 未安裝，已跳過模型創建\")\n",
    "    print(\"   安裝 TensorRT 後可體驗完整功能\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 性能基準測試系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveBenchmark:\n",
    "    \"\"\"綜合性能基準測試系統\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.results = {}\n",
    "        \n",
    "    def benchmark_model(self, \n",
    "                       model_name: str,\n",
    "                       test_data: List[np.ndarray],\n",
    "                       iterations: int = 100,\n",
    "                       warmup_iterations: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"單個模型基準測試\"\"\"\n",
    "        \n",
    "        try:\n",
    "            client = httpclient.InferenceServerClient(url=self.triton_url)\n",
    "            \n",
    "            if not client.is_model_ready(model_name):\n",
    "                logger.warning(f\"模型 {model_name} 未就緒\")\n",
    "                return {\"error\": \"Model not ready\"}\n",
    "            \n",
    "            logger.info(f\"開始基準測試: {model_name}\")\n",
    "            \n",
    "            # 預熱\n",
    "            for i in range(warmup_iterations):\n",
    "                test_input = test_data[i % len(test_data)]\n",
    "                self._single_inference(client, model_name, test_input)\n",
    "            \n",
    "            logger.info(f\"預熱完成，開始正式測試 ({iterations} 次)\")\n",
    "            \n",
    "            # 正式測試\n",
    "            latencies = []\n",
    "            errors = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for i in range(iterations):\n",
    "                test_input = test_data[i % len(test_data)]\n",
    "                \n",
    "                try:\n",
    "                    request_start = time.time()\n",
    "                    response = self._single_inference(client, model_name, test_input)\n",
    "                    request_end = time.time()\n",
    "                    \n",
    "                    latency = (request_end - request_start) * 1000\n",
    "                    latencies.append(latency)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    logger.error(f\"推理錯誤: {e}\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "            \n",
    "            # 計算統計\n",
    "            if latencies:\n",
    "                stats = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"total_requests\": iterations,\n",
    "                    \"successful_requests\": len(latencies),\n",
    "                    \"error_count\": errors,\n",
    "                    \"error_rate_percent\": (errors / iterations) * 100,\n",
    "                    \"total_time_seconds\": total_time,\n",
    "                    \"throughput_rps\": len(latencies) / total_time,\n",
    "                    \"latency_ms\": {\n",
    "                        \"mean\": np.mean(latencies),\n",
    "                        \"median\": np.median(latencies),\n",
    "                        \"p95\": np.percentile(latencies, 95),\n",
    "                        \"p99\": np.percentile(latencies, 99),\n",
    "                        \"min\": np.min(latencies),\n",
    "                        \"max\": np.max(latencies),\n",
    "                        \"std\": np.std(latencies)\n",
    "                    },\n",
    "                    \"raw_latencies\": latencies\n",
    "                }\n",
    "            else:\n",
    "                stats = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"total_requests\": iterations,\n",
    "                    \"successful_requests\": 0,\n",
    "                    \"error_count\": errors,\n",
    "                    \"error_rate_percent\": 100.0,\n",
    "                    \"throughput_rps\": 0\n",
    "                }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"基準測試失敗 {model_name}: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def _single_inference(self, client, model_name: str, input_data: np.ndarray):\n",
    "        \"\"\"執行單次推理\"\"\"\n",
    "        inputs = [httpclient.InferInput(\"input_ids\", input_data.shape, \"INT64\")]\n",
    "        inputs[0].set_data_from_numpy(input_data)\n",
    "        \n",
    "        response = client.infer(model_name, inputs)\n",
    "        return response.as_numpy(\"logits\")\n",
    "    \n",
    "    def compare_models(self, \n",
    "                      model_names: List[str],\n",
    "                      test_data: List[np.ndarray],\n",
    "                      iterations: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"比較多個模型性能\"\"\"\n",
    "        \n",
    "        comparison_results = {}\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            print(f\"\\n🚀 測試模型: {model_name}\")\n",
    "            result = self.benchmark_model(model_name, test_data, iterations)\n",
    "            comparison_results[model_name] = result\n",
    "            \n",
    "            if \"error\" not in result and \"latency_ms\" in result:\n",
    "                latency_stats = result[\"latency_ms\"]\n",
    "                print(f\"  ⚡ 吞吐量: {result['throughput_rps']:.2f} RPS\")\n",
    "                print(f\"  📊 平均延遲: {latency_stats['mean']:.2f}ms\")\n",
    "                print(f\"  📈 P95 延遲: {latency_stats['p95']:.2f}ms\")\n",
    "                print(f\"  ❌ 錯誤率: {result['error_rate_percent']:.2f}%\")\n",
    "            else:\n",
    "                print(f\"  ❌ 測試失敗: {result.get('error', '未知錯誤')}\")\n",
    "        \n",
    "        return comparison_results\n",
    "    \n",
    "    def generate_performance_report(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"生成性能報告\"\"\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"# TensorRT vs PyTorch 性能對比報告\",\n",
    "            \"=\" * 50,\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        # 成功的結果\n",
    "        successful_results = {\n",
    "            k: v for k, v in results.items() \n",
    "            if \"error\" not in v and \"latency_ms\" in v\n",
    "        }\n",
    "        \n",
    "        if not successful_results:\n",
    "            return \"無可用的測試結果\\n\"\n",
    "        \n",
    "        # 按吞吐量排序\n",
    "        sorted_results = sorted(\n",
    "            successful_results.items(),\n",
    "            key=lambda x: x[1][\"throughput_rps\"],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # 詳細結果表格\n",
    "        report_lines.extend([\n",
    "            \"## 詳細性能指標\",\n",
    "            \"\",\n",
    "            \"| 模型 | 吞吐量 (RPS) | 平均延遲 (ms) | P95 延遲 (ms) | 錯誤率 (%) |\",\n",
    "            \"|------|-------------|--------------|--------------|----------|\"\n",
    "        ])\n",
    "        \n",
    "        for model_name, result in sorted_results:\n",
    "            throughput = result[\"throughput_rps\"]\n",
    "            mean_latency = result[\"latency_ms\"][\"mean\"]\n",
    "            p95_latency = result[\"latency_ms\"][\"p95\"]\n",
    "            error_rate = result[\"error_rate_percent\"]\n",
    "            \n",
    "            report_lines.append(\n",
    "                f\"| {model_name} | {throughput:.2f} | {mean_latency:.2f} | {p95_latency:.2f} | {error_rate:.2f} |\"\n",
    "            )\n",
    "        \n",
    "        # 性能提升分析\n",
    "        if len(sorted_results) > 1:\n",
    "            baseline_name, baseline_result = sorted_results[-1]  # 最慢的作為基線\n",
    "            baseline_throughput = baseline_result[\"throughput_rps\"]\n",
    "            baseline_latency = baseline_result[\"latency_ms\"][\"mean\"]\n",
    "            \n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"## 性能提升分析\",\n",
    "                f\"(以 {baseline_name} 作為基線)\",\n",
    "                \"\"\n",
    "            ])\n",
    "            \n",
    "            for model_name, result in sorted_results[:-1]:\n",
    "                throughput_speedup = result[\"throughput_rps\"] / baseline_throughput\n",
    "                latency_speedup = baseline_latency / result[\"latency_ms\"][\"mean\"]\n",
    "                \n",
    "                report_lines.append(\n",
    "                    f\"- **{model_name}**: {throughput_speedup:.2f}x 吞吐量提升, {latency_speedup:.2f}x 延遲改善\"\n",
    "                )\n",
    "        \n",
    "        # 建議\n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"## 部署建議\",\n",
    "            \"\",\n",
    "            \"### 高吞吐量場景\",\n",
    "            f\"- 推薦: {sorted_results[0][0]}\",\n",
    "            f\"- 吞吐量: {sorted_results[0][1]['throughput_rps']:.2f} RPS\",\n",
    "            \"\",\n",
    "            \"### 低延遲場景\",\n",
    "        ])\n",
    "        \n",
    "        # 找出延遲最低的模型\n",
    "        lowest_latency = min(\n",
    "            successful_results.items(),\n",
    "            key=lambda x: x[1][\"latency_ms\"][\"mean\"]\n",
    "        )\n",
    "        \n",
    "        report_lines.extend([\n",
    "            f\"- 推薦: {lowest_latency[0]}\",\n",
    "            f\"- 平均延遲: {lowest_latency[1]['latency_ms']['mean']:.2f} ms\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# 創建基準測試器\n",
    "benchmark = ComprehensiveBenchmark()\n",
    "\n",
    "# 準備測試數據\n",
    "def generate_tensorrt_test_data(num_samples: int = 50) -> List[np.ndarray]:\n",
    "    \"\"\"生成 TensorRT 測試數據\"\"\"\n",
    "    test_data = []\n",
    "    \n",
    "    # 固定長度序列（適合固定形狀引擎）\n",
    "    lengths = [128]  # TensorRT 通常使用固定形狀效果更好\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        length = np.random.choice(lengths)\n",
    "        # 生成隨機序列\n",
    "        sequence = np.random.randint(1, 5000, size=(1, length), dtype=np.int64)\n",
    "        test_data.append(sequence)\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "tensorrt_test_data = generate_tensorrt_test_data()\n",
    "print(f\"📋 TensorRT 測試數據已生成: {len(tensorrt_test_data)} 個樣本\")\n",
    "print(f\"📏 序列長度: {[data.shape[1] for data in tensorrt_test_data[:5]]}\")\n",
    "\n",
    "# 顯示可用模型\n",
    "available_models = [name for name, info in triton_models.items() if info[\"success\"]]\n",
    "print(f\"\\n📋 可用的 TensorRT 模型:\")\n",
    "for model in available_models:\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "if not available_models:\n",
    "    print(\"⚠️  沒有可用的 TensorRT 模型進行測試\")\n",
    "    print(\"   請確保 TensorRT 引擎構建成功並且 Triton Server 正在運行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 執行性能對比測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_triton_server(max_wait_time: int = 60) -> bool:\n",
    "    \"\"\"等待 Triton Server 就緒\"\"\"\n",
    "    print(\"⏳ 檢查 Triton Server 狀態...\")\n",
    "    \n",
    "    for i in range(max_wait_time):\n",
    "        try:\n",
    "            client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "            if client.is_server_ready():\n",
    "                print(\"✅ Triton Server 已就緒\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(f\"  等待中... ({i}/{max_wait_time}s)\")\n",
    "    \n",
    "    print(\"❌ Triton Server 連接超時\")\n",
    "    return False\n",
    "\n",
    "def run_tensorrt_benchmark():\n",
    "    \"\"\"執行 TensorRT 性能基準測試\"\"\"\n",
    "    \n",
    "    if not wait_for_triton_server():\n",
    "        print(\"無法連接到 Triton Server\")\n",
    "        print(\"\\n🚀 要啟動 Triton Server，請執行:\")\n",
    "        print(\"   docker run -d --name triton-trt --gpus all \\\\\")\n",
    "        print(\"     -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\\\")\n",
    "        print(f\"     -v {MODEL_REPO.absolute()}:/models \\\\\")\n",
    "        print(\"     nvcr.io/nvidia/tritonserver:24.01-py3 \\\\\")\n",
    "        print(\"     tritonserver --model-repository=/models\")\n",
    "        return None\n",
    "    \n",
    "    # 檢查可用模型\n",
    "    try:\n",
    "        client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "        server_models = [m[\"name\"] for m in client.get_model_repository_index()]\n",
    "        \n",
    "        # 過濾出實際可用的模型\n",
    "        testable_models = []\n",
    "        for model_name in available_models:\n",
    "            if model_name in server_models:\n",
    "                try:\n",
    "                    if client.is_model_ready(model_name):\n",
    "                        testable_models.append(model_name)\n",
    "                        print(f\"✅ 模型就緒: {model_name}\")\n",
    "                    else:\n",
    "                        print(f\"⚠️  模型未就緒: {model_name}\")\n",
    "                except:\n",
    "                    print(f\"❌ 模型檢查失敗: {model_name}\")\n",
    "        \n",
    "        if not testable_models:\n",
    "            print(\"❌ 沒有可測試的模型\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\n🎯 開始性能對比測試 ({len(testable_models)} 個模型)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 執行基準測試\n",
    "        comparison_results = benchmark.compare_models(\n",
    "            testable_models,\n",
    "            tensorrt_test_data,\n",
    "            iterations=100\n",
    "        )\n",
    "        \n",
    "        # 生成報告\n",
    "        performance_report = benchmark.generate_performance_report(comparison_results)\n",
    "        \n",
    "        # 保存報告\n",
    "        report_path = BENCHMARK_DIR / \"tensorrt_performance_report.md\"\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(performance_report)\n",
    "        \n",
    "        print(f\"\\n📊 性能報告已儲存: {report_path}\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(performance_report)\n",
    "        \n",
    "        return comparison_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 基準測試執行失敗: {e}\")\n",
    "        return None\n",
    "\n",
    "# 執行測試（如果有可用模型）\n",
    "if available_models:\n",
    "    print(\"🎯 準備執行 TensorRT 性能基準測試\")\n",
    "    benchmark_results = run_tensorrt_benchmark()\n",
    "else:\n",
    "    print(\"⚠️  跳過性能測試：沒有可用的 TensorRT 模型\")\n",
    "    benchmark_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能視覺化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_visualization(results: Dict[str, Any]):\n",
    "    \"\"\"創建性能視覺化圖表\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"⚠️  沒有性能數據可視覺化\")\n",
    "        return\n",
    "    \n",
    "    # 過濾成功的結果\n",
    "    successful_results = {\n",
    "        k: v for k, v in results.items() \n",
    "        if \"error\" not in v and \"latency_ms\" in v\n",
    "    }\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"⚠️  沒有成功的測試結果\")\n",
    "        return\n",
    "    \n",
    "    # 準備數據\n",
    "    model_names = list(successful_results.keys())\n",
    "    throughputs = [result[\"throughput_rps\"] for result in successful_results.values()]\n",
    "    mean_latencies = [result[\"latency_ms\"][\"mean\"] for result in successful_results.values()]\n",
    "    p95_latencies = [result[\"latency_ms\"][\"p95\"] for result in successful_results.values()]\n",
    "    \n",
    "    # 簡化模型名稱（用於顯示）\n",
    "    display_names = []\n",
    "    for name in model_names:\n",
    "        if \"fp32\" in name:\n",
    "            display_names.append(\"TensorRT FP32\")\n",
    "        elif \"fp16\" in name:\n",
    "            if \"dynamic\" in name:\n",
    "                display_names.append(\"TensorRT FP16\\n(Dynamic)\")\n",
    "            else:\n",
    "                display_names.append(\"TensorRT FP16\")\n",
    "        else:\n",
    "            display_names.append(name.replace(\"text_classifier_trt_\", \"TRT \"))\n",
    "    \n",
    "    # 創建圖表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('TensorRT Backend 性能分析', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 顏色配置\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    \n",
    "    # 1. 吞吐量對比\n",
    "    axes[0, 0].bar(range(len(display_names)), throughputs, \n",
    "                   color=colors[:len(display_names)], alpha=0.8)\n",
    "    axes[0, 0].set_title('吞吐量對比 (RPS)', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('模型配置')\n",
    "    axes[0, 0].set_ylabel('每秒請求數 (RPS)')\n",
    "    axes[0, 0].set_xticks(range(len(display_names)))\n",
    "    axes[0, 0].set_xticklabels(display_names, rotation=45, ha='right')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 添加數值標籤\n",
    "    for i, v in enumerate(throughputs):\n",
    "        axes[0, 0].text(i, v + max(throughputs) * 0.01, f'{v:.1f}', \n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. 延遲對比\n",
    "    x = np.arange(len(display_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, mean_latencies, width, label='平均延遲', \n",
    "                   color='skyblue', alpha=0.8)\n",
    "    axes[0, 1].bar(x + width/2, p95_latencies, width, label='P95 延遲',\n",
    "                   color='orange', alpha=0.8)\n",
    "    axes[0, 1].set_title('延遲對比 (毫秒)', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('模型配置')\n",
    "    axes[0, 1].set_ylabel('延遲 (ms)')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(display_names, rotation=45, ha='right')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. 延遲分佈（箱線圖）\n",
    "    if len(successful_results) > 1:\n",
    "        latency_data = [result[\"raw_latencies\"] for result in successful_results.values()]\n",
    "        bp = axes[1, 0].boxplot(latency_data, labels=display_names, patch_artist=True)\n",
    "        \n",
    "        # 設定顏色\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(display_names)]):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        axes[1, 0].set_title('延遲分佈', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('模型配置')\n",
    "        axes[1, 0].set_ylabel('延遲 (ms)')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, '需要多個模型\\n進行分佈比較', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    \n",
    "    # 4. 性能提升倍數\n",
    "    if len(successful_results) > 1:\n",
    "        # 以最慢的模型作為基線\n",
    "        baseline_throughput = min(throughputs)\n",
    "        baseline_latency = max(mean_latencies)\n",
    "        \n",
    "        speedup_throughput = [t / baseline_throughput for t in throughputs]\n",
    "        speedup_latency = [baseline_latency / l for l in mean_latencies]\n",
    "        \n",
    "        x = np.arange(len(display_names))\n",
    "        axes[1, 1].bar(x - width/2, speedup_throughput, width, \n",
    "                      label='吞吐量提升', color='lightgreen', alpha=0.8)\n",
    "        axes[1, 1].bar(x + width/2, speedup_latency, width,\n",
    "                      label='延遲改善', color='lightcoral', alpha=0.8)\n",
    "        \n",
    "        axes[1, 1].set_title('性能提升倍數', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('模型配置')\n",
    "        axes[1, 1].set_ylabel('提升倍數')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(display_names, rotation=45, ha='right')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 添加 1x 基線\n",
    "        axes[1, 1].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='基線')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, '需要多個模型\\n進行提升比較', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存圖表\n",
    "    chart_path = BENCHMARK_DIR / \"tensorrt_performance_charts.png\"\n",
    "    plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"📊 性能圖表已儲存: {chart_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 創建視覺化圖表（如果有測試結果）\n",
    "if benchmark_results:\n",
    "    print(\"📊 生成性能視覺化圖表...\")\n",
    "    create_performance_visualization(benchmark_results)\n",
    "else:\n",
    "    print(\"⚠️  跳過視覺化：沒有測試結果\")\n",
    "    \n",
    "    # 創建模擬圖表用於演示\n",
    "    print(\"📊 生成示例性能圖表...\")\n",
    "    \n",
    "    # 模擬數據\n",
    "    mock_results = {\n",
    "        \"text_classifier_trt_fp32\": {\n",
    "            \"throughput_rps\": 85.2,\n",
    "            \"latency_ms\": {\n",
    "                \"mean\": 11.7,\n",
    "                \"p95\": 18.4\n",
    "            },\n",
    "            \"raw_latencies\": np.random.normal(11.7, 2.5, 100).tolist()\n",
    "        },\n",
    "        \"text_classifier_trt_fp16\": {\n",
    "            \"throughput_rps\": 156.8,\n",
    "            \"latency_ms\": {\n",
    "                \"mean\": 6.4,\n",
    "                \"p95\": 10.2\n",
    "            },\n",
    "            \"raw_latencies\": np.random.normal(6.4, 1.8, 100).tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    create_performance_visualization(mock_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Triton TensorRT 部署腳本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成完整的 Triton TensorRT 部署腳本\n",
    "deployment_script = f'''\n",
    "#!/bin/bash\n",
    "\n",
    "# Triton TensorRT Backend 完整部署腳本\n",
    "# 日期: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "set -e  # 遇到錯誤立即退出\n",
    "\n",
    "echo \"🚀 Triton TensorRT Backend 部署腳本\"\n",
    "echo \"=====================================\"\n",
    "\n",
    "# 配置變數\n",
    "MODEL_REPO=\"{MODEL_REPO.absolute()}\"\n",
    "CONTAINER_NAME=\"triton-tensorrt-server\"\n",
    "TRITON_IMAGE=\"nvcr.io/nvidia/tritonserver:24.01-py3\"\n",
    "\n",
    "# 檢查 Docker\n",
    "if ! command -v docker &> /dev/null; then\n",
    "    echo \"❌ Docker 未安裝，請先安裝 Docker\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# 檢查 NVIDIA Container Toolkit\n",
    "if ! docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi &> /dev/null; then\n",
    "    echo \"❌ NVIDIA Container Toolkit 未安裝或配置錯誤\"\n",
    "    echo \"   請安裝 NVIDIA Container Toolkit\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"✅ Docker 和 GPU 支援檢查通過\"\n",
    "\n",
    "# 停止並移除現有容器\n",
    "echo \"🧹 清理現有容器...\"\n",
    "docker stop $CONTAINER_NAME 2>/dev/null || true\n",
    "docker rm $CONTAINER_NAME 2>/dev/null || true\n",
    "\n",
    "# 檢查模型倉庫\n",
    "if [ ! -d \"$MODEL_REPO\" ]; then\n",
    "    echo \"❌ 模型倉庫不存在: $MODEL_REPO\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"📁 模型倉庫: $MODEL_REPO\"\n",
    "echo \"📋 可用模型:\"\n",
    "ls -la \"$MODEL_REPO\"\n",
    "\n",
    "# 啟動 Triton Server\n",
    "echo \"🚀 啟動 Triton TensorRT Server...\"\n",
    "docker run -d \\\n",
    "  --name $CONTAINER_NAME \\\n",
    "  --gpus all \\\n",
    "  --shm-size=1g \\\n",
    "  --ulimit memlock=-1 \\\n",
    "  --ulimit stack=67108864 \\\n",
    "  -p 8000:8000 \\\n",
    "  -p 8001:8001 \\\n",
    "  -p 8002:8002 \\\n",
    "  -v \"$MODEL_REPO:/models\" \\\n",
    "  -e CUDA_VISIBLE_DEVICES=0 \\\n",
    "  $TRITON_IMAGE \\\n",
    "  tritonserver \\\n",
    "    --model-repository=/models \\\n",
    "    --backend-directory=/opt/tritonserver/backends \\\n",
    "    --model-control-mode=explicit \\\n",
    "    --strict-model-config=false \\\n",
    "    --log-verbose=1 \\\n",
    "    --log-info=true \\\n",
    "    --log-warning=true \\\n",
    "    --log-error=true \\\n",
    "    --exit-on-error=false \\\n",
    "    --exit-timeout-secs=120 \\\n",
    "    --buffer-manager-thread-count=4 \\\n",
    "    --model-load-thread-count=8 \\\n",
    "    --backend-config=tensorrt,default-max-workspace-size=1073741824\n",
    "\n",
    "# 等待服務器啟動\n",
    "echo \"⏳ 等待 Triton Server 啟動...\"\n",
    "for i in {{1..30}}; do\n",
    "    if curl -s http://localhost:8000/v2/health/ready > /dev/null; then\n",
    "        echo \"✅ Triton Server 已就緒\"\n",
    "        break\n",
    "    fi\n",
    "    echo \"  等待中... ($i/30)\"\n",
    "    sleep 2\n",
    "done\n",
    "\n",
    "# 檢查服務器狀態\n",
    "if ! curl -s http://localhost:8000/v2/health/ready > /dev/null; then\n",
    "    echo \"❌ Triton Server 啟動失敗\"\n",
    "    echo \"容器日誌:\"\n",
    "    docker logs $CONTAINER_NAME\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# 顯示服務資訊\n",
    "echo \"🌐 Triton Server 端點:\"\n",
    "echo \"   HTTP:    http://localhost:8000\"\n",
    "echo \"   gRPC:    localhost:8001\"\n",
    "echo \"   Metrics: http://localhost:8002/metrics\"\n",
    "\n",
    "# 載入所有可用模型\n",
    "echo \"📥 載入 TensorRT 模型...\"\n",
    "'''\n",
    "\n",
    "# 為每個可用模型添加載入命令\n",
    "for model_name in available_models:\n",
    "    deployment_script += f'''\n",
    "echo \"  載入模型: {model_name}\"\n",
    "curl -X POST \"http://localhost:8000/v2/repository/models/{model_name}/load\" || echo \"載入失敗: {model_name}\"\n",
    "'''\n",
    "\n",
    "deployment_script += '''\n",
    "\n",
    "# 檢查模型狀態\n",
    "echo \"📋 檢查模型狀態:\"\n",
    "curl -s http://localhost:8000/v2/models | python3 -m json.tool\n",
    "\n",
    "echo \"🎉 部署完成！\"\n",
    "echo \"\\n💡 有用的命令:\"\n",
    "echo \"   查看容器日誌: docker logs -f $CONTAINER_NAME\"\n",
    "echo \"   停止服務:     docker stop $CONTAINER_NAME\"\n",
    "echo \"   進入容器:     docker exec -it $CONTAINER_NAME bash\"\n",
    "echo \"   模型統計:     curl http://localhost:8000/v2/models/stats\"\n",
    "'''\n",
    "\n",
    "# 儲存部署腳本\n",
    "deploy_script_path = SCRIPTS_DIR / \"deploy_tensorrt_triton.sh\"\n",
    "with open(deploy_script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(deployment_script.strip())\n",
    "\n",
    "# 設定執行權限\n",
    "os.chmod(deploy_script_path, 0o755)\n",
    "\n",
    "print(f\"📜 TensorRT 部署腳本已創建: {deploy_script_path}\")\n",
    "print(\"\\n🚀 要部署 Triton TensorRT Server，請執行:\")\n",
    "print(f\"   bash {deploy_script_path}\")\n",
    "\n",
    "# 創建測試腳本\n",
    "test_script = f'''\n",
    "#!/usr/bin/env python3\n",
    "# TensorRT 模型測試腳本\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "import time\n",
    "import json\n",
    "\n",
    "def test_tensorrt_models():\n",
    "    \"\"\"測試 TensorRT 模型\"\"\"\n",
    "    \n",
    "    triton_url = \"localhost:8000\"\n",
    "    client = httpclient.InferenceServerClient(url=triton_url)\n",
    "    \n",
    "    print(\"🎯 TensorRT 模型測試\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 檢查服務器狀態\n",
    "    if not client.is_server_ready():\n",
    "        print(\"❌ Triton Server 未就緒\")\n",
    "        return\n",
    "    \n",
    "    # 獲取模型列表\n",
    "    models = client.get_model_repository_index()\n",
    "    tensorrt_models = [m[\"name\"] for m in models if \"trt\" in m[\"name\"].lower()]\n",
    "    \n",
    "    if not tensorrt_models:\n",
    "        print(\"❌ 沒有找到 TensorRT 模型\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📋 找到 {{len(tensorrt_models)}} 個 TensorRT 模型\")\n",
    "    \n",
    "    # 測試每個模型\n",
    "    for model_name in tensorrt_models:\n",
    "        print(f\"\\n🚀 測試模型: {{model_name}}\")\n",
    "        \n",
    "        try:\n",
    "            # 檢查模型狀態\n",
    "            if not client.is_model_ready(model_name):\n",
    "                print(f\"  ⚠️  模型未就緒\")\n",
    "                continue\n",
    "            \n",
    "            # 創建測試輸入\n",
    "            test_input = np.random.randint(1, 1000, size=(1, 128), dtype=np.int64)\n",
    "            \n",
    "            # 推理測試\n",
    "            inputs = [httpclient.InferInput(\"input_ids\", test_input.shape, \"INT64\")]\n",
    "            inputs[0].set_data_from_numpy(test_input)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = client.infer(model_name, inputs)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # 獲取輸出\n",
    "            output = response.as_numpy(\"logits\")\n",
    "            \n",
    "            print(f\"  ✅ 推理成功\")\n",
    "            print(f\"  ⚡ 延遲: {{(end_time - start_time) * 1000:.2f}} ms\")\n",
    "            print(f\"  📊 輸出形狀: {{output.shape}}\")\n",
    "            print(f\"  🎯 預測類別: {{np.argmax(output[0])}}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 測試失敗: {{e}}\")\n",
    "    \n",
    "    print(\"\\n🎉 測試完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tensorrt_models()\n",
    "'''\n",
    "\n",
    "# 儲存測試腳本\n",
    "test_script_path = SCRIPTS_DIR / \"test_tensorrt_models.py\"\n",
    "with open(test_script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(test_script.strip())\n",
    "\n",
    "os.chmod(test_script_path, 0o755)\n",
    "\n",
    "print(f\"🧪 測試腳本已創建: {test_script_path}\")\n",
    "print(\"\\n📋 部署後測試命令:\")\n",
    "print(f\"   python3 {test_script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorRT 最佳實踐總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensorrt_best_practices():\n",
    "    \"\"\"生成 TensorRT 最佳實踐指南\"\"\"\n",
    "    \n",
    "    best_practices = {\n",
    "        \"模型轉換優化\": {\n",
    "            \"ONNX 導出\": [\n",
    "                \"使用較新的 ONNX opset 版本 (11+)\",\n",
    "                \"啟用常數摺疊 (constant folding)\",\n",
    "                \"合理設計動態軸，避免過度靈活性\",\n",
    "                \"驗證 PyTorch 和 ONNX 輸出一致性\"\n",
    "            ],\n",
    "            \"TensorRT 構建\": [\n",
    "                \"根據硬體選擇合適精度 (FP16/INT8)\",\n",
    "                \"設定足夠的工作空間大小 (1GB+)\",\n",
    "                \"為常用輸入形狀創建優化配置檔案\",\n",
    "                \"使用批次推理提高吞吐量\"\n",
    "            ]\n",
    "        },\n",
    "        \"性能優化策略\": {\n",
    "            \"精度選擇\": [\n",
    "                \"FP16: 2x 加速，極小精度損失，推薦首選\",\n",
    "                \"INT8: 4x 加速，需要校準數據，適合生產環境\",\n",
    "                \"動態範圍量化: 無校準數據的 INT8 替代\"\n",
    "            ],\n",
    "            \"批次策略\": [\n",
    "                \"固定批次大小獲得最佳性能\",\n",
    "                \"動態批次提供靈活性但犧牲部分性能\",\n",
    "                \"根據 GPU 記憶體調整最大批次大小\"\n",
    "            ],\n",
    "            \"記憶體管理\": [\n",
    "                \"預分配推理記憶體避免動態分配\",\n",
    "                \"使用 CUDA Stream 重疊計算和記憶體傳輸\",\n",
    "                \"監控 GPU 記憶體使用避免 OOM\"\n",
    "            ]\n",
    "        },\n",
    "        \"Triton 整合配置\": {\n",
    "            \"實例組設定\": [\n",
    "                \"根據 GPU 記憶體和延遲需求設定實例數量\",\n",
    "                \"大模型使用單實例，小模型可多實例並行\",\n",
    "                \"合理配置 GPU 親和性\"\n",
    "            ],\n",
    "            \"動態批次\": [\n",
    "                \"設定合理的偏好批次大小\",\n",
    "                \"調整佇列延遲平衡吞吐量和響應時間\",\n",
    "                \"使用優先級處理不同重要性的請求\"\n",
    "            ],\n",
    "            \"模型預熱\": [\n",
    "                \"為常用輸入大小配置預熱樣本\",\n",
    "                \"包含不同批次大小的預熱配置\",\n",
    "                \"避免首次推理的冷啟動延遲\"\n",
    "            ]\n",
    "        },\n",
    "        \"部署環境優化\": {\n",
    "            \"硬體配置\": [\n",
    "                \"使用支援 Tensor Core 的 GPU (V100/T4/RTX/A100)\",\n",
    "                \"確保 CUDA 版本與 TensorRT 版本兼容\",\n",
    "                \"配置足夠的系統記憶體支援模型載入\"\n",
    "            ],\n",
    "            \"容器化部署\": [\n",
    "                \"使用官方 Triton 容器映像\",\n",
    "                \"正確配置 GPU 資源分配\",\n",
    "                \"設定合適的共享記憶體大小 (--shm-size)\",\n",
    "                \"配置記憶體鎖定限制 (--ulimit memlock=-1)\"\n",
    "            ]\n",
    "        },\n",
    "        \"監控告警\": {\n",
    "            \"關鍵指標\": [\n",
    "                \"GPU 利用率和記憶體使用率\",\n",
    "                \"推理延遲分佈 (P50/P95/P99)\",\n",
    "                \"吞吐量和錯誤率\",\n",
    "                \"模型載入時間和預熱狀態\"\n",
    "            ],\n",
    "            \"告警閾值建議\": [\n",
    "                \"GPU 記憶體使用率 > 85%\",\n",
    "                \"P99 延遲增長 > 50%\",\n",
    "                \"錯誤率 > 0.1%\",\n",
    "                \"吞吐量下降 > 20%\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 生成報告\n",
    "    report_lines = [\n",
    "        \"# TensorRT Backend 最佳實踐指南\",\n",
    "        \"=\" * 40,\n",
    "        f\"生成時間: {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"\",\n",
    "        \"## 🎯 概述\",\n",
    "        \"\",\n",
    "        \"本指南總結了在 Triton Inference Server 中使用 TensorRT Backend 的最佳實踐，\",\n",
    "        \"包括模型轉換、性能優化、部署配置和監控策略。\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    for section_name, content in best_practices.items():\n",
    "        report_lines.append(f\"## 🔧 {section_name}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        for subsection, items in content.items():\n",
    "            report_lines.append(f\"### 📌 {subsection}\")\n",
    "            report_lines.append(\"\")\n",
    "            \n",
    "            for item in items:\n",
    "                report_lines.append(f\"- {item}\")\n",
    "            \n",
    "            report_lines.append(\"\")\n",
    "    \n",
    "    # 性能對比總結\n",
    "    if benchmark_results:\n",
    "        report_lines.extend([\n",
    "            \"## 📊 本次測試性能總結\",\n",
    "            \"\",\n",
    "            \"| 配置 | 吞吐量 (RPS) | 平均延遲 (ms) | 備註 |\",\n",
    "            \"|------|-------------|--------------|------|\"\n",
    "        ])\n",
    "        \n",
    "        for model_name, result in benchmark_results.items():\n",
    "            if \"error\" not in result and \"latency_ms\" in result:\n",
    "                throughput = result[\"throughput_rps\"]\n",
    "                latency = result[\"latency_ms\"][\"mean\"]\n",
    "                \n",
    "                # 簡化名稱和添加備註\n",
    "                if \"fp16\" in model_name:\n",
    "                    config = \"TensorRT FP16\"\n",
    "                    note = \"推薦配置\"\n",
    "                elif \"fp32\" in model_name:\n",
    "                    config = \"TensorRT FP32\"\n",
    "                    note = \"基線配置\"\n",
    "                else:\n",
    "                    config = model_name.replace(\"text_classifier_trt_\", \"\")\n",
    "                    note = \"-\"\n",
    "                \n",
    "                report_lines.append(\n",
    "                    f\"| {config} | {throughput:.1f} | {latency:.2f} | {note} |\"\n",
    "                )\n",
    "    \n",
    "    # 推薦配置模板\n",
    "    report_lines.extend([\n",
    "        \"\",\n",
    "        \"## 🏗️ 推薦配置模板\",\n",
    "        \"\",\n",
    "        \"### 高吞吐量場景\",\n",
    "        \"```protobuf\",\n",
    "        'name: \"model_high_throughput\"',\n",
    "        'platform: \"tensorrt_plan\"',\n",
    "        'max_batch_size: 32',\n",
    "        \"\",\n",
    "        'instance_group {',\n",
    "        '  count: 2',\n",
    "        '  kind: KIND_GPU',\n",
    "        '}',\n",
    "        \"\",\n",
    "        'dynamic_batching {',\n",
    "        '  preferred_batch_size: [ 8, 16, 32 ]',\n",
    "        '  max_queue_delay_microseconds: 1000',\n",
    "        '}',\n",
    "        \"```\",\n",
    "        \"\",\n",
    "        \"### 低延遲場景\",\n",
    "        \"```protobuf\",\n",
    "        'name: \"model_low_latency\"',\n",
    "        'platform: \"tensorrt_plan\"',\n",
    "        'max_batch_size: 4',\n",
    "        \"\",\n",
    "        'instance_group {',\n",
    "        '  count: 1',\n",
    "        '  kind: KIND_GPU',\n",
    "        '}',\n",
    "        \"\",\n",
    "        'dynamic_batching {',\n",
    "        '  preferred_batch_size: [ 1, 2 ]',\n",
    "        '  max_queue_delay_microseconds: 100',\n",
    "        '}',\n",
    "        \"```\",\n",
    "        \"\",\n",
    "        \"---\",\n",
    "        \"\",\n",
    "        \"💡 **提示**: 根據具體業務場景和硬體資源調整上述配置參數。\"\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "# 生成最佳實踐指南\n",
    "best_practices_guide = generate_tensorrt_best_practices()\n",
    "\n",
    "# 儲存指南\n",
    "guide_path = BASE_DIR / \"TensorRT_Best_Practices.md\"\n",
    "with open(guide_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(best_practices_guide)\n",
    "\n",
    "print(f\"📚 TensorRT 最佳實踐指南已生成: {guide_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(best_practices_guide[:2000] + \"...\" if len(best_practices_guide) > 2000 else best_practices_guide)\n",
    "\n",
    "# 生成實驗總結\n",
    "print(\"\\n🎉 TensorRT Backend 整合實驗完成！\")\n",
    "print(\"\\n📊 實驗成果總結:\")\n",
    "print(f\"  ✅ ONNX 模型導出: {sum(1 for r in export_results.values() if r['success'])}/{len(export_results)}\")\n",
    "\n",
    "if TRT_AVAILABLE:\n",
    "    print(f\"  ✅ TensorRT 引擎構建: {sum(1 for r in build_results.values() if r['success'])}/{len(build_results)}\")\n",
    "    print(f\"  ✅ Triton 模型配置: {sum(1 for m in triton_models.values() if m['success'])}/{len(triton_models)}\")\n",
    "else:\n",
    "    print(\"  ⚠️  TensorRT 引擎構建: 已跳過 (TensorRT 未安裝)\")\n",
    "    print(\"  ⚠️  Triton 模型配置: 已跳過\")\n",
    "\n",
    "print(f\"  ✅ 部署腳本生成: 完成\")\n",
    "print(f\"  ✅ 最佳實踐指南: 完成\")\n",
    "\n",
    "print(\"\\n📚 下一步學習建議:\")\n",
    "print(\"1. 🔧 實踐 vLLM Backend 整合 (Notebook 03)\")\n",
    "print(\"2. 🛠️  開發自定義 Python Backend (Notebook 04)\")\n",
    "print(\"3. 📊 深入 INT8 量化優化\")\n",
    "print(\"4. 🏢 多 GPU 分散式部署\")\n",
    "print(\"5. 🚀 生產環境性能調優\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 相關資源與延伸閱讀\n",
    "\n",
    "### 官方文檔\n",
    "- [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html)\n",
    "- [Triton TensorRT Backend](https://github.com/triton-inference-server/tensorrt_backend)\n",
    "- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)\n",
    "\n",
    "### 性能優化\n",
    "- [TensorRT 最佳實踐](https://docs.nvidia.com/deeplearning/tensorrt/best-practices/index.html)\n",
    "- [GPU 推理優化指南](https://developer.nvidia.com/blog/optimizing-inference-performance-using-nvidia-tensorrt/)\n",
    "- [混合精度推理](https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/)\n",
    "\n",
    "### 實踐案例\n",
    "- [TensorRT 在生產環境的部署](https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/)\n",
    "- [大規模推理服務優化](https://developer.nvidia.com/blog/maximizing-deep-learning-inference-performance-with-nvidia-model-analyzer/)\n",
    "\n",
    "### 工具資源\n",
    "- [Netron (模型視覺化)](https://netron.app/)\n",
    "- [NVIDIA Model Analyzer](https://github.com/triton-inference-server/model_analyzer)\n",
    "- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) (大語言模型專用)\n",
    "\n",
    "---\n",
    "\n",
    "**🎓 實驗完成標誌**: TensorRT Backend 整合與極致優化技術掌握 ✅\n",
    "\n",
    "**🚀 性能提升**: 相比 PyTorch 原生推理，TensorRT 可實現 2-10x 的性能提升，同時減少 30-50% 的記憶體使用。在企業級部署中，這種優化對於降低推理成本和提升用戶體驗具有重大意義。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}