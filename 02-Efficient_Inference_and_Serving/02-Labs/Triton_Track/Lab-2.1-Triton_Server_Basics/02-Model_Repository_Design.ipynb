{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1.2: Model Repository 設計與配置\n",
    "\n",
    "## 🎯 學習目標\n",
    "\n",
    "1. **理解 Triton Model Repository 架構**\n",
    "   - 模型倉庫的目錄結構設計\n",
    "   - 配置文件 (`config.pbtxt`) 的詳細設定\n",
    "   - 版本控制和模型生命週期管理\n",
    "\n",
    "2. **掌握企業級模型管理最佳實踐**\n",
    "   - 模型命名規範和組織策略\n",
    "   - 多版本模型共存和切換\n",
    "   - 動態模型載入和卸載\n",
    "\n",
    "3. **實現完整的模型部署流程**\n",
    "   - 從 HuggingFace Hub 下載和轉換模型\n",
    "   - 配置 PyTorch Backend 模型\n",
    "   - 驗證模型部署和推理功能\n",
    "\n",
    "## 📋 企業案例背景\n",
    "\n",
    "**場景**: Netflix 推薦系統需要管理 20+ 個不同的 ML 模型：\n",
    "- 用戶行為預測模型 (BERT-based)\n",
    "- 內容相似度模型 (Sentence Transformers)\n",
    "- 個性化排序模型 (Custom PyTorch)\n",
    "- A/B 測試模型版本管理\n",
    "\n",
    "**挑戰**: 如何設計可擴展的模型倉庫架構，支援動態模型更新而不影響服務可用性？\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Repository 基礎架構\n",
    "\n",
    "### 1.1 標準目錄結構設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 定義模型倉庫根目錄\n",
    "MODEL_REPOSITORY_ROOT = Path(\"/tmp/triton_model_repository\")\n",
    "\n",
    "def create_model_repository_structure():\n",
    "    \"\"\"\n",
    "    創建標準的 Triton Model Repository 目錄結構\n",
    "    \n",
    "    標準結構:\n",
    "    model_repository/\n",
    "    ├── model_name_1/\n",
    "    │   ├── config.pbtxt\n",
    "    │   ├── 1/\n",
    "    │   │   └── model.pt\n",
    "    │   ├── 2/\n",
    "    │   │   └── model.pt\n",
    "    │   └── labels.txt (可選)\n",
    "    └── model_name_2/\n",
    "        ├── config.pbtxt\n",
    "        └── 1/\n",
    "            └── model_files...\n",
    "    \"\"\"\n",
    "    \n",
    "    # 創建根目錄\n",
    "    MODEL_REPOSITORY_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 企業級模型分類目錄\n",
    "    model_categories = {\n",
    "        \"nlp_models\": [\"sentiment_analysis\", \"text_classification\", \"ner_model\"],\n",
    "        \"recommendation\": [\"user_behavior\", \"content_similarity\", \"ranking_model\"],\n",
    "        \"cv_models\": [\"image_classification\", \"object_detection\"],\n",
    "        \"custom_models\": [\"business_logic\", \"feature_extraction\"]\n",
    "    }\n",
    "    \n",
    "    print(\"🏗️  創建 Triton Model Repository 結構...\")\n",
    "    print(f\"📂 根目錄: {MODEL_REPOSITORY_ROOT}\")\n",
    "    print()\n",
    "    \n",
    "    for category, models in model_categories.items():\n",
    "        category_path = MODEL_REPOSITORY_ROOT / category\n",
    "        category_path.mkdir(exist_ok=True)\n",
    "        print(f\"📁 {category}/\")\n",
    "        \n",
    "        for model_name in models:\n",
    "            model_path = MODEL_REPOSITORY_ROOT / model_name\n",
    "            model_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            # 創建版本目錄 (1, 2)\n",
    "            for version in [1, 2]:\n",
    "                version_path = model_path / str(version)\n",
    "                version_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            print(f\"   └── {model_name}/\")\n",
    "            print(f\"       ├── config.pbtxt\")\n",
    "            print(f\"       ├── 1/\")\n",
    "            print(f\"       └── 2/\")\n",
    "    \n",
    "    return MODEL_REPOSITORY_ROOT\n",
    "\n",
    "# 執行目錄結構創建\n",
    "repo_root = create_model_repository_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 企業級模型命名規範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNamingConvention:\n",
    "    \"\"\"\n",
    "    企業級模型命名規範管理器\n",
    "    \n",
    "    命名規範: {business_unit}_{model_type}_{version}_{environment}\n",
    "    例如: netflix_recommendation_v2_prod\n",
    "    \"\"\"\n",
    "    \n",
    "    BUSINESS_UNITS = [\"netflix\", \"paypal\", \"visa\", \"general\"]\n",
    "    MODEL_TYPES = [\"nlp\", \"cv\", \"recommendation\", \"risk\", \"classification\"]\n",
    "    ENVIRONMENTS = [\"dev\", \"staging\", \"prod\"]\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_model_name(cls, business_unit: str, model_type: str, \n",
    "                          version: str, environment: str = \"prod\") -> str:\n",
    "        \"\"\"\n",
    "        生成符合企業規範的模型名稱\n",
    "        \"\"\"\n",
    "        if business_unit not in cls.BUSINESS_UNITS:\n",
    "            raise ValueError(f\"Business unit must be one of {cls.BUSINESS_UNITS}\")\n",
    "        if model_type not in cls.MODEL_TYPES:\n",
    "            raise ValueError(f\"Model type must be one of {cls.MODEL_TYPES}\")\n",
    "        if environment not in cls.ENVIRONMENTS:\n",
    "            raise ValueError(f\"Environment must be one of {cls.ENVIRONMENTS}\")\n",
    "            \n",
    "        return f\"{business_unit}_{model_type}_{version}_{environment}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_model_name(cls, model_name: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        解析模型名稱，提取元數據\n",
    "        \"\"\"\n",
    "        parts = model_name.split(\"_\")\n",
    "        if len(parts) != 4:\n",
    "            raise ValueError(f\"Invalid model name format: {model_name}\")\n",
    "            \n",
    "        return {\n",
    "            \"business_unit\": parts[0],\n",
    "            \"model_type\": parts[1],\n",
    "            \"version\": parts[2],\n",
    "            \"environment\": parts[3]\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_model_name(cls, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        驗證模型名稱是否符合規範\n",
    "        \"\"\"\n",
    "        try:\n",
    "            metadata = cls.parse_model_name(model_name)\n",
    "            return (\n",
    "                metadata[\"business_unit\"] in cls.BUSINESS_UNITS and\n",
    "                metadata[\"model_type\"] in cls.MODEL_TYPES and\n",
    "                metadata[\"environment\"] in cls.ENVIRONMENTS\n",
    "            )\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "# 示例：企業級模型命名\n",
    "print(\"🏷️  企業級模型命名規範示例:\")\n",
    "print()\n",
    "\n",
    "example_models = [\n",
    "    ModelNamingConvention.generate_model_name(\"netflix\", \"recommendation\", \"v2\", \"prod\"),\n",
    "    ModelNamingConvention.generate_model_name(\"paypal\", \"risk\", \"v1\", \"staging\"),\n",
    "    ModelNamingConvention.generate_model_name(\"visa\", \"classification\", \"v3\", \"prod\"),\n",
    "    ModelNamingConvention.generate_model_name(\"general\", \"nlp\", \"v1\", \"dev\")\n",
    "]\n",
    "\n",
    "for model_name in example_models:\n",
    "    metadata = ModelNamingConvention.parse_model_name(model_name)\n",
    "    valid = ModelNamingConvention.validate_model_name(model_name)\n",
    "    \n",
    "    print(f\"📋 {model_name}\")\n",
    "    print(f\"   業務單位: {metadata['business_unit']}\")\n",
    "    print(f\"   模型類型: {metadata['model_type']}\")\n",
    "    print(f\"   版本: {metadata['version']}\")\n",
    "    print(f\"   環境: {metadata['environment']}\")\n",
    "    print(f\"   有效性: {'✅' if valid else '❌'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config.pbtxt 配置深度解析\n",
    "\n",
    "### 2.1 基礎配置模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonConfigGenerator:\n",
    "    \"\"\"\n",
    "    Triton Model Configuration 生成器\n",
    "    支援多種 Backend 和複雜配置場景\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_pytorch_config(\n",
    "        model_name: str,\n",
    "        max_batch_size: int = 8,\n",
    "        input_specs: List[Dict] = None,\n",
    "        output_specs: List[Dict] = None,\n",
    "        instance_group: Dict = None,\n",
    "        dynamic_batching: Dict = None,\n",
    "        optimization: Dict = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        生成 PyTorch Backend 的配置文件\n",
    "        \"\"\"\n",
    "        \n",
    "        # 默認輸入輸出規格 (BERT-like model)\n",
    "        if input_specs is None:\n",
    "            input_specs = [\n",
    "                {\n",
    "                    \"name\": \"input_ids\",\n",
    "                    \"data_type\": \"TYPE_INT64\",\n",
    "                    \"dims\": [-1]  # 可變長度序列\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"attention_mask\",\n",
    "                    \"data_type\": \"TYPE_INT64\",\n",
    "                    \"dims\": [-1]\n",
    "                }\n",
    "            ]\n",
    "        \n",
    "        if output_specs is None:\n",
    "            output_specs = [\n",
    "                {\n",
    "                    \"name\": \"logits\",\n",
    "                    \"data_type\": \"TYPE_FP32\",\n",
    "                    \"dims\": [2]  # 二分類輸出\n",
    "                }\n",
    "            ]\n",
    "        \n",
    "        # 默認實例組配置\n",
    "        if instance_group is None:\n",
    "            instance_group = {\n",
    "                \"count\": 1,\n",
    "                \"kind\": \"KIND_GPU\",\n",
    "                \"gpus\": [0]\n",
    "            }\n",
    "        \n",
    "        # 默認動態批處理配置\n",
    "        if dynamic_batching is None:\n",
    "            dynamic_batching = {\n",
    "                \"enabled\": True,\n",
    "                \"max_queue_delay_microseconds\": 100,\n",
    "                \"preferred_batch_size\": [4, 8]\n",
    "            }\n",
    "        \n",
    "        # 生成配置內容\n",
    "        config_lines = [\n",
    "            f'name: \"{model_name}\"',\n",
    "            'backend: \"pytorch\"',\n",
    "            f'max_batch_size: {max_batch_size}',\n",
    "            ''\n",
    "        ]\n",
    "        \n",
    "        # 輸入配置\n",
    "        for input_spec in input_specs:\n",
    "            config_lines.extend([\n",
    "                'input [',\n",
    "                '  {',\n",
    "                f'    name: \"{input_spec[\"name\"]}\"',\n",
    "                f'    data_type: {input_spec[\"data_type\"]}',\n",
    "                f'    dims: {input_spec[\"dims\"]}',\n",
    "                '  }',\n",
    "                ']',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # 輸出配置\n",
    "        for output_spec in output_specs:\n",
    "            config_lines.extend([\n",
    "                'output [',\n",
    "                '  {',\n",
    "                f'    name: \"{output_spec[\"name\"]}\"',\n",
    "                f'    data_type: {output_spec[\"data_type\"]}',\n",
    "                f'    dims: {output_spec[\"dims\"]}',\n",
    "                '  }',\n",
    "                ']',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # 實例組配置\n",
    "        config_lines.extend([\n",
    "            'instance_group [',\n",
    "            '  {',\n",
    "            f'    count: {instance_group[\"count\"]}',\n",
    "            f'    kind: {instance_group[\"kind\"]}',\n",
    "        ])\n",
    "        \n",
    "        if \"gpus\" in instance_group:\n",
    "            gpu_list = \", \".join(map(str, instance_group[\"gpus\"]))\n",
    "            config_lines.append(f'    gpus: [ {gpu_list} ]')\n",
    "        \n",
    "        config_lines.extend([\n",
    "            '  }',\n",
    "            ']',\n",
    "            ''\n",
    "        ])\n",
    "        \n",
    "        # 動態批處理配置\n",
    "        if dynamic_batching[\"enabled\"]:\n",
    "            config_lines.extend([\n",
    "                'dynamic_batching {',\n",
    "                f'  max_queue_delay_microseconds: {dynamic_batching[\"max_queue_delay_microseconds\"]}',\n",
    "            ])\n",
    "            \n",
    "            if \"preferred_batch_size\" in dynamic_batching:\n",
    "                batch_sizes = \", \".join(map(str, dynamic_batching[\"preferred_batch_size\"]))\n",
    "                config_lines.append(f'  preferred_batch_size: [ {batch_sizes} ]')\n",
    "            \n",
    "            config_lines.extend([\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # 優化配置 (可選)\n",
    "        if optimization:\n",
    "            config_lines.extend([\n",
    "                'optimization {',\n",
    "                f'  execution_accelerators {{',\n",
    "                f'    gpu_execution_accelerator : [ {{',\n",
    "                f'      name : \"tensorrt\"',\n",
    "                f'      parameters {{ key: \"precision_mode\" value: \"FP16\" }}',\n",
    "                f'    }} ]',\n",
    "                f'  }}',\n",
    "                '}'\n",
    "            ])\n",
    "        \n",
    "        return '\\n'.join(config_lines)\n",
    "\n",
    "# 生成企業級模型配置示例\n",
    "print(\"⚙️  生成企業級 Triton 模型配置...\")\n",
    "print()\n",
    "\n",
    "# Netflix 推薦系統模型配置\n",
    "netflix_config = TritonConfigGenerator.generate_pytorch_config(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    max_batch_size=16,\n",
    "    input_specs=[\n",
    "        {\"name\": \"user_features\", \"data_type\": \"TYPE_FP32\", \"dims\": [128]},\n",
    "        {\"name\": \"item_features\", \"data_type\": \"TYPE_FP32\", \"dims\": [256]}\n",
    "    ],\n",
    "    output_specs=[\n",
    "        {\"name\": \"recommendation_scores\", \"data_type\": \"TYPE_FP32\", \"dims\": [100]}\n",
    "    ],\n",
    "    instance_group={\n",
    "        \"count\": 2,  # 雙實例提高吞吐量\n",
    "        \"kind\": \"KIND_GPU\",\n",
    "        \"gpus\": [0, 1]\n",
    "    },\n",
    "    dynamic_batching={\n",
    "        \"enabled\": True,\n",
    "        \"max_queue_delay_microseconds\": 50,  # 低延遲要求\n",
    "        \"preferred_batch_size\": [4, 8, 16]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"📄 Netflix 推薦系統模型配置:\")\n",
    "print(\"```\")\n",
    "print(netflix_config)\n",
    "print(\"```\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 高級配置場景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PayPal 風控模型配置 (高安全性需求)\n",
    "paypal_config = TritonConfigGenerator.generate_pytorch_config(\n",
    "    model_name=\"paypal_risk_v1_prod\",\n",
    "    max_batch_size=32,  # 高吞吐量批處理\n",
    "    input_specs=[\n",
    "        {\"name\": \"transaction_features\", \"data_type\": \"TYPE_FP32\", \"dims\": [50]},\n",
    "        {\"name\": \"user_profile\", \"data_type\": \"TYPE_FP32\", \"dims\": [30]},\n",
    "        {\"name\": \"merchant_info\", \"data_type\": \"TYPE_FP32\", \"dims\": [20]}\n",
    "    ],\n",
    "    output_specs=[\n",
    "        {\"name\": \"risk_score\", \"data_type\": \"TYPE_FP32\", \"dims\": [1]},\n",
    "        {\"name\": \"fraud_probability\", \"data_type\": \"TYPE_FP32\", \"dims\": [1]}\n",
    "    ],\n",
    "    instance_group={\n",
    "        \"count\": 4,  # 高可用性多實例\n",
    "        \"kind\": \"KIND_GPU\",\n",
    "        \"gpus\": [0, 1, 2, 3]\n",
    "    },\n",
    "    dynamic_batching={\n",
    "        \"enabled\": True,\n",
    "        \"max_queue_delay_microseconds\": 10,  # 極低延遲 (10微秒)\n",
    "        \"preferred_batch_size\": [8, 16, 32]\n",
    "    },\n",
    "    optimization={\n",
    "        \"tensorrt_fp16\": True  # 啟用 TensorRT FP16 優化\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"💳 PayPal 風控模型配置 (高安全性):\")\n",
    "print(\"```\")\n",
    "print(paypal_config)\n",
    "print(\"```\")\n",
    "print()\n",
    "\n",
    "# 保存配置文件到模型倉庫\n",
    "def save_model_config(model_name: str, config_content: str):\n",
    "    \"\"\"\n",
    "    保存模型配置到對應的模型目錄\n",
    "    \"\"\"\n",
    "    model_path = MODEL_REPOSITORY_ROOT / model_name\n",
    "    model_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    config_path = model_path / \"config.pbtxt\"\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    logger.info(f\"✅ 配置文件已保存: {config_path}\")\n",
    "    return config_path\n",
    "\n",
    "# 保存企業級模型配置\n",
    "netflix_config_path = save_model_config(\"netflix_recommendation_v2_prod\", netflix_config)\n",
    "paypal_config_path = save_model_config(\"paypal_risk_v1_prod\", paypal_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 實際模型部署實踐\n",
    "\n",
    "### 3.1 從 HuggingFace 下載和準備模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import numpy as np\n",
    "\n",
    "class TritonModelDeployer:\n",
    "    \"\"\"\n",
    "    Triton 模型部署器 - 處理從 HuggingFace 到 Triton 的完整部署流程\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_repository_root: Path):\n",
    "        self.model_repository_root = model_repository_root\n",
    "    \n",
    "    def deploy_huggingface_model(\n",
    "        self, \n",
    "        model_name_or_path: str,\n",
    "        triton_model_name: str,\n",
    "        model_version: int = 1,\n",
    "        task_type: str = \"classification\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        部署 HuggingFace 模型到 Triton Model Repository\n",
    "        \n",
    "        Args:\n",
    "            model_name_or_path: HuggingFace 模型名稱或路徑\n",
    "            triton_model_name: Triton 中的模型名稱\n",
    "            model_version: 模型版本號\n",
    "            task_type: 任務類型 (classification, regression, generation)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"🚀 開始部署模型: {model_name_or_path} -> {triton_model_name}\")\n",
    "        \n",
    "        # 1. 創建模型目錄結構\n",
    "        model_path = self.model_repository_root / triton_model_name\n",
    "        version_path = model_path / str(model_version)\n",
    "        version_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # 2. 下載 HuggingFace 模型\n",
    "            print(f\"📥 下載模型: {model_name_or_path}\")\n",
    "            \n",
    "            # 模擬下載過程 (在實際環境中會真正下載)\n",
    "            print(\"   ├── 下載 tokenizer...\")\n",
    "            # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "            \n",
    "            print(\"   ├── 下載模型配置...\")\n",
    "            # config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "            \n",
    "            print(\"   └── 下載模型權重...\")\n",
    "            # model = AutoModel.from_pretrained(model_name_or_path)\n",
    "            \n",
    "            # 3. 創建 Triton 兼容的模型包裝器\n",
    "            wrapper_code = self._generate_model_wrapper(task_type, triton_model_name)\n",
    "            wrapper_path = version_path / \"model.py\"\n",
    "            \n",
    "            with open(wrapper_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(wrapper_code)\n",
    "            \n",
    "            print(f\"📝 生成模型包裝器: {wrapper_path}\")\n",
    "            \n",
    "            # 4. 創建模型配置文件\n",
    "            config_content = self._generate_model_config(triton_model_name, task_type)\n",
    "            config_path = model_path / \"config.pbtxt\"\n",
    "            \n",
    "            with open(config_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(config_content)\n",
    "            \n",
    "            print(f\"⚙️  生成配置文件: {config_path}\")\n",
    "            \n",
    "            # 5. 創建模型元數據文件\n",
    "            metadata = {\n",
    "                \"model_name\": triton_model_name,\n",
    "                \"version\": model_version,\n",
    "                \"source_model\": model_name_or_path,\n",
    "                \"task_type\": task_type,\n",
    "                \"deployment_date\": \"2024-10-09\",\n",
    "                \"backend\": \"pytorch\"\n",
    "            }\n",
    "            \n",
    "            metadata_path = model_path / \"metadata.json\"\n",
    "            with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"📋 生成元數據文件: {metadata_path}\")\n",
    "            \n",
    "            print(f\"✅ 模型部署完成: {triton_model_name}\")\n",
    "            \n",
    "            return {\n",
    "                \"model_path\": str(model_path),\n",
    "                \"version_path\": str(version_path),\n",
    "                \"config_path\": str(config_path),\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 模型部署失敗: {str(e)}\")\n",
    "            # 清理失敗的部署\n",
    "            if model_path.exists():\n",
    "                shutil.rmtree(model_path)\n",
    "            raise\n",
    "    \n",
    "    def _generate_model_wrapper(self, task_type: str, model_name: str) -> str:\n",
    "        \"\"\"\n",
    "        生成 Triton PyTorch Backend 模型包裝器代碼\n",
    "        \"\"\"\n",
    "        \n",
    "        if task_type == \"classification\":\n",
    "            return f'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"\n",
    "    Triton 分類模型包裝器 - {model_name}\n",
    "    \n",
    "    支援企業級特性:\n",
    "    - 批量推理優化\n",
    "    - 錯誤處理和日誌記錄\n",
    "    - 性能監控集成\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        初始化模型 - 在模型載入時執行一次\n",
    "        \"\"\"\n",
    "        # 獲取模型配置\n",
    "        self.model_config = model_config = json.loads(args[\\'model_config\\'])\n",
    "        \n",
    "        # 設置輸出配置\n",
    "        output0_config = pb_utils.get_output_config_by_name(\n",
    "            model_config, \"logits\"\n",
    "        )\n",
    "        self.output0_dtype = pb_utils.triton_string_to_numpy(\n",
    "            output0_config[\\'data_type\\'] \n",
    "        )\n",
    "        \n",
    "        # 載入模型 (在實際部署中載入真實模型)\n",
    "        print(f\"🔄 載入模型: {model_name}\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 模擬模型載入\n",
    "        # self.model = AutoModel.from_pretrained(model_path)\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        # self.model.to(self.device)\n",
    "        # self.model.eval()\n",
    "        \n",
    "        print(f\"✅ 模型載入完成，設備: {{self.device}}\")\n",
    "    \n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        執行推理 - 處理批量請求\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            # 獲取輸入數據\n",
    "            input_ids = pb_utils.get_input_tensor_by_name(\n",
    "                request, \"input_ids\"\n",
    "            ).as_numpy()\n",
    "            \n",
    "            attention_mask = pb_utils.get_input_tensor_by_name(\n",
    "                request, \"attention_mask\"\n",
    "            ).as_numpy()\n",
    "            \n",
    "            # 模擬推理過程\n",
    "            batch_size = input_ids.shape[0]\n",
    "            \n",
    "            # 在實際部署中執行真實推理\n",
    "            # with torch.no_grad():\n",
    "            #     outputs = self.model(\n",
    "            #         input_ids=torch.tensor(input_ids).to(self.device),\n",
    "            #         attention_mask=torch.tensor(attention_mask).to(self.device)\n",
    "            #     )\n",
    "            #     logits = outputs.logits\n",
    "            \n",
    "            # 模擬輸出 (2分類)\n",
    "            logits = np.random.rand(batch_size, 2).astype(self.output0_dtype)\n",
    "            \n",
    "            # 創建輸出張量\n",
    "            output_tensor = pb_utils.Tensor(\"logits\", logits)\n",
    "            \n",
    "            # 創建響應\n",
    "            inference_response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[output_tensor]\n",
    "            )\n",
    "            responses.append(inference_response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        清理資源 - 在模型卸載時執行\n",
    "        \"\"\"\n",
    "        print(f\"🧹 清理模型資源: {model_name}\")\n",
    "'''\n",
    "        \n",
    "        elif task_type == \"generation\":\n",
    "            # 生成式模型包裝器 (簡化版)\n",
    "            return \"# Generation model wrapper - 實現類似於分類模型，但處理文本生成任務\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"不支持的任務類型: {task_type}\")\n",
    "    \n",
    "    def _generate_model_config(self, model_name: str, task_type: str) -> str:\n",
    "        \"\"\"\n",
    "        根據任務類型生成相應的模型配置\n",
    "        \"\"\"\n",
    "        \n",
    "        if task_type == \"classification\":\n",
    "            return TritonConfigGenerator.generate_pytorch_config(\n",
    "                model_name=model_name,\n",
    "                max_batch_size=8,\n",
    "                input_specs=[\n",
    "                    {\"name\": \"input_ids\", \"data_type\": \"TYPE_INT64\", \"dims\": [-1]},\n",
    "                    {\"name\": \"attention_mask\", \"data_type\": \"TYPE_INT64\", \"dims\": [-1]}\n",
    "                ],\n",
    "                output_specs=[\n",
    "                    {\"name\": \"logits\", \"data_type\": \"TYPE_FP32\", \"dims\": [2]}\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的任務類型: {task_type}\")\n",
    "\n",
    "# 初始化部署器\n",
    "deployer = TritonModelDeployer(MODEL_REPOSITORY_ROOT)\n",
    "\n",
    "print(\"🏭 開始企業級模型部署演示...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 部署企業級模型示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部署 Netflix 用戶情感分析模型\n",
    "netflix_sentiment_deployment = deployer.deploy_huggingface_model(\n",
    "    model_name_or_path=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    triton_model_name=\"netflix_sentiment_v1_prod\",\n",
    "    model_version=1,\n",
    "    task_type=\"classification\"\n",
    ")\n",
    "\n",
    "print(f\"Netflix 情感分析模型部署結果:\")\n",
    "print(f\"  📂 模型路徑: {netflix_sentiment_deployment['model_path']}\")\n",
    "print(f\"  📋 元數據: {netflix_sentiment_deployment['metadata']}\")\n",
    "print()\n",
    "\n",
    "# 部署 PayPal 欺詐檢測模型\n",
    "paypal_fraud_deployment = deployer.deploy_huggingface_model(\n",
    "    model_name_or_path=\"ProsusAI/finbert\",\n",
    "    triton_model_name=\"paypal_fraud_detection_v2_prod\",\n",
    "    model_version=2,\n",
    "    task_type=\"classification\"\n",
    ")\n",
    "\n",
    "print(f\"PayPal 欺詐檢測模型部署結果:\")\n",
    "print(f\"  📂 模型路徑: {paypal_fraud_deployment['model_path']}\")\n",
    "print(f\"  📋 元數據: {paypal_fraud_deployment['metadata']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型版本管理和生命週期\n",
    "\n",
    "### 4.1 企業級版本控制策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVersionManager:\n",
    "    \"\"\"\n",
    "    企業級模型版本管理器\n",
    "    \n",
    "    支援功能:\n",
    "    - 語義化版本控制 (Semantic Versioning)\n",
    "    - A/B 測試版本管理\n",
    "    - 回滾和金絲雀部署\n",
    "    - 版本性能追蹤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_repository_root: Path):\n",
    "        self.model_repository_root = model_repository_root\n",
    "        self.version_registry = {}\n",
    "    \n",
    "    def create_model_version(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        version: str,\n",
    "        description: str = \"\",\n",
    "        performance_metrics: Dict = None,\n",
    "        deployment_strategy: str = \"blue_green\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        創建新的模型版本\n",
    "        \n",
    "        Args:\n",
    "            model_name: 模型名稱\n",
    "            version: 版本號 (語義化版本 如 1.2.3)\n",
    "            description: 版本描述\n",
    "            performance_metrics: 性能指標\n",
    "            deployment_strategy: 部署策略\n",
    "        \"\"\"\n",
    "        \n",
    "        if performance_metrics is None:\n",
    "            performance_metrics = {}\n",
    "        \n",
    "        # 驗證語義化版本格式\n",
    "        version_parts = version.split('.')\n",
    "        if len(version_parts) != 3:\n",
    "            raise ValueError(f\"版本號必須遵循語義化版本格式 (如 1.2.3): {version}\")\n",
    "        \n",
    "        try:\n",
    "            major, minor, patch = map(int, version_parts)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"版本號必須為數字: {version}\")\n",
    "        \n",
    "        # 創建版本目錄\n",
    "        model_path = self.model_repository_root / model_name\n",
    "        version_path = model_path / f\"{major}_{minor}_{patch}\"  # Triton 版本目錄命名\n",
    "        version_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 創建版本元數據\n",
    "        version_metadata = {\n",
    "            \"version\": version,\n",
    "            \"major\": major,\n",
    "            \"minor\": minor,\n",
    "            \"patch\": patch,\n",
    "            \"description\": description,\n",
    "            \"created_at\": \"2024-10-09T10:00:00Z\",\n",
    "            \"deployment_strategy\": deployment_strategy,\n",
    "            \"performance_metrics\": performance_metrics,\n",
    "            \"status\": \"deployed\",\n",
    "            \"triton_version_dir\": f\"{major}_{minor}_{patch}\"\n",
    "        }\n",
    "        \n",
    "        # 保存版本元數據\n",
    "        metadata_path = version_path / \"version_metadata.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(version_metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # 更新版本註冊表\n",
    "        if model_name not in self.version_registry:\n",
    "            self.version_registry[model_name] = []\n",
    "        \n",
    "        self.version_registry[model_name].append(version_metadata)\n",
    "        \n",
    "        print(f\"✅ 創建模型版本: {model_name} v{version}\")\n",
    "        print(f\"   📂 版本路徑: {version_path}\")\n",
    "        print(f\"   📋 部署策略: {deployment_strategy}\")\n",
    "        \n",
    "        return version_metadata\n",
    "    \n",
    "    def list_model_versions(self, model_name: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        列出模型的所有版本\n",
    "        \"\"\"\n",
    "        return self.version_registry.get(model_name, [])\n",
    "    \n",
    "    def get_latest_version(self, model_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        獲取模型的最新版本\n",
    "        \"\"\"\n",
    "        versions = self.list_model_versions(model_name)\n",
    "        if not versions:\n",
    "            raise ValueError(f\"模型 {model_name} 沒有任何版本\")\n",
    "        \n",
    "        # 按語義化版本排序\n",
    "        sorted_versions = sorted(\n",
    "            versions, \n",
    "            key=lambda v: (v[\"major\"], v[\"minor\"], v[\"patch\"]),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return sorted_versions[0]\n",
    "    \n",
    "    def setup_ab_testing(self, model_name: str, version_a: str, version_b: str, traffic_split: float = 0.5):\n",
    "        \"\"\"\n",
    "        設置 A/B 測試版本分流\n",
    "        \n",
    "        Args:\n",
    "            model_name: 模型名稱\n",
    "            version_a: A 版本\n",
    "            version_b: B 版本\n",
    "            traffic_split: 流量分配比例 (0.5 = 50/50)\n",
    "        \"\"\"\n",
    "        \n",
    "        ab_config = {\n",
    "            \"model_name\": model_name,\n",
    "            \"version_a\": version_a,\n",
    "            \"version_b\": version_b,\n",
    "            \"traffic_split\": traffic_split,\n",
    "            \"start_time\": \"2024-10-09T10:00:00Z\",\n",
    "            \"status\": \"active\",\n",
    "            \"metrics_collection\": True\n",
    "        }\n",
    "        \n",
    "        # 保存 A/B 測試配置\n",
    "        model_path = self.model_repository_root / model_name\n",
    "        ab_config_path = model_path / \"ab_testing_config.json\"\n",
    "        \n",
    "        with open(ab_config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(ab_config, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"🧪 A/B 測試設置完成: {model_name}\")\n",
    "        print(f\"   🅰️  版本 A: {version_a} ({traffic_split * 100:.1f}% 流量)\")\n",
    "        print(f\"   🅱️  版本 B: {version_b} ({(1-traffic_split) * 100:.1f}% 流量)\")\n",
    "        \n",
    "        return ab_config\n",
    "\n",
    "# 初始化版本管理器\n",
    "version_manager = ModelVersionManager(MODEL_REPOSITORY_ROOT)\n",
    "\n",
    "print(\"📊 企業級模型版本管理演示...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 實際版本管理場景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netflix 推薦系統版本演進\n",
    "print(\"🎬 Netflix 推薦系統版本管理場景:\")\n",
    "print()\n",
    "\n",
    "# 版本 1.0.0 - 基礎推薦算法\n",
    "v1_metadata = version_manager.create_model_version(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    version=\"1.0.0\",\n",
    "    description=\"基礎協同過濾推薦模型\",\n",
    "    performance_metrics={\n",
    "        \"precision_at_k\": 0.85,\n",
    "        \"recall_at_k\": 0.72,\n",
    "        \"latency_p95_ms\": 45,\n",
    "        \"throughput_qps\": 1200\n",
    "    },\n",
    "    deployment_strategy=\"blue_green\"\n",
    ")\n",
    "\n",
    "# 版本 1.1.0 - 加入深度學習特徵\n",
    "v1_1_metadata = version_manager.create_model_version(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    version=\"1.1.0\",\n",
    "    description=\"加入用戶行為深度學習特徵\",\n",
    "    performance_metrics={\n",
    "        \"precision_at_k\": 0.89,\n",
    "        \"recall_at_k\": 0.76,\n",
    "        \"latency_p95_ms\": 52,\n",
    "        \"throughput_qps\": 1100\n",
    "    },\n",
    "    deployment_strategy=\"canary\"\n",
    ")\n",
    "\n",
    "# 版本 2.0.0 - 全新 Transformer 架構\n",
    "v2_metadata = version_manager.create_model_version(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    version=\"2.0.0\",\n",
    "    description=\"Transformer-based 序列推薦模型\",\n",
    "    performance_metrics={\n",
    "        \"precision_at_k\": 0.93,\n",
    "        \"recall_at_k\": 0.81,\n",
    "        \"latency_p95_ms\": 38,\n",
    "        \"throughput_qps\": 1400\n",
    "    },\n",
    "    deployment_strategy=\"blue_green\"\n",
    ")\n",
    "\n",
    "# 查看版本歷史\n",
    "versions = version_manager.list_model_versions(\"netflix_recommendation_v2_prod\")\n",
    "print(f\"📈 Netflix 推薦系統版本歷史 ({len(versions)} 個版本):\")\n",
    "\n",
    "for version in versions:\n",
    "    print(f\"\")\n",
    "    print(f\"   🏷️  版本: {version['version']}\")\n",
    "    print(f\"   📝 描述: {version['description']}\")\n",
    "    print(f\"   🎯 Precision@K: {version['performance_metrics']['precision_at_k']}\")\n",
    "    print(f\"   ⚡ 延遲 P95: {version['performance_metrics']['latency_p95_ms']}ms\")\n",
    "    print(f\"   🚀 QPS: {version['performance_metrics']['throughput_qps']}\")\n",
    "    print(f\"   📦 部署策略: {version['deployment_strategy']}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 獲取最新版本\n",
    "latest_version = version_manager.get_latest_version(\"netflix_recommendation_v2_prod\")\n",
    "print(f\"🔝 最新版本: {latest_version['version']}\")\n",
    "print(f\"   性能提升: Precision@K {latest_version['performance_metrics']['precision_at_k']} (+{latest_version['performance_metrics']['precision_at_k'] - 0.85:.2f})\")\n",
    "print()\n",
    "\n",
    "# 設置 A/B 測試 (v1.1.0 vs v2.0.0)\n",
    "ab_config = version_manager.setup_ab_testing(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    version_a=\"1.1.0\",\n",
    "    version_b=\"2.0.0\",\n",
    "    traffic_split=0.3  # 30% 流量給版本 A，70% 給版本 B\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 動態模型管理\n",
    "\n",
    "### 5.1 模型載入和卸載機制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "class TritonModelManager:\n",
    "    \"\"\"\n",
    "    Triton 動態模型管理器\n",
    "    \n",
    "    支援功能:\n",
    "    - 動態載入/卸載模型\n",
    "    - 模型狀態監控\n",
    "    - 優雅的模型切換\n",
    "    - 資源使用最佳化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"http://localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.management_api_url = f\"{triton_url}/v2/repository\"\n",
    "    \n",
    "    def load_model(self, model_name: str, wait_for_ready: bool = True) -> bool:\n",
    "        \"\"\"\n",
    "        動態載入模型到 Triton Server\n",
    "        \n",
    "        Args:\n",
    "            model_name: 要載入的模型名稱\n",
    "            wait_for_ready: 是否等待模型完全載入\n",
    "        \n",
    "        Returns:\n",
    "            bool: 載入是否成功\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"🔄 載入模型: {model_name}\")\n",
    "            \n",
    "            # 模擬 Triton Model Management API 調用\n",
    "            # 實際代碼: \n",
    "            # response = requests.post(f\"{self.management_api_url}/models/{model_name}/load\")\n",
    "            \n",
    "            # 模擬載入過程\n",
    "            print(f\"   ├── 驗證模型配置...\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            print(f\"   ├── 分配 GPU 資源...\")\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            print(f\"   ├── 載入模型權重...\")\n",
    "            time.sleep(1.0)\n",
    "            \n",
    "            print(f\"   └── 初始化推理引擎...\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if wait_for_ready:\n",
    "                print(f\"   ⏳ 等待模型就緒...\")\n",
    "                # 模擬等待模型就緒\n",
    "                for i in range(3):\n",
    "                    time.sleep(0.5)\n",
    "                    ready = self.is_model_ready(model_name)\n",
    "                    if ready:\n",
    "                        break\n",
    "            \n",
    "            print(f\"✅ 模型載入成功: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 模型載入失敗: {model_name} - {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def unload_model(self, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        動態卸載模型\n",
    "        \n",
    "        Args:\n",
    "            model_name: 要卸載的模型名稱\n",
    "        \n",
    "        Returns:\n",
    "            bool: 卸載是否成功\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"🔄 卸載模型: {model_name}\")\n",
    "            \n",
    "            # 模擬 Triton Model Management API 調用\n",
    "            # response = requests.post(f\"{self.management_api_url}/models/{model_name}/unload\")\n",
    "            \n",
    "            print(f\"   ├── 停止推理請求...\")\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            print(f\"   ├── 釋放 GPU 記憶體...\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            print(f\"   └── 清理資源...\")\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "            print(f\"✅ 模型卸載成功: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 模型卸載失敗: {model_name} - {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def is_model_ready(self, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        檢查模型是否已準備好接受推理請求\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 模擬模型狀態檢查\n",
    "            # response = requests.get(f\"{self.triton_url}/v2/models/{model_name}/ready\")\n",
    "            # return response.status_code == 200\n",
    "            \n",
    "            # 模擬檢查結果\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    def get_model_status(self, model_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        獲取模型的詳細狀態信息\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 模擬模型狀態信息\n",
    "            return {\n",
    "                \"name\": model_name,\n",
    "                \"state\": \"READY\",\n",
    "                \"reason\": \"\",\n",
    "                \"version\": \"1\",\n",
    "                \"backend\": \"pytorch\",\n",
    "                \"instances\": [\n",
    "                    {\n",
    "                        \"name\": f\"{model_name}_0\",\n",
    "                        \"state\": \"READY\",\n",
    "                        \"kind\": \"GPU\",\n",
    "                        \"gpu_id\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def graceful_model_switch(self, old_model: str, new_model: str) -> bool:\n",
    "        \"\"\"\n",
    "        優雅地切換模型版本\n",
    "        \n",
    "        流程:\n",
    "        1. 載入新模型\n",
    "        2. 等待新模型就緒\n",
    "        3. 執行健康檢查\n",
    "        4. 卸載舊模型\n",
    "        \"\"\"\n",
    "        print(f\"🔄 開始優雅模型切換: {old_model} → {new_model}\")\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # 1. 載入新模型\n",
    "            print(f\"📥 第一步: 載入新模型 {new_model}\")\n",
    "            if not self.load_model(new_model, wait_for_ready=True):\n",
    "                print(f\"❌ 新模型載入失敗，中止切換\")\n",
    "                return False\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # 2. 執行健康檢查\n",
    "            print(f\"🏥 第二步: 執行新模型健康檢查\")\n",
    "            health_check_passed = self._perform_health_check(new_model)\n",
    "            \n",
    "            if not health_check_passed:\n",
    "                print(f\"❌ 健康檢查失敗，回滾操作\")\n",
    "                self.unload_model(new_model)\n",
    "                return False\n",
    "            \n",
    "            print(f\"✅ 健康檢查通過\")\n",
    "            print()\n",
    "            \n",
    "            # 3. 卸載舊模型\n",
    "            print(f\"📤 第三步: 卸載舊模型 {old_model}\")\n",
    "            if not self.unload_model(old_model):\n",
    "                print(f\"⚠️  舊模型卸載失敗，但新模型已成功部署\")\n",
    "            \n",
    "            print()\n",
    "            print(f\"🎉 模型切換完成: {old_model} → {new_model}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 模型切換失敗: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _perform_health_check(self, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        執行模型健康檢查\n",
    "        \"\"\"\n",
    "        print(f\"   ├── 檢查模型狀態...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        print(f\"   ├── 執行示例推理...\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        print(f\"   ├── 驗證輸出格式...\")\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        print(f\"   └── 檢查性能指標...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        return True\n",
    "\n",
    "# 初始化模型管理器\n",
    "model_manager = TritonModelManager()\n",
    "\n",
    "print(\"🎛️  Triton 動態模型管理演示...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 實際模型切換場景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 場景：Netflix 推薦系統模型升級\n",
    "print(\"🎬 Netflix 推薦系統模型升級場景:\")\n",
    "print(\"📋 需求：從 v1.1.0 升級到 v2.0.0，零停機時間\")\n",
    "print()\n",
    "\n",
    "# 模擬當前運行的模型\n",
    "current_model = \"netflix_recommendation_v1_1_prod\"\n",
    "new_model = \"netflix_recommendation_v2_0_prod\"\n",
    "\n",
    "# 執行優雅切換\n",
    "switch_success = model_manager.graceful_model_switch(current_model, new_model)\n",
    "\n",
    "if switch_success:\n",
    "    print(\"📊 切換後性能對比:\")\n",
    "    print(f\"   🎯 Precision@K: 0.89 → 0.93 (+4.5%)\")\n",
    "    print(f\"   ⚡ 延遲: 52ms → 38ms (-26.9%)\")\n",
    "    print(f\"   🚀 吞吐量: 1100 QPS → 1400 QPS (+27.3%)\")\n",
    "else:\n",
    "    print(\"❌ 模型切換失敗，保持原有模型運行\")\n",
    "\n",
    "print()\n",
    "print(\"─\" * 60)\n",
    "print()\n",
    "\n",
    "# 場景：PayPal 風控模型緊急回滾\n",
    "print(\"💳 PayPal 風控模型緊急回滾場景:\")\n",
    "print(\"📋 需求：檢測到新版本誤報率過高，緊急回滾到穩定版本\")\n",
    "print()\n",
    "\n",
    "problematic_model = \"paypal_fraud_detection_v2_1_prod\"\n",
    "stable_model = \"paypal_fraud_detection_v2_0_prod\"\n",
    "\n",
    "# 緊急回滾\n",
    "print(f\"🚨 執行緊急回滾操作...\")\n",
    "rollback_success = model_manager.graceful_model_switch(problematic_model, stable_model)\n",
    "\n",
    "if rollback_success:\n",
    "    print(\"📈 回滾後指標恢復:\")\n",
    "    print(f\"   ✅ 誤報率: 8.5% → 2.1% (恢復正常)\")\n",
    "    print(f\"   ✅ 召回率: 89.2% → 94.7% (恢復正常)\")\n",
    "    print(f\"   ✅ 系統穩定性: 恢復\")\n",
    "else:\n",
    "    print(\"❌ 緊急回滾失敗，需要人工介入\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 檢查最終模型狀態\n",
    "print(\"📋 當前載入的模型狀態:\")\n",
    "models_to_check = [new_model, stable_model]\n",
    "\n",
    "for model_name in models_to_check:\n",
    "    status = model_manager.get_model_status(model_name)\n",
    "    print(f\"   🔍 {model_name}:\")\n",
    "    print(f\"      狀態: {status.get('state', 'UNKNOWN')}\")\n",
    "    print(f\"      後端: {status.get('backend', 'UNKNOWN')}\")\n",
    "    print(f\"      版本: {status.get('version', 'UNKNOWN')}\")\n",
    "    print(f\"      實例數: {len(status.get('instances', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 企業級最佳實踐總結\n",
    "\n",
    "### 6.1 Model Repository 設計原則"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseModelRepositoryBestPractices:\n",
    "    \"\"\"\n",
    "    企業級 Model Repository 最佳實踐指南\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_best_practices():\n",
    "        print(\"🏆 企業級 Model Repository 最佳實踐\")\n",
    "        print(\"═\" * 60)\n",
    "        print()\n",
    "        \n",
    "        practices = {\n",
    "            \"🏗️  架構設計原則\": [\n",
    "                \"採用語義化版本控制 (Semantic Versioning)\",\n",
    "                \"實施清晰的模型命名規範\",\n",
    "                \"分離模型配置和業務邏輯\",\n",
    "                \"支援多環境部署 (dev/staging/prod)\",\n",
    "                \"實現模型元數據管理\"\n",
    "            ],\n",
    "            \"⚡ 性能最佳化\": [\n",
    "                \"配置動態批處理 (Dynamic Batching)\",\n",
    "                \"使用 GPU 實例組提高吞吐量\",\n",
    "                \"啟用 TensorRT 或其他加速器\",\n",
    "                \"優化模型輸入輸出格式\",\n",
    "                \"實施模型預熱 (Model Warmup)\"\n",
    "            ],\n",
    "            \"🔒 安全性考量\": [\n",
    "                \"實施模型訪問控制\",\n",
    "                \"加密敏感模型文件\",\n",
    "                \"審計模型部署和訪問日誌\",\n",
    "                \"實施網路安全措施\",\n",
    "                \"定期安全漏洞掃描\"\n",
    "            ],\n",
    "            \"📊 監控與可觀測性\": [\n",
    "                \"實時性能指標監控\",\n",
    "                \"模型準確性追蹤\",\n",
    "                \"資源使用監控\",\n",
    "                \"異常檢測和告警\",\n",
    "                \"分散式追蹤整合\"\n",
    "            ],\n",
    "            \"🔄 DevOps 整合\": [\n",
    "                \"自動化模型部署流水線\",\n",
    "                \"A/B 測試自動化\",\n",
    "                \"金絲雀部署支援\",\n",
    "                \"自動回滾機制\",\n",
    "                \"容器化和 Kubernetes 整合\"\n",
    "            ],\n",
    "            \"📈 擴展性設計\": [\n",
    "                \"支援水平擴展\",\n",
    "                \"負載均衡配置\",\n",
    "                \"多區域部署\",\n",
    "                \"彈性資源調配\",\n",
    "                \"高可用性架構\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for category, items in practices.items():\n",
    "            print(f\"{category}:\")\n",
    "            for item in items:\n",
    "                print(f\"   ✅ {item}\")\n",
    "            print()\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_common_pitfalls():\n",
    "        print(\"⚠️  常見陷阱與解決方案\")\n",
    "        print(\"═\" * 60)\n",
    "        print()\n",
    "        \n",
    "        pitfalls = {\n",
    "            \"🐛 配置錯誤\": {\n",
    "                \"問題\": \"模型輸入輸出維度配置錯誤\",\n",
    "                \"解決方案\": \"使用自動化配置生成和驗證工具\",\n",
    "                \"預防措施\": \"實施配置模板和測試套件\"\n",
    "            },\n",
    "            \"💾 記憶體洩漏\": {\n",
    "                \"問題\": \"模型長時間運行後記憶體使用持續增長\",\n",
    "                \"解決方案\": \"實施記憶體監控和自動重啟機制\",\n",
    "                \"預防措施\": \"定期記憶體使用審計和優化\"\n",
    "            },\n",
    "            \"🔥 版本衝突\": {\n",
    "                \"問題\": \"多個模型版本之間的依賴衝突\",\n",
    "                \"解決方案\": \"使用容器隔離和版本鎖定\",\n",
    "                \"預防措施\": \"制定嚴格的版本管理策略\"\n",
    "            },\n",
    "            \"📉 性能退化\": {\n",
    "                \"問題\": \"模型推理性能隨時間逐漸下降\",\n",
    "                \"解決方案\": \"持續性能監控和自動調優\",\n",
    "                \"預防措施\": \"建立性能基準線和告警機制\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for category, details in pitfalls.items():\n",
    "            print(f\"{category}:\")\n",
    "            print(f\"   ❌ 問題: {details['問題']}\")\n",
    "            print(f\"   💡 解決方案: {details['解決方案']}\")\n",
    "            print(f\"   🛡️  預防措施: {details['預防措施']}\")\n",
    "            print()\n",
    "\n",
    "# 顯示最佳實踐指南\n",
    "EnterpriseModelRepositoryBestPractices.print_best_practices()\n",
    "EnterpriseModelRepositoryBestPractices.print_common_pitfalls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 本章總結\n",
    "\n",
    "### 核心學習成果\n",
    "\n",
    "通過本實驗室，您已經掌握了：\n",
    "\n",
    "1. **🏗️ Model Repository 架構設計**\n",
    "   - 標準目錄結構創建\n",
    "   - 企業級命名規範制定\n",
    "   - 多版本模型共存管理\n",
    "\n",
    "2. **⚙️ 配置文件深度定制**\n",
    "   - config.pbtxt 全面配置\n",
    "   - 動態批處理優化\n",
    "   - 多 GPU 實例組設置\n",
    "\n",
    "3. **🚀 模型部署自動化**\n",
    "   - HuggingFace 模型整合\n",
    "   - 模型包裝器生成\n",
    "   - 元數據管理體系\n",
    "\n",
    "4. **📊 版本控制與生命週期**\n",
    "   - 語義化版本管理\n",
    "   - A/B 測試配置\n",
    "   - 優雅模型切換\n",
    "\n",
    "### 企業級技能提升\n",
    "\n",
    "您現在具備了：\n",
    "- **Netflix 級別**的多模型管理能力\n",
    "- **PayPal 級別**的高可用性部署技能\n",
    "- **完整 MLOps 流程**的設計和實施能力\n",
    "\n",
    "### 下一步學習路徑\n",
    "\n",
    "在下一個實驗室 **Lab-2.1.3: PyTorch Backend Deployment** 中，我們將：\n",
    "- 深入 PyTorch Backend 的高級特性\n",
    "- 實現自定義推理邏輯\n",
    "- 優化模型推理性能\n",
    "- 整合企業級監控系統\n",
    "\n",
    "---\n",
    "\n",
    "**🏆 恭喜！您已經完成了 Triton Model Repository 的企業級設計與配置！**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}