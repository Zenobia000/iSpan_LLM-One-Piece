{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. 合併 LoRA 權重並儲存模型\n",
        "\n",
        "在本實驗的最後一個環節，我們將學習 `peft` 函式庫的一個非常實用的功能：將訓練好的 LoRA adapter 權重合併回原始的基礎模型中。\n",
        "\n",
        "這樣做的好處是，我們可以得到一個**單一的、標準的 Hugging Face 模型**，它已經包含了微調後的所有知識。這個合併後的模型可以像任何普通的 `transformers` 模型一樣被載入和使用，不再需要 `peft` 函式庫，這對於簡化部署流程非常方便。\n",
        "\n",
        "## 步驟 1: 載入基礎模型與 LoRA Adapter\n",
        "\n",
        "和上一個筆記本一樣，我們首先需要載入量化後的基礎模型，以及我們訓練好的 LoRA adapter。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/accelerate/utils/modeling.py:1582: UserWarning: Current model requires 32.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2277b0bc6cc340d09bdbabd565e41e3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型與 Adapter 載入完成。\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "# --- 重新載入基礎模型與 Tokenizer ---\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 載入 LoRA Adapter ---\n",
        "output_dir = \"./lora-llama2-7b-chat\"\n",
        "latest_checkpoint = max([os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")], key=os.path.getmtime)\n",
        "peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "print(\"模型與 Adapter 載入完成。\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 2: 合併權重\n",
        "\n",
        "`peft` 提供了一個非常簡單的方法來合併權重：`merge_and_unload()`。\n",
        "\n",
        "這個方法會執行以下操作：\n",
        "1.  將 LoRA adapter (矩陣 A 和 B) 的權重計算出來 (`B * A`)。\n",
        "2.  將計算出的權重加回到原始的 `target_modules` (例如 \"q_proj\", \"v_proj\") 上。\n",
        "3.  從模型中移除 LoRA adapter，並返回一個合併後的標準 `transformers` 模型。\n",
        "\n",
        "**注意**: 由於我們使用了量化模型，`merge_and_unload()` 目前可能無法直接在量化層上運作。在理想情況下（非量化模型），這個步驟會非常直接。對於量化模型，`peft` 函式庫正在持續改進支援。這裡我們展示標準流程。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "權重合併完成！\n",
            "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
          ]
        }
      ],
      "source": [
        "# 合併權重\n",
        "# 在非量化模型上，這會直接返回一個 AutoModelForCausalLM 物件\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "print(\"權重合併完成！\")\n",
        "# 合併後，模型類型變回原本的 LlamaForCausalLM\n",
        "print(type(merged_model))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 3: 儲存完整模型與 Tokenizer\n",
        "\n",
        "合併後的模型現在是一個完整的、獨立的模型。我們可以使用 `save_pretrained()` 方法將它和它的 Tokenizer 一起儲存到一個新的目錄中。\n",
        "\n",
        "這個儲存下來的模型未來可以直接使用 `AutoModelForCausalLM.from_pretrained()` 載入，不再需要 `peft` 函式庫。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-10-21 16:51:59,340] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, weight, bias=None):\n",
            "/home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "合併後的模型已儲存至: ./llama2-7b-chat-guanaco-merged\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# 定義儲存目錄\n",
        "save_directory = \"./llama2-7b-chat-guanaco-merged\"\n",
        "\n",
        "# 儲存模型\n",
        "merged_model.save_pretrained(save_directory)\n",
        "\n",
        "# 儲存 Tokenizer\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"合併後的模型已儲存至: {save_directory}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 實驗總結\n",
        "\n",
        "恭喜您完成了整個 PEFT 微調實驗！\n",
        "\n",
        "您已經學會了：\n",
        "- **環境設定**: 準備進行 PEFT 的開發環境。\n",
        "- **載入與量化**: 如何載入大型語言模型並使用 4-bit 量化來節省資源。\n",
        "- **應用 LoRA**: 如何使用 `peft` 函式庫設定並應用 LoRA。\n",
        "- **訓練**: 如何使用 `Trainer` 進行微調。\n",
        "- **推理**: 如何使用微調後的模型生成文本。\n",
        "- **合併與儲存**: 如何將 LoRA 權重合併回主模型，以便於部署。\n",
        "\n",
        "希望這個實驗能幫助您掌握參數高效微調的核心技術，並將其應用到您自己的專案中！\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM Engineering Course (Poetry)",
      "language": "python",
      "name": "llm-engineering-course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
