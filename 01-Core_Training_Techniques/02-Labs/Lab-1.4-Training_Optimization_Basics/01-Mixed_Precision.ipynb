{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.4: 混合精度訓練 (Mixed Precision Training)\n",
    "\n",
    "**學習目標**:\n",
    "- 理解 FP32, FP16, BF16 精度差異\n",
    "- 掌握 PyTorch AMP (Automatic Mixed Precision) 使用\n",
    "- 實現動態損失縮放防止梯度下溢\n",
    "- 對比不同精度的訓練效果與性能\n",
    "\n",
    "**預計時間**: 60-90分鐘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設置與依賴導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 理論背景\n",
    "\n",
    "### 2.1 浮點數精度格式\n",
    "\n",
    "| 格式 | 位元結構 | 範圍 | 精度 | 特性 |\n",
    "|------|---------|------|------|------|\n",
    "| **FP32** | 1+8+23 | ±3.4×10³⁸ | ~7位 | 標準精度 |\n",
    "| **FP16** | 1+5+10 | ±6.5×10⁴ | ~3位 | 高速推理 |\n",
    "| **BF16** | 1+8+7 | ±3.4×10³⁸ | ~2位 | 訓練友好 |\n",
    "\n",
    "### 2.2 混合精度訓練原理\n",
    "\n",
    "1. **前向傳播**: 使用 FP16 計算 (節省記憶體)\n",
    "2. **損失計算**: FP16 → FP32 損失\n",
    "3. **反向傳播**: FP16 梯度計算\n",
    "4. **損失縮放**: 防止 FP16 梯度下溢\n",
    "5. **參數更新**: FP32 主副本更新\n",
    "6. **參數複製**: FP32 → FP16 供下次迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 數據準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的文本數據集\n",
    "class SimpleTextDataset(Dataset):\n",
    "    \"\"\"簡單的文本生成數據集\"\"\"\n",
    "    def __init__(self, tokenizer, num_samples=1000, seq_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # 生成簡單的訓練文本\n",
    "        self.texts = [\n",
    "            f\"The quick brown fox jumps over the lazy dog. \" * 10\n",
    "            for _ in range(num_samples)\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encodings[\"input_ids\"].squeeze()\n",
    "        attention_mask = encodings[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids.clone()  # 語言模型任務\n",
    "        }\n",
    "\n",
    "# 初始化 tokenizer\n",
    "print(\"載入 GPT-2 Tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 創建數據集\n",
    "train_dataset = SimpleTextDataset(tokenizer, num_samples=500, seq_length=128)\n",
    "print(f\"訓練數據集大小: {len(train_dataset)}\")\n",
    "\n",
    "# 創建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "print(f\"批次數量: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 記憶體監控工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTracker:\n",
    "    \"\"\"GPU 記憶體追蹤工具\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    def get_memory_stats(self):\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\"allocated\": 0, \"reserved\": 0, \"peak\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9,  # GB\n",
    "            \"reserved\": torch.cuda.memory_reserved() / 1e9,    # GB\n",
    "            \"peak\": torch.cuda.max_memory_allocated() / 1e9   # GB\n",
    "        }\n",
    "    \n",
    "    def print_memory(self, prefix=\"\"):\n",
    "        stats = self.get_memory_stats()\n",
    "        print(f\"{prefix}記憶體 - 已分配: {stats['allocated']:.2f}GB, \"\n",
    "              f\"已保留: {stats['reserved']:.2f}GB, \"\n",
    "              f\"峰值: {stats['peak']:.2f}GB\")\n",
    "        return stats\n",
    "\n",
    "memory_tracker = MemoryTracker()\n",
    "memory_tracker.print_memory(\"初始\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 實驗 1: FP32 標準訓練 (基準測試)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, device, num_steps=100, use_amp=False, dtype=None):\n",
    "    \"\"\"\n",
    "    通用訓練函數\n",
    "    \n",
    "    Args:\n",
    "        model: 待訓練模型\n",
    "        dataloader: 數據載入器\n",
    "        optimizer: 優化器\n",
    "        device: 設備 (cuda/cpu)\n",
    "        num_steps: 訓練步數\n",
    "        use_amp: 是否使用混合精度\n",
    "        dtype: autocast 的數據類型 (torch.float16 或 torch.bfloat16)\n",
    "    \n",
    "    Returns:\n",
    "        dict: 訓練統計信息\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    # 記憶體追蹤\n",
    "    memory_tracker.reset()\n",
    "    \n",
    "    # 訓練統計\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataloader_iter = iter(dataloader)\n",
    "    \n",
    "    pbar = tqdm(range(num_steps), desc=\"Training\")\n",
    "    for step in pbar:\n",
    "        try:\n",
    "            batch = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "            batch = next(dataloader_iter)\n",
    "        \n",
    "        # 將數據移到設備\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向傳播\n",
    "        if use_amp:\n",
    "            with autocast(dtype=dtype if dtype else torch.float16):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # 反向傳播 (with gradient scaling)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # 計算統計\n",
    "    training_time = time.time() - start_time\n",
    "    memory_stats = memory_tracker.get_memory_stats()\n",
    "    \n",
    "    return {\n",
    "        \"losses\": losses,\n",
    "        \"avg_loss\": np.mean(losses),\n",
    "        \"final_loss\": losses[-1],\n",
    "        \"training_time\": training_time,\n",
    "        \"steps_per_sec\": num_steps / training_time,\n",
    "        \"peak_memory_gb\": memory_stats[\"peak\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32 基準測試\n",
    "print(\"=\" * 60)\n",
    "print(\"實驗 1: FP32 標準訓練\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 載入模型\n",
    "print(\"\\n載入 GPT-2 Small (124M 參數)...\")\n",
    "model_fp32 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_fp32 = model_fp32.to(device)\n",
    "\n",
    "print(f\"模型參數量: {sum(p.numel() for p in model_fp32.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"模型設備: {next(model_fp32.parameters()).device}\")\n",
    "print(f\"模型精度: {next(model_fp32.parameters()).dtype}\")\n",
    "\n",
    "# 初始化優化器\n",
    "optimizer_fp32 = torch.optim.AdamW(model_fp32.parameters(), lr=5e-5)\n",
    "\n",
    "# 訓練\n",
    "print(\"\\n開始訓練...\")\n",
    "results_fp32 = train_model(\n",
    "    model=model_fp32,\n",
    "    dataloader=train_loader,\n",
    "    optimizer=optimizer_fp32,\n",
    "    device=device,\n",
    "    num_steps=100,\n",
    "    use_amp=False\n",
    ")\n",
    "\n",
    "# 顯示結果\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FP32 訓練結果\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"平均 Loss: {results_fp32['avg_loss']:.4f}\")\n",
    "print(f\"最終 Loss: {results_fp32['final_loss']:.4f}\")\n",
    "print(f\"訓練時間: {results_fp32['training_time']:.2f} 秒\")\n",
    "print(f\"訓練速度: {results_fp32['steps_per_sec']:.2f} steps/sec\")\n",
    "print(f\"峰值記憶體: {results_fp32['peak_memory_gb']:.2f} GB\")\n",
    "\n",
    "# 清理記憶體\n",
    "del model_fp32, optimizer_fp32\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 實驗 2: FP16 混合精度訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"實驗 2: FP16 混合精度訓練\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 載入新模型\n",
    "print(\"\\n載入 GPT-2 Small (124M 參數)...\")\n",
    "model_fp16 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_fp16 = model_fp16.to(device)\n",
    "\n",
    "# 初始化優化器\n",
    "optimizer_fp16 = torch.optim.AdamW(model_fp16.parameters(), lr=5e-5)\n",
    "\n",
    "# 訓練 (使用 AMP)\n",
    "print(\"\\n開始 FP16 混合精度訓練...\")\n",
    "print(\"⚡ 使用 torch.cuda.amp.autocast(dtype=torch.float16)\")\n",
    "results_fp16 = train_model(\n",
    "    model=model_fp16,\n",
    "    dataloader=train_loader,\n",
    "    optimizer=optimizer_fp16,\n",
    "    device=device,\n",
    "    num_steps=100,\n",
    "    use_amp=True,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 顯示結果\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FP16 訓練結果\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"平均 Loss: {results_fp16['avg_loss']:.4f}\")\n",
    "print(f\"最終 Loss: {results_fp16['final_loss']:.4f}\")\n",
    "print(f\"訓練時間: {results_fp16['training_time']:.2f} 秒\")\n",
    "print(f\"訓練速度: {results_fp16['steps_per_sec']:.2f} steps/sec\")\n",
    "print(f\"峰值記憶體: {results_fp16['peak_memory_gb']:.2f} GB\")\n",
    "\n",
    "# 清理記憶體\n",
    "del model_fp16, optimizer_fp16\n",
    "memory_tracker.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 實驗 3: BF16 混合精度訓練 (可選)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 GPU 是否支持 BF16\n",
    "bf16_supported = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "if bf16_supported:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"實驗 3: BF16 混合精度訓練\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 載入新模型\n",
    "    print(\"\\n載入 GPT-2 Small (124M 參數)...\")\n",
    "    model_bf16 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    model_bf16 = model_bf16.to(device)\n",
    "    \n",
    "    # 初始化優化器\n",
    "    optimizer_bf16 = torch.optim.AdamW(model_bf16.parameters(), lr=5e-5)\n",
    "    \n",
    "    # 訓練 (使用 BF16 AMP)\n",
    "    print(\"\\n開始 BF16 混合精度訓練...\")\n",
    "    print(\"⚡ 使用 torch.cuda.amp.autocast(dtype=torch.bfloat16)\")\n",
    "    results_bf16 = train_model(\n",
    "        model=model_bf16,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer_bf16,\n",
    "        device=device,\n",
    "        num_steps=100,\n",
    "        use_amp=True,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # 顯示結果\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BF16 訓練結果\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"平均 Loss: {results_bf16['avg_loss']:.4f}\")\n",
    "    print(f\"最終 Loss: {results_bf16['final_loss']:.4f}\")\n",
    "    print(f\"訓練時間: {results_bf16['training_time']:.2f} 秒\")\n",
    "    print(f\"訓練速度: {results_bf16['steps_per_sec']:.2f} steps/sec\")\n",
    "    print(f\"峰值記憶體: {results_bf16['peak_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    # 清理記憶體\n",
    "    del model_bf16, optimizer_bf16\n",
    "    memory_tracker.reset()\n",
    "else:\n",
    "    print(\"⚠️ 當前 GPU 不支持 BF16, 跳過 BF16 實驗\")\n",
    "    results_bf16 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能對比分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理結果\n",
    "comparison_data = {\n",
    "    \"Precision\": [\"FP32\", \"FP16\"],\n",
    "    \"Avg Loss\": [results_fp32[\"avg_loss\"], results_fp16[\"avg_loss\"]],\n",
    "    \"Training Time (s)\": [results_fp32[\"training_time\"], results_fp16[\"training_time\"]],\n",
    "    \"Speed (steps/s)\": [results_fp32[\"steps_per_sec\"], results_fp16[\"steps_per_sec\"]],\n",
    "    \"Peak Memory (GB)\": [results_fp32[\"peak_memory_gb\"], results_fp16[\"peak_memory_gb\"]]\n",
    "}\n",
    "\n",
    "if results_bf16:\n",
    "    comparison_data[\"Precision\"].append(\"BF16\")\n",
    "    comparison_data[\"Avg Loss\"].append(results_bf16[\"avg_loss\"])\n",
    "    comparison_data[\"Training Time (s)\"].append(results_bf16[\"training_time\"])\n",
    "    comparison_data[\"Speed (steps/s)\"].append(results_bf16[\"steps_per_sec\"])\n",
    "    comparison_data[\"Peak Memory (GB)\"].append(results_bf16[\"peak_memory_gb\"])\n",
    "\n",
    "# 打印對比表格\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"性能對比總結\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Precision':<12} {'Avg Loss':<12} {'Time (s)':<12} {'Speed':<15} {'Memory (GB)':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for i in range(len(comparison_data[\"Precision\"])):\n",
    "    print(f\"{comparison_data['Precision'][i]:<12} \"\n",
    "          f\"{comparison_data['Avg Loss'][i]:<12.4f} \"\n",
    "          f\"{comparison_data['Training Time (s)'][i]:<12.2f} \"\n",
    "          f\"{comparison_data['Speed (steps/s)'][i]:<15.2f} \"\n",
    "          f\"{comparison_data['Peak Memory (GB)'][i]:<12.2f}\")\n",
    "\n",
    "# 計算提升百分比 (相對於 FP32)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"相對於 FP32 的提升\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fp32_time = results_fp32[\"training_time\"]\n",
    "fp32_memory = results_fp32[\"peak_memory_gb\"]\n",
    "\n",
    "for i, prec in enumerate(comparison_data[\"Precision\"]):\n",
    "    if prec != \"FP32\":\n",
    "        time_speedup = (fp32_time / comparison_data[\"Training Time (s)\"][i])\n",
    "        memory_saving = ((fp32_memory - comparison_data[\"Peak Memory (GB)\"][i]) / fp32_memory * 100)\n",
    "        print(f\"{prec}: 速度提升 {time_speedup:.2f}x, 記憶體節省 {memory_saving:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 視覺化對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建對比圖表\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"混合精度訓練性能對比\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "precisions = comparison_data[\"Precision\"]\n",
    "colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\"][:len(precisions)]\n",
    "\n",
    "# 1. 訓練時間對比\n",
    "axes[0, 0].bar(precisions, comparison_data[\"Training Time (s)\"], color=colors)\n",
    "axes[0, 0].set_title(\"訓練時間對比\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 0].set_ylabel(\"時間 (秒)\")\n",
    "axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 2. 訓練速度對比\n",
    "axes[0, 1].bar(precisions, comparison_data[\"Speed (steps/s)\"], color=colors)\n",
    "axes[0, 1].set_title(\"訓練速度對比\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 1].set_ylabel(\"速度 (steps/sec)\")\n",
    "axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 3. 記憶體使用對比\n",
    "axes[1, 0].bar(precisions, comparison_data[\"Peak Memory (GB)\"], color=colors)\n",
    "axes[1, 0].set_title(\"峰值記憶體使用\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"記憶體 (GB)\")\n",
    "axes[1, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 4. Loss 曲線對比\n",
    "axes[1, 1].plot(results_fp32[\"losses\"], label=\"FP32\", linewidth=2, color=colors[0])\n",
    "axes[1, 1].plot(results_fp16[\"losses\"], label=\"FP16\", linewidth=2, color=colors[1])\n",
    "if results_bf16:\n",
    "    axes[1, 1].plot(results_bf16[\"losses\"], label=\"BF16\", linewidth=2, color=colors[2])\n",
    "axes[1, 1].set_title(\"訓練 Loss 曲線\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 1].set_xlabel(\"Step\")\n",
    "axes[1, 1].set_ylabel(\"Loss\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 動態損失縮放演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"動態損失縮放 (GradScaler) 演示\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 創建一個簡單的模型和優化器\n",
    "simple_model = nn.Linear(10, 1).to(device)\n",
    "optimizer = torch.optim.SGD(simple_model.parameters(), lr=0.1)\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(f\"\\n初始損失縮放因子: {scaler.get_scale():.0f}\")\n",
    "\n",
    "# 模擬訓練過程\n",
    "scales = []\n",
    "for step in range(20):\n",
    "    # 生成隨機輸入\n",
    "    x = torch.randn(32, 10, device=device)\n",
    "    y = torch.randn(32, 1, device=device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(dtype=torch.float16):\n",
    "        output = simple_model(x)\n",
    "        loss = ((output - y) ** 2).mean()\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # 模擬梯度異常 (在第10步)\n",
    "    if step == 10:\n",
    "        # 製造 inf/nan\n",
    "        for param in simple_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data = param.grad.data * 1e10\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    current_scale = scaler.get_scale()\n",
    "    scales.append(current_scale)\n",
    "    \n",
    "    if step in [0, 10, 11, 19]:\n",
    "        print(f\"Step {step:2d}: 損失縮放因子 = {current_scale:.0f}\")\n",
    "\n",
    "# 繪製損失縮放變化\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(scales, marker=\"o\", linewidth=2)\n",
    "plt.axvline(x=10, color=\"r\", linestyle=\"--\", label=\"梯度異常點\")\n",
    "plt.title(\"動態損失縮放因子變化\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss Scale\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n說明: 在 Step 10 模擬梯度異常, GradScaler 自動降低縮放因子\")\n",
    "\n",
    "# 清理\n",
    "del simple_model, optimizer, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 實驗總結與最佳實踐\n",
    "\n",
    "### 實驗結論\n",
    "\n",
    "1. **記憶體節省**: FP16/BF16 可節省約 **50%** 記憶體\n",
    "2. **速度提升**: FP16/BF16 可提升 **2-3x** 訓練速度\n",
    "3. **精度影響**: 正確使用 AMP 時, 訓練效果與 FP32 基本相同\n",
    "4. **BF16 優勢**: BF16 數值範圍更大, 更穩定, 但需要 GPU 支持\n",
    "\n",
    "### 最佳實踐\n",
    "\n",
    "#### 何時使用混合精度?\n",
    "✅ **推薦場景**:\n",
    "- 所有現代 GPU 訓練 (Volta 架構以上)\n",
    "- 記憶體不足的情況\n",
    "- 需要加速訓練的場景\n",
    "\n",
    "❌ **不推薦場景**:\n",
    "- CPU 訓練\n",
    "- 老舊 GPU (GTX 10系列以下)\n",
    "- 對數值精度極度敏感的任務\n",
    "\n",
    "#### FP16 vs BF16 選擇\n",
    "\n",
    "**FP16**:\n",
    "- ✅ 更廣泛的 GPU 支持\n",
    "- ✅ 速度略快\n",
    "- ❌ 需要損失縮放\n",
    "- ❌ 數值範圍較小\n",
    "\n",
    "**BF16**:\n",
    "- ✅ 數值範圍與 FP32 相同\n",
    "- ✅ 訓練更穩定, 不需要損失縮放\n",
    "- ❌ 需要 Ampere 架構以上 (RTX 30系列+, A100等)\n",
    "\n",
    "### 常見問題處理\n",
    "\n",
    "#### 1. Loss 出現 NaN/Inf\n",
    "```python\n",
    "# 解決方案 1: 檢查損失縮放\n",
    "scaler = GradScaler(init_scale=2**10)  # 降低初始縮放\n",
    "\n",
    "# 解決方案 2: 添加梯度裁剪\n",
    "scaler.unscale_(optimizer)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "scaler.step(optimizer)\n",
    "\n",
    "# 解決方案 3: 使用 BF16 替代 FP16\n",
    "with autocast(dtype=torch.bfloat16):\n",
    "    loss = model(**batch).loss\n",
    "```\n",
    "\n",
    "#### 2. 速度沒有提升\n",
    "```python\n",
    "# 檢查是否正確使用 GPU\n",
    "print(next(model.parameters()).device)  # 應該是 cuda\n",
    "\n",
    "# 檢查 AMP 是否正確啟用\n",
    "print(torch.is_autocast_enabled())  # autocast 內部應該是 True\n",
    "\n",
    "# 確保批次大小足夠大 (至少 8-16)\n",
    "```\n",
    "\n",
    "### 生產環境建議\n",
    "\n",
    "```python\n",
    "# 推薦的生產配置\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 選擇合適的精度\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    amp_dtype = torch.bfloat16\n",
    "    use_scaler = False  # BF16 不需要 scaler\n",
    "else:\n",
    "    amp_dtype = torch.float16\n",
    "    use_scaler = True\n",
    "    scaler = GradScaler()\n",
    "\n",
    "# 訓練循環\n",
    "for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(dtype=amp_dtype):\n",
    "        loss = model(**batch).loss\n",
    "    \n",
    "    if use_scaler:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 下一步學習\n",
    "\n",
    "完成本 Notebook 後, 建議繼續學習:\n",
    "\n",
    "1. **02-Gradient_Accumulation.ipynb** - 梯度累積技術\n",
    "2. **03-Gradient_Checkpointing.ipynb** - 梯度檢查點\n",
    "3. **04-Memory_Profiling.ipynb** - 記憶體分析工具\n",
    "\n",
    "將混合精度訓練與其他優化技術組合使用, 可以達到最佳的訓練效率!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
