{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5.1 - Kubernetes 原生部署\n",
    "\n",
    "## 🎯 實驗目標\n",
    "\n",
    "本實驗將教您如何：\n",
    "1. 設計雲原生 Triton 部署架構\n",
    "2. 實現 GPU 資源調度與管理\n",
    "3. 配置服務發現與負載均衡\n",
    "4. 建立自動擴縮容機制\n",
    "5. 實施滾動更新與回滾策略\n",
    "\n",
    "## 📋 前置需求\n",
    "\n",
    "- Kubernetes 叢集 (v1.24+)\n",
    "- NVIDIA GPU Operator 或 Device Plugin\n",
    "- Helm 3.x\n",
    "- kubectl 已配置\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 理論背景\n",
    "\n",
    "### Kubernetes 中的 AI 推理挑戰\n",
    "\n",
    "**1. GPU 資源管理**\n",
    "- GPU 是稀缺且昂貴的計算資源\n",
    "- 需要精確的資源調度和分配\n",
    "- 支援多租戶和資源隔離\n",
    "\n",
    "**2. 模型生命週期管理**\n",
    "- 模型文件通常體積龐大 (數 GB 到數十 GB)\n",
    "- 需要版本控制和快速部署\n",
    "- 支援 A/B 測試和漸進式發布\n",
    "\n",
    "**3. 服務高可用性**\n",
    "- 零停機時間的模型更新\n",
    "- 自動故障轉移和恢復\n",
    "- 彈性擴縮容應對流量變化\n",
    "\n",
    "### 雲原生 AI 推理架構\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Ingress Controller                       │\n",
    "│                  (NGINX/Traefik/Istio)                     │\n",
    "└─────────────────┬───────────────────────────────────────────┘\n",
    "                  │\n",
    "      ┌───────────▼─────────────────────────────────┐\n",
    "      │            Service Mesh (Optional)          │\n",
    "      │              (Istio/Linkerd)                │\n",
    "      └─────────────────┬───────────────────────────┘\n",
    "                        │\n",
    "    ┌───────────────────▼───────────────────────────┐\n",
    "    │              LoadBalancer Service             │\n",
    "    └─────┬─────────────────────────────────┬───────┘\n",
    "          │                                 │\n",
    "    ┌─────▼──────┐                   ┌─────▼──────┐\n",
    "    │   Pod-1    │                   │   Pod-2    │\n",
    "    │ ┌────────┐ │                   │ ┌────────┐ │\n",
    "    │ │ Triton │ │                   │ │ Triton │ │\n",
    "    │ │ Server │ │                   │ │ Server │ │\n",
    "    │ └────────┘ │                   │ └────────┘ │\n",
    "    │ ┌────────┐ │                   │ ┌────────┐ │\n",
    "    │ │  GPU   │ │                   │ │  GPU   │ │\n",
    "    │ └────────┘ │                   │ └────────┘ │\n",
    "    └────────────┘                   └────────────┘\n",
    "          │                                 │\n",
    "    ┌─────▼─────────────────────────────────▼───────┐\n",
    "    │           Persistent Volume (Models)          │\n",
    "    │              (NFS/GlusterFS/S3)               │\n",
    "    └───────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "print(f\"Environment ready at {datetime.now()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置 Kubernetes 部署環境\n",
    "K8S_DIR = \"/tmp/triton-k8s\"\n",
    "MANIFESTS_DIR = f\"{K8S_DIR}/manifests\"\n",
    "HELM_DIR = f\"{K8S_DIR}/helm\"\n",
    "CONFIGS_DIR = f\"{K8S_DIR}/configs\"\n",
    "\n",
    "# 創建目錄結構\n",
    "directories = [\n",
    "    MANIFESTS_DIR,\n",
    "    HELM_DIR,\n",
    "    CONFIGS_DIR,\n",
    "    f\"{HELM_DIR}/triton-inference\",\n",
    "    f\"{HELM_DIR}/triton-inference/templates\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(\"📁 Kubernetes 部署目錄結構:\")\n",
    "for directory in directories:\n",
    "    print(f\"   {directory}\")\n",
    "\n",
    "# 全域配置\n",
    "NAMESPACE = \"triton-inference\"\n",
    "APP_NAME = \"triton-server\"\n",
    "IMAGE = \"nvcr.io/nvidia/tritonserver:24.10-py3\"\n",
    "MODEL_REPOSITORY = \"/models\"\n",
    "\n",
    "print(f\"\\n⚙️  部署配置:\")\n",
    "print(f\"   命名空間: {NAMESPACE}\")\n",
    "print(f\"   應用名稱: {APP_NAME}\")\n",
    "print(f\"   容器映像: {IMAGE}\")\n",
    "print(f\"   模型倉庫: {MODEL_REPOSITORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 1：集群環境驗證與準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Kubernetes 集群檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_kubernetes_cluster():\n",
    "    \"\"\"檢查 Kubernetes 集群狀態\"\"\"\n",
    "    try:\n",
    "        # 載入 kubeconfig\n",
    "        config.load_kube_config()\n",
    "        v1 = client.CoreV1Api()\n",
    "        \n",
    "        # 檢查節點狀態\n",
    "        print(\"🔍 檢查集群節點:\")\n",
    "        nodes = v1.list_node()\n",
    "        for node in nodes.items:\n",
    "            name = node.metadata.name\n",
    "            status = \"Ready\" if any(condition.type == \"Ready\" and condition.status == \"True\" \n",
    "                                  for condition in node.status.conditions) else \"NotReady\"\n",
    "            \n",
    "            # 檢查 GPU 資源\n",
    "            gpu_capacity = node.status.capacity.get('nvidia.com/gpu', '0')\n",
    "            gpu_allocatable = node.status.allocatable.get('nvidia.com/gpu', '0')\n",
    "            \n",
    "            print(f\"   節點: {name}\")\n",
    "            print(f\"   狀態: {status}\")\n",
    "            print(f\"   GPU 容量: {gpu_capacity}\")\n",
    "            print(f\"   GPU 可分配: {gpu_allocatable}\")\n",
    "            print()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 集群檢查失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "cluster_ready = check_kubernetes_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GPU Operator 驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_operator():\n",
    "    \"\"\"檢查 NVIDIA GPU Operator 狀態\"\"\"\n",
    "    try:\n",
    "        # 檢查 GPU Operator namespace\n",
    "        v1 = client.CoreV1Api()\n",
    "        apps_v1 = client.AppsV1Api()\n",
    "        \n",
    "        print(\"🔍 檢查 GPU Operator:\")\n",
    "        \n",
    "        # 檢查 gpu-operator namespace\n",
    "        try:\n",
    "            namespace = v1.read_namespace(name=\"gpu-operator\")\n",
    "            print(f\"   ✅ GPU Operator namespace 存在\")\n",
    "        except ApiException as e:\n",
    "            if e.status == 404:\n",
    "                print(f\"   ⚠️  GPU Operator namespace 不存在\")\n",
    "                return False\n",
    "        \n",
    "        # 檢查 DaemonSet\n",
    "        daemonsets = apps_v1.list_namespaced_daemon_set(namespace=\"gpu-operator\")\n",
    "        for ds in daemonsets.items:\n",
    "            name = ds.metadata.name\n",
    "            desired = ds.status.desired_number_scheduled or 0\n",
    "            ready = ds.status.number_ready or 0\n",
    "            print(f\"   DaemonSet: {name} ({ready}/{desired} ready)\")\n",
    "        \n",
    "        # 檢查 Device Plugin\n",
    "        pods = v1.list_namespaced_pod(namespace=\"gpu-operator\", \n",
    "                                     label_selector=\"app=nvidia-device-plugin-daemonset\")\n",
    "        \n",
    "        if pods.items:\n",
    "            print(f\"   ✅ NVIDIA Device Plugin 運行中 ({len(pods.items)} pods)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   ⚠️  NVIDIA Device Plugin 未找到\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ GPU Operator 檢查失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "gpu_ready = check_gpu_operator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 創建命名空間和基礎資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建命名空間 YAML\n",
    "namespace_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: {NAMESPACE}\n",
    "  labels:\n",
    "    name: {NAMESPACE}\n",
    "    purpose: ai-inference\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ResourceQuota\n",
    "metadata:\n",
    "  name: triton-quota\n",
    "  namespace: {NAMESPACE}\n",
    "spec:\n",
    "  hard:\n",
    "    requests.cpu: \"8\"\n",
    "    requests.memory: 32Gi\n",
    "    requests.nvidia.com/gpu: \"4\"\n",
    "    limits.cpu: \"16\"\n",
    "    limits.memory: 64Gi\n",
    "    limits.nvidia.com/gpu: \"4\"\n",
    "    persistentvolumeclaims: \"2\"\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/namespace.yaml\", \"w\") as f:\n",
    "    f.write(namespace_yaml)\n",
    "\n",
    "print(\"📝 已創建命名空間配置\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/namespace.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kubernetes_manifest(manifest_path: str) -> bool:\n",
    "    \"\"\"應用 Kubernetes manifest\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"kubectl\", \"apply\", \"-f\", manifest_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"✅ 成功應用: {manifest_path}\")\n",
    "        print(f\"   輸出: {result.stdout.strip()}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ 應用失敗: {manifest_path}\")\n",
    "        print(f\"   錯誤: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "# 應用命名空間配置\n",
    "if cluster_ready:\n",
    "    apply_kubernetes_manifest(f\"{MANIFESTS_DIR}/namespace.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 2：模型儲存與 PV 配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 設計模型儲存策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PersistentVolume 和 PersistentVolumeClaim 配置\n",
    "storage_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: triton-models-pv\n",
    "  labels:\n",
    "    type: models\n",
    "    app: triton-server\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 100Gi\n",
    "  accessModes:\n",
    "    - ReadOnlyMany\n",
    "  persistentVolumeReclaimPolicy: Retain\n",
    "  storageClassName: models-storage\n",
    "  hostPath:\n",
    "    path: /data/triton/models\n",
    "    type: DirectoryOrCreate\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: triton-models-pvc\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: storage\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadOnlyMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 100Gi\n",
    "  storageClassName: models-storage\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: models\n",
    "      app: triton-server\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: triton-cache-pv\n",
    "  labels:\n",
    "    type: cache\n",
    "    app: triton-server\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 50Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  persistentVolumeReclaimPolicy: Delete\n",
    "  storageClassName: cache-storage\n",
    "  hostPath:\n",
    "    path: /data/triton/cache\n",
    "    type: DirectoryOrCreate\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: triton-cache-pvc\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: cache\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 50Gi\n",
    "  storageClassName: cache-storage\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: cache\n",
    "      app: triton-server\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/storage.yaml\", \"w\") as f:\n",
    "    f.write(storage_yaml)\n",
    "\n",
    "print(\"📝 已創建儲存配置\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/storage.yaml\")\n",
    "print(\"\\n💾 儲存策略:\")\n",
    "print(\"   - 模型儲存: 100Gi (ReadOnlyMany)\")\n",
    "print(\"   - 快取儲存: 50Gi (ReadWriteMany)\")\n",
    "print(\"   - 回收策略: Retain (models), Delete (cache)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ConfigMap 配置管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfigMap 用於 Triton 配置\n",
    "configmap_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: triton-config\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: config\n",
    "data:\n",
    "  # Triton 服務器配置\n",
    "  triton-config.json: |\n",
    "    {{\n",
    "      \"backend_config\": {{\n",
    "        \"pytorch\": {{\n",
    "          \"cmdline\": {{\n",
    "            \"auto-complete-config\": \"true\",\n",
    "            \"backend-directory\": \"/opt/tritonserver/backends\",\n",
    "            \"min-compute-capability\": \"6.0\"\n",
    "          }}\n",
    "        }}\n",
    "      }},\n",
    "      \"model_config_name\": \"config.pbtxt\",\n",
    "      \"log_level\": 1,\n",
    "      \"log_verbose\": 1,\n",
    "      \"metrics\": {{\n",
    "        \"allow_metrics\": true,\n",
    "        \"allow_gpu_metrics\": true,\n",
    "        \"allow_cpu_metrics\": true,\n",
    "        \"metrics_interval_ms\": 1000\n",
    "      }}\n",
    "    }}\n",
    "  \n",
    "  # 健康檢查腳本\n",
    "  health-check.sh: |\n",
    "    #!/bin/bash\n",
    "    set -e\n",
    "    \n",
    "    # 檢查 Triton 服務器狀態\n",
    "    curl -f http://localhost:8000/v2/health/ready || exit 1\n",
    "    \n",
    "    # 檢查模型狀態\n",
    "    MODEL_COUNT=$(curl -s http://localhost:8000/v2/models | jq '.models | length')\n",
    "    if [ \"$MODEL_COUNT\" -eq 0 ]; then\n",
    "      echo \"No models loaded\"\n",
    "      exit 1\n",
    "    fi\n",
    "    \n",
    "    echo \"Health check passed: $MODEL_COUNT models loaded\"\n",
    "  \n",
    "  # 模型載入腳本\n",
    "  model-loader.sh: |\n",
    "    #!/bin/bash\n",
    "    set -e\n",
    "    \n",
    "    echo \"Starting model loader...\"\n",
    "    \n",
    "    # 等待模型儲存掛載\n",
    "    while [ ! -d \"{MODEL_REPOSITORY}\" ]; do\n",
    "      echo \"Waiting for model repository to be mounted...\"\n",
    "      sleep 5\n",
    "    done\n",
    "    \n",
    "    # 檢查模型文件\n",
    "    MODEL_COUNT=$(find {MODEL_REPOSITORY} -name \"config.pbtxt\" | wc -l)\n",
    "    echo \"Found $MODEL_COUNT models in repository\"\n",
    "    \n",
    "    if [ \"$MODEL_COUNT\" -eq 0 ]; then\n",
    "      echo \"Warning: No models found in repository\"\n",
    "    fi\n",
    "    \n",
    "    echo \"Model loader completed\"\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: triton-secrets\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: security\n",
    "type: Opaque\n",
    "data:\n",
    "  # Base64 編碼的憑證 (範例)\n",
    "  model-access-key: bW9kZWwtYWNjZXNzLWtleQ==\n",
    "  registry-token: cmVnaXN0cnktdG9rZW4=\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/configmap.yaml\", \"w\") as f:\n",
    "    f.write(configmap_yaml)\n",
    "\n",
    "print(\"📝 已創建配置管理\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/configmap.yaml\")\n",
    "print(\"\\n⚙️  配置內容:\")\n",
    "print(\"   - Triton 服務器配置\")\n",
    "print(\"   - 健康檢查腳本\")\n",
    "print(\"   - 模型載入腳本\")\n",
    "print(\"   - 安全憑證\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 3：Triton Server Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 設計高可用部署配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triton Server Deployment\n",
    "deployment_yaml = f\"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: {APP_NAME}\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    version: v1\n",
    "    component: inference-server\n",
    "spec:\n",
    "  replicas: 2\n",
    "  strategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxSurge: 1\n",
    "      maxUnavailable: 0\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: {APP_NAME}\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: {APP_NAME}\n",
    "        version: v1\n",
    "        component: inference-server\n",
    "      annotations:\n",
    "        prometheus.io/scrape: \"true\"\n",
    "        prometheus.io/port: \"8002\"\n",
    "        prometheus.io/path: \"/metrics\"\n",
    "    spec:\n",
    "      serviceAccountName: triton-service-account\n",
    "      securityContext:\n",
    "        runAsNonRoot: true\n",
    "        runAsUser: 1000\n",
    "        fsGroup: 1000\n",
    "      \n",
    "      # 節點選擇器 - 選擇有 GPU 的節點\n",
    "      nodeSelector:\n",
    "        accelerator: nvidia-tesla-gpu\n",
    "      \n",
    "      # 容忍度 - 允許在有 GPU 污點的節點上調度\n",
    "      tolerations:\n",
    "      - key: nvidia.com/gpu\n",
    "        operator: Exists\n",
    "        effect: NoSchedule\n",
    "      \n",
    "      # 反親和性 - 避免多個 Pod 調度到同一節點\n",
    "      affinity:\n",
    "        podAntiAffinity:\n",
    "          preferredDuringSchedulingIgnoredDuringExecution:\n",
    "          - weight: 100\n",
    "            podAffinityTerm:\n",
    "              labelSelector:\n",
    "                matchLabels:\n",
    "                  app: {APP_NAME}\n",
    "              topologyKey: kubernetes.io/hostname\n",
    "      \n",
    "      # Init Container - 模型預載入\n",
    "      initContainers:\n",
    "      - name: model-loader\n",
    "        image: curlimages/curl:latest\n",
    "        command: [\"/bin/sh\"]\n",
    "        args: [\"/scripts/model-loader.sh\"]\n",
    "        volumeMounts:\n",
    "        - name: triton-models\n",
    "          mountPath: {MODEL_REPOSITORY}\n",
    "          readOnly: true\n",
    "        - name: triton-config\n",
    "          mountPath: /scripts\n",
    "          readOnly: true\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 128Mi\n",
    "          limits:\n",
    "            cpu: 500m\n",
    "            memory: 512Mi\n",
    "      \n",
    "      containers:\n",
    "      - name: triton-server\n",
    "        image: {IMAGE}\n",
    "        imagePullPolicy: IfNotPresent\n",
    "        \n",
    "        command: [\"tritonserver\"]\n",
    "        args:\n",
    "        - --model-repository={MODEL_REPOSITORY}\n",
    "        - --allow-http=true\n",
    "        - --allow-grpc=true\n",
    "        - --allow-metrics=true\n",
    "        - --allow-gpu-metrics=true\n",
    "        - --strict-model-config=false\n",
    "        - --strict-readiness=false\n",
    "        - --http-port=8000\n",
    "        - --grpc-port=8001\n",
    "        - --metrics-port=8002\n",
    "        - --log-verbose=1\n",
    "        - --model-control-mode=poll\n",
    "        - --repository-poll-secs=30\n",
    "        \n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "          name: http\n",
    "          protocol: TCP\n",
    "        - containerPort: 8001\n",
    "          name: grpc\n",
    "          protocol: TCP\n",
    "        - containerPort: 8002\n",
    "          name: metrics\n",
    "          protocol: TCP\n",
    "        \n",
    "        # 資源請求和限制\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 2\n",
    "            memory: 8Gi\n",
    "            nvidia.com/gpu: 1\n",
    "          limits:\n",
    "            cpu: 4\n",
    "            memory: 16Gi\n",
    "            nvidia.com/gpu: 1\n",
    "        \n",
    "        # 健康檢查\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /v2/health/live\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 30\n",
    "          timeoutSeconds: 10\n",
    "          failureThreshold: 3\n",
    "        \n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /v2/health/ready\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 15\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          failureThreshold: 3\n",
    "        \n",
    "        startupProbe:\n",
    "          httpGet:\n",
    "            path: /v2/health/ready\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          failureThreshold: 30\n",
    "        \n",
    "        # 環境變數\n",
    "        env:\n",
    "        - name: CUDA_VISIBLE_DEVICES\n",
    "          value: \"0\"\n",
    "        - name: TRITON_MODEL_REPOSITORY\n",
    "          value: {MODEL_REPOSITORY}\n",
    "        - name: NVIDIA_VISIBLE_DEVICES\n",
    "          value: all\n",
    "        - name: NVIDIA_DRIVER_CAPABILITIES\n",
    "          value: compute,utility\n",
    "        \n",
    "        # 掛載點\n",
    "        volumeMounts:\n",
    "        - name: triton-models\n",
    "          mountPath: {MODEL_REPOSITORY}\n",
    "          readOnly: true\n",
    "        - name: triton-cache\n",
    "          mountPath: /opt/tritonserver/cache\n",
    "        - name: triton-config\n",
    "          mountPath: /opt/tritonserver/config\n",
    "          readOnly: true\n",
    "        - name: triton-secrets\n",
    "          mountPath: /opt/tritonserver/secrets\n",
    "          readOnly: true\n",
    "        - name: shm\n",
    "          mountPath: /dev/shm\n",
    "        \n",
    "        # 安全上下文\n",
    "        securityContext:\n",
    "          allowPrivilegeEscalation: false\n",
    "          readOnlyRootFilesystem: false\n",
    "          capabilities:\n",
    "            drop:\n",
    "            - ALL\n",
    "      \n",
    "      # 存儲卷\n",
    "      volumes:\n",
    "      - name: triton-models\n",
    "        persistentVolumeClaim:\n",
    "          claimName: triton-models-pvc\n",
    "      - name: triton-cache\n",
    "        persistentVolumeClaim:\n",
    "          claimName: triton-cache-pvc\n",
    "      - name: triton-config\n",
    "        configMap:\n",
    "          name: triton-config\n",
    "          defaultMode: 0755\n",
    "      - name: triton-secrets\n",
    "        secret:\n",
    "          secretName: triton-secrets\n",
    "          defaultMode: 0600\n",
    "      - name: shm\n",
    "        emptyDir:\n",
    "          medium: Memory\n",
    "          sizeLimit: 1Gi\n",
    "      \n",
    "      # 重啟策略\n",
    "      restartPolicy: Always\n",
    "      terminationGracePeriodSeconds: 30\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/deployment.yaml\", \"w\") as f:\n",
    "    f.write(deployment_yaml)\n",
    "\n",
    "print(\"📝 已創建部署配置\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/deployment.yaml\")\n",
    "print(\"\\n🚀 部署特性:\")\n",
    "print(\"   - 高可用性: 2 副本 + 反親和性\")\n",
    "print(\"   - 滾動更新: 零停機部署\")\n",
    "print(\"   - GPU 調度: 節點選擇器 + 容忍度\")\n",
    "print(\"   - 健康檢查: 存活/就緒/啟動探針\")\n",
    "print(\"   - 資源管理: CPU/Memory/GPU 限制\")\n",
    "print(\"   - 安全性: 非root用戶 + 安全上下文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 服務帳戶和 RBAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ServiceAccount 和 RBAC 配置\n",
    "rbac_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: triton-service-account\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: security\n",
    "automountServiceAccountToken: true\n",
    "\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: Role\n",
    "metadata:\n",
    "  name: triton-role\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: security\n",
    "rules:\n",
    "# 允許讀取 ConfigMaps 和 Secrets\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"configmaps\", \"secrets\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "\n",
    "# 允許讀取 PVC 狀態\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"persistentvolumeclaims\"]\n",
    "  verbs: [\"get\", \"list\"]\n",
    "\n",
    "# 允許讀取自己的 Pod 資訊\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"pods\"]\n",
    "  verbs: [\"get\", \"list\"]\n",
    "\n",
    "# 允許創建 Events\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"events\"]\n",
    "  verbs: [\"create\", \"patch\"]\n",
    "\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: RoleBinding\n",
    "metadata:\n",
    "  name: triton-role-binding\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: security\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: triton-service-account\n",
    "  namespace: {NAMESPACE}\n",
    "roleRef:\n",
    "  kind: Role\n",
    "  name: triton-role\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: triton-monitoring\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: monitoring\n",
    "\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRole\n",
    "metadata:\n",
    "  name: triton-monitoring-cluster-role\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: monitoring\n",
    "rules:\n",
    "# 允許讀取節點和 Pod 指標\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"nodes\", \"nodes/metrics\", \"services\", \"endpoints\", \"pods\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "\n",
    "# 允許讀取 Deployment 和 ReplicaSet\n",
    "- apiGroups: [\"apps\"]\n",
    "  resources: [\"deployments\", \"replicasets\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "\n",
    "# 允許讀取指標\n",
    "- apiGroups: [\"metrics.k8s.io\"]\n",
    "  resources: [\"nodes\", \"pods\"]\n",
    "  verbs: [\"get\", \"list\"]\n",
    "\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: triton-monitoring-cluster-role-binding\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: monitoring\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: triton-monitoring\n",
    "  namespace: {NAMESPACE}\n",
    "roleRef:\n",
    "  kind: ClusterRole\n",
    "  name: triton-monitoring-cluster-role\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/rbac.yaml\", \"w\") as f:\n",
    "    f.write(rbac_yaml)\n",
    "\n",
    "print(\"📝 已創建 RBAC 配置\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/rbac.yaml\")\n",
    "print(\"\\n🔐 安全配置:\")\n",
    "print(\"   - 服務帳戶: triton-service-account\")\n",
    "print(\"   - 最小權限原則: 僅必要的資源訪問\")\n",
    "print(\"   - 監控權限: 獨立的監控服務帳戶\")\n",
    "print(\"   - 命名空間隔離: 權限限制在命名空間內\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 4：服務發現與負載均衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 設計服務層級"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service 配置\n",
    "service_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: {APP_NAME}-service\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: load-balancer\n",
    "  annotations:\n",
    "    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n",
    "    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"tcp\"\n",
    "    prometheus.io/scrape: \"true\"\n",
    "    prometheus.io/port: \"8002\"\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: {APP_NAME}\n",
    "  ports:\n",
    "  - name: http\n",
    "    port: 8000\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "  - name: grpc\n",
    "    port: 8001\n",
    "    targetPort: 8001\n",
    "    protocol: TCP\n",
    "  - name: metrics\n",
    "    port: 8002\n",
    "    targetPort: 8002\n",
    "    protocol: TCP\n",
    "  sessionAffinity: None\n",
    "  loadBalancerSourceRanges:\n",
    "  - 10.0.0.0/8\n",
    "  - 172.16.0.0/12\n",
    "  - 192.168.0.0/16\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: {APP_NAME}-headless\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: discovery\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  clusterIP: None\n",
    "  selector:\n",
    "    app: {APP_NAME}\n",
    "  ports:\n",
    "  - name: http\n",
    "    port: 8000\n",
    "    targetPort: 8000\n",
    "  - name: grpc\n",
    "    port: 8001\n",
    "    targetPort: 8001\n",
    "  - name: metrics\n",
    "    port: 8002\n",
    "    targetPort: 8002\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: {APP_NAME}-internal\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: internal\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  selector:\n",
    "    app: {APP_NAME}\n",
    "  ports:\n",
    "  - name: http\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  - name: grpc\n",
    "    port: 8001\n",
    "    targetPort: 8001\n",
    "  sessionAffinity: ClientIP\n",
    "  sessionAffinityConfig:\n",
    "    clientIP:\n",
    "      timeoutSeconds: 300\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/service.yaml\", \"w\") as f:\n",
    "    f.write(service_yaml)\n",
    "\n",
    "print(\"📝 已創建服務配置\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/service.yaml\")\n",
    "print(\"\\n🌐 服務層級:\")\n",
    "print(\"   - LoadBalancer: 外部訪問 (HTTP/gRPC/Metrics)\")\n",
    "print(\"   - Headless: 服務發現 (DNS)\")\n",
    "print(\"   - ClusterIP: 內部訪問 (會話親和性)\")\n",
    "print(\"   - 安全性: 私有網路來源限制\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ingress 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingress 配置\n",
    "ingress_yaml = f\"\"\"\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: {APP_NAME}-ingress\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: ingress\n",
    "  annotations:\n",
    "    # NGINX Ingress Controller 配置\n",
    "    kubernetes.io/ingress.class: \"nginx\"\n",
    "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
    "    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n",
    "    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n",
    "    \n",
    "    # 速率限制\n",
    "    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n",
    "    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\n",
    "    \n",
    "    # 連接和請求超時\n",
    "    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"60\"\n",
    "    nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n",
    "    nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n",
    "    nginx.ingress.kubernetes.io/proxy-body-size: \"100m\"\n",
    "    \n",
    "    # 健康檢查\n",
    "    nginx.ingress.kubernetes.io/upstream-healthcheck-path: \"/v2/health/ready\"\n",
    "    nginx.ingress.kubernetes.io/upstream-healthcheck-interval: \"30s\"\n",
    "    \n",
    "    # 負載均衡\n",
    "    nginx.ingress.kubernetes.io/load-balance: \"round_robin\"\n",
    "    nginx.ingress.kubernetes.io/upstream-hash-by: \"$remote_addr\"\n",
    "    \n",
    "    # 安全標頭\n",
    "    nginx.ingress.kubernetes.io/configuration-snippet: |\n",
    "      more_set_headers \"X-Content-Type-Options: nosniff\";\n",
    "      more_set_headers \"X-Frame-Options: DENY\";\n",
    "      more_set_headers \"X-XSS-Protection: 1; mode=block\";\n",
    "      more_set_headers \"Referrer-Policy: strict-origin-when-cross-origin\";\n",
    "      \n",
    "    # TLS 配置\n",
    "    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n",
    "    \n",
    "spec:\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - triton.example.com\n",
    "    - api.triton.example.com\n",
    "    secretName: triton-tls-secret\n",
    "  \n",
    "  rules:\n",
    "  # 主要 API 端點\n",
    "  - host: triton.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /v2\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: {APP_NAME}-service\n",
    "            port:\n",
    "              number: 8000\n",
    "      - path: /metrics\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: {APP_NAME}-service\n",
    "            port:\n",
    "              number: 8002\n",
    "  \n",
    "  # API 專用域名\n",
    "  - host: api.triton.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: {APP_NAME}-service\n",
    "            port:\n",
    "              number: 8000\n",
    "\n",
    "---\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: {APP_NAME}-network-policy\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: security\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app: {APP_NAME}\n",
    "  \n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  - Egress\n",
    "  \n",
    "  ingress:\n",
    "  # 允許來自 Ingress Controller 的流量\n",
    "  - from:\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          name: ingress-nginx\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 8000\n",
    "    - protocol: TCP\n",
    "      port: 8001\n",
    "    - protocol: TCP\n",
    "      port: 8002\n",
    "  \n",
    "  # 允許來自相同命名空間的流量\n",
    "  - from:\n",
    "    - podSelector: {{}}\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 8000\n",
    "    - protocol: TCP\n",
    "      port: 8001\n",
    "    - protocol: TCP\n",
    "      port: 8002\n",
    "  \n",
    "  # 允許來自監控命名空間的流量\n",
    "  - from:\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          name: monitoring\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 8002\n",
    "  \n",
    "  egress:\n",
    "  # 允許 DNS 查詢\n",
    "  - to: []\n",
    "    ports:\n",
    "    - protocol: UDP\n",
    "      port: 53\n",
    "    - protocol: TCP\n",
    "      port: 53\n",
    "  \n",
    "  # 允許 HTTPS 出站 (模型下載)\n",
    "  - to: []\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 443\n",
    "  \n",
    "  # 允許與 Kubernetes API 通信\n",
    "  - to:\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          name: kube-system\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 6443\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/ingress.yaml\", \"w\") as f:\n",
    "    f.write(ingress_yaml)\n",
    "\n",
    "print(\"📝 已創建 Ingress 配置\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/ingress.yaml\")\n",
    "print(\"\\n🌍 Ingress 特性:\")\n",
    "print(\"   - TLS 終止: Let's Encrypt 自動證書\")\n",
    "print(\"   - 速率限制: 100 req/min\")\n",
    "print(\"   - 健康檢查: 自動後端檢測\")\n",
    "print(\"   - 安全標頭: 防護常見攻擊\")\n",
    "print(\"   - 網路政策: 精細流量控制\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 5：自動擴縮容配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 HPA (Horizontal Pod Autoscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPA 配置\n",
    "hpa_yaml = f\"\"\"\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: {APP_NAME}-hpa\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: autoscaling\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: {APP_NAME}\n",
    "  \n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  \n",
    "  metrics:\n",
    "  # CPU 使用率\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  \n",
    "  # 記憶體使用率\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  \n",
    "  # GPU 使用率 (自定義指標)\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: nvidia_gpu_utilization\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"75\"\n",
    "  \n",
    "  # 請求速率 (自定義指標)\n",
    "  - type: Object\n",
    "    object:\n",
    "      metric:\n",
    "        name: triton_request_rate\n",
    "      describedObject:\n",
    "        apiVersion: v1\n",
    "        kind: Service\n",
    "        name: {APP_NAME}-service\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"100\"\n",
    "  \n",
    "  behavior:\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 300\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100\n",
    "        periodSeconds: 60\n",
    "      - type: Pods\n",
    "        value: 2\n",
    "        periodSeconds: 60\n",
    "      selectPolicy: Max\n",
    "    \n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 600\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50\n",
    "        periodSeconds: 300\n",
    "      - type: Pods\n",
    "        value: 1\n",
    "        periodSeconds: 300\n",
    "      selectPolicy: Min\n",
    "\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: VerticalPodAutoscaler\n",
    "metadata:\n",
    "  name: {APP_NAME}-vpa\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: autoscaling\n",
    "spec:\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: {APP_NAME}\n",
    "  \n",
    "  updatePolicy:\n",
    "    updateMode: \"Off\"  # 僅建議，不自動應用\n",
    "  \n",
    "  resourcePolicy:\n",
    "    containerPolicies:\n",
    "    - containerName: triton-server\n",
    "      minAllowed:\n",
    "        cpu: 1\n",
    "        memory: 4Gi\n",
    "      maxAllowed:\n",
    "        cpu: 8\n",
    "        memory: 32Gi\n",
    "      controlledResources: [\"cpu\", \"memory\"]\n",
    "      controlledValues: RequestsAndLimits\n",
    "\n",
    "---\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: {APP_NAME}-pdb\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: availability\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: {APP_NAME}\n",
    "  \n",
    "  # 至少保持 50% 的 Pod 可用\n",
    "  minAvailable: 50%\n",
    "  \n",
    "  # 或者最多允許 1 個 Pod 不可用\n",
    "  # maxUnavailable: 1\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/autoscaling.yaml\", \"w\") as f:\n",
    "    f.write(hpa_yaml)\n",
    "\n",
    "print(\"📝 已創建自動擴縮容配置\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/autoscaling.yaml\")\n",
    "print(\"\\n📈 擴縮容策略:\")\n",
    "print(\"   - HPA: CPU/Memory/GPU/RPS 多指標\")\n",
    "print(\"   - VPA: 垂直擴展建議 (手動應用)\")\n",
    "print(\"   - PDB: 50% 最小可用性保證\")\n",
    "print(\"   - 擴展行為: 快速擴展 + 緩慢收縮\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Custom Metrics 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus Adapter 配置\n",
    "metrics_config_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: adapter-config\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: prometheus-adapter\n",
    "    component: metrics\n",
    "data:\n",
    "  config.yaml: |\n",
    "    rules:\n",
    "    # Triton 請求速率指標\n",
    "    - seriesQuery: 'nv_inference_request_success_rate{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^nv_inference_request_success_rate\"\n",
    "        as: \"triton_request_rate\"\n",
    "      metricsQuery: 'sum(rate(<<.Series>>{{<<.LabelMatchers>>}}[2m])) by (<<.GroupBy>>)'\n",
    "    \n",
    "    # GPU 使用率指標\n",
    "    - seriesQuery: 'DCGM_FI_DEV_GPU_UTIL{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^DCGM_FI_DEV_GPU_UTIL\"\n",
    "        as: \"nvidia_gpu_utilization\"\n",
    "      metricsQuery: 'avg(<<.Series>>{{<<.LabelMatchers>>}}) by (<<.GroupBy>>)'\n",
    "    \n",
    "    # 模型推理延遲\n",
    "    - seriesQuery: 'nv_inference_request_duration_us{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^nv_inference_request_duration_us\"\n",
    "        as: \"triton_inference_latency\"\n",
    "      metricsQuery: 'avg(rate(<<.Series>>{{<<.LabelMatchers>>}}[2m])) by (<<.GroupBy>>)'\n",
    "    \n",
    "    # 模型記憶體使用量\n",
    "    - seriesQuery: 'nv_gpu_memory_used_bytes{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^nv_gpu_memory_used_bytes\"\n",
    "        as: \"triton_gpu_memory_usage\"\n",
    "      metricsQuery: 'avg(<<.Series>>{{<<.LabelMatchers>>}}) by (<<.GroupBy>>)'\n",
    "    \n",
    "    # 佇列長度\n",
    "    - seriesQuery: 'nv_inference_queue_duration_us{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^nv_inference_queue_duration_us\"\n",
    "        as: \"triton_queue_time\"\n",
    "      metricsQuery: 'avg(rate(<<.Series>>{{<<.LabelMatchers>>}}[2m])) by (<<.GroupBy>>)'\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: {APP_NAME}-metrics\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: monitoring\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: {APP_NAME}\n",
    "  \n",
    "  endpoints:\n",
    "  - port: metrics\n",
    "    path: /metrics\n",
    "    interval: 30s\n",
    "    scrapeTimeout: 10s\n",
    "    honorLabels: true\n",
    "    \n",
    "    # 指標重新標記\n",
    "    metricRelabelings:\n",
    "    - sourceLabels: [__name__]\n",
    "      regex: 'nv_(.*)'\n",
    "      targetLabel: triton_metric\n",
    "      replacement: '${{1}}'\n",
    "    \n",
    "    # 樣本限制\n",
    "    sampleLimit: 10000\n",
    "    \n",
    "  namespaceSelector:\n",
    "    matchNames:\n",
    "    - {NAMESPACE}\n",
    "\"\"\"\n",
    "\n",
    "# 寫入文件\n",
    "with open(f\"{MANIFESTS_DIR}/metrics-config.yaml\", \"w\") as f:\n",
    "    f.write(metrics_config_yaml)\n",
    "\n",
    "print(\"📝 已創建自定義指標配置\")\n",
    "print(f\"   文件位置: {MANIFESTS_DIR}/metrics-config.yaml\")\n",
    "print(\"\\n📊 自定義指標:\")\n",
    "print(\"   - triton_request_rate: 請求速率\")\n",
    "print(\"   - nvidia_gpu_utilization: GPU 使用率\")\n",
    "print(\"   - triton_inference_latency: 推理延遲\")\n",
    "print(\"   - triton_gpu_memory_usage: GPU 記憶體\")\n",
    "print(\"   - triton_queue_time: 佇列等待時間\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 6：部署驗證與測試"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 批量部署所有資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_all_manifests():\n",
    "    \"\"\"批量部署所有 Kubernetes 資源\"\"\"\n",
    "    \n",
    "    # 定義部署順序 (依賴關係)\n",
    "    deployment_order = [\n",
    "        \"namespace.yaml\",\n",
    "        \"rbac.yaml\", \n",
    "        \"storage.yaml\",\n",
    "        \"configmap.yaml\",\n",
    "        \"deployment.yaml\",\n",
    "        \"service.yaml\",\n",
    "        \"ingress.yaml\",\n",
    "        \"autoscaling.yaml\",\n",
    "        \"metrics-config.yaml\"\n",
    "    ]\n",
    "    \n",
    "    deployment_results = []\n",
    "    \n",
    "    print(\"🚀 開始批量部署...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, manifest in enumerate(deployment_order, 1):\n",
    "        manifest_path = f\"{MANIFESTS_DIR}/{manifest}\"\n",
    "        \n",
    "        if not os.path.exists(manifest_path):\n",
    "            print(f\"⚠️  檔案不存在: {manifest}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n[{i}/{len(deployment_order)}] 部署 {manifest}...\")\n",
    "        \n",
    "        success = apply_kubernetes_manifest(manifest_path)\n",
    "        deployment_results.append((manifest, success))\n",
    "        \n",
    "        if success:\n",
    "            print(f\"✅ {manifest} 部署成功\")\n",
    "            # 等待資源創建\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(f\"❌ {manifest} 部署失敗\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"📊 部署結果總結:\")\n",
    "    \n",
    "    success_count = 0\n",
    "    for manifest, success in deployment_results:\n",
    "        status = \"✅ 成功\" if success else \"❌ 失敗\"\n",
    "        print(f\"   {manifest:<25} {status}\")\n",
    "        if success:\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"\\n總計: {success_count}/{len(deployment_results)} 成功\")\n",
    "    \n",
    "    return deployment_results\n",
    "\n",
    "# 執行部署 (如果集群可用)\n",
    "if cluster_ready:\n",
    "    deployment_results = deploy_all_manifests()\n",
    "else:\n",
    "    print(\"⚠️  跳過部署 - Kubernetes 集群不可用\")\n",
    "    print(\"\\n📝 生成的配置文件:\")\n",
    "    for file in os.listdir(MANIFESTS_DIR):\n",
    "        if file.endswith('.yaml'):\n",
    "            print(f\"   {MANIFESTS_DIR}/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 部署狀態檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_deployment_status():\n",
    "    \"\"\"檢查部署狀態\"\"\"\n",
    "    try:\n",
    "        v1 = client.CoreV1Api()\n",
    "        apps_v1 = client.AppsV1Api()\n",
    "        \n",
    "        print(\"🔍 檢查部署狀態...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 檢查命名空間\n",
    "        try:\n",
    "            namespace = v1.read_namespace(name=NAMESPACE)\n",
    "            print(f\"✅ 命名空間: {NAMESPACE} (活躍)\")\n",
    "        except ApiException as e:\n",
    "            print(f\"❌ 命名空間: {NAMESPACE} (不存在)\")\n",
    "            return False\n",
    "        \n",
    "        # 檢查 PVC\n",
    "        print(\"\\n📦 持久化存儲:\")\n",
    "        pvcs = v1.list_namespaced_persistent_volume_claim(namespace=NAMESPACE)\n",
    "        for pvc in pvcs.items:\n",
    "            name = pvc.metadata.name\n",
    "            status = pvc.status.phase\n",
    "            capacity = pvc.status.capacity.get('storage', 'Unknown') if pvc.status.capacity else 'Unknown'\n",
    "            print(f\"   {name}: {status} ({capacity})\")\n",
    "        \n",
    "        # 檢查 Deployment\n",
    "        print(\"\\n🚀 部署狀態:\")\n",
    "        deployments = apps_v1.list_namespaced_deployment(namespace=NAMESPACE)\n",
    "        for deployment in deployments.items:\n",
    "            name = deployment.metadata.name\n",
    "            replicas = deployment.spec.replicas\n",
    "            ready_replicas = deployment.status.ready_replicas or 0\n",
    "            available_replicas = deployment.status.available_replicas or 0\n",
    "            \n",
    "            print(f\"   {name}:\")\n",
    "            print(f\"     期望副本: {replicas}\")\n",
    "            print(f\"     就緒副本: {ready_replicas}\")\n",
    "            print(f\"     可用副本: {available_replicas}\")\n",
    "            \n",
    "            # 檢查部署條件\n",
    "            if deployment.status.conditions:\n",
    "                for condition in deployment.status.conditions:\n",
    "                    if condition.type == \"Available\":\n",
    "                        status_icon = \"✅\" if condition.status == \"True\" else \"❌\"\n",
    "                        print(f\"     {status_icon} 可用性: {condition.status}\")\n",
    "        \n",
    "        # 檢查 Pod\n",
    "        print(\"\\n🏃 Pod 狀態:\")\n",
    "        pods = v1.list_namespaced_pod(namespace=NAMESPACE)\n",
    "        for pod in pods.items:\n",
    "            name = pod.metadata.name\n",
    "            phase = pod.status.phase\n",
    "            node = pod.spec.node_name or \"未調度\"\n",
    "            \n",
    "            # 檢查容器狀態\n",
    "            ready_containers = 0\n",
    "            total_containers = len(pod.spec.containers)\n",
    "            \n",
    "            if pod.status.container_statuses:\n",
    "                ready_containers = sum(1 for status in pod.status.container_statuses if status.ready)\n",
    "            \n",
    "            status_icon = \"✅\" if phase == \"Running\" and ready_containers == total_containers else \"⚠️\"\n",
    "            print(f\"   {status_icon} {name}:\")\n",
    "            print(f\"     階段: {phase}\")\n",
    "            print(f\"     節點: {node}\")\n",
    "            print(f\"     容器: {ready_containers}/{total_containers} 就緒\")\n",
    "        \n",
    "        # 檢查 Service\n",
    "        print(\"\\n🌐 服務狀態:\")\n",
    "        services = v1.list_namespaced_service(namespace=NAMESPACE)\n",
    "        for service in services.items:\n",
    "            name = service.metadata.name\n",
    "            service_type = service.spec.type\n",
    "            cluster_ip = service.spec.cluster_ip\n",
    "            \n",
    "            print(f\"   {name}:\")\n",
    "            print(f\"     類型: {service_type}\")\n",
    "            print(f\"     Cluster IP: {cluster_ip}\")\n",
    "            \n",
    "            if service_type == \"LoadBalancer\":\n",
    "                if service.status.load_balancer.ingress:\n",
    "                    external_ip = service.status.load_balancer.ingress[0].ip\n",
    "                    print(f\"     外部 IP: {external_ip}\")\n",
    "                else:\n",
    "                    print(f\"     外部 IP: <待分配>\")\n",
    "        \n",
    "        # 檢查 HPA\n",
    "        print(\"\\n📈 自動擴縮容:\")\n",
    "        try:\n",
    "            autoscaling_v2 = client.AutoscalingV2Api()\n",
    "            hpas = autoscaling_v2.list_namespaced_horizontal_pod_autoscaler(namespace=NAMESPACE)\n",
    "            for hpa in hpas.items:\n",
    "                name = hpa.metadata.name\n",
    "                current_replicas = hpa.status.current_replicas or 0\n",
    "                desired_replicas = hpa.status.desired_replicas or 0\n",
    "                min_replicas = hpa.spec.min_replicas\n",
    "                max_replicas = hpa.spec.max_replicas\n",
    "                \n",
    "                print(f\"   {name}:\")\n",
    "                print(f\"     當前副本: {current_replicas}\")\n",
    "                print(f\"     目標副本: {desired_replicas}\")\n",
    "                print(f\"     範圍: {min_replicas}-{max_replicas}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  無法檢查 HPA: {e}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 狀態檢查失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "# 檢查部署狀態\n",
    "if cluster_ready:\n",
    "    check_deployment_status()\n",
    "else:\n",
    "    print(\"⚠️  跳過狀態檢查 - Kubernetes 集群不可用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 功能測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_triton_deployment():\n",
    "    \"\"\"測試 Triton 部署功能\"\"\"\n",
    "    print(\"🧪 開始功能測試...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 測試配置\n",
    "    test_results = []\n",
    "    \n",
    "    try:\n",
    "        v1 = client.CoreV1Api()\n",
    "        \n",
    "        # 獲取服務端點\n",
    "        services = v1.list_namespaced_service(namespace=NAMESPACE)\n",
    "        service_endpoint = None\n",
    "        \n",
    "        for service in services.items:\n",
    "            if service.metadata.name == f\"{APP_NAME}-service\":\n",
    "                cluster_ip = service.spec.cluster_ip\n",
    "                http_port = None\n",
    "                \n",
    "                for port in service.spec.ports:\n",
    "                    if port.name == \"http\":\n",
    "                        http_port = port.port\n",
    "                        break\n",
    "                \n",
    "                if cluster_ip and http_port:\n",
    "                    service_endpoint = f\"http://{cluster_ip}:{http_port}\"\n",
    "                break\n",
    "        \n",
    "        if not service_endpoint:\n",
    "            print(\"❌ 無法獲取服務端點\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"🎯 測試端點: {service_endpoint}\")\n",
    "        \n",
    "        # 測試 1: 健康檢查\n",
    "        print(\"\\n1️⃣ 健康檢查測試:\")\n",
    "        try:\n",
    "            response = requests.get(f\"{service_endpoint}/v2/health/live\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                print(\"   ✅ 存活檢查: 通過\")\n",
    "                test_results.append(('health_live', True))\n",
    "            else:\n",
    "                print(f\"   ❌ 存活檢查: 失敗 ({response.status_code})\")\n",
    "                test_results.append(('health_live', False))\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 存活檢查: 連接失敗 ({e})\")\n",
    "            test_results.append(('health_live', False))\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(f\"{service_endpoint}/v2/health/ready\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                print(\"   ✅ 就緒檢查: 通過\")\n",
    "                test_results.append(('health_ready', True))\n",
    "            else:\n",
    "                print(f\"   ❌ 就緒檢查: 失敗 ({response.status_code})\")\n",
    "                test_results.append(('health_ready', False))\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 就緒檢查: 連接失敗 ({e})\")\n",
    "            test_results.append(('health_ready', False))\n",
    "        \n",
    "        # 測試 2: 模型庫檢查\n",
    "        print(\"\\n2️⃣ 模型庫測試:\")\n",
    "        try:\n",
    "            response = requests.get(f\"{service_endpoint}/v2/models\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                models_data = response.json()\n",
    "                model_count = len(models_data.get('models', []))\n",
    "                print(f\"   ✅ 模型庫: 可訪問 ({model_count} 個模型)\")\n",
    "                test_results.append(('models_api', True))\n",
    "                \n",
    "                if model_count > 0:\n",
    "                    print(\"   📋 可用模型:\")\n",
    "                    for model in models_data['models'][:5]:  # 只顯示前5個\n",
    "                        model_name = model.get('name', 'Unknown')\n",
    "                        model_version = model.get('version', 'Unknown')\n",
    "                        print(f\"      - {model_name} (v{model_version})\")\n",
    "                else:\n",
    "                    print(\"   ⚠️  沒有可用的模型\")\n",
    "            else:\n",
    "                print(f\"   ❌ 模型庫: API 錯誤 ({response.status_code})\")\n",
    "                test_results.append(('models_api', False))\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 模型庫: 連接失敗 ({e})\")\n",
    "            test_results.append(('models_api', False))\n",
    "        \n",
    "        # 測試 3: 指標檢查\n",
    "        print(\"\\n3️⃣ 指標測試:\")\n",
    "        try:\n",
    "            # 嘗試獲取指標端點\n",
    "            metrics_endpoint = service_endpoint.replace(':8000', ':8002')\n",
    "            response = requests.get(f\"{metrics_endpoint}/metrics\", timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                metrics_text = response.text\n",
    "                # 檢查關鍵指標\n",
    "                key_metrics = [\n",
    "                    'nv_inference_request_success',\n",
    "                    'nv_inference_request_failure', \n",
    "                    'nv_inference_queue_duration_us',\n",
    "                    'nv_gpu_utilization'\n",
    "                ]\n",
    "                \n",
    "                found_metrics = []\n",
    "                for metric in key_metrics:\n",
    "                    if metric in metrics_text:\n",
    "                        found_metrics.append(metric)\n",
    "                \n",
    "                print(f\"   ✅ 指標端點: 可訪問\")\n",
    "                print(f\"   📊 找到指標: {len(found_metrics)}/{len(key_metrics)}\")\n",
    "                test_results.append(('metrics_api', True))\n",
    "                \n",
    "                if found_metrics:\n",
    "                    print(\"   📈 可用指標:\")\n",
    "                    for metric in found_metrics:\n",
    "                        print(f\"      - {metric}\")\n",
    "            else:\n",
    "                print(f\"   ❌ 指標端點: HTTP 錯誤 ({response.status_code})\")\n",
    "                test_results.append(('metrics_api', False))\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 指標端點: 連接失敗 ({e})\")\n",
    "            test_results.append(('metrics_api', False))\n",
    "        \n",
    "        # 測試 4: 負載均衡測試\n",
    "        print(\"\\n4️⃣ 負載均衡測試:\")\n",
    "        try:\n",
    "            # 多次請求檢查負載分配\n",
    "            server_ids = set()\n",
    "            success_count = 0\n",
    "            \n",
    "            for i in range(5):\n",
    "                response = requests.get(f\"{service_endpoint}/v2/health/live\", timeout=5)\n",
    "                if response.status_code == 200:\n",
    "                    success_count += 1\n",
    "                    # 嘗試從響應標頭獲取服務器資訊\n",
    "                    server_id = response.headers.get('Server', f\"unknown-{i}\")\n",
    "                    server_ids.add(server_id)\n",
    "            \n",
    "            print(f\"   ✅ 負載均衡: {success_count}/5 請求成功\")\n",
    "            print(f\"   🔄 服務器實例: {len(server_ids)} 個\")\n",
    "            test_results.append(('load_balancing', success_count >= 4))\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 負載均衡: 測試失敗 ({e})\")\n",
    "            test_results.append(('load_balancing', False))\n",
    "        \n",
    "        # 總結測試結果\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"📊 測試結果總結:\")\n",
    "        \n",
    "        success_count = 0\n",
    "        for test_name, success in test_results:\n",
    "            status = \"✅ 通過\" if success else \"❌ 失敗\"\n",
    "            test_display_names = {\n",
    "                'health_live': '存活檢查',\n",
    "                'health_ready': '就緒檢查', \n",
    "                'models_api': '模型庫 API',\n",
    "                'metrics_api': '指標端點',\n",
    "                'load_balancing': '負載均衡'\n",
    "            }\n",
    "            display_name = test_display_names.get(test_name, test_name)\n",
    "            print(f\"   {display_name:<15} {status}\")\n",
    "            if success:\n",
    "                success_count += 1\n",
    "        \n",
    "        overall_success = success_count >= len(test_results) * 0.8  # 80% 通過率\n",
    "        overall_status = \"✅ 整體通過\" if overall_success else \"❌ 整體失敗\"\n",
    "        print(f\"\\n{overall_status} ({success_count}/{len(test_results)})\")\n",
    "        \n",
    "        return overall_success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 測試執行失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "# 執行功能測試\n",
    "if cluster_ready:\n",
    "    test_success = test_triton_deployment()\n",
    "else:\n",
    "    print(\"⚠️  跳過功能測試 - Kubernetes 集群不可用\")\n",
    "    test_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 實驗總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成部署報告\n",
    "def generate_deployment_report():\n",
    "    \"\"\"生成部署報告\"\"\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# Kubernetes 部署報告\n",
    "\n",
    "## 📋 部署概要\n",
    "\n",
    "**時間**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**命名空間**: {NAMESPACE}\n",
    "**應用**: {APP_NAME}\n",
    "**映像**: {IMAGE}\n",
    "\n",
    "## 🏗️ 資源配置\n",
    "\n",
    "### 計算資源\n",
    "- **CPU**: 2-4 cores per pod\n",
    "- **記憶體**: 8-16Gi per pod\n",
    "- **GPU**: 1x NVIDIA GPU per pod\n",
    "- **副本數**: 2 (min) - 10 (max)\n",
    "\n",
    "### 存儲資源\n",
    "- **模型存儲**: 100Gi (ReadOnlyMany)\n",
    "- **快取存儲**: 50Gi (ReadWriteMany)\n",
    "- **共享記憶體**: 1Gi per pod\n",
    "\n",
    "### 網路配置\n",
    "- **HTTP 端口**: 8000\n",
    "- **gRPC 端口**: 8001 \n",
    "- **指標端口**: 8002\n",
    "- **TLS**: 支援 (Let's Encrypt)\n",
    "\n",
    "## 🔧 功能特性\n",
    "\n",
    "### 高可用性\n",
    "- ✅ 多副本部署 (反親和性)\n",
    "- ✅ 滾動更新 (零停機)\n",
    "- ✅ 健康檢查 (存活/就緒/啟動)\n",
    "- ✅ Pod 中斷預算 (50% 最小可用)\n",
    "\n",
    "### 自動擴縮容\n",
    "- ✅ HPA (CPU/Memory/GPU/RPS)\n",
    "- ✅ VPA 建議模式\n",
    "- ✅ 自定義指標支援\n",
    "- ✅ 智能擴縮策略\n",
    "\n",
    "### 安全性\n",
    "- ✅ RBAC 最小權限\n",
    "- ✅ 網路政策\n",
    "- ✅ 非 root 用戶\n",
    "- ✅ 安全上下文\n",
    "\n",
    "### 監控與可觀測性\n",
    "- ✅ Prometheus 指標\n",
    "- ✅ 自定義指標\n",
    "- ✅ ServiceMonitor\n",
    "- ✅ 指標重新標記\n",
    "\n",
    "## 📁 生成的配置文件\n",
    "\"\"\"\n",
    "    \n",
    "    # 列出所有生成的文件\n",
    "    manifest_files = []\n",
    "    for file in os.listdir(MANIFESTS_DIR):\n",
    "        if file.endswith('.yaml'):\n",
    "            file_path = f\"{MANIFESTS_DIR}/{file}\"\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            manifest_files.append((file, file_size))\n",
    "    \n",
    "    for file, size in sorted(manifest_files):\n",
    "        report += f\"\\n- `{file}` ({size:,} bytes)\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "## 🚀 部署命令\n",
    "\n",
    "```bash\n",
    "# 一鍵部署所有資源\n",
    "kubectl apply -f {MANIFESTS_DIR}/\n",
    "\n",
    "# 檢查部署狀態\n",
    "kubectl get all -n {NAMESPACE}\n",
    "\n",
    "# 查看 Pod 日誌\n",
    "kubectl logs -f deployment/{APP_NAME} -n {NAMESPACE}\n",
    "\n",
    "# 端口轉發 (測試用)\n",
    "kubectl port-forward svc/{APP_NAME}-service 8000:8000 -n {NAMESPACE}\n",
    "```\n",
    "\n",
    "## 🧪 驗證步驟\n",
    "\n",
    "```bash\n",
    "# 健康檢查\n",
    "curl http://localhost:8000/v2/health/live\n",
    "curl http://localhost:8000/v2/health/ready\n",
    "\n",
    "# 模型列表\n",
    "curl http://localhost:8000/v2/models\n",
    "\n",
    "# Prometheus 指標\n",
    "curl http://localhost:8002/metrics\n",
    "```\n",
    "\n",
    "## 📚 下一步\n",
    "\n",
    "1. **模型部署**: 上傳模型到 PV 並配置 config.pbtxt\n",
    "2. **監控設置**: 配置 Prometheus 和 Grafana\n",
    "3. **CI/CD 整合**: 設置自動化部署流程\n",
    "4. **壓力測試**: 驗證自動擴縮容功能\n",
    "5. **災難恢復**: 測試故障轉移機制\n",
    "\n",
    "---\n",
    "**生成時間**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**實驗室**: Lab 2.5.1 - Kubernetes 原生部署\n",
    "\"\"\"\n",
    "    \n",
    "    # 保存報告\n",
    "    report_path = f\"{K8S_DIR}/deployment-report.md\"\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"📊 部署報告已生成\")\n",
    "    print(f\"   文件位置: {report_path}\")\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "# 生成報告\n",
    "report_path = generate_deployment_report()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 Lab 2.5.1 實驗完成！\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✅ 完成項目:\")\n",
    "print(\"   - Kubernetes 集群環境準備\")\n",
    "print(\"   - GPU 資源調度配置\")\n",
    "print(\"   - 高可用 Triton 部署\")\n",
    "print(\"   - 服務發現與負載均衡\")\n",
    "print(\"   - 自動擴縮容機制\")\n",
    "print(\"   - 安全與網路政策\")\n",
    "print(\"   - 監控指標整合\")\n",
    "print(\"\\n📁 輸出文件:\")\n",
    "print(f\"   - 配置文件: {MANIFESTS_DIR}/\")\n",
    "print(f\"   - 部署報告: {report_path}\")\n",
    "print(\"\\n🚀 準備進入下一階段: CI/CD 與 MLOps 整合\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}