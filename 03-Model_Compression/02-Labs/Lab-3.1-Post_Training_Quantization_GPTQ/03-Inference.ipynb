{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1: GPTQ Post-Training Quantization - Inference and Comparison\n",
    "\n",
    "**Goal:** Compare the quantized model's output quality and performance against the FP16 baseline.\n",
    "\n",
    "**Metrics:**\n",
    "- **Output Quality**: Side-by-side text generation comparison\n",
    "- **Latency**: Single inference time (milliseconds)\n",
    "- **Throughput**: Tokens generated per second\n",
    "- **Memory**: GPU memory usage (GB)\n",
    "- **Perplexity**: Language modeling performance on test set\n",
    "\n",
    "**Expected results**:\n",
    "- Latency: ~2.8x faster than FP16\n",
    "- Memory: ~3x reduction\n",
    "- Perplexity: +0.1-0.2 increase (minimal)\n",
    "- Output quality: Visually indistinguishable for most tasks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Both Models\n",
    "\n",
    "We'll load both the FP16 baseline and INT4 quantized models for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Model paths\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "QUANTIZED_MODEL_PATH = \"./llama-2-7b-gptq-4bit\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Model Comparison: FP16 vs GPTQ INT4\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Load tokenizer (same for both)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"‚úÖ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FP16 baseline model\n",
    "print(\"\\nüì• Loading FP16 baseline model...\")\n",
    "print(\"   (This requires ~15GB GPU memory)\")\n",
    "\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "memory_fp16 = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"‚úÖ FP16 model loaded\")\n",
    "print(f\"   Memory: {memory_fp16:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory and load quantized model\n",
    "del model_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nüì• Loading GPTQ quantized model...\")\n",
    "model_gptq = AutoModelForCausalLM.from_pretrained(\n",
    "    QUANTIZED_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "memory_gptq = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"‚úÖ GPTQ model loaded\")\n",
    "print(f\"   Memory: {memory_gptq:.2f} GB\")\n",
    "print(f\"   Memory reduction: {memory_fp16 / memory_gptq:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Define Test Prompts\n",
    "\n",
    "We'll use diverse prompts to test different capabilities:\n",
    "- Creative writing\n",
    "- Factual knowledge\n",
    "- Reasoning\n",
    "- Code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    {\n",
    "        \"name\": \"Creative Writing\",\n",
    "        \"prompt\": \"Once upon a time in a land of endless possibilities,\",\n",
    "        \"max_tokens\": 80\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Factual Knowledge\",\n",
    "        \"prompt\": \"Explain the theory of relativity in simple terms:\",\n",
    "        \"max_tokens\": 100\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Reasoning\",\n",
    "        \"prompt\": \"If a train leaves Station A at 3 PM traveling at 60 mph, and another train leaves Station B at 4 PM traveling at 80 mph toward Station A, which is 300 miles away, when will they meet? Solution:\",\n",
    "        \"max_tokens\": 150\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Generation\",\n",
    "        \"prompt\": \"Write a Python function to calculate the Fibonacci sequence:\\n\\n```python\\n\",\n",
    "        \"max_tokens\": 120\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Conversation\",\n",
    "        \"prompt\": \"Human: What are the benefits of artificial intelligence?\\nAssistant:\",\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(test_prompts)} test prompts\")\n",
    "for i, test in enumerate(test_prompts, 1):\n",
    "    print(f\"   {i}. {test['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Side-by-Side Output Comparison\n",
    "\n",
    "Let's generate outputs from both models and compare them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_model(model, prompt, max_new_tokens=100, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text using the given model\n",
    "    \n",
    "    Returns: (generated_text, latency_ms, tokens_per_sec)\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Warmup (first inference is slower)\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=10)\n",
    "    \n",
    "    # Actual generation with timing\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Metrics\n",
    "    latency_ms = (end_time - start_time) * 1000\n",
    "    num_tokens = len(outputs[0]) - len(inputs['input_ids'][0])\n",
    "    tokens_per_sec = num_tokens / (end_time - start_time)\n",
    "    \n",
    "    return generated_text, latency_ms, tokens_per_sec\n",
    "\n",
    "print(\"‚úÖ Generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload FP16 model for comparison\n",
    "print(\"üì• Loading FP16 model for comparison...\")\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"‚úÖ FP16 model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison for all test prompts\n",
    "results = []\n",
    "\n",
    "for test in test_prompts:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Test: {test['name']}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Prompt: {test['prompt'][:100]}...\" if len(test['prompt']) > 100 else f\"Prompt: {test['prompt']}\")\n",
    "    print()\n",
    "    \n",
    "    # Generate with FP16\n",
    "    print(\"üìù FP16 Model:\")\n",
    "    text_fp16, latency_fp16, tps_fp16 = generate_with_model(\n",
    "        model_fp16, test['prompt'], test['max_tokens']\n",
    "    )\n",
    "    print(f\"Output: {text_fp16}\")\n",
    "    print(f\"‚è±Ô∏è  Latency: {latency_fp16:.2f} ms | Tokens/s: {tps_fp16:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Generate with GPTQ\n",
    "    print(\"üìù GPTQ Model:\")\n",
    "    text_gptq, latency_gptq, tps_gptq = generate_with_model(\n",
    "        model_gptq, test['prompt'], test['max_tokens']\n",
    "    )\n",
    "    print(f\"Output: {text_gptq}\")\n",
    "    print(f\"‚è±Ô∏è  Latency: {latency_gptq:.2f} ms | Tokens/s: {tps_gptq:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Speedup\n",
    "    speedup = latency_fp16 / latency_gptq\n",
    "    print(f\"üöÄ Speedup: {speedup:.2f}x (GPTQ is {speedup:.2f}x faster)\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"test\": test['name'],\n",
    "        \"latency_fp16\": latency_fp16,\n",
    "        \"latency_gptq\": latency_gptq,\n",
    "        \"tps_fp16\": tps_fp16,\n",
    "        \"tps_gptq\": tps_gptq,\n",
    "        \"speedup\": speedup\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ All comparisons complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Performance Summary\n",
    "\n",
    "Let's aggregate the results and show overall performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create results dataframe\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Calculate statistics\n",
    "avg_speedup = df_results['speedup'].mean()\n",
    "avg_tps_fp16 = df_results['tps_fp16'].mean()\n",
    "avg_tps_gptq = df_results['tps_gptq'].mean()\n",
    "avg_latency_fp16 = df_results['latency_fp16'].mean()\n",
    "avg_latency_gptq = df_results['latency_gptq'].mean()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Performance Summary\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"üìä Average Metrics Across All Tests:\")\n",
    "print(f\"   FP16 Latency:  {avg_latency_fp16:.2f} ms\")\n",
    "print(f\"   GPTQ Latency:  {avg_latency_gptq:.2f} ms\")\n",
    "print(f\"   Speedup:       {avg_speedup:.2f}x\")\n",
    "print()\n",
    "print(f\"   FP16 Throughput:  {avg_tps_fp16:.2f} tokens/s\")\n",
    "print(f\"   GPTQ Throughput:  {avg_tps_gptq:.2f} tokens/s\")\n",
    "print(f\"   Throughput Gain:  {avg_tps_gptq / avg_tps_fp16:.2f}x\")\n",
    "print()\n",
    "print(\"üíæ Memory Usage:\")\n",
    "print(f\"   FP16: {memory_fp16:.2f} GB\")\n",
    "print(f\"   GPTQ: {memory_gptq:.2f} GB\")\n",
    "print(f\"   Memory Reduction: {memory_fp16 / memory_gptq:.2f}x\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed per-test results\n",
    "print(\"\\nüìã Detailed Results by Test:\")\n",
    "print()\n",
    "\n",
    "# Format table\n",
    "df_display = df_results[['test', 'latency_fp16', 'latency_gptq', 'speedup', 'tps_gptq']].copy()\n",
    "df_display.columns = ['Test', 'FP16 Latency (ms)', 'GPTQ Latency (ms)', 'Speedup', 'GPTQ Tokens/s']\n",
    "df_display = df_display.round(2)\n",
    "\n",
    "print(df_display.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Perplexity Evaluation (Optional)\n",
    "\n",
    "Perplexity is a standard metric for language modeling quality. Lower is better.\n",
    "\n",
    "**Note**: This requires loading a test dataset and is computationally expensive (~5-10 minutes).\n",
    "Skip this cell if you want to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate perplexity on WikiText-2\n",
    "# Uncomment to run (takes 5-10 minutes)\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def calculate_perplexity(model, tokenizer, dataset_name=\"wikitext\", dataset_config=\"wikitext-2-raw-v1\", max_samples=100):\n",
    "#     \"\"\"\n",
    "#     Calculate perplexity on a test dataset\n",
    "#     \"\"\"\n",
    "#     # Load dataset\n",
    "#     dataset = load_dataset(dataset_name, dataset_config, split=\"test\")\n",
    "#     \n",
    "#     # Filter out empty texts\n",
    "#     dataset = dataset.filter(lambda x: len(x['text'].strip()) > 0)\n",
    "#     \n",
    "#     # Limit samples\n",
    "#     dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "#     \n",
    "#     total_loss = 0\n",
    "#     total_tokens = 0\n",
    "#     \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for example in tqdm(dataset, desc=\"Calculating PPL\"):\n",
    "#             inputs = tokenizer(example['text'], return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "#             outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "#             total_loss += outputs.loss.item() * inputs['input_ids'].size(1)\n",
    "#             total_tokens += inputs['input_ids'].size(1)\n",
    "#     \n",
    "#     avg_loss = total_loss / total_tokens\n",
    "#     perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "#     \n",
    "#     return perplexity\n",
    "\n",
    "# print(\"\\nüìä Evaluating Perplexity (this may take 5-10 minutes)...\")\n",
    "# ppl_fp16 = calculate_perplexity(model_fp16, tokenizer, max_samples=100)\n",
    "# ppl_gptq = calculate_perplexity(model_gptq, tokenizer, max_samples=100)\n",
    "\n",
    "# print(f\"\\n‚úÖ Perplexity Results:\")\n",
    "# print(f\"   FP16: {ppl_fp16:.2f}\")\n",
    "# print(f\"   GPTQ: {ppl_gptq:.2f}\")\n",
    "# print(f\"   Difference: +{ppl_gptq - ppl_fp16:.2f} ({(ppl_gptq - ppl_fp16) / ppl_fp16 * 100:.2f}%)\")\n",
    "\n",
    "print(\"‚è≠Ô∏è  Perplexity evaluation skipped (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Quality Assessment\n",
    "\n",
    "Let's perform a qualitative assessment of output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Qualitative Assessment\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"üîç Output Quality Analysis:\")\n",
    "print()\n",
    "print(\"‚úÖ Expected Observations:\")\n",
    "print(\"   1. Outputs should be coherent and grammatically correct\")\n",
    "print(\"   2. Minor variations are normal (due to quantization noise)\")\n",
    "print(\"   3. No repetitive text or gibberish\")\n",
    "print(\"   4. Factual knowledge should be preserved\")\n",
    "print(\"   5. Reasoning capabilities should be largely intact\")\n",
    "print()\n",
    "print(\"‚ö†Ô∏è  Potential Issues to Watch For:\")\n",
    "print(\"   - Repetitive phrases (indicates quantization degradation)\")\n",
    "print(\"   - Nonsensical outputs (rare with 4-bit GPTQ)\")\n",
    "print(\"   - Hallucinations (should be similar to FP16)\")\n",
    "print()\n",
    "print(\"üìã Assessment:\")\n",
    "print(\"   Based on the side-by-side comparisons above,\")\n",
    "print(\"   the GPTQ quantized model should produce outputs\")\n",
    "print(\"   that are visually indistinguishable from FP16\")\n",
    "print(\"   for most practical applications.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Final Comparison Table\n",
    "\n",
    "Comprehensive comparison of all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "comparison_data = {\n",
    "    \"Metric\": [\n",
    "        \"Model Size (GB)\",\n",
    "        \"GPU Memory (GB)\",\n",
    "        \"Avg Latency (ms)\",\n",
    "        \"Avg Throughput (tok/s)\",\n",
    "        \"Output Quality\",\n",
    "        \"Perplexity\"\n",
    "    ],\n",
    "    \"FP16 Baseline\": [\n",
    "        \"13.5\",\n",
    "        f\"{memory_fp16:.1f}\",\n",
    "        f\"{avg_latency_fp16:.2f}\",\n",
    "        f\"{avg_tps_fp16:.2f}\",\n",
    "        \"Reference\",\n",
    "        \"~5.68 (typical)\"\n",
    "    ],\n",
    "    \"GPTQ INT4\": [\n",
    "        \"3.5\",\n",
    "        f\"{memory_gptq:.1f}\",\n",
    "        f\"{avg_latency_gptq:.2f}\",\n",
    "        f\"{avg_tps_gptq:.2f}\",\n",
    "        \"Comparable\",\n",
    "        \"~5.85 (est.)\"\n",
    "    ],\n",
    "    \"Ratio/Diff\": [\n",
    "        \"3.86x smaller\",\n",
    "        f\"{memory_fp16 / memory_gptq:.2f}x less\",\n",
    "        f\"{avg_speedup:.2f}x faster\",\n",
    "        f\"{avg_tps_gptq / avg_tps_fp16:.2f}x higher\",\n",
    "        \"~Equal\",\n",
    "        \"+0.17 (+3%)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Final Comparison: FP16 vs GPTQ INT4\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(df_comparison.to_string(index=False))\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ GPTQ quantization achieves excellent compression-performance tradeoff!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Key Findings\n",
    "\n",
    "**Performance Gains**:\n",
    "- ‚úÖ **Inference Speed**: 2-3x faster than FP16\n",
    "- ‚úÖ **Memory Usage**: 3x reduction (15GB ‚Üí 5GB)\n",
    "- ‚úÖ **Model Size**: 3.86x smaller (13.5GB ‚Üí 3.5GB)\n",
    "- ‚úÖ **Throughput**: 2-3x more tokens per second\n",
    "\n",
    "**Quality Preservation**:\n",
    "- ‚úÖ **Output Quality**: Visually indistinguishable from FP16\n",
    "- ‚úÖ **Perplexity**: Minimal increase (<0.2 points)\n",
    "- ‚úÖ **Coherence**: No repetition or gibberish\n",
    "- ‚úÖ **Factual Accuracy**: Knowledge retained\n",
    "\n",
    "**Why GPTQ Works So Well**:\n",
    "1. **Hessian-guided quantization**: Protects sensitive weights\n",
    "2. **Error compensation**: Propagates errors to minimize accumulation\n",
    "3. **Group quantization**: Balances precision and compression\n",
    "4. **Activation ordering**: Improves grouping effectiveness\n",
    "\n",
    "**Trade-offs**:\n",
    "- ‚ö†Ô∏è **Quantization time**: 20-40 minutes (one-time cost)\n",
    "- ‚ö†Ô∏è **Calibration data**: Requires representative dataset\n",
    "- ‚ö†Ô∏è **Hardware support**: ExLlama requires Ampere+ GPUs\n",
    "\n",
    "**When to Use GPTQ**:\n",
    "- ‚úÖ Production deployment (cost reduction)\n",
    "- ‚úÖ Edge devices (limited memory)\n",
    "- ‚úÖ High-throughput services (batch inference)\n",
    "- ‚ö†Ô∏è Not for ultra-sensitive tasks (medical diagnosis)\n",
    "\n",
    "---\n",
    "\n",
    "**‚è≠Ô∏è Continue to**: [04-Benchmark.ipynb](./04-Benchmark.ipynb) for detailed performance analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
