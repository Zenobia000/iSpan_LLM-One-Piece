{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2: Wanda Pruning - Inference Comparison\n",
    "\n",
    "**Goal:** Compare dense vs sparse model inference quality and performance.\n",
    "\n",
    "**You will learn to:**\n",
    "- Load both dense and pruned sparse models\n",
    "- Compare text generation quality side-by-side\n",
    "- Measure latency and throughput differences\n",
    "- Evaluate generation quality across diverse tasks\n",
    "- Understand sparsity-quality tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "## Why Compare Dense vs Sparse?\n",
    "\n",
    "**Key questions to answer**:\n",
    "1. **Quality**: How much precision is lost due to 50% pruning?\n",
    "2. **Performance**: Is sparse inference faster? (Depends on hardware support)\n",
    "3. **Usability**: Can sparse model handle diverse tasks?\n",
    "\n",
    "**Expected outcomes** (50% Wanda pruning):\n",
    "- **Perplexity**: +7.7% (WikiText-2: 5.68 ‚Üí 6.12)\n",
    "- **Quality**: Minor degradation, mostly coherent\n",
    "- **Speed**: Similar (without sparse acceleration)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have completed:\n",
    "- **01-Setup.ipynb**: Environment setup\n",
    "- **02-Prune.ipynb**: Applied Wanda pruning and saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Pruned Sparse Model\n",
    "\n",
    "Load the pruned model from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "PRUNED_MODEL_DIR = \"./pruned_model\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Pruned Sparse Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if pruned model exists\n",
    "if not os.path.exists(PRUNED_MODEL_DIR):\n",
    "    print(f\"‚ùå Pruned model not found at {PRUNED_MODEL_DIR}\")\n",
    "    print(\"   Please run 02-Prune.ipynb first!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found pruned model directory\\n\")\n",
    "    \n",
    "    # Load pruning configuration\n",
    "    config_path = os.path.join(PRUNED_MODEL_DIR, \"pruning_config.json\")\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            pruning_config = json.load(f)\n",
    "        \n",
    "        print(\"üìä Pruning Configuration:\")\n",
    "        print(f\"   Method: {pruning_config['method']}\")\n",
    "        print(f\"   Target sparsity: {pruning_config['target_sparsity']:.1%}\")\n",
    "        print(f\"   Achieved sparsity: {pruning_config['achieved_sparsity']:.2%}\")\n",
    "        print(f\"   Pruned layers: {pruning_config['pruned_layers']}\")\n",
    "        print()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"‚è≥ Loading tokenizer...\")\n",
    "    sparse_tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_DIR)\n",
    "    sparse_tokenizer.pad_token = sparse_tokenizer.eos_token\n",
    "    print(\"‚úÖ Tokenizer loaded\")\n",
    "    \n",
    "    # Load model\n",
    "    print(\"‚è≥ Loading sparse model (may take 1-2 minutes)...\")\n",
    "    sparse_model = AutoModelForCausalLM.from_pretrained(\n",
    "        PRUNED_MODEL_DIR,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"‚úÖ Sparse model loaded\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"\\nüñ•Ô∏è  GPU Memory: {memory_allocated:.2f} GB\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load Dense Baseline Model (for comparison)\n",
    "\n",
    "Load the original dense model to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "# Alternative: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Dense Baseline Model\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"‚è≥ Loading tokenizer...\")\n",
    "dense_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "dense_tokenizer.pad_token = dense_tokenizer.eos_token\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load model\n",
    "print(\"‚è≥ Loading dense model (may take 1-2 minutes)...\")\n",
    "dense_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"‚úÖ Dense model loaded\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\nüñ•Ô∏è  GPU Memory: {memory_allocated:.2f} GB\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Define Test Prompts\n",
    "\n",
    "Create diverse test prompts to evaluate different capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts covering different tasks\n",
    "test_prompts = [\n",
    "    {\n",
    "        \"name\": \"Creative Writing\",\n",
    "        \"prompt\": \"Write a short story about a robot learning to paint:\",\n",
    "        \"max_tokens\": 100\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Factual Knowledge\",\n",
    "        \"prompt\": \"Explain the process of photosynthesis in plants:\",\n",
    "        \"max_tokens\": 80\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Reasoning\",\n",
    "        \"prompt\": \"If all roses are flowers, and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "        \"max_tokens\": 60\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Generation\",\n",
    "        \"prompt\": \"Write a Python function to calculate the Fibonacci sequence:\",\n",
    "        \"max_tokens\": 80\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Conversation\",\n",
    "        \"prompt\": \"What are the benefits of regular exercise?\",\n",
    "        \"max_tokens\": 80\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test Prompts Defined\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal test cases: {len(test_prompts)}\\n\")\n",
    "for i, test in enumerate(test_prompts, 1):\n",
    "    print(f\"{i}. {test['name']}\")\n",
    "    print(f\"   Prompt: {test['prompt'][:60]}...\")\n",
    "    print(f\"   Max tokens: {test['max_tokens']}\\n\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Generate Outputs (Dense vs Sparse)\n",
    "\n",
    "Run both models on all test prompts and collect outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generation configuration\n",
    "generation_config = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Running Inference Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚è≥ Generating outputs...\\n\")\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for i, test in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Case {i}: {test['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Prompt: {test['prompt']}\\n\")\n",
    "    \n",
    "    result = {\n",
    "        \"name\": test['name'],\n",
    "        \"prompt\": test['prompt']\n",
    "    }\n",
    "    \n",
    "    # --- Dense Model ---\n",
    "    print(\"[Dense Model]\")\n",
    "    inputs = dense_tokenizer(test['prompt'], return_tensors=\"pt\").to(dense_model.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = dense_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=test['max_tokens'],\n",
    "            pad_token_id=dense_tokenizer.eos_token_id,\n",
    "            **generation_config\n",
    "        )\n",
    "    dense_latency = time.time() - start_time\n",
    "    \n",
    "    dense_output = dense_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    dense_tokens = len(outputs[0])\n",
    "    \n",
    "    print(f\"Output: {dense_output}\")\n",
    "    print(f\"‚è±Ô∏è  Latency: {dense_latency:.2f}s | Tokens: {dense_tokens} | Speed: {dense_tokens/dense_latency:.2f} tok/s\\n\")\n",
    "    \n",
    "    result[\"dense_output\"] = dense_output\n",
    "    result[\"dense_latency\"] = dense_latency\n",
    "    result[\"dense_tokens\"] = dense_tokens\n",
    "    \n",
    "    # --- Sparse Model ---\n",
    "    print(\"[Sparse Model (50% pruned)]\")\n",
    "    inputs = sparse_tokenizer(test['prompt'], return_tensors=\"pt\").to(sparse_model.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = sparse_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=test['max_tokens'],\n",
    "            pad_token_id=sparse_tokenizer.eos_token_id,\n",
    "            **generation_config\n",
    "        )\n",
    "    sparse_latency = time.time() - start_time\n",
    "    \n",
    "    sparse_output = sparse_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    sparse_tokens = len(outputs[0])\n",
    "    \n",
    "    print(f\"Output: {sparse_output}\")\n",
    "    print(f\"‚è±Ô∏è  Latency: {sparse_latency:.2f}s | Tokens: {sparse_tokens} | Speed: {sparse_tokens/sparse_latency:.2f} tok/s\\n\")\n",
    "    \n",
    "    result[\"sparse_output\"] = sparse_output\n",
    "    result[\"sparse_latency\"] = sparse_latency\n",
    "    result[\"sparse_tokens\"] = sparse_tokens\n",
    "    \n",
    "    # Performance comparison\n",
    "    speedup = dense_latency / sparse_latency\n",
    "    result[\"speedup\"] = speedup\n",
    "    \n",
    "    print(f\"üìä Comparison:\")\n",
    "    print(f\"   Speedup: {speedup:.2f}x {'(sparse faster)' if speedup > 1 else '(dense faster)'}\")\n",
    "    print(f\"   Latency difference: {abs(dense_latency - sparse_latency):.2f}s\")\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ All test cases completed!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Side-by-Side Output Comparison\n",
    "\n",
    "Display outputs in a readable comparison format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SIDE-BY-SIDE OUTPUT COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case {i}: {result['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüìù Prompt:\\n{result['prompt']}\\n\")\n",
    "    \n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(\"üü¢ DENSE MODEL OUTPUT:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(result['dense_output'])\n",
    "    print(f\"\\n‚è±Ô∏è  {result['dense_latency']:.2f}s | {result['dense_tokens']} tokens | {result['dense_tokens']/result['dense_latency']:.2f} tok/s\")\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(\"üîµ SPARSE MODEL OUTPUT (50% pruned):\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(result['sparse_output'])\n",
    "    print(f\"\\n‚è±Ô∏è  {result['sparse_latency']:.2f}s | {result['sparse_tokens']} tokens | {result['sparse_tokens']/result['sparse_latency']:.2f} tok/s\")\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(\"üìä COMPARISON:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    print(f\"Speedup: {result['speedup']:.2f}x {'‚úÖ (sparse faster)' if result['speedup'] > 1 else '‚ö†Ô∏è  (dense faster)'}\")\n",
    "    print(f\"Latency diff: {abs(result['dense_latency'] - result['sparse_latency']):.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Performance Summary\n",
    "\n",
    "Aggregate statistics across all test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate aggregate statistics\n",
    "dense_latencies = [r['dense_latency'] for r in results]\n",
    "sparse_latencies = [r['sparse_latency'] for r in results]\n",
    "speedups = [r['speedup'] for r in results]\n",
    "\n",
    "dense_tokens = [r['dense_tokens'] for r in results]\n",
    "sparse_tokens = [r['sparse_tokens'] for r in results]\n",
    "\n",
    "dense_throughputs = [t/l for t, l in zip(dense_tokens, dense_latencies)]\n",
    "sparse_throughputs = [t/l for t, l in zip(sparse_tokens, sparse_latencies)]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä Latency Statistics:\")\n",
    "print(f\"   Dense Model:\")\n",
    "print(f\"      Mean: {np.mean(dense_latencies):.2f}s\")\n",
    "print(f\"      Std:  {np.std(dense_latencies):.2f}s\")\n",
    "print(f\"      Min:  {np.min(dense_latencies):.2f}s\")\n",
    "print(f\"      Max:  {np.max(dense_latencies):.2f}s\")\n",
    "\n",
    "print(f\"\\n   Sparse Model (50% pruned):\")\n",
    "print(f\"      Mean: {np.mean(sparse_latencies):.2f}s\")\n",
    "print(f\"      Std:  {np.std(sparse_latencies):.2f}s\")\n",
    "print(f\"      Min:  {np.min(sparse_latencies):.2f}s\")\n",
    "print(f\"      Max:  {np.max(sparse_latencies):.2f}s\")\n",
    "\n",
    "print(f\"\\nüìä Throughput Statistics:\")\n",
    "print(f\"   Dense Model:\")\n",
    "print(f\"      Mean: {np.mean(dense_throughputs):.2f} tok/s\")\n",
    "print(f\"      Std:  {np.std(dense_throughputs):.2f} tok/s\")\n",
    "\n",
    "print(f\"\\n   Sparse Model (50% pruned):\")\n",
    "print(f\"      Mean: {np.mean(sparse_throughputs):.2f} tok/s\")\n",
    "print(f\"      Std:  {np.std(sparse_throughputs):.2f} tok/s\")\n",
    "\n",
    "print(f\"\\nüìä Speedup Statistics:\")\n",
    "print(f\"   Mean speedup: {np.mean(speedups):.2f}x\")\n",
    "print(f\"   Std:          {np.std(speedups):.2f}x\")\n",
    "print(f\"   Min speedup:  {np.min(speedups):.2f}x\")\n",
    "print(f\"   Max speedup:  {np.max(speedups):.2f}x\")\n",
    "\n",
    "avg_speedup = np.mean(speedups)\n",
    "if avg_speedup > 1.1:\n",
    "    print(f\"\\n‚úÖ Sparse model is {avg_speedup:.2f}x faster on average!\")\n",
    "elif avg_speedup < 0.9:\n",
    "    print(f\"\\n‚ö†Ô∏è  Dense model is {1/avg_speedup:.2f}x faster on average\")\n",
    "    print(\"   (Expected: Sparse needs hardware acceleration for speedup)\")\n",
    "else:\n",
    "    print(f\"\\n‚öñÔ∏è  Performance is similar (speedup: {avg_speedup:.2f}x)\")\n",
    "    print(\"   (Expected: Without sparse acceleration, performance is similar)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Quality Assessment\n",
    "\n",
    "Subjective quality evaluation across test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã Evaluation Criteria:\")\n",
    "print(\"   ‚úÖ Excellent: No noticeable degradation\")\n",
    "print(\"   üü° Good:      Minor quality loss, acceptable\")\n",
    "print(\"   ‚ö†Ô∏è  Fair:      Noticeable degradation, needs improvement\")\n",
    "print(\"   ‚ùå Poor:      Significant quality loss\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Manual Quality Comparison:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {result['name']}:\")\n",
    "    print(f\"   Prompt length: {len(result['prompt'])} chars\")\n",
    "    print(f\"   Dense output: {len(result['dense_output'])} chars\")\n",
    "    print(f\"   Sparse output: {len(result['sparse_output'])} chars\")\n",
    "    print(f\"   Output length ratio: {len(result['sparse_output'])/len(result['dense_output']):.2f}\")\n",
    "    print(f\"   Quality: [To be assessed by human evaluation]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù Expected Quality (50% Wanda Pruning):\")\n",
    "print(\"=\"*60)\n",
    "print(\"According to the Wanda paper:\")\n",
    "print(\"   Perplexity increase: +7.7% (5.68 ‚Üí 6.12 on WikiText-2)\")\n",
    "print(\"   Generation quality: Minor degradation\")\n",
    "print(\"   Coherence: Mostly maintained\")\n",
    "print(\"   Task performance: ~90-95% of dense model\")\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"   50% sparsity offers good balance between compression and quality\")\n",
    "print(\"   For production: Consider 40% sparsity for better quality\")\n",
    "print(\"   For extreme compression: 60% sparsity (with higher quality loss)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Comparison Visualization\n",
    "\n",
    "Visualize performance and quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Performance Visualization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "test_names = [r['name'] for r in results]\n",
    "x_pos = np.arange(len(test_names))\n",
    "\n",
    "# Plot 1: Latency Comparison\n",
    "ax = axes[0, 0]\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, dense_latencies, width, label='Dense', color='green', alpha=0.7)\n",
    "ax.bar(x_pos + width/2, sparse_latencies, width, label='Sparse (50%)', color='blue', alpha=0.7)\n",
    "ax.set_xlabel('Test Case', fontsize=10)\n",
    "ax.set_ylabel('Latency (seconds)', fontsize=10)\n",
    "ax.set_title('Latency Comparison', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(test_names, rotation=45, ha='right', fontsize=8)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Throughput Comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x_pos - width/2, dense_throughputs, width, label='Dense', color='green', alpha=0.7)\n",
    "ax.bar(x_pos + width/2, sparse_throughputs, width, label='Sparse (50%)', color='blue', alpha=0.7)\n",
    "ax.set_xlabel('Test Case', fontsize=10)\n",
    "ax.set_ylabel('Throughput (tokens/sec)', fontsize=10)\n",
    "ax.set_title('Throughput Comparison', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(test_names, rotation=45, ha='right', fontsize=8)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Speedup by Test Case\n",
    "ax = axes[1, 0]\n",
    "colors = ['green' if s > 1 else 'orange' for s in speedups]\n",
    "ax.bar(x_pos, speedups, color=colors, alpha=0.7)\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', label='Baseline (1.0x)', linewidth=2)\n",
    "ax.set_xlabel('Test Case', fontsize=10)\n",
    "ax.set_ylabel('Speedup (sparse/dense)', fontsize=10)\n",
    "ax.set_title('Speedup Analysis (>1 = sparse faster)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(test_names, rotation=45, ha='right', fontsize=8)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Output Length Comparison\n",
    "ax = axes[1, 1]\n",
    "output_length_ratios = [len(r['sparse_output'])/len(r['dense_output']) for r in results]\n",
    "ax.bar(x_pos, output_length_ratios, color='steelblue', alpha=0.7)\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', label='Same length', linewidth=2)\n",
    "ax.set_xlabel('Test Case', fontsize=10)\n",
    "ax.set_ylabel('Output Length Ratio', fontsize=10)\n",
    "ax.set_title('Output Length (sparse/dense)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(test_names, rotation=45, ha='right', fontsize=8)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PRUNED_MODEL_DIR, 'inference_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization saved to {PRUNED_MODEL_DIR}/inference_comparison.png\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Save Comparison Results\n",
    "\n",
    "Export results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Saving Comparison Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create DataFrame\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Test Case': r['name'],\n",
    "        'Dense Latency (s)': f\"{r['dense_latency']:.2f}\",\n",
    "        'Sparse Latency (s)': f\"{r['sparse_latency']:.2f}\",\n",
    "        'Dense Throughput (tok/s)': f\"{r['dense_tokens']/r['dense_latency']:.2f}\",\n",
    "        'Sparse Throughput (tok/s)': f\"{r['sparse_tokens']/r['sparse_latency']:.2f}\",\n",
    "        'Speedup': f\"{r['speedup']:.2f}x\",\n",
    "        'Dense Tokens': r['dense_tokens'],\n",
    "        'Sparse Tokens': r['sparse_tokens']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join(PRUNED_MODEL_DIR, 'inference_comparison.csv')\n",
    "comparison_df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ CSV saved to {csv_path}\")\n",
    "\n",
    "# Display table\n",
    "print(\"\\nüìä Comparison Table:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save detailed results (JSON)\n",
    "json_path = os.path.join(PRUNED_MODEL_DIR, 'inference_results.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\n‚úÖ Detailed results saved to {json_path}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Inference Comparison Complete!\n",
    "\n",
    "**Summary**:\n",
    "- ‚úÖ Loaded dense and sparse (50% pruned) models\n",
    "- ‚úÖ Ran 5 diverse test cases\n",
    "- ‚úÖ Compared outputs side-by-side\n",
    "- ‚úÖ Analyzed performance metrics (latency, throughput, speedup)\n",
    "- ‚úÖ Assessed generation quality\n",
    "- ‚úÖ Visualized comparison results\n",
    "- ‚úÖ Saved results to CSV and JSON\n",
    "\n",
    "**Key Findings**:\n",
    "- **Sparsity**: 50% of weights pruned\n",
    "- **Quality**: Minor degradation (expected +7.7% perplexity)\n",
    "- **Performance**: Similar latency (without sparse hardware acceleration)\n",
    "- **Speedup**: ~1.0x (needs NVIDIA A100 + 2:4 sparse for 2x speedup)\n",
    "\n",
    "**Important Notes**:\n",
    "1. **Hardware Acceleration**: Sparse models need specialized hardware (NVIDIA A100 with 2:4 sparse support) for actual speedup\n",
    "2. **Storage Format**: Current implementation stores sparse model in dense format (no size reduction)\n",
    "3. **Quality-Sparsity Tradeoff**: 50% sparsity offers good balance; adjust based on requirements\n",
    "\n",
    "**Next Steps**:\n",
    "1. Proceed to **04-Benchmark.ipynb** for comprehensive performance analysis\n",
    "2. Run perplexity evaluation on WikiText-2\n",
    "3. Profile memory usage and latency distribution\n",
    "4. Export to sparse format (CSR/COO) for size reduction\n",
    "\n",
    "**Files Created**:\n",
    "- `pruned_model/inference_comparison.png`: Performance visualizations\n",
    "- `pruned_model/inference_comparison.csv`: Summary table\n",
    "- `pruned_model/inference_results.json`: Detailed results\n",
    "\n",
    "---\n",
    "\n",
    "**‚è≠Ô∏è Continue to**: [04-Benchmark.ipynb](./04-Benchmark.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
