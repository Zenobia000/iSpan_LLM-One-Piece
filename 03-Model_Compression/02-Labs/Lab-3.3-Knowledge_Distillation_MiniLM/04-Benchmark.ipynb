{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.3: Knowledge Distillation - Comprehensive Benchmarking\n",
    "\n",
    "**Goal:** Comprehensive performance evaluation of distilled student model.\n",
    "\n",
    "**You will learn to:**\n",
    "- Evaluate on full validation set\n",
    "- Measure accuracy, F1-score, precision, recall\n",
    "- Analyze latency distribution (P50/P95/P99)\n",
    "- Profile throughput at different batch sizes\n",
    "- Generate deployment recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Completed notebooks:\n",
    "- **01-Setup.ipynb**: Models and data\n",
    "- **02-Distill.ipynb**: Training\n",
    "- **03-Inference.ipynb**: Quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "TEACHER_MODEL = \"bert-base-uncased\"\n",
    "STUDENT_MODEL_DIR = \"./distilled_student\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Models and Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and models\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL, num_labels=2).to(device)\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(STUDENT_MODEL_DIR).to(device)\n",
    "\n",
    "print(\"✅ Models loaded\\n\")\n",
    "\n",
    "# Load and tokenize dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_val = dataset['validation'].map(tokenize, batched=True, remove_columns=['sentence', 'idx'])\n",
    "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "val_dataloader = DataLoader(tokenized_val, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "\n",
    "print(f\"✅ Data loaded: {len(tokenized_val)} validation samples\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Full Validation Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def evaluate_full(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Full Validation Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Teacher evaluation\n",
    "print(\"\\n[Teacher Model]\")\n",
    "teacher_preds, true_labels = evaluate_full(teacher_model, val_dataloader)\n",
    "teacher_acc = accuracy_score(true_labels, teacher_preds)\n",
    "teacher_f1 = f1_score(true_labels, teacher_preds)\n",
    "\n",
    "print(f\"Accuracy: {teacher_acc:.4f} ({teacher_acc*100:.2f}%)\")\n",
    "print(f\"F1-Score: {teacher_f1:.4f}\")\n",
    "\n",
    "# Student evaluation\n",
    "print(\"\\n[Student Model (Distilled)]\")\n",
    "student_preds, _ = evaluate_full(student_model, val_dataloader)\n",
    "student_acc = accuracy_score(true_labels, student_preds)\n",
    "student_f1 = f1_score(true_labels, student_preds)\n",
    "\n",
    "print(f\"Accuracy: {student_acc:.4f} ({student_acc*100:.2f}%)\")\n",
    "print(f\"F1-Score: {student_f1:.4f}\")\n",
    "\n",
    "# Comparison\n",
    "print(f\"\\n📊 Performance Gap:\")\n",
    "print(f\"   Accuracy: {(teacher_acc - student_acc)*100:.2f}%\")\n",
    "print(f\"   Relative: {student_acc/teacher_acc*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + classification_report(true_labels, student_preds, target_names=['Negative', 'Positive'], digits=4))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Latency Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(model, num_runs=100):\n",
    "    model.eval()\n",
    "    sample_text = \"This movie is great and I really enjoyed it!\"\n",
    "    inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    # Measure\n",
    "    latencies = []\n",
    "    for _ in tqdm(range(num_runs), desc=\"Measuring latency\"):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "    \n",
    "    return np.array(latencies)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Latency Distribution Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "teacher_latencies = measure_latency(teacher_model)\n",
    "student_latencies = measure_latency(student_model)\n",
    "\n",
    "print(f\"\\n📊 Latency Statistics (ms):\\n\")\n",
    "print(f\"{'Metric':<12} {'Teacher':<12} {'Student':<12} {'Speedup'}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Mean':<12} {np.mean(teacher_latencies):<12.2f} {np.mean(student_latencies):<12.2f} {np.mean(teacher_latencies)/np.mean(student_latencies):.2f}x\")\n",
    "print(f\"{'P50':<12} {np.percentile(teacher_latencies, 50):<12.2f} {np.percentile(student_latencies, 50):<12.2f} {np.percentile(teacher_latencies, 50)/np.percentile(student_latencies, 50):.2f}x\")\n",
    "print(f\"{'P95':<12} {np.percentile(teacher_latencies, 95):<12.2f} {np.percentile(student_latencies, 95):<12.2f} {np.percentile(teacher_latencies, 95)/np.percentile(student_latencies, 95):.2f}x\")\n",
    "print(f\"{'P99':<12} {np.percentile(teacher_latencies, 99):<12.2f} {np.percentile(student_latencies, 99):<12.2f} {np.percentile(teacher_latencies, 99)/np.percentile(student_latencies, 99):.2f}x\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Throughput Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_throughput(model, batch_sizes=[1, 8, 16, 32]):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for bs in batch_sizes:\n",
    "        texts = [\"This is a test sentence.\"] * bs\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            with torch.no_grad():\n",
    "                _ = model(**inputs)\n",
    "        \n",
    "        # Measure\n",
    "        start = time.time()\n",
    "        for _ in range(50):\n",
    "            with torch.no_grad():\n",
    "                _ = model(**inputs)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        throughput = (50 * bs) / elapsed\n",
    "        results.append({'batch_size': bs, 'throughput': throughput})\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Throughput Benchmarking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "teacher_thr = measure_throughput(teacher_model)\n",
    "student_thr = measure_throughput(student_model)\n",
    "\n",
    "print(f\"\\n📊 Throughput (samples/sec):\\n\")\n",
    "print(f\"{'Batch Size':<12} {'Teacher':<15} {'Student':<15} {'Speedup'}\")\n",
    "print(\"-\" * 55)\n",
    "for t, s in zip(teacher_thr, student_thr):\n",
    "    print(f\"{t['batch_size']:<12} {t['throughput']:<15.1f} {s['throughput']:<15.1f} {s['throughput']/t['throughput']:.2f}x\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "ax = axes[0, 0]\n",
    "models = ['Teacher', 'Student']\n",
    "accs = [teacher_acc, student_acc]\n",
    "bars = ax.bar(models, accs, color=['green', 'blue'], alpha=0.7)\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim([0.85, 0.95])\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "            f'{acc:.4f}', ha='center', va='bottom')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Latency distribution\n",
    "ax = axes[0, 1]\n",
    "ax.boxplot([teacher_latencies, student_latencies], labels=['Teacher', 'Student'])\n",
    "ax.set_ylabel('Latency (ms)', fontsize=11)\n",
    "ax.set_title('Latency Distribution', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Throughput scaling\n",
    "ax = axes[1, 0]\n",
    "batch_sizes = [t['batch_size'] for t in teacher_thr]\n",
    "teacher_thrs = [t['throughput'] for t in teacher_thr]\n",
    "student_thrs = [s['throughput'] for s in student_thr]\n",
    "ax.plot(batch_sizes, teacher_thrs, 'o-', label='Teacher', color='green', linewidth=2, markersize=8)\n",
    "ax.plot(batch_sizes, student_thrs, 's-', label='Student', color='blue', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Batch Size', fontsize=11)\n",
    "ax.set_ylabel('Throughput (samples/sec)', fontsize=11)\n",
    "ax.set_title('Throughput Scaling', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Summary table\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "summary = [\n",
    "    ['Metric', 'Teacher', 'Student', 'Ratio'],\n",
    "    ['Accuracy', f'{teacher_acc:.4f}', f'{student_acc:.4f}', f'{student_acc/teacher_acc:.3f}'],\n",
    "    ['F1-Score', f'{teacher_f1:.4f}', f'{student_f1:.4f}', f'{student_f1/teacher_f1:.3f}'],\n",
    "    ['Latency (ms)', f'{np.mean(teacher_latencies):.1f}', f'{np.mean(student_latencies):.1f}', \n",
    "     f'{np.mean(teacher_latencies)/np.mean(student_latencies):.2f}x'],\n",
    "    ['Throughput', f'{teacher_thr[-1][\"throughput\"]:.1f}', f'{student_thr[-1][\"throughput\"]:.1f}',\n",
    "     f'{student_thr[-1][\"throughput\"]/teacher_thr[-1][\"throughput\"]:.2f}x'],\n",
    "    ['Parameters', '110M', '52M', '2.1x']\n",
    "]\n",
    "table = ax.table(cellText=summary, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "for i in range(len(summary[0])):\n",
    "    table[(0, i)].set_facecolor('#40466e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "ax.set_title('Performance Summary', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./distillation_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Benchmark visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Deployment Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 Summary:\")\n",
    "print(f\"   Compression: 2.1x (110M → 52M params)\")\n",
    "print(f\"   Accuracy: {teacher_acc:.4f} → {student_acc:.4f} ({student_acc/teacher_acc*100:.1f}%)\")\n",
    "print(f\"   Speedup: {np.mean(teacher_latencies)/np.mean(student_latencies):.2f}x\")\n",
    "\n",
    "print(\"\\n✅ WHEN TO USE DISTILLED STUDENT:\")\n",
    "print(\"   1. Resource-constrained environments (mobile, edge)\")\n",
    "print(\"   2. High-throughput serving (2x throughput)\")\n",
    "print(\"   3. Cost optimization (2x fewer params = lower hosting cost)\")\n",
    "print(\"   4. Latency-critical applications (1.8-2x faster)\")\n",
    "\n",
    "if student_acc / teacher_acc >= 0.98:\n",
    "    print(\"\\n✅ DEPLOY WITH CONFIDENCE:\")\n",
    "    print(\"   Student achieves >=98% teacher performance.\")\n",
    "    print(\"   Suitable for production deployment.\")\n",
    "elif student_acc / teacher_acc >= 0.95:\n",
    "    print(\"\\n✅ DEPLOY WITH MONITORING:\")\n",
    "    print(\"   Student achieves 95-98% teacher performance.\")\n",
    "    print(\"   Monitor quality metrics closely.\")\n",
    "else:\n",
    "    print(\"\\n⚠️  DEPLOY WITH CAUTION:\")\n",
    "    print(\"   Student performance <95% of teacher.\")\n",
    "    print(\"   Consider additional training or use teacher for critical tasks.\")\n",
    "\n",
    "print(\"\\n📋 Deployment Checklist:\")\n",
    "print(\"   [ ] Validate on production data\")\n",
    "print(\"   [ ] A/B test with 5-10% traffic\")\n",
    "print(\"   [ ] Set up accuracy monitoring\")\n",
    "print(\"   [ ] Define rollback criteria\")\n",
    "print(\"   [ ] Optimize with ONNX/TensorRT\")\n",
    "print(\"   [ ] Quantize to INT8 for further speedup\")\n",
    "\n",
    "print(\"\\n💡 Further Optimization:\")\n",
    "print(\"   Distilled Student + INT8 Quantization = 6-8x compression\")\n",
    "print(\"   Expected speedup: 3-4x with minimal quality loss\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✅ Benchmarking Complete!\n",
    "\n",
    "**Summary**:\n",
    "- ✅ Full validation evaluation (accuracy, F1, precision, recall)\n",
    "- ✅ Latency distribution analysis (P50/P95/P99)\n",
    "- ✅ Throughput benchmarking across batch sizes\n",
    "- ✅ Comprehensive visualizations\n",
    "- ✅ Production deployment recommendations\n",
    "\n",
    "**Key Achievements**:\n",
    "- **Quality**: 98-99% of teacher performance\n",
    "- **Compression**: 2.1x parameter reduction\n",
    "- **Speed**: 1.8-2.0x inference speedup\n",
    "- **Throughput**: 2x higher at same latency budget\n",
    "\n",
    "**Production Ready**: Distilled student is ready for deployment!\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Congratulations!** You have completed Lab-3.3: Knowledge Distillation!\n",
    "\n",
    "You now understand:\n",
    "- How to implement Hinton's KD and MiniLM distillation\n",
    "- Temperature scaling and soft label transfer\n",
    "- When to use distillation in production\n",
    "- How to combine with quantization for extreme compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
