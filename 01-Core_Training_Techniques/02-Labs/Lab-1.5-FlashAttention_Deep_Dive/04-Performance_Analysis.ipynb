{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab-1.5: æ€§èƒ½æ·±åº¦åˆ†æ\n",
    "## Performance Analysis - Deep Dive into FlashAttention\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- ä½¿ç”¨ PyTorch Profiler æ·±å…¥åˆ†æ FlashAttention\n",
    "- ç†è§£è¨˜æ†¶é«”è¨ªå•æ¨¡å¼èˆ‡æ€§èƒ½ç“¶é ¸\n",
    "- å„ªåŒ–ä¸åŒå ´æ™¯çš„è¶…åƒæ•¸é…ç½®\n",
    "- å»ºç«‹æ€§èƒ½é æ¸¬æ¨¡å‹\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 45-60åˆ†é˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from torch.cuda.amp import autocast\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# FlashAttention\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"âœ… FlashAttention å¯ç”¨\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"âŒ FlashAttention æœªå®‰è£\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute Capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Profiler å·¥å…·è¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "def profile_attention(attn_func, Q, K, V, name=\"Attention\", num_iters=10):\n    \"\"\"\n    ä½¿ç”¨ PyTorch Profiler åˆ†æ attention æ€§èƒ½\n    \n    Args:\n        attn_func: attention å‡½æ•¸\n        Q, K, V: è¼¸å…¥å¼µé‡\n        name: å‡½æ•¸åç¨±\n        num_iters: æ¸¬è©¦æ¬¡æ•¸\n    \"\"\"\n    # é ç†±\n    for _ in range(3):\n        _ = attn_func(Q, K, V)\n    \n    # Profiling\n    with profile(\n        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n        record_shapes=True,\n        profile_memory=True,\n        with_flops=True\n    ) as prof:\n        with record_function(name):\n            for _ in range(num_iters):\n                _ = attn_func(Q, K, V)\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n    \n    return prof\n\n\ndef print_profiler_summary(prof, sort_by=\"cuda_time_total\", row_limit=10):\n    \"\"\"æ‰“å° Profiler æ‘˜è¦\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"Profiler æ‘˜è¦ (æ’åº: {sort_by}, å‰ {row_limit} é …)\")\n    print(\"=\"*80)\n    \n    # å˜—è©¦ä¸åŒçš„æ’åºéµï¼Œä»¥å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ PyTorch\n    available_sort_keys = [\"cuda_time_total\", \"device_time_total\", \"self_cuda_time_total\", \"self_device_time_total\"]\n    \n    used_sort_key = sort_by\n    for key in available_sort_keys:\n        try:\n            # æ¸¬è©¦æ˜¯å¦æ”¯æŒé€™å€‹æ’åºéµ\n            table = prof.key_averages().table(sort_by=key, row_limit=1)\n            used_sort_key = key\n            break\n        except (AttributeError, KeyError):\n            continue\n    \n    try:\n        print(prof.key_averages().table(\n            sort_by=used_sort_key,\n            row_limit=row_limit\n        ))\n    except Exception as e:\n        print(f\"âš ï¸  ç„¡æ³•é¡¯ç¤ºè©³ç´°è¡¨æ ¼: {e}\")\n        print(\"åŸºæœ¬çµ±è¨ˆ:\")\n        events = prof.key_averages()\n        print(f\"  ç¸½äº‹ä»¶æ•¸: {len(events)}\")\n        for i, evt in enumerate(events[:5]):\n            print(f\"  {i+1}. {evt.key}\")\n\n\nprint(\"âœ… Profiler å·¥å…·æº–å‚™å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. æ¨™æº– Attention æ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def standard_attention(Q, K, V):\n",
    "    \"\"\"æ¨™æº– Self-Attention å¯¦ç¾\"\"\"\n",
    "    # Q, K, V: [batch, seq_len, num_heads, head_dim]\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # è½‰æ›ç‚º [batch, num_heads, seq_len, head_dim]\n",
    "    Q = Q.transpose(1, 2)\n",
    "    K = K.transpose(1, 2)\n",
    "    V = V.transpose(1, 2)\n",
    "    \n",
    "    # Attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attn_weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    # è½‰å› [batch, seq_len, num_heads, head_dim]\n",
    "    output = output.transpose(1, 2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"æ¨™æº– Attention æ€§èƒ½åˆ†æ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æ¸¬è©¦é…ç½®\n",
    "batch_size, seq_len, num_heads, head_dim = 4, 512, 12, 64\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "K = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "V = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "\n",
    "print(f\"\\næ¸¬è©¦é…ç½®:\")\n",
    "print(f\"  Batch Size: {batch_size}\")\n",
    "print(f\"  Sequence Length: {seq_len}\")\n",
    "print(f\"  Num Heads: {num_heads}\")\n",
    "print(f\"  Head Dim: {head_dim}\")\n",
    "\n",
    "# Profile æ¨™æº– Attention\n",
    "prof_std = profile_attention(\n",
    "    standard_attention,\n",
    "    Q, K, V,\n",
    "    name=\"Standard_Attention\",\n",
    "    num_iters=10\n",
    ")\n",
    "\n",
    "print_profiler_summary(prof_std, sort_by=\"cuda_time_total\", row_limit=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. FlashAttention æ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "if FLASH_ATTN_AVAILABLE:\n    print(\"=\"*70)\n    print(\"FlashAttention æ€§èƒ½åˆ†æ\")\n    print(\"=\"*70)\n    \n    def flash_attention_wrapper(Q, K, V):\n        \"\"\"FlashAttention åŒ…è£ (åŒ¹é…æ¨™æº–æ ¼å¼)\"\"\"\n        return flash_attn_func(Q, K, V, causal=False)\n    \n    # Profile FlashAttention\n    prof_flash = profile_attention(\n        flash_attention_wrapper,\n        Q, K, V,\n        name=\"FlashAttention\",\n        num_iters=10\n    )\n    \n    print_profiler_summary(prof_flash, sort_by=\"cuda_time_total\", row_limit=15)\n    \n    # å°æ¯”åˆ†æ\n    print(\"\\n\" + \"=\"*70)\n    print(\"æ€§èƒ½å°æ¯”\")\n    print(\"=\"*70)\n    \n    std_events = prof_std.key_averages()\n    flash_events = prof_flash.key_averages()\n    \n    # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ PyTorch profiler API\n    def get_cuda_time(events):\n        total_time = 0\n        for evt in events:\n            # å˜—è©¦ä¸åŒçš„å±¬æ€§å\n            if hasattr(evt, 'cuda_time_total'):\n                total_time += evt.cuda_time_total\n            elif hasattr(evt, 'device_time_total'):\n                total_time += evt.device_time_total\n            elif hasattr(evt, 'self_cuda_time_total'):\n                total_time += evt.self_cuda_time_total\n            elif hasattr(evt, 'self_device_time_total'):\n                total_time += evt.self_device_time_total\n        return total_time\n    \n    std_cuda_time = get_cuda_time(std_events)\n    flash_cuda_time = get_cuda_time(flash_events)\n    \n    print(f\"\\næ¨™æº– Attention CUDA æ™‚é–“: {std_cuda_time / 1e3:.2f} ms\")\n    print(f\"FlashAttention CUDA æ™‚é–“: {flash_cuda_time / 1e3:.2f} ms\")\n    \n    if flash_cuda_time > 0:\n        print(f\"\\nâš¡ åŠ é€Ÿæ¯”: {std_cuda_time / flash_cuda_time:.2f}x\")\n    else:\n        print(f\"\\nâš ï¸  ç„¡æ³•è¨ˆç®—åŠ é€Ÿæ¯” (FlashAttention æ™‚é–“ç‚º 0)\")\n    \nelse:\n    print(\"\\nâš ï¸  FlashAttention æœªå®‰è£, è·³éæ€§èƒ½åˆ†æ\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. è¨˜æ†¶é«”è¨ªå•æ¨¡å¼åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_memory_pattern(prof, name=\"Model\"):\n    \"\"\"åˆ†æè¨˜æ†¶é«”è¨ªå•æ¨¡å¼\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(f\"{name} è¨˜æ†¶é«”åˆ†æ\")\n    print(\"=\"*70)\n    \n    events = prof.key_averages()\n    \n    # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ PyTorch profiler API\n    def get_memory_usage(evt):\n        cpu_mem = 0\n        cuda_mem = 0\n        \n        # å˜—è©¦ä¸åŒçš„å±¬æ€§å\n        if hasattr(evt, 'cpu_memory_usage'):\n            cpu_mem = evt.cpu_memory_usage\n        elif hasattr(evt, 'self_cpu_memory_usage'):\n            cpu_mem = evt.self_cpu_memory_usage\n            \n        if hasattr(evt, 'cuda_memory_usage'):\n            cuda_mem = evt.cuda_memory_usage\n        elif hasattr(evt, 'self_cuda_memory_usage'):\n            cuda_mem = evt.self_cuda_memory_usage\n        elif hasattr(evt, 'device_memory_usage'):\n            cuda_mem = evt.device_memory_usage\n        elif hasattr(evt, 'self_device_memory_usage'):\n            cuda_mem = evt.self_device_memory_usage\n            \n        return cpu_mem, cuda_mem\n    \n    # æŒ‰è¨˜æ†¶é«”ä½¿ç”¨æ’åº\n    memory_events = []\n    for evt in events:\n        cpu_mem, cuda_mem = get_memory_usage(evt)\n        if cpu_mem > 0 or cuda_mem > 0:\n            memory_events.append((evt, cpu_mem, cuda_mem))\n    \n    # æŒ‰ CUDA è¨˜æ†¶é«”ä½¿ç”¨æ’åº\n    memory_events.sort(key=lambda x: abs(x[2]), reverse=True)\n    \n    print(f\"\\n{'æ“ä½œ':<40} {'CPUè¨˜æ†¶é«”(MB)':<18} {'CUDAè¨˜æ†¶é«”(MB)':<18}\")\n    print(\"-\"*76)\n    \n    for evt, cpu_mem, cuda_mem in memory_events[:10]:\n        cpu_mem_mb = cpu_mem / 1e6\n        cuda_mem_mb = cuda_mem / 1e6\n        print(f\"{evt.key[:38]:<40} {cpu_mem_mb:>15.2f} {cuda_mem_mb:>15.2f}\")\n\n\n# åˆ†ææ¨™æº– Attention è¨˜æ†¶é«”\nanalyze_memory_pattern(prof_std, \"æ¨™æº– Attention\")\n\n# åˆ†æ FlashAttention è¨˜æ†¶é«”\nif FLASH_ATTN_AVAILABLE:\n    analyze_memory_pattern(prof_flash, \"FlashAttention\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. ä¸åŒåºåˆ—é•·åº¦çš„æ€§èƒ½ç¸®æ”¾åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"åºåˆ—é•·åº¦ç¸®æ”¾åˆ†æ\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    seq_lengths = [256, 512, 1024, 2048]\n",
    "    batch_size = 4\n",
    "    num_heads = 12\n",
    "    head_dim = 64\n",
    "    num_iters = 20\n",
    "    \n",
    "    results = {\n",
    "        \"seq_len\": [],\n",
    "        \"std_time\": [],\n",
    "        \"flash_time\": [],\n",
    "        \"std_memory\": [],\n",
    "        \"flash_memory\": []\n",
    "    }\n",
    "    \n",
    "    for seq_len in tqdm(seq_lengths, desc=\"æ¸¬è©¦ä¸åŒåºåˆ—é•·åº¦\"):\n",
    "        print(f\"\\næ¸¬è©¦åºåˆ—é•·åº¦: {seq_len}\")\n",
    "        \n",
    "        Q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "        K = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "        V = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "        \n",
    "        results[\"seq_len\"].append(seq_len)\n",
    "        \n",
    "        # æ¨™æº– Attention\n",
    "        try:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            times = []\n",
    "            for _ in range(num_iters):\n",
    "                start = time.time()\n",
    "                _ = standard_attention(Q, K, V)\n",
    "                torch.cuda.synchronize()\n",
    "                times.append(time.time() - start)\n",
    "            \n",
    "            std_time = np.mean(times) * 1000  # ms\n",
    "            std_mem = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "            \n",
    "            results[\"std_time\"].append(std_time)\n",
    "            results[\"std_memory\"].append(std_mem)\n",
    "            \n",
    "            print(f\"  æ¨™æº–: {std_time:.2f}ms, {std_mem:.2f}GB\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                results[\"std_time\"].append(None)\n",
    "                results[\"std_memory\"].append(None)\n",
    "                print(f\"  æ¨™æº–: OOM\")\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        # FlashAttention\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        times = []\n",
    "        for _ in range(num_iters):\n",
    "            start = time.time()\n",
    "            _ = flash_attn_func(Q, K, V, causal=False)\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "        \n",
    "        flash_time = np.mean(times) * 1000  # ms\n",
    "        flash_mem = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "        \n",
    "        results[\"flash_time\"].append(flash_time)\n",
    "        results[\"flash_memory\"].append(flash_mem)\n",
    "        \n",
    "        print(f\"  Flash: {flash_time:.2f}ms, {flash_mem:.2f}GB\")\n",
    "    \n",
    "    # è½‰æ›ç‚º DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ç¸®æ”¾æ€§èƒ½è¡¨\")\n",
    "    print(\"=\"*70)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  FlashAttention æœªå®‰è£, è·³éç¸®æ”¾åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. æ€§èƒ½ç¸®æ”¾è¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE and 'results' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\"FlashAttention æ€§èƒ½ç¸®æ”¾åˆ†æ\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    seq_lens = results[\"seq_len\"]\n",
    "    \n",
    "    # 1. åŸ·è¡Œæ™‚é–“ç¸®æ”¾\n",
    "    std_times = [t for t in results[\"std_time\"] if t is not None]\n",
    "    flash_times = results[\"flash_time\"]\n",
    "    valid_seq_lens = seq_lens[:len(std_times)]\n",
    "    \n",
    "    axes[0, 0].plot(valid_seq_lens, std_times, 'o-', linewidth=2, markersize=8, label='æ¨™æº– Attention', color='#e74c3c')\n",
    "    axes[0, 0].plot(seq_lens, flash_times, 's-', linewidth=2, markersize=8, label='FlashAttention', color='#2ecc71')\n",
    "    axes[0, 0].set_xlabel('åºåˆ—é•·åº¦')\n",
    "    axes[0, 0].set_ylabel('åŸ·è¡Œæ™‚é–“ (ms)')\n",
    "    axes[0, 0].set_title('åŸ·è¡Œæ™‚é–“ vs åºåˆ—é•·åº¦', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # 2. è¨˜æ†¶é«”ç¸®æ”¾\n",
    "    std_mems = [m for m in results[\"std_memory\"] if m is not None]\n",
    "    flash_mems = results[\"flash_memory\"]\n",
    "    \n",
    "    axes[0, 1].plot(valid_seq_lens, std_mems, 'o-', linewidth=2, markersize=8, label='æ¨™æº– Attention', color='#e74c3c')\n",
    "    axes[0, 1].plot(seq_lens, flash_mems, 's-', linewidth=2, markersize=8, label='FlashAttention', color='#2ecc71')\n",
    "    axes[0, 1].set_xlabel('åºåˆ—é•·åº¦')\n",
    "    axes[0, 1].set_ylabel('å³°å€¼è¨˜æ†¶é«” (GB)')\n",
    "    axes[0, 1].set_title('è¨˜æ†¶é«”ä½¿ç”¨ vs åºåˆ—é•·åº¦', fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. åŠ é€Ÿæ¯”\n",
    "    speedups = [std_times[i] / flash_times[i] for i in range(len(std_times))]\n",
    "    axes[1, 0].bar(range(len(valid_seq_lens)), speedups, color='#3498db')\n",
    "    axes[1, 0].set_xticks(range(len(valid_seq_lens)))\n",
    "    axes[1, 0].set_xticklabels([str(sl) for sl in valid_seq_lens])\n",
    "    axes[1, 0].set_xlabel('åºåˆ—é•·åº¦')\n",
    "    axes[1, 0].set_ylabel('åŠ é€Ÿæ¯” (x)')\n",
    "    axes[1, 0].set_title('FlashAttention åŠ é€Ÿæ¯”', fontweight='bold')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for i, v in enumerate(speedups):\n",
    "        axes[1, 0].text(i, v, f'{v:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. è¤‡é›œåº¦åˆ†æ (Log-Log plot)\n",
    "    axes[1, 1].loglog(seq_lens, flash_times, 's-', linewidth=2, markersize=8, label='FlashAttention å¯¦éš›', color='#2ecc71')\n",
    "    \n",
    "    # æ“¬åˆ O(NÂ²) æ›²ç·š\n",
    "    seq_lens_arr = np.array(seq_lens)\n",
    "    flash_times_arr = np.array(flash_times)\n",
    "    \n",
    "    # y = a*N^2\n",
    "    a = np.mean(flash_times_arr / (seq_lens_arr ** 2))\n",
    "    theoretical = a * (seq_lens_arr ** 2)\n",
    "    \n",
    "    axes[1, 1].loglog(seq_lens, theoretical, '--', linewidth=2, label='O(NÂ²) ç†è«–', color='#e74c3c', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('åºåˆ—é•·åº¦ (log scale)')\n",
    "    axes[1, 1].set_ylabel('åŸ·è¡Œæ™‚é–“ (ms, log scale)')\n",
    "    axes[1, 1].set_title('æ™‚é–“è¤‡é›œåº¦åˆ†æ (Log-Log)', fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3, which='both')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  ç„¡æ³•ç¹ªè£½åœ–è¡¨ - FlashAttention æœªå®‰è£æˆ–ç„¡æ¸¬è©¦çµæœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. æ‰¹æ¬¡å¤§å°å½±éŸ¿åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"æ‰¹æ¬¡å¤§å°å½±éŸ¿åˆ†æ\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    seq_len = 1024\n",
    "    num_heads = 12\n",
    "    head_dim = 64\n",
    "    batch_sizes = [1, 2, 4, 8, 16]\n",
    "    \n",
    "    batch_results = {\n",
    "        \"batch_size\": [],\n",
    "        \"time_ms\": [],\n",
    "        \"memory_gb\": [],\n",
    "        \"throughput\": []  # tokens/sec\n",
    "    }\n",
    "    \n",
    "    for bs in tqdm(batch_sizes, desc=\"æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°\"):\n",
    "        try:\n",
    "            Q = torch.randn(bs, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "            K = torch.randn(bs, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "            V = torch.randn(bs, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
    "            \n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # æ¸¬è©¦æ™‚é–“\n",
    "            times = []\n",
    "            for _ in range(20):\n",
    "                start = time.time()\n",
    "                _ = flash_attn_func(Q, K, V, causal=False)\n",
    "                torch.cuda.synchronize()\n",
    "                times.append(time.time() - start)\n",
    "            \n",
    "            avg_time = np.mean(times) * 1000  # ms\n",
    "            peak_mem = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "            throughput = (bs * seq_len) / (avg_time / 1000)  # tokens/sec\n",
    "            \n",
    "            batch_results[\"batch_size\"].append(bs)\n",
    "            batch_results[\"time_ms\"].append(avg_time)\n",
    "            batch_results[\"memory_gb\"].append(peak_mem)\n",
    "            batch_results[\"throughput\"].append(throughput)\n",
    "            \n",
    "            print(f\"  Batch={bs}: {avg_time:.2f}ms, {peak_mem:.2f}GB, {throughput:.0f} tokens/s\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  Batch={bs}: OOM\")\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # è¦–è¦ºåŒ–\n",
    "    if batch_results[\"batch_size\"]:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        fig.suptitle(\"æ‰¹æ¬¡å¤§å°å½±éŸ¿åˆ†æ\", fontsize=14, fontweight='bold')\n",
    "        \n",
    "        bs_list = batch_results[\"batch_size\"]\n",
    "        \n",
    "        # æ™‚é–“\n",
    "        axes[0].plot(bs_list, batch_results[\"time_ms\"], 'o-', linewidth=2, markersize=8, color='#3498db')\n",
    "        axes[0].set_xlabel('æ‰¹æ¬¡å¤§å°')\n",
    "        axes[0].set_ylabel('åŸ·è¡Œæ™‚é–“ (ms)')\n",
    "        axes[0].set_title('æ™‚é–“ vs æ‰¹æ¬¡å¤§å°', fontweight='bold')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # è¨˜æ†¶é«”\n",
    "        axes[1].plot(bs_list, batch_results[\"memory_gb\"], 's-', linewidth=2, markersize=8, color='#e74c3c')\n",
    "        axes[1].set_xlabel('æ‰¹æ¬¡å¤§å°')\n",
    "        axes[1].set_ylabel('å³°å€¼è¨˜æ†¶é«” (GB)')\n",
    "        axes[1].set_title('è¨˜æ†¶é«” vs æ‰¹æ¬¡å¤§å°', fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        # ååé‡\n",
    "        axes[2].plot(bs_list, batch_results[\"throughput\"], '^-', linewidth=2, markersize=8, color='#2ecc71')\n",
    "        axes[2].set_xlabel('æ‰¹æ¬¡å¤§å°')\n",
    "        axes[2].set_ylabel('ååé‡ (tokens/sec)')\n",
    "        axes[2].set_title('ååé‡ vs æ‰¹æ¬¡å¤§å°', fontweight='bold')\n",
    "        axes[2].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸  FlashAttention æœªå®‰è£, è·³éæ‰¹æ¬¡å¤§å°åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. æœ€ä½³é…ç½®å»ºè­°ç”Ÿæˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimal_config(gpu_memory_gb, seq_length, model_size=\"medium\"):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šç¡¬é«”é…ç½®ç”Ÿæˆæœ€ä½³è¨“ç·´é…ç½®\n",
    "    \n",
    "    Args:\n",
    "        gpu_memory_gb: GPU è¨˜æ†¶é«” (GB)\n",
    "        seq_length: ç›®æ¨™åºåˆ—é•·åº¦\n",
    "        model_size: æ¨¡å‹å¤§å° (small, medium, large)\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"FlashAttention æœ€ä½³é…ç½®å»ºè­°\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nè¼¸å…¥é…ç½®:\")\n",
    "    print(f\"  GPU è¨˜æ†¶é«”: {gpu_memory_gb} GB\")\n",
    "    print(f\"  åºåˆ—é•·åº¦: {seq_length}\")\n",
    "    print(f\"  æ¨¡å‹å¤§å°: {model_size}\")\n",
    "    \n",
    "    # åŸºæ–¼åºåˆ—é•·åº¦æ¨è–¦æ‰¹æ¬¡å¤§å°\n",
    "    if seq_length <= 512:\n",
    "        recommended_bs = min(16, int(gpu_memory_gb // 2))\n",
    "    elif seq_length <= 1024:\n",
    "        recommended_bs = min(8, int(gpu_memory_gb // 3))\n",
    "    elif seq_length <= 2048:\n",
    "        recommended_bs = min(4, int(gpu_memory_gb // 4))\n",
    "    elif seq_length <= 4096:\n",
    "        recommended_bs = min(2, int(gpu_memory_gb // 8))\n",
    "    else:\n",
    "        recommended_bs = 1\n",
    "    \n",
    "    # æ¢¯åº¦ç´¯ç©å»ºè­°\n",
    "    effective_bs = 32  # ç›®æ¨™æœ‰æ•ˆæ‰¹æ¬¡\n",
    "    accumulation_steps = max(1, effective_bs // recommended_bs)\n",
    "    \n",
    "    # å…¶ä»–å„ªåŒ–å»ºè­°\n",
    "    use_amp = True\n",
    "    use_grad_checkpoint = seq_length > 1024 or model_size == \"large\"\n",
    "    use_flash_attn = True\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"æ¨è–¦é…ç½®\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\næ ¸å¿ƒé…ç½®:\")\n",
    "    print(f\"  æ‰¹æ¬¡å¤§å°: {recommended_bs}\")\n",
    "    print(f\"  æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {accumulation_steps}\")\n",
    "    print(f\"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {recommended_bs * accumulation_steps}\")\n",
    "    \n",
    "    print(f\"\\nå„ªåŒ–æŠ€è¡“:\")\n",
    "    print(f\"  æ··åˆç²¾åº¦è¨“ç·´: {'âœ… å•Ÿç”¨' if use_amp else 'âŒ é—œé–‰'}\")\n",
    "    print(f\"  æ¢¯åº¦æª¢æŸ¥é»: {'âœ… å•Ÿç”¨' if use_grad_checkpoint else 'âŒ é—œé–‰'}\")\n",
    "    print(f\"  FlashAttention: {'âœ… å•Ÿç”¨' if use_flash_attn else 'âŒ é—œé–‰'}\")\n",
    "    \n",
    "    # ä»£ç¢¼ç¯„ä¾‹\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"é…ç½®ä»£ç¢¼ç¯„ä¾‹\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    code = f\"\"\"\n",
    "# è¨“ç·´é…ç½®\n",
    "config = {{\n",
    "    'batch_size': {recommended_bs},\n",
    "    'accumulation_steps': {accumulation_steps},\n",
    "    'seq_length': {seq_length},\n",
    "    'use_amp': {use_amp},\n",
    "    'use_grad_checkpoint': {use_grad_checkpoint},\n",
    "}}\n",
    "\n",
    "# æ¨¡å‹é…ç½®\n",
    "model = create_model(seq_length={seq_length})\n",
    "if config['use_grad_checkpoint']:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# è¨“ç·´å¾ªç’°\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler() if config['use_amp'] else None\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    if step % config['accumulation_steps'] == 0:\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    if config['use_amp']:\n",
    "        with autocast(dtype=torch.float16):\n",
    "            loss = model(**batch).loss / config['accumulation_steps']\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss = model(**batch).loss / config['accumulation_steps']\n",
    "        loss.backward()\n",
    "    \n",
    "    if (step + 1) % config['accumulation_steps'] == 0:\n",
    "        if config['use_amp']:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\"\"\"\n",
    "    print(code)\n",
    "    \n",
    "    # é æœŸæ€§èƒ½\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"é æœŸæ€§èƒ½\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if seq_length <= 1024:\n",
    "        speedup_range = \"2-3x\"\n",
    "        memory_saving = \"30-40%\"\n",
    "    elif seq_length <= 2048:\n",
    "        speedup_range = \"3-5x\"\n",
    "        memory_saving = \"40-50%\"\n",
    "    else:\n",
    "        speedup_range = \"5-8x\"\n",
    "        memory_saving = \"50-70%\"\n",
    "    \n",
    "    print(f\"\\nç›¸å°æ–¼æ¨™æº– Attention:\")\n",
    "    print(f\"  é€Ÿåº¦æå‡: {speedup_range}\")\n",
    "    print(f\"  è¨˜æ†¶é«”ç¯€çœ: {memory_saving}\")\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    generate_optimal_config(gpu_mem, seq_length=2048, model_size=\"medium\")\n",
    "else:\n",
    "    print(\"ç¤ºä¾‹é…ç½®:\")\n",
    "    generate_optimal_config(16, seq_length=2048, model_size=\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. å¯¦é©—ç¸½çµ\n",
    "\n",
    "### é—œéµç™¼ç¾\n",
    "\n",
    "é€šéæ·±å…¥çš„æ€§èƒ½åˆ†æ, æˆ‘å€‘ç™¼ç¾:\n",
    "\n",
    "1. **FlashAttention çš„æ ¸å¿ƒå„ªå‹¢**:\n",
    "   - **IO å„ªåŒ–**: æ¸›å°‘ HBM è¨ªå•æ¬¡æ•¸, æå‡è¨˜æ†¶é«”é »å¯¬åˆ©ç”¨ç‡\n",
    "   - **èåˆå…§æ ¸**: å–®å€‹ CUDA kernel å®Œæˆæ‰€æœ‰è¨ˆç®—\n",
    "   - **ç·šæ€§è¨˜æ†¶é«”**: O(N) è¨˜æ†¶é«”è¤‡é›œåº¦ vs æ¨™æº–çš„ O(NÂ²)\n",
    "   - **æ•¸å­¸ç­‰åƒ¹**: å®Œå…¨ç­‰åƒ¹æ–¼æ¨™æº– attention, ç„¡è¿‘ä¼¼\n",
    "\n",
    "2. **æ€§èƒ½ç¸®æ”¾è¦å¾‹**:\n",
    "   - æ™‚é–“è¤‡é›œåº¦: ä»ç‚º O(NÂ²), ä½†å¸¸æ•¸é …æ›´å°\n",
    "   - è¨˜æ†¶é«”è¤‡é›œåº¦: O(N), ç·šæ€§å¢é•·\n",
    "   - åŠ é€Ÿæ¯”: éš¨åºåˆ—é•·åº¦å¢åŠ è€Œæå‡ (2x â†’ 8x)\n",
    "\n",
    "3. **æ‰¹æ¬¡å¤§å°å½±éŸ¿**:\n",
    "   - æ›´å¤§æ‰¹æ¬¡: æ›´å¥½çš„ GPU åˆ©ç”¨ç‡, ä½†è¨˜æ†¶é«”éœ€æ±‚å¢åŠ \n",
    "   - ååé‡: æ‰¹æ¬¡å¢å¤§æ™‚æ¥è¿‘ç·šæ€§æå‡\n",
    "   - æœ€ä½³é…ç½®: å–æ±ºæ–¼ GPU è¨˜æ†¶é«”å’Œåºåˆ—é•·åº¦\n",
    "\n",
    "4. **Profiler æ´å¯Ÿ**:\n",
    "   - æ¨™æº– Attention: å¤§é‡æ™‚é–“èŠ±åœ¨ matmul å’Œ softmax\n",
    "   - FlashAttention: æ™‚é–“ä¸»è¦åœ¨èåˆ kernel\n",
    "   - è¨˜æ†¶é«”è¨ªå•: FlashAttention å¤§å¹…æ¸›å°‘ HBM è®€å¯«\n",
    "\n",
    "### æœ€ä½³å¯¦è¸ç¸½çµ\n",
    "\n",
    "#### 1. ä½•æ™‚ä½¿ç”¨ FlashAttention?\n",
    "\n",
    "**å¼·çƒˆæ¨è–¦**:\n",
    "- âœ… è¨“ç·´ Transformer æ¨¡å‹\n",
    "- âœ… åºåˆ—é•·åº¦ >512 tokens\n",
    "- âœ… GPU è¨˜æ†¶é«”å—é™\n",
    "- âœ… éœ€è¦åŠ é€Ÿè¨“ç·´\n",
    "\n",
    "**è¬¹æ…è€ƒæ…®**:\n",
    "- âš ï¸  çŸ­åºåˆ— (<256 tokens): åŠ é€Ÿæ•ˆæœæœ‰é™\n",
    "- âš ï¸  éœ€è¦è¤‡é›œè‡ªå®šç¾© mask: FlashAttention æ”¯æ´æœ‰é™\n",
    "- âš ï¸  èˆŠ GPU (compute capability <7.5): ä¸æ”¯æ´\n",
    "\n",
    "#### 2. é…ç½®å„ªåŒ–å»ºè­°\n",
    "\n",
    "```python\n",
    "# å° GPU (8GB)\n",
    "config_8gb = {\n",
    "    'max_seq_len': 1024,\n",
    "    'batch_size': 2,\n",
    "    'accumulation': 16,\n",
    "    'use_grad_checkpoint': True\n",
    "}\n",
    "\n",
    "# ä¸­ GPU (16GB)\n",
    "config_16gb = {\n",
    "    'max_seq_len': 2048,\n",
    "    'batch_size': 4,\n",
    "    'accumulation': 8,\n",
    "    'use_grad_checkpoint': True\n",
    "}\n",
    "\n",
    "# å¤§ GPU (24GB+)\n",
    "config_24gb = {\n",
    "    'max_seq_len': 4096,\n",
    "    'batch_size': 8,\n",
    "    'accumulation': 4,\n",
    "    'use_grad_checkpoint': False  # è¨˜æ†¶é«”å……è¶³æ™‚å¯é—œé–‰\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3. æ€§èƒ½èª¿å„ªæŠ€å·§\n",
    "\n",
    "**è¨˜æ†¶é«”å„ªåŒ–**:\n",
    "1. FlashAttention (å¿…é ˆ)\n",
    "2. æ··åˆç²¾åº¦è¨“ç·´ (FP16/BF16)\n",
    "3. æ¢¯åº¦æª¢æŸ¥é» (é•·åºåˆ—)\n",
    "4. æ¢¯åº¦ç´¯ç© (å°æ‰¹æ¬¡)\n",
    "\n",
    "**é€Ÿåº¦å„ªåŒ–**:\n",
    "1. å¢å¤§æ‰¹æ¬¡å¤§å° (è¨˜æ†¶é«”å…è¨±)\n",
    "2. ä½¿ç”¨ BF16 (Ampere æ¶æ§‹)\n",
    "3. å„ªåŒ–æ•¸æ“šè¼‰å…¥ (num_workers, pin_memory)\n",
    "4. æ¸›å°‘åŒæ­¥é»\n",
    "\n",
    "### Profiling å·¥ä½œæµ\n",
    "\n",
    "```python\n",
    "# 1. åŸºç¤æ€§èƒ½æ¸¬è©¦\n",
    "benchmark_attention(model, seq_lengths=[512, 1024, 2048])\n",
    "\n",
    "# 2. è©³ç´° Profiling\n",
    "with profile(activities=[ProfilerActivity.CUDA]) as prof:\n",
    "    train_step(model, batch)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n",
    "\n",
    "# 3. è¨˜æ†¶é«”åˆ†æ\n",
    "torch.cuda.memory._record_memory_history()\n",
    "# ... è¨“ç·´\n",
    "torch.cuda.memory._dump_snapshot(\"memory.pickle\")\n",
    "\n",
    "# 4. å„ªåŒ–è¿­ä»£\n",
    "# æ ¹æ“š profiling çµæœèª¿æ•´é…ç½®\n",
    "```\n",
    "\n",
    "### é€²éšä¸»é¡Œ\n",
    "\n",
    "1. **Flash-Decoding**: æ¨ç†å„ªåŒ–ç‰ˆæœ¬\n",
    "2. **PagedAttention**: vLLM çš„ KV Cache å„ªåŒ–\n",
    "3. **Ring Attention**: è¶…é•·åºåˆ— (100K+)\n",
    "4. **Multi-Query/Grouped-Query Attention**: æ¸›å°‘ KV heads\n",
    "\n",
    "### è³‡æºèˆ‡å·¥å…·\n",
    "\n",
    "- [FlashAttention è«–æ–‡](https://arxiv.org/abs/2205.14135)\n",
    "- [å®˜æ–¹å¯¦ç¾](https://github.com/Dao-AILab/flash-attention)\n",
    "- [PyTorch Profiler æ–‡æª”](https://pytorch.org/docs/stable/profiler.html)\n",
    "- [CUDA æ€§èƒ½å„ªåŒ–æŒ‡å—](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)\n",
    "\n",
    "### çµèª\n",
    "\n",
    "FlashAttention æ˜¯ç¾ä»£ LLM è¨“ç·´çš„åŸºçŸ³æŠ€è¡“ã€‚é€šéæœ¬å¯¦é©—å®¤çš„å­¸ç¿’:\n",
    "- âœ… ç†è§£äº† FlashAttention çš„åŸç†èˆ‡å¯¦ç¾\n",
    "- âœ… æŒæ¡äº†æ€§èƒ½åˆ†æèˆ‡å„ªåŒ–æ–¹æ³•\n",
    "- âœ… å­¸æœƒäº†é…ç½®æœ€ä½³è¨“ç·´åƒæ•¸\n",
    "- âœ… å¯ä»¥åœ¨å¯¦éš›é …ç›®ä¸­æ‡‰ç”¨ FlashAttention\n",
    "\n",
    "æ­å–œå®Œæˆ Lab-1.5 æ‰€æœ‰å¯¦é©—! ğŸ‰\n",
    "\n",
    "å»ºè­°ä¸‹ä¸€æ­¥:\n",
    "- **Lab-1.6**: MQA/GQA - é€²ä¸€æ­¥å„ªåŒ–æ¨ç†æ•ˆç‡\n",
    "- **Lab-1.7**: DPO Alignment - æ¨¡å‹å°é½ŠæŠ€è¡“\n",
    "- **å¯¦éš›æ‡‰ç”¨**: åœ¨è‡ªå·±çš„ LLM é …ç›®ä¸­é›†æˆ FlashAttention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}