# Lab-1.5: FlashAttention æ·±åº¦è§£æ
## FlashAttention Deep Dive

**å¯¦é©—å®¤é¡å‹**: æ³¨æ„åŠ›æ©Ÿåˆ¶å„ªåŒ–
**é›£åº¦ç­‰ç´š**: â­â­â­â­ (ä¸­é«˜ç´š)
**é ä¼°æ™‚é–“**: 4-6å°æ™‚
**é©ç”¨GPU**: 8GB+ VRAM (å»ºè­° 16GB+)

---

## ğŸ“š å¯¦é©—å®¤æ¦‚è¿°

FlashAttention æ˜¯ç¾ä»£ LLM è¨“ç·´èˆ‡æ¨ç†çš„æ ¸å¿ƒå„ªåŒ–æŠ€è¡“ï¼Œé€šéé‡æ–°è¨­è¨ˆæ³¨æ„åŠ›è¨ˆç®—çš„è¨˜æ†¶é«”è¨ªå•æ¨¡å¼ï¼Œå¯¦ç¾äº†é¡¯è‘—çš„é€Ÿåº¦æå‡èˆ‡è¨˜æ†¶é«”ç¯€çœã€‚æœ¬å¯¦é©—å®¤å°‡æ·±å…¥æ¢ç´¢ FlashAttention çš„åŸç†ã€å¯¦ç¾èˆ‡å¯¦éš›æ‡‰ç”¨ã€‚

### å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š
- âœ… ç†è§£æ¨™æº–æ³¨æ„åŠ›æ©Ÿåˆ¶çš„è¨˜æ†¶é«”ç“¶é ¸
- âœ… æŒæ¡ FlashAttention çš„æ ¸å¿ƒç®—æ³•åŸç†
- âœ… å°æ¯” FlashAttention v1 vs v2 çš„æ€§èƒ½å·®ç•°
- âœ… åœ¨è¨“ç·´èˆ‡æ¨ç†ä¸­æ‡‰ç”¨ FlashAttention
- âœ… è¨“ç·´è¶…é•·åºåˆ—æ¨¡å‹ (>8K tokens)
- âœ… åˆ†æ FlashAttention çš„æ€§èƒ½å„ªå‹¢

---

## ğŸ¯ FlashAttention æ ¸å¿ƒæŠ€è¡“

### ç‚ºä»€éº¼éœ€è¦ FlashAttention?

**æ¨™æº– Attention çš„å•é¡Œ**:
```
æ¨™æº– Self-Attention è¨ˆç®—:
1. Q @ K^T â†’ ç”Ÿæˆ attention scores (NÃ—N çŸ©é™£)
2. Softmax(scores) â†’ è¨ˆç®— attention weights (NÃ—N çŸ©é™£)
3. weights @ V â†’ ç”¢ç”Ÿè¼¸å‡º

è¨˜æ†¶é«”éœ€æ±‚: O(NÂ²) - åºåˆ—é•·åº¦çš„å¹³æ–¹
æ™‚é–“è¤‡é›œåº¦: O(NÂ²d) - N: åºåˆ—é•·åº¦, d: éš±è—ç¶­åº¦
```

**å•é¡Œåˆ†æ**:
- **è¨˜æ†¶é«”ç“¶é ¸**: NÃ—N æ³¨æ„åŠ›çŸ©é™£åœ¨é•·åºåˆ—æ™‚è¨˜æ†¶é«”çˆ†ç‚¸
  - N=1024: ~4MB (FP32)
  - N=2048: ~16MB
  - N=8192: ~256MB
  - N=16384: ~1GB (å–®å€‹ head!)

- **è¨˜æ†¶é«”é »å¯¬ç“¶é ¸**: GPU HBM (High Bandwidth Memory) è¨ªå•æ…¢
  - SRAM (on-chip): ~19 TB/s
  - HBM (off-chip): ~1.5 TB/s
  - **é€Ÿåº¦å·®è·**: ~12x

### FlashAttention æ ¸å¿ƒå‰µæ–°

**ç®—æ³•åŸç†**: Tiling + Recomputation

```
FlashAttention ç­–ç•¥:
1. åˆ†å¡Š (Tiling): å°‡ Q, K, V åˆ†æˆå°å¡Šè¼‰å…¥ SRAM
2. èåˆé‹ç®— (Kernel Fusion): åœ¨ SRAM å…§å®Œæˆæ‰€æœ‰è¨ˆç®—
3. åœ¨ç·š Softmax: ä¸å„²å­˜å®Œæ•´ attention matrix
4. åå‘å‚³æ’­é‡è¨ˆç®—: éœ€è¦æ™‚é‡æ–°è¨ˆç®—è€Œéå„²å­˜

è¨˜æ†¶é«”éœ€æ±‚: O(N) - ç·šæ€§æ–¼åºåˆ—é•·åº¦
æ™‚é–“è¤‡é›œåº¦: O(NÂ²d) - ä¸è®Šï¼Œä½† IO é–‹éŠ·å¤§å¹…é™ä½
```

**é—œéµæŠ€è¡“ç‰¹æ€§**:
- **IO æ„ŸçŸ¥ç®—æ³•**: å„ªåŒ– GPU è¨˜æ†¶é«”éšå±¤è¨ªå•
- **èåˆå…§æ ¸**: æ¸›å°‘ HBM è®€å¯«æ¬¡æ•¸
- **ç²¾ç¢ºè¨ˆç®—**: æ•¸å­¸ç­‰åƒ¹æ–¼æ¨™æº– attention (ç„¡è¿‘ä¼¼)
- **åå‘å‚³æ’­**: é‡è¨ˆç®—ç­–ç•¥å¹³è¡¡é€Ÿåº¦èˆ‡è¨˜æ†¶é«”

### FlashAttention v1 vs v2

| ç‰¹æ€§ | FlashAttention v1 | FlashAttention v2 | æ”¹é€² |
|------|------------------|-------------------|------|
| **åˆ†å¡Šç­–ç•¥** | Outer loop over seq | Outer loop over heads | æ›´å¥½ä¸¦è¡Œ |
| **å·¥ä½œåˆ†é…** | åŸºæ–¼åºåˆ— | åŸºæ–¼ warp | GPU åˆ©ç”¨ç‡ â¬† |
| **é€Ÿåº¦** | 2-3x vs æ¨™æº– | 2-4x vs v1 | ç¸½è¨ˆ 5-8x â¬† |
| **è¨˜æ†¶é«”** | O(N) | O(N) | æŒå¹³ |
| **é•·åºåˆ—** | æ”¯æ´è‡³ 8K | æ”¯æ´è‡³ 64K+ | æ“´å±•æ€§ â¬† |

---

## ğŸ”§ æŠ€è¡“åŸç†æ·±åº¦è§£æ

### æ¨™æº– Attention çš„è¨˜æ†¶é«”è¨ªå•æ¨¡å¼

```python
# æ¨™æº–å¯¦ç¾ (å½ä»£ç¢¼)
def standard_attention(Q, K, V):
    # Step 1: è¨ˆç®— scores (HBM â†’ HBM)
    scores = Q @ K.T  # [batch, heads, seq_len, seq_len]
    # âŒ å¯«å…¥ HBM: NÂ² å…ƒç´ 

    # Step 2: Softmax (HBM â†’ HBM)
    weights = softmax(scores)  # [batch, heads, seq_len, seq_len]
    # âŒ è®€å– + å¯«å…¥ HBM: 2NÂ² æ¬¡è¨ªå•

    # Step 3: è¨ˆç®—è¼¸å‡º (HBM â†’ HBM)
    output = weights @ V  # [batch, heads, seq_len, head_dim]
    # âŒ è®€å– HBM: NÂ² å…ƒç´ 

    return output

# HBM è¨ªå•ç¸½é‡: 4NÂ² (è®€å¯« scores + weights)
```

### FlashAttention çš„å„ªåŒ–è¨ªå•æ¨¡å¼

```python
# FlashAttention å¯¦ç¾ (å½ä»£ç¢¼)
def flash_attention(Q, K, V):
    # åœ¨ SRAM ä¸­åˆ†å¡Šè™•ç†
    for block_q in range(0, N, block_size):
        # è¼‰å…¥ Q çš„ä¸€å€‹å¡Šåˆ° SRAM
        Q_block = load_to_sram(Q[block_q:block_q+block_size])

        # åˆå§‹åŒ–è¼¸å‡ºç´¯åŠ å™¨
        O_block = zeros(block_size, d)

        for block_kv in range(0, N, block_size):
            # è¼‰å…¥ K, V çš„ä¸€å€‹å¡Šåˆ° SRAM
            K_block = load_to_sram(K[block_kv:block_kv+block_size])
            V_block = load_to_sram(V[block_kv:block_kv+block_size])

            # âœ… åœ¨ SRAM å…§å®Œæˆæ‰€æœ‰è¨ˆç®—
            scores_block = Q_block @ K_block.T
            weights_block = softmax(scores_block)  # åœ¨ç·š Softmax
            O_block += weights_block @ V_block

        # å¯«å› HBM
        write_to_hbm(O_block)

# HBM è¨ªå•ç¸½é‡: O(N) - åƒ…è®€å– Q,K,V å’Œå¯«å…¥ O
```

**é—œéµå„ªåŒ–**:
1. **Tiling**: æ•¸æ“šåˆ†å¡Šï¼Œç¢ºä¿åœ¨ SRAM å…§è¨ˆç®—
2. **Online Softmax**: ä¸å„²å­˜ä¸­é–“ attention matrix
3. **Kernel Fusion**: å–®å€‹ CUDA kernel å®Œæˆæ‰€æœ‰æ“ä½œ
4. **IO æœ€å°åŒ–**: HBM è¨ªå•å¾ O(NÂ²) é™è‡³ O(N)

### æ•¸å­¸ç­‰åƒ¹æ€§è­‰æ˜

FlashAttention èˆ‡æ¨™æº– attention **æ•¸å­¸ç­‰åƒ¹**:

```
æ¨™æº– Attention:
Attention(Q, K, V) = softmax(QK^T / âˆšd) V

FlashAttention:
é€šéåˆ†å¡Šè¨ˆç®— + åœ¨ç·š Softmaxï¼Œæœ€çµ‚çµæœå®Œå…¨ç›¸åŒ
(ç„¡è¿‘ä¼¼ï¼Œç„¡ç²¾åº¦æå¤±)
```

**åœ¨ç·š Softmax ç®—æ³•**:
```python
# ç´¯ç©å¼ Softmax (ç”¨æ–¼åˆ†å¡Šè¨ˆç®—)
def online_softmax(x_blocks):
    m = -inf  # æœ€å¤§å€¼
    l = 0     # æŒ‡æ•¸å’Œ

    for x in x_blocks:
        m_new = max(m, max(x))
        l = l * exp(m - m_new) + sum(exp(x - m_new))
        m = m_new

    return l, m

# ä¿è­‰æ•¸å€¼ç©©å®šæ€§ä¸”èˆ‡æ¨™æº– softmax ç­‰åƒ¹
```

---

## ğŸ“‚ å¯¦é©—å®¤çµæ§‹

```
Lab-1.5-FlashAttention_Deep_Dive/
â”œâ”€â”€ README.md                           # æœ¬æ–‡æª”
â”œâ”€â”€ 01-Setup_and_Comparison.ipynb      # ç’°å¢ƒè¨­ç½®èˆ‡æ¨™æº–å°æ¯”
â”œâ”€â”€ 02-FlashAttention_Demo.ipynb       # FlashAttention å¯¦æˆ°æ¼”ç¤º
â”œâ”€â”€ 03-Long_Sequence_Training.ipynb    # é•·åºåˆ—è¨“ç·´æ‡‰ç”¨
â””â”€â”€ 04-Performance_Analysis.ipynb      # æ€§èƒ½æ·±åº¦åˆ†æ
```

---

## ğŸ“Š å¯¦é©—å…§å®¹è©³è§£

### Notebook 1: ç’°å¢ƒè¨­ç½®èˆ‡æ¨™æº–å°æ¯” (01-Setup_and_Comparison.ipynb)
**æ™‚é–“**: 60-90åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- é©—è­‰ FlashAttention å®‰è£èˆ‡å¯ç”¨æ€§
- å¯¦ç¾æ¨™æº– Attention æ©Ÿåˆ¶
- å°æ¯”æ¨™æº– vs FlashAttention çš„åŸºç¤æ€§èƒ½
- ç†è§£è¨˜æ†¶é«”èˆ‡é€Ÿåº¦çš„æ¬Šè¡¡

#### å¯¦é©—å…§å®¹
1. **ç’°å¢ƒé©—è­‰**
   - æª¢æŸ¥ CUDA ç‰ˆæœ¬èˆ‡ GPU æ”¯æ´
   - å®‰è£ flash-attn å¥—ä»¶
   - é©—è­‰ FlashAttention å¯ç”¨æ€§

2. **æ¨™æº– Attention å¯¦ç¾**
   ```python
   import torch
   import torch.nn.functional as F

   def standard_attention(Q, K, V, mask=None):
       """æ¨™æº– Self-Attention å¯¦ç¾"""
       d_k = Q.size(-1)

       # Attention scores
       scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

       # Optional mask
       if mask is not None:
           scores = scores.masked_fill(mask == 0, -1e9)

       # Softmax
       attn_weights = F.softmax(scores, dim=-1)

       # Output
       output = torch.matmul(attn_weights, V)

       return output, attn_weights
   ```

3. **FlashAttention ä½¿ç”¨**
   ```python
   from flash_attn import flash_attn_func

   def flash_attention(Q, K, V):
       """FlashAttention åŒ…è£"""
       # FlashAttention è¦æ±‚ç‰¹å®šæ ¼å¼
       # (batch, seq_len, num_heads, head_dim)
       output = flash_attn_func(Q, K, V, causal=False)
       return output
   ```

4. **åŸºç¤æ€§èƒ½å°æ¯”**
   - åºåˆ—é•·åº¦: [512, 1024, 2048, 4096]
   - æ¸¬é‡: é€Ÿåº¦ã€è¨˜æ†¶é«”ã€ç²¾åº¦
   - è¦–è¦ºåŒ–çµæœå°æ¯”

#### é æœŸçµæœ
| åºåˆ—é•·åº¦ | æ¨™æº– Attention æ™‚é–“ | FlashAttention æ™‚é–“ | åŠ é€Ÿæ¯” |
|---------|-------------------|-------------------|--------|
| 512 | 10ms | 5ms | 2.0x |
| 1024 | 35ms | 12ms | 2.9x |
| 2048 | 140ms | 30ms | 4.7x |
| 4096 | OOM or 560ms | 80ms | 7.0x |

---

### Notebook 2: FlashAttention å¯¦æˆ°æ¼”ç¤º (02-FlashAttention_Demo.ipynb)
**æ™‚é–“**: 45-60åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- åœ¨çœŸå¯¦æ¨¡å‹ä¸­é›†æˆ FlashAttention
- å°æ¯”è¨“ç·´èˆ‡æ¨ç†æ€§èƒ½
- ç†è§£ causal vs non-causal attention
- æ¸¬è©¦ä¸åŒé…ç½®çš„å½±éŸ¿

#### å¯¦é©—å…§å®¹
1. **Transformer æ¨¡å‹é›†æˆ**
   ```python
   from transformers import GPT2Config, GPT2LMHeadModel

   # æ¨™æº– GPT-2
   config = GPT2Config(
       n_positions=2048,
       use_flash_attn=False
   )
   model_std = GPT2LMHeadModel(config)

   # FlashAttention GPT-2
   config_flash = GPT2Config(
       n_positions=2048,
       use_flash_attn=True  # å•Ÿç”¨ FlashAttention
   )
   model_flash = GPT2LMHeadModel(config_flash)
   ```

2. **è¨“ç·´æ€§èƒ½æ¸¬è©¦**
   - å‰å‘å‚³æ’­æ™‚é–“å°æ¯”
   - åå‘å‚³æ’­æ™‚é–“å°æ¯”
   - ç¸½é«”è¨“ç·´ååé‡

3. **æ¨ç†æ€§èƒ½æ¸¬è©¦**
   - å–®æ¬¡æ¨ç†å»¶é²
   - æ‰¹æ¬¡æ¨ç†ååé‡
   - è¨˜æ†¶é«”å ç”¨å°æ¯”

4. **Causal Attention åˆ†æ**
   ```python
   # Causal (GPT-style): åªçœ‹éå»çš„ token
   output_causal = flash_attn_func(Q, K, V, causal=True)

   # Non-causal (BERT-style): å¯çœ‹å…¨éƒ¨ token
   output_non_causal = flash_attn_func(Q, K, V, causal=False)
   ```

#### é æœŸçµæœ
- è¨“ç·´é€Ÿåº¦: 2-4x æå‡
- æ¨ç†é€Ÿåº¦: 1.5-2x æå‡
- è¨˜æ†¶é«”ç¯€çœ: 30-50% (é•·åºåˆ—)

---

### Notebook 3: é•·åºåˆ—è¨“ç·´æ‡‰ç”¨ (03-Long_Sequence_Training.ipynb)
**æ™‚é–“**: 60-90åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- è¨“ç·´è¶…é•·åºåˆ—æ¨¡å‹ (>4K tokens)
- å°æ¯”ä¸åŒåºåˆ—é•·åº¦çš„æ€§èƒ½
- ç†è§£é•·åºåˆ—è¨“ç·´çš„æŒ‘æˆ°èˆ‡è§£æ±ºæ–¹æ¡ˆ
- å¯¦ä½œé•·æ–‡æœ¬æ‘˜è¦/QA ä»»å‹™

#### å¯¦é©—å…§å®¹
1. **é•·åºåˆ—æ•¸æ“šæº–å‚™**
   ```python
   # æº–å‚™é•·æ–‡æœ¬æ•¸æ“š (2K, 4K, 8K tokens)
   long_sequences = [
       tokenize(text, max_length=2048),
       tokenize(text, max_length=4096),
       tokenize(text, max_length=8192)
   ]
   ```

2. **é•·åºåˆ—è¨“ç·´å¯¦é©—**
   - åºåˆ—é•·åº¦æ“´å±•å¯¦é©—
   - è¨˜æ†¶é«”å ç”¨ç›£æ§
   - è¨“ç·´ç©©å®šæ€§åˆ†æ

3. **Position Encoding è™•ç†**
   ```python
   # RoPE (Rotary Position Embedding) + FlashAttention
   # é©ç”¨æ–¼è¶…é•·åºåˆ—
   from flash_attn.modules.rotary import apply_rotary_emb
   ```

4. **å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹**
   - é•·æ–‡æª”æ‘˜è¦
   - é•·å°è©±ç†è§£
   - ä»£ç¢¼ç†è§£ (å®Œæ•´æ–‡ä»¶)

#### é æœŸæˆæœ
- æˆåŠŸè¨“ç·´ 8K+ åºåˆ—æ¨¡å‹
- è¨˜æ†¶é«”å ç”¨ <16GB (with FlashAttention)
- vs æ¨™æº– attention: OOM at 2K

---

### Notebook 4: æ€§èƒ½æ·±åº¦åˆ†æ (04-Performance_Analysis.ipynb)
**æ™‚é–“**: 45-60åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- æ·±å…¥åˆ†æ FlashAttention çš„æ€§èƒ½ç‰¹æ€§
- ç†è§£ä¸åŒç¡¬é«”ä¸Šçš„è¡¨ç¾
- å„ªåŒ–è¶…åƒæ•¸é…ç½®
- å»ºç«‹æ€§èƒ½é æ¸¬æ¨¡å‹

#### å¯¦é©—å…§å®¹
1. **è©³ç´°æ€§èƒ½ Profiling**
   ```python
   import torch.profiler as profiler

   with profiler.profile(
       activities=[profiler.ProfilerActivity.CPU,
                   profiler.ProfilerActivity.CUDA],
       record_shapes=True,
       with_flops=True
   ) as prof:
       # é‹è¡Œ FlashAttention
       output = flash_attn_func(Q, K, V)

   print(prof.key_averages().table(sort_by="cuda_time_total"))
   ```

2. **è¨˜æ†¶é«”åˆ†æ**
   - HBM è¨ªå•æ¨¡å¼åˆ†æ
   - SRAM ä½¿ç”¨æ•ˆç‡
   - è¨˜æ†¶é«”é »å¯¬åˆ©ç”¨ç‡

3. **æ“´å±•æ€§åˆ†æ**
   - æ‰¹æ¬¡å¤§å°å½±éŸ¿
   - åºåˆ—é•·åº¦æ“´å±•æ€§
   - å¤šé ­æ³¨æ„åŠ›ä¸¦è¡Œåº¦

4. **æœ€ä½³å¯¦è¸å»ºè­°**
   - ä½•æ™‚ä½¿ç”¨ FlashAttention
   - é…ç½®å„ªåŒ–å»ºè­°
   - å¸¸è¦‹é™·é˜±èˆ‡é¿å‘æŒ‡å—

#### åˆ†æçµæœç¯„ä¾‹
```
FlashAttention æ€§èƒ½ç‰¹å¾µ:
- è¨˜æ†¶é«”ç¯€çœ: 40-60% (åºåˆ—é•·åº¦ >2K)
- é€Ÿåº¦æå‡: 2-8x (å–æ±ºæ–¼åºåˆ—é•·åº¦)
- æœ€ä½³å ´æ™¯: é•·åºåˆ— (>1K), å¤šé ­æ³¨æ„åŠ›
- é™åˆ¶: éœ€è¦ CUDA 7.5+, Ampere æ¶æ§‹æœ€ä½³
```

---

## ğŸš€ ç’°å¢ƒæº–å‚™

### å‰ç½®è¦æ±‚

#### ç¡¬é«”è¦æ±‚
- **GPU**: NVIDIA GPU with CUDA Capability â‰¥ 7.5
  - æ¨è–¦: RTX 3090, A100, H100
  - æœ€ä½: RTX 2080, V100
- **VRAM**: æœ€å°‘ 8GBï¼Œå»ºè­° 16GB+
- **CUDA**: 11.6+ (å»ºè­° 12.1+)

#### è»Ÿé«”è¦æ±‚
```bash
# ç¢ºèª PyTorch ç‰ˆæœ¬
python -c "import torch; print(torch.__version__)"
# éœ€è¦: PyTorch 2.0+

# ç¢ºèª CUDA ç‰ˆæœ¬
nvcc --version
# éœ€è¦: CUDA 11.6+
```

### FlashAttention å®‰è£

#### æ–¹æ³• 1: pip å®‰è£ (æ¨è–¦)
```bash
# ç¢ºä¿åœ¨ Poetry ç’°å¢ƒä¸­
source .venv/bin/activate

# å®‰è£ FlashAttention
pip install flash-attn --no-build-isolation

# é©—è­‰å®‰è£
python -c "import flash_attn; print(flash_attn.__version__)"
```

#### æ–¹æ³• 2: å¾æºç¢¼ç·¨è­¯ (å¦‚æœ pip å¤±æ•—)
```bash
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
python setup.py install
```

#### å¸¸è¦‹å®‰è£å•é¡Œ

**å•é¡Œ 1**: ç·¨è­¯å¤±æ•— - CUDA ç‰ˆæœ¬ä¸åŒ¹é…
```bash
# è§£æ±ºæ–¹æ¡ˆ: ç¢ºèª PyTorch CUDA ç‰ˆæœ¬
python -c "import torch; print(torch.version.cuda)"

# ç¢ºä¿ç³»çµ± CUDA ç‰ˆæœ¬èˆ‡ PyTorch ä¸€è‡´
```

**å•é¡Œ 2**: è¨˜æ†¶é«”ä¸è¶³ - ç·¨è­¯éç¨‹ OOM
```bash
# è§£æ±ºæ–¹æ¡ˆ: é™åˆ¶ä¸¦è¡Œç·¨è­¯
MAX_JOBS=2 pip install flash-attn --no-build-isolation
```

**å•é¡Œ 3**: GPU ä¸æ”¯æ´
```bash
# æª¢æŸ¥ GPU compute capability
python -c "import torch; print(torch.cuda.get_device_capability())"

# FlashAttention éœ€è¦ â‰¥ 7.5 (Turing æ¶æ§‹ä»¥ä¸Š)
```

---

## ğŸ’¡ å­¸ç¿’è·¯å¾‘å»ºè­°

### æ¨è–¦å­¸ç¿’é †åº
```
Day 1 (2-3å°æ™‚):
â”œâ”€â”€ é–±è®€ç†è«–éƒ¨åˆ† (README.md + 1.3-Optimization_and_Alignment.md)
â””â”€â”€ 01-Setup_and_Comparison.ipynb (ç’°å¢ƒè¨­ç½® + åŸºç¤å°æ¯”)

Day 2 (2-3å°æ™‚):
â”œâ”€â”€ 02-FlashAttention_Demo.ipynb (å¯¦æˆ°é›†æˆ)
â””â”€â”€ 03-Long_Sequence_Training.ipynb (é•·åºåˆ—è¨“ç·´)

Day 3 (1-2å°æ™‚):
â”œâ”€â”€ 04-Performance_Analysis.ipynb (æ€§èƒ½åˆ†æ)
â””â”€â”€ ç¶œåˆç·´ç¿’: åœ¨è‡ªå·±çš„é …ç›®ä¸­æ‡‰ç”¨ FlashAttention
```

### é€²éšå­¸ç¿’è·¯å¾‘
1. **ç ”è®€è«–æ–‡**: FlashAttention åŸå§‹è«–æ–‡èˆ‡ v2 æ”¹é€²
2. **æºç¢¼é–±è®€**: flash-attention GitHub å¯¦ç¾ç´°ç¯€
3. **CUDA å„ªåŒ–**: ç†è§£ CUDA kernel å„ªåŒ–æŠ€å·§
4. **è®Šé«”æ¢ç´¢**: FlashAttention-2, PagedAttention, xFormers

---

## ğŸ“ˆ æ€§èƒ½å°æ¯”ç¸½è¦½

### é€Ÿåº¦å°æ¯” (GPT-2 Medium, Batch Size=4)

| åºåˆ—é•·åº¦ | æ¨™æº– Attention | FlashAttention v1 | FlashAttention v2 | åŠ é€Ÿæ¯” (v2) |
|---------|---------------|------------------|------------------|------------|
| 512 | 18ms | 10ms | 8ms | 2.3x |
| 1024 | 65ms | 25ms | 18ms | 3.6x |
| 2048 | 250ms | 60ms | 40ms | 6.3x |
| 4096 | OOM | 180ms | 120ms | ~8x+ |
| 8192 | OOM | 650ms | 400ms | ~15x+ |

### è¨˜æ†¶é«”å°æ¯” (GPT-2 Medium)

| åºåˆ—é•·åº¦ | æ¨™æº– Attention | FlashAttention | è¨˜æ†¶é«”ç¯€çœ |
|---------|---------------|---------------|-----------|
| 512 | 2.1GB | 1.8GB | 14% |
| 1024 | 4.5GB | 2.5GB | 44% |
| 2048 | 12GB | 4.2GB | 65% |
| 4096 | OOM (>24GB) | 8.5GB | >65% |
| 8192 | OOM | 18GB | N/A |

---

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. FlashAttention å®‰è£å¤±æ•—
**ç—‡ç‹€**: `pip install flash-attn` å ±éŒ¯

**è¨ºæ–·æ­¥é©Ÿ**:
```bash
# 1. æª¢æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version

# 2. æª¢æŸ¥ PyTorch CUDA ç‰ˆæœ¬
python -c "import torch; print(torch.version.cuda)"

# 3. æª¢æŸ¥ GPU compute capability
python -c "import torch; print(torch.cuda.get_device_capability())"
```

**è§£æ±ºæ–¹æ¡ˆ**:
- CUDA ç‰ˆæœ¬ä¸åŒ¹é…: é‡æ–°å®‰è£åŒ¹é…çš„ PyTorch
- GPU ä¸æ”¯æ´: éœ€è¦ compute capability â‰¥ 7.5
- ç·¨è­¯è³‡æºä¸è¶³: `MAX_JOBS=2 pip install flash-attn`

#### 2. RuntimeError: FlashAttention only supports ...
**ç—‡ç‹€**: é‹è¡Œæ™‚å ±æ ¼å¼éŒ¯èª¤

**åŸå› **: FlashAttention å°è¼¸å…¥æ ¼å¼æœ‰ç‰¹å®šè¦æ±‚
```python
# âŒ éŒ¯èª¤æ ¼å¼
Q = torch.randn(batch, heads, seq_len, head_dim)

# âœ… æ­£ç¢ºæ ¼å¼
Q = torch.randn(batch, seq_len, heads, head_dim)
```

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# è½‰æ›æ ¼å¼
Q = Q.transpose(1, 2)  # [B, H, N, D] â†’ [B, N, H, D]
```

#### 3. ç²¾åº¦å·®ç•°
**ç—‡ç‹€**: FlashAttention çµæœèˆ‡æ¨™æº– attention ç•¥æœ‰ä¸åŒ

**åŸå› **: æµ®é»é‹ç®—é †åºå·®ç•°
```python
# é©—è­‰ç²¾åº¦
output_std, _ = standard_attention(Q, K, V)
output_flash = flash_attn_func(Q, K, V)

diff = (output_std - output_flash).abs().max()
print(f"æœ€å¤§å·®ç•°: {diff:.6f}")  # æ‡‰è©² < 1e-3
```

**æ­£å¸¸ç¯„åœ**: å·®ç•° < 1e-3 (FP16), < 1e-5 (FP32)

#### 4. OOM éŒ¯èª¤
**ç—‡ç‹€**: ä½¿ç”¨ FlashAttention ä»ç„¶ OOM

**å¯èƒ½åŸå› **:
- æ‰¹æ¬¡å¤§å°éå¤§
- å…¶ä»–å±¤å ç”¨éå¤šè¨˜æ†¶é«”
- æ¢¯åº¦ç´¯ç©æœªæ­£ç¢ºé…ç½®

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# æ¸›å°æ‰¹æ¬¡
batch_size = 1

# å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
model.gradient_checkpointing_enable()

# ä½¿ç”¨æ··åˆç²¾åº¦
from torch.cuda.amp import autocast
with autocast(dtype=torch.float16):
    output = model(**batch)
```

---

## ğŸ“š å»¶ä¼¸å­¸ç¿’è³‡æº

### è«–æ–‡é–±è®€
- **FlashAttention**: [Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135) (NeurIPS 2022)
- **FlashAttention-2**: [Faster Attention with Better Parallelism](https://arxiv.org/abs/2307.08691) (2023)
- **Self-Attention Does Not Need O(nÂ²) Memory**: ç†è«–åŸºç¤è«–æ–‡

### é–‹æºå¯¦ç¾
- [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention): å®˜æ–¹å¯¦ç¾
- [HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention): ç ”ç©¶ç‰ˆæœ¬
- [xFormers](https://github.com/facebookresearch/xformers): Meta çš„è¨˜æ†¶é«”é«˜æ•ˆ Transformers

### ç›¸é—œæŠ€è¡“
- **PagedAttention** (vLLM): KV Cache å„ªåŒ–
- **Multi-Query Attention** (MQA): æ¸›å°‘ KV heads
- **Grouped-Query Attention** (GQA): MQA èˆ‡ MHA æŠ˜è¡·
- **Linear Attention**: ç·šæ€§è¤‡é›œåº¦è¿‘ä¼¼

### å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹
- **GPT-4**: ä½¿ç”¨ FlashAttention è¨“ç·´
- **Llama 2**: å®˜æ–¹å¯¦ç¾æ”¯æ´ FlashAttention
- **MPT**: MosaicML çš„é è¨“ç·´æ¨¡å‹
- **Falcon**: TII çš„é–‹æº LLM

---

## ğŸ“ å­¸ç¿’æª¢æŸ¥æ¸…å–®

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œæ‚¨æ‡‰è©²èƒ½å¤ :

### ç†è«–ç†è§£
- [ ] è§£é‡‹æ¨™æº– attention çš„è¨˜æ†¶é«”ç“¶é ¸
- [ ] ç†è§£ GPU è¨˜æ†¶é«”éšå±¤ (HBM vs SRAM)
- [ ] èªªæ˜ FlashAttention çš„æ ¸å¿ƒç®—æ³•åŸç†
- [ ] å°æ¯” FlashAttention v1 vs v2 çš„æ”¹é€²
- [ ] ç†è§£ IO æ„ŸçŸ¥ç®—æ³•è¨­è¨ˆæ€æƒ³

### å¯¦ä½œæŠ€èƒ½
- [ ] å®‰è£ä¸¦é©—è­‰ FlashAttention
- [ ] å¯¦ç¾æ¨™æº– attention æ©Ÿåˆ¶
- [ ] åœ¨æ¨¡å‹ä¸­é›†æˆ FlashAttention
- [ ] è¨“ç·´é•·åºåˆ—æ¨¡å‹ (>4K tokens)
- [ ] åˆ†æ FlashAttention æ€§èƒ½ç‰¹å¾µ

### æ‡‰ç”¨èƒ½åŠ›
- [ ] ç‚ºé …ç›®é¸æ“‡åˆé©çš„ attention æ©Ÿåˆ¶
- [ ] å„ªåŒ–é•·åºåˆ—è¨“ç·´é…ç½®
- [ ] è¨ºæ–·èˆ‡è§£æ±ºå¸¸è¦‹å•é¡Œ
- [ ] è©•ä¼° FlashAttention çš„æ•ˆç›Š

---

## ğŸš€ ä¸‹ä¸€æ­¥å­¸ç¿’

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œå»ºè­°ç¹¼çºŒ:

1. **Lab-1.6: Efficient Attention (MQA/GQA)**
   - Multi-Query Attention åŸç†
   - Grouped-Query Attention å¯¦ä½œ
   - KV Cache å„ªåŒ–æŠ€è¡“

2. **æ¨ç†å„ªåŒ–**
   - vLLM èˆ‡ PagedAttention
   - Speculative Decoding
   - Continuous Batching

3. **ç”Ÿç”¢éƒ¨ç½²**
   - æ¨¡å‹æœå‹™åŒ–
   - æ¨ç†æ€§èƒ½èª¿å„ª
   - æˆæœ¬å„ªåŒ–ç­–ç•¥

---

**å¯¦é©—å®¤ç‹€æ…‹**: ğŸ”„ é–‹ç™¼ä¸­
**æœ€å¾Œæ›´æ–°**: 2025-10-08
**ç¶­è­·è€…**: LLM æ•™å­¸å°ˆæ¡ˆåœ˜éšŠ

**ç›¸é—œæ–‡ä»¶**:
- ç†è«–: `01-Theory/1.3-Optimization_and_Alignment.md` (FlashAttention ç« ç¯€)
- å‰ç½®å¯¦é©—: `Lab-1.4-Training_Optimization_Basics`
- å¾ŒçºŒå¯¦é©—: `Lab-1.6-Efficient_Attention`
