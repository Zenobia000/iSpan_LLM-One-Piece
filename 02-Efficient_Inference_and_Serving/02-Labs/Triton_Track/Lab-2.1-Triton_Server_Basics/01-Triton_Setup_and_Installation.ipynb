{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1 Part 1: Triton Inference Server è¨­ç½®èˆ‡å®‰è£\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£ Triton Inference Server æ¶æ§‹\n",
    "- å®‰è£å’Œé…ç½® Triton Server\n",
    "- é©—è­‰åŸºç¤æ¨ç†åŠŸèƒ½\n",
    "- äº†è§£ REST å’Œ gRPC API\n",
    "\n",
    "## â±ï¸ é ä¼°æ™‚é–“: 60-90 åˆ†é˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒé©—è­‰èˆ‡æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Triton Server ç’°å¢ƒæª¢æŸ¥\n",
      "==================================================\n",
      "Python ç‰ˆæœ¬: 3.10.12\n",
      "PyTorch ç‰ˆæœ¬: 2.8.0+cu128\n",
      "CUDA å¯ç”¨: True\n",
      "CUDA ç‰ˆæœ¬: 12.8\n",
      "GPU æ•¸é‡: 1\n",
      "  GPU 0: NVIDIA RTX 2000 Ada Generation (16.7 GB)\n",
      "    è¨ˆç®—èƒ½åŠ›: 8.9\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥ç³»çµ±ç’°å¢ƒ\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ” Triton Server ç’°å¢ƒæª¢æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åŸºç¤ç’°å¢ƒ\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        memory_gb = gpu.total_memory / 1e9\n",
    "        print(f\"  GPU {i}: {gpu.name} ({memory_gb:.1f} GB)\")\n",
    "        print(f\"    è¨ˆç®—èƒ½åŠ›: {gpu.major}.{gpu.minor}\")\n",
    "else:\n",
    "    print(\"âš ï¸ è­¦å‘Š: æœªæª¢æ¸¬åˆ° CUDA GPU\")\n",
    "    print(\"Triton Server éœ€è¦ GPU ä»¥ç²å¾—æœ€ä½³æ€§èƒ½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Docker: Docker version 28.4.0, build d8eb465\n",
      "âœ… Docker æ¬Šé™æ­£å¸¸\n",
      "âœ… Docker GPU æ”¯æ´: æª¢æ¸¬åˆ° 1 å€‹ GPU\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥ Docker ç’°å¢ƒ\n",
    "def check_docker():\n",
    "    \"\"\"æª¢æŸ¥ Docker æ˜¯å¦å®‰è£ä¸¦æ”¯æ´ GPU\"\"\"\n",
    "    try:\n",
    "        # æª¢æŸ¥ Docker ç‰ˆæœ¬\n",
    "        result = subprocess.run(['docker', '--version'], \n",
    "                               capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Docker: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(\"âŒ Docker æœªå®‰è£\")\n",
    "            return False\n",
    "            \n",
    "        # æª¢æŸ¥ Docker æ¬Šé™\n",
    "        result = subprocess.run(['docker', 'ps'], \n",
    "                               capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Docker æ¬Šé™æ­£å¸¸\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Docker æ¬Šé™å•é¡Œï¼Œå¯èƒ½éœ€è¦ sudo\")\n",
    "            \n",
    "        # æª¢æŸ¥ NVIDIA Container Toolkit\n",
    "        result = subprocess.run(['docker', 'run', '--rm', '--gpus', 'all', \n",
    "                               'nvidia/cuda:11.0.3-base-ubuntu20.04', \n",
    "                               'nvidia-smi', '--list-gpus'], \n",
    "                               capture_output=True, text=True, timeout=30)\n",
    "        if result.returncode == 0:\n",
    "            gpu_count = len(result.stdout.strip().split('\\n'))\n",
    "            print(f\"âœ… Docker GPU æ”¯æ´: æª¢æ¸¬åˆ° {gpu_count} å€‹ GPU\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Docker GPU æ”¯æ´æœªé…ç½®\")\n",
    "            print(\"è«‹å®‰è£ NVIDIA Container Toolkit\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Docker GPU æª¢æŸ¥è¶…æ™‚\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Docker å‘½ä»¤æœªæ‰¾åˆ°\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Docker æª¢æŸ¥éŒ¯èª¤: {e}\")\n",
    "        return False\n",
    "\n",
    "docker_ready = check_docker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Triton Server å®‰è£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ å®‰è£ Triton å®¢æˆ¶ç«¯åº«...\n",
      "âŒ tritonclient æœªå®‰è£\n",
      "\n",
      "å®‰è£å‘½ä»¤:\n",
      "  pip install tritonclient[all]\n",
      "\n",
      "è«‹åœ¨çµ‚ç«¯åŸ·è¡Œä¸Šè¿°å‘½ä»¤å¾Œé‡æ–°é‹è¡Œæ­¤ cell\n",
      "âŒ Triton Server æ˜ åƒæœªæ‰¾åˆ°\n",
      "\n",
      "ä¸‹è¼‰å‘½ä»¤:\n",
      "  docker pull nvcr.io/nvidia/tritonserver:25.05-py3\n",
      "\n",
      "â³ ä¸‹è¼‰ Triton Server æ˜ åƒ (é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)...\n",
      "\n",
      "åœ¨çµ‚ç«¯åŸ·è¡Œ:\n",
      " docker pull nvcr.io/nvidia/tritonserver:25.05-py3\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£ Triton å®¢æˆ¶ç«¯åº«\n",
    "print(\"ğŸ“¦ å®‰è£ Triton å®¢æˆ¶ç«¯åº«...\")\n",
    "\n",
    "try:\n",
    "    import tritonclient.http as httpclient\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    print(\"âœ… tritonclient å·²å®‰è£\")\n",
    "except ImportError:\n",
    "    print(\"âŒ tritonclient æœªå®‰è£\")\n",
    "    print(\"\\nå®‰è£å‘½ä»¤:\")\n",
    "    print(\"  pip install tritonclient[all]\")\n",
    "    print(\"\\nè«‹åœ¨çµ‚ç«¯åŸ·è¡Œä¸Šè¿°å‘½ä»¤å¾Œé‡æ–°é‹è¡Œæ­¤ cell\")\n",
    "\n",
    "# æª¢æŸ¥ Triton Server Docker æ˜ åƒ\n",
    "def check_triton_image():\n",
    "    \"\"\"æª¢æŸ¥ Triton Server Docker æ˜ åƒ\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['docker', 'images', 'nvcr.io/nvidia/tritonserver'], \n",
    "                               capture_output=True, text=True)\n",
    "        \n",
    "        if 'tritonserver' in result.stdout:\n",
    "            print(\"âœ… Triton Server æ˜ åƒå·²å­˜åœ¨\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Triton Server æ˜ åƒæœªæ‰¾åˆ°\")\n",
    "            print(\"\\nä¸‹è¼‰å‘½ä»¤:\")\n",
    "            print(\"docker pull nvcr.io/nvidia/tritonserver:25.05-py3\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æª¢æŸ¥æ˜ åƒæ™‚å‡ºéŒ¯: {e}\")\n",
    "        return False\n",
    "\n",
    "triton_image_ready = check_triton_image()\n",
    "\n",
    "if not triton_image_ready:\n",
    "    print(\"\\nâ³ ä¸‹è¼‰ Triton Server æ˜ åƒ (é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)...\")\n",
    "    print(\"\\nåœ¨çµ‚ç«¯åŸ·è¡Œ:\")\n",
    "    print(\" docker pull nvcr.io/nvidia/tritonserver:25.05-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. æ¨¡å‹å€‰åº«æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ å‰µå»ºæ¨¡å‹å€‰åº«: /home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/02-Efficient_Inference_and_Serving/02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics/model_repository\n",
      "  âœ… model_repository\n",
      "  âœ… model_repository/simple_pytorch_model\n",
      "  âœ… model_repository/simple_pytorch_model/1\n",
      "  âœ… model_repository/simple_pytorch_model/config\n",
      "\n",
      "ğŸ“‹ æ¨™æº–æ¨¡å‹å€‰åº«çµæ§‹:\n",
      "\n",
      "    model_repository/\n",
      "    â””â”€â”€ simple_pytorch_model/          # æ¨¡å‹åç¨±\n",
      "        â”œâ”€â”€ config.pbtxt               # æ¨¡å‹é…ç½®æ–‡ä»¶\n",
      "        â”œâ”€â”€ 1/                         # ç‰ˆæœ¬è™Ÿ\n",
      "        â”‚   â””â”€â”€ model.pt               # æ¨¡å‹æ¬Šé‡æ–‡ä»¶\n",
      "        â””â”€â”€ config/                    # é¡å¤–é…ç½® (å¯é¸)\n",
      "    \n",
      "\n",
      "ğŸ’¡ Triton æ¨¡å‹å€‰åº«è¨­è¨ˆåŸå‰‡:\n",
      "â€¢ æ¯å€‹æ¨¡å‹æœ‰ç¨ç«‹çš„ç›®éŒ„\n",
      "â€¢ ç‰ˆæœ¬è™Ÿç›®éŒ„åŒ…å«æ¨¡å‹æª”æ¡ˆ\n",
      "â€¢ config.pbtxt å®šç¾©æ¨¡å‹é…ç½®\n",
      "â€¢ æ”¯æ´å¤šç‰ˆæœ¬ä¸¦è¡Œéƒ¨ç½²\n"
     ]
    }
   ],
   "source": [
    "# å‰µå»º Triton æ¨¡å‹å€‰åº«çµæ§‹\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# è¨­ç½®å·¥ä½œç›®éŒ„\n",
    "work_dir = Path.cwd()\n",
    "model_repo_dir = work_dir / \"model_repository\"\n",
    "\n",
    "print(f\"ğŸ“ å‰µå»ºæ¨¡å‹å€‰åº«: {model_repo_dir}\")\n",
    "\n",
    "# å‰µå»ºæ¨¡å‹å€‰åº«çµæ§‹\n",
    "def create_model_repository():\n",
    "    \"\"\"\n",
    "    å‰µå»ºæ¨™æº–çš„ Triton æ¨¡å‹å€‰åº«çµæ§‹\n",
    "    \"\"\"\n",
    "    # åŸºç¤ç›®éŒ„çµæ§‹\n",
    "    directories = [\n",
    "        \"model_repository\",\n",
    "        \"model_repository/simple_pytorch_model\",\n",
    "        \"model_repository/simple_pytorch_model/1\",\n",
    "        \"model_repository/simple_pytorch_model/config\",\n",
    "    ]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"  âœ… {dir_path}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ æ¨™æº–æ¨¡å‹å€‰åº«çµæ§‹:\")\n",
    "    print(\"\"\"\n",
    "    model_repository/\n",
    "    â””â”€â”€ simple_pytorch_model/          # æ¨¡å‹åç¨±\n",
    "        â”œâ”€â”€ config.pbtxt               # æ¨¡å‹é…ç½®æ–‡ä»¶\n",
    "        â”œâ”€â”€ 1/                         # ç‰ˆæœ¬è™Ÿ\n",
    "        â”‚   â””â”€â”€ model.pt               # æ¨¡å‹æ¬Šé‡æ–‡ä»¶\n",
    "        â””â”€â”€ config/                    # é¡å¤–é…ç½® (å¯é¸)\n",
    "    \"\"\")\n",
    "\n",
    "create_model_repository()\n",
    "\n",
    "print(\"\\nğŸ’¡ Triton æ¨¡å‹å€‰åº«è¨­è¨ˆåŸå‰‡:\")\n",
    "print(\"â€¢ æ¯å€‹æ¨¡å‹æœ‰ç¨ç«‹çš„ç›®éŒ„\")\n",
    "print(\"â€¢ ç‰ˆæœ¬è™Ÿç›®éŒ„åŒ…å«æ¨¡å‹æª”æ¡ˆ\")\n",
    "print(\"â€¢ config.pbtxt å®šç¾©æ¨¡å‹é…ç½®\")\n",
    "print(\"â€¢ æ”¯æ´å¤šç‰ˆæœ¬ä¸¦è¡Œéƒ¨ç½²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– å‰µå»ºæ¸¬è©¦ç”¨ PyTorch æ¨¡å‹...\n",
      "ç¤ºä¾‹è¼¸å…¥å½¢ç‹€: torch.Size([1, 10])\n",
      "æ¨¡å‹è¼¸å‡ºå½¢ç‹€: torch.Size([1, 3])\n",
      "è¼¸å‡ºæ©Ÿç‡: [[0.398 0.299 0.303]]\n",
      "âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: /home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/02-Efficient_Inference_and_Serving/02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics/model_repository/simple_pytorch_model/1/model.pt\n",
      "âœ… æ¨¡å‹å¤§å°: 0.53 MB\n"
     ]
    }
   ],
   "source": [
    "# å‰µå»ºç°¡å–®çš„ PyTorch æ¨¡å‹ç”¨æ–¼æ¸¬è©¦\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SimpleTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    ç°¡å–®çš„æ–‡æœ¬åˆ†é¡æ¨¡å‹ï¼Œç”¨æ–¼ Triton éƒ¨ç½²æ¸¬è©¦\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=1000, embed_dim=128, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        x = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]\n",
    "        x = torch.mean(x, dim=1)       # [batch_size, embed_dim] - ç°¡å–®çš„å¹³å‡æ± åŒ–\n",
    "        x = torch.relu(self.fc1(x))    # [batch_size, 64]\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)           # [batch_size, num_classes]\n",
    "        return torch.softmax(output, dim=-1)\n",
    "\n",
    "# å‰µå»ºå’Œä¿å­˜æ¨¡å‹\n",
    "print(\"ğŸ¤– å‰µå»ºæ¸¬è©¦ç”¨ PyTorch æ¨¡å‹...\")\n",
    "\n",
    "model = SimpleTextClassifier(vocab_size=1000, embed_dim=128, num_classes=3)\n",
    "model.eval()\n",
    "\n",
    "# å‰µå»ºç¤ºä¾‹è¼¸å…¥\n",
    "example_input = torch.randint(0, 1000, (1, 10))  # batch=1, seq_l en=10\n",
    "print(f\"ç¤ºä¾‹è¼¸å…¥å½¢ç‹€: {example_input.shape}\")\n",
    "\n",
    "# æ¸¬è©¦æ¨¡å‹\n",
    "with torch.no_grad():\n",
    "    output = model(example_input)\n",
    "    print(f\"æ¨¡å‹è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "    print(f\"è¼¸å‡ºæ©Ÿç‡: {output.numpy().round(3)}\")\n",
    "\n",
    "# è½‰æ›ç‚º TorchScript (Triton æ¨è–¦æ ¼å¼)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹åˆ°å€‰åº«\n",
    "model_path = model_repo_dir / \"simple_pytorch_model\" / \"1\" / \"model.pt\"\n",
    "traced_model.save(str(model_path))\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}\")\n",
    "print(f\"âœ… æ¨¡å‹å¤§å°: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹é…ç½®æ–‡ä»¶å·²å‰µå»º: /home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/02-Efficient_Inference_and_Serving/02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics/model_repository/simple_pytorch_model/config.pbtxt\n",
      "\n",
      "ğŸ“‚ æ¨¡å‹å€‰åº«çµæ§‹:\n",
      "model_repository/\n",
      "  simple_pytorch_model/\n",
      "    config.pbtxt (0.6KB)\n",
      "    1/\n",
      "      model.pt (543.6KB)\n",
      "    config/\n"
     ]
    }
   ],
   "source": [
    "# å‰µå»º Triton æ¨¡å‹é…ç½®æ–‡ä»¶\n",
    "config_content = '''\n",
    "name: \"simple_pytorch_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ 10 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "# å‹•æ…‹æ‰¹æ¬¡è™•ç†é…ç½®\n",
    "dynamic_batching {\n",
    "  # åå¥½çš„æ‰¹æ¬¡å¤§å°\n",
    "  preferred_batch_size: [ 4, 8, 16 ]\n",
    "  \n",
    "  # æœ€å¤§ç­‰å¾…æ™‚é–“ (å¾®ç§’)\n",
    "  max_queue_delay_microseconds: 5000\n",
    "  \n",
    "  # ä¿æŒé †åº\n",
    "  preserve_ordering: true\n",
    "}\n",
    "\n",
    "# å¯¦ä¾‹çµ„é…ç½® (å¯é¸)\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]\n",
    "\n",
    "# ç‰ˆæœ¬ç­–ç•¥\n",
    "version_policy: { latest { num_versions: 1}}\n",
    "'''\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_path = model_repo_dir / \"simple_pytorch_model\" / \"config.pbtxt\"\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_content.strip())\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹é…ç½®æ–‡ä»¶å·²å‰µå»º: {config_path}\")\n",
    "\n",
    "# æª¢æŸ¥æ¨¡å‹å€‰åº«çµæ§‹\n",
    "print(f\"\\nğŸ“‚ æ¨¡å‹å€‰åº«çµæ§‹:\")\n",
    "for root, dirs, files in os.walk(model_repo_dir):\n",
    "    level = root.replace(str(model_repo_dir), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        size = os.path.getsize(os.path.join(root, file))\n",
    "        size_str = f\"{size/1024:.1f}KB\" if size < 1024*1024 else f\"{size/1024/1024:.1f}MB\"\n",
    "        print(f\"{subindent}{file} ({size_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. å•Ÿå‹• Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹• Triton Server...\n",
      "æ¨¡å‹å€‰åº«è·¯å¾‘: /home/os-sunnie.gd.weng/python_workstation/side-project/iSpan_LLM-One-Piece/02-Efficient_Inference_and_Serving/02-Labs/Triton_Track/Lab-2.1-Triton_Server_Basics/model_repository\n",
      "âœ… Triton Server å®¹å™¨å·²å•Ÿå‹•: 0c8e6a7be1fd\n",
      "â³ ç­‰å¾…æœå‹™å°±ç·’...\n",
      "âœ… Triton Server å°±ç·’! (è€—æ™‚ 2 ç§’)\n",
      "\n",
      "ğŸŒ Triton Server ç«¯é»:\n",
      "  â€¢ HTTP API: http://localhost:8000\n",
      "  â€¢ gRPC API: localhost:8001\n",
      "  â€¢ Metrics: http://localhost:8002/metrics\n",
      "  â€¢ Health: http://localhost:8000/v2/health/ready\n"
     ]
    }
   ],
   "source": [
    "# å•Ÿå‹• Triton Server (Docker å®¹å™¨)\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def start_triton_server():\n",
    "    \"\"\"\n",
    "    å•Ÿå‹• Triton Inference Server\n",
    "    \"\"\"\n",
    "    # Triton Server Docker å‘½ä»¤\n",
    "    model_repo_path = str(model_repo_dir.absolute())\n",
    "    \n",
    "    docker_cmd = [\n",
    "        'docker', 'run', '--rm', '-d',\n",
    "        '--name', 'triton-server',\n",
    "        '--gpus', 'all',\n",
    "        '-p', '8000:8000',  # HTTP port\n",
    "        '-p', '8001:8001',  # gRPC port  \n",
    "        '-p', '8002:8002',  # Metrics port\n",
    "        '-v', f'{model_repo_path}:/models',\n",
    "        'nvcr.io/nvidia/tritonserver:25.05-py3',\n",
    "        'tritonserver',\n",
    "        '--model-repository=/models',\n",
    "        '--strict-model-config=false',\n",
    "        '--log-verbose=1'\n",
    "    ]\n",
    "\n",
    "    print(\"ğŸš€ å•Ÿå‹• Triton Server...\")\n",
    "    print(f\"æ¨¡å‹å€‰åº«è·¯å¾‘: {model_repo_path}\")\n",
    "    \n",
    "    try:\n",
    "        # åœæ­¢ç¾æœ‰å®¹å™¨ (å¦‚æœå­˜åœ¨)\n",
    "        subprocess.run(['docker', 'stop', 'triton-server'], \n",
    "                      capture_output=True, text=True)\n",
    "        \n",
    "        # å•Ÿå‹•æ–°å®¹å™¨\n",
    "        result = subprocess.run(docker_cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            container_id = result.stdout.strip()\n",
    "            print(f\"âœ… Triton Server å®¹å™¨å·²å•Ÿå‹•: {container_id[:12]}\")\n",
    "            \n",
    "            # ç­‰å¾…æœå‹™å•Ÿå‹•\n",
    "            print(\"â³ ç­‰å¾…æœå‹™å°±ç·’...\")\n",
    "            \n",
    "            for attempt in range(30):  # æœ€å¤šç­‰å¾… 30 ç§’\n",
    "                time.sleep(1)\n",
    "                try:\n",
    "                    response = requests.get('http://localhost:8000/v2/health/ready', \n",
    "                                          timeout=2)\n",
    "                    if response.status_code == 200:\n",
    "                        print(f\"âœ… Triton Server å°±ç·’! (è€—æ™‚ {attempt+1} ç§’)\")\n",
    "                        return True\n",
    "                except requests.exceptions.RequestException:\n",
    "                    pass\n",
    "                    \n",
    "                if attempt % 5 == 4:\n",
    "                    print(f\"   ä»åœ¨å•Ÿå‹•ä¸­... ({attempt+1}/30)\")\n",
    "            \n",
    "            print(\"âŒ Triton Server å•Ÿå‹•è¶…æ™‚\")\n",
    "            return False\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ å•Ÿå‹•å¤±æ•—: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å•Ÿå‹•ç•°å¸¸: {e}\")\n",
    "        return False\n",
    "\n",
    "server_ready = start_triton_server()\n",
    "\n",
    "if server_ready:\n",
    "    print(\"\\nğŸŒ Triton Server ç«¯é»:\")\n",
    "    print(\"  â€¢ HTTP API: http://localhost:8000\")\n",
    "    print(\"  â€¢ gRPC API: localhost:8001\")\n",
    "    print(\"  â€¢ Metrics: http://localhost:8002/metrics\")\n",
    "    print(\"  â€¢ Health: http://localhost:8000/v2/health/ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Triton API åŸºç¤æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Triton Server åŸºç¤æ¸¬è©¦\n",
      "========================================\n",
      "âœ… Triton Server å¥åº·æª¢æŸ¥é€šé\n",
      "âœ… æœå‹™å™¨ç‰ˆæœ¬: 2.58.0\n",
      "âœ… æœå‹™å™¨åç¨±: triton\n",
      "ğŸ“‹ å·²è¼‰å…¥æ¨¡å‹æ•¸é‡: 1\n",
      "  â€¢ simple_pytorch_model v1: READY\n"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦ Triton Server HTTP API\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "TRITON_HTTP_URL = \"http://localhost:8000\"\n",
    "\n",
    "def test_server_health():\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦æœå‹™å™¨å¥åº·ç‹€æ…‹\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # æª¢æŸ¥æœå‹™å™¨å¥åº·\n",
    "        health_url = f\"{TRITON_HTTP_URL}/v2/health/ready\"\n",
    "        response = requests.get(health_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ… Triton Server å¥åº·æª¢æŸ¥é€šé\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ å¥åº·æª¢æŸ¥å¤±æ•—: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¥åº·æª¢æŸ¥ç•°å¸¸: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_server_metadata():\n",
    "    \"\"\"\n",
    "    ç²å–æœå‹™å™¨å…ƒæ•¸æ“š\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metadata_url = f\"{TRITON_HTTP_URL}/v2\"\n",
    "        response = requests.get(metadata_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            metadata = response.json()\n",
    "            print(f\"âœ… æœå‹™å™¨ç‰ˆæœ¬: {metadata.get('version', 'Unknown')}\")\n",
    "            print(f\"âœ… æœå‹™å™¨åç¨±: {metadata.get('name', 'Unknown')}\")\n",
    "            return metadata\n",
    "        else:\n",
    "            print(f\"âŒ ç²å–å…ƒæ•¸æ“šå¤±æ•—: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å…ƒæ•¸æ“šæŸ¥è©¢ç•°å¸¸: {e}\")\n",
    "        return None\n",
    "\n",
    "def list_models():\n",
    "    try:\n",
    "        # ä½¿ç”¨æ­£ç¢ºçš„ç«¯é»å’Œæ–¹æ³•\n",
    "        models_url = f\"{TRITON_HTTP_URL}/v2/repository/index\"\n",
    "        response = requests.post(models_url)  # æ”¹ç‚º POST\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(f\"ğŸ“‹ å·²è¼‰å…¥æ¨¡å‹æ•¸é‡: {len(models)}\")\n",
    "\n",
    "            for model in models:\n",
    "                model_name = model['name']\n",
    "                model_version = model['version']\n",
    "                model_state = model['state']\n",
    "                print(f\"  â€¢ {model_name} v{model_version}: {model_state}\")\n",
    "\n",
    "            return models\n",
    "        else:\n",
    "            print(f\"âŒ æ¨¡å‹åˆ—è¡¨ç²å–å¤±æ•—: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹æŸ¥è©¢ç•°å¸¸: {e}\")\n",
    "        return None\n",
    "\n",
    "# åŸ·è¡ŒåŸºç¤æ¸¬è©¦\n",
    "print(\"ğŸ§ª Triton Server åŸºç¤æ¸¬è©¦\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if test_server_health():\n",
    "    metadata = get_server_metadata()\n",
    "    models = list_models()\n",
    "    \n",
    "    if not models:\n",
    "        print(\"\\nâš ï¸ æ²’æœ‰æ¨¡å‹è¢«è¼‰å…¥ï¼Œæª¢æŸ¥æ¨¡å‹å€‰åº«é…ç½®\")\n",
    "else:\n",
    "    print(\"\\nâŒ Triton Server æœªæ­£å¸¸é‹è¡Œ\")\n",
    "    print(\"è«‹æª¢æŸ¥ Docker å®¹å™¨ç‹€æ…‹: docker logs triton-server\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tritonclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtritonclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhttpclient\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtritonclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tritonclient'"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import tritonclient.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ tritonclient æœªå®‰è£\n",
      "å®‰è£å‘½ä»¤: pip install tritonclient[all]\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ Triton å®¢æˆ¶ç«¯é€²è¡Œæ¨ç†æ¸¬è©¦\n",
    "if 'tritonclient' in globals() or True:  # å‡è¨­å·²å®‰è£\n",
    "    try:\n",
    "        import tritonclient.http as httpclient\n",
    "        import tritonclient.utils as utils\n",
    "        \n",
    "        print(\"ğŸ¯ Triton å®¢æˆ¶ç«¯æ¨ç†æ¸¬è©¦\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # å‰µå»ºå®¢æˆ¶ç«¯\n",
    "        triton_client = httpclient.InferenceServerClient(\n",
    "            url='localhost:8000',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å°±ç·’\n",
    "        model_name = \"simple_pytorch_model\"\n",
    "        \n",
    "        if triton_client.is_model_ready(model_name):\n",
    "            print(f\"âœ… æ¨¡å‹ '{model_name}' å°±ç·’\")\n",
    "            \n",
    "            # ç²å–æ¨¡å‹å…ƒæ•¸æ“š\n",
    "            model_metadata = triton_client.get_model_metadata(model_name)\n",
    "            print(f\"âœ… æ¨¡å‹ç‰ˆæœ¬: {model_metadata.versions}\")\n",
    "            print(f\"âœ… æ¨¡å‹å¹³å°: {model_metadata.platform}\")\n",
    "            \n",
    "            # æº–å‚™æ¨ç†æ•¸æ“š\n",
    "            input_data = np.random.randint(0, 1000, (1, 10), dtype=np.int64)\n",
    "            print(f\"ğŸ“Š è¼¸å…¥æ•¸æ“šå½¢ç‹€: {input_data.shape}\")\n",
    "            \n",
    "            # å‰µå»ºæ¨ç†è«‹æ±‚\n",
    "            inputs = [\n",
    "                httpclient.InferInput(\"INPUT__0\", input_data.shape, \"INT64\")\n",
    "            ]\n",
    "            inputs[0].set_data_from_numpy(input_data)\n",
    "            \n",
    "            outputs = [\n",
    "                httpclient.InferRequestedOutput(\"OUTPUT__0\")\n",
    "            ]\n",
    "            \n",
    "            # åŸ·è¡Œæ¨ç†\n",
    "            start_time = time.time()\n",
    "            response = triton_client.infer(\n",
    "                model_name=model_name,\n",
    "                inputs=inputs,\n",
    "                outputs=outputs\n",
    "            )\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # ç²å–çµæœ\n",
    "            output_data = response.as_numpy(\"OUTPUT__0\")\n",
    "            \n",
    "            print(f\"\\nğŸ¯ æ¨ç†çµæœ:\")\n",
    "            print(f\"  è¼¸å…¥: {input_data.flatten()[:5]}... (é¡¯ç¤ºå‰5å€‹)\")\n",
    "            print(f\"  è¼¸å‡ºæ©Ÿç‡: {output_data.round(4)}\")\n",
    "            print(f\"  é æ¸¬é¡åˆ¥: {np.argmax(output_data)}\")\n",
    "            print(f\"  æ¨ç†æ™‚é–“: {inference_time*1000:.2f} ms\")\n",
    "            \n",
    "            print(\"\\nâœ… Triton æ¨ç†æ¸¬è©¦æˆåŠŸ!\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ æ¨¡å‹ '{model_name}' æœªå°±ç·’\")\n",
    "            print(\"æª¢æŸ¥æ¨¡å‹é…ç½®å’Œå€‰åº«çµæ§‹\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"âŒ tritonclient æœªå®‰è£\")\n",
    "        print(\"å®‰è£å‘½ä»¤: pip install tritonclient[all]\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨ç†æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "        print(\"\\nğŸ“‹ æ•…éšœæ’é™¤å»ºè­°:\")\n",
    "        print(\"1. æª¢æŸ¥ Triton Server æ˜¯å¦é‹è¡Œ: docker ps\")\n",
    "        print(\"2. æŸ¥çœ‹æœå‹™å™¨æ—¥èªŒ: docker logs triton-server\")\n",
    "        print(\"3. é©—è­‰æ¨¡å‹é…ç½®æ–‡ä»¶èªæ³•\")\n",
    "        print(\"4. ç¢ºèªæ¨¡å‹æ–‡ä»¶å­˜åœ¨ä¸”å¯è®€å–\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. REST API è©³ç´°æ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¢ç´¢ Triton REST API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def explore_triton_api():\n",
    "    \"\"\"\n",
    "    æ¢ç´¢ Triton Server REST API ç«¯é»\n",
    "    \"\"\"\n",
    "    base_url = \"http://localhost:8000\"\n",
    "    \n",
    "    endpoints = {\n",
    "        \"æœå‹™å™¨å…ƒæ•¸æ“š\": \"/v2\",\n",
    "        \"å¥åº·æª¢æŸ¥ (Live)\": \"/v2/health/live\",\n",
    "        \"å¥åº·æª¢æŸ¥ (Ready)\": \"/v2/health/ready\",\n",
    "        \"æ¨¡å‹åˆ—è¡¨\": \"/v2/models\",\n",
    "        \"æ¨¡å‹å…ƒæ•¸æ“š\": \"/v2/models/simple_pytorch_model\",\n",
    "        \"æ¨¡å‹é…ç½®\": \"/v2/models/simple_pytorch_model/config\",\n",
    "        \"æ¨¡å‹çµ±è¨ˆ\": \"/v2/models/simple_pytorch_model/stats\",\n",
    "        \"æœå‹™å™¨çµ±è¨ˆ\": \"/v2/models/stats\",\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸŒ Triton REST API æ¢ç´¢\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for description, endpoint in endpoints.items():\n",
    "        try:\n",
    "            url = base_url + endpoint\n",
    "            response = requests.get(url, timeout=5)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(f\"\\nâœ… {description}:\")\n",
    "                print(f\"   URL: {endpoint}\")\n",
    "                \n",
    "                data = response.json()\n",
    "                \n",
    "                # é¡¯ç¤ºé—œéµä¿¡æ¯\n",
    "                if \"version\" in data:\n",
    "                    print(f\"   ç‰ˆæœ¬: {data['version']}\")\n",
    "                    \n",
    "                if \"name\" in data:\n",
    "                    print(f\"   åç¨±: {data['name']}\")\n",
    "                    \n",
    "                if \"platform\" in data:\n",
    "                    print(f\"   å¹³å°: {data['platform']}\")\n",
    "                    \n",
    "                if \"max_batch_size\" in data:\n",
    "                    print(f\"   æœ€å¤§æ‰¹æ¬¡: {data['max_batch_size']}\")\n",
    "                    \n",
    "                # é¡¯ç¤ºéŸ¿æ‡‰å¤§å°\n",
    "                response_size = len(response.text)\n",
    "                print(f\"   éŸ¿æ‡‰å¤§å°: {response_size} bytes\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ {description}: HTTP {response.status_code}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ {description}: ç¶²è·¯éŒ¯èª¤ - {e}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âš ï¸ {description}: é JSON éŸ¿æ‡‰\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {description}: æœªçŸ¥éŒ¯èª¤ - {e}\")\n",
    "\n",
    "explore_triton_api()\n",
    "\n",
    "print(\"\\nğŸ“š API æ–‡æª”åƒè€ƒ:\")\n",
    "print(\"â€¢ Triton HTTP API: https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md\")\n",
    "print(\"â€¢ gRPC API: https://github.com/triton-inference-server/common/blob/main/protobuf/grpc_service.proto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹æ¬¡æ¨ç†æ¸¬è©¦\n",
    "def test_batch_inference():\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦ Triton å‹•æ…‹æ‰¹æ¬¡è™•ç†\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import tritonclient.http as httpclient\n",
    "        \n",
    "        triton_client = httpclient.InferenceServerClient(url='localhost:8000')\n",
    "        model_name = \"simple_pytorch_model\"\n",
    "        \n",
    "        # æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°\n",
    "        batch_sizes = [1, 4, 8, 16]\n",
    "        results = {}\n",
    "        \n",
    "        print(\"ğŸš€ æ‰¹æ¬¡æ¨ç†æ€§èƒ½æ¸¬è©¦\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # æº–å‚™æ‰¹æ¬¡æ•¸æ“š\n",
    "            batch_input = np.random.randint(0, 1000, (batch_size, 10), dtype=np.int64)\n",
    "            \n",
    "            # å‰µå»ºæ¨ç†è«‹æ±‚\n",
    "            inputs = [httpclient.InferInput(\"INPUT__0\", batch_input.shape, \"INT64\")]\n",
    "            inputs[0].set_data_from_numpy(batch_input)\n",
    "            \n",
    "            outputs = [httpclient.InferRequestedOutput(\"OUTPUT__0\")]\n",
    "            \n",
    "            # æ¸¬é‡æ¨ç†æ™‚é–“ (å¤šæ¬¡æ¸¬è©¦å–å¹³å‡)\n",
    "            times = []\n",
    "            for _ in range(5):\n",
    "                start = time.time()\n",
    "                response = triton_client.infer(\n",
    "                    model_name=model_name,\n",
    "                    inputs=inputs,\n",
    "                    outputs=outputs\n",
    "                )\n",
    "                times.append(time.time() - start)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            throughput = batch_size / avg_time\n",
    "            latency_per_sample = avg_time / batch_size * 1000  # ms\n",
    "            \n",
    "            results[batch_size] = {\n",
    "                'avg_time': avg_time,\n",
    "                'throughput': throughput,\n",
    "                'latency_per_sample': latency_per_sample\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ“Š æ‰¹æ¬¡å¤§å° {batch_size:2d}:\")\n",
    "            print(f\"   å¹³å‡æ™‚é–“: {avg_time*1000:.2f} ms\")\n",
    "            print(f\"   ååé‡: {throughput:.1f} samples/sec\")\n",
    "            print(f\"   å–®æ¨£æœ¬å»¶é²: {latency_per_sample:.2f} ms\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âŒ tritonclient æœªå®‰è£\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ‰¹æ¬¡æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "batch_results = test_batch_inference()\n",
    "\n",
    "if batch_results:\n",
    "    # å¯è¦–åŒ–æ‰¹æ¬¡æ€§èƒ½\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    batch_sizes = list(batch_results.keys())\n",
    "    throughputs = [batch_results[bs]['throughput'] for bs in batch_sizes]\n",
    "    latencies = [batch_results[bs]['latency_per_sample'] for bs in batch_sizes]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # ååé‡\n",
    "    ax1.plot(batch_sizes, throughputs, 'o-', linewidth=2, markersize=8, color='#4ECDC4')\n",
    "    ax1.set_xlabel('æ‰¹æ¬¡å¤§å°')\n",
    "    ax1.set_ylabel('ååé‡ (samples/sec)')\n",
    "    ax1.set_title('æ‰¹æ¬¡å¤§å° vs ååé‡')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bs, tp in zip(batch_sizes, throughputs):\n",
    "        ax1.annotate(f'{tp:.1f}', xy=(bs, tp), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center')\n",
    "    \n",
    "    # å»¶é²\n",
    "    ax2.plot(batch_sizes, latencies, 'o-', linewidth=2, markersize=8, color='#FF6B6B')\n",
    "    ax2.set_xlabel('æ‰¹æ¬¡å¤§å°')\n",
    "    ax2.set_ylabel('å–®æ¨£æœ¬å»¶é² (ms)')\n",
    "    ax2.set_title('æ‰¹æ¬¡å¤§å° vs å»¶é²')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bs, lat in zip(batch_sizes, latencies):\n",
    "        ax2.annotate(f'{lat:.1f}', xy=(bs, lat), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ æ€§èƒ½åˆ†æ:\")\n",
    "    max_throughput = max(throughputs)\n",
    "    min_latency = min(latencies)\n",
    "    optimal_batch = batch_sizes[throughputs.index(max_throughput)]\n",
    "    \n",
    "    print(f\"  æœ€é«˜ååé‡: {max_throughput:.1f} samples/sec (æ‰¹æ¬¡={optimal_batch})\")\n",
    "    print(f\"  æœ€ä½å»¶é²: {min_latency:.1f} ms (æ‰¹æ¬¡={batch_sizes[latencies.index(min_latency)]})\")\n",
    "    print(f\"  æ¨è–¦æ‰¹æ¬¡å¤§å°: {optimal_batch} (å¹³è¡¡æ€§èƒ½)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Triton vs å…¶ä»–æ¨ç†æ¡†æ¶å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°æ¯”æ¸¬è©¦ï¼šTriton vs PyTorch ç›´æ¥æ¨ç†\n",
    "def compare_with_pytorch():\n",
    "    \"\"\"\n",
    "    å°æ¯” Triton èˆ‡ç›´æ¥ PyTorch æ¨ç†çš„æ€§èƒ½\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ç›´æ¥è¼‰å…¥ PyTorch æ¨¡å‹\n",
    "        model_path = model_repo_dir / \"simple_pytorch_model\" / \"1\" / \"model.pt\"\n",
    "        pytorch_model = torch.jit.load(str(model_path))\n",
    "        pytorch_model.eval()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pytorch_model = pytorch_model.cuda()\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "        \n",
    "        print(f\"ğŸ”„ PyTorch vs Triton æ€§èƒ½å°æ¯”\")\n",
    "        print(f\"è¨­å‚™: {device}\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # æ¸¬è©¦ç›¸åŒçš„è¼¸å…¥\n",
    "        test_input = torch.randint(0, 1000, (8, 10), dtype=torch.int64)  # æ‰¹æ¬¡=8\n",
    "        \n",
    "        # PyTorch ç›´æ¥æ¨ç†\n",
    "        pytorch_times = []\n",
    "        for _ in range(10):\n",
    "            test_input_device = test_input.to(device)\n",
    "            \n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                pytorch_output = pytorch_model(test_input_device)\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()  # ç­‰å¾… GPU å®Œæˆ\n",
    "                \n",
    "            pytorch_times.append(time.time() - start)\n",
    "        \n",
    "        pytorch_avg = np.mean(pytorch_times)\n",
    "        pytorch_std = np.std(pytorch_times)\n",
    "        \n",
    "        print(f\"\\nğŸ”¥ PyTorch ç›´æ¥æ¨ç†:\")\n",
    "        print(f\"   å¹³å‡æ™‚é–“: {pytorch_avg*1000:.2f} Â± {pytorch_std*1000:.2f} ms\")\n",
    "        print(f\"   ååé‡: {8/pytorch_avg:.1f} samples/sec\")\n",
    "        \n",
    "        # Triton æ¨ç† (ä½¿ç”¨ä¹‹å‰çš„çµæœ)\n",
    "        if batch_results and 8 in batch_results:\n",
    "            triton_time = batch_results[8]['avg_time']\n",
    "            triton_throughput = batch_results[8]['throughput']\n",
    "            \n",
    "            print(f\"\\nâš¡ Triton Server æ¨ç†:\")\n",
    "            print(f\"   å¹³å‡æ™‚é–“: {triton_time*1000:.2f} ms\")\n",
    "            print(f\"   ååé‡: {triton_throughput:.1f} samples/sec\")\n",
    "            \n",
    "            # è¨ˆç®—åŠ é€Ÿæ¯”\n",
    "            speedup = pytorch_avg / triton_time\n",
    "            overhead = (triton_time - pytorch_avg) / pytorch_avg * 100\n",
    "            \n",
    "            print(f\"\\nğŸ“Š æ€§èƒ½å°æ¯”:\")\n",
    "            if speedup > 1:\n",
    "                print(f\"   Triton åŠ é€Ÿ: {speedup:.2f}x ğŸš€\")\n",
    "            else:\n",
    "                print(f\"   Triton é–‹éŠ·: {abs(overhead):.1f}% âš ï¸\")\n",
    "                \n",
    "            print(f\"\\nğŸ’¡ åˆ†æ:\")\n",
    "            if speedup < 1:\n",
    "                print(\"   â€¢ å°æ¨¡å‹å¯èƒ½æœ‰æœå‹™é–‹éŠ·\")\n",
    "                print(\"   â€¢ Triton å„ªå‹¢åœ¨å¤§æ¨¡å‹å’Œæ‰¹æ¬¡è™•ç†\")\n",
    "                print(\"   â€¢ ç¶²è·¯å»¶é²å’Œåºåˆ—åŒ–é–‹éŠ·\")\n",
    "            else:\n",
    "                print(\"   â€¢ Triton å‹•æ…‹æ‰¹æ¬¡è™•ç†å„ªåŒ–\")\n",
    "                print(\"   â€¢ æ›´å¥½çš„è¨˜æ†¶é«”ç®¡ç†\")\n",
    "                print(\"   â€¢ GPU kernel å„ªåŒ–\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å°æ¯”æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "\n",
    "compare_with_pytorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. åŸºç¤ç›£æ§è¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ Triton å…§å»ºæŒ‡æ¨™\n",
    "def explore_triton_metrics():\n",
    "    \"\"\"\n",
    "    æ¢ç´¢ Triton Server å…§å»ºç›£æ§æŒ‡æ¨™\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metrics_url = \"http://localhost:8002/metrics\"\n",
    "        response = requests.get(metrics_url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            metrics_text = response.text\n",
    "            lines = metrics_text.split('\\n')\n",
    "            \n",
    "            print(\"ğŸ“Š Triton Server ç›£æ§æŒ‡æ¨™\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # åˆ†æä¸åŒé¡å‹çš„æŒ‡æ¨™\n",
    "            metric_categories = {\n",
    "                'Request': [],\n",
    "                'Inference': [],\n",
    "                'Queue': [],\n",
    "                'Cache': [],\n",
    "                'GPU': [],\n",
    "                'Other': []\n",
    "            }\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                    \n",
    "                if 'nv_inference_request' in line:\n",
    "                    metric_categories['Request'].append(line.split()[0])\n",
    "                elif 'nv_inference_exec' in line or 'nv_inference_compute' in line:\n",
    "                    metric_categories['Inference'].append(line.split()[0])\n",
    "                elif 'nv_inference_queue' in line:\n",
    "                    metric_categories['Queue'].append(line.split()[0])\n",
    "                elif 'cache' in line.lower():\n",
    "                    metric_categories['Cache'].append(line.split()[0])\n",
    "                elif 'gpu' in line.lower():\n",
    "                    metric_categories['GPU'].append(line.split()[0])\n",
    "                else:\n",
    "                    if line.split()[0] not in [cat for cat_list in metric_categories.values() for cat in cat_list]:\n",
    "                        metric_categories['Other'].append(line.split()[0])\n",
    "            \n",
    "            # é¡¯ç¤ºæŒ‡æ¨™åˆ†é¡\n",
    "            for category, metrics in metric_categories.items():\n",
    "                if metrics:\n",
    "                    unique_metrics = list(set(metrics))\n",
    "                    print(f\"\\nğŸ”¸ {category} æŒ‡æ¨™ ({len(unique_metrics)}å€‹):\")\n",
    "                    for metric in unique_metrics[:5]:  # é¡¯ç¤ºå‰5å€‹\n",
    "                        print(f\"   â€¢ {metric}\")\n",
    "                    \n",
    "                    if len(unique_metrics) > 5:\n",
    "                        print(f\"   ... é‚„æœ‰ {len(unique_metrics)-5} å€‹æŒ‡æ¨™\")\n",
    "            \n",
    "            # æå–é—œéµæŒ‡æ¨™å€¼\n",
    "            key_metrics = {}\n",
    "            for line in lines:\n",
    "                if 'nv_inference_request_success' in line and '{' not in line:\n",
    "                    key_metrics['successful_requests'] = float(line.split()[1])\n",
    "                elif 'nv_inference_request_failure' in line and '{' not in line:\n",
    "                    key_metrics['failed_requests'] = float(line.split()[1])\n",
    "                elif 'nv_inference_exec_count' in line and '{' not in line:\n",
    "                    key_metrics['inference_count'] = float(line.split()[1])\n",
    "            \n",
    "            print(f\"\\nğŸ“ˆ ç•¶å‰æŒ‡æ¨™å€¼:\")\n",
    "            for metric, value in key_metrics.items():\n",
    "                print(f\"   {metric}: {value}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ æŒ‡æ¨™ç«¯é»è¨ªå•å¤±æ•—: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æŒ‡æ¨™ç²å–å¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "metrics_available = explore_triton_metrics()\n",
    "\n",
    "if metrics_available:\n",
    "    print(\"\\nğŸ¯ ç›£æ§å»ºè­°:\")\n",
    "    print(\"â€¢ å°‡æŒ‡æ¨™ç«¯é»æ•´åˆåˆ° Prometheus\")\n",
    "    print(\"â€¢ è¨­ç½®é—œéµæŒ‡æ¨™çš„å‘Šè­¦é–¾å€¼\")\n",
    "    print(\"â€¢ ç›£æ§æ¨¡å‹æ€§èƒ½è¶¨å‹¢\")\n",
    "    print(\"â€¢ è¨­ç½® GPU åˆ©ç”¨ç‡ç›£æ§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. æ¸…ç†èˆ‡ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†è³‡æº\n",
    "def cleanup_resources():\n",
    "    \"\"\"\n",
    "    æ¸…ç† Docker å®¹å™¨å’Œè³‡æº\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§¹ æ¸…ç†è³‡æº...\")\n",
    "    \n",
    "    try:\n",
    "        # åœæ­¢ Triton Server å®¹å™¨\n",
    "        result = subprocess.run(['docker', 'stop', 'triton-server'], \n",
    "                               capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Triton Server å®¹å™¨å·²åœæ­¢\")\n",
    "        else:\n",
    "            print(\"âš ï¸ å®¹å™¨å¯èƒ½å·²ç¶“åœæ­¢\")\n",
    "            \n",
    "        # æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"âœ… GPU è¨˜æ†¶é«”å·²æ¸…ç†\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¸…ç†éç¨‹ä¸­å‡ºéŒ¯: {e}\")\n",
    "        return False\n",
    "\n",
    "# å¯¦é©—å®¤ç¸½çµ\n",
    "print(\"ğŸ¯ Lab-2.1 Part 1 å®Œæˆç¸½çµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "completion_checklist = [\n",
    "    (\"ç’°å¢ƒé©—è­‰\", torch.cuda.is_available() and docker_ready),\n",
    "    (\"Triton Server å®‰è£\", triton_image_ready),\n",
    "    (\"æ¨¡å‹å€‰åº«å‰µå»º\", model_repo_dir.exists()),\n",
    "    (\"æ¨¡å‹éƒ¨ç½²\", (model_repo_dir / \"simple_pytorch_model\" / \"1\" / \"model.pt\").exists()),\n",
    "    (\"API æ¸¬è©¦\", server_ready if 'server_ready' in locals() else False),\n",
    "    (\"ç›£æ§æŒ‡æ¨™\", metrics_available),\n",
    "]\n",
    "\n",
    "completed_tasks = 0\n",
    "for task, status in completion_checklist:\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"  {status_icon} {task}\")\n",
    "    if status:\n",
    "        completed_tasks += 1\n",
    "\n",
    "completion_rate = completed_tasks / len(completion_checklist) * 100\n",
    "print(f\"\\nğŸ“Š å®Œæˆåº¦: {completion_rate:.1f}% ({completed_tasks}/{len(completion_checklist)})\")\n",
    "\n",
    "if completion_rate >= 80:\n",
    "    print(\"\\nğŸ‰ æ­å–œ! æ‚¨å·²æŒæ¡ Triton Server åŸºç¤\")\n",
    "    print(\"\\nâ¡ï¸ ä¸‹ä¸€æ­¥: 02-Model_Repository_Design.ipynb\")\n",
    "    print(\"   å­¸ç¿’æ¨¡å‹å€‰åº«è¨­è¨ˆå’Œé«˜ç´šé…ç½®\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ å»ºè­°è§£æ±ºæœªå®Œæˆçš„é …ç›®å¾Œå†ç¹¼çºŒ\")\n",
    "    print(\"\\nğŸ“‹ æ•…éšœæ’é™¤æŒ‡å—:\")\n",
    "    print(\"1. æª¢æŸ¥ Docker å’Œ NVIDIA é©…å‹•å®‰è£\")\n",
    "    print(\"2. ç¢ºèª GPU å¯ç”¨æ€§\")\n",
    "    print(\"3. é©—è­‰ç¶²è·¯é€£æ¥å’Œç«¯å£\")\n",
    "    print(\"4. æŸ¥çœ‹ Triton Server æ—¥èªŒ\")\n",
    "\n",
    "# å¯é¸ï¼šè‡ªå‹•æ¸…ç†\n",
    "auto_cleanup = False  # è¨­ç‚º True è‡ªå‹•æ¸…ç†\n",
    "\n",
    "if auto_cleanup:\n",
    "    cleanup_success = cleanup_resources()\n",
    "else:\n",
    "    print(\"\\nğŸ’¡ æç¤º: é‹è¡Œä¸‹é¢çš„ cell ä¾†æ¸…ç†å®¹å™¨\")\n",
    "    print(\"   (å¦‚æœè¦ç¹¼çºŒä¸‹ä¸€å€‹ notebookï¼Œè«‹ä¿ç•™å®¹å™¨é‹è¡Œ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯é¸ï¼šæ‰‹å‹•æ¸…ç†\n",
    "cleanup_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… å¯¦é©—å®¤ç¸½çµ\n",
    "\n",
    "### ğŸ¯ å·²å®Œæˆå­¸ç¿’ç›®æ¨™\n",
    "1. âœ… **Triton Server åŸºç¤**: å®‰è£ã€é…ç½®å’Œå•Ÿå‹•\n",
    "2. âœ… **æ¨¡å‹å€‰åº«è¨­è¨ˆ**: æ¨™æº–ç›®éŒ„çµæ§‹å’Œé…ç½®æ–‡ä»¶\n",
    "3. âœ… **PyTorch æ¨¡å‹éƒ¨ç½²**: TorchScript è½‰æ›å’Œéƒ¨ç½²\n",
    "4. âœ… **API ä»‹é¢æ¸¬è©¦**: REST API èª¿ç”¨å’Œæ‰¹æ¬¡æ¨ç†\n",
    "5. âœ… **æ€§èƒ½åˆ†æ**: æ‰¹æ¬¡è™•ç†å„ªåŒ–å’Œæ€§èƒ½å°æ¯”\n",
    "6. âœ… **ç›£æ§åŸºç¤**: å…§å»ºæŒ‡æ¨™ç³»çµ±æ¢ç´¢\n",
    "\n",
    "### ğŸ“Š æ ¸å¿ƒæŠ€èƒ½ç²å¾—\n",
    "- **ä¼æ¥­ç´šæ¨ç†æœå‹™**: æŒæ¡ Triton Server éƒ¨ç½²å’Œé…ç½®\n",
    "- **æ¨¡å‹ç®¡ç†**: ç†è§£æ¨¡å‹å€‰åº«è¨­è¨ˆåŸå‰‡\n",
    "- **å‹•æ…‹æ‰¹æ¬¡è™•ç†**: å„ªåŒ–æ¨ç†æ€§èƒ½çš„æ ¸å¿ƒæŠ€è¡“\n",
    "- **API ä»‹é¢**: REST/gRPC é›™å”è­°æ”¯æ´\n",
    "- **ç›£æ§æ•´åˆ**: ç‚ºç”Ÿç”¢ç’°å¢ƒç›£æ§å¥ å®šåŸºç¤\n",
    "\n",
    "### ğŸŒŸ Triton vs vLLM å°æ¯”\n",
    "| ç‰¹æ€§ | Triton | vLLM |\n",
    "|------|--------|------|\n",
    "| **å¤šæ¨¡å‹ç®¡ç†** | âœ… åŸç”Ÿæ”¯æ´ | âŒ å–®æ¨¡å‹ |\n",
    "| **Backend éˆæ´»æ€§** | âœ… å¤šç¨® Backend | âŒ å›ºå®šå¯¦ç¾ |\n",
    "| **ä¼æ¥­ç´šåŠŸèƒ½** | âœ… å®Œæ•´æ”¯æ´ | âš ï¸ æœ‰é™ |\n",
    "| **ç‰ˆæœ¬ç®¡ç†** | âœ… å…§å»º A/B æ¸¬è©¦ | âŒ éœ€è‡ªè¡Œå¯¦ç¾ |\n",
    "| **ç›£æ§é‹ç¶­** | âœ… è±å¯ŒæŒ‡æ¨™ | âš ï¸ åŸºç¤æŒ‡æ¨™ |\n",
    "| **å­¸ç¿’æ›²ç·š** | âš ï¸ è¼ƒè¤‡é›œ | âœ… ç°¡å–®æ˜“ç”¨ |\n",
    "\n",
    "### ğŸ’¡ é—œéµæ´å¯Ÿ\n",
    "- **Triton é©åˆä¼æ¥­ç´šå¤šæ¨¡å‹å ´æ™¯**\n",
    "- **å‹•æ…‹æ‰¹æ¬¡è™•ç†æ˜¯æ€§èƒ½é—œéµ**\n",
    "- **é…ç½®æ–‡ä»¶æ˜¯ Triton çš„æ ¸å¿ƒ**\n",
    "- **ç›£æ§æŒ‡æ¨™è±å¯Œä¸”è©³ç´°**\n",
    "\n",
    "### â¡ï¸ ä¸‹ä¸€æ­¥å­¸ç¿’è·¯å¾‘\n",
    "1. **02-Model_Repository_Design.ipynb**: æ·±å…¥æ¨¡å‹å€‰åº«è¨­è¨ˆ\n",
    "2. **03-PyTorch_Backend_Deployment.ipynb**: PyTorch Backend é«˜ç´šé…ç½®\n",
    "3. **04-Monitoring_and_Performance.ipynb**: ç›£æ§ç³»çµ±æ•´åˆ\n",
    "\n",
    "### ğŸ”— åƒè€ƒè³‡æº\n",
    "- [Triton Model Repository Guide](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md)\n",
    "- [Dynamic Batching](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#dynamic-batching)\n",
    "- [PyTorch Backend](https://github.com/triton-inference-server/pytorch_backend)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ å­¸ç¿’æˆæœ**: æ‚¨å·²å…·å‚™ä½¿ç”¨ Triton Inference Server é€²è¡ŒåŸºç¤æ¨ç†éƒ¨ç½²çš„èƒ½åŠ›ï¼\n",
    "\n",
    "**ğŸ’¼ æ¥­ç•Œæ‡‰ç”¨**: é€™äº›æŠ€èƒ½ç›´æ¥æ‡‰ç”¨æ–¼ Netflixã€PayPalã€VISA ç­‰ä¼æ¥­çš„ AI æ¨ç†å¹³å°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš€ å»¶ä¼¸ç·´ç¿’\n",
    "\n",
    "### åŸºç¤ç·´ç¿’\n",
    "1. **ä¿®æ”¹æ¨¡å‹**: æ”¹è®Šåˆ†é¡å™¨çš„é¡åˆ¥æ•¸é‡ï¼Œé‡æ–°éƒ¨ç½²\n",
    "2. **æ‰¹æ¬¡å„ªåŒ–**: å¯¦é©—ä¸åŒçš„ `preferred_batch_size` é…ç½®\n",
    "3. **ç›£æ§è§€å¯Ÿ**: åœ¨æ¨ç†éç¨‹ä¸­å¯¦æ™‚è§€å¯ŸæŒ‡æ¨™è®ŠåŒ–\n",
    "\n",
    "### é€²éšæŒ‘æˆ°\n",
    "1. **å¤šç‰ˆæœ¬éƒ¨ç½²**: éƒ¨ç½²åŒä¸€æ¨¡å‹çš„å¤šå€‹ç‰ˆæœ¬\n",
    "2. **gRPC API**: ä½¿ç”¨ gRPC å®¢æˆ¶ç«¯é€²è¡Œæ¨ç†\n",
    "3. **æ€§èƒ½èª¿å„ª**: èª¿æ•´ `max_queue_delay_microseconds` åƒæ•¸\n",
    "\n",
    "### å°ˆå®¶ç´šä»»å‹™\n",
    "1. **è‡ªå®šç¾©æŒ‡æ¨™**: å‰µå»ºè‡ªå®šç¾©æ¥­å‹™æŒ‡æ¨™\n",
    "2. **è² è¼‰æ¸¬è©¦**: ä½¿ç”¨ locust é€²è¡Œå£“åŠ›æ¸¬è©¦\n",
    "3. **æ•…éšœæ¨¡æ“¬**: æ¨¡æ“¬å„ç¨®æ•…éšœæƒ…æ³ä¸¦æ¸¬è©¦æ¢å¾©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering Course (Poetry)",
   "language": "python",
   "name": "llm-engineering-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
