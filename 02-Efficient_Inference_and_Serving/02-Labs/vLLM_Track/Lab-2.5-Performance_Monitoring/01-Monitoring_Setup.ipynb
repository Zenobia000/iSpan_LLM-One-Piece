{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5-01: ç›£æ§ç³»çµ±å»ºç½®\n",
    "\n",
    "## å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬ç¯€å°‡å»ºç«‹å®Œæ•´çš„ vLLM æ€§èƒ½ç›£æ§åŸºç¤è¨­æ–½ï¼ŒåŒ…æ‹¬ï¼š\n",
    "- Prometheus æ™‚é–“åºåˆ—è³‡æ–™åº«é…ç½®\n",
    "- vLLM æœå‹™çš„ metrics ç«¯é»è¨­ç½®\n",
    "- Grafana è¦–è¦ºåŒ–å„€è¡¨æ¿å‰µå»º\n",
    "- åŸºç¤ç›£æ§æŒ‡æ¨™é©—è­‰\n",
    "\n",
    "## ç›£æ§æ¶æ§‹æ¦‚è¦½\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP /metrics    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   vLLM Server   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Prometheus    â”‚\n",
    "â”‚  (Port 8000)    â”‚                     â”‚   (Port 9090)   â”‚\n",
    "â”‚  /metrics       â”‚                     â”‚                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                  â”‚\n",
    "                                         PromQL Queries\n",
    "                                                  â”‚\n",
    "                                                  â–¼\n",
    "                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                        â”‚    Grafana      â”‚\n",
    "                                        â”‚   (Port 3000)   â”‚\n",
    "                                        â”‚   Dashboard     â”‚\n",
    "                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™èˆ‡ä¾è³´å®‰è£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£ç›£æ§ç›¸é—œä¾è³´å¥—ä»¶\n",
    "!pip install prometheus-client\n",
    "!pip install psutil\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install requests\n",
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Prometheus å®¢æˆ¶ç«¯\n",
    "from prometheus_client import start_http_server, Gauge, Counter, Histogram, CollectorRegistry\n",
    "from prometheus_client.core import REGISTRY\n",
    "\n",
    "# ç³»çµ±ç›£æ§\n",
    "import psutil\n",
    "try:\n",
    "    import pynvml\n",
    "    NVIDIA_GPU_AVAILABLE = True\n",
    "    pynvml.nvmlInit()\n",
    "except ImportError:\n",
    "    NVIDIA_GPU_AVAILABLE = False\n",
    "    print(\"è­¦å‘Š: NVIDIA GPU ç›£æ§ä¸å¯ç”¨\")\n",
    "\n",
    "# æ•¸æ“šè™•ç†èˆ‡è¦–è¦ºåŒ–\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰ä¾è³´å¥—ä»¶å·²è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. vLLM æœå‹™é…ç½®èˆ‡ Metrics ç«¯é»è¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½® vLLM æœå‹™çš„ç›£æ§åƒæ•¸\n",
    "VLLM_CONFIG = {\n",
    "    \"model\": \"microsoft/DialoGPT-medium\",  # ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹é€²è¡Œæ¸¬è©¦\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": 8000,\n",
    "    \"metrics_port\": 8001,  # å°ˆç”¨çš„ metrics ç«¯é»\n",
    "    \"max_model_len\": 1024,\n",
    "    \"gpu_memory_utilization\": 0.7\n",
    "}\n",
    "\n",
    "print(f\"vLLM é…ç½®:\")\n",
    "for key, value in VLLM_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ vLLM å•Ÿå‹•è…³æœ¬\n",
    "vllm_script = f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# å•Ÿå‹• vLLM æœå‹™ä¸¦å•Ÿç”¨ç›£æ§\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model {VLLM_CONFIG['model']} \\\n",
    "    --host {VLLM_CONFIG['host']} \\\n",
    "    --port {VLLM_CONFIG['port']} \\\n",
    "    --max-model-len {VLLM_CONFIG['max_model_len']} \\\n",
    "    --gpu-memory-utilization {VLLM_CONFIG['gpu_memory_utilization']} \\\n",
    "    --enable-metrics \\\n",
    "    --metrics-port {VLLM_CONFIG['metrics_port']}\n",
    "\"\"\"\n",
    "\n",
    "# å„²å­˜å•Ÿå‹•è…³æœ¬\n",
    "script_path = \"start_vllm_with_metrics.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(vllm_script)\n",
    "\n",
    "# è³¦äºˆåŸ·è¡Œæ¬Šé™\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"âœ… vLLM å•Ÿå‹•è…³æœ¬å·²å»ºç«‹: {script_path}\")\n",
    "print(\"\\nè…³æœ¬å…§å®¹:\")\n",
    "print(vllm_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è‡ªå®šç¾© Metrics æ”¶é›†å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLLMMetricsCollector:\n",
    "    \"\"\"vLLM è‡ªå®šç¾©æŒ‡æ¨™æ”¶é›†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, vllm_metrics_url=\"http://127.0.0.1:8001/metrics\"):\n",
    "        self.vllm_metrics_url = vllm_metrics_url\n",
    "        self.registry = CollectorRegistry()\n",
    "        \n",
    "        # å®šç¾©è‡ªå®šç¾©æŒ‡æ¨™\n",
    "        self.request_latency = Histogram(\n",
    "            'vllm_request_latency_seconds',\n",
    "            'Request latency in seconds',\n",
    "            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "            registry=self.registry\n",
    "        )\n",
    "        \n",
    "        self.active_requests = Gauge(\n",
    "            'vllm_active_requests',\n",
    "            'Number of active requests',\n",
    "            registry=self.registry\n",
    "        )\n",
    "        \n",
    "        self.tokens_generated = Counter(\n",
    "            'vllm_tokens_generated_total',\n",
    "            'Total number of tokens generated',\n",
    "            registry=self.registry\n",
    "        )\n",
    "        \n",
    "        self.gpu_memory_usage = Gauge(\n",
    "            'vllm_gpu_memory_usage_bytes',\n",
    "            'GPU memory usage in bytes',\n",
    "            ['gpu_id'],\n",
    "            registry=self.registry\n",
    "        )\n",
    "        \n",
    "        self.cpu_usage = Gauge(\n",
    "            'vllm_cpu_usage_percent',\n",
    "            'CPU usage percentage',\n",
    "            registry=self.registry\n",
    "        )\n",
    "    \n",
    "    def collect_system_metrics(self):\n",
    "        \"\"\"æ”¶é›†ç³»çµ±ç´šæŒ‡æ¨™\"\"\"\n",
    "        # CPU ä½¿ç”¨ç‡\n",
    "        cpu_percent = psutil.cpu_percent(interval=1)\n",
    "        self.cpu_usage.set(cpu_percent)\n",
    "        \n",
    "        # GPU æŒ‡æ¨™ (å¦‚æœå¯ç”¨)\n",
    "        if NVIDIA_GPU_AVAILABLE:\n",
    "            try:\n",
    "                device_count = pynvml.nvmlDeviceGetCount()\n",
    "                for i in range(device_count):\n",
    "                    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                    self.gpu_memory_usage.labels(gpu_id=str(i)).set(mem_info.used)\n",
    "            except Exception as e:\n",
    "                print(f\"GPU æŒ‡æ¨™æ”¶é›†å¤±æ•—: {e}\")\n",
    "    \n",
    "    def get_vllm_native_metrics(self):\n",
    "        \"\"\"ç²å– vLLM åŸç”ŸæŒ‡æ¨™\"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.vllm_metrics_url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                return None\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "    \n",
    "    def start_metrics_server(self, port=8002):\n",
    "        \"\"\"å•Ÿå‹• Prometheus metrics ä¼ºæœå™¨\"\"\"\n",
    "        start_http_server(port, registry=self.registry)\n",
    "        print(f\"âœ… Metrics ä¼ºæœå™¨å·²å•Ÿå‹•åœ¨ http://127.0.0.1:{port}/metrics\")\n",
    "    \n",
    "    def simulate_request_metrics(self, latency=1.5, tokens=150):\n",
    "        \"\"\"æ¨¡æ“¬è«‹æ±‚æŒ‡æ¨™ (ç”¨æ–¼æ¸¬è©¦)\"\"\"\n",
    "        self.request_latency.observe(latency)\n",
    "        self.tokens_generated.inc(tokens)\n",
    "        self.active_requests.inc()\n",
    "        \n",
    "        # æ¨¡æ“¬è«‹æ±‚å®Œæˆ\n",
    "        threading.Timer(latency, lambda: self.active_requests.dec()).start()\n",
    "\n",
    "# åˆå§‹åŒ– metrics æ”¶é›†å™¨\n",
    "metrics_collector = VLLMMetricsCollector()\n",
    "print(\"âœ… Metrics æ”¶é›†å™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prometheus é…ç½®æª”æ¡ˆç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ Prometheus é…ç½®æª”æ¡ˆ\n",
    "prometheus_config = {\n",
    "    \"global\": {\n",
    "        \"scrape_interval\": \"15s\",\n",
    "        \"evaluation_interval\": \"15s\"\n",
    "    },\n",
    "    \"scrape_configs\": [\n",
    "        {\n",
    "            \"job_name\": \"vllm-native\",\n",
    "            \"static_configs\": [\n",
    "                {\n",
    "                    \"targets\": [f\"127.0.0.1:{VLLM_CONFIG['metrics_port']}\"]\n",
    "                }\n",
    "            ],\n",
    "            \"scrape_interval\": \"5s\",\n",
    "            \"metrics_path\": \"/metrics\"\n",
    "        },\n",
    "        {\n",
    "            \"job_name\": \"vllm-custom\",\n",
    "            \"static_configs\": [\n",
    "                {\n",
    "                    \"targets\": [\"127.0.0.1:8002\"]\n",
    "                }\n",
    "            ],\n",
    "            \"scrape_interval\": \"5s\",\n",
    "            \"metrics_path\": \"/metrics\"\n",
    "        },\n",
    "        {\n",
    "            \"job_name\": \"prometheus\",\n",
    "            \"static_configs\": [\n",
    "                {\n",
    "                    \"targets\": [\"127.0.0.1:9090\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# å°‡é…ç½®è½‰æ›ç‚º YAML æ ¼å¼\n",
    "import yaml\n",
    "\n",
    "prometheus_yaml = yaml.dump(prometheus_config, default_flow_style=False)\n",
    "\n",
    "# å„²å­˜ Prometheus é…ç½®æª”æ¡ˆ\n",
    "with open(\"prometheus.yml\", \"w\") as f:\n",
    "    f.write(prometheus_yaml)\n",
    "\n",
    "print(\"âœ… Prometheus é…ç½®æª”æ¡ˆå·²ç”Ÿæˆ: prometheus.yml\")\n",
    "print(\"\\né…ç½®å…§å®¹:\")\n",
    "print(prometheus_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grafana å„€è¡¨æ¿é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ Grafana å„€è¡¨æ¿ JSON é…ç½®\n",
    "grafana_dashboard = {\n",
    "    \"dashboard\": {\n",
    "        \"id\": None,\n",
    "        \"title\": \"vLLM Performance Monitoring\",\n",
    "        \"tags\": [\"vllm\", \"llm\", \"performance\"],\n",
    "        \"timezone\": \"browser\",\n",
    "        \"refresh\": \"5s\",\n",
    "        \"time\": {\n",
    "            \"from\": \"now-30m\",\n",
    "            \"to\": \"now\"\n",
    "        },\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Request Latency\",\n",
    "                \"type\": \"graph\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": \"histogram_quantile(0.95, rate(vllm_request_latency_seconds_bucket[5m]))\",\n",
    "                        \"legendFormat\": \"P95 Latency\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expr\": \"histogram_quantile(0.50, rate(vllm_request_latency_seconds_bucket[5m]))\",\n",
    "                        \"legendFormat\": \"P50 Latency\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0},\n",
    "                \"yAxes\": [\n",
    "                    {\n",
    "                        \"label\": \"Seconds\",\n",
    "                        \"min\": 0\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"Active Requests\",\n",
    "                \"type\": \"singlestat\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": \"vllm_active_requests\",\n",
    "                        \"legendFormat\": \"Active Requests\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 0}\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"Token Generation Rate\",\n",
    "                \"type\": \"graph\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": \"rate(vllm_tokens_generated_total[5m])\",\n",
    "                        \"legendFormat\": \"Tokens/sec\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 8},\n",
    "                \"yAxes\": [\n",
    "                    {\n",
    "                        \"label\": \"Tokens per second\",\n",
    "                        \"min\": 0\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"GPU Memory Usage\",\n",
    "                \"type\": \"graph\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": \"vllm_gpu_memory_usage_bytes\",\n",
    "                        \"legendFormat\": \"GPU {{gpu_id}} Memory\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 8},\n",
    "                \"yAxes\": [\n",
    "                    {\n",
    "                        \"label\": \"Bytes\",\n",
    "                        \"min\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"overwrite\": True\n",
    "}\n",
    "\n",
    "# å„²å­˜ Grafana å„€è¡¨æ¿é…ç½®\n",
    "with open(\"vllm_dashboard.json\", \"w\") as f:\n",
    "    json.dump(grafana_dashboard, f, indent=2)\n",
    "\n",
    "print(\"âœ… Grafana å„€è¡¨æ¿é…ç½®å·²ç”Ÿæˆ: vllm_dashboard.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç›£æ§æœå‹™å•Ÿå‹•èˆ‡é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•Ÿå‹•è‡ªå®šç¾© metrics ä¼ºæœå™¨\n",
    "metrics_collector.start_metrics_server(port=8002)\n",
    "\n",
    "# é–‹å§‹æ”¶é›†ç³»çµ±æŒ‡æ¨™\n",
    "def collect_metrics_loop():\n",
    "    while True:\n",
    "        metrics_collector.collect_system_metrics()\n",
    "        time.sleep(5)\n",
    "\n",
    "# åœ¨èƒŒæ™¯åŸ·è¡ŒæŒ‡æ¨™æ”¶é›†\n",
    "metrics_thread = threading.Thread(target=collect_metrics_loop, daemon=True)\n",
    "metrics_thread.start()\n",
    "\n",
    "print(\"âœ… ç³»çµ±æŒ‡æ¨™æ”¶é›†å·²å•Ÿå‹•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é©—è­‰ metrics ç«¯é»\n",
    "def verify_metrics_endpoint(url, name):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… {name} metrics ç«¯é»æ­£å¸¸: {url}\")\n",
    "            # é¡¯ç¤ºéƒ¨åˆ† metrics å…§å®¹\n",
    "            lines = response.text.split('\\n')[:10]\n",
    "            print(f\"   å‰ 10 è¡Œ metrics:\")\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    print(f\"     {line}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ {name} metrics ç«¯é»ç•°å¸¸: {url} (ç‹€æ…‹ç¢¼: {response.status_code})\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ {name} metrics ç«¯é»ç„¡æ³•é€£æ¥: {url} ({e})\")\n",
    "        return False\n",
    "\n",
    "# é©—è­‰è‡ªå®šç¾© metrics\n",
    "custom_metrics_ok = verify_metrics_endpoint(\"http://127.0.0.1:8002/metrics\", \"è‡ªå®šç¾©\")\n",
    "\n",
    "# å˜—è©¦é©—è­‰ vLLM åŸç”Ÿ metrics (å¦‚æœæœå‹™åœ¨é‹è¡Œ)\n",
    "vllm_metrics_ok = verify_metrics_endpoint(f\"http://127.0.0.1:{VLLM_CONFIG['metrics_port']}/metrics\", \"vLLM åŸç”Ÿ\")\n",
    "\n",
    "if not vllm_metrics_ok:\n",
    "    print(\"\\nğŸ’¡ æç¤º: vLLM æœå‹™å°šæœªå•Ÿå‹•ï¼Œè«‹ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å•Ÿå‹•:\")\n",
    "    print(f\"   bash {script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ¨¡æ“¬ç›£æ§æ•¸æ“šç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# æ¨¡æ“¬ç”Ÿæˆç›£æ§æ•¸æ“š\n",
    "def simulate_monitoring_data(duration_minutes=5):\n",
    "    \"\"\"æ¨¡æ“¬ç”Ÿæˆç›£æ§æ•¸æ“š\"\"\"\n",
    "    print(f\"é–‹å§‹æ¨¡æ“¬ {duration_minutes} åˆ†é˜çš„ç›£æ§æ•¸æ“š...\")\n",
    "    \n",
    "    end_time = time.time() + duration_minutes * 60\n",
    "    request_count = 0\n",
    "    \n",
    "    while time.time() < end_time:\n",
    "        # æ¨¡æ“¬è«‹æ±‚å»¶é² (æ­£æ…‹åˆ†ä½ˆ)\n",
    "        latency = max(0.1, np.random.normal(1.5, 0.5))\n",
    "        \n",
    "        # æ¨¡æ“¬ç”Ÿæˆçš„ token æ•¸é‡\n",
    "        tokens = random.randint(50, 300)\n",
    "        \n",
    "        # è¨˜éŒ„ metrics\n",
    "        metrics_collector.simulate_request_metrics(latency, tokens)\n",
    "        \n",
    "        request_count += 1\n",
    "        \n",
    "        # éš¨æ©Ÿé–“éš”\n",
    "        time.sleep(random.uniform(0.5, 3.0))\n",
    "        \n",
    "        if request_count % 10 == 0:\n",
    "            print(f\"å·²æ¨¡æ“¬ {request_count} å€‹è«‹æ±‚...\")\n",
    "    \n",
    "    print(f\"âœ… æ¨¡æ“¬å®Œæˆï¼Œç¸½å…±ç”Ÿæˆ {request_count} å€‹è«‹æ±‚çš„ç›£æ§æ•¸æ“š\")\n",
    "\n",
    "# åœ¨èƒŒæ™¯åŸ·è¡Œæ¨¡æ“¬æ•¸æ“šç”Ÿæˆ\n",
    "simulation_thread = threading.Thread(\n",
    "    target=simulate_monitoring_data, \n",
    "    args=(2,),  # æ¨¡æ“¬ 2 åˆ†é˜\n",
    "    daemon=True\n",
    ")\n",
    "simulation_thread.start()\n",
    "\n",
    "print(\"ğŸ“Š ç›£æ§æ•¸æ“šæ¨¡æ“¬å·²é–‹å§‹...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç›£æ§æŒ‡æ¨™å³æ™‚è¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_current_metrics():\n",
    "    \"\"\"è¦–è¦ºåŒ–ç•¶å‰ç›£æ§æŒ‡æ¨™\"\"\"\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('vLLM å³æ™‚ç›£æ§æŒ‡æ¨™', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # ç²å–ç•¶å‰æŒ‡æ¨™å€¼\n",
    "    try:\n",
    "        response = requests.get(\"http://127.0.0.1:8002/metrics\", timeout=5)\n",
    "        metrics_text = response.text\n",
    "        \n",
    "        # è§£æ metrics (ç°¡åŒ–ç‰ˆ)\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        \n",
    "        # CPU ä½¿ç”¨ç‡\n",
    "        axes[0, 0].bar(['CPU'], [cpu_usage], color='skyblue')\n",
    "        axes[0, 0].set_title('CPU ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 0].set_ylim(0, 100)\n",
    "        axes[0, 0].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        \n",
    "        # è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "        axes[0, 1].bar(['Memory'], [memory_usage], color='lightcoral')\n",
    "        axes[0, 1].set_title('è¨˜æ†¶é«”ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 1].set_ylim(0, 100)\n",
    "        axes[0, 1].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        \n",
    "        # GPU è³‡è¨Š (å¦‚æœå¯ç”¨)\n",
    "        if NVIDIA_GPU_AVAILABLE:\n",
    "            try:\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                gpu_mem_percent = (mem_info.used / mem_info.total) * 100\n",
    "                \n",
    "                axes[1, 0].bar(['GPU Compute', 'GPU Memory'], \n",
    "                              [gpu_util.gpu, gpu_mem_percent], \n",
    "                              color=['orange', 'purple'])\n",
    "                axes[1, 0].set_title('GPU ä½¿ç”¨ç‡ (%)')\n",
    "                axes[1, 0].set_ylim(0, 100)\n",
    "                axes[1, 0].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "            except:\n",
    "                axes[1, 0].text(0.5, 0.5, 'GPU è³‡è¨Š\\nç„¡æ³•å–å¾—', \n",
    "                               ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "                axes[1, 0].set_title('GPU ä½¿ç”¨ç‡ (N/A)')\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'GPU ä¸å¯ç”¨', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('GPU ä½¿ç”¨ç‡ (N/A)')\n",
    "        \n",
    "        # Metrics å¯ç”¨æ€§ç‹€æ…‹\n",
    "        endpoints = [\n",
    "            ('è‡ªå®šç¾© Metrics', 'http://127.0.0.1:8002/metrics'),\n",
    "            ('vLLM Metrics', f'http://127.0.0.1:{VLLM_CONFIG[\"metrics_port\"]}/metrics')\n",
    "        ]\n",
    "        \n",
    "        statuses = []\n",
    "        labels = []\n",
    "        colors = []\n",
    "        \n",
    "        for name, url in endpoints:\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=2)\n",
    "                if resp.status_code == 200:\n",
    "                    statuses.append(1)\n",
    "                    colors.append('green')\n",
    "                else:\n",
    "                    statuses.append(0)\n",
    "                    colors.append('red')\n",
    "            except:\n",
    "                statuses.append(0)\n",
    "                colors.append('red')\n",
    "            labels.append(name)\n",
    "        \n",
    "        axes[1, 1].bar(labels, statuses, color=colors)\n",
    "        axes[1, 1].set_title('Metrics ç«¯é»ç‹€æ…‹')\n",
    "        axes[1, 1].set_ylim(0, 1.2)\n",
    "        axes[1, 1].set_ylabel('ç‹€æ…‹ (1=æ­£å¸¸, 0=ç•°å¸¸)')\n",
    "        axes[1, 1].set_xticklabels(labels, rotation=45)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"è¦–è¦ºåŒ–éŒ¯èª¤: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ç”Ÿæˆå³æ™‚ç›£æ§è¦–è¦ºåŒ–\n",
    "visualize_current_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. éƒ¨ç½²è…³æœ¬ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå®Œæ•´çš„éƒ¨ç½²è…³æœ¬\n",
    "deployment_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# vLLM ç›£æ§ç³»çµ±éƒ¨ç½²è…³æœ¬\n",
    "echo \"ğŸš€ é–‹å§‹éƒ¨ç½² vLLM ç›£æ§ç³»çµ±...\"\n",
    "\n",
    "# æª¢æŸ¥ä¾è³´\n",
    "echo \"ğŸ“‹ æª¢æŸ¥ç³»çµ±ä¾è³´...\"\n",
    "command -v docker >/dev/null 2>&1 || { echo \"âŒ Docker æœªå®‰è£\"; exit 1; }\n",
    "command -v docker-compose >/dev/null 2>&1 || { echo \"âŒ Docker Compose æœªå®‰è£\"; exit 1; }\n",
    "\n",
    "# å»ºç«‹ç›®éŒ„çµæ§‹\n",
    "mkdir -p monitoring/{prometheus,grafana}\n",
    "cp prometheus.yml monitoring/prometheus/\n",
    "cp vllm_dashboard.json monitoring/grafana/\n",
    "\n",
    "# ç”Ÿæˆ Docker Compose æª”æ¡ˆ\n",
    "cat > monitoring/docker-compose.yml << 'EOF'\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    container_name: vllm-prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "      - '--storage.tsdb.retention.time=200h'\n",
    "      - '--web.enable-lifecycle'\n",
    "    networks:\n",
    "      - monitoring\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    container_name: vllm-grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana-storage:/var/lib/grafana\n",
    "      - ./grafana:/etc/grafana/provisioning/dashboards\n",
    "    networks:\n",
    "      - monitoring\n",
    "\n",
    "networks:\n",
    "  monitoring:\n",
    "    driver: bridge\n",
    "\n",
    "volumes:\n",
    "  grafana-storage:\n",
    "EOF\n",
    "\n",
    "echo \"âœ… éƒ¨ç½²æª”æ¡ˆå·²ç”Ÿæˆ\"\n",
    "echo \"ğŸ“ åŸ·è¡Œä»¥ä¸‹å‘½ä»¤å•Ÿå‹•ç›£æ§æœå‹™:\"\n",
    "echo \"   cd monitoring\"\n",
    "echo \"   docker-compose up -d\"\n",
    "echo \"\"\n",
    "echo \"ğŸŒ æœå‹™ç«¯é»:\"\n",
    "echo \"   Prometheus: http://localhost:9090\"\n",
    "echo \"   Grafana: http://localhost:3000 (admin/admin)\"\n",
    "echo \"   vLLM API: http://localhost:8000\"\n",
    "echo \"   vLLM Metrics: http://localhost:8001/metrics\"\n",
    "echo \"   Custom Metrics: http://localhost:8002/metrics\"\n",
    "\"\"\"\n",
    "\n",
    "# å„²å­˜éƒ¨ç½²è…³æœ¬\n",
    "with open(\"deploy_monitoring.sh\", \"w\") as f:\n",
    "    f.write(deployment_script)\n",
    "\n",
    "os.chmod(\"deploy_monitoring.sh\", 0o755)\n",
    "\n",
    "print(\"âœ… éƒ¨ç½²è…³æœ¬å·²ç”Ÿæˆ: deploy_monitoring.sh\")\n",
    "print(\"\\nåŸ·è¡Œ `bash deploy_monitoring.sh` ä¾†éƒ¨ç½²å®Œæ•´çš„ç›£æ§ç³»çµ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ç›£æ§è¨­ç½®é©—è­‰ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›£æ§è¨­ç½®ç¸½çµå ±å‘Š\n",
    "def generate_setup_summary():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¯ vLLM ç›£æ§ç³»çµ±å»ºç½®å®Œæˆç¸½çµ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æª¢æŸ¥ç”Ÿæˆçš„æª”æ¡ˆ\n",
    "    files_to_check = [\n",
    "        \"start_vllm_with_metrics.sh\",\n",
    "        \"prometheus.yml\",\n",
    "        \"vllm_dashboard.json\",\n",
    "        \"deploy_monitoring.sh\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ“ ç”Ÿæˆçš„è¨­å®šæª”æ¡ˆ:\")\n",
    "    for filename in files_to_check:\n",
    "        if os.path.exists(filename):\n",
    "            size = os.path.getsize(filename)\n",
    "            print(f\"   âœ… {filename} ({size} bytes)\")\n",
    "        else:\n",
    "            print(f\"   âŒ {filename} (éºå¤±)\")\n",
    "    \n",
    "    # æª¢æŸ¥æœå‹™ç‹€æ…‹\n",
    "    print(\"\\nğŸŒ æœå‹™ç«¯é»ç‹€æ…‹:\")\n",
    "    endpoints = {\n",
    "        \"è‡ªå®šç¾© Metrics\": \"http://127.0.0.1:8002/metrics\",\n",
    "        \"vLLM Metrics\": f\"http://127.0.0.1:{VLLM_CONFIG['metrics_port']}/metrics\",\n",
    "        \"vLLM API\": f\"http://127.0.0.1:{VLLM_CONFIG['port']}/v1/models\"\n",
    "    }\n",
    "    \n",
    "    for name, url in endpoints.items():\n",
    "        try:\n",
    "            response = requests.get(url, timeout=3)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"   âœ… {name}: {url}\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸  {name}: {url} (ç‹€æ…‹: {response.status_code})\")\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"   âŒ {name}: {url} (ç„¡æ³•é€£æ¥)\")\n",
    "    \n",
    "    print(\"\\nğŸš€ å¾ŒçºŒæ­¥é©Ÿ:\")\n",
    "    print(\"   1. å•Ÿå‹• vLLM æœå‹™: bash start_vllm_with_metrics.sh\")\n",
    "    print(\"   2. éƒ¨ç½²ç›£æ§æœå‹™: bash deploy_monitoring.sh\")\n",
    "    print(\"   3. è¨ªå• Grafana: http://localhost:3000 (admin/admin)\")\n",
    "    print(\"   4. å°å…¥å„€è¡¨æ¿: vllm_dashboard.json\")\n",
    "    print(\"   5. ç¹¼çºŒé€²è¡Œ 02-Real_Time_Metrics.ipynb\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "generate_setup_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯¦é©—ç¸½çµ\n",
    "\n",
    "æœ¬å¯¦é©—æˆåŠŸå»ºç«‹äº†å®Œæ•´çš„ vLLM ç›£æ§åŸºç¤è¨­æ–½ï¼ŒåŒ…æ‹¬ï¼š\n",
    "\n",
    "### âœ… å®Œæˆé …ç›®\n",
    "1. **ç›£æ§æ¶æ§‹è¨­è¨ˆ**: å»ºç«‹äº† Prometheus + Grafana çš„ç›£æ§æŠ€è¡“æ£§\n",
    "2. **Metrics æ”¶é›†å™¨**: é–‹ç™¼äº†è‡ªå®šç¾©çš„ vLLM æŒ‡æ¨™æ”¶é›†å™¨\n",
    "3. **é…ç½®æª”æ¡ˆç”Ÿæˆ**: è‡ªå‹•ç”Ÿæˆäº† Prometheus å’Œ Grafana çš„é…ç½®\n",
    "4. **éƒ¨ç½²è…³æœ¬**: å»ºç«‹äº†ä¸€éµéƒ¨ç½²ç›£æ§ç³»çµ±çš„è…³æœ¬\n",
    "5. **å³æ™‚è¦–è¦ºåŒ–**: å¯¦ç¾äº†åŸºç¤çš„ç›£æ§æŒ‡æ¨™è¦–è¦ºåŒ–\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒæˆæœ\n",
    "- **è‡ªå®šç¾© Metrics ä¼ºæœå™¨**: åœ¨ port 8002 æä¾›ç³»çµ±ç´šç›£æ§æŒ‡æ¨™\n",
    "- **vLLM æ•´åˆ**: æº–å‚™å¥½èˆ‡ vLLM åŸç”Ÿ metrics çš„æ•´åˆ\n",
    "- **Grafana å„€è¡¨æ¿**: é é…ç½®çš„æ•ˆèƒ½ç›£æ§å„€è¡¨æ¿\n",
    "- **Docker åŒ–éƒ¨ç½²**: å®¹å™¨åŒ–çš„ç›£æ§æœå‹™éƒ¨ç½²æ–¹æ¡ˆ\n",
    "\n",
    "### ğŸ“‹ ä¸‹ä¸€æ­¥\n",
    "ç¹¼çºŒé€²è¡Œ **02-Real_Time_Metrics.ipynb**ï¼Œå­¸ç¿’å¯¦æ™‚ç›£æ§æŒ‡æ¨™çš„æ”¶é›†èˆ‡åˆ†æã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}