{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1.2: Model Repository è¨­è¨ˆèˆ‡é…ç½®\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. **ç†è§£ Triton Model Repository æ¶æ§‹**\n",
    "   - æ¨¡å‹å€‰åº«çš„ç›®éŒ„çµæ§‹è¨­è¨ˆ\n",
    "   - é…ç½®æ–‡ä»¶ (`config.pbtxt`) çš„è©³ç´°è¨­å®š\n",
    "   - ç‰ˆæœ¬æ§åˆ¶å’Œæ¨¡å‹ç”Ÿå‘½é€±æœŸç®¡ç†\n",
    "\n",
    "2. **æŒæ¡ä¼æ¥­ç´šæ¨¡å‹ç®¡ç†æœ€ä½³å¯¦è¸**\n",
    "   - æ¨¡å‹å‘½åè¦ç¯„å’Œçµ„ç¹”ç­–ç•¥\n",
    "   - å¤šç‰ˆæœ¬æ¨¡å‹å…±å­˜å’Œåˆ‡æ›\n",
    "   - å‹•æ…‹æ¨¡å‹è¼‰å…¥å’Œå¸è¼‰\n",
    "\n",
    "3. **å¯¦ç¾å®Œæ•´çš„æ¨¡å‹éƒ¨ç½²æµç¨‹**\n",
    "   - å¾ HuggingFace Hub ä¸‹è¼‰å’Œè½‰æ›æ¨¡å‹\n",
    "   - é…ç½® PyTorch Backend æ¨¡å‹\n",
    "   - é©—è­‰æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†åŠŸèƒ½\n",
    "\n",
    "## ğŸ“‹ ä¼æ¥­æ¡ˆä¾‹èƒŒæ™¯\n",
    "\n",
    "**å ´æ™¯**: Netflix æ¨è–¦ç³»çµ±éœ€è¦ç®¡ç† 20+ å€‹ä¸åŒçš„ ML æ¨¡å‹ï¼š\n",
    "- ç”¨æˆ¶è¡Œç‚ºé æ¸¬æ¨¡å‹ (BERT-based)\n",
    "- å…§å®¹ç›¸ä¼¼åº¦æ¨¡å‹ (Sentence Transformers)\n",
    "- å€‹æ€§åŒ–æ’åºæ¨¡å‹ (Custom PyTorch)\n",
    "- A/B æ¸¬è©¦æ¨¡å‹ç‰ˆæœ¬ç®¡ç†\n",
    "\n",
    "**æŒ‘æˆ°**: å¦‚ä½•è¨­è¨ˆå¯æ“´å±•çš„æ¨¡å‹å€‰åº«æ¶æ§‹ï¼Œæ”¯æ´å‹•æ…‹æ¨¡å‹æ›´æ–°è€Œä¸å½±éŸ¿æœå‹™å¯ç”¨æ€§ï¼Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Repository åŸºç¤æ¶æ§‹\n",
    "\n",
    "### 1.1 æ¨™æº–ç›®éŒ„çµæ§‹è¨­è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# å®šç¾©æ¨¡å‹å€‰åº«æ ¹ç›®éŒ„\n",
    "MODEL_REPOSITORY_ROOT = Path(\"/tmp/triton_model_repository\")\n",
    "\n",
    "def create_model_repository_structure():\n",
    "    \"\"\"\n",
    "    å‰µå»ºæ¨™æº–çš„ Triton Model Repository ç›®éŒ„çµæ§‹\n",
    "    \n",
    "    æ¨™æº–çµæ§‹:\n",
    "    model_repository/\n",
    "    â”œâ”€â”€ model_name_1/\n",
    "    â”‚   â”œâ”€â”€ config.pbtxt\n",
    "    â”‚   â”œâ”€â”€ 1/\n",
    "    â”‚   â”‚   â””â”€â”€ model.pt\n",
    "    â”‚   â”œâ”€â”€ 2/\n",
    "    â”‚   â”‚   â””â”€â”€ model.pt\n",
    "    â”‚   â””â”€â”€ labels.txt (å¯é¸)\n",
    "    â””â”€â”€ model_name_2/\n",
    "        â”œâ”€â”€ config.pbtxt\n",
    "        â””â”€â”€ 1/\n",
    "            â””â”€â”€ model_files...\n",
    "    \"\"\"\n",
    "    \n",
    "    # å‰µå»ºæ ¹ç›®éŒ„\n",
    "    MODEL_REPOSITORY_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # ä¼æ¥­ç´šæ¨¡å‹åˆ†é¡ç›®éŒ„\n",
    "    model_categories = {\n",
    "        \"nlp_models\": [\"sentiment_analysis\", \"text_classification\", \"ner_model\"],\n",
    "        \"recommendation\": [\"user_behavior\", \"content_similarity\", \"ranking_model\"],\n",
    "        \"cv_models\": [\"image_classification\", \"object_detection\"],\n",
    "        \"custom_models\": [\"business_logic\", \"feature_extraction\"]\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ—ï¸  å‰µå»º Triton Model Repository çµæ§‹...\")\n",
    "    print(f\"ğŸ“‚ æ ¹ç›®éŒ„: {MODEL_REPOSITORY_ROOT}\")\n",
    "    print()\n",
    "    \n",
    "    for category, models in model_categories.items():\n",
    "        category_path = MODEL_REPOSITORY_ROOT / category\n",
    "        category_path.mkdir(exist_ok=True)\n",
    "        print(f\"ğŸ“ {category}/\")\n",
    "        \n",
    "        for model_name in models:\n",
    "            model_path = MODEL_REPOSITORY_ROOT / model_name\n",
    "            model_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            # å‰µå»ºç‰ˆæœ¬ç›®éŒ„ (1, 2)\n",
    "            for version in [1, 2]:\n",
    "                version_path = model_path / str(version)\n",
    "                version_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            print(f\"   â””â”€â”€ {model_name}/\")\n",
    "            print(f\"       â”œâ”€â”€ config.pbtxt\")\n",
    "            print(f\"       â”œâ”€â”€ 1/\")\n",
    "            print(f\"       â””â”€â”€ 2/\")\n",
    "    \n",
    "    return MODEL_REPOSITORY_ROOT\n",
    "\n",
    "# åŸ·è¡Œç›®éŒ„çµæ§‹å‰µå»º\n",
    "repo_root = create_model_repository_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ä¼æ¥­ç´šæ¨¡å‹å‘½åè¦ç¯„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNamingConvention:\n",
    "    \"\"\"\n",
    "    ä¼æ¥­ç´šæ¨¡å‹å‘½åè¦ç¯„ç®¡ç†å™¨\n",
    "    \n",
    "    å‘½åè¦ç¯„: {business_unit}_{model_type}_{version}_{environment}\n",
    "    ä¾‹å¦‚: netflix_recommendation_v2_prod\n",
    "    \"\"\"\n",
    "    \n",
    "    BUSINESS_UNITS = [\"netflix\", \"paypal\", \"visa\", \"general\"]\n",
    "    MODEL_TYPES = [\"nlp\", \"cv\", \"recommendation\", \"risk\", \"classification\"]\n",
    "    ENVIRONMENTS = [\"dev\", \"staging\", \"prod\"]\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_model_name(cls, business_unit: str, model_type: str, \n",
    "                          version: str, environment: str = \"prod\") -> str:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆç¬¦åˆä¼æ¥­è¦ç¯„çš„æ¨¡å‹åç¨±\n",
    "        \"\"\"\n",
    "        if business_unit not in cls.BUSINESS_UNITS:\n",
    "            raise ValueError(f\"Business unit must be one of {cls.BUSINESS_UNITS}\")\n",
    "        if model_type not in cls.MODEL_TYPES:\n",
    "            raise ValueError(f\"Model type must be one of {cls.MODEL_TYPES}\")\n",
    "        if environment not in cls.ENVIRONMENTS:\n",
    "            raise ValueError(f\"Environment must be one of {cls.ENVIRONMENTS}\")\n",
    "            \n",
    "        return f\"{business_unit}_{model_type}_{version}_{environment}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_model_name(cls, model_name: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        è§£ææ¨¡å‹åç¨±ï¼Œæå–å…ƒæ•¸æ“š\n",
    "        \"\"\"\n",
    "        parts = model_name.split(\"_\")\n",
    "        if len(parts) != 4:\n",
    "            raise ValueError(f\"Invalid model name format: {model_name}\")\n",
    "            \n",
    "        return {\n",
    "            \"business_unit\": parts[0],\n",
    "            \"model_type\": parts[1],\n",
    "            \"version\": parts[2],\n",
    "            \"environment\": parts[3]\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_model_name(cls, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        é©—è­‰æ¨¡å‹åç¨±æ˜¯å¦ç¬¦åˆè¦ç¯„\n",
    "        \"\"\"\n",
    "        try:\n",
    "            metadata = cls.parse_model_name(model_name)\n",
    "            return (\n",
    "                metadata[\"business_unit\"] in cls.BUSINESS_UNITS and\n",
    "                metadata[\"model_type\"] in cls.MODEL_TYPES and\n",
    "                metadata[\"environment\"] in cls.ENVIRONMENTS\n",
    "            )\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "# ç¤ºä¾‹ï¼šä¼æ¥­ç´šæ¨¡å‹å‘½å\n",
    "print(\"ğŸ·ï¸  ä¼æ¥­ç´šæ¨¡å‹å‘½åè¦ç¯„ç¤ºä¾‹:\")\n",
    "print()\n",
    "\n",
    "example_models = [\n",
    "    ModelNamingConvention.generate_model_name(\"netflix\", \"recommendation\", \"v2\", \"prod\"),\n",
    "    ModelNamingConvention.generate_model_name(\"paypal\", \"risk\", \"v1\", \"staging\"),\n",
    "    ModelNamingConvention.generate_model_name(\"visa\", \"classification\", \"v3\", \"prod\"),\n",
    "    ModelNamingConvention.generate_model_name(\"general\", \"nlp\", \"v1\", \"dev\")\n",
    "]\n",
    "\n",
    "for model_name in example_models:\n",
    "    metadata = ModelNamingConvention.parse_model_name(model_name)\n",
    "    valid = ModelNamingConvention.validate_model_name(model_name)\n",
    "    \n",
    "    print(f\"ğŸ“‹ {model_name}\")\n",
    "    print(f\"   æ¥­å‹™å–®ä½: {metadata['business_unit']}\")\n",
    "    print(f\"   æ¨¡å‹é¡å‹: {metadata['model_type']}\")\n",
    "    print(f\"   ç‰ˆæœ¬: {metadata['version']}\")\n",
    "    print(f\"   ç’°å¢ƒ: {metadata['environment']}\")\n",
    "    print(f\"   æœ‰æ•ˆæ€§: {'âœ…' if valid else 'âŒ'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config.pbtxt é…ç½®æ·±åº¦è§£æ\n",
    "\n",
    "### 2.1 åŸºç¤é…ç½®æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonConfigGenerator:\n",
    "    \"\"\"\n",
    "    Triton Model Configuration ç”Ÿæˆå™¨\n",
    "    æ”¯æ´å¤šç¨® Backend å’Œè¤‡é›œé…ç½®å ´æ™¯\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_pytorch_config(\n",
    "        model_name: str,\n",
    "        max_batch_size: int = 8,\n",
    "        input_specs: List[Dict] = None,\n",
    "        output_specs: List[Dict] = None,\n",
    "        instance_group: Dict = None,\n",
    "        dynamic_batching: Dict = None,\n",
    "        optimization: Dict = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆ PyTorch Backend çš„é…ç½®æ–‡ä»¶\n",
    "        \"\"\"\n",
    "        \n",
    "        # é»˜èªè¼¸å…¥è¼¸å‡ºè¦æ ¼ (BERT-like model)\n",
    "        if input_specs is None:\n",
    "            input_specs = [\n",
    "                {\n",
    "                    \"name\": \"input_ids\",\n",
    "                    \"data_type\": \"TYPE_INT64\",\n",
    "                    \"dims\": [-1]  # å¯è®Šé•·åº¦åºåˆ—\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"attention_mask\",\n",
    "                    \"data_type\": \"TYPE_INT64\",\n",
    "                    \"dims\": [-1]\n",
    "                }\n",
    "            ]\n",
    "        \n",
    "        if output_specs is None:\n",
    "            output_specs = [\n",
    "                {\n",
    "                    \"name\": \"logits\",\n",
    "                    \"data_type\": \"TYPE_FP32\",\n",
    "                    \"dims\": [2]  # äºŒåˆ†é¡è¼¸å‡º\n",
    "                }\n",
    "            ]\n",
    "        \n",
    "        # é»˜èªå¯¦ä¾‹çµ„é…ç½®\n",
    "        if instance_group is None:\n",
    "            instance_group = {\n",
    "                \"count\": 1,\n",
    "                \"kind\": \"KIND_GPU\",\n",
    "                \"gpus\": [0]\n",
    "            }\n",
    "        \n",
    "        # é»˜èªå‹•æ…‹æ‰¹è™•ç†é…ç½®\n",
    "        if dynamic_batching is None:\n",
    "            dynamic_batching = {\n",
    "                \"enabled\": True,\n",
    "                \"max_queue_delay_microseconds\": 100,\n",
    "                \"preferred_batch_size\": [4, 8]\n",
    "            }\n",
    "        \n",
    "        # ç”Ÿæˆé…ç½®å…§å®¹\n",
    "        config_lines = [\n",
    "            f'name: \"{model_name}\"',\n",
    "            'backend: \"pytorch\"',\n",
    "            f'max_batch_size: {max_batch_size}',\n",
    "            ''\n",
    "        ]\n",
    "        \n",
    "        # è¼¸å…¥é…ç½®\n",
    "        for input_spec in input_specs:\n",
    "            config_lines.extend([\n",
    "                'input [',\n",
    "                '  {',\n",
    "                f'    name: \"{input_spec[\"name\"]}\"',\n",
    "                f'    data_type: {input_spec[\"data_type\"]}',\n",
    "                f'    dims: {input_spec[\"dims\"]}',\n",
    "                '  }',\n",
    "                ']',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # è¼¸å‡ºé…ç½®\n",
    "        for output_spec in output_specs:\n",
    "            config_lines.extend([\n",
    "                'output [',\n",
    "                '  {',\n",
    "                f'    name: \"{output_spec[\"name\"]}\"',\n",
    "                f'    data_type: {output_spec[\"data_type\"]}',\n",
    "                f'    dims: {output_spec[\"dims\"]}',\n",
    "                '  }',\n",
    "                ']',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # å¯¦ä¾‹çµ„é…ç½®\n",
    "        config_lines.extend([\n",
    "            'instance_group [',\n",
    "            '  {',\n",
    "            f'    count: {instance_group[\"count\"]}',\n",
    "            f'    kind: {instance_group[\"kind\"]}',\n",
    "        ])\n",
    "        \n",
    "        if \"gpus\" in instance_group:\n",
    "            gpu_list = \", \".join(map(str, instance_group[\"gpus\"]))\n",
    "            config_lines.append(f'    gpus: [ {gpu_list} ]')\n",
    "        \n",
    "        config_lines.extend([\n",
    "            '  }',\n",
    "            ']',\n",
    "            ''\n",
    "        ])\n",
    "        \n",
    "        # å‹•æ…‹æ‰¹è™•ç†é…ç½®\n",
    "        if dynamic_batching[\"enabled\"]:\n",
    "            config_lines.extend([\n",
    "                'dynamic_batching {',\n",
    "                f'  max_queue_delay_microseconds: {dynamic_batching[\"max_queue_delay_microseconds\"]}',\n",
    "            ])\n",
    "            \n",
    "            if \"preferred_batch_size\" in dynamic_batching:\n",
    "                batch_sizes = \", \".join(map(str, dynamic_batching[\"preferred_batch_size\"]))\n",
    "                config_lines.append(f'  preferred_batch_size: [ {batch_sizes} ]')\n",
    "            \n",
    "            config_lines.extend([\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "        \n",
    "        # å„ªåŒ–é…ç½® (å¯é¸)\n",
    "        if optimization:\n",
    "            config_lines.extend([\n",
    "                'optimization {',\n",
    "                f'  execution_accelerators {{',\n",
    "                f'    gpu_execution_accelerator : [ {{',\n",
    "                f'      name : \"tensorrt\"',\n",
    "                f'      parameters {{ key: \"precision_mode\" value: \"FP16\" }}',\n",
    "                f'    }} ]',\n",
    "                f'  }}',\n",
    "                '}'\n",
    "            ])\n",
    "        \n",
    "        return '\\n'.join(config_lines)\n",
    "\n",
    "# ç”Ÿæˆä¼æ¥­ç´šæ¨¡å‹é…ç½®ç¤ºä¾‹\n",
    "print(\"âš™ï¸  ç”Ÿæˆä¼æ¥­ç´š Triton æ¨¡å‹é…ç½®...\")\n",
    "print()\n",
    "\n",
    "# Netflix æ¨è–¦ç³»çµ±æ¨¡å‹é…ç½®\n",
    "netflix_config = TritonConfigGenerator.generate_pytorch_config(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    max_batch_size=16,\n",
    "    input_specs=[\n",
    "        {\"name\": \"user_features\", \"data_type\": \"TYPE_FP32\", \"dims\": [128]},\n",
    "        {\"name\": \"item_features\", \"data_type\": \"TYPE_FP32\", \"dims\": [256]}\n",
    "    ],\n",
    "    output_specs=[\n",
    "        {\"name\": \"recommendation_scores\", \"data_type\": \"TYPE_FP32\", \"dims\": [100]}\n",
    "    ],\n",
    "    instance_group={\n",
    "        \"count\": 2,  # é›™å¯¦ä¾‹æé«˜ååé‡\n",
    "        \"kind\": \"KIND_GPU\",\n",
    "        \"gpus\": [0, 1]\n",
    "    },\n",
    "    dynamic_batching={\n",
    "        \"enabled\": True,\n",
    "        \"max_queue_delay_microseconds\": 50,  # ä½å»¶é²è¦æ±‚\n",
    "        \"preferred_batch_size\": [4, 8, 16]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ğŸ“„ Netflix æ¨è–¦ç³»çµ±æ¨¡å‹é…ç½®:\")\n",
    "print(\"```\")\n",
    "print(netflix_config)\n",
    "print(\"```\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 é«˜ç´šé…ç½®å ´æ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PayPal é¢¨æ§æ¨¡å‹é…ç½® (é«˜å®‰å…¨æ€§éœ€æ±‚)\n",
    "paypal_config = TritonConfigGenerator.generate_pytorch_config(\n",
    "    model_name=\"paypal_risk_v1_prod\",\n",
    "    max_batch_size=32,  # é«˜ååé‡æ‰¹è™•ç†\n",
    "    input_specs=[\n",
    "        {\"name\": \"transaction_features\", \"data_type\": \"TYPE_FP32\", \"dims\": [50]},\n",
    "        {\"name\": \"user_profile\", \"data_type\": \"TYPE_FP32\", \"dims\": [30]},\n",
    "        {\"name\": \"merchant_info\", \"data_type\": \"TYPE_FP32\", \"dims\": [20]}\n",
    "    ],\n",
    "    output_specs=[\n",
    "        {\"name\": \"risk_score\", \"data_type\": \"TYPE_FP32\", \"dims\": [1]},\n",
    "        {\"name\": \"fraud_probability\", \"data_type\": \"TYPE_FP32\", \"dims\": [1]}\n",
    "    ],\n",
    "    instance_group={\n",
    "        \"count\": 4,  # é«˜å¯ç”¨æ€§å¤šå¯¦ä¾‹\n",
    "        \"kind\": \"KIND_GPU\",\n",
    "        \"gpus\": [0, 1, 2, 3]\n",
    "    },\n",
    "    dynamic_batching={\n",
    "        \"enabled\": True,\n",
    "        \"max_queue_delay_microseconds\": 10,  # æ¥µä½å»¶é² (10å¾®ç§’)\n",
    "        \"preferred_batch_size\": [8, 16, 32]\n",
    "    },\n",
    "    optimization={\n",
    "        \"tensorrt_fp16\": True  # å•Ÿç”¨ TensorRT FP16 å„ªåŒ–\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ğŸ’³ PayPal é¢¨æ§æ¨¡å‹é…ç½® (é«˜å®‰å…¨æ€§):\")\n",
    "print(\"```\")\n",
    "print(paypal_config)\n",
    "print(\"```\")\n",
    "print()\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶åˆ°æ¨¡å‹å€‰åº«\n",
    "def save_model_config(model_name: str, config_content: str):\n",
    "    \"\"\"\n",
    "    ä¿å­˜æ¨¡å‹é…ç½®åˆ°å°æ‡‰çš„æ¨¡å‹ç›®éŒ„\n",
    "    \"\"\"\n",
    "    model_path = MODEL_REPOSITORY_ROOT / model_name\n",
    "    model_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    config_path = model_path / \"config.pbtxt\"\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    logger.info(f\"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}\")\n",
    "    return config_path\n",
    "\n",
    "# ä¿å­˜ä¼æ¥­ç´šæ¨¡å‹é…ç½®\n",
    "netflix_config_path = save_model_config(\"netflix_recommendation_v2_prod\", netflix_config)\n",
    "paypal_config_path = save_model_config(\"paypal_risk_v1_prod\", paypal_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å¯¦éš›æ¨¡å‹éƒ¨ç½²å¯¦è¸\n",
    "\n",
    "### 3.1 å¾ HuggingFace ä¸‹è¼‰å’Œæº–å‚™æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import numpy as np\n",
    "\n",
    "class TritonModelDeployer:\n",
    "    \"\"\"\n",
    "    Triton æ¨¡å‹éƒ¨ç½²å™¨ - è™•ç†å¾ HuggingFace åˆ° Triton çš„å®Œæ•´éƒ¨ç½²æµç¨‹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_repository_root: Path):\n",
    "        self.model_repository_root = model_repository_root\n",
    "    \n",
    "    def deploy_huggingface_model(\n",
    "        self, \n",
    "        model_name_or_path: str,\n",
    "        triton_model_name: str,\n",
    "        model_version: int = 1,\n",
    "        task_type: str = \"classification\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        éƒ¨ç½² HuggingFace æ¨¡å‹åˆ° Triton Model Repository\n",
    "        \n",
    "        Args:\n",
    "            model_name_or_path: HuggingFace æ¨¡å‹åç¨±æˆ–è·¯å¾‘\n",
    "            triton_model_name: Triton ä¸­çš„æ¨¡å‹åç¨±\n",
    "            model_version: æ¨¡å‹ç‰ˆæœ¬è™Ÿ\n",
    "            task_type: ä»»å‹™é¡å‹ (classification, regression, generation)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ğŸš€ é–‹å§‹éƒ¨ç½²æ¨¡å‹: {model_name_or_path} -> {triton_model_name}\")\n",
    "        \n",
    "        # 1. å‰µå»ºæ¨¡å‹ç›®éŒ„çµæ§‹\n",
    "        model_path = self.model_repository_root / triton_model_name\n",
    "        version_path = model_path / str(model_version)\n",
    "        version_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # 2. ä¸‹è¼‰ HuggingFace æ¨¡å‹\n",
    "            print(f\"ğŸ“¥ ä¸‹è¼‰æ¨¡å‹: {model_name_or_path}\")\n",
    "            \n",
    "            # æ¨¡æ“¬ä¸‹è¼‰éç¨‹ (åœ¨å¯¦éš›ç’°å¢ƒä¸­æœƒçœŸæ­£ä¸‹è¼‰)\n",
    "            print(\"   â”œâ”€â”€ ä¸‹è¼‰ tokenizer...\")\n",
    "            # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "            \n",
    "            print(\"   â”œâ”€â”€ ä¸‹è¼‰æ¨¡å‹é…ç½®...\")\n",
    "            # config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "            \n",
    "            print(\"   â””â”€â”€ ä¸‹è¼‰æ¨¡å‹æ¬Šé‡...\")\n",
    "            # model = AutoModel.from_pretrained(model_name_or_path)\n",
    "            \n",
    "            # 3. å‰µå»º Triton å…¼å®¹çš„æ¨¡å‹åŒ…è£å™¨\n",
    "            wrapper_code = self._generate_model_wrapper(task_type, triton_model_name)\n",
    "            wrapper_path = version_path / \"model.py\"\n",
    "            \n",
    "            with open(wrapper_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(wrapper_code)\n",
    "            \n",
    "            print(f\"ğŸ“ ç”Ÿæˆæ¨¡å‹åŒ…è£å™¨: {wrapper_path}\")\n",
    "            \n",
    "            # 4. å‰µå»ºæ¨¡å‹é…ç½®æ–‡ä»¶\n",
    "            config_content = self._generate_model_config(triton_model_name, task_type)\n",
    "            config_path = model_path / \"config.pbtxt\"\n",
    "            \n",
    "            with open(config_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(config_content)\n",
    "            \n",
    "            print(f\"âš™ï¸  ç”Ÿæˆé…ç½®æ–‡ä»¶: {config_path}\")\n",
    "            \n",
    "            # 5. å‰µå»ºæ¨¡å‹å…ƒæ•¸æ“šæ–‡ä»¶\n",
    "            metadata = {\n",
    "                \"model_name\": triton_model_name,\n",
    "                \"version\": model_version,\n",
    "                \"source_model\": model_name_or_path,\n",
    "                \"task_type\": task_type,\n",
    "                \"deployment_date\": \"2024-10-09\",\n",
    "                \"backend\": \"pytorch\"\n",
    "            }\n",
    "            \n",
    "            metadata_path = model_path / \"metadata.json\"\n",
    "            with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"ğŸ“‹ ç”Ÿæˆå…ƒæ•¸æ“šæ–‡ä»¶: {metadata_path}\")\n",
    "            \n",
    "            print(f\"âœ… æ¨¡å‹éƒ¨ç½²å®Œæˆ: {triton_model_name}\")\n",
    "            \n",
    "            return {\n",
    "                \"model_path\": str(model_path),\n",
    "                \"version_path\": str(version_path),\n",
    "                \"config_path\": str(config_path),\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹éƒ¨ç½²å¤±æ•—: {str(e)}\")\n",
    "            # æ¸…ç†å¤±æ•—çš„éƒ¨ç½²\n",
    "            if model_path.exists():\n",
    "                shutil.rmtree(model_path)\n",
    "            raise\n",
    "    \n",
    "    def _generate_model_wrapper(self, task_type: str, model_name: str) -> str:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆ Triton PyTorch Backend æ¨¡å‹åŒ…è£å™¨ä»£ç¢¼\n",
    "        \"\"\"\n",
    "        \n",
    "        if task_type == \"classification\":\n",
    "            return f'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"\n",
    "    Triton åˆ†é¡æ¨¡å‹åŒ…è£å™¨ - {model_name}\n",
    "    \n",
    "    æ”¯æ´ä¼æ¥­ç´šç‰¹æ€§:\n",
    "    - æ‰¹é‡æ¨ç†å„ªåŒ–\n",
    "    - éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„\n",
    "    - æ€§èƒ½ç›£æ§é›†æˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize(self, args):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ¨¡å‹ - åœ¨æ¨¡å‹è¼‰å…¥æ™‚åŸ·è¡Œä¸€æ¬¡\n",
    "        \"\"\"\n",
    "        # ç²å–æ¨¡å‹é…ç½®\n",
    "        self.model_config = model_config = json.loads(args[\\'model_config\\'])\n",
    "        \n",
    "        # è¨­ç½®è¼¸å‡ºé…ç½®\n",
    "        output0_config = pb_utils.get_output_config_by_name(\n",
    "            model_config, \"logits\"\n",
    "        )\n",
    "        self.output0_dtype = pb_utils.triton_string_to_numpy(\n",
    "            output0_config[\\'data_type\\'] \n",
    "        )\n",
    "        \n",
    "        # è¼‰å…¥æ¨¡å‹ (åœ¨å¯¦éš›éƒ¨ç½²ä¸­è¼‰å…¥çœŸå¯¦æ¨¡å‹)\n",
    "        print(f\"ğŸ”„ è¼‰å…¥æ¨¡å‹: {model_name}\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # æ¨¡æ“¬æ¨¡å‹è¼‰å…¥\n",
    "        # self.model = AutoModel.from_pretrained(model_path)\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        # self.model.to(self.device)\n",
    "        # self.model.eval()\n",
    "        \n",
    "        print(f\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè¨­å‚™: {{self.device}}\")\n",
    "    \n",
    "    def execute(self, requests):\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œæ¨ç† - è™•ç†æ‰¹é‡è«‹æ±‚\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            # ç²å–è¼¸å…¥æ•¸æ“š\n",
    "            input_ids = pb_utils.get_input_tensor_by_name(\n",
    "                request, \"input_ids\"\n",
    "            ).as_numpy()\n",
    "            \n",
    "            attention_mask = pb_utils.get_input_tensor_by_name(\n",
    "                request, \"attention_mask\"\n",
    "            ).as_numpy()\n",
    "            \n",
    "            # æ¨¡æ“¬æ¨ç†éç¨‹\n",
    "            batch_size = input_ids.shape[0]\n",
    "            \n",
    "            # åœ¨å¯¦éš›éƒ¨ç½²ä¸­åŸ·è¡ŒçœŸå¯¦æ¨ç†\n",
    "            # with torch.no_grad():\n",
    "            #     outputs = self.model(\n",
    "            #         input_ids=torch.tensor(input_ids).to(self.device),\n",
    "            #         attention_mask=torch.tensor(attention_mask).to(self.device)\n",
    "            #     )\n",
    "            #     logits = outputs.logits\n",
    "            \n",
    "            # æ¨¡æ“¬è¼¸å‡º (2åˆ†é¡)\n",
    "            logits = np.random.rand(batch_size, 2).astype(self.output0_dtype)\n",
    "            \n",
    "            # å‰µå»ºè¼¸å‡ºå¼µé‡\n",
    "            output_tensor = pb_utils.Tensor(\"logits\", logits)\n",
    "            \n",
    "            # å‰µå»ºéŸ¿æ‡‰\n",
    "            inference_response = pb_utils.InferenceResponse(\n",
    "                output_tensors=[output_tensor]\n",
    "            )\n",
    "            responses.append(inference_response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        æ¸…ç†è³‡æº - åœ¨æ¨¡å‹å¸è¼‰æ™‚åŸ·è¡Œ\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ§¹ æ¸…ç†æ¨¡å‹è³‡æº: {model_name}\")\n",
    "'''\n",
    "        \n",
    "        elif task_type == \"generation\":\n",
    "            # ç”Ÿæˆå¼æ¨¡å‹åŒ…è£å™¨ (ç°¡åŒ–ç‰ˆ)\n",
    "            return \"# Generation model wrapper - å¯¦ç¾é¡ä¼¼æ–¼åˆ†é¡æ¨¡å‹ï¼Œä½†è™•ç†æ–‡æœ¬ç”Ÿæˆä»»å‹™\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æŒçš„ä»»å‹™é¡å‹: {task_type}\")\n",
    "    \n",
    "    def _generate_model_config(self, model_name: str, task_type: str) -> str:\n",
    "        \"\"\"\n",
    "        æ ¹æ“šä»»å‹™é¡å‹ç”Ÿæˆç›¸æ‡‰çš„æ¨¡å‹é…ç½®\n",
    "        \"\"\"\n",
    "        \n",
    "        if task_type == \"classification\":\n",
    "            return TritonConfigGenerator.generate_pytorch_config(\n",
    "                model_name=model_name,\n",
    "                max_batch_size=8,\n",
    "                input_specs=[\n",
    "                    {\"name\": \"input_ids\", \"data_type\": \"TYPE_INT64\", \"dims\": [-1]},\n",
    "                    {\"name\": \"attention_mask\", \"data_type\": \"TYPE_INT64\", \"dims\": [-1]}\n",
    "                ],\n",
    "                output_specs=[\n",
    "                    {\"name\": \"logits\", \"data_type\": \"TYPE_FP32\", \"dims\": [2]}\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æŒçš„ä»»å‹™é¡å‹: {task_type}\")\n",
    "\n",
    "# åˆå§‹åŒ–éƒ¨ç½²å™¨\n",
    "deployer = TritonModelDeployer(MODEL_REPOSITORY_ROOT)\n",
    "\n",
    "print(\"ğŸ­ é–‹å§‹ä¼æ¥­ç´šæ¨¡å‹éƒ¨ç½²æ¼”ç¤º...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 éƒ¨ç½²ä¼æ¥­ç´šæ¨¡å‹ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éƒ¨ç½² Netflix ç”¨æˆ¶æƒ…æ„Ÿåˆ†ææ¨¡å‹\n",
    "netflix_sentiment_deployment = deployer.deploy_huggingface_model(\n",
    "    model_name_or_path=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    triton_model_name=\"netflix_sentiment_v1_prod\",\n",
    "    model_version=1,\n",
    "    task_type=\"classification\"\n",
    ")\n",
    "\n",
    "print(f\"Netflix æƒ…æ„Ÿåˆ†ææ¨¡å‹éƒ¨ç½²çµæœ:\")\n",
    "print(f\"  ğŸ“‚ æ¨¡å‹è·¯å¾‘: {netflix_sentiment_deployment['model_path']}\")\n",
    "print(f\"  ğŸ“‹ å…ƒæ•¸æ“š: {netflix_sentiment_deployment['metadata']}\")\n",
    "print()\n",
    "\n",
    "# éƒ¨ç½² PayPal æ¬ºè©æª¢æ¸¬æ¨¡å‹\n",
    "paypal_fraud_deployment = deployer.deploy_huggingface_model(\n",
    "    model_name_or_path=\"ProsusAI/finbert\",\n",
    "    triton_model_name=\"paypal_fraud_detection_v2_prod\",\n",
    "    model_version=2,\n",
    "    task_type=\"classification\"\n",
    ")\n",
    "\n",
    "print(f\"PayPal æ¬ºè©æª¢æ¸¬æ¨¡å‹éƒ¨ç½²çµæœ:\")\n",
    "print(f\"  ğŸ“‚ æ¨¡å‹è·¯å¾‘: {paypal_fraud_deployment['model_path']}\")\n",
    "print(f\"  ğŸ“‹ å…ƒæ•¸æ“š: {paypal_fraud_deployment['metadata']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’Œç”Ÿå‘½é€±æœŸ\n",
    "\n",
    "### 4.1 ä¼æ¥­ç´šç‰ˆæœ¬æ§åˆ¶ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVersionManager:\n",
    "    \"\"\"\n",
    "    ä¼æ¥­ç´šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†å™¨\n",
    "    \n",
    "    æ”¯æ´åŠŸèƒ½:\n",
    "    - èªç¾©åŒ–ç‰ˆæœ¬æ§åˆ¶ (Semantic Versioning)\n",
    "    - A/B æ¸¬è©¦ç‰ˆæœ¬ç®¡ç†\n",
    "    - å›æ»¾å’Œé‡‘çµ²é›€éƒ¨ç½²\n",
    "    - ç‰ˆæœ¬æ€§èƒ½è¿½è¹¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_repository_root: Path):\n",
    "        self.model_repository_root = model_repository_root\n",
    "        self.version_registry = {}\n",
    "    \n",
    "    def create_model_version(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        version: str,\n",
    "        description: str = \"\",\n",
    "        performance_metrics: Dict = None,\n",
    "        deployment_strategy: str = \"blue_green\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‰µå»ºæ–°çš„æ¨¡å‹ç‰ˆæœ¬\n",
    "        \n",
    "        Args:\n",
    "            model_name: æ¨¡å‹åç¨±\n",
    "            version: ç‰ˆæœ¬è™Ÿ (èªç¾©åŒ–ç‰ˆæœ¬ å¦‚ 1.2.3)\n",
    "            description: ç‰ˆæœ¬æè¿°\n",
    "            performance_metrics: æ€§èƒ½æŒ‡æ¨™\n",
    "            deployment_strategy: éƒ¨ç½²ç­–ç•¥\n",
    "        \"\"\"\n",
    "        \n",
    "        if performance_metrics is None:\n",
    "            performance_metrics = {}\n",
    "        \n",
    "        # é©—è­‰èªç¾©åŒ–ç‰ˆæœ¬æ ¼å¼\n",
    "        version_parts = version.split('.')\n",
    "        if len(version_parts) != 3:\n",
    "            raise ValueError(f\"ç‰ˆæœ¬è™Ÿå¿…é ˆéµå¾ªèªç¾©åŒ–ç‰ˆæœ¬æ ¼å¼ (å¦‚ 1.2.3): {version}\")\n",
    "        \n",
    "        try:\n",
    "            major, minor, patch = map(int, version_parts)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"ç‰ˆæœ¬è™Ÿå¿…é ˆç‚ºæ•¸å­—: {version}\")\n",
    "        \n",
    "        # å‰µå»ºç‰ˆæœ¬ç›®éŒ„\n",
    "        model_path = self.model_repository_root / model_name\n",
    "        version_path = model_path / f\"{major}_{minor}_{patch}\"  # Triton ç‰ˆæœ¬ç›®éŒ„å‘½å\n",
    "        version_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # å‰µå»ºç‰ˆæœ¬å…ƒæ•¸æ“š\n",
    "        version_metadata = {\n",
    "            \"version\": version,\n",
    "            \"major\": major,\n",
    "            \"minor\": minor,\n",
    "            \"patch\": patch,\n",
    "            \"description\": description,\n",
    "            \"created_at\": \"2024-10-09T10:00:00Z\",\n",
    "            \"deployment_strategy\": deployment_strategy,\n",
    "            \"performance_metrics\": performance_metrics,\n",
    "            \"status\": \"deployed\",\n",
    "            \"triton_version_dir\": f\"{major}_{minor}_{patch}\"\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜ç‰ˆæœ¬å…ƒæ•¸æ“š\n",
    "        metadata_path = version_path / \"version_metadata.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(version_metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # æ›´æ–°ç‰ˆæœ¬è¨»å†Šè¡¨\n",
    "        if model_name not in self.version_registry:\n",
    "            self.version_registry[model_name] = []\n",
    "        \n",
    "        self.version_registry[model_name].append(version_metadata)\n",
    "        \n",
    "        print(f\"âœ… å‰µå»ºæ¨¡å‹ç‰ˆæœ¬: {model_name} v{version}\")\n",
    "        print(f\"   ğŸ“‚ ç‰ˆæœ¬è·¯å¾‘: {version_path}\")\n",
    "        print(f\"   ğŸ“‹ éƒ¨ç½²ç­–ç•¥: {deployment_strategy}\")\n",
    "        \n",
    "        return version_metadata\n",
    "    \n",
    "    def list_model_versions(self, model_name: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        åˆ—å‡ºæ¨¡å‹çš„æ‰€æœ‰ç‰ˆæœ¬\n",
    "        \"\"\"\n",
    "        return self.version_registry.get(model_name, [])\n",
    "    \n",
    "    def get_latest_version(self, model_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        ç²å–æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬\n",
    "        \"\"\"\n",
    "        versions = self.list_model_versions(model_name)\n",
    "        if not versions:\n",
    "            raise ValueError(f\"æ¨¡å‹ {model_name} æ²’æœ‰ä»»ä½•ç‰ˆæœ¬\")\n",
    "        \n",
    "        # æŒ‰èªç¾©åŒ–ç‰ˆæœ¬æ’åº\n",
    "        sorted_versions = sorted(\n",
    "            versions, \n",
    "            key=lambda v: (v[\"major\"], v[\"minor\"], v[\"patch\"]),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return sorted_versions[0]\n",
    "    \n",
    "    def setup_ab_testing(self, model_name: str, version_a: str, version_b: str, traffic_split: float = 0.5):\n",
    "        \"\"\"\n",
    "        è¨­ç½® A/B æ¸¬è©¦ç‰ˆæœ¬åˆ†æµ\n",
    "        \n",
    "        Args:\n",
    "            model_name: æ¨¡å‹åç¨±\n",
    "            version_a: A ç‰ˆæœ¬\n",
    "            version_b: B ç‰ˆæœ¬\n",
    "            traffic_split: æµé‡åˆ†é…æ¯”ä¾‹ (0.5 = 50/50)\n",
    "        \"\"\"\n",
    "        \n",
    "        ab_config = {\n",
    "            \"model_name\": model_name,\n",
    "            \"version_a\": version_a,\n",
    "            \"version_b\": version_b,\n",
    "            \"traffic_split\": traffic_split,\n",
    "            \"start_time\": \"2024-10-09T10:00:00Z\",\n",
    "            \"status\": \"active\",\n",
    "            \"metrics_collection\": True\n",
    "        }\n",
    "        \n",
    "        # ä¿å­˜ A/B æ¸¬è©¦é…ç½®\n",
    "        model_path = self.model_repository_root / model_name\n",
    "        ab_config_path = model_path / \"ab_testing_config.json\"\n",
    "        \n",
    "        with open(ab_config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(ab_config, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"ğŸ§ª A/B æ¸¬è©¦è¨­ç½®å®Œæˆ: {model_name}\")\n",
    "        print(f\"   ğŸ…°ï¸  ç‰ˆæœ¬ A: {version_a} ({traffic_split * 100:.1f}% æµé‡)\")\n",
    "        print(f\"   ğŸ…±ï¸  ç‰ˆæœ¬ B: {version_b} ({(1-traffic_split) * 100:.1f}% æµé‡)\")\n",
    "        \n",
    "        return ab_config\n",
    "\n",
    "# åˆå§‹åŒ–ç‰ˆæœ¬ç®¡ç†å™¨\n",
    "version_manager = ModelVersionManager(MODEL_REPOSITORY_ROOT)\n",
    "\n",
    "print(\"ğŸ“Š ä¼æ¥­ç´šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†æ¼”ç¤º...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å¯¦éš›ç‰ˆæœ¬ç®¡ç†å ´æ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netflix æ¨è–¦ç³»çµ±ç‰ˆæœ¬æ¼”é€²\n",
    "print(\"ğŸ¬ Netflix æ¨è–¦ç³»çµ±ç‰ˆæœ¬ç®¡ç†å ´æ™¯:\")\n",
    "print()\n",
    "\n",
    "# ç‰ˆæœ¬ 1.0.0 - åŸºç¤æ¨è–¦ç®—æ³•\n",
    "v1_metadata = version_manager.create_model_version(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    version=\"1.0.0\",\n",
    "    description=\"åŸºç¤å”åŒéæ¿¾æ¨è–¦æ¨¡å‹\",\n",
    "    performance_metrics={\n",
    "        \"precision_at_k\": 0.85,\n",
    "        \"recall_at_k\": 0.72,\n",
    "        \"latency_p95_ms\": 45,\n",
    "        \"throughput_qps\": 1200\n",
    "    },\n",
    "    deployment_strategy=\"blue_green\"\n",
    ")\n",
    "\n",
    "# ç‰ˆæœ¬ 1.1.0 - åŠ å…¥æ·±åº¦å­¸ç¿’ç‰¹å¾µ\n",
    "v1_1_metadata = version_manager.create_model_version(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    version=\"1.1.0\",\n",
    "    description=\"åŠ å…¥ç”¨æˆ¶è¡Œç‚ºæ·±åº¦å­¸ç¿’ç‰¹å¾µ\",\n",
    "    performance_metrics={\n",
    "        \"precision_at_k\": 0.89,\n",
    "        \"recall_at_k\": 0.76,\n",
    "        \"latency_p95_ms\": 52,\n",
    "        \"throughput_qps\": 1100\n",
    "    },\n",
    "    deployment_strategy=\"canary\"\n",
    ")\n",
    "\n",
    "# ç‰ˆæœ¬ 2.0.0 - å…¨æ–° Transformer æ¶æ§‹\n",
    "v2_metadata = version_manager.create_model_version(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    version=\"2.0.0\",\n",
    "    description=\"Transformer-based åºåˆ—æ¨è–¦æ¨¡å‹\",\n",
    "    performance_metrics={\n",
    "        \"precision_at_k\": 0.93,\n",
    "        \"recall_at_k\": 0.81,\n",
    "        \"latency_p95_ms\": 38,\n",
    "        \"throughput_qps\": 1400\n",
    "    },\n",
    "    deployment_strategy=\"blue_green\"\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹ç‰ˆæœ¬æ­·å²\n",
    "versions = version_manager.list_model_versions(\"netflix_recommendation_v2_prod\")\n",
    "print(f\"ğŸ“ˆ Netflix æ¨è–¦ç³»çµ±ç‰ˆæœ¬æ­·å² ({len(versions)} å€‹ç‰ˆæœ¬):\")\n",
    "\n",
    "for version in versions:\n",
    "    print(f\"\")\n",
    "    print(f\"   ğŸ·ï¸  ç‰ˆæœ¬: {version['version']}\")\n",
    "    print(f\"   ğŸ“ æè¿°: {version['description']}\")\n",
    "    print(f\"   ğŸ¯ Precision@K: {version['performance_metrics']['precision_at_k']}\")\n",
    "    print(f\"   âš¡ å»¶é² P95: {version['performance_metrics']['latency_p95_ms']}ms\")\n",
    "    print(f\"   ğŸš€ QPS: {version['performance_metrics']['throughput_qps']}\")\n",
    "    print(f\"   ğŸ“¦ éƒ¨ç½²ç­–ç•¥: {version['deployment_strategy']}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ç²å–æœ€æ–°ç‰ˆæœ¬\n",
    "latest_version = version_manager.get_latest_version(\"netflix_recommendation_v2_prod\")\n",
    "print(f\"ğŸ” æœ€æ–°ç‰ˆæœ¬: {latest_version['version']}\")\n",
    "print(f\"   æ€§èƒ½æå‡: Precision@K {latest_version['performance_metrics']['precision_at_k']} (+{latest_version['performance_metrics']['precision_at_k'] - 0.85:.2f})\")\n",
    "print()\n",
    "\n",
    "# è¨­ç½® A/B æ¸¬è©¦ (v1.1.0 vs v2.0.0)\n",
    "ab_config = version_manager.setup_ab_testing(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    version_a=\"1.1.0\",\n",
    "    version_b=\"2.0.0\",\n",
    "    traffic_split=0.3  # 30% æµé‡çµ¦ç‰ˆæœ¬ Aï¼Œ70% çµ¦ç‰ˆæœ¬ B\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å‹•æ…‹æ¨¡å‹ç®¡ç†\n",
    "\n",
    "### 5.1 æ¨¡å‹è¼‰å…¥å’Œå¸è¼‰æ©Ÿåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "class TritonModelManager:\n",
    "    \"\"\"\n",
    "    Triton å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨\n",
    "    \n",
    "    æ”¯æ´åŠŸèƒ½:\n",
    "    - å‹•æ…‹è¼‰å…¥/å¸è¼‰æ¨¡å‹\n",
    "    - æ¨¡å‹ç‹€æ…‹ç›£æ§\n",
    "    - å„ªé›…çš„æ¨¡å‹åˆ‡æ›\n",
    "    - è³‡æºä½¿ç”¨æœ€ä½³åŒ–\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"http://localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.management_api_url = f\"{triton_url}/v2/repository\"\n",
    "    \n",
    "    def load_model(self, model_name: str, wait_for_ready: bool = True) -> bool:\n",
    "        \"\"\"\n",
    "        å‹•æ…‹è¼‰å…¥æ¨¡å‹åˆ° Triton Server\n",
    "        \n",
    "        Args:\n",
    "            model_name: è¦è¼‰å…¥çš„æ¨¡å‹åç¨±\n",
    "            wait_for_ready: æ˜¯å¦ç­‰å¾…æ¨¡å‹å®Œå…¨è¼‰å…¥\n",
    "        \n",
    "        Returns:\n",
    "            bool: è¼‰å…¥æ˜¯å¦æˆåŠŸ\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ”„ è¼‰å…¥æ¨¡å‹: {model_name}\")\n",
    "            \n",
    "            # æ¨¡æ“¬ Triton Model Management API èª¿ç”¨\n",
    "            # å¯¦éš›ä»£ç¢¼: \n",
    "            # response = requests.post(f\"{self.management_api_url}/models/{model_name}/load\")\n",
    "            \n",
    "            # æ¨¡æ“¬è¼‰å…¥éç¨‹\n",
    "            print(f\"   â”œâ”€â”€ é©—è­‰æ¨¡å‹é…ç½®...\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            print(f\"   â”œâ”€â”€ åˆ†é… GPU è³‡æº...\")\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            print(f\"   â”œâ”€â”€ è¼‰å…¥æ¨¡å‹æ¬Šé‡...\")\n",
    "            time.sleep(1.0)\n",
    "            \n",
    "            print(f\"   â””â”€â”€ åˆå§‹åŒ–æ¨ç†å¼•æ“...\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if wait_for_ready:\n",
    "                print(f\"   â³ ç­‰å¾…æ¨¡å‹å°±ç·’...\")\n",
    "                # æ¨¡æ“¬ç­‰å¾…æ¨¡å‹å°±ç·’\n",
    "                for i in range(3):\n",
    "                    time.sleep(0.5)\n",
    "                    ready = self.is_model_ready(model_name)\n",
    "                    if ready:\n",
    "                        break\n",
    "            \n",
    "            print(f\"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {model_name} - {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def unload_model(self, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        å‹•æ…‹å¸è¼‰æ¨¡å‹\n",
    "        \n",
    "        Args:\n",
    "            model_name: è¦å¸è¼‰çš„æ¨¡å‹åç¨±\n",
    "        \n",
    "        Returns:\n",
    "            bool: å¸è¼‰æ˜¯å¦æˆåŠŸ\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ”„ å¸è¼‰æ¨¡å‹: {model_name}\")\n",
    "            \n",
    "            # æ¨¡æ“¬ Triton Model Management API èª¿ç”¨\n",
    "            # response = requests.post(f\"{self.management_api_url}/models/{model_name}/unload\")\n",
    "            \n",
    "            print(f\"   â”œâ”€â”€ åœæ­¢æ¨ç†è«‹æ±‚...\")\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            print(f\"   â”œâ”€â”€ é‡‹æ”¾ GPU è¨˜æ†¶é«”...\")\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            print(f\"   â””â”€â”€ æ¸…ç†è³‡æº...\")\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "            print(f\"âœ… æ¨¡å‹å¸è¼‰æˆåŠŸ: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹å¸è¼‰å¤±æ•—: {model_name} - {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def is_model_ready(self, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²æº–å‚™å¥½æ¥å—æ¨ç†è«‹æ±‚\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æ¨¡æ“¬æ¨¡å‹ç‹€æ…‹æª¢æŸ¥\n",
    "            # response = requests.get(f\"{self.triton_url}/v2/models/{model_name}/ready\")\n",
    "            # return response.status_code == 200\n",
    "            \n",
    "            # æ¨¡æ“¬æª¢æŸ¥çµæœ\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    def get_model_status(self, model_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        ç²å–æ¨¡å‹çš„è©³ç´°ç‹€æ…‹ä¿¡æ¯\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æ¨¡æ“¬æ¨¡å‹ç‹€æ…‹ä¿¡æ¯\n",
    "            return {\n",
    "                \"name\": model_name,\n",
    "                \"state\": \"READY\",\n",
    "                \"reason\": \"\",\n",
    "                \"version\": \"1\",\n",
    "                \"backend\": \"pytorch\",\n",
    "                \"instances\": [\n",
    "                    {\n",
    "                        \"name\": f\"{model_name}_0\",\n",
    "                        \"state\": \"READY\",\n",
    "                        \"kind\": \"GPU\",\n",
    "                        \"gpu_id\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def graceful_model_switch(self, old_model: str, new_model: str) -> bool:\n",
    "        \"\"\"\n",
    "        å„ªé›…åœ°åˆ‡æ›æ¨¡å‹ç‰ˆæœ¬\n",
    "        \n",
    "        æµç¨‹:\n",
    "        1. è¼‰å…¥æ–°æ¨¡å‹\n",
    "        2. ç­‰å¾…æ–°æ¨¡å‹å°±ç·’\n",
    "        3. åŸ·è¡Œå¥åº·æª¢æŸ¥\n",
    "        4. å¸è¼‰èˆŠæ¨¡å‹\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”„ é–‹å§‹å„ªé›…æ¨¡å‹åˆ‡æ›: {old_model} â†’ {new_model}\")\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # 1. è¼‰å…¥æ–°æ¨¡å‹\n",
    "            print(f\"ğŸ“¥ ç¬¬ä¸€æ­¥: è¼‰å…¥æ–°æ¨¡å‹ {new_model}\")\n",
    "            if not self.load_model(new_model, wait_for_ready=True):\n",
    "                print(f\"âŒ æ–°æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œä¸­æ­¢åˆ‡æ›\")\n",
    "                return False\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # 2. åŸ·è¡Œå¥åº·æª¢æŸ¥\n",
    "            print(f\"ğŸ¥ ç¬¬äºŒæ­¥: åŸ·è¡Œæ–°æ¨¡å‹å¥åº·æª¢æŸ¥\")\n",
    "            health_check_passed = self._perform_health_check(new_model)\n",
    "            \n",
    "            if not health_check_passed:\n",
    "                print(f\"âŒ å¥åº·æª¢æŸ¥å¤±æ•—ï¼Œå›æ»¾æ“ä½œ\")\n",
    "                self.unload_model(new_model)\n",
    "                return False\n",
    "            \n",
    "            print(f\"âœ… å¥åº·æª¢æŸ¥é€šé\")\n",
    "            print()\n",
    "            \n",
    "            # 3. å¸è¼‰èˆŠæ¨¡å‹\n",
    "            print(f\"ğŸ“¤ ç¬¬ä¸‰æ­¥: å¸è¼‰èˆŠæ¨¡å‹ {old_model}\")\n",
    "            if not self.unload_model(old_model):\n",
    "                print(f\"âš ï¸  èˆŠæ¨¡å‹å¸è¼‰å¤±æ•—ï¼Œä½†æ–°æ¨¡å‹å·²æˆåŠŸéƒ¨ç½²\")\n",
    "            \n",
    "            print()\n",
    "            print(f\"ğŸ‰ æ¨¡å‹åˆ‡æ›å®Œæˆ: {old_model} â†’ {new_model}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹åˆ‡æ›å¤±æ•—: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _perform_health_check(self, model_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œæ¨¡å‹å¥åº·æª¢æŸ¥\n",
    "        \"\"\"\n",
    "        print(f\"   â”œâ”€â”€ æª¢æŸ¥æ¨¡å‹ç‹€æ…‹...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        print(f\"   â”œâ”€â”€ åŸ·è¡Œç¤ºä¾‹æ¨ç†...\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        print(f\"   â”œâ”€â”€ é©—è­‰è¼¸å‡ºæ ¼å¼...\")\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        print(f\"   â””â”€â”€ æª¢æŸ¥æ€§èƒ½æŒ‡æ¨™...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        return True\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨\n",
    "model_manager = TritonModelManager()\n",
    "\n",
    "print(\"ğŸ›ï¸  Triton å‹•æ…‹æ¨¡å‹ç®¡ç†æ¼”ç¤º...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 å¯¦éš›æ¨¡å‹åˆ‡æ›å ´æ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å ´æ™¯ï¼šNetflix æ¨è–¦ç³»çµ±æ¨¡å‹å‡ç´š\n",
    "print(\"ğŸ¬ Netflix æ¨è–¦ç³»çµ±æ¨¡å‹å‡ç´šå ´æ™¯:\")\n",
    "print(\"ğŸ“‹ éœ€æ±‚ï¼šå¾ v1.1.0 å‡ç´šåˆ° v2.0.0ï¼Œé›¶åœæ©Ÿæ™‚é–“\")\n",
    "print()\n",
    "\n",
    "# æ¨¡æ“¬ç•¶å‰é‹è¡Œçš„æ¨¡å‹\n",
    "current_model = \"netflix_recommendation_v1_1_prod\"\n",
    "new_model = \"netflix_recommendation_v2_0_prod\"\n",
    "\n",
    "# åŸ·è¡Œå„ªé›…åˆ‡æ›\n",
    "switch_success = model_manager.graceful_model_switch(current_model, new_model)\n",
    "\n",
    "if switch_success:\n",
    "    print(\"ğŸ“Š åˆ‡æ›å¾Œæ€§èƒ½å°æ¯”:\")\n",
    "    print(f\"   ğŸ¯ Precision@K: 0.89 â†’ 0.93 (+4.5%)\")\n",
    "    print(f\"   âš¡ å»¶é²: 52ms â†’ 38ms (-26.9%)\")\n",
    "    print(f\"   ğŸš€ ååé‡: 1100 QPS â†’ 1400 QPS (+27.3%)\")\n",
    "else:\n",
    "    print(\"âŒ æ¨¡å‹åˆ‡æ›å¤±æ•—ï¼Œä¿æŒåŸæœ‰æ¨¡å‹é‹è¡Œ\")\n",
    "\n",
    "print()\n",
    "print(\"â”€\" * 60)\n",
    "print()\n",
    "\n",
    "# å ´æ™¯ï¼šPayPal é¢¨æ§æ¨¡å‹ç·Šæ€¥å›æ»¾\n",
    "print(\"ğŸ’³ PayPal é¢¨æ§æ¨¡å‹ç·Šæ€¥å›æ»¾å ´æ™¯:\")\n",
    "print(\"ğŸ“‹ éœ€æ±‚ï¼šæª¢æ¸¬åˆ°æ–°ç‰ˆæœ¬èª¤å ±ç‡éé«˜ï¼Œç·Šæ€¥å›æ»¾åˆ°ç©©å®šç‰ˆæœ¬\")\n",
    "print()\n",
    "\n",
    "problematic_model = \"paypal_fraud_detection_v2_1_prod\"\n",
    "stable_model = \"paypal_fraud_detection_v2_0_prod\"\n",
    "\n",
    "# ç·Šæ€¥å›æ»¾\n",
    "print(f\"ğŸš¨ åŸ·è¡Œç·Šæ€¥å›æ»¾æ“ä½œ...\")\n",
    "rollback_success = model_manager.graceful_model_switch(problematic_model, stable_model)\n",
    "\n",
    "if rollback_success:\n",
    "    print(\"ğŸ“ˆ å›æ»¾å¾ŒæŒ‡æ¨™æ¢å¾©:\")\n",
    "    print(f\"   âœ… èª¤å ±ç‡: 8.5% â†’ 2.1% (æ¢å¾©æ­£å¸¸)\")\n",
    "    print(f\"   âœ… å¬å›ç‡: 89.2% â†’ 94.7% (æ¢å¾©æ­£å¸¸)\")\n",
    "    print(f\"   âœ… ç³»çµ±ç©©å®šæ€§: æ¢å¾©\")\n",
    "else:\n",
    "    print(\"âŒ ç·Šæ€¥å›æ»¾å¤±æ•—ï¼Œéœ€è¦äººå·¥ä»‹å…¥\")\n",
    "\n",
    "print()\n",
    "\n",
    "# æª¢æŸ¥æœ€çµ‚æ¨¡å‹ç‹€æ…‹\n",
    "print(\"ğŸ“‹ ç•¶å‰è¼‰å…¥çš„æ¨¡å‹ç‹€æ…‹:\")\n",
    "models_to_check = [new_model, stable_model]\n",
    "\n",
    "for model_name in models_to_check:\n",
    "    status = model_manager.get_model_status(model_name)\n",
    "    print(f\"   ğŸ” {model_name}:\")\n",
    "    print(f\"      ç‹€æ…‹: {status.get('state', 'UNKNOWN')}\")\n",
    "    print(f\"      å¾Œç«¯: {status.get('backend', 'UNKNOWN')}\")\n",
    "    print(f\"      ç‰ˆæœ¬: {status.get('version', 'UNKNOWN')}\")\n",
    "    print(f\"      å¯¦ä¾‹æ•¸: {len(status.get('instances', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä¼æ¥­ç´šæœ€ä½³å¯¦è¸ç¸½çµ\n",
    "\n",
    "### 6.1 Model Repository è¨­è¨ˆåŸå‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseModelRepositoryBestPractices:\n",
    "    \"\"\"\n",
    "    ä¼æ¥­ç´š Model Repository æœ€ä½³å¯¦è¸æŒ‡å—\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_best_practices():\n",
    "        print(\"ğŸ† ä¼æ¥­ç´š Model Repository æœ€ä½³å¯¦è¸\")\n",
    "        print(\"â•\" * 60)\n",
    "        print()\n",
    "        \n",
    "        practices = {\n",
    "            \"ğŸ—ï¸  æ¶æ§‹è¨­è¨ˆåŸå‰‡\": [\n",
    "                \"æ¡ç”¨èªç¾©åŒ–ç‰ˆæœ¬æ§åˆ¶ (Semantic Versioning)\",\n",
    "                \"å¯¦æ–½æ¸…æ™°çš„æ¨¡å‹å‘½åè¦ç¯„\",\n",
    "                \"åˆ†é›¢æ¨¡å‹é…ç½®å’Œæ¥­å‹™é‚è¼¯\",\n",
    "                \"æ”¯æ´å¤šç’°å¢ƒéƒ¨ç½² (dev/staging/prod)\",\n",
    "                \"å¯¦ç¾æ¨¡å‹å…ƒæ•¸æ“šç®¡ç†\"\n",
    "            ],\n",
    "            \"âš¡ æ€§èƒ½æœ€ä½³åŒ–\": [\n",
    "                \"é…ç½®å‹•æ…‹æ‰¹è™•ç† (Dynamic Batching)\",\n",
    "                \"ä½¿ç”¨ GPU å¯¦ä¾‹çµ„æé«˜ååé‡\",\n",
    "                \"å•Ÿç”¨ TensorRT æˆ–å…¶ä»–åŠ é€Ÿå™¨\",\n",
    "                \"å„ªåŒ–æ¨¡å‹è¼¸å…¥è¼¸å‡ºæ ¼å¼\",\n",
    "                \"å¯¦æ–½æ¨¡å‹é ç†± (Model Warmup)\"\n",
    "            ],\n",
    "            \"ğŸ”’ å®‰å…¨æ€§è€ƒé‡\": [\n",
    "                \"å¯¦æ–½æ¨¡å‹è¨ªå•æ§åˆ¶\",\n",
    "                \"åŠ å¯†æ•æ„Ÿæ¨¡å‹æ–‡ä»¶\",\n",
    "                \"å¯©è¨ˆæ¨¡å‹éƒ¨ç½²å’Œè¨ªå•æ—¥èªŒ\",\n",
    "                \"å¯¦æ–½ç¶²è·¯å®‰å…¨æªæ–½\",\n",
    "                \"å®šæœŸå®‰å…¨æ¼æ´æƒæ\"\n",
    "            ],\n",
    "            \"ğŸ“Š ç›£æ§èˆ‡å¯è§€æ¸¬æ€§\": [\n",
    "                \"å¯¦æ™‚æ€§èƒ½æŒ‡æ¨™ç›£æ§\",\n",
    "                \"æ¨¡å‹æº–ç¢ºæ€§è¿½è¹¤\",\n",
    "                \"è³‡æºä½¿ç”¨ç›£æ§\",\n",
    "                \"ç•°å¸¸æª¢æ¸¬å’Œå‘Šè­¦\",\n",
    "                \"åˆ†æ•£å¼è¿½è¹¤æ•´åˆ\"\n",
    "            ],\n",
    "            \"ğŸ”„ DevOps æ•´åˆ\": [\n",
    "                \"è‡ªå‹•åŒ–æ¨¡å‹éƒ¨ç½²æµæ°´ç·š\",\n",
    "                \"A/B æ¸¬è©¦è‡ªå‹•åŒ–\",\n",
    "                \"é‡‘çµ²é›€éƒ¨ç½²æ”¯æ´\",\n",
    "                \"è‡ªå‹•å›æ»¾æ©Ÿåˆ¶\",\n",
    "                \"å®¹å™¨åŒ–å’Œ Kubernetes æ•´åˆ\"\n",
    "            ],\n",
    "            \"ğŸ“ˆ æ“´å±•æ€§è¨­è¨ˆ\": [\n",
    "                \"æ”¯æ´æ°´å¹³æ“´å±•\",\n",
    "                \"è² è¼‰å‡è¡¡é…ç½®\",\n",
    "                \"å¤šå€åŸŸéƒ¨ç½²\",\n",
    "                \"å½ˆæ€§è³‡æºèª¿é…\",\n",
    "                \"é«˜å¯ç”¨æ€§æ¶æ§‹\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for category, items in practices.items():\n",
    "            print(f\"{category}:\")\n",
    "            for item in items:\n",
    "                print(f\"   âœ… {item}\")\n",
    "            print()\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_common_pitfalls():\n",
    "        print(\"âš ï¸  å¸¸è¦‹é™·é˜±èˆ‡è§£æ±ºæ–¹æ¡ˆ\")\n",
    "        print(\"â•\" * 60)\n",
    "        print()\n",
    "        \n",
    "        pitfalls = {\n",
    "            \"ğŸ› é…ç½®éŒ¯èª¤\": {\n",
    "                \"å•é¡Œ\": \"æ¨¡å‹è¼¸å…¥è¼¸å‡ºç¶­åº¦é…ç½®éŒ¯èª¤\",\n",
    "                \"è§£æ±ºæ–¹æ¡ˆ\": \"ä½¿ç”¨è‡ªå‹•åŒ–é…ç½®ç”Ÿæˆå’Œé©—è­‰å·¥å…·\",\n",
    "                \"é é˜²æªæ–½\": \"å¯¦æ–½é…ç½®æ¨¡æ¿å’Œæ¸¬è©¦å¥—ä»¶\"\n",
    "            },\n",
    "            \"ğŸ’¾ è¨˜æ†¶é«”æ´©æ¼\": {\n",
    "                \"å•é¡Œ\": \"æ¨¡å‹é•·æ™‚é–“é‹è¡Œå¾Œè¨˜æ†¶é«”ä½¿ç”¨æŒçºŒå¢é•·\",\n",
    "                \"è§£æ±ºæ–¹æ¡ˆ\": \"å¯¦æ–½è¨˜æ†¶é«”ç›£æ§å’Œè‡ªå‹•é‡å•Ÿæ©Ÿåˆ¶\",\n",
    "                \"é é˜²æªæ–½\": \"å®šæœŸè¨˜æ†¶é«”ä½¿ç”¨å¯©è¨ˆå’Œå„ªåŒ–\"\n",
    "            },\n",
    "            \"ğŸ”¥ ç‰ˆæœ¬è¡çª\": {\n",
    "                \"å•é¡Œ\": \"å¤šå€‹æ¨¡å‹ç‰ˆæœ¬ä¹‹é–“çš„ä¾è³´è¡çª\",\n",
    "                \"è§£æ±ºæ–¹æ¡ˆ\": \"ä½¿ç”¨å®¹å™¨éš”é›¢å’Œç‰ˆæœ¬é–å®š\",\n",
    "                \"é é˜²æªæ–½\": \"åˆ¶å®šåš´æ ¼çš„ç‰ˆæœ¬ç®¡ç†ç­–ç•¥\"\n",
    "            },\n",
    "            \"ğŸ“‰ æ€§èƒ½é€€åŒ–\": {\n",
    "                \"å•é¡Œ\": \"æ¨¡å‹æ¨ç†æ€§èƒ½éš¨æ™‚é–“é€æ¼¸ä¸‹é™\",\n",
    "                \"è§£æ±ºæ–¹æ¡ˆ\": \"æŒçºŒæ€§èƒ½ç›£æ§å’Œè‡ªå‹•èª¿å„ª\",\n",
    "                \"é é˜²æªæ–½\": \"å»ºç«‹æ€§èƒ½åŸºæº–ç·šå’Œå‘Šè­¦æ©Ÿåˆ¶\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for category, details in pitfalls.items():\n",
    "            print(f\"{category}:\")\n",
    "            print(f\"   âŒ å•é¡Œ: {details['å•é¡Œ']}\")\n",
    "            print(f\"   ğŸ’¡ è§£æ±ºæ–¹æ¡ˆ: {details['è§£æ±ºæ–¹æ¡ˆ']}\")\n",
    "            print(f\"   ğŸ›¡ï¸  é é˜²æªæ–½: {details['é é˜²æªæ–½']}\")\n",
    "            print()\n",
    "\n",
    "# é¡¯ç¤ºæœ€ä½³å¯¦è¸æŒ‡å—\n",
    "EnterpriseModelRepositoryBestPractices.print_best_practices()\n",
    "EnterpriseModelRepositoryBestPractices.print_common_pitfalls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æœ¬ç« ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒå­¸ç¿’æˆæœ\n",
    "\n",
    "é€šéæœ¬å¯¦é©—å®¤ï¼Œæ‚¨å·²ç¶“æŒæ¡äº†ï¼š\n",
    "\n",
    "1. **ğŸ—ï¸ Model Repository æ¶æ§‹è¨­è¨ˆ**\n",
    "   - æ¨™æº–ç›®éŒ„çµæ§‹å‰µå»º\n",
    "   - ä¼æ¥­ç´šå‘½åè¦ç¯„åˆ¶å®š\n",
    "   - å¤šç‰ˆæœ¬æ¨¡å‹å…±å­˜ç®¡ç†\n",
    "\n",
    "2. **âš™ï¸ é…ç½®æ–‡ä»¶æ·±åº¦å®šåˆ¶**\n",
    "   - config.pbtxt å…¨é¢é…ç½®\n",
    "   - å‹•æ…‹æ‰¹è™•ç†å„ªåŒ–\n",
    "   - å¤š GPU å¯¦ä¾‹çµ„è¨­ç½®\n",
    "\n",
    "3. **ğŸš€ æ¨¡å‹éƒ¨ç½²è‡ªå‹•åŒ–**\n",
    "   - HuggingFace æ¨¡å‹æ•´åˆ\n",
    "   - æ¨¡å‹åŒ…è£å™¨ç”Ÿæˆ\n",
    "   - å…ƒæ•¸æ“šç®¡ç†é«”ç³»\n",
    "\n",
    "4. **ğŸ“Š ç‰ˆæœ¬æ§åˆ¶èˆ‡ç”Ÿå‘½é€±æœŸ**\n",
    "   - èªç¾©åŒ–ç‰ˆæœ¬ç®¡ç†\n",
    "   - A/B æ¸¬è©¦é…ç½®\n",
    "   - å„ªé›…æ¨¡å‹åˆ‡æ›\n",
    "\n",
    "### ä¼æ¥­ç´šæŠ€èƒ½æå‡\n",
    "\n",
    "æ‚¨ç¾åœ¨å…·å‚™äº†ï¼š\n",
    "- **Netflix ç´šåˆ¥**çš„å¤šæ¨¡å‹ç®¡ç†èƒ½åŠ›\n",
    "- **PayPal ç´šåˆ¥**çš„é«˜å¯ç”¨æ€§éƒ¨ç½²æŠ€èƒ½\n",
    "- **å®Œæ•´ MLOps æµç¨‹**çš„è¨­è¨ˆå’Œå¯¦æ–½èƒ½åŠ›\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¸ç¿’è·¯å¾‘\n",
    "\n",
    "åœ¨ä¸‹ä¸€å€‹å¯¦é©—å®¤ **Lab-2.1.3: PyTorch Backend Deployment** ä¸­ï¼Œæˆ‘å€‘å°‡ï¼š\n",
    "- æ·±å…¥ PyTorch Backend çš„é«˜ç´šç‰¹æ€§\n",
    "- å¯¦ç¾è‡ªå®šç¾©æ¨ç†é‚è¼¯\n",
    "- å„ªåŒ–æ¨¡å‹æ¨ç†æ€§èƒ½\n",
    "- æ•´åˆä¼æ¥­ç´šç›£æ§ç³»çµ±\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ† æ­å–œï¼æ‚¨å·²ç¶“å®Œæˆäº† Triton Model Repository çš„ä¼æ¥­ç´šè¨­è¨ˆèˆ‡é…ç½®ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}