# LLMæ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¿«é€ŸæŒ‡å—

## ğŸ“‹ ç¸½è¦½

æœ¬æŒ‡å—æä¾›å¾æ¨¡å‹è¼‰å…¥åˆ°æ¨ç†éƒ¨ç½²çš„å®Œæ•´å·¥ç¨‹åŒ–å£“ç¸®æµç¨‹å¯¦ç”¨æ–¹æ¡ˆã€‚

> **ğŸ’¡ è©³ç´°è¨ˆç®—å…¬å¼åƒè€ƒ**: `01-Theory/Parameter_Estimation/README.md` ç¬¬0.5.6-0.5.7ç¯€

## ğŸ¯ å¿«é€Ÿæ±ºç­–è¡¨

### æ¨¡å‹è¦æ¨¡èˆ‡ç¡¬é«”åŒ¹é…

| æ¨¡å‹è¦æ¨¡ | æ¨è–¦GPU | é‡åŒ–ç­–ç•¥ | é æœŸå£“ç¸®æ¯” | é©ç”¨å ´æ™¯ |
|----------|---------|----------|------------|----------|
| <1B | RTX 4070 | INT8/INT4 | 4-8x | é‚Šç·£è¨­å‚™ |
| 1B-7B | RTX 4090 | FP16â†’INT8 | 2-4x | å€‹äººå·¥ä½œç«™ |
| 7B-30B | A100 40GB | AWQ/GPTQ | 2-4x | å°å‹æœå‹™ |
| 30B-70B | A100 80GB Ã—2 | GPTQ+åˆ†ç‰‡ | 4x+ | ä¼æ¥­æœå‹™ |
| >70B | H100 Ã—4+ | æ··åˆå£“ç¸® | è‡ªå®šç¾© | å¤§è¦æ¨¡æœå‹™ |

### å£“ç¸®æ–¹æ³•é¸æ“‡

```python
def choose_compression_method(model_size_gb: float, target_device: str) -> str:
    """å£“ç¸®æ–¹æ³•é¸æ“‡æ±ºç­–å‡½æ•¸"""

    if target_device == "edge":
        return "INT4é‡åŒ– + çµæ§‹åŒ–å‰ªæ"
    elif model_size_gb < 4:
        return "INT8 PTQ"
    elif model_size_gb < 16:
        return "AWQé‡åŒ–"
    else:
        return "GPTQ + æ¨¡å‹åˆ†ç‰‡"
```

## ğŸ”§ 4éšæ®µå¯¦æ–½æµç¨‹

### éšæ®µ1: å¿«é€Ÿåˆ†æï¼ˆ10åˆ†é˜ï¼‰
```bash
# 1. æª¢æŸ¥æ¨¡å‹åŸºæœ¬ä¿¡æ¯
python -c "from transformers import AutoModel; model=AutoModel.from_pretrained('model_name'); print(f'åƒæ•¸é‡: {model.num_parameters():,}')"

# 2. ä¼°ç®—è¨˜æ†¶é«”éœ€æ±‚
python quick_memory_check.py --model model_name --precision fp16
```

### éšæ®µ2: ç­–ç•¥é¸æ“‡ï¼ˆ5åˆ†é˜ï¼‰
```python
# å¿«é€Ÿç­–ç•¥é¸æ“‡
strategy = {
    "å°æ¨¡å‹(<3GB)": "INT8_PTQ",
    "ä¸­æ¨¡å‹(3-15GB)": "AWQ_INT8",
    "å¤§æ¨¡å‹(>15GB)": "GPTQ_INT4"
}
```

### éšæ®µ3: ä¸€éµå£“ç¸®ï¼ˆ30åˆ†é˜ï¼‰
```python
# çµ±ä¸€å£“ç¸®æ¥å£
from compression_toolkit import AutoCompress

compressor = AutoCompress(model_name, target_memory_gb=16)
compressed_model = compressor.compress()  # è‡ªå‹•é¸æ“‡æœ€ä½³ç­–ç•¥
```

### éšæ®µ4: éƒ¨ç½²é©—è­‰ï¼ˆ15åˆ†é˜ï¼‰
```python
# å¿«é€Ÿéƒ¨ç½²æ¸¬è©¦
from deployment_tester import DeploymentValidator

validator = DeploymentValidator(compressed_model)
results = validator.run_quick_test()  # åŠŸèƒ½ã€æ€§èƒ½ã€æº–ç¢ºæ€§æ¸¬è©¦
```

## ğŸ› ï¸ å¯¦ç”¨å·¥å…·è…³æœ¬

### è¨˜æ†¶é«”éœ€æ±‚å¿«é€Ÿæª¢æŸ¥
```python
# quick_memory_check.py
import sys
from transformers import AutoConfig

def quick_memory_estimate(model_name, precision='fp16'):
    config = AutoConfig.from_pretrained(model_name)

    # åŸºæœ¬åƒæ•¸é‡ä¼°ç®—
    if hasattr(config, 'n_parameters'):
        params = config.n_parameters
    else:
        # ä¼°ç®—å…¬å¼
        params = config.n_layer * config.n_embd * config.n_embd * 6

    # è¨˜æ†¶é«”ä¼°ç®—
    precision_bytes = {'fp32': 4, 'fp16': 2, 'int8': 1, 'int4': 0.5}
    model_memory_gb = params * precision_bytes[precision] / (1024**3)

    # KV Cacheä¼°ç®—ï¼ˆå‡è¨­æ‰¹æ¬¡=8ï¼Œåºåˆ—=2048ï¼‰
    kv_cache_gb = (2 * config.n_layer * 8 * config.n_head *
                   2048 * (config.n_embd // config.n_head) *
                   precision_bytes[precision]) / (1024**3)

    total_memory = model_memory_gb + kv_cache_gb

    print(f"æ¨¡å‹: {model_name}")
    print(f"åƒæ•¸é‡: {params:,} ({params/1e9:.1f}B)")
    print(f"æ¨¡å‹è¨˜æ†¶é«”: {model_memory_gb:.1f} GB")
    print(f"KV Cache: {kv_cache_gb:.1f} GB")
    print(f"ç¸½è¨˜æ†¶é«”éœ€æ±‚: {total_memory:.1f} GB")

    # GPUæ¨è–¦
    if total_memory <= 16:
        print("æ¨è–¦GPU: RTX 4090 (24GB)")
    elif total_memory <= 24:
        print("æ¨è–¦GPU: RTX A6000 (48GB)")
    elif total_memory <= 80:
        print("æ¨è–¦GPU: A100 80GB")
    else:
        print("æ¨è–¦: å¤šGPUé…ç½®æˆ–é›²ç«¯æœå‹™")

if __name__ == "__main__":
    model_name = sys.argv[1] if len(sys.argv) > 1 else "microsoft/DialoGPT-small"
    quick_memory_estimate(model_name)
```

### ä¸€éµå£“ç¸®å·¥å…·
```python
# auto_compress.py
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

class AutoCompress:
    """è‡ªå‹•å£“ç¸®å·¥å…·"""

    def __init__(self, model_name, target_memory_gb=16):
        self.model_name = model_name
        self.target_memory = target_memory_gb

    def compress(self):
        """è‡ªå‹•é¸æ“‡æœ€ä½³å£“ç¸®ç­–ç•¥ä¸¦åŸ·è¡Œ"""

        # 1. åˆ†ææ¨¡å‹å¤§å°
        original_size = self._estimate_model_size()

        # 2. é¸æ“‡å£“ç¸®ç­–ç•¥
        if original_size > self.target_memory * 4:
            strategy = "int4_nf4"
        elif original_size > self.target_memory * 2:
            strategy = "int8"
        else:
            strategy = "fp16"

        print(f"é¸æ“‡ç­–ç•¥: {strategy} (åŸå§‹å¤§å°: {original_size:.1f}GB)")

        # 3. åŸ·è¡Œå£“ç¸®
        return self._execute_compression(strategy)

    def _execute_compression(self, strategy):
        """åŸ·è¡Œå£“ç¸®"""

        if strategy == "int4_nf4":
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
        elif strategy == "int8":
            bnb_config = BitsAndBytesConfig(load_in_8bit=True)
        else:
            bnb_config = None

        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=bnb_config,
            device_map="auto"
        )

        return model, strategy
```

## âš¡ å¿«é€Ÿä¸Šæ‰‹æµç¨‹

### 5åˆ†é˜å¿«é€Ÿè©•ä¼°
```bash
# 1. æª¢æŸ¥æ¨¡å‹å¤§å°
python quick_memory_check.py "your_model_name"

# 2. é¸æ“‡å£“ç¸®ç­–ç•¥ (åŸºæ–¼ä¸Šè¿°è¼¸å‡º)

# 3. ä¸€éµå£“ç¸®
python auto_compress.py "your_model_name" --target-memory 16

# 4. é©—è­‰æ•ˆæœ
python validate_compression.py --model compressed_model_path
```

### 30åˆ†é˜å®Œæ•´æµç¨‹
1. **åˆ†æéšæ®µ(5min)**: é‹è¡Œæ¨¡å‹åˆ†æè…³æœ¬
2. **ç­–ç•¥é¸æ“‡(5min)**: åŸºæ–¼åˆ†æçµæœé¸æ“‡ç­–ç•¥
3. **åŸ·è¡Œå£“ç¸®(15min)**: é‹è¡Œå£“ç¸®è…³æœ¬
4. **æ•ˆæœé©—è­‰(5min)**: é©—è­‰å£“ç¸®æ•ˆæœ

## ğŸš¨ å¸¸è¦‹å•é¡Œè§£æ±º

### è¨˜æ†¶é«”ä¸è¶³
```python
# è§£æ±ºæ–¹æ¡ˆ1: é™ä½ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=BitsAndBytesConfig(load_in_8bit=True)
)

# è§£æ±ºæ–¹æ¡ˆ2: åˆ†ç‰‡è¼‰å…¥
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    low_cpu_mem_usage=True
)
```

### ç²¾åº¦ä¸‹é™éå¤§
```python
# è§£æ±ºæ–¹æ¡ˆ1: æ··åˆç²¾åº¦
# ä¿ç•™é‡è¦å±¤ç‚ºFP16ï¼Œå…¶ä»–å±¤INT8

# è§£æ±ºæ–¹æ¡ˆ2: æ ¡æº–æ•¸æ“šå„ªåŒ–
# ä½¿ç”¨æ›´å¤šé«˜è³ªé‡æ ¡æº–æ•¸æ“š

# è§£æ±ºæ–¹æ¡ˆ3: QATå¾®èª¿
# é‡åŒ–å¾Œé€²è¡Œå°‘é‡å¾®èª¿æ¢å¾©æ€§èƒ½
```

## ğŸ“Š æ•ˆæœè©•ä¼°æª¢æŸ¥æ¸…å–®

- [ ] **åŠŸèƒ½æ­£å¸¸**: åŸºæœ¬æ¨ç†åŠŸèƒ½ç„¡ç•°å¸¸
- [ ] **æ€§èƒ½æå‡**: æ¨ç†é€Ÿåº¦æå‡1.5x+
- [ ] **è¨˜æ†¶é«”ç¯€çœ**: è¨˜æ†¶é«”ä½¿ç”¨æ¸›å°‘50%+
- [ ] **ç²¾åº¦å¯æ¥å—**: æº–ç¢ºç‡ä¸‹é™<5%
- [ ] **ç©©å®šæ€§è‰¯å¥½**: é•·æ™‚é–“é‹è¡Œç„¡ç•°å¸¸

## ğŸ”— ç›¸é—œè³‡æº

- **ç†è«–åŸºç¤**: `01-Theory/` ç›®éŒ„ä¸‹çš„5å€‹å°ˆè«–
- **å¯¦è¸æ•™ç¨‹**: `02-Labs/Lab-0.6-Model_Compression_Engineering/`
- **è¨ˆç®—å·¥å…·**: `02-Labs/Lab-0.5-Parameter_Calculator/`
- **è©•ä¼°å·¥å…·**: `02-Labs/Lab-0.2-Evaluation_Benchmark/`

---

**âš¡ æœ€é‡è¦çš„å»ºè­°**: å…ˆé‹è¡ŒLab 0.6ç²å¾—å®Œæ•´å¯¦è¸ç¶“é©—ï¼Œå†ä½¿ç”¨æœ¬å¿«é€ŸæŒ‡å—é€²è¡Œç”Ÿç”¢ç’°å¢ƒæ‡‰ç”¨ï¼