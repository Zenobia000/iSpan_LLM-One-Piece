{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.2: PyTorch DDP 分散式訓練基礎 - 03-Optimization\n",
    "## 通訊優化與性能調優\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 注意事項\n",
    "\n",
    "本notebook專注於**DDP通訊優化的理論和實作技巧**，在單GPU環境中演示優化配置。\n",
    "- ✅ **可學習**: 通訊優化策略、性能調優技巧、配置最佳實踐\n",
    "- ⚠️ **限制**: 無法測量真實的多GPU通訊性能\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 學習目標\n",
    "\n",
    "1. 理解DDP通訊瓶頸和優化策略\n",
    "2. 掌握梯度累積和混合精度訓練\n",
    "3. 學習通訊與計算重疊技術\n",
    "4. 實作性能監控和調優工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 載入基礎設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.utils.data as data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import psutil\n",
    "import threading\n",
    "\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU 數量: {torch.cuda.device_count()}\")\n",
    "print(f\"分散式支援: {torch.distributed.is_available()}\")\n",
    "print(f\"NCCL 支援: {torch.distributed.is_nccl_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用設備: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 通訊分析工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CommunicationProfile:\n",
    "    \"\"\"通訊性能分析結果\"\"\"\n",
    "    operation: str\n",
    "    data_size_mb: float\n",
    "    time_ms: float\n",
    "    bandwidth_gbps: float\n",
    "    gpu_count: int\n",
    "    backend: str\n",
    "\n",
    "class DDPCommunicationAnalyzer:\n",
    "    \"\"\"\n",
    "    DDP通訊分析器\n",
    "    分析和優化分散式訓練中的通訊模式\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, world_size=1):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.world_size = world_size\n",
    "        self.profiles = []\n",
    "        \n",
    "        # 計算模型通訊量\n",
    "        self.model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        self.gradient_size_mb = self.model_params * 4 / (1024 * 1024)  # FP32\n",
    "        self.gradient_size_mb_fp16 = self.model_params * 2 / (1024 * 1024)  # FP16\n",
    "        \n",
    "        print(f\"=== DDP 通訊分析器初始化 ===\")\n",
    "        print(f\"模型參數量: {self.model_params:,}\")\n",
    "        print(f\"梯度大小 (FP32): {self.gradient_size_mb:.2f} MB\")\n",
    "        print(f\"梯度大小 (FP16): {self.gradient_size_mb_fp16:.2f} MB\")\n",
    "        print(f\"World Size: {self.world_size}\")\n",
    "    \n",
    "    def analyze_communication_patterns(self):\n",
    "        \"\"\"分析不同通訊模式的特性\"\"\"\n",
    "        print(\"\\n=== 通訊模式分析 ===\")\n",
    "        \n",
    "        patterns = {\n",
    "            'All-Reduce (Ring)': {\n",
    "                'description': 'Ring拓撲的All-Reduce算法',\n",
    "                'steps': self.world_size - 1,\n",
    "                'data_per_step': self.gradient_size_mb / self.world_size,\n",
    "                'total_data': self.gradient_size_mb * (self.world_size - 1) / self.world_size,\n",
    "                'latency_factor': self.world_size - 1\n",
    "            },\n",
    "            'All-Reduce (Tree)': {\n",
    "                'description': 'Tree拓撲的All-Reduce算法',\n",
    "                'steps': 2 * int(np.log2(self.world_size)),\n",
    "                'data_per_step': self.gradient_size_mb,\n",
    "                'total_data': self.gradient_size_mb * 2 * int(np.log2(self.world_size)),\n",
    "                'latency_factor': 2 * int(np.log2(self.world_size))\n",
    "            },\n",
    "            'Parameter Server': {\n",
    "                'description': '參數服務器模式',\n",
    "                'steps': 2,  # 收集 + 廣播\n",
    "                'data_per_step': self.gradient_size_mb * (self.world_size - 1),\n",
    "                'total_data': self.gradient_size_mb * 2 * (self.world_size - 1),\n",
    "                'latency_factor': 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for name, pattern in patterns.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  描述: {pattern['description']}\")\n",
    "            print(f\"  通訊步數: {pattern['steps']}\")\n",
    "            print(f\"  每步數據量: {pattern['data_per_step']:.2f} MB\")\n",
    "            print(f\"  總數據量: {pattern['total_data']:.2f} MB\")\n",
    "            print(f\"  延遲係數: {pattern['latency_factor']}\")\n",
    "            \n",
    "            # 計算理論性能\n",
    "            if self.world_size > 1:\n",
    "                efficiency = 1.0 / pattern['latency_factor']\n",
    "                print(f\"  理論效率: {efficiency:.3f}\")\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def simulate_communication_overhead(self, bandwidth_gbps=10.0, latency_us=5.0):\n",
    "        \"\"\"模擬通訊開銷\"\"\"\n",
    "        print(f\"\\n=== 通訊開銷模擬 ===\")\n",
    "        print(f\"假設條件: 帶寬 {bandwidth_gbps} Gbps, 延遲 {latency_us} μs\")\n",
    "        \n",
    "        # 不同GPU數量的通訊開銷\n",
    "        gpu_counts = [2, 4, 8, 16, 32] if self.world_size == 1 else [self.world_size]\n",
    "        \n",
    "        results = {}\n",
    "        for gpu_count in gpu_counts:\n",
    "            # Ring All-Reduce\n",
    "            chunk_size_mb = self.gradient_size_mb / gpu_count\n",
    "            transfer_time_ms = (chunk_size_mb * 8) / bandwidth_gbps  # 轉換為毫秒\n",
    "            latency_time_ms = latency_us * (gpu_count - 1) / 1000\n",
    "            total_time_ms = transfer_time_ms + latency_time_ms\n",
    "            \n",
    "            results[gpu_count] = {\n",
    "                'transfer_time_ms': transfer_time_ms,\n",
    "                'latency_time_ms': latency_time_ms,\n",
    "                'total_time_ms': total_time_ms,\n",
    "                'effective_bandwidth_gbps': (self.gradient_size_mb * 8) / (total_time_ms / 1000)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{gpu_count} GPU:\")\n",
    "            print(f\"  數據傳輸時間: {transfer_time_ms:.2f} ms\")\n",
    "            print(f\"  延遲時間: {latency_time_ms:.2f} ms\")\n",
    "            print(f\"  總通訊時間: {total_time_ms:.2f} ms\")\n",
    "            print(f\"  有效帶寬: {results[gpu_count]['effective_bandwidth_gbps']:.2f} Gbps\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_gradient_compression(self):\n",
    "        \"\"\"分析梯度壓縮技術\"\"\"\n",
    "        print(f\"\\n=== 梯度壓縮分析 ===\")\n",
    "        \n",
    "        compression_methods = {\n",
    "            'FP32 (無壓縮)': {\n",
    "                'bits_per_param': 32,\n",
    "                'compression_ratio': 1.0,\n",
    "                'accuracy_loss': 0.0\n",
    "            },\n",
    "            'FP16': {\n",
    "                'bits_per_param': 16,\n",
    "                'compression_ratio': 2.0,\n",
    "                'accuracy_loss': 0.01\n",
    "            },\n",
    "            'BF16': {\n",
    "                'bits_per_param': 16,\n",
    "                'compression_ratio': 2.0,\n",
    "                'accuracy_loss': 0.005\n",
    "            },\n",
    "            'INT8 量化': {\n",
    "                'bits_per_param': 8,\n",
    "                'compression_ratio': 4.0,\n",
    "                'accuracy_loss': 0.02\n",
    "            },\n",
    "            'Top-K稀疏化': {\n",
    "                'bits_per_param': 32,  # 保持精度\n",
    "                'compression_ratio': 10.0,  # 只傳輸10%\n",
    "                'accuracy_loss': 0.03\n",
    "            },\n",
    "            '隨機量化': {\n",
    "                'bits_per_param': 1,  # 極端壓縮\n",
    "                'compression_ratio': 32.0,\n",
    "                'accuracy_loss': 0.05\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for method, props in compression_methods.items():\n",
    "            compressed_size = self.gradient_size_mb / props['compression_ratio']\n",
    "            bandwidth_savings = (1 - 1/props['compression_ratio']) * 100\n",
    "            \n",
    "            print(f\"\\n{method}:\")\n",
    "            print(f\"  每參數位數: {props['bits_per_param']}\")\n",
    "            print(f\"  壓縮比: {props['compression_ratio']:.1f}x\")\n",
    "            print(f\"  壓縮後大小: {compressed_size:.2f} MB\")\n",
    "            print(f\"  帶寬節省: {bandwidth_savings:.1f}%\")\n",
    "            print(f\"  精度損失: {props['accuracy_loss']:.1%}\")\n",
    "        \n",
    "        return compression_methods\n",
    "\n",
    "# 創建通訊分析器\n",
    "# 使用簡單模型進行演示\n",
    "demo_model = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ").to(device)\n",
    "\n",
    "analyzer = DDPCommunicationAnalyzer(demo_model, device, world_size=4)\n",
    "\n",
    "# 執行分析\n",
    "communication_patterns = analyzer.analyze_communication_patterns()\n",
    "communication_overhead = analyzer.simulate_communication_overhead()\n",
    "compression_analysis = analyzer.analyze_gradient_compression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 梯度累積優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulationOptimizer:\n",
    "    \"\"\"\n",
    "    梯度累積優化器\n",
    "    實現高效的梯度累積和同步策略\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, accumulation_steps=4, sync_every_n_steps=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.sync_every_n_steps = sync_every_n_steps or accumulation_steps\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.accumulated_gradients = 0\n",
    "        \n",
    "        # 性能統計\n",
    "        self.sync_times = []\n",
    "        self.compute_times = []\n",
    "        \n",
    "        print(f\"=== 梯度累積優化器 ===\")\n",
    "        print(f\"累積步數: {self.accumulation_steps}\")\n",
    "        print(f\"同步間隔: {self.sync_every_n_steps}\")\n",
    "        print(f\"有效批次大小倍數: {self.accumulation_steps}x\")\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"清零梯度\"\"\"\n",
    "        if self.accumulated_gradients == 0:\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        \"\"\"反向傳播\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 縮放損失\n",
    "        scaled_loss = loss / self.accumulation_steps\n",
    "        scaled_loss.backward()\n",
    "        \n",
    "        self.accumulated_gradients += 1\n",
    "        compute_time = time.time() - start_time\n",
    "        self.compute_times.append(compute_time * 1000)  # 轉換為毫秒\n",
    "        \n",
    "        return scaled_loss.item()\n",
    "    \n",
    "    def step(self, clip_grad_norm=None):\n",
    "        \"\"\"優化器步驟\"\"\"\n",
    "        if self.accumulated_gradients >= self.accumulation_steps:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad_norm)\n",
    "            \n",
    "            # 參數更新\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            sync_time = time.time() - start_time\n",
    "            self.sync_times.append(sync_time * 1000)  # 轉換為毫秒\n",
    "            \n",
    "            self.accumulated_gradients = 0\n",
    "            self.step_count += 1\n",
    "            \n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_performance_stats(self):\n",
    "        \"\"\"獲取性能統計\"\"\"\n",
    "        stats = {\n",
    "            'avg_compute_time_ms': np.mean(self.compute_times) if self.compute_times else 0,\n",
    "            'avg_sync_time_ms': np.mean(self.sync_times) if self.sync_times else 0,\n",
    "            'total_steps': self.step_count,\n",
    "            'compute_sync_ratio': 0\n",
    "        }\n",
    "        \n",
    "        if stats['avg_sync_time_ms'] > 0:\n",
    "            stats['compute_sync_ratio'] = stats['avg_compute_time_ms'] / stats['avg_sync_time_ms']\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def analyze_gradient_accumulation_benefits():\n",
    "    \"\"\"分析梯度累積的優勢\"\"\"\n",
    "    print(\"\\n=== 梯度累積優勢分析 ===\")\n",
    "    \n",
    "    base_batch_size = 8\n",
    "    accumulation_steps_list = [1, 2, 4, 8, 16]\n",
    "    \n",
    "    for acc_steps in accumulation_steps_list:\n",
    "        effective_batch_size = base_batch_size * acc_steps\n",
    "        communication_reduction = (acc_steps - 1) / acc_steps * 100\n",
    "        memory_efficiency = 1.0  # 記憶體使用不變\n",
    "        \n",
    "        print(f\"\\n累積步數 {acc_steps}:\")\n",
    "        print(f\"  有效批次大小: {effective_batch_size}\")\n",
    "        print(f\"  通訊減少: {communication_reduction:.1f}%\")\n",
    "        print(f\"  記憶體效率: {memory_efficiency:.1f}x\")\n",
    "        print(f\"  同步頻率: 1/{acc_steps}\")\n",
    "        \n",
    "        # 理論加速比\n",
    "        if acc_steps > 1:\n",
    "            speedup = acc_steps / (1 + 0.1 * (acc_steps - 1))  # 假設10%的開銷\n",
    "            print(f\"  理論加速比: {speedup:.2f}x\")\n",
    "    \n",
    "    print(f\"\\n💡 關鍵優勢:\")\n",
    "    print(f\"  1. 減少通訊頻率 → 降低網絡開銷\")\n",
    "    print(f\"  2. 增大有效批次 → 改善訓練穩定性\")\n",
    "    print(f\"  3. 保持記憶體使用 → 適合記憶體受限環境\")\n",
    "    print(f\"  4. 更好的計算通訊比 → 提升整體效率\")\n",
    "\n",
    "# 演示梯度累積\n",
    "demo_optimizer = torch.optim.AdamW(demo_model.parameters(), lr=1e-3)\n",
    "grad_acc_optimizer = GradientAccumulationOptimizer(\n",
    "    model=demo_model,\n",
    "    optimizer=demo_optimizer,\n",
    "    accumulation_steps=4\n",
    ")\n",
    "\n",
    "# 模擬訓練步驟\n",
    "print(\"\\n=== 梯度累積演示 ===\")\n",
    "demo_input = torch.randn(8, 1024).to(device)\n",
    "demo_target = torch.randint(0, 10, (8,)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(8):\n",
    "    grad_acc_optimizer.zero_grad()\n",
    "    \n",
    "    output = demo_model(demo_input)\n",
    "    loss = criterion(output, demo_target)\n",
    "    \n",
    "    scaled_loss = grad_acc_optimizer.backward(loss)\n",
    "    step_taken = grad_acc_optimizer.step(clip_grad_norm=1.0)\n",
    "    \n",
    "    print(f\"步驟 {step+1}: 損失 {scaled_loss:.4f}, 參數更新: {step_taken}\")\n",
    "\n",
    "# 性能統計\n",
    "perf_stats = grad_acc_optimizer.get_performance_stats()\n",
    "print(f\"\\n性能統計:\")\n",
    "for key, value in perf_stats.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"  {key}: {value:.2f} ms\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "\n",
    "# 分析梯度累積優勢\n",
    "analyze_gradient_accumulation_benefits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 混合精度訓練優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecisionDDPTrainer:\n",
    "    \"\"\"\n",
    "    混合精度DDP訓練器\n",
    "    結合自動混合精度和梯度縮放\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, scheduler=None, \n",
    "                 use_amp=True, gradient_accumulation_steps=1):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.use_amp = use_amp and torch.cuda.is_available()\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        \n",
    "        # 混合精度相關\n",
    "        if self.use_amp:\n",
    "            self.scaler = GradScaler()\n",
    "            print(\"✅ 啟用自動混合精度 (AMP)\")\n",
    "        else:\n",
    "            self.scaler = None\n",
    "            print(\"⚠️ 未啟用混合精度\")\n",
    "        \n",
    "        # 性能監控\n",
    "        self.performance_metrics = {\n",
    "            'forward_time': [],\n",
    "            'backward_time': [],\n",
    "            'optimizer_time': [],\n",
    "            'memory_usage': [],\n",
    "            'loss_scale': []\n",
    "        }\n",
    "        \n",
    "        self.step_count = 0\n",
    "        self.accumulated_loss = 0.0\n",
    "    \n",
    "    def train_step(self, batch_data, criterion, clip_grad_norm=None):\n",
    "        \"\"\"單個訓練步驟\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 前向傳播\n",
    "        if self.use_amp:\n",
    "            with autocast():\n",
    "                outputs = self.model(batch_data['input'])\n",
    "                loss = criterion(outputs, batch_data['target'])\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "        else:\n",
    "            outputs = self.model(batch_data['input'])\n",
    "            loss = criterion(outputs, batch_data['target'])\n",
    "            loss = loss / self.gradient_accumulation_steps\n",
    "        \n",
    "        forward_time = time.time() - start_time\n",
    "        \n",
    "        # 反向傳播\n",
    "        start_time = time.time()\n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        backward_time = time.time() - start_time\n",
    "        self.accumulated_loss += loss.item()\n",
    "        \n",
    "        # 檢查是否需要參數更新\n",
    "        if (self.step_count + 1) % self.gradient_accumulation_steps == 0:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if self.use_amp:\n",
    "                # 梯度縮放和更新\n",
    "                if clip_grad_norm is not None:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad_norm)\n",
    "                \n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                # 記錄loss scale\n",
    "                self.performance_metrics['loss_scale'].append(self.scaler.get_scale())\n",
    "            else:\n",
    "                # 標準更新\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad_norm)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            optimizer_time = time.time() - start_time\n",
    "            \n",
    "            # 記錄性能指標\n",
    "            self.performance_metrics['forward_time'].append(forward_time * 1000)\n",
    "            self.performance_metrics['backward_time'].append(backward_time * 1000)\n",
    "            self.performance_metrics['optimizer_time'].append(optimizer_time * 1000)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                memory_usage = torch.cuda.memory_allocated() / (1024**2)  # MB\n",
    "                self.performance_metrics['memory_usage'].append(memory_usage)\n",
    "            \n",
    "            avg_loss = self.accumulated_loss\n",
    "            self.accumulated_loss = 0.0\n",
    "            \n",
    "            return avg_loss, True  # 返回損失和是否更新了參數\n",
    "        \n",
    "        self.step_count += 1\n",
    "        return loss.item(), False\n",
    "    \n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"獲取性能摘要\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for metric, values in self.performance_metrics.items():\n",
    "            if values:\n",
    "                summary[f'{metric}_avg'] = np.mean(values)\n",
    "                summary[f'{metric}_std'] = np.std(values)\n",
    "                summary[f'{metric}_max'] = np.max(values)\n",
    "                summary[f'{metric}_min'] = np.min(values)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "def compare_precision_modes():\n",
    "    \"\"\"比較不同精度模式的特性\"\"\"\n",
    "    print(\"\\n=== 精度模式比較 ===\")\n",
    "    \n",
    "    precision_modes = {\n",
    "        'FP32 (單精度)': {\n",
    "            'memory_factor': 1.0,\n",
    "            'speed_factor': 1.0,\n",
    "            'accuracy': 'highest',\n",
    "            'numerical_range': '±3.4e38',\n",
    "            'mantissa_bits': 23\n",
    "        },\n",
    "        'FP16 (半精度)': {\n",
    "            'memory_factor': 0.5,\n",
    "            'speed_factor': 1.5,  # Tensor Core加速\n",
    "            'accuracy': 'good',\n",
    "            'numerical_range': '±6.5e4',\n",
    "            'mantissa_bits': 10\n",
    "        },\n",
    "        'BF16 (Brain Float16)': {\n",
    "            'memory_factor': 0.5,\n",
    "            'speed_factor': 1.4,\n",
    "            'accuracy': 'better',\n",
    "            'numerical_range': '±3.4e38',\n",
    "            'mantissa_bits': 7\n",
    "        },\n",
    "        'Mixed Precision': {\n",
    "            'memory_factor': 0.6,  # 主要計算FP16，權重FP32\n",
    "            'speed_factor': 1.6,\n",
    "            'accuracy': 'high',\n",
    "            'numerical_range': 'adaptive',\n",
    "            'mantissa_bits': 'adaptive'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for mode, props in precision_modes.items():\n",
    "        print(f\"\\n{mode}:\")\n",
    "        print(f\"  記憶體使用: {props['memory_factor']:.1f}x\")\n",
    "        print(f\"  計算速度: {props['speed_factor']:.1f}x\")\n",
    "        print(f\"  數值精度: {props['accuracy']}\")\n",
    "        print(f\"  數值範圍: {props['numerical_range']}\")\n",
    "        print(f\"  尾數位數: {props['mantissa_bits']}\")\n",
    "    \n",
    "    print(f\"\\n🎯 選擇建議:\")\n",
    "    print(f\"  - 最高精度需求: FP32\")\n",
    "    print(f\"  - 平衡性能精度: Mixed Precision\")\n",
    "    print(f\"  - 極致性能: FP16 (需要仔細調優)\")\n",
    "    print(f\"  - 大模型訓練: BF16 (更穩定的FP16替代)\")\n",
    "\n",
    "# 演示混合精度訓練\n",
    "print(\"=== 混合精度訓練演示 ===\")\n",
    "\n",
    "# 創建訓練器\n",
    "demo_optimizer = torch.optim.AdamW(demo_model.parameters(), lr=1e-3)\n",
    "mixed_precision_trainer = MixedPrecisionDDPTrainer(\n",
    "    model=demo_model,\n",
    "    optimizer=demo_optimizer,\n",
    "    use_amp=True,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n",
    "\n",
    "# 模擬訓練數據\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for step in range(6):\n",
    "    batch_data = {\n",
    "        'input': torch.randn(8, 1024).to(device),\n",
    "        'target': torch.randint(0, 10, (8,)).to(device)\n",
    "    }\n",
    "    \n",
    "    loss, param_updated = mixed_precision_trainer.train_step(\n",
    "        batch_data, criterion, clip_grad_norm=1.0\n",
    "    )\n",
    "    \n",
    "    print(f\"步驟 {step+1}: 損失 {loss:.4f}, 參數更新: {param_updated}\")\n",
    "\n",
    "# 性能摘要\n",
    "perf_summary = mixed_precision_trainer.get_performance_summary()\n",
    "print(f\"\\n=== 性能摘要 ===\")\n",
    "for key, value in perf_summary.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"{key}: {value:.2f} ms\")\n",
    "    elif 'memory' in key:\n",
    "        print(f\"{key}: {value:.1f} MB\")\n",
    "    elif 'scale' in key:\n",
    "        print(f\"{key}: {value:.0f}\")\n",
    "\n",
    "# 比較精度模式\n",
    "compare_precision_modes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 通訊與計算重疊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationCommunicationOverlap:\n",
    "    \"\"\"\n",
    "    計算通訊重疊分析器\n",
    "    分析和優化計算與通訊的重疊策略\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.layer_info = self._analyze_model_layers()\n",
    "        \n",
    "    def _analyze_model_layers(self):\n",
    "        \"\"\"分析模型層級結構\"\"\"\n",
    "        layer_info = []\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # 葉子節點\n",
    "                param_count = sum(p.numel() for p in module.parameters())\n",
    "                if param_count > 0:\n",
    "                    layer_info.append({\n",
    "                        'name': name,\n",
    "                        'type': type(module).__name__,\n",
    "                        'parameters': param_count,\n",
    "                        'gradient_size_mb': param_count * 4 / (1024 * 1024)  # FP32\n",
    "                    })\n",
    "        \n",
    "        return layer_info\n",
    "    \n",
    "    def analyze_overlap_potential(self):\n",
    "        \"\"\"分析重疊潛力\"\"\"\n",
    "        print(\"\\n=== 計算通訊重疊分析 ===\")\n",
    "        \n",
    "        total_params = sum(layer['parameters'] for layer in self.layer_info)\n",
    "        total_gradient_mb = sum(layer['gradient_size_mb'] for layer in self.layer_info)\n",
    "        \n",
    "        print(f\"模型總參數: {total_params:,}\")\n",
    "        print(f\"總梯度大小: {total_gradient_mb:.2f} MB\")\n",
    "        print()\n",
    "        \n",
    "        print(\"層級梯度分布:\")\n",
    "        for i, layer in enumerate(self.layer_info):\n",
    "            percentage = layer['parameters'] / total_params * 100\n",
    "            print(f\"  {layer['name']:<20} ({layer['type']:<15}): \"\n",
    "                  f\"{layer['parameters']:>8,} params ({percentage:5.1f}%), \"\n",
    "                  f\"{layer['gradient_size_mb']:6.2f} MB\")\n",
    "        \n",
    "        return self.layer_info\n",
    "    \n",
    "    def simulate_overlap_strategies(self):\n",
    "        \"\"\"模擬不同重疊策略\"\"\"\n",
    "        print(f\"\\n=== 重疊策略模擬 ===\")\n",
    "        \n",
    "        # 假設的計算和通訊時間\n",
    "        compute_time_per_layer = 10  # ms\n",
    "        communication_bandwidth = 10  # GB/s\n",
    "        \n",
    "        strategies = {\n",
    "            '無重疊': {\n",
    "                'description': '串行執行計算和通訊',\n",
    "                'overlap_ratio': 0.0\n",
    "            },\n",
    "            '層級重疊': {\n",
    "                'description': '按層重疊梯度通訊',\n",
    "                'overlap_ratio': 0.7\n",
    "            },\n",
    "            '參數組重疊': {\n",
    "                'description': '按參數組重疊通訊',\n",
    "                'overlap_ratio': 0.8\n",
    "            },\n",
    "            '完全異步': {\n",
    "                'description': '完全異步計算通訊',\n",
    "                'overlap_ratio': 0.95\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        total_compute_time = len(self.layer_info) * compute_time_per_layer\n",
    "        total_gradient_mb = sum(layer['gradient_size_mb'] for layer in self.layer_info)\n",
    "        total_comm_time = total_gradient_mb * 1000 / communication_bandwidth  # ms\n",
    "        \n",
    "        print(f\"假設條件:\")\n",
    "        print(f\"  每層計算時間: {compute_time_per_layer} ms\")\n",
    "        print(f\"  通訊帶寬: {communication_bandwidth} GB/s\")\n",
    "        print(f\"  總計算時間: {total_compute_time} ms\")\n",
    "        print(f\"  總通訊時間: {total_comm_time:.1f} ms\")\n",
    "        print()\n",
    "        \n",
    "        for strategy, props in strategies.items():\n",
    "            # 計算重疊後的總時間\n",
    "            overlapped_comm_time = total_comm_time * (1 - props['overlap_ratio'])\n",
    "            total_time = total_compute_time + overlapped_comm_time\n",
    "            \n",
    "            # 計算加速比\n",
    "            baseline_time = total_compute_time + total_comm_time\n",
    "            speedup = baseline_time / total_time\n",
    "            \n",
    "            efficiency = props['overlap_ratio'] * 100\n",
    "            \n",
    "            print(f\"{strategy}:\")\n",
    "            print(f\"  描述: {props['description']}\")\n",
    "            print(f\"  重疊效率: {efficiency:.1f}%\")\n",
    "            print(f\"  總時間: {total_time:.1f} ms\")\n",
    "            print(f\"  加速比: {speedup:.2f}x\")\n",
    "            print()\n",
    "        \n",
    "        return strategies\n",
    "    \n",
    "    def recommend_optimization_strategy(self):\n",
    "        \"\"\"推薦優化策略\"\"\"\n",
    "        print(f\"\\n=== 優化策略推薦 ===\")\n",
    "        \n",
    "        total_params = sum(layer['parameters'] for layer in self.layer_info)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # 基於模型大小的推薦\n",
    "        if total_params < 1e6:  # 小模型\n",
    "            recommendations.append(\"🔹 小模型 (<1M參數): 通訊開銷相對較小，重點優化計算\")\n",
    "            recommendations.append(\"  - 使用更大的批次大小\")\n",
    "            recommendations.append(\"  - 簡單的梯度累積即可\")\n",
    "        elif total_params < 1e8:  # 中等模型\n",
    "            recommendations.append(\"🔹 中等模型 (1M-100M參數): 平衡計算和通訊優化\")\n",
    "            recommendations.append(\"  - 啟用混合精度訓練\")\n",
    "            recommendations.append(\"  - 使用層級梯度重疊\")\n",
    "            recommendations.append(\"  - 適度的梯度累積 (4-8步)\")\n",
    "        else:  # 大模型\n",
    "            recommendations.append(\"🔹 大模型 (>100M參數): 重點優化通訊\")\n",
    "            recommendations.append(\"  - 啟用所有通訊優化\")\n",
    "            recommendations.append(\"  - 使用梯度壓縮\")\n",
    "            recommendations.append(\"  - 大步數梯度累積 (16+步)\")\n",
    "            recommendations.append(\"  - 考慮ZeRO優化器\")\n",
    "        \n",
    "        # 基於層級分布的推薦\n",
    "        max_layer_params = max(layer['parameters'] for layer in self.layer_info)\n",
    "        min_layer_params = min(layer['parameters'] for layer in self.layer_info)\n",
    "        param_variance = max_layer_params / min_layer_params if min_layer_params > 0 else float('inf')\n",
    "        \n",
    "        if param_variance > 10:\n",
    "            recommendations.append(\"\\n🔹 層級參數分布不均: 使用分層通訊策略\")\n",
    "            recommendations.append(\"  - 大層單獨通訊\")\n",
    "            recommendations.append(\"  - 小層聚合通訊\")\n",
    "        else:\n",
    "            recommendations.append(\"\\n🔹 層級參數分布均勻: 使用統一通訊策略\")\n",
    "            recommendations.append(\"  - 均勻的重疊策略\")\n",
    "            recommendations.append(\"  - 標準的All-Reduce\")\n",
    "        \n",
    "        # 輸出推薦\n",
    "        for rec in recommendations:\n",
    "            print(rec)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# 創建重疊分析器\n",
    "overlap_analyzer = ComputationCommunicationOverlap(demo_model)\n",
    "\n",
    "# 執行分析\n",
    "layer_analysis = overlap_analyzer.analyze_overlap_potential()\n",
    "overlap_strategies = overlap_analyzer.simulate_overlap_strategies()\n",
    "optimization_recommendations = overlap_analyzer.recommend_optimization_strategy()\n",
    "\n",
    "# DDP特定的重疊技術\n",
    "print(f\"\\n=== DDP重疊技術詳解 ===\")\n",
    "print(f\"\\n1. 梯度重疊 (Gradient Overlapping):\")\n",
    "print(f\"   - 計算後向梯度的同時進行All-Reduce\")\n",
    "   f\"   - 使用DDP的bucket機制分批通訊\")\n",
    "print(f\"   - 參數: ddp_comm_hook, bucket_size_mb\")\n",
    "print(f\"\\n2. 參數重疊 (Parameter Overlapping):\")\n",
    "print(f\"   - 在計算梯度時重疊參數廣播\")\n",
    "print(f\"   - 適用於ZeRO-3等參數分片方案\")\n",
    "print(f\"   - 減少參數載入的等待時間\")\n",
    "print(f\"\\n3. 流水線重疊 (Pipeline Overlapping):\")\n",
    "print(f\"   - 不同層的計算和通訊流水線執行\")\n",
    "print(f\"   - 需要仔細的依賴關係管理\")\n",
    "print(f\"   - 最大化硬體利用率\")\n",
    "print(f\"\\n4. 異步通訊 (Asynchronous Communication):\")\n",
    "print(f\"   - 使用CUDA流實現真正的異步\")\n",
    "print(f\"   - 需要同步點來保證正確性\")\n",
    "print(f\"   - 平衡延遲和一致性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 性能監控與調優"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPPerformanceMonitor:\n",
    "    \"\"\"\n",
    "    DDP性能監控器\n",
    "    實時監控和分析分散式訓練性能\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, world_size=1):\n",
    "        self.model = model\n",
    "        self.world_size = world_size\n",
    "        self.monitoring_data = {\n",
    "            'timestamps': [],\n",
    "            'gpu_memory': [],\n",
    "            'gpu_utilization': [],\n",
    "            'cpu_usage': [],\n",
    "            'network_io': [],\n",
    "            'step_times': [],\n",
    "            'loss_values': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        self.is_monitoring = False\n",
    "        self.monitor_thread = None\n",
    "        \n",
    "    def start_monitoring(self, interval=1.0):\n",
    "        \"\"\"開始性能監控\"\"\"\n",
    "        if self.is_monitoring:\n",
    "            print(\"監控已在運行中\")\n",
    "            return\n",
    "        \n",
    "        self.is_monitoring = True\n",
    "        self.monitor_thread = threading.Thread(\n",
    "            target=self._monitoring_loop,\n",
    "            args=(interval,),\n",
    "            daemon=True\n",
    "        )\n",
    "        self.monitor_thread.start()\n",
    "        print(f\"✅ 開始性能監控 (間隔: {interval}s)\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"停止性能監控\"\"\"\n",
    "        if not self.is_monitoring:\n",
    "            print(\"監控未運行\")\n",
    "            return\n",
    "        \n",
    "        self.is_monitoring = False\n",
    "        if self.monitor_thread:\n",
    "            self.monitor_thread.join()\n",
    "        print(\"⏹️ 性能監控已停止\")\n",
    "    \n",
    "    def _monitoring_loop(self, interval):\n",
    "        \"\"\"監控循環\"\"\"\n",
    "        while self.is_monitoring:\n",
    "            try:\n",
    "                timestamp = time.time()\n",
    "                \n",
    "                # GPU監控\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB\n",
    "                    gpu_utilization = torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else 0\n",
    "                else:\n",
    "                    gpu_memory = 0\n",
    "                    gpu_utilization = 0\n",
    "                \n",
    "                # CPU監控\n",
    "                cpu_usage = psutil.cpu_percent()\n",
    "                \n",
    "                # 網絡IO (簡化)\n",
    "                net_io = psutil.net_io_counters()\n",
    "                network_io = net_io.bytes_sent + net_io.bytes_recv\n",
    "                \n",
    "                # 記錄數據\n",
    "                self.monitoring_data['timestamps'].append(timestamp)\n",
    "                self.monitoring_data['gpu_memory'].append(gpu_memory)\n",
    "                self.monitoring_data['gpu_utilization'].append(gpu_utilization)\n",
    "                self.monitoring_data['cpu_usage'].append(cpu_usage)\n",
    "                self.monitoring_data['network_io'].append(network_io)\n",
    "                \n",
    "                time.sleep(interval)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"監控錯誤: {e}\")\n",
    "                break\n",
    "    \n",
    "    def record_training_step(self, step_time, loss, learning_rate):\n",
    "        \"\"\"記錄訓練步驟數據\"\"\"\n",
    "        self.monitoring_data['step_times'].append(step_time)\n",
    "        self.monitoring_data['loss_values'].append(loss)\n",
    "        self.monitoring_data['learning_rates'].append(learning_rate)\n",
    "    \n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"生成性能報告\"\"\"\n",
    "        print(\"\\n=== DDP 性能報告 ===\")\n",
    "        \n",
    "        # 基本統計\n",
    "        if self.monitoring_data['step_times']:\n",
    "            avg_step_time = np.mean(self.monitoring_data['step_times'])\n",
    "            throughput = 1000 / avg_step_time  # steps per second\n",
    "            print(f\"\\n訓練性能:\")\n",
    "            print(f\"  平均步驟時間: {avg_step_time:.2f} ms\")\n",
    "            print(f\"  訓練吞吐量: {throughput:.2f} steps/sec\")\n",
    "            print(f\"  總訓練步數: {len(self.monitoring_data['step_times'])}\")\n",
    "        \n",
    "        # 資源使用統計\n",
    "        if self.monitoring_data['gpu_memory']:\n",
    "            avg_gpu_mem = np.mean(self.monitoring_data['gpu_memory'])\n",
    "            max_gpu_mem = np.max(self.monitoring_data['gpu_memory'])\n",
    "            print(f\"\\nGPU資源:\")\n",
    "            print(f\"  平均GPU記憶體: {avg_gpu_mem:.1f} MB\")\n",
    "            print(f\"  峰值GPU記憶體: {max_gpu_mem:.1f} MB\")\n",
    "        \n",
    "        if self.monitoring_data['cpu_usage']:\n",
    "            avg_cpu = np.mean(self.monitoring_data['cpu_usage'])\n",
    "            max_cpu = np.max(self.monitoring_data['cpu_usage'])\n",
    "            print(f\"\\nCPU資源:\")\n",
    "            print(f\"  平均CPU使用率: {avg_cpu:.1f}%\")\n",
    "            print(f\"  峰值CPU使用率: {max_cpu:.1f}%\")\n",
    "        \n",
    "        # 網絡統計\n",
    "        if len(self.monitoring_data['network_io']) > 1:\n",
    "            total_io = self.monitoring_data['network_io'][-1] - self.monitoring_data['network_io'][0]\n",
    "            duration = self.monitoring_data['timestamps'][-1] - self.monitoring_data['timestamps'][0]\n",
    "            avg_bandwidth = total_io / duration / (1024**2)  # MB/s\n",
    "            print(f\"\\n網絡IO:\")\n",
    "            print(f\"  總數據傳輸: {total_io / (1024**2):.1f} MB\")\n",
    "            print(f\"  平均帶寬: {avg_bandwidth:.2f} MB/s\")\n",
    "        \n",
    "        # 效率分析\n",
    "        self._analyze_efficiency()\n",
    "        \n",
    "        return self.monitoring_data\n",
    "    \n",
    "    def _analyze_efficiency(self):\n",
    "        \"\"\"分析訓練效率\"\"\"\n",
    "        print(f\"\\n=== 效率分析 ===\")\n",
    "        \n",
    "        # 計算理論vs實際性能\n",
    "        model_params = sum(p.numel() for p in self.model.parameters())\n",
    "        model_flops = model_params * 2  # 簡化估算\n",
    "        \n",
    "        if self.monitoring_data['step_times']:\n",
    "            avg_step_time_s = np.mean(self.monitoring_data['step_times']) / 1000\n",
    "            actual_flops_per_sec = model_flops / avg_step_time_s\n",
    "            \n",
    "            print(f\"計算效率:\")\n",
    "            print(f\"  模型參數量: {model_params:,}\")\n",
    "            print(f\"  估算FLOPS: {model_flops:,}\")\n",
    "            print(f\"  實際FLOPS/秒: {actual_flops_per_sec:.2e}\")\n",
    "        \n",
    "        # 通訊效率（僅多GPU時有意義）\n",
    "        if self.world_size > 1:\n",
    "            gradient_size_mb = model_params * 4 / (1024**2)  # FP32\n",
    "            print(f\"\\n通訊效率:\")\n",
    "            print(f\"  梯度大小: {gradient_size_mb:.2f} MB\")\n",
    "            print(f\"  理論通訊時間 (10GB/s): {gradient_size_mb * 8 / 10:.2f} ms\")\n",
    "            if self.monitoring_data['step_times']:\n",
    "                comm_overhead = avg_step_time_s * 1000 * 0.1  # 假設10%是通訊\n",
    "                print(f\"  估算通訊開銷: {comm_overhead:.2f} ms\")\n",
    "        \n",
    "        # 擴展效率\n",
    "        if self.world_size > 1:\n",
    "            ideal_speedup = self.world_size\n",
    "            if self.monitoring_data['step_times']:\n",
    "                # 假設單GPU基準時間\n",
    "                single_gpu_time = avg_step_time_s * self.world_size * 0.9  # 90%理想\n",
    "                actual_speedup = single_gpu_time / avg_step_time_s\n",
    "                efficiency = actual_speedup / ideal_speedup * 100\n",
    "                \n",
    "                print(f\"\\n擴展效率:\")\n",
    "                print(f\"  理想加速比: {ideal_speedup:.1f}x\")\n",
    "                print(f\"  實際加速比: {actual_speedup:.1f}x\")\n",
    "                print(f\"  擴展效率: {efficiency:.1f}%\")\n",
    "    \n",
    "    def plot_performance_curves(self):\n",
    "        \"\"\"繪製性能曲線\"\"\"\n",
    "        if not self.monitoring_data['timestamps']:\n",
    "            print(\"無監控數據可繪製\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # GPU記憶體使用\n",
    "        if self.monitoring_data['gpu_memory']:\n",
    "            axes[0, 0].plot(self.monitoring_data['gpu_memory'])\n",
    "            axes[0, 0].set_title('GPU 記憶體使用')\n",
    "            axes[0, 0].set_ylabel('記憶體 (MB)')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # CPU使用率\n",
    "        if self.monitoring_data['cpu_usage']:\n",
    "            axes[0, 1].plot(self.monitoring_data['cpu_usage'])\n",
    "            axes[0, 1].set_title('CPU 使用率')\n",
    "            axes[0, 1].set_ylabel('使用率 (%)')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 訓練步驟時間\n",
    "        if self.monitoring_data['step_times']:\n",
    "            axes[1, 0].plot(self.monitoring_data['step_times'])\n",
    "            axes[1, 0].set_title('訓練步驟時間')\n",
    "            axes[1, 0].set_ylabel('時間 (ms)')\n",
    "            axes[1, 0].set_xlabel('步驟')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 損失曲線\n",
    "        if self.monitoring_data['loss_values']:\n",
    "            axes[1, 1].plot(self.monitoring_data['loss_values'])\n",
    "            axes[1, 1].set_title('訓練損失')\n",
    "            axes[1, 1].set_ylabel('損失')\n",
    "            axes[1, 1].set_xlabel('步驟')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('ddp_performance_curves.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"📊 性能曲線已保存到 ddp_performance_curves.png\")\n",
    "\n",
    "# 演示性能監控\n",
    "print(\"=== DDP 性能監控演示 ===\")\n",
    "\n",
    "# 創建監控器\n",
    "monitor = DDPPerformanceMonitor(demo_model, world_size=4)\n",
    "\n",
    "# 開始監控\n",
    "monitor.start_monitoring(interval=0.5)\n",
    "\n",
    "# 模擬訓練過程\n",
    "print(\"\\n模擬訓練過程...\")\n",
    "optimizer = torch.optim.AdamW(demo_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(10):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 模擬前向傳播\n",
    "    inputs = torch.randn(8, 1024).to(device)\n",
    "    targets = torch.randint(0, 10, (8,)).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = demo_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    step_time = (time.time() - start_time) * 1000  # ms\n",
    "    \n",
    "    # 記錄訓練數據\n",
    "    monitor.record_training_step(step_time, loss.item(), 1e-3)\n",
    "    \n",
    "    print(f\"步驟 {step+1}: 損失 {loss.item():.4f}, 時間 {step_time:.2f}ms\")\n",
    "    \n",
    "    time.sleep(0.1)  # 模擬訓練間隔\n",
    "\n",
    "# 停止監控並生成報告\n",
    "time.sleep(1)  # 確保收集到足夠數據\n",
    "monitor.stop_monitoring()\n",
    "\n",
    "# 生成性能報告\n",
    "performance_data = monitor.generate_performance_report()\n",
    "\n",
    "# 繪製性能曲線\n",
    "monitor.plot_performance_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 優化配置生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPOptimizationConfigGenerator:\n",
    "    \"\"\"\n",
    "    DDP優化配置生成器\n",
    "    根據硬體環境和模型特性生成最優配置\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hardware_profiles = {\n",
    "            'single_gpu': {\n",
    "                'gpu_count': 1,\n",
    "                'memory_per_gpu': 16,\n",
    "                'bandwidth': 0,\n",
    "                'description': '單GPU環境'\n",
    "            },\n",
    "            'dual_gpu': {\n",
    "                'gpu_count': 2,\n",
    "                'memory_per_gpu': 16,\n",
    "                'bandwidth': 50,  # GB/s\n",
    "                'description': '雙GPU工作站'\n",
    "            },\n",
    "            'quad_gpu': {\n",
    "                'gpu_count': 4,\n",
    "                'memory_per_gpu': 24,\n",
    "                'bandwidth': 100,\n",
    "                'description': '四GPU高端工作站'\n",
    "            },\n",
    "            'dgx_node': {\n",
    "                'gpu_count': 8,\n",
    "                'memory_per_gpu': 80,\n",
    "                'bandwidth': 600,  # NVLink\n",
    "                'description': 'DGX節點'\n",
    "            },\n",
    "            'multi_node': {\n",
    "                'gpu_count': 32,\n",
    "                'memory_per_gpu': 80,\n",
    "                'bandwidth': 200,  # InfiniBand\n",
    "                'description': '多節點集群'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_model_requirements(self, model):\n",
    "        \"\"\"分析模型需求\"\"\"\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # 估算記憶體需求 (FP32)\n",
    "        model_memory = total_params * 4 / (1024**3)  # GB\n",
    "        gradient_memory = model_memory\n",
    "        optimizer_memory = model_memory * 2  # Adam state\n",
    "        activation_memory = 2  # 估算值\n",
    "        \n",
    "        total_memory = model_memory + gradient_memory + optimizer_memory + activation_memory\n",
    "        \n",
    "        return {\n",
    "            'total_params': total_params,\n",
    "            'model_memory_gb': model_memory,\n",
    "            'total_memory_gb': total_memory,\n",
    "            'gradient_size_gb': gradient_memory\n",
    "        }\n",
    "    \n",
    "    def generate_config(self, model, hardware_profile='auto', target_batch_size=32):\n",
    "        \"\"\"生成優化配置\"\"\"\n",
    "        model_reqs = self.analyze_model_requirements(model)\n",
    "        \n",
    "        if hardware_profile == 'auto':\n",
    "            hardware_profile = self._select_hardware_profile(model_reqs)\n",
    "        \n",
    "        hw_profile = self.hardware_profiles[hardware_profile]\n",
    "        \n",
    "        config = {\n",
    "            'hardware': {\n",
    "                'profile': hardware_profile,\n",
    "                'gpu_count': hw_profile['gpu_count'],\n",
    "                'memory_per_gpu': hw_profile['memory_per_gpu'],\n",
    "                'bandwidth_gbps': hw_profile['bandwidth']\n",
    "            },\n",
    "            'model': model_reqs,\n",
    "            'training': self._generate_training_config(model_reqs, hw_profile, target_batch_size),\n",
    "            'optimization': self._generate_optimization_config(model_reqs, hw_profile),\n",
    "            'monitoring': self._generate_monitoring_config(hw_profile)\n",
    "        }\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _select_hardware_profile(self, model_reqs):\n",
    "        \"\"\"自動選擇硬體配置\"\"\"\n",
    "        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "        \n",
    "        if gpu_count == 1:\n",
    "            return 'single_gpu'\n",
    "        elif gpu_count == 2:\n",
    "            return 'dual_gpu'\n",
    "        elif gpu_count <= 4:\n",
    "            return 'quad_gpu'\n",
    "        elif gpu_count <= 8:\n",
    "            return 'dgx_node'\n",
    "        else:\n",
    "            return 'multi_node'\n",
    "    \n",
    "    def _generate_training_config(self, model_reqs, hw_profile, target_batch_size):\n",
    "        \"\"\"生成訓練配置\"\"\"\n",
    "        gpu_count = hw_profile['gpu_count']\n",
    "        memory_per_gpu = hw_profile['memory_per_gpu']\n",
    "        \n",
    "        # 計算最大可行批次大小\n",
    "        available_memory = memory_per_gpu * 0.8  # 留20%緩衝\n",
    "        max_batch_per_gpu = max(1, int(available_memory / model_reqs['total_memory_gb']))\n",
    "        \n",
    "        # 計算梯度累積\n",
    "        total_batch_capacity = max_batch_per_gpu * gpu_count\n",
    "        if target_batch_size <= total_batch_capacity:\n",
    "            micro_batch_size = target_batch_size // gpu_count\n",
    "            gradient_accumulation = 1\n",
    "        else:\n",
    "            micro_batch_size = max_batch_per_gpu\n",
    "            gradient_accumulation = target_batch_size // total_batch_capacity\n",
    "        \n",
    "        # 混合精度推薦\n",
    "        use_amp = model_reqs['total_params'] > 1e6  # 大於1M參數推薦AMP\n",
    "        \n",
    "        return {\n",
    "            'batch_size_per_gpu': micro_batch_size,\n",
    "            'gradient_accumulation_steps': gradient_accumulation,\n",
    "            'effective_batch_size': micro_batch_size * gpu_count * gradient_accumulation,\n",
    "            'use_mixed_precision': use_amp,\n",
    "            'gradient_clipping': 1.0,\n",
    "            'dataloader_num_workers': min(4, max(1, gpu_count))\n",
    "        }\n",
    "    \n",
    "    def _generate_optimization_config(self, model_reqs, hw_profile):\n",
    "        \"\"\"生成優化配置\"\"\"\n",
    "        gpu_count = hw_profile['gpu_count']\n",
    "        bandwidth = hw_profile['bandwidth']\n",
    "        \n",
    "        # 通訊後端\n",
    "        if gpu_count > 1:\n",
    "            backend = 'nccl'\n",
    "        else:\n",
    "            backend = None\n",
    "        \n",
    "        # DDP參數\n",
    "        bucket_size_mb = 25  # 預設值\n",
    "        if model_reqs['gradient_size_gb'] * 1024 < bucket_size_mb:\n",
    "            bucket_size_mb = max(1, int(model_reqs['gradient_size_gb'] * 1024))\n",
    "        \n",
    "        # 通訊優化\n",
    "        use_gradient_compression = model_reqs['total_params'] > 1e8 and gpu_count > 4\n",
    "        overlap_communication = gpu_count > 2\n",
    "        \n",
    "        config = {\n",
    "            'backend': backend,\n",
    "            'bucket_size_mb': bucket_size_mb,\n",
    "            'find_unused_parameters': False,\n",
    "            'broadcast_buffers': True,\n",
    "            'gradient_as_bucket_view': True\n",
    "        }\n",
    "        \n",
    "        if use_gradient_compression:\n",
    "            config['gradient_compression'] = {\n",
    "                'method': 'fp16',\n",
    "                'ratio': 2.0\n",
    "            }\n",
    "        \n",
    "        if overlap_communication:\n",
    "            config['communication_overlap'] = {\n",
    "                'enabled': True,\n",
    "                'overlap_ratio': 0.8\n",
    "            }\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _generate_monitoring_config(self, hw_profile):\n",
    "        \"\"\"生成監控配置\"\"\"\n",
    "        return {\n",
    "            'log_interval': 50,\n",
    "            'save_interval': 1000,\n",
    "            'eval_interval': 500,\n",
    "            'monitor_gpu_memory': True,\n",
    "            'monitor_communication': hw_profile['gpu_count'] > 1,\n",
    "            'profile_steps': 100 if hw_profile['gpu_count'] > 1 else 0\n",
    "        }\n",
    "    \n",
    "    def print_config(self, config):\n",
    "        \"\"\"打印配置\"\"\"\n",
    "        print(\"\\n=== DDP 優化配置 ===\")\n",
    "        \n",
    "        print(f\"\\n🖥️ 硬體配置:\")\n",
    "        hw = config['hardware']\n",
    "        print(f\"  配置類型: {hw['profile']}\")\n",
    "        print(f\"  GPU數量: {hw['gpu_count']}\")\n",
    "        print(f\"  每GPU記憶體: {hw['memory_per_gpu']} GB\")\n",
    "        print(f\"  網絡帶寬: {hw['bandwidth_gbps']} GB/s\")\n",
    "        \n",
    "        print(f\"\\n🤖 模型分析:\")\n",
    "        model = config['model']\n",
    "        print(f\"  參數量: {model['total_params']:,}\")\n",
    "        print(f\"  模型記憶體: {model['model_memory_gb']:.2f} GB\")\n",
    "        print(f\"  總記憶體需求: {model['total_memory_gb']:.2f} GB\")\n",
    "        \n",
    "        print(f\"\\n🚀 訓練配置:\")\n",
    "        train = config['training']\n",
    "        print(f\"  每GPU批次大小: {train['batch_size_per_gpu']}\")\n",
    "        print(f\"  梯度累積步數: {train['gradient_accumulation_steps']}\")\n",
    "        print(f\"  有效批次大小: {train['effective_batch_size']}\")\n",
    "        print(f\"  混合精度: {'✅' if train['use_mixed_precision'] else '❌'}\")\n",
    "        print(f\"  梯度裁剪: {train['gradient_clipping']}\")\n",
    "        \n",
    "        print(f\"\\n⚡ 優化配置:\")\n",
    "        opt = config['optimization']\n",
    "        print(f\"  通訊後端: {opt['backend'] or 'N/A'}\")\n",
    "        print(f\"  DDP桶大小: {opt['bucket_size_mb']} MB\")\n",
    "        print(f\"  查找未使用參數: {'✅' if opt['find_unused_parameters'] else '❌'}\")\n",
    "        \n",
    "        if 'gradient_compression' in opt:\n",
    "            gc = opt['gradient_compression']\n",
    "            print(f\"  梯度壓縮: {gc['method']} ({gc['ratio']}x)\")\n",
    "        \n",
    "        if 'communication_overlap' in opt:\n",
    "            co = opt['communication_overlap']\n",
    "            print(f\"  通訊重疊: ✅ ({co['overlap_ratio']:.1%})\")\n",
    "        \n",
    "        print(f\"\\n📊 監控配置:\")\n",
    "        mon = config['monitoring']\n",
    "        print(f\"  日誌間隔: {mon['log_interval']}步\")\n",
    "        print(f\"  保存間隔: {mon['save_interval']}步\")\n",
    "        print(f\"  驗證間隔: {mon['eval_interval']}步\")\n",
    "        print(f\"  GPU監控: {'✅' if mon['monitor_gpu_memory'] else '❌'}\")\n",
    "        print(f\"  通訊監控: {'✅' if mon['monitor_communication'] else '❌'}\")\n",
    "    \n",
    "    def save_config_file(self, config, filename='ddp_config.json'):\n",
    "        \"\"\"保存配置文件\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n💾 配置已保存到 {filename}\")\n",
    "\n",
    "# 演示配置生成\n",
    "print(\"=== DDP 配置生成演示 ===\")\n",
    "\n",
    "config_generator = DDPOptimizationConfigGenerator()\n",
    "\n",
    "# 為示例模型生成配置\n",
    "optimized_config = config_generator.generate_config(\n",
    "    model=demo_model,\n",
    "    hardware_profile='auto',\n",
    "    target_batch_size=64\n",
    ")\n",
    "\n",
    "# 打印配置\n",
    "config_generator.print_config(optimized_config)\n",
    "\n",
    "# 保存配置\n",
    "config_generator.save_config_file(optimized_config, 'optimal_ddp_config.json')\n",
    "\n",
    "# 生成不同硬體環境的配置對比\n",
    "print(\"\\n=== 不同硬體環境配置對比 ===\")\n",
    "hardware_types = ['single_gpu', 'quad_gpu', 'dgx_node']\n",
    "\n",
    "for hw_type in hardware_types:\n",
    "    print(f\"\\n--- {hw_type.upper()} ---\")\n",
    "    test_config = config_generator.generate_config(\n",
    "        model=demo_model,\n",
    "        hardware_profile=hw_type,\n",
    "        target_batch_size=64\n",
    "    )\n",
    "    \n",
    "    # 只打印關鍵信息\n",
    "    hw = test_config['hardware']\n",
    "    train = test_config['training']\n",
    "    print(f\"GPU: {hw['gpu_count']}, 批次: {train['batch_size_per_gpu']}, \"\n",
    "          f\"累積: {train['gradient_accumulation_steps']}, \"\n",
    "          f\"有效批次: {train['effective_batch_size']}, \"\n",
    "          f\"AMP: {'✅' if train['use_mixed_precision'] else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 總結與最佳實踐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Lab-1.2 Optimization 完成總結 ===\")\n",
    "print()\n",
    "print(\"✅ 已完成的優化技術學習:\")\n",
    "print(\"  1. ✅ 通訊分析與瓶頸識別\")\n",
    "print(\"  2. ✅ 梯度累積優化策略\")\n",
    "print(\"  3. ✅ 混合精度訓練配置\")\n",
    "print(\"  4. ✅ 計算通訊重疊技術\")\n",
    "print(\"  5. ✅ 性能監控與分析\")\n",
    "print(\"  6. ✅ 自動化配置生成\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 關鍵優化策略總結:\")\n",
    "print()\n",
    "\n",
    "print(\"📡 通訊優化:\")\n",
    "print(\"  • 使用適當的通訊後端 (NCCL for GPU)\")\n",
    "print(\"  • 調整DDP bucket大小以平衡延遲和吞吐量\")\n",
    "print(\"  • 啟用梯度壓縮 (FP16/BF16) 減少數據傳輸\")\n",
    "print(\"  • 使用梯度累積減少通訊頻率\")\n",
    "print()\n",
    "\n",
    "print(\"💾 記憶體優化:\")\n",
    "print(\"  • 混合精度訓練 (AMP) 節省記憶體\")\n",
    "print(\"  • 梯度檢查點技術適用於超大模型\")\n",
    "print(\"  • 優化器狀態卸載到CPU (ZeRO-2)\")\n",
    "print(\"  • 動態批次大小調整\")\n",
    "print()\n",
    "\n",
    "print(\"⚡ 計算優化:\")\n",
    "print(\"  • 計算通訊重疊最大化硬體利用率\")\n",
    "print(\"  • 使用編譯優化 (torch.compile)\")\n",
    "print(\"  • Tensor Core優化混合精度運算\")\n",
    "print(\"  • 適當的數據載入並行度\")\n",
    "print()\n",
    "\n",
    "print(\"📊 監控優化:\")\n",
    "print(\"  • 實時性能監控識別瓶頸\")\n",
    "print(\"  • GPU利用率和記憶體使用追蹤\")\n",
    "print(\"  • 通訊開銷分析\")\n",
    "print(\"  • 自動化性能調優\")\n",
    "print()\n",
    "\n",
    "print(\"🔧 配置最佳實踐:\")\n",
    "optimization_best_practices = {\n",
    "    '小模型 (<10M參數)': {\n",
    "        'batch_size': '盡量大',\n",
    "        'gradient_accumulation': '1-2步',\n",
    "        'mixed_precision': '可選',\n",
    "        'communication': '標準設置'\n",
    "    },\n",
    "    '中型模型 (10M-1B參數)': {\n",
    "        'batch_size': '適中',\n",
    "        'gradient_accumulation': '4-8步',\n",
    "        'mixed_precision': '強烈推薦',\n",
    "        'communication': '梯度壓縮'\n",
    "    },\n",
    "    '大型模型 (>1B參數)': {\n",
    "        'batch_size': '記憶體限制',\n",
    "        'gradient_accumulation': '16+步',\n",
    "        'mixed_precision': '必需',\n",
    "        'communication': '全面優化'\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_size, practices in optimization_best_practices.items():\n",
    "    print(f\"\\n{model_size}:\")\n",
    "    for aspect, recommendation in practices.items():\n",
    "        print(f\"  • {aspect}: {recommendation}\")\n",
    "\n",
    "print()\n",
    "print(\"🚀 多GPU環境實戰建議:\")\n",
    "print(\"  1. 從小規模開始，逐步擴展到多GPU\")\n",
    "print(\"  2. 使用性能監控識別瓶頸\")\n",
    "print(\"  3. 測試不同的批次大小和累積策略\")\n",
    "print(\"  4. 驗證數值穩定性和收斂性\")\n",
    "print(\"  5. 建立自動化調優流程\")\n",
    "print()\n",
    "\n",
    "print(\"📁 生成的工具和配置:\")\n",
    "print(\"  • ddp_performance_curves.png: 性能監控圖表\")\n",
    "print(\"  • optimal_ddp_config.json: 優化配置文件\")\n",
    "print(\"  • DDPCommunicationAnalyzer: 通訊分析工具\")\n",
    "print(\"  • MixedPrecisionDDPTrainer: 混合精度訓練器\")\n",
    "print(\"  • DDPPerformanceMonitor: 性能監控器\")\n",
    "print(\"  • DDPOptimizationConfigGenerator: 配置生成器\")\n",
    "print()\n",
    "\n",
    "print(\"📝 下一步學習:\")\n",
    "print(\"  - 04-Advanced.ipynb: 進階技術和故障處理\")\n",
    "print(\"  - Lab-1.3: DeepSpeed ZeRO 優化\")\n",
    "print(\"  - 在真實多GPU環境中驗證優化效果\")\n",
    "print()\n",
    "\n",
    "print(\"💡 關鍵記憶點:\")\n",
    "print(\"  - 通訊開銷隨GPU數量線性增長\")\n",
    "print(\"  - 梯度累積是減少通訊的最有效方法\")\n",
    "print(\"  - 混合精度既節省記憶體又提升速度\")\n",
    "print(\"  - 計算通訊重疊可獲得顯著加速\")\n",
    "print(\"  - 性能監控是持續優化的基礎\")\n",
    "\n",
    "# 保存優化總結\n",
    "optimization_summary = {\n",
    "    'techniques_covered': [\n",
    "        'Communication Analysis',\n",
    "        'Gradient Accumulation',\n",
    "        'Mixed Precision Training',\n",
    "        'Computation-Communication Overlap',\n",
    "        'Performance Monitoring',\n",
    "        'Automated Configuration'\n",
    "    ],\n",
    "    'tools_created': [\n",
    "        'DDPCommunicationAnalyzer',\n",
    "        'GradientAccumulationOptimizer', \n",
    "        'MixedPrecisionDDPTrainer',\n",
    "        'ComputationCommunicationOverlap',\n",
    "        'DDPPerformanceMonitor',\n",
    "        'DDPOptimizationConfigGenerator'\n",
    "    ],\n",
    "    'best_practices': optimization_best_practices,\n",
    "    'files_generated': [\n",
    "        'ddp_performance_curves.png',\n",
    "        'optimal_ddp_config.json'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('ddp_optimization_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(optimization_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n💾 優化總結已保存到 ddp_optimization_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}