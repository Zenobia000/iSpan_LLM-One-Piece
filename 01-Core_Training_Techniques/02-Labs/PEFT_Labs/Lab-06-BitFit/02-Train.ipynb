{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 6: BitFit - Fine-Tuning a BERT Model by Only Training Bias Terms\n",
        "---\n",
        "## Notebook 2: The Training Process\n",
        "\n",
        "**Goal:** In this notebook, you will implement BitFit by manually selecting which parameters of a `bert-base-uncased` model to train.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Load a classification dataset and a base model.\n",
        "-   Write code to iterate through all model parameters and freeze them by default.\n",
        "-   Selectively unfreeze only the bias (`.bias`) parameters and the final classification head.\n",
        "-   Use the standard `transformers.Trainer` to fine-tune the partially frozen model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Dataset and Model\n",
        "\n",
        "This step is identical to the Adapter Layers lab. We will load the GLUE MRPC dataset, preprocess it, and load a `bert-base-uncased` model for sequence classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# --- Load Dataset and Tokenizer ---\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
        "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"])\n",
        "\n",
        "# --- Load Model ---\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "\n",
        "print(\"âœ… Dataset and model loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Implement BitFit by Freezing Parameters\n",
        "\n",
        "This is the core of the BitFit method. We will manually control which parameters are trainable. The logic is as follows:\n",
        "1.  First, freeze all parameters in the model by setting `param.requires_grad = False`.\n",
        "2.  Then, iterate through all parameters again. If a parameter's name contains `.bias`, unfreeze it by setting `param.requires_grad = True`.\n",
        "3.  Finally, explicitly unfreeze all parameters of the final classification head (named `classifier` in BERT). This is crucial so the model can adapt to the new task's output format.\n",
        "\n",
        "After this process, we'll print the number of trainable parameters to see how efficient BitFit is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Freeze all parameters first\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze bias parameters\n",
        "for name, param in model.named_parameters():\n",
        "    if \".bias\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Unfreeze the classification head\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Set Up and Run Training\n",
        "\n",
        "Now that the model is correctly configured for BitFit, we can use the standard `transformers.Trainer` to fine-tune it. The trainer will automatically detect which parameters have `requires_grad=True` and only update those during the optimization process.\n",
        "\n",
        "The training setup is identical to the Adapter Layers lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import datasets as nlp_datasets\n",
        "\n",
        "# --- Metrics Calculation Function ---\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy_metric = nlp_datasets.load_metric(\"accuracy\")\n",
        "    f1_metric = nlp_datasets.load_metric(\"f1\")\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
        "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-bitfit-mrpc\",\n",
        "    learning_rate=3e-4, # BitFit can also benefit from a slightly higher learning rate\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# --- Create Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model, # Note: We are using the modified `model` directly, not a `PeftModel`\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"ðŸš€ Starting training with BitFit...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
