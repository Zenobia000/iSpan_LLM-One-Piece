{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.2: PyTorch DDP 分散式訓練基礎 - 02-Train\n",
    "## 分散式訓練流程實作\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 注意事項\n",
    "\n",
    "本notebook在**單GPU環境**中演示DDP訓練流程的**概念和代碼結構**。\n",
    "- ✅ **可學習**: 完整的訓練邏輯、梯度同步機制、性能監控\n",
    "- ⚠️ **限制**: 無法展示真正的多GPU加速和通訊效果\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 學習目標\n",
    "\n",
    "1. 實作完整的DDP訓練循環\n",
    "2. 理解梯度同步和參數更新機制\n",
    "3. 學習分散式訓練的日誌和監控\n",
    "4. 掌握檢查點保存和載入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 載入設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 載入前一個notebook的設置\n",
    "if os.path.exists('ddp_setup.pth'):\n",
    "    setup_data = torch.load('ddp_setup.pth', map_location='cpu')\n",
    "    config = setup_data['config']\n",
    "    device = torch.device(setup_data['device'])\n",
    "    print(\"✅ 成功載入前一步的配置\")\n",
    "else:\n",
    "    print(\"⚠️ 未找到setup配置，請先運行 01-Setup.ipynb\")\n",
    "    # 提供預設配置\n",
    "    config = {\n",
    "        'batch_size': 8, 'learning_rate': 5e-4, 'num_epochs': 3,\n",
    "        'warmup_steps': 100, 'weight_decay': 0.01,\n",
    "        'vocab_size': 8000, 'd_model': 256, 'nhead': 8, \n",
    "        'num_layers': 4, 'max_seq_len': 128\n",
    "    }\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"設備: {device}\")\n",
    "print(f\"批次大小: {config['batch_size']}\")\n",
    "print(f\"學習率: {config['learning_rate']}\")\n",
    "print(f\"訓練輪數: {config['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 重新創建模型和數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新定義模型和數據集（與01-Setup相同）\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=8000, d_model=256, nhead=8, num_layers=4, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.embedding(input_ids) + self.pos_encoding(positions)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.bool()\n",
    "            attention_mask = ~attention_mask\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=attention_mask)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTextDataset(data.Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_len=128, vocab_size=8000):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.data = np.random.randint(1, vocab_size, (num_samples, seq_len))\n",
    "        self.attention_masks = np.ones((num_samples, seq_len))\n",
    "        for i in range(num_samples):\n",
    "            actual_len = np.random.randint(seq_len // 2, seq_len + 1)\n",
    "            self.attention_masks[i, actual_len:] = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.attention_masks[idx], dtype=torch.long)\n",
    "        labels = torch.cat([input_ids[1:], torch.tensor([0])], dim=0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# 創建模型和數據集\n",
    "model = SimpleTransformer(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    nhead=config['nhead'],\n",
    "    num_layers=config['num_layers'],\n",
    "    max_seq_len=config['max_seq_len']\n",
    ").to(device)\n",
    "\n",
    "train_dataset = DummyTextDataset(num_samples=2000, seq_len=config['max_seq_len'])\n",
    "val_dataset = DummyTextDataset(num_samples=400, seq_len=config['max_seq_len'])\n",
    "\n",
    "print(f\"模型參數量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"訓練數據: {len(train_dataset)} 樣本\")\n",
    "print(f\"驗證數據: {len(val_dataset)} 樣本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DDP 訓練器類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPTrainer:\n",
    "    \"\"\"\n",
    "    分散式訓練器類別\n",
    "    支援單GPU演示和真正的多GPU訓練\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_dataset, val_dataset, config, rank=0, world_size=1):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.config = config\n",
    "        self.device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 模型設置\n",
    "        self.model = model.to(self.device)\n",
    "        self.is_distributed = world_size > 1 and dist.is_initialized()\n",
    "        \n",
    "        if self.is_distributed:\n",
    "            self.ddp_model = DDP(self.model, device_ids=[rank])\n",
    "            print(f\"[Rank {rank}] 使用DDP模型\")\n",
    "        else:\n",
    "            self.ddp_model = self.model\n",
    "            print(\"使用單GPU模型（非DDP）\")\n",
    "        \n",
    "        # 數據載入器\n",
    "        self.setup_dataloaders(train_dataset, val_dataset)\n",
    "        \n",
    "        # 優化器和調度器\n",
    "        self.setup_optimizer_and_scheduler()\n",
    "        \n",
    "        # 訓練狀態\n",
    "        self.global_step = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_history = {'loss': [], 'lr': [], 'step': []}\n",
    "        self.val_history = {'loss': [], 'step': []}\n",
    "        \n",
    "        # 創建保存目錄\n",
    "        self.save_dir = Path('checkpoints')\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def setup_dataloaders(self, train_dataset, val_dataset):\n",
    "        \"\"\"設置數據載入器\"\"\"\n",
    "        # 訓練數據載入器\n",
    "        if self.is_distributed:\n",
    "            train_sampler = DistributedSampler(\n",
    "                train_dataset, num_replicas=self.world_size, rank=self.rank, shuffle=True\n",
    "            )\n",
    "        else:\n",
    "            train_sampler = None\n",
    "        \n",
    "        self.train_loader = data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            sampler=train_sampler,\n",
    "            shuffle=(train_sampler is None),\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # 驗證數據載入器\n",
    "        if self.is_distributed:\n",
    "            val_sampler = DistributedSampler(\n",
    "                val_dataset, num_replicas=self.world_size, rank=self.rank, shuffle=False\n",
    "            )\n",
    "        else:\n",
    "            val_sampler = None\n",
    "        \n",
    "        self.val_loader = data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            sampler=val_sampler,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.train_sampler = train_sampler\n",
    "        self.val_sampler = val_sampler\n",
    "    \n",
    "    def setup_optimizer_and_scheduler(self):\n",
    "        \"\"\"設置優化器和學習率調度器\"\"\"\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.ddp_model.parameters(),\n",
    "            lr=self.config['learning_rate'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # 計算總步數\n",
    "        total_steps = self.config['num_epochs'] * len(self.train_loader)\n",
    "        warmup_steps = self.config['warmup_steps']\n",
    "        \n",
    "        # Warmup + Cosine 調度\n",
    "        warmup_scheduler = LinearLR(\n",
    "            self.optimizer, start_factor=0.1, total_iters=warmup_steps\n",
    "        )\n",
    "        cosine_scheduler = CosineAnnealingLR(\n",
    "            self.optimizer, T_max=total_steps - warmup_steps\n",
    "        )\n",
    "        \n",
    "        self.scheduler = SequentialLR(\n",
    "            self.optimizer,\n",
    "            schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "            milestones=[warmup_steps]\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"單個訓練步驟\"\"\"\n",
    "        self.ddp_model.train()\n",
    "        \n",
    "        # 準備數據\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.device)\n",
    "        labels = batch['labels'].to(self.device)\n",
    "        \n",
    "        # 前向傳播\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.ddp_model(input_ids, attention_mask)\n",
    "        \n",
    "        # 計算損失\n",
    "        # logits: [batch_size, seq_len, vocab_size]\n",
    "        # labels: [batch_size, seq_len]\n",
    "        loss = self.criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # 反向傳播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        if 'gradient_clipping' in self.config:\n",
    "            torch.nn.utils.clip_grad_norm_(self.ddp_model.parameters(), self.config['gradient_clipping'])\n",
    "        \n",
    "        # 更新參數\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"驗證階段\"\"\"\n",
    "        self.ddp_model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                logits = self.ddp_model(input_ids, attention_mask)\n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "        \n",
    "        # 在分散式環境中聚合驗證損失\n",
    "        if self.is_distributed:\n",
    "            avg_loss_tensor = torch.tensor(avg_loss, device=self.device)\n",
    "            dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)\n",
    "            avg_loss = avg_loss_tensor.item() / self.world_size\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"保存檢查點\"\"\"\n",
    "        if self.rank == 0:  # 只有rank 0保存檢查點\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': self.global_step,\n",
    "                'model_state_dict': self.ddp_model.module.state_dict() if hasattr(self.ddp_model, 'module') else self.ddp_model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'config': self.config,\n",
    "                'train_history': self.train_history,\n",
    "                'val_history': self.val_history,\n",
    "                'best_val_loss': self.best_val_loss\n",
    "            }\n",
    "            \n",
    "            # 保存最新檢查點\n",
    "            torch.save(checkpoint, self.save_dir / 'latest_checkpoint.pth')\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if is_best:\n",
    "                torch.save(checkpoint, self.save_dir / 'best_model.pth')\n",
    "                print(f\"💾 保存最佳模型 (驗證損失: {self.best_val_loss:.4f})\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"主要訓練循環\"\"\"\n",
    "        print(f\"\\n=== 開始DDP訓練 ===\")\n",
    "        print(f\"Rank: {self.rank}/{self.world_size}\")\n",
    "        print(f\"設備: {self.device}\")\n",
    "        print(f\"是否分散式: {self.is_distributed}\")\n",
    "        print(f\"批次大小: {self.config['batch_size']}\")\n",
    "        print(f\"訓練輪數: {self.config['num_epochs']}\")\n",
    "        print(f\"總訓練步數: {self.config['num_epochs'] * len(self.train_loader)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config['num_epochs']):\n",
    "            # 為分散式採樣器設置epoch\n",
    "            if self.train_sampler is not None:\n",
    "                self.train_sampler.set_epoch(epoch)\n",
    "            \n",
    "            # 訓練階段\n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            if self.rank == 0:\n",
    "                pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config[\"num_epochs\"]}')\n",
    "            else:\n",
    "                pbar = self.train_loader\n",
    "            \n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                loss = self.train_step(batch)\n",
    "                epoch_loss += loss\n",
    "                num_batches += 1\n",
    "                self.global_step += 1\n",
    "                \n",
    "                # 記錄訓練歷史\n",
    "                if batch_idx % 10 == 0:  # 每10步記錄一次\n",
    "                    current_lr = self.scheduler.get_last_lr()[0]\n",
    "                    self.train_history['loss'].append(loss)\n",
    "                    self.train_history['lr'].append(current_lr)\n",
    "                    self.train_history['step'].append(self.global_step)\n",
    "                \n",
    "                # 更新進度條\n",
    "                if self.rank == 0 and isinstance(pbar, tqdm):\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{loss:.4f}',\n",
    "                        'lr': f'{self.scheduler.get_last_lr()[0]:.2e}',\n",
    "                        'step': self.global_step\n",
    "                    })\n",
    "            \n",
    "            # 計算平均訓練損失\n",
    "            avg_train_loss = epoch_loss / num_batches\n",
    "            \n",
    "            # 驗證階段\n",
    "            val_loss = self.validate()\n",
    "            self.val_history['loss'].append(val_loss)\n",
    "            self.val_history['step'].append(self.global_step)\n",
    "            \n",
    "            # 檢查是否為最佳模型\n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "            \n",
    "            # 保存檢查點\n",
    "            self.save_checkpoint(epoch, is_best)\n",
    "            \n",
    "            # 只有rank 0打印日誌\n",
    "            if self.rank == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"\\nEpoch {epoch+1}/{self.config['num_epochs']}:\")\n",
    "                print(f\"  訓練損失: {avg_train_loss:.4f}\")\n",
    "                print(f\"  驗證損失: {val_loss:.4f} {'📉' if is_best else ''}\")\n",
    "                print(f\"  學習率: {self.scheduler.get_last_lr()[0]:.2e}\")\n",
    "                print(f\"  已用時間: {elapsed/60:.1f} 分鐘\")\n",
    "        \n",
    "        if self.rank == 0:\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\n✅ 訓練完成！\")\n",
    "            print(f\"總用時: {total_time/60:.1f} 分鐘\")\n",
    "            print(f\"最佳驗證損失: {self.best_val_loss:.4f}\")\n",
    "            print(f\"總訓練步數: {self.global_step}\")\n",
    "\n",
    "print(\"✅ DDPTrainer 類別定義完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 單GPU訓練演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在單GPU環境中運行DDP訓練器\n",
    "print(\"=== 單GPU環境下的DDP訓練演示 ===\")\n",
    "print(\"注意: 這展示了DDP的完整訓練邏輯，但沒有多GPU加速\")\n",
    "print()\n",
    "\n",
    "# 創建訓練器\n",
    "trainer = DDPTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    config=config,\n",
    "    rank=0,\n",
    "    world_size=1\n",
    ")\n",
    "\n",
    "print(f\"訓練器設置完成:\")\n",
    "print(f\"  訓練批次數: {len(trainer.train_loader)}\")\n",
    "print(f\"  驗證批次數: {len(trainer.val_loader)}\")\n",
    "print(f\"  優化器: {type(trainer.optimizer).__name__}\")\n",
    "print(f\"  調度器: {type(trainer.scheduler).__name__}\")\n",
    "\n",
    "# 測試一個訓練步驟\n",
    "print(\"\\n=== 測試單個訓練步驟 ===\")\n",
    "sample_batch = next(iter(trainer.train_loader))\n",
    "initial_loss = trainer.train_step(sample_batch)\n",
    "print(f\"初始損失: {initial_loss:.4f}\")\n",
    "print(f\"當前學習率: {trainer.scheduler.get_last_lr()[0]:.2e}\")\n",
    "print(f\"全局步數: {trainer.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 執行完整訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始完整訓練\n",
    "print(\"🚀 開始完整訓練...\")\n",
    "print(\"注意: 這可能需要幾分鐘時間\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️ 訓練被用戶中斷\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 訓練過程中發生錯誤: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 訓練結果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製訓練曲線\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 訓練損失曲線\n",
    "plt.subplot(1, 3, 1)\n",
    "if trainer.train_history['loss']:\n",
    "    plt.plot(trainer.train_history['step'], trainer.train_history['loss'], label='訓練損失', alpha=0.7)\n",
    "    # 平滑曲線\n",
    "    if len(trainer.train_history['loss']) > 10:\n",
    "        from scipy.ndimage import uniform_filter1d\n",
    "        smoothed = uniform_filter1d(trainer.train_history['loss'], size=10)\n",
    "        plt.plot(trainer.train_history['step'], smoothed, label='平滑訓練損失', linewidth=2)\n",
    "plt.xlabel('訓練步數')\n",
    "plt.ylabel('損失')\n",
    "plt.title('訓練損失曲線')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 驗證損失曲線\n",
    "plt.subplot(1, 3, 2)\n",
    "if trainer.val_history['loss']:\n",
    "    plt.plot(trainer.val_history['step'], trainer.val_history['loss'], 'o-', label='驗證損失', color='orange')\n",
    "    plt.axhline(y=trainer.best_val_loss, color='red', linestyle='--', alpha=0.7, label=f'最佳: {trainer.best_val_loss:.4f}')\n",
    "plt.xlabel('訓練步數')\n",
    "plt.ylabel('損失')\n",
    "plt.title('驗證損失曲線')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 學習率曲線\n",
    "plt.subplot(1, 3, 3)\n",
    "if trainer.train_history['lr']:\n",
    "    plt.plot(trainer.train_history['step'], trainer.train_history['lr'], label='學習率', color='green')\n",
    "plt.xlabel('訓練步數')\n",
    "plt.ylabel('學習率')\n",
    "plt.title('學習率調度')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 訓練統計\n",
    "print(\"\\n=== 訓練統計 ===\")\n",
    "print(f\"總訓練步數: {trainer.global_step}\")\n",
    "print(f\"最佳驗證損失: {trainer.best_val_loss:.4f}\")\n",
    "if trainer.train_history['loss']:\n",
    "    print(f\"最終訓練損失: {trainer.train_history['loss'][-1]:.4f}\")\n",
    "    print(f\"初始訓練損失: {trainer.train_history['loss'][0]:.4f}\")\n",
    "    print(f\"損失改善: {trainer.train_history['loss'][0] - trainer.train_history['loss'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n檢查點已保存到: {trainer.save_dir}\")\n",
    "if (trainer.save_dir / 'best_model.pth').exists():\n",
    "    print(\"✅ 最佳模型已保存\")\n",
    "if (trainer.save_dir / 'latest_checkpoint.pth').exists():\n",
    "    print(\"✅ 最新檢查點已保存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 多GPU訓練代碼範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成完整的多GPU訓練腳本\n",
    "multi_gpu_train_script = '''\n",
    "#!/usr/bin/env python3\n",
    "# multi_gpu_train.py - 完整的多GPU DDP訓練腳本\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"初始化分散式環境\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # 初始化進程組\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    print(f\"[Rank {rank}] 進程組初始化完成\")\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"清理分散式環境\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def main_worker(rank, world_size, args):\n",
    "    \"\"\"主要的工作進程函數\"\"\"\n",
    "    try:\n",
    "        # 設置分散式環境\n",
    "        setup(rank, world_size)\n",
    "        \n",
    "        # 創建模型、數據集、訓練器等\n",
    "        # (使用本notebook中定義的類別)\n",
    "        model = SimpleTransformer().to(rank)\n",
    "        train_dataset = DummyTextDataset(num_samples=args.num_samples)\n",
    "        val_dataset = DummyTextDataset(num_samples=args.num_samples // 5)\n",
    "        \n",
    "        config = {\n",
    "            'batch_size': args.batch_size,\n",
    "            'learning_rate': args.lr,\n",
    "            'num_epochs': args.epochs,\n",
    "            'warmup_steps': 100,\n",
    "            'weight_decay': 0.01,\n",
    "            'gradient_clipping': 1.0\n",
    "        }\n",
    "        \n",
    "        # 創建DDP訓練器\n",
    "        trainer = DDPTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            config=config,\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "        \n",
    "        # 開始訓練\n",
    "        trainer.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Rank {rank}] 錯誤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        cleanup()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='多GPU DDP訓練')\n",
    "    parser.add_argument('--epochs', type=int, default=5, help='訓練輪數')\n",
    "    parser.add_argument('--batch-size', type=int, default=8, help='批次大小')\n",
    "    parser.add_argument('--lr', type=float, default=5e-4, help='學習率')\n",
    "    parser.add_argument('--num-samples', type=int, default=2000, help='訓練樣本數')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 檢查GPU數量\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"錯誤: 未檢測到CUDA支持\")\n",
    "        return\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size < 2:\n",
    "        print(f\"警告: 檢測到 {world_size} 個GPU，建議至少2個GPU進行DDP訓練\")\n",
    "    \n",
    "    print(f\"開始 {world_size} GPU DDP訓練\")\n",
    "    print(f\"配置: epochs={args.epochs}, batch_size={args.batch_size}, lr={args.lr}\")\n",
    "    \n",
    "    # 啟動多進程訓練\n",
    "    mp.spawn(\n",
    "        main_worker,\n",
    "        args=(world_size, args),\n",
    "        nprocs=world_size,\n",
    "        join=True\n",
    "    )\n",
    "    \n",
    "    print(\"訓練完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# 保存腳本\n",
    "script_path = Path('multi_gpu_train.py')\n",
    "with open(script_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(multi_gpu_train_script)\n",
    "\n",
    "print(f\"✅ 多GPU訓練腳本已保存到: {script_path}\")\n",
    "print()\n",
    "print(\"=== 使用方法 ===\")\n",
    "print(\"# 使用 torchrun (推薦)\")\n",
    "print(\"torchrun --nproc_per_node=4 multi_gpu_train.py --epochs 10 --batch-size 16\")\n",
    "print()\n",
    "print(\"# 使用 mp.spawn\")\n",
    "print(\"python multi_gpu_train.py --epochs 10 --batch-size 16\")\n",
    "print()\n",
    "print(\"# 多節點訓練\")\n",
    "print(\"# 節點 0 (master):\")\n",
    "print(\"torchrun --nnodes=2 --nproc_per_node=4 --node_rank=0 \\\\\")\n",
    "print(\"         --master_addr=192.168.1.100 --master_port=29500 \\\\\")\n",
    "print(\"         multi_gpu_train.py\")\n",
    "print()\n",
    "print(\"# 節點 1 (worker):\")\n",
    "print(\"torchrun --nnodes=2 --nproc_per_node=4 --node_rank=1 \\\\\")\n",
    "print(\"         --master_addr=192.168.1.100 --master_port=29500 \\\\\")\n",
    "print(\"         multi_gpu_train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DDP 性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP 性能特性分析\n",
    "print(\"=== DDP 性能特性分析 ===\")\n",
    "print()\n",
    "\n",
    "# 理論性能計算\n",
    "model_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_mb = model_params * 4 / (1024 * 1024)  # FP32\n",
    "gradient_size_mb = model_size_mb  # 梯度大小約等於參數大小\n",
    "\n",
    "print(f\"模型分析:\")\n",
    "print(f\"  參數量: {model_params:,}\")\n",
    "print(f\"  模型大小: {model_size_mb:.1f} MB (FP32)\")\n",
    "print(f\"  梯度大小: {gradient_size_mb:.1f} MB\")\n",
    "print()\n",
    "\n",
    "# 多GPU性能預期\n",
    "print(f\"多GPU性能預期 (理論值):\")\n",
    "gpu_counts = [1, 2, 4, 8]\n",
    "for gpu_count in gpu_counts:\n",
    "    # 理論計算時間\n",
    "    compute_time = 1.0  # 基準計算時間\n",
    "    parallel_compute_time = compute_time / gpu_count  # 完美並行\n",
    "    \n",
    "    # 通訊時間 (All-Reduce)\n",
    "    communication_time = gradient_size_mb * 0.001 * (gpu_count - 1) / gpu_count  # 簡化估算\n",
    "    \n",
    "    total_time = parallel_compute_time + communication_time\n",
    "    speedup = 1.0 / total_time\n",
    "    efficiency = speedup / gpu_count * 100\n",
    "    \n",
    "    print(f\"  {gpu_count} GPU: 加速比 {speedup:.2f}x, 效率 {efficiency:.1f}%\")\n",
    "\n",
    "print()\n",
    "print(f\"DDP 優化特性:\")\n",
    "print(f\"  ✅ 梯度自動同步 (All-Reduce)\")\n",
    "print(f\"  ✅ 通訊與計算重疊\")\n",
    "print(f\"  ✅ 梯度壓縮 (可選)\")\n",
    "print(f\"  ✅ 錯誤檢測和恢復\")\n",
    "print(f\"  ✅ 動態批次大小調整\")\n",
    "print()\n",
    "\n",
    "# 記憶體使用分析\n",
    "print(f\"記憶體使用分析 (單GPU vs 多GPU):\")\n",
    "base_memory = model_size_mb * 2  # 模型 + 梯度\n",
    "optimizer_memory = model_size_mb * 2  # Adam狀態\n",
    "activation_memory = config['batch_size'] * config['max_seq_len'] * config['d_model'] * 4 / (1024 * 1024)\n",
    "\n",
    "print(f\"  單GPU記憶體需求:\")\n",
    "print(f\"    模型 + 梯度: {base_memory:.1f} MB\")\n",
    "print(f\"    優化器狀態: {optimizer_memory:.1f} MB\")\n",
    "print(f\"    激活值: {activation_memory:.1f} MB\")\n",
    "print(f\"    總計: {base_memory + optimizer_memory + activation_memory:.1f} MB\")\n",
    "print()\n",
    "print(f\"  多GPU記憶體優勢:\")\n",
    "print(f\"    - 激活值分散到各GPU\")\n",
    "print(f\"    - 可支持更大的全局批次大小\")\n",
    "print(f\"    - 參數和梯度在需要時同步\")\n",
    "\n",
    "# 通訊模式分析\n",
    "print(f\"\\n=== DDP 通訊模式 ===\")\n",
    "print(f\"1. All-Reduce 模式:\")\n",
    "print(f\"   - 每個GPU計算本地梯度\")\n",
    "print(f\"   - 使用All-Reduce聚合所有梯度\")\n",
    "print(f\"   - 所有GPU獲得相同的平均梯度\")\n",
    "print(f\"   - 同步更新模型參數\")\n",
    "print()\n",
    "print(f\"2. 通訊拓撲:\")\n",
    "print(f\"   - Ring All-Reduce: O(N) 步驟, 帶寬利用率高\")\n",
    "print(f\"   - Tree All-Reduce: O(log N) 步驟, 適合大規模\")\n",
    "print(f\"   - NCCL自動選擇最優拓撲\")\n",
    "print()\n",
    "print(f\"3. 優化策略:\")\n",
    "print(f\"   - 梯度累積: 減少通訊頻率\")\n",
    "print(f\"   - 梯度壓縮: 減少通訊量\")\n",
    "print(f\"   - 計算通訊重疊: 隱藏通訊延遲\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 總結與下一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Lab-1.2 Train 完成總結 ===\")\n",
    "print()\n",
    "print(\"✅ 已完成:\")\n",
    "print(\"  1. ✅ 實作完整的DDP訓練器類別\")\n",
    "print(\"  2. ✅ 演示單GPU環境下的DDP訓練流程\")\n",
    "print(\"  3. ✅ 實現梯度同步和參數更新機制\")\n",
    "print(\"  4. ✅ 訓練歷史記錄和可視化\")\n",
    "print(\"  5. ✅ 檢查點保存和載入功能\")\n",
    "print(\"  6. ✅ 多GPU訓練腳本生成\")\n",
    "print(\"  7. ✅ DDP性能特性分析\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 關鍵學習成果:\")\n",
    "print(\"  - 理解DDP的完整訓練流程\")\n",
    "print(\"  - 掌握分散式數據載入和採樣\")\n",
    "print(\"  - 學會梯度同步和通訊優化\")\n",
    "print(\"  - 熟悉多GPU訓練的配置方法\")\n",
    "print()\n",
    "\n",
    "print(\"📁 生成的文件:\")\n",
    "print(\"  - checkpoints/: 訓練檢查點目錄\")\n",
    "print(\"  - training_curves.png: 訓練曲線圖\")\n",
    "print(\"  - multi_gpu_train.py: 多GPU訓練腳本\")\n",
    "print()\n",
    "\n",
    "print(\"📝 下一步建議:\")\n",
    "print(\"  - 03-Optimization.ipynb: 通訊優化和性能調優\")\n",
    "print(\"  - 04-Advanced.ipynb: 進階技術和故障處理\")\n",
    "print(\"  - 在多GPU環境中測試生成的訓練腳本\")\n",
    "print()\n",
    "\n",
    "print(\"🔧 多GPU環境使用指南:\")\n",
    "print(\"  1. 檢查GPU數量: nvidia-smi\")\n",
    "print(\"  2. 驗證NCCL: python -c 'import torch; print(torch.distributed.is_nccl_available())'\")\n",
    "print(\"  3. 運行訓練: torchrun --nproc_per_node=N multi_gpu_train.py\")\n",
    "print(\"  4. 監控訓練: watch -n 1 nvidia-smi\")\n",
    "print()\n",
    "\n",
    "print(\"💡 重要概念回顧:\")\n",
    "print(\"  - DDP = 數據並行 + 參數同步\")\n",
    "print(\"  - All-Reduce = 高效的梯度聚合算法\")\n",
    "print(\"  - DistributedSampler = 確保數據不重複\")\n",
    "print(\"  - Rank 0 = 主進程負責日誌和檢查點\")\n",
    "print(\"  - 通訊後端: NCCL (GPU) vs Gloo (CPU)\")\n",
    "\n",
    "# 保存訓練結果摘要\n",
    "summary = {\n",
    "    'training_completed': True,\n",
    "    'final_train_loss': trainer.train_history['loss'][-1] if trainer.train_history['loss'] else None,\n",
    "    'best_val_loss': trainer.best_val_loss,\n",
    "    'total_steps': trainer.global_step,\n",
    "    'model_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'config': config\n",
    "}\n",
    "\n",
    "with open('training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n💾 訓練摘要已保存到 training_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}