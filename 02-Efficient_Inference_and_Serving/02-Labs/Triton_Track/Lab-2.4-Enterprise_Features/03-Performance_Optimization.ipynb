{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.4.3: æ¥µè‡´æ€§èƒ½èª¿å„ª ğŸš€\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "- **ç“¶é ¸åˆ†æ**: è­˜åˆ¥å’Œè§£æ±ºæ€§èƒ½ç“¶é ¸\n",
    "- **GPU è¨˜æ†¶é«”ç®¡ç†**: å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨ç­–ç•¥\n",
    "- **Multi-GPU é…ç½®**: æ“´å±•åˆ°å¤š GPU ç’°å¢ƒ\n",
    "- **æ€§èƒ½ç›£æ§**: å¯¦æ™‚ç›£æ§å’Œèª¿å„ªæŒ‡æ¨™\n",
    "\n",
    "## ğŸ—ï¸ å¯¦é©—æ¶æ§‹\n",
    "\n",
    "```\n",
    "Performance Optimization Pipeline\n",
    "â”œâ”€â”€ ç“¶é ¸åˆ†æå’Œè¨ºæ–·\n",
    "â”œâ”€â”€ GPU è¨˜æ†¶é«”å„ªåŒ–\n",
    "â”œâ”€â”€ Multi-GPU è² è¼‰å‡è¡¡\n",
    "â””â”€â”€ å¯¦æ™‚æ€§èƒ½ç›£æ§\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™èˆ‡åŸºç¤è¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "import psutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import tritonclient.http as httpclient\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# GPU ç›£æ§ç›¸é—œ\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"âœ… NVIDIA GPU ç›£æ§å·²åˆå§‹åŒ–\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"âš ï¸  NVIDIA GPU ç›£æ§ä¸å¯ç”¨\")\n",
    "\n",
    "# è¨­ç½®åœ–è¡¨æ¨£å¼\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ”§ ç’°å¢ƒè¨­ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ€§èƒ½ç›£æ§åŸºç¤è¨­æ–½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹\"\"\"\n",
    "    timestamp: float\n",
    "    latency_ms: float\n",
    "    throughput_rps: float\n",
    "    cpu_usage: float\n",
    "    memory_usage: float\n",
    "    gpu_usage: Optional[float] = None\n",
    "    gpu_memory_usage: Optional[float] = None\n",
    "    queue_size: Optional[int] = None\n",
    "    batch_size: Optional[int] = None\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"å¯¦æ™‚æ€§èƒ½ç›£æ§å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.metrics_history: List[PerformanceMetrics] = []\n",
    "        self.monitoring = False\n",
    "        \n",
    "    def get_system_metrics(self) -> Dict:\n",
    "        \"\"\"ç²å–ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³\"\"\"\n",
    "        metrics = {\n",
    "            'cpu_usage': psutil.cpu_percent(interval=0.1),\n",
    "            'memory_usage': psutil.virtual_memory().percent,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        # GPU ç›£æ§\n",
    "        if GPU_AVAILABLE:\n",
    "            try:\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                \n",
    "                metrics.update({\n",
    "                    'gpu_usage': gpu_util.gpu,\n",
    "                    'gpu_memory_usage': (mem_info.used / mem_info.total) * 100\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"GPU ç›£æ§éŒ¯èª¤: {e}\")\n",
    "                \n",
    "        return metrics\n",
    "    \n",
    "    def get_triton_metrics(self) -> Dict:\n",
    "        \"\"\"ç²å– Triton æœå‹™å™¨æŒ‡æ¨™\"\"\"\n",
    "        try:\n",
    "            # ç²å–æœå‹™å™¨çµ±è¨ˆä¿¡æ¯\n",
    "            stats = self.client.get_inference_statistics()\n",
    "            \n",
    "            # ç²å–æœå‹™å™¨ç‹€æ…‹\n",
    "            server_metadata = self.client.get_server_metadata()\n",
    "            \n",
    "            return {\n",
    "                'server_stats': stats,\n",
    "                'server_metadata': server_metadata,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Triton æŒ‡æ¨™ç²å–éŒ¯èª¤: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    async def monitor_performance(self, duration: int = 60, interval: float = 1.0):\n",
    "        \"\"\"æŒçºŒç›£æ§æ€§èƒ½\"\"\"\n",
    "        self.monitoring = True\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"ğŸ” é–‹å§‹æ€§èƒ½ç›£æ§ ({duration} ç§’)\")\n",
    "        \n",
    "        while self.monitoring and (time.time() - start_time) < duration:\n",
    "            # æ”¶é›†æŒ‡æ¨™\n",
    "            system_metrics = self.get_system_metrics()\n",
    "            triton_metrics = self.get_triton_metrics()\n",
    "            \n",
    "            # è¨˜éŒ„æŒ‡æ¨™\n",
    "            metric = PerformanceMetrics(\n",
    "                timestamp=system_metrics['timestamp'],\n",
    "                latency_ms=0,  # å°‡åœ¨è² è¼‰æ¸¬è©¦ä¸­æ›´æ–°\n",
    "                throughput_rps=0,  # å°‡åœ¨è² è¼‰æ¸¬è©¦ä¸­æ›´æ–°\n",
    "                cpu_usage=system_metrics['cpu_usage'],\n",
    "                memory_usage=system_metrics['memory_usage'],\n",
    "                gpu_usage=system_metrics.get('gpu_usage'),\n",
    "                gpu_memory_usage=system_metrics.get('gpu_memory_usage')\n",
    "            )\n",
    "            \n",
    "            self.metrics_history.append(metric)\n",
    "            \n",
    "            await asyncio.sleep(interval)\n",
    "        \n",
    "        print(\"âœ… æ€§èƒ½ç›£æ§å®Œæˆ\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"åœæ­¢ç›£æ§\"\"\"\n",
    "        self.monitoring = False\n",
    "    \n",
    "    def plot_metrics(self, figsize: Tuple[int, int] = (15, 10)):\n",
    "        \"\"\"ç¹ªè£½æ€§èƒ½æŒ‡æ¨™åœ–è¡¨\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"âš ï¸  æ²’æœ‰æ€§èƒ½æ•¸æ“šå¯é¡¯ç¤º\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        timestamps = [m.timestamp for m in self.metrics_history]\n",
    "        start_time = timestamps[0]\n",
    "        relative_times = [(t - start_time) for t in timestamps]\n",
    "        \n",
    "        # CPU ä½¿ç”¨ç‡\n",
    "        axes[0, 0].plot(relative_times, [m.cpu_usage for m in self.metrics_history], 'b-')\n",
    "        axes[0, 0].set_title('CPU ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 0].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "        axes[0, 1].plot(relative_times, [m.memory_usage for m in self.metrics_history], 'g-')\n",
    "        axes[0, 1].set_title('è¨˜æ†¶é«”ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 1].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # GPU ä½¿ç”¨ç‡\n",
    "        if any(m.gpu_usage is not None for m in self.metrics_history):\n",
    "            gpu_usage = [m.gpu_usage or 0 for m in self.metrics_history]\n",
    "            axes[1, 0].plot(relative_times, gpu_usage, 'r-')\n",
    "            axes[1, 0].set_title('GPU ä½¿ç”¨ç‡ (%)')\n",
    "            axes[1, 0].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "        if any(m.gpu_memory_usage is not None for m in self.metrics_history):\n",
    "            gpu_mem = [m.gpu_memory_usage or 0 for m in self.metrics_history]\n",
    "            axes[1, 1].plot(relative_times, gpu_mem, 'orange')\n",
    "            axes[1, 1].set_title('GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡ (%)')\n",
    "            axes[1, 1].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "            axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# åˆå§‹åŒ–ç›£æ§å™¨\n",
    "monitor = PerformanceMonitor()\n",
    "print(\"ğŸ“Š æ€§èƒ½ç›£æ§å™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç“¶é ¸åˆ†æå’Œè¨ºæ–·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckAnalyzer:\n",
    "    \"\"\"ç“¶é ¸åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        \n",
    "    def analyze_model_performance(self, model_name: str, test_duration: int = 30) -> Dict:\n",
    "        \"\"\"åˆ†æå–®å€‹æ¨¡å‹çš„æ€§èƒ½ç“¶é ¸\"\"\"\n",
    "        print(f\"ğŸ” åˆ†ææ¨¡å‹ {model_name} çš„æ€§èƒ½...\")\n",
    "        \n",
    "        # ç²å–æ¨¡å‹é…ç½®\n",
    "        try:\n",
    "            model_config = self.client.get_model_config(model_name)\n",
    "            model_metadata = self.client.get_model_metadata(model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯: {e}\")\n",
    "            return {}\n",
    "        \n",
    "        # åˆ†æçµæœ\n",
    "        analysis = {\n",
    "            'model_name': model_name,\n",
    "            'config': model_config,\n",
    "            'metadata': model_metadata,\n",
    "            'bottlenecks': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # æª¢æŸ¥é…ç½®æ½›åœ¨å•é¡Œ\n",
    "        config_dict = model_config\n",
    "        \n",
    "        # æª¢æŸ¥å‹•æ…‹æ‰¹æ¬¡è¨­ç½®\n",
    "        if 'dynamic_batching' in config_dict:\n",
    "            db_config = config_dict['dynamic_batching']\n",
    "            max_queue_delay = db_config.get('max_queue_delay_microseconds', 0)\n",
    "            \n",
    "            if max_queue_delay == 0:\n",
    "                analysis['bottlenecks'].append(\"å‹•æ…‹æ‰¹æ¬¡æœªè¨­ç½®æœ€å¤§å»¶é²\")\n",
    "                analysis['recommendations'].append(\"è¨­ç½®é©ç•¶çš„ max_queue_delay_microseconds\")\n",
    "            \n",
    "            preferred_batch_size = db_config.get('preferred_batch_size', [])\n",
    "            if not preferred_batch_size:\n",
    "                analysis['bottlenecks'].append(\"æœªè¨­ç½®é¦–é¸æ‰¹æ¬¡å¤§å°\")\n",
    "                analysis['recommendations'].append(\"é…ç½® preferred_batch_size å„ªåŒ–ååé‡\")\n",
    "        else:\n",
    "            analysis['bottlenecks'].append(\"æœªå•Ÿç”¨å‹•æ…‹æ‰¹æ¬¡\")\n",
    "            analysis['recommendations'].append(\"å•Ÿç”¨å‹•æ…‹æ‰¹æ¬¡ä»¥æé«˜ååé‡\")\n",
    "        \n",
    "        # æª¢æŸ¥å¯¦ä¾‹æ•¸é‡\n",
    "        instance_group = config_dict.get('instance_group', [{}])[0]\n",
    "        count = instance_group.get('count', 1)\n",
    "        \n",
    "        if count == 1:\n",
    "            analysis['bottlenecks'].append(\"å–®å¯¦ä¾‹å¯èƒ½æˆç‚ºç“¶é ¸\")\n",
    "            analysis['recommendations'].append(\"è€ƒæ…®å¢åŠ å¯¦ä¾‹æ•¸é‡\")\n",
    "        \n",
    "        # æª¢æŸ¥ GPU é¡å‹\n",
    "        kind = instance_group.get('kind', 'KIND_AUTO')\n",
    "        if kind == 'KIND_CPU':\n",
    "            analysis['bottlenecks'].append(\"ä½¿ç”¨ CPU æ¨ç†å¯èƒ½è¼ƒæ…¢\")\n",
    "            analysis['recommendations'].append(\"è€ƒæ…®ä½¿ç”¨ GPU æ¨ç†\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def benchmark_latency_throughput(self, model_name: str, \n",
    "                                   batch_sizes: List[int] = [1, 2, 4, 8, 16, 32],\n",
    "                                   concurrency_levels: List[int] = [1, 2, 4, 8, 16]) -> Dict:\n",
    "        \"\"\"åŸºæº–æ¸¬è©¦å»¶é²å’Œååé‡\"\"\"\n",
    "        print(f\"ğŸ“Š å°æ¨¡å‹ {model_name} é€²è¡ŒåŸºæº–æ¸¬è©¦...\")\n",
    "        \n",
    "        results = {\n",
    "            'batch_size_analysis': {},\n",
    "            'concurrency_analysis': {},\n",
    "            'optimal_config': {}\n",
    "        }\n",
    "        \n",
    "        # æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°\n",
    "        print(\"ğŸ”¬ æ¸¬è©¦æ‰¹æ¬¡å¤§å°å½±éŸ¿...\")\n",
    "        for batch_size in batch_sizes:\n",
    "            try:\n",
    "                latency, throughput = self._test_batch_size(model_name, batch_size)\n",
    "                results['batch_size_analysis'][batch_size] = {\n",
    "                    'latency_ms': latency,\n",
    "                    'throughput_rps': throughput\n",
    "                }\n",
    "                print(f\"  æ‰¹æ¬¡å¤§å° {batch_size}: å»¶é² {latency:.2f}ms, ååé‡ {throughput:.2f} RPS\")\n",
    "            except Exception as e:\n",
    "                print(f\"  æ‰¹æ¬¡å¤§å° {batch_size} æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "        \n",
    "        # æ¸¬è©¦ä¸åŒä¸¦ç™¼ç´šåˆ¥\n",
    "        print(\"ğŸ”¬ æ¸¬è©¦ä¸¦ç™¼ç´šåˆ¥å½±éŸ¿...\")\n",
    "        for concurrency in concurrency_levels:\n",
    "            try:\n",
    "                latency, throughput = self._test_concurrency(model_name, concurrency)\n",
    "                results['concurrency_analysis'][concurrency] = {\n",
    "                    'latency_ms': latency,\n",
    "                    'throughput_rps': throughput\n",
    "                }\n",
    "                print(f\"  ä¸¦ç™¼ç´šåˆ¥ {concurrency}: å»¶é² {latency:.2f}ms, ååé‡ {throughput:.2f} RPS\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ä¸¦ç™¼ç´šåˆ¥ {concurrency} æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "        \n",
    "        # æ‰¾å‡ºæœ€å„ªé…ç½®\n",
    "        if results['batch_size_analysis']:\n",
    "            best_batch = max(results['batch_size_analysis'].items(), \n",
    "                           key=lambda x: x[1]['throughput_rps'])\n",
    "            results['optimal_config']['best_batch_size'] = best_batch[0]\n",
    "            \n",
    "        if results['concurrency_analysis']:\n",
    "            best_concurrency = max(results['concurrency_analysis'].items(), \n",
    "                                 key=lambda x: x[1]['throughput_rps'])\n",
    "            results['optimal_config']['best_concurrency'] = best_concurrency[0]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _test_batch_size(self, model_name: str, batch_size: int) -> Tuple[float, float]:\n",
    "        \"\"\"æ¸¬è©¦ç‰¹å®šæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½\"\"\"\n",
    "        # å‰µå»ºæ¸¬è©¦æ•¸æ“š\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        \n",
    "        try:\n",
    "            # ç²å–æ¨¡å‹å…ƒæ•¸æ“šä»¥äº†è§£è¼¸å…¥æ ¼å¼\n",
    "            metadata = self.client.get_model_metadata(model_name)\n",
    "            \n",
    "            # å‰µå»ºç¤ºä¾‹è¼¸å…¥ (é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›æ¨¡å‹èª¿æ•´)\n",
    "            for inp in metadata['inputs']:\n",
    "                input_name = inp['name']\n",
    "                input_shape = inp['shape']\n",
    "                input_dtype = inp['datatype']\n",
    "                \n",
    "                # å‰µå»ºæ¸¬è©¦æ•¸æ“š\n",
    "                if input_dtype == 'FP32':\n",
    "                    test_data = np.random.random([batch_size] + input_shape).astype(np.float32)\n",
    "                elif input_dtype == 'INT64':\n",
    "                    test_data = np.random.randint(0, 1000, [batch_size] + input_shape).astype(np.int64)\n",
    "                else:\n",
    "                    test_data = np.random.random([batch_size] + input_shape).astype(np.float32)\n",
    "                \n",
    "                inputs.append(httpclient.InferInput(input_name, test_data.shape, input_dtype))\n",
    "                inputs[-1].set_data_from_numpy(test_data)\n",
    "            \n",
    "            for out in metadata['outputs']:\n",
    "                outputs.append(httpclient.InferRequestedOutput(out['name']))\n",
    "        \n",
    "        except Exception as e:\n",
    "            # å¦‚æœç„¡æ³•ç²å–å…ƒæ•¸æ“šï¼Œä½¿ç”¨é»˜èªæ¸¬è©¦\n",
    "            test_data = np.random.random((batch_size, 224, 224, 3)).astype(np.float32)\n",
    "            inputs = [httpclient.InferInput(\"input\", test_data.shape, \"FP32\")]\n",
    "            inputs[0].set_data_from_numpy(test_data)\n",
    "            outputs = [httpclient.InferRequestedOutput(\"output\")]\n",
    "        \n",
    "        # åŸ·è¡Œæ¸¬è©¦\n",
    "        num_requests = 100\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_requests):\n",
    "            try:\n",
    "                self.client.infer(model_name, inputs, outputs=outputs)\n",
    "            except Exception:\n",
    "                pass  # å¿½ç•¥å€‹åˆ¥è«‹æ±‚éŒ¯èª¤\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        total_time = end_time - start_time\n",
    "        avg_latency = (total_time / num_requests) * 1000  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "        throughput = (num_requests * batch_size) / total_time\n",
    "        \n",
    "        return avg_latency, throughput\n",
    "    \n",
    "    def _test_concurrency(self, model_name: str, concurrency: int) -> Tuple[float, float]:\n",
    "        \"\"\"æ¸¬è©¦ç‰¹å®šä¸¦ç™¼ç´šåˆ¥çš„æ€§èƒ½\"\"\"\n",
    "        # ç°¡åŒ–çš„ä¸¦ç™¼æ¸¬è©¦\n",
    "        return self._test_batch_size(model_name, 1)  # æš«æ™‚ä½¿ç”¨æ‰¹æ¬¡å¤§å°æ¸¬è©¦\n",
    "    \n",
    "    def generate_optimization_report(self, model_name: str) -> str:\n",
    "        \"\"\"ç”Ÿæˆå„ªåŒ–å»ºè­°å ±å‘Š\"\"\"\n",
    "        analysis = self.analyze_model_performance(model_name)\n",
    "        benchmark = self.benchmark_latency_throughput(model_name)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# ğŸ¯ æ¨¡å‹æ€§èƒ½å„ªåŒ–å ±å‘Š: {model_name}\n",
    "\n",
    "## ğŸ“Š ç“¶é ¸åˆ†æ\n",
    "\"\"\"\n",
    "        \n",
    "        if analysis.get('bottlenecks'):\n",
    "            report += \"\\n### ğŸš¨ ç™¼ç¾çš„ç“¶é ¸:\\n\"\n",
    "            for bottleneck in analysis['bottlenecks']:\n",
    "                report += f\"- {bottleneck}\\n\"\n",
    "        \n",
    "        if analysis.get('recommendations'):\n",
    "            report += \"\\n### ğŸ’¡ å„ªåŒ–å»ºè­°:\\n\"\n",
    "            for rec in analysis['recommendations']:\n",
    "                report += f\"- {rec}\\n\"\n",
    "        \n",
    "        if benchmark.get('optimal_config'):\n",
    "            report += \"\\n### ğŸ¯ æœ€å„ªé…ç½®:\\n\"\n",
    "            config = benchmark['optimal_config']\n",
    "            if 'best_batch_size' in config:\n",
    "                report += f\"- æœ€ä½³æ‰¹æ¬¡å¤§å°: {config['best_batch_size']}\\n\"\n",
    "            if 'best_concurrency' in config:\n",
    "                report += f\"- æœ€ä½³ä¸¦ç™¼ç´šåˆ¥: {config['best_concurrency']}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†æå™¨\n",
    "analyzer = BottleneckAnalyzer()\n",
    "print(\"ğŸ” ç“¶é ¸åˆ†æå™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPU è¨˜æ†¶é«”ç®¡ç†å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryOptimizer:\n",
    "    \"\"\"GPU è¨˜æ†¶é«”å„ªåŒ–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_available = GPU_AVAILABLE\n",
    "        \n",
    "    def get_gpu_memory_info(self) -> Dict:\n",
    "        \"\"\"ç²å– GPU è¨˜æ†¶é«”ä¿¡æ¯\"\"\"\n",
    "        if not self.gpu_available:\n",
    "            return {'error': 'GPU ä¸å¯ç”¨'}\n",
    "        \n",
    "        try:\n",
    "            device_count = pynvml.nvmlDeviceGetCount()\n",
    "            gpu_info = {}\n",
    "            \n",
    "            for i in range(device_count):\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "                \n",
    "                gpu_info[f'gpu_{i}'] = {\n",
    "                    'name': name,\n",
    "                    'total_memory_gb': mem_info.total / (1024**3),\n",
    "                    'used_memory_gb': mem_info.used / (1024**3),\n",
    "                    'free_memory_gb': mem_info.free / (1024**3),\n",
    "                    'utilization_percent': (mem_info.used / mem_info.total) * 100\n",
    "                }\n",
    "            \n",
    "            return gpu_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'ç²å– GPU ä¿¡æ¯å¤±æ•—: {e}'}\n",
    "    \n",
    "    def analyze_memory_usage_pattern(self, duration: int = 60) -> Dict:\n",
    "        \"\"\"åˆ†æè¨˜æ†¶é«”ä½¿ç”¨æ¨¡å¼\"\"\"\n",
    "        print(f\"ğŸ“ˆ åˆ†æ GPU è¨˜æ†¶é«”ä½¿ç”¨æ¨¡å¼ ({duration} ç§’)...\")\n",
    "        \n",
    "        if not self.gpu_available:\n",
    "            return {'error': 'GPU ä¸å¯ç”¨'}\n",
    "        \n",
    "        memory_history = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            gpu_info = self.get_gpu_memory_info()\n",
    "            if 'error' not in gpu_info:\n",
    "                timestamp = time.time()\n",
    "                for gpu_id, info in gpu_info.items():\n",
    "                    memory_history.append({\n",
    "                        'timestamp': timestamp,\n",
    "                        'gpu_id': gpu_id,\n",
    "                        'used_memory_gb': info['used_memory_gb'],\n",
    "                        'utilization_percent': info['utilization_percent']\n",
    "                    })\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # åˆ†ææ¨¡å¼\n",
    "        analysis = {\n",
    "            'memory_history': memory_history,\n",
    "            'peak_usage': {},\n",
    "            'average_usage': {},\n",
    "            'memory_spikes': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯\n",
    "        if memory_history:\n",
    "            gpus = set(item['gpu_id'] for item in memory_history)\n",
    "            \n",
    "            for gpu_id in gpus:\n",
    "                gpu_data = [item for item in memory_history if item['gpu_id'] == gpu_id]\n",
    "                usage_values = [item['utilization_percent'] for item in gpu_data]\n",
    "                \n",
    "                analysis['peak_usage'][gpu_id] = max(usage_values)\n",
    "                analysis['average_usage'][gpu_id] = sum(usage_values) / len(usage_values)\n",
    "                \n",
    "                # æª¢æ¸¬è¨˜æ†¶é«”å³°å€¼\n",
    "                threshold = 90  # 90% ä½¿ç”¨ç‡é–¾å€¼\n",
    "                spikes = [item for item in gpu_data if item['utilization_percent'] > threshold]\n",
    "                if spikes:\n",
    "                    analysis['memory_spikes'].extend(spikes)\n",
    "        \n",
    "        # ç”Ÿæˆå»ºè­°\n",
    "        if analysis['memory_spikes']:\n",
    "            analysis['recommendations'].append(\"æª¢æ¸¬åˆ°è¨˜æ†¶é«”ä½¿ç”¨å³°å€¼ï¼Œè€ƒæ…®å„ªåŒ–æ‰¹æ¬¡å¤§å°\")\n",
    "        \n",
    "        avg_usage = analysis.get('average_usage', {})\n",
    "        if avg_usage and any(usage > 80 for usage in avg_usage.values()):\n",
    "            analysis['recommendations'].append(\"è¨˜æ†¶é«”ä½¿ç”¨ç‡è¼ƒé«˜ï¼Œå»ºè­°å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def generate_memory_optimization_config(self, target_memory_usage: float = 75.0) -> Dict:\n",
    "        \"\"\"ç”Ÿæˆè¨˜æ†¶é«”å„ªåŒ–é…ç½®\"\"\"\n",
    "        gpu_info = self.get_gpu_memory_info()\n",
    "        \n",
    "        if 'error' in gpu_info:\n",
    "            return gpu_info\n",
    "        \n",
    "        optimization_config = {\n",
    "            'memory_pool_config': {},\n",
    "            'instance_group_config': {},\n",
    "            'batching_config': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        for gpu_id, info in gpu_info.items():\n",
    "            total_memory_gb = info['total_memory_gb']\n",
    "            target_memory_gb = total_memory_gb * (target_memory_usage / 100)\n",
    "            \n",
    "            # è¨˜æ†¶é«”æ± é…ç½®\n",
    "            optimization_config['memory_pool_config'][gpu_id] = {\n",
    "                'gpu_memory_pool_byte_size': int(target_memory_gb * 1024**3),\n",
    "                'pinned_memory_pool_byte_size': 268435456  # 256MB\n",
    "            }\n",
    "            \n",
    "            # å¯¦ä¾‹çµ„é…ç½®\n",
    "            optimization_config['instance_group_config'][gpu_id] = {\n",
    "                'count': 1,\n",
    "                'kind': 'KIND_GPU',\n",
    "                'gpus': [int(gpu_id.split('_')[1])]\n",
    "            }\n",
    "            \n",
    "            # å»ºè­°çš„æ‰¹æ¬¡é…ç½®\n",
    "            optimization_config['batching_config'][gpu_id] = {\n",
    "                'max_batch_size': min(32, max(1, int(target_memory_gb / 2))),\n",
    "                'preferred_batch_size': [1, 2, 4, 8]\n",
    "            }\n",
    "        \n",
    "        # é€šç”¨å»ºè­°\n",
    "        optimization_config['recommendations'] = [\n",
    "            \"ä½¿ç”¨å‹•æ…‹æ‰¹æ¬¡ä»¥æœ€å¤§åŒ–è¨˜æ†¶é«”æ•ˆç‡\",\n",
    "            \"ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ä¸¦èª¿æ•´æ‰¹æ¬¡å¤§å°\",\n",
    "            \"è€ƒæ…®ä½¿ç”¨æ¨¡å‹ä¸¦è¡ŒåŒ–åˆ†æ•£è¨˜æ†¶é«”è² è¼‰\",\n",
    "            f\"ç›®æ¨™è¨˜æ†¶é«”ä½¿ç”¨ç‡: {target_memory_usage}%\"\n",
    "        ]\n",
    "        \n",
    "        return optimization_config\n",
    "    \n",
    "    def plot_memory_usage(self, memory_history: List[Dict], figsize: Tuple[int, int] = (12, 6)):\n",
    "        \"\"\"ç¹ªè£½è¨˜æ†¶é«”ä½¿ç”¨åœ–è¡¨\"\"\"\n",
    "        if not memory_history:\n",
    "            print(\"âš ï¸  æ²’æœ‰è¨˜æ†¶é«”ä½¿ç”¨æ•¸æ“š\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # æŒ‰ GPU åˆ†çµ„\n",
    "        gpus = set(item['gpu_id'] for item in memory_history)\n",
    "        \n",
    "        for gpu_id in gpus:\n",
    "            gpu_data = [item for item in memory_history if item['gpu_id'] == gpu_id]\n",
    "            timestamps = [item['timestamp'] for item in gpu_data]\n",
    "            utilization = [item['utilization_percent'] for item in gpu_data]\n",
    "            \n",
    "            # è½‰æ›ç‚ºç›¸å°æ™‚é–“\n",
    "            start_time = min(timestamps)\n",
    "            relative_times = [(t - start_time) / 60 for t in timestamps]  # è½‰æ›ç‚ºåˆ†é˜\n",
    "            \n",
    "            plt.plot(relative_times, utilization, label=f'{gpu_id.upper()}')\n",
    "        \n",
    "        plt.title('GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡è¶¨å‹¢')\n",
    "        plt.xlabel('æ™‚é–“ (åˆ†é˜)')\n",
    "        plt.ylabel('è¨˜æ†¶é«”ä½¿ç”¨ç‡ (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='è­¦å‘Šç·š (80%)')\n",
    "        plt.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='å±éšªç·š (90%)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# åˆå§‹åŒ– GPU è¨˜æ†¶é«”å„ªåŒ–å™¨\n",
    "gpu_optimizer = GPUMemoryOptimizer()\n",
    "print(\"ğŸ¯ GPU è¨˜æ†¶é«”å„ªåŒ–å™¨å·²åˆå§‹åŒ–\")\n",
    "\n",
    "# é¡¯ç¤ºç•¶å‰ GPU ç‹€æ…‹\n",
    "gpu_info = gpu_optimizer.get_gpu_memory_info()\n",
    "print(\"\\nğŸ“Š ç•¶å‰ GPU ç‹€æ…‹:\")\n",
    "for gpu_id, info in gpu_info.items():\n",
    "    if gpu_id != 'error':\n",
    "        print(f\"  {gpu_id.upper()}: {info['name']}\")\n",
    "        print(f\"    ç¸½è¨˜æ†¶é«”: {info['total_memory_gb']:.2f} GB\")\n",
    "        print(f\"    å·²ä½¿ç”¨: {info['used_memory_gb']:.2f} GB ({info['utilization_percent']:.1f}%)\")\n",
    "        print(f\"    å¯ç”¨: {info['free_memory_gb']:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"  {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-GPU é…ç½®å’Œè² è¼‰å‡è¡¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPUManager:\n",
    "    \"\"\"Multi-GPU ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.gpu_optimizer = GPUMemoryOptimizer()\n",
    "        \n",
    "    def analyze_gpu_topology(self) -> Dict:\n",
    "        \"\"\"åˆ†æ GPU æ‹“æ’²çµæ§‹\"\"\"\n",
    "        print(\"ğŸ” åˆ†æ GPU æ‹“æ’²çµæ§‹...\")\n",
    "        \n",
    "        if not GPU_AVAILABLE:\n",
    "            return {'error': 'GPU ä¸å¯ç”¨'}\n",
    "        \n",
    "        try:\n",
    "            device_count = pynvml.nvmlDeviceGetCount()\n",
    "            topology = {\n",
    "                'device_count': device_count,\n",
    "                'devices': {},\n",
    "                'nvlink_topology': {},\n",
    "                'recommendations': []\n",
    "            }\n",
    "            \n",
    "            for i in range(device_count):\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                \n",
    "                try:\n",
    "                    # å˜—è©¦ç²å– GPU æ‹“æ’²ä¿¡æ¯\n",
    "                    pci_info = pynvml.nvmlDeviceGetPciInfo(handle)\n",
    "                    \n",
    "                    topology['devices'][f'gpu_{i}'] = {\n",
    "                        'name': name,\n",
    "                        'memory_total_gb': mem_info.total / (1024**3),\n",
    "                        'pci_bus': pci_info.bus,\n",
    "                        'pci_device': pci_info.device,\n",
    "                        'pci_domain': pci_info.domain\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    topology['devices'][f'gpu_{i}'] = {\n",
    "                        'name': name,\n",
    "                        'memory_total_gb': mem_info.total / (1024**3),\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "            \n",
    "            # åˆ†æ NVLink é€£æ¥ (å¦‚æœå¯ç”¨)\n",
    "            if device_count > 1:\n",
    "                try:\n",
    "                    for i in range(device_count):\n",
    "                        handle_i = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                        for j in range(i + 1, device_count):\n",
    "                            handle_j = pynvml.nvmlDeviceGetHandleByIndex(j)\n",
    "                            # é€™è£¡å¯ä»¥æ·»åŠ  NVLink é€£æ¥æª¢æ¸¬\n",
    "                            # ç”±æ–¼ NVML API çš„é™åˆ¶ï¼Œé€™éƒ¨åˆ†å¯èƒ½éœ€è¦å…¶ä»–æ–¹æ³•\n",
    "                            pass\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            # ç”Ÿæˆå»ºè­°\n",
    "            if device_count == 1:\n",
    "                topology['recommendations'].append(\"å–® GPU é…ç½®ï¼Œè€ƒæ…®å¢åŠ  GPU æ•¸é‡ä»¥æé«˜æ€§èƒ½\")\n",
    "            elif device_count > 1:\n",
    "                topology['recommendations'].append(f\"æª¢æ¸¬åˆ° {device_count} å€‹ GPUï¼Œå¯ä»¥é…ç½®æ¨¡å‹ä¸¦è¡Œ\")\n",
    "                topology['recommendations'].append(\"å»ºè­°ç‚ºä¸åŒæ¨¡å‹åˆ†é…ä¸åŒ GPU ä»¥é¿å…ç«¶çˆ­\")\n",
    "            \n",
    "            return topology\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'åˆ†æ GPU æ‹“æ’²å¤±æ•—: {e}'}\n",
    "    \n",
    "    def generate_multi_gpu_config(self, model_configs: List[Dict]) -> Dict:\n",
    "        \"\"\"ç”Ÿæˆ Multi-GPU é…ç½®\"\"\"\n",
    "        print(\"âš™ï¸  ç”Ÿæˆ Multi-GPU é…ç½®...\")\n",
    "        \n",
    "        topology = self.analyze_gpu_topology()\n",
    "        if 'error' in topology:\n",
    "            return topology\n",
    "        \n",
    "        device_count = topology['device_count']\n",
    "        if device_count < 2:\n",
    "            return {'error': 'éœ€è¦è‡³å°‘ 2 å€‹ GPU ä¾†é…ç½® Multi-GPU'}\n",
    "        \n",
    "        multi_gpu_config = {\n",
    "            'gpu_allocation': {},\n",
    "            'load_balancing': {},\n",
    "            'model_configs': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # GPU åˆ†é…ç­–ç•¥\n",
    "        num_models = len(model_configs)\n",
    "        \n",
    "        if num_models <= device_count:\n",
    "            # æ¯å€‹æ¨¡å‹åˆ†é…ä¸€å€‹ GPU\n",
    "            for i, model_config in enumerate(model_configs):\n",
    "                model_name = model_config.get('name', f'model_{i}')\n",
    "                gpu_id = i % device_count\n",
    "                \n",
    "                multi_gpu_config['gpu_allocation'][model_name] = [gpu_id]\n",
    "                multi_gpu_config['model_configs'][model_name] = {\n",
    "                    'instance_group': [\n",
    "                        {\n",
    "                            'count': 1,\n",
    "                            'kind': 'KIND_GPU',\n",
    "                            'gpus': [gpu_id]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "        else:\n",
    "            # æ¨¡å‹æ•¸é‡è¶…é GPU æ•¸é‡ï¼Œéœ€è¦å…±äº«\n",
    "            for i, model_config in enumerate(model_configs):\n",
    "                model_name = model_config.get('name', f'model_{i}')\n",
    "                gpu_id = i % device_count\n",
    "                \n",
    "                if gpu_id not in multi_gpu_config['gpu_allocation']:\n",
    "                    multi_gpu_config['gpu_allocation'][gpu_id] = []\n",
    "                \n",
    "                multi_gpu_config['gpu_allocation'][gpu_id].append(model_name)\n",
    "                multi_gpu_config['model_configs'][model_name] = {\n",
    "                    'instance_group': [\n",
    "                        {\n",
    "                            'count': 1,\n",
    "                            'kind': 'KIND_GPU',\n",
    "                            'gpus': [gpu_id]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "        \n",
    "        # è² è¼‰å‡è¡¡é…ç½®\n",
    "        multi_gpu_config['load_balancing'] = {\n",
    "            'strategy': 'round_robin',\n",
    "            'health_check': {\n",
    "                'enabled': True,\n",
    "                'interval_ms': 5000\n",
    "            },\n",
    "            'failover': {\n",
    "                'enabled': True,\n",
    "                'retry_count': 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ç”Ÿæˆå»ºè­°\n",
    "        multi_gpu_config['recommendations'] = [\n",
    "            f\"å·²ç‚º {num_models} å€‹æ¨¡å‹é…ç½® {device_count} å€‹ GPU\",\n",
    "            \"å»ºè­°ç›£æ§å„ GPU çš„è² è¼‰å‡è¡¡æƒ…æ³\",\n",
    "            \"å¯ä»¥æ ¹æ“šæ¨¡å‹ä½¿ç”¨é »ç‡èª¿æ•´ GPU åˆ†é…\",\n",
    "            \"è€ƒæ…®ç‚ºé«˜é »æ¨¡å‹é…ç½®å¤šå€‹å¯¦ä¾‹\"\n",
    "        ]\n",
    "        \n",
    "        if num_models > device_count:\n",
    "            multi_gpu_config['recommendations'].append(\n",
    "                \"æ¨¡å‹æ•¸é‡è¶…é GPU æ•¸é‡ï¼Œå»ºè­°ç›£æ§ GPU ä½¿ç”¨ç‡\"\n",
    "            )\n",
    "        \n",
    "        return multi_gpu_config\n",
    "    \n",
    "    def monitor_multi_gpu_performance(self, duration: int = 60) -> Dict:\n",
    "        \"\"\"ç›£æ§ Multi-GPU æ€§èƒ½\"\"\"\n",
    "        print(f\"ğŸ“Š ç›£æ§ Multi-GPU æ€§èƒ½ ({duration} ç§’)...\")\n",
    "        \n",
    "        if not GPU_AVAILABLE:\n",
    "            return {'error': 'GPU ä¸å¯ç”¨'}\n",
    "        \n",
    "        performance_data = {\n",
    "            'timeline': [],\n",
    "            'gpu_metrics': {},\n",
    "            'load_balance_analysis': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            timestamp = time.time()\n",
    "            gpu_info = self.gpu_optimizer.get_gpu_memory_info()\n",
    "            \n",
    "            if 'error' not in gpu_info:\n",
    "                frame_data = {'timestamp': timestamp}\n",
    "                \n",
    "                for gpu_id, info in gpu_info.items():\n",
    "                    frame_data[gpu_id] = {\n",
    "                        'memory_usage': info['utilization_percent'],\n",
    "                        'memory_used_gb': info['used_memory_gb']\n",
    "                    }\n",
    "                    \n",
    "                    # ç´¯ç©çµ±è¨ˆ\n",
    "                    if gpu_id not in performance_data['gpu_metrics']:\n",
    "                        performance_data['gpu_metrics'][gpu_id] = {\n",
    "                            'usage_history': [],\n",
    "                            'peak_usage': 0,\n",
    "                            'avg_usage': 0\n",
    "                        }\n",
    "                    \n",
    "                    performance_data['gpu_metrics'][gpu_id]['usage_history'].append(\n",
    "                        info['utilization_percent']\n",
    "                    )\n",
    "                    performance_data['gpu_metrics'][gpu_id]['peak_usage'] = max(\n",
    "                        performance_data['gpu_metrics'][gpu_id]['peak_usage'],\n",
    "                        info['utilization_percent']\n",
    "                    )\n",
    "                \n",
    "                performance_data['timeline'].append(frame_data)\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        # è¨ˆç®—å¹³å‡ä½¿ç”¨ç‡å’Œè² è¼‰å‡è¡¡åˆ†æ\n",
    "        for gpu_id, metrics in performance_data['gpu_metrics'].items():\n",
    "            usage_history = metrics['usage_history']\n",
    "            if usage_history:\n",
    "                metrics['avg_usage'] = sum(usage_history) / len(usage_history)\n",
    "        \n",
    "        # è² è¼‰å‡è¡¡åˆ†æ\n",
    "        avg_usages = [metrics['avg_usage'] for metrics in performance_data['gpu_metrics'].values()]\n",
    "        if len(avg_usages) > 1:\n",
    "            usage_variance = np.var(avg_usages)\n",
    "            performance_data['load_balance_analysis'] = {\n",
    "                'usage_variance': usage_variance,\n",
    "                'max_usage_diff': max(avg_usages) - min(avg_usages),\n",
    "                'is_balanced': usage_variance < 100  # è®Šç•°æ•¸é–¾å€¼\n",
    "            }\n",
    "            \n",
    "            if not performance_data['load_balance_analysis']['is_balanced']:\n",
    "                performance_data['recommendations'].append(\n",
    "                    \"æª¢æ¸¬åˆ°è² è¼‰ä¸å‡è¡¡ï¼Œå»ºè­°é‡æ–°åˆ†é…æ¨¡å‹æˆ–èª¿æ•´å¯¦ä¾‹æ•¸é‡\"\n",
    "                )\n",
    "        \n",
    "        return performance_data\n",
    "    \n",
    "    def plot_multi_gpu_performance(self, performance_data: Dict, figsize: Tuple[int, int] = (15, 8)):\n",
    "        \"\"\"ç¹ªè£½ Multi-GPU æ€§èƒ½åœ–è¡¨\"\"\"\n",
    "        if not performance_data.get('timeline'):\n",
    "            print(\"âš ï¸  æ²’æœ‰æ€§èƒ½æ•¸æ“šå¯é¡¯ç¤º\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # æå–æ•¸æ“š\n",
    "        timeline = performance_data['timeline']\n",
    "        timestamps = [frame['timestamp'] for frame in timeline]\n",
    "        start_time = min(timestamps)\n",
    "        relative_times = [(t - start_time) / 60 for t in timestamps]  # è½‰æ›ç‚ºåˆ†é˜\n",
    "        \n",
    "        # GPU ä½¿ç”¨ç‡æ™‚é–“ç·š\n",
    "        gpu_ids = [key for key in timeline[0].keys() if key.startswith('gpu_')]\n",
    "        \n",
    "        for gpu_id in gpu_ids:\n",
    "            usage_data = [frame[gpu_id]['memory_usage'] for frame in timeline]\n",
    "            axes[0, 0].plot(relative_times, usage_data, label=gpu_id.upper())\n",
    "        \n",
    "        axes[0, 0].set_title('GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡æ™‚é–“ç·š')\n",
    "        axes[0, 0].set_xlabel('æ™‚é–“ (åˆ†é˜)')\n",
    "        axes[0, 0].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # å¹³å‡ä½¿ç”¨ç‡æ¯”è¼ƒ\n",
    "        gpu_metrics = performance_data['gpu_metrics']\n",
    "        gpu_names = list(gpu_metrics.keys())\n",
    "        avg_usages = [gpu_metrics[gpu]['avg_usage'] for gpu in gpu_names]\n",
    "        \n",
    "        axes[0, 1].bar(range(len(gpu_names)), avg_usages)\n",
    "        axes[0, 1].set_title('å¹³å‡ GPU ä½¿ç”¨ç‡æ¯”è¼ƒ')\n",
    "        axes[0, 1].set_xlabel('GPU')\n",
    "        axes[0, 1].set_ylabel('å¹³å‡ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 1].set_xticks(range(len(gpu_names)))\n",
    "        axes[0, 1].set_xticklabels([gpu.upper() for gpu in gpu_names])\n",
    "        axes[0, 1].grid(True, axis='y')\n",
    "        \n",
    "        # å³°å€¼ä½¿ç”¨ç‡æ¯”è¼ƒ\n",
    "        peak_usages = [gpu_metrics[gpu]['peak_usage'] for gpu in gpu_names]\n",
    "        \n",
    "        axes[1, 0].bar(range(len(gpu_names)), peak_usages, color='orange')\n",
    "        axes[1, 0].set_title('å³°å€¼ GPU ä½¿ç”¨ç‡æ¯”è¼ƒ')\n",
    "        axes[1, 0].set_xlabel('GPU')\n",
    "        axes[1, 0].set_ylabel('å³°å€¼ä½¿ç”¨ç‡ (%)')\n",
    "        axes[1, 0].set_xticks(range(len(gpu_names)))\n",
    "        axes[1, 0].set_xticklabels([gpu.upper() for gpu in gpu_names])\n",
    "        axes[1, 0].grid(True, axis='y')\n",
    "        \n",
    "        # è² è¼‰å‡è¡¡åˆ†æ\n",
    "        if 'load_balance_analysis' in performance_data:\n",
    "            balance_info = performance_data['load_balance_analysis']\n",
    "            axes[1, 1].text(0.1, 0.8, f\"ä½¿ç”¨ç‡è®Šç•°æ•¸: {balance_info['usage_variance']:.2f}\", \n",
    "                          transform=axes[1, 1].transAxes, fontsize=12)\n",
    "            axes[1, 1].text(0.1, 0.6, f\"æœ€å¤§ä½¿ç”¨ç‡å·®ç•°: {balance_info['max_usage_diff']:.2f}%\", \n",
    "                          transform=axes[1, 1].transAxes, fontsize=12)\n",
    "            balance_status = \"æ˜¯\" if balance_info['is_balanced'] else \"å¦\"\n",
    "            axes[1, 1].text(0.1, 0.4, f\"è² è¼‰å‡è¡¡: {balance_status}\", \n",
    "                          transform=axes[1, 1].transAxes, fontsize=12, \n",
    "                          color='green' if balance_info['is_balanced'] else 'red')\n",
    "            axes[1, 1].set_title('è² è¼‰å‡è¡¡åˆ†æ')\n",
    "            axes[1, 1].set_xlim(0, 1)\n",
    "            axes[1, 1].set_ylim(0, 1)\n",
    "            axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# åˆå§‹åŒ– Multi-GPU ç®¡ç†å™¨\n",
    "multi_gpu_manager = MultiGPUManager()\n",
    "print(\"ğŸš€ Multi-GPU ç®¡ç†å™¨å·²åˆå§‹åŒ–\")\n",
    "\n",
    "# åˆ†æ GPU æ‹“æ’²\n",
    "topology = multi_gpu_manager.analyze_gpu_topology()\n",
    "if 'error' not in topology:\n",
    "    print(f\"\\nğŸ” GPU æ‹“æ’²åˆ†æ:\")\n",
    "    print(f\"  æª¢æ¸¬åˆ° {topology['device_count']} å€‹ GPU\")\n",
    "    for gpu_id, info in topology['devices'].items():\n",
    "        print(f\"    {gpu_id.upper()}: {info['name']} ({info['memory_total_gb']:.1f} GB)\")\n",
    "else:\n",
    "    print(f\"GPU æ‹“æ’²åˆ†æå¤±æ•—: {topology['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç¶œåˆæ€§èƒ½å„ªåŒ–å¯¦æˆ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveOptimizer:\n",
    "    \"\"\"ç¶œåˆæ€§èƒ½å„ªåŒ–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url: str = \"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.monitor = PerformanceMonitor(triton_url)\n",
    "        self.analyzer = BottleneckAnalyzer(triton_url)\n",
    "        self.gpu_optimizer = GPUMemoryOptimizer()\n",
    "        self.multi_gpu_manager = MultiGPUManager(triton_url)\n",
    "    \n",
    "    async def run_comprehensive_optimization(self, model_names: List[str], \n",
    "                                           optimization_duration: int = 300) -> Dict:\n",
    "        \"\"\"åŸ·è¡Œç¶œåˆæ€§èƒ½å„ªåŒ–åˆ†æ\"\"\"\n",
    "        print(\"ğŸš€ é–‹å§‹ç¶œåˆæ€§èƒ½å„ªåŒ–åˆ†æ...\")\n",
    "        \n",
    "        optimization_results = {\n",
    "            'models_analyzed': model_names,\n",
    "            'baseline_performance': {},\n",
    "            'bottleneck_analysis': {},\n",
    "            'memory_analysis': {},\n",
    "            'multi_gpu_analysis': {},\n",
    "            'optimization_recommendations': {},\n",
    "            'projected_improvements': {}\n",
    "        }\n",
    "        \n",
    "        # 1. åŸºç·šæ€§èƒ½æ¸¬è©¦\n",
    "        print(\"ğŸ“Š 1/5 åŸ·è¡ŒåŸºç·šæ€§èƒ½æ¸¬è©¦...\")\n",
    "        for model_name in model_names:\n",
    "            try:\n",
    "                baseline = await self._measure_baseline_performance(model_name)\n",
    "                optimization_results['baseline_performance'][model_name] = baseline\n",
    "                print(f\"  {model_name}: å»¶é² {baseline.get('avg_latency_ms', 'N/A')}ms, \"\n",
    "                      f\"ååé‡ {baseline.get('throughput_rps', 'N/A')} RPS\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {model_name} åŸºç·šæ¸¬è©¦å¤±æ•—: {e}\")\n",
    "        \n",
    "        # 2. ç“¶é ¸åˆ†æ\n",
    "        print(\"ğŸ” 2/5 åŸ·è¡Œç“¶é ¸åˆ†æ...\")\n",
    "        for model_name in model_names:\n",
    "            try:\n",
    "                analysis = self.analyzer.analyze_model_performance(model_name)\n",
    "                optimization_results['bottleneck_analysis'][model_name] = analysis\n",
    "                if analysis.get('bottlenecks'):\n",
    "                    print(f\"  {model_name}: ç™¼ç¾ {len(analysis['bottlenecks'])} å€‹ç“¶é ¸\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {model_name} ç“¶é ¸åˆ†æå¤±æ•—: {e}\")\n",
    "        \n",
    "        # 3. è¨˜æ†¶é«”åˆ†æ\n",
    "        print(\"ğŸ’¾ 3/5 åŸ·è¡Œè¨˜æ†¶é«”åˆ†æ...\")\n",
    "        try:\n",
    "            memory_analysis = self.gpu_optimizer.analyze_memory_usage_pattern(60)\n",
    "            optimization_results['memory_analysis'] = memory_analysis\n",
    "            if memory_analysis.get('memory_spikes'):\n",
    "                print(f\"  æª¢æ¸¬åˆ° {len(memory_analysis['memory_spikes'])} å€‹è¨˜æ†¶é«”å³°å€¼\")\n",
    "        except Exception as e:\n",
    "            print(f\"  è¨˜æ†¶é«”åˆ†æå¤±æ•—: {e}\")\n",
    "        \n",
    "        # 4. Multi-GPU åˆ†æ\n",
    "        print(\"ğŸ”„ 4/5 åŸ·è¡Œ Multi-GPU åˆ†æ...\")\n",
    "        try:\n",
    "            model_configs = [{'name': name} for name in model_names]\n",
    "            multi_gpu_config = self.multi_gpu_manager.generate_multi_gpu_config(model_configs)\n",
    "            optimization_results['multi_gpu_analysis'] = multi_gpu_config\n",
    "            if 'error' not in multi_gpu_config:\n",
    "                print(f\"  ç”Ÿæˆäº† Multi-GPU é…ç½®ï¼Œæ”¯æŒ {len(model_names)} å€‹æ¨¡å‹\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Multi-GPU åˆ†æå¤±æ•—: {e}\")\n",
    "        \n",
    "        # 5. ç”Ÿæˆå„ªåŒ–å»ºè­°\n",
    "        print(\"ğŸ’¡ 5/5 ç”Ÿæˆå„ªåŒ–å»ºè­°...\")\n",
    "        optimization_results['optimization_recommendations'] = \\\n",
    "            self._generate_comprehensive_recommendations(optimization_results)\n",
    "        \n",
    "        # 6. é æ¸¬æ”¹é€²æ•ˆæœ\n",
    "        optimization_results['projected_improvements'] = \\\n",
    "            self._project_improvements(optimization_results)\n",
    "        \n",
    "        print(\"âœ… ç¶œåˆæ€§èƒ½å„ªåŒ–åˆ†æå®Œæˆ\")\n",
    "        return optimization_results\n",
    "    \n",
    "    async def _measure_baseline_performance(self, model_name: str) -> Dict:\n",
    "        \"\"\"æ¸¬é‡åŸºç·šæ€§èƒ½\"\"\"\n",
    "        try:\n",
    "            # ç²å–æ¨¡å‹å…ƒæ•¸æ“š\n",
    "            metadata = self.client.get_model_metadata(model_name)\n",
    "            \n",
    "            # å‰µå»ºæ¸¬è©¦è¼¸å…¥\n",
    "            inputs = []\n",
    "            outputs = []\n",
    "            \n",
    "            for inp in metadata['inputs']:\n",
    "                input_name = inp['name']\n",
    "                input_shape = inp['shape']\n",
    "                input_dtype = inp['datatype']\n",
    "                \n",
    "                if input_dtype == 'FP32':\n",
    "                    test_data = np.random.random([1] + input_shape).astype(np.float32)\n",
    "                elif input_dtype == 'INT64':\n",
    "                    test_data = np.random.randint(0, 1000, [1] + input_shape).astype(np.int64)\n",
    "                else:\n",
    "                    test_data = np.random.random([1] + input_shape).astype(np.float32)\n",
    "                \n",
    "                inputs.append(httpclient.InferInput(input_name, test_data.shape, input_dtype))\n",
    "                inputs[-1].set_data_from_numpy(test_data)\n",
    "            \n",
    "            for out in metadata['outputs']:\n",
    "                outputs.append(httpclient.InferRequestedOutput(out['name']))\n",
    "            \n",
    "            # é ç†±\n",
    "            for _ in range(10):\n",
    "                self.client.infer(model_name, inputs, outputs=outputs)\n",
    "            \n",
    "            # æ¸¬é‡æ€§èƒ½\n",
    "            num_requests = 100\n",
    "            latencies = []\n",
    "            \n",
    "            for _ in range(num_requests):\n",
    "                start_time = time.time()\n",
    "                self.client.infer(model_name, inputs, outputs=outputs)\n",
    "                end_time = time.time()\n",
    "                latencies.append((end_time - start_time) * 1000)  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "            \n",
    "            # è¨ˆç®—çµ±è¨ˆä¿¡æ¯\n",
    "            avg_latency = sum(latencies) / len(latencies)\n",
    "            p95_latency = np.percentile(latencies, 95)\n",
    "            p99_latency = np.percentile(latencies, 99)\n",
    "            throughput = num_requests / (sum(latencies) / 1000)  # RPS\n",
    "            \n",
    "            return {\n",
    "                'avg_latency_ms': avg_latency,\n",
    "                'p95_latency_ms': p95_latency,\n",
    "                'p99_latency_ms': p99_latency,\n",
    "                'throughput_rps': throughput,\n",
    "                'min_latency_ms': min(latencies),\n",
    "                'max_latency_ms': max(latencies)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _generate_comprehensive_recommendations(self, results: Dict) -> Dict:\n",
    "        \"\"\"ç”Ÿæˆç¶œåˆå„ªåŒ–å»ºè­°\"\"\"\n",
    "        recommendations = {\n",
    "            'high_priority': [],\n",
    "            'medium_priority': [],\n",
    "            'low_priority': [],\n",
    "            'implementation_order': []\n",
    "        }\n",
    "        \n",
    "        # åˆ†æç“¶é ¸ä¸¦ç”Ÿæˆå»ºè­°\n",
    "        for model_name, analysis in results.get('bottleneck_analysis', {}).items():\n",
    "            if analysis.get('bottlenecks'):\n",
    "                for bottleneck in analysis['bottlenecks']:\n",
    "                    if 'å‹•æ…‹æ‰¹æ¬¡' in bottleneck:\n",
    "                        recommendations['high_priority'].append(\n",
    "                            f\"{model_name}: å•Ÿç”¨ä¸¦å„ªåŒ–å‹•æ…‹æ‰¹æ¬¡é…ç½®\"\n",
    "                        )\n",
    "                    elif 'å¯¦ä¾‹' in bottleneck:\n",
    "                        recommendations['medium_priority'].append(\n",
    "                            f\"{model_name}: å¢åŠ æ¨¡å‹å¯¦ä¾‹æ•¸é‡\"\n",
    "                        )\n",
    "                    elif 'CPU' in bottleneck:\n",
    "                        recommendations['high_priority'].append(\n",
    "                            f\"{model_name}: å¾ CPU é·ç§»åˆ° GPU æ¨ç†\"\n",
    "                        )\n",
    "        \n",
    "        # è¨˜æ†¶é«”å„ªåŒ–å»ºè­°\n",
    "        memory_analysis = results.get('memory_analysis', {})\n",
    "        if memory_analysis.get('memory_spikes'):\n",
    "            recommendations['high_priority'].append(\n",
    "                \"å„ªåŒ– GPU è¨˜æ†¶é«”ä½¿ç”¨ï¼Œæ¸›å°‘è¨˜æ†¶é«”å³°å€¼\"\n",
    "            )\n",
    "        \n",
    "        if memory_analysis.get('recommendations'):\n",
    "            recommendations['medium_priority'].extend(\n",
    "                memory_analysis['recommendations']\n",
    "            )\n",
    "        \n",
    "        # Multi-GPU å»ºè­°\n",
    "        multi_gpu_analysis = results.get('multi_gpu_analysis', {})\n",
    "        if 'error' not in multi_gpu_analysis and multi_gpu_analysis.get('recommendations'):\n",
    "            recommendations['low_priority'].extend(\n",
    "                multi_gpu_analysis['recommendations']\n",
    "            )\n",
    "        \n",
    "        # å¯¦æ–½é †åºå»ºè­°\n",
    "        recommendations['implementation_order'] = [\n",
    "            \"1. å•Ÿç”¨å’Œå„ªåŒ–å‹•æ…‹æ‰¹æ¬¡è™•ç†\",\n",
    "            \"2. å„ªåŒ– GPU è¨˜æ†¶é«”é…ç½®\",\n",
    "            \"3. èª¿æ•´æ¨¡å‹å¯¦ä¾‹æ•¸é‡\",\n",
    "            \"4. é…ç½® Multi-GPU è² è¼‰å‡è¡¡\",\n",
    "            \"5. å¯¦æ–½æ€§èƒ½ç›£æ§å’Œå‘Šè­¦\"\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _project_improvements(self, results: Dict) -> Dict:\n",
    "        \"\"\"é æ¸¬å„ªåŒ–æ”¹é€²æ•ˆæœ\"\"\"\n",
    "        improvements = {\n",
    "            'latency_improvement': {},\n",
    "            'throughput_improvement': {},\n",
    "            'memory_efficiency': {},\n",
    "            'overall_score': {}\n",
    "        }\n",
    "        \n",
    "        for model_name, baseline in results.get('baseline_performance', {}).items():\n",
    "            if 'error' in baseline:\n",
    "                continue\n",
    "            \n",
    "            # åŸºæ–¼ç“¶é ¸åˆ†æé æ¸¬æ”¹é€²\n",
    "            bottlenecks = results.get('bottleneck_analysis', {}).get(model_name, {}).get('bottlenecks', [])\n",
    "            \n",
    "            latency_improvement = 1.0  # ç„¡æ”¹é€²åŸºç·š\n",
    "            throughput_improvement = 1.0\n",
    "            \n",
    "            # å‹•æ…‹æ‰¹æ¬¡æ”¹é€²é æ¸¬\n",
    "            if any('å‹•æ…‹æ‰¹æ¬¡' in b for b in bottlenecks):\n",
    "                latency_improvement *= 0.8  # 20% å»¶é²æ”¹é€²\n",
    "                throughput_improvement *= 3.0  # 3å€ååé‡æ”¹é€²\n",
    "            \n",
    "            # å¯¦ä¾‹æ•¸é‡æ”¹é€²é æ¸¬\n",
    "            if any('å¯¦ä¾‹' in b for b in bottlenecks):\n",
    "                throughput_improvement *= 2.0  # 2å€ååé‡æ”¹é€²\n",
    "            \n",
    "            # GPU é·ç§»æ”¹é€²é æ¸¬\n",
    "            if any('CPU' in b for b in bottlenecks):\n",
    "                latency_improvement *= 0.3  # 70% å»¶é²æ”¹é€²\n",
    "                throughput_improvement *= 5.0  # 5å€ååé‡æ”¹é€²\n",
    "            \n",
    "            improvements['latency_improvement'][model_name] = {\n",
    "                'current_ms': baseline.get('avg_latency_ms', 0),\n",
    "                'projected_ms': baseline.get('avg_latency_ms', 0) * latency_improvement,\n",
    "                'improvement_percent': (1 - latency_improvement) * 100\n",
    "            }\n",
    "            \n",
    "            improvements['throughput_improvement'][model_name] = {\n",
    "                'current_rps': baseline.get('throughput_rps', 0),\n",
    "                'projected_rps': baseline.get('throughput_rps', 0) * throughput_improvement,\n",
    "                'improvement_percent': (throughput_improvement - 1) * 100\n",
    "            }\n",
    "            \n",
    "            # æ•´é«”è©•åˆ†\n",
    "            score = (throughput_improvement + (2 - latency_improvement)) / 2\n",
    "            improvements['overall_score'][model_name] = min(score * 20, 100)  # æœ€é«˜100åˆ†\n",
    "        \n",
    "        return improvements\n",
    "    \n",
    "    def generate_optimization_report(self, results: Dict) -> str:\n",
    "        \"\"\"ç”Ÿæˆè©³ç´°çš„å„ªåŒ–å ±å‘Š\"\"\"\n",
    "        report = f\"\"\"\n",
    "# ğŸš€ ç¶œåˆæ€§èƒ½å„ªåŒ–å ±å‘Š\n",
    "\n",
    "## ğŸ“Š åˆ†ææ¦‚è¿°\n",
    "- åˆ†ææ¨¡å‹æ•¸é‡: {len(results['models_analyzed'])}\n",
    "- åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- æ¨¡å‹æ¸…å–®: {', '.join(results['models_analyzed'])}\n",
    "\n",
    "## ğŸ“ˆ åŸºç·šæ€§èƒ½\n",
    "\"\"\"\n",
    "        \n",
    "        for model_name, baseline in results.get('baseline_performance', {}).items():\n",
    "            if 'error' not in baseline:\n",
    "                report += f\"\"\"\n",
    "### {model_name}\n",
    "- å¹³å‡å»¶é²: {baseline.get('avg_latency_ms', 'N/A'):.2f} ms\n",
    "- P95 å»¶é²: {baseline.get('p95_latency_ms', 'N/A'):.2f} ms\n",
    "- ååé‡: {baseline.get('throughput_rps', 'N/A'):.2f} RPS\n",
    "\"\"\"\n",
    "        \n",
    "        report += \"\\n## ğŸ” ç“¶é ¸åˆ†æ\\n\"\n",
    "        for model_name, analysis in results.get('bottleneck_analysis', {}).items():\n",
    "            if analysis.get('bottlenecks'):\n",
    "                report += f\"\\n### {model_name}\\n\"\n",
    "                for bottleneck in analysis['bottlenecks']:\n",
    "                    report += f\"- âš ï¸  {bottleneck}\\n\"\n",
    "        \n",
    "        report += \"\\n## ğŸ’¡ å„ªåŒ–å»ºè­°\\n\"\n",
    "        recommendations = results.get('optimization_recommendations', {})\n",
    "        \n",
    "        if recommendations.get('high_priority'):\n",
    "            report += \"\\n### ğŸ”´ é«˜å„ªå…ˆç´š\\n\"\n",
    "            for rec in recommendations['high_priority']:\n",
    "                report += f\"- {rec}\\n\"\n",
    "        \n",
    "        if recommendations.get('medium_priority'):\n",
    "            report += \"\\n### ğŸŸ¡ ä¸­å„ªå…ˆç´š\\n\"\n",
    "            for rec in recommendations['medium_priority']:\n",
    "                report += f\"- {rec}\\n\"\n",
    "        \n",
    "        if recommendations.get('implementation_order'):\n",
    "            report += \"\\n### ğŸ“‹ å¯¦æ–½é †åº\\n\"\n",
    "            for order in recommendations['implementation_order']:\n",
    "                report += f\"{order}\\n\"\n",
    "        \n",
    "        report += \"\\n## ğŸ“Š é æœŸæ”¹é€²æ•ˆæœ\\n\"\n",
    "        improvements = results.get('projected_improvements', {})\n",
    "        \n",
    "        for model_name in results['models_analyzed']:\n",
    "            if model_name in improvements.get('latency_improvement', {}):\n",
    "                lat_imp = improvements['latency_improvement'][model_name]\n",
    "                thr_imp = improvements['throughput_improvement'][model_name]\n",
    "                score = improvements['overall_score'].get(model_name, 0)\n",
    "                \n",
    "                report += f\"\"\"\n",
    "### {model_name}\n",
    "- å»¶é²æ”¹é€²: {lat_imp['improvement_percent']:.1f}% ({lat_imp['current_ms']:.2f}ms â†’ {lat_imp['projected_ms']:.2f}ms)\n",
    "- ååé‡æ”¹é€²: {thr_imp['improvement_percent']:.1f}% ({thr_imp['current_rps']:.2f} â†’ {thr_imp['projected_rps']:.2f} RPS)\n",
    "- æ•´é«”è©•åˆ†: {score:.1f}/100\n",
    "\"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# åˆå§‹åŒ–ç¶œåˆå„ªåŒ–å™¨\n",
    "comprehensive_optimizer = ComprehensiveOptimizer()\n",
    "print(\"ğŸ¯ ç¶œåˆæ€§èƒ½å„ªåŒ–å™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¯¦æˆ°æ¼”ç¤ºï¼šå®Œæ•´å„ªåŒ–æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®è¦åˆ†æçš„æ¨¡å‹\n",
    "models_to_analyze = [\n",
    "    \"text_classification\",  # æ›¿æ›ç‚ºæ‚¨çš„å¯¦éš›æ¨¡å‹åç¨±\n",
    "    \"sentiment_analysis\",   # æ›¿æ›ç‚ºæ‚¨çš„å¯¦éš›æ¨¡å‹åç¨±\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹å®Œæ•´çš„æ€§èƒ½å„ªåŒ–æµç¨‹æ¼”ç¤º\")\n",
    "print(f\"å°‡åˆ†æä»¥ä¸‹æ¨¡å‹: {', '.join(models_to_analyze)}\")\n",
    "print(\"\\næ³¨æ„: è«‹ç¢ºä¿é€™äº›æ¨¡å‹å·²åœ¨ Triton æœå‹™å™¨ä¸­åŠ è¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§\n",
    "available_models = []\n",
    "\n",
    "try:\n",
    "    client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "    server_metadata = client.get_server_metadata()\n",
    "    print(\"ğŸ” æª¢æŸ¥å¯ç”¨æ¨¡å‹...\")\n",
    "    \n",
    "    for model_name in models_to_analyze:\n",
    "        try:\n",
    "            model_metadata = client.get_model_metadata(model_name)\n",
    "            available_models.append(model_name)\n",
    "            print(f\"  âœ… {model_name} - å¯ç”¨\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {model_name} - ä¸å¯ç”¨: {e}\")\n",
    "    \n",
    "    if not available_models:\n",
    "        print(\"\\nâš ï¸  æ²’æœ‰å¯ç”¨çš„æ¨¡å‹é€²è¡Œåˆ†æ\")\n",
    "        print(\"è«‹ç¢ºä¿ Triton æœå‹™å™¨æ­£åœ¨é‹è¡Œä¸¦ä¸”å·²åŠ è¼‰æ¨¡å‹\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… æ‰¾åˆ° {len(available_models)} å€‹å¯ç”¨æ¨¡å‹\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç„¡æ³•é€£æ¥åˆ° Triton æœå‹™å™¨: {e}\")\n",
    "    print(\"è«‹ç¢ºä¿ Triton æœå‹™å™¨åœ¨ localhost:8000 ä¸Šé‹è¡Œ\")\n",
    "    available_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œç¶œåˆå„ªåŒ–åˆ†æï¼ˆå¦‚æœæœ‰å¯ç”¨æ¨¡å‹ï¼‰\n",
    "if available_models:\n",
    "    print(\"ğŸ¯ é–‹å§‹ç¶œåˆå„ªåŒ–åˆ†æ...\")\n",
    "    \n",
    "    # é€™æ˜¯ä¸€å€‹ç•°æ­¥å‡½æ•¸ï¼Œåœ¨ Jupyter ä¸­éœ€è¦ä½¿ç”¨ await\n",
    "    optimization_results = await comprehensive_optimizer.run_comprehensive_optimization(\n",
    "        model_names=available_models,\n",
    "        optimization_duration=60  # ç¸®çŸ­åˆ†ææ™‚é–“ä»¥ä¾¿æ¼”ç¤º\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆè©³ç´°å ±å‘Š\n",
    "    detailed_report = comprehensive_optimizer.generate_optimization_report(optimization_results)\n",
    "    print(detailed_report)\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  è·³éç¶œåˆåˆ†æ - æ²’æœ‰å¯ç”¨æ¨¡å‹\")\n",
    "    \n",
    "    # æ¼”ç¤ºå…¶ä»–åŠŸèƒ½\n",
    "    print(\"\\nğŸ”§ æ¼”ç¤ºå…¶ä»–å„ªåŒ–åŠŸèƒ½...\")\n",
    "    \n",
    "    # GPU è¨˜æ†¶é«”ä¿¡æ¯\n",
    "    gpu_info = gpu_optimizer.get_gpu_memory_info()\n",
    "    print(\"\\nğŸ“Š GPU è¨˜æ†¶é«”ç‹€æ…‹:\")\n",
    "    if 'error' not in gpu_info:\n",
    "        for gpu_id, info in gpu_info.items():\n",
    "            print(f\"  {gpu_id}: {info['utilization_percent']:.1f}% ä½¿ç”¨ç‡\")\n",
    "    else:\n",
    "        print(f\"  {gpu_info}\")\n",
    "    \n",
    "    # è¨˜æ†¶é«”å„ªåŒ–é…ç½®\n",
    "    memory_config = gpu_optimizer.generate_memory_optimization_config()\n",
    "    print(\"\\nâš™ï¸  è¨˜æ†¶é«”å„ªåŒ–é…ç½®å·²ç”Ÿæˆ\")\n",
    "    \n",
    "    # Multi-GPU æ‹“æ’²\n",
    "    topology = multi_gpu_manager.analyze_gpu_topology()\n",
    "    if 'error' not in topology:\n",
    "        print(f\"\\nğŸ”„ æª¢æ¸¬åˆ° {topology['device_count']} å€‹ GPU\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ”„ GPU æ‹“æ’²: {topology['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ€§èƒ½ç›£æ§å„€è¡¨æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•Ÿå‹•å¯¦æ™‚æ€§èƒ½ç›£æ§ï¼ˆæ¼”ç¤ºç‰ˆæœ¬ï¼‰\n",
    "if available_models:\n",
    "    print(\"ğŸ“Š å•Ÿå‹•å¯¦æ™‚æ€§èƒ½ç›£æ§...\")\n",
    "    \n",
    "    # å•Ÿå‹•å¾Œå°ç›£æ§\n",
    "    monitoring_task = asyncio.create_task(\n",
    "        monitor.monitor_performance(duration=30, interval=1.0)\n",
    "    )\n",
    "    \n",
    "    # ç­‰å¾…ç›£æ§å®Œæˆ\n",
    "    await monitoring_task\n",
    "    \n",
    "    # é¡¯ç¤ºç›£æ§çµæœ\n",
    "    if monitor.metrics_history:\n",
    "        print(f\"\\nğŸ“ˆ æ”¶é›†äº† {len(monitor.metrics_history)} å€‹ç›£æ§æ•¸æ“šé»\")\n",
    "        monitor.plot_metrics()\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  æ²’æœ‰æ”¶é›†åˆ°ç›£æ§æ•¸æ“š\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸  è·³éå¯¦æ™‚ç›£æ§ - æ²’æœ‰å¯ç”¨æ¨¡å‹\")\n",
    "    \n",
    "    # æ¼”ç¤ºéœæ…‹ GPU ç›£æ§\n",
    "    print(\"\\nğŸ“Š æ¼”ç¤ºéœæ…‹ GPU ç›£æ§...\")\n",
    "    \n",
    "    # æ¨¡æ“¬ç›£æ§æ•¸æ“š\n",
    "    import random\n",
    "    \n",
    "    mock_history = []\n",
    "    for i in range(30):\n",
    "        mock_history.append({\n",
    "            'timestamp': time.time() + i,\n",
    "            'gpu_id': 'gpu_0',\n",
    "            'used_memory_gb': 4.5 + random.uniform(-0.5, 0.5),\n",
    "            'utilization_percent': 60 + random.uniform(-20, 20)\n",
    "        })\n",
    "    \n",
    "    gpu_optimizer.plot_memory_usage(mock_history)\n",
    "    print(\"ä¸Šåœ–é¡¯ç¤ºäº†æ¨¡æ“¬çš„ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. å„ªåŒ–é…ç½®ç”Ÿæˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimized_model_config(model_name: str, optimization_level: str = \"balanced\") -> Dict:\n",
    "    \"\"\"ç”Ÿæˆå„ªåŒ–çš„æ¨¡å‹é…ç½®\"\"\"\n",
    "    \n",
    "    base_config = {\n",
    "        \"name\": model_name,\n",
    "        \"platform\": \"tensorrt_plan\",  # æˆ–å…¶ä»–é©ç•¶çš„å¹³å°\n",
    "        \"max_batch_size\": 32,\n",
    "        \"input\": [],\n",
    "        \"output\": [],\n",
    "        \"dynamic_batching\": {\n",
    "            \"preferred_batch_size\": [1, 2, 4, 8, 16],\n",
    "            \"max_queue_delay_microseconds\": 1000\n",
    "        },\n",
    "        \"instance_group\": [\n",
    "            {\n",
    "                \"count\": 1,\n",
    "                \"kind\": \"KIND_GPU\",\n",
    "                \"gpus\": [0]\n",
    "            }\n",
    "        ],\n",
    "        \"optimization\": {\n",
    "            \"graph\": {\n",
    "                \"level\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # æ ¹æ“šå„ªåŒ–ç´šåˆ¥èª¿æ•´é…ç½®\n",
    "    if optimization_level == \"latency\":\n",
    "        # å„ªåŒ–å»¶é²\n",
    "        base_config[\"max_batch_size\"] = 4\n",
    "        base_config[\"dynamic_batching\"][\"preferred_batch_size\"] = [1, 2]\n",
    "        base_config[\"dynamic_batching\"][\"max_queue_delay_microseconds\"] = 100\n",
    "        base_config[\"instance_group\"][0][\"count\"] = 2\n",
    "        \n",
    "    elif optimization_level == \"throughput\":\n",
    "        # å„ªåŒ–ååé‡\n",
    "        base_config[\"max_batch_size\"] = 64\n",
    "        base_config[\"dynamic_batching\"][\"preferred_batch_size\"] = [8, 16, 32, 64]\n",
    "        base_config[\"dynamic_batching\"][\"max_queue_delay_microseconds\"] = 5000\n",
    "        base_config[\"instance_group\"][0][\"count\"] = 1\n",
    "        \n",
    "    elif optimization_level == \"memory\":\n",
    "        # å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨\n",
    "        base_config[\"max_batch_size\"] = 16\n",
    "        base_config[\"dynamic_batching\"][\"preferred_batch_size\"] = [1, 2, 4, 8]\n",
    "        base_config[\"optimization\"][\"memory_pool_byte_size\"] = 1073741824  # 1GB\n",
    "        \n",
    "    return base_config\n",
    "\n",
    "# ç”Ÿæˆä¸åŒå„ªåŒ–ç´šåˆ¥çš„é…ç½®ç¤ºä¾‹\n",
    "print(\"âš™ï¸  ç”Ÿæˆå„ªåŒ–é…ç½®ç¤ºä¾‹...\")\n",
    "\n",
    "optimization_levels = [\"latency\", \"throughput\", \"memory\", \"balanced\"]\n",
    "\n",
    "for level in optimization_levels:\n",
    "    config = generate_optimized_model_config(\"example_model\", level)\n",
    "    print(f\"\\nğŸ“‹ {level.upper()} å„ªåŒ–é…ç½®:\")\n",
    "    print(f\"  æœ€å¤§æ‰¹æ¬¡å¤§å°: {config['max_batch_size']}\")\n",
    "    print(f\"  é¦–é¸æ‰¹æ¬¡å¤§å°: {config['dynamic_batching']['preferred_batch_size']}\")\n",
    "    print(f\"  æœ€å¤§éšŠåˆ—å»¶é²: {config['dynamic_batching']['max_queue_delay_microseconds']} Î¼s\")\n",
    "    print(f\"  å¯¦ä¾‹æ•¸é‡: {config['instance_group'][0]['count']}\")\n",
    "\n",
    "print(\"\\nğŸ’¾ é…ç½®æ–‡ä»¶å·²ç”Ÿæˆï¼Œå¯ä»¥ä¿å­˜ç‚º config.pbtxt ä¸¦æ‡‰ç”¨åˆ° Triton æœå‹™å™¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ç¸½çµå’Œæœ€ä½³å¯¦è¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ æ€§èƒ½å„ªåŒ–ç¸½çµ\n",
    "\n",
    "é€šéæœ¬å¯¦é©—å®¤ï¼Œæˆ‘å€‘å­¸ç¿’äº† Triton Inference Server çš„æ¥µè‡´æ€§èƒ½å„ªåŒ–æŠ€è¡“ï¼š\n",
    "\n",
    "#### âœ… å®Œæˆçš„å„ªåŒ–é …ç›®\n",
    "\n",
    "1. **ç“¶é ¸åˆ†æå’Œè¨ºæ–·**\n",
    "   - è‡ªå‹•åŒ–æ¨¡å‹æ€§èƒ½ç“¶é ¸è­˜åˆ¥\n",
    "   - æ‰¹æ¬¡å¤§å°å’Œä¸¦ç™¼ç´šåˆ¥å„ªåŒ–\n",
    "   - é…ç½®å•é¡Œæª¢æ¸¬å’Œå»ºè­°\n",
    "\n",
    "2. **GPU è¨˜æ†¶é«”ç®¡ç†**\n",
    "   - å¯¦æ™‚è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§\n",
    "   - è¨˜æ†¶é«”ä½¿ç”¨æ¨¡å¼åˆ†æ\n",
    "   - è‡ªå‹•åŒ–è¨˜æ†¶é«”é…ç½®ç”Ÿæˆ\n",
    "\n",
    "3. **Multi-GPU é…ç½®**\n",
    "   - GPU æ‹“æ’²çµæ§‹åˆ†æ\n",
    "   - æ™ºèƒ½è² è¼‰å‡è¡¡é…ç½®\n",
    "   - è·¨ GPU æ€§èƒ½ç›£æ§\n",
    "\n",
    "4. **ç¶œåˆæ€§èƒ½å„ªåŒ–**\n",
    "   - ç«¯åˆ°ç«¯å„ªåŒ–æµç¨‹\n",
    "   - æ”¹é€²æ•ˆæœé æ¸¬\n",
    "   - è©³ç´°å„ªåŒ–å ±å‘Šç”Ÿæˆ\n",
    "\n",
    "#### ğŸ† æœ€ä½³å¯¦è¸\n",
    "\n",
    "1. **é…ç½®å„ªåŒ–**\n",
    "   - å•Ÿç”¨å‹•æ…‹æ‰¹æ¬¡è™•ç†\n",
    "   - åˆç†è¨­ç½®é¦–é¸æ‰¹æ¬¡å¤§å°\n",
    "   - æ ¹æ“šç¡¬ä»¶èª¿æ•´å¯¦ä¾‹æ•¸é‡\n",
    "\n",
    "2. **è¨˜æ†¶é«”ç®¡ç†**\n",
    "   - ç›£æ§ GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "   - è¨­ç½®åˆç†çš„è¨˜æ†¶é«”æ± å¤§å°\n",
    "   - é¿å…è¨˜æ†¶é«”ç¢ç‰‡åŒ–\n",
    "\n",
    "3. **å¤š GPU éƒ¨ç½²**\n",
    "   - åˆç†åˆ†é…æ¨¡å‹åˆ°ä¸åŒ GPU\n",
    "   - å¯¦æ–½è² è¼‰å‡è¡¡å’Œæ•…éšœè½‰ç§»\n",
    "   - ç›£æ§ GPU é–“è² è¼‰åˆ†ä½ˆ\n",
    "\n",
    "4. **æŒçºŒå„ªåŒ–**\n",
    "   - å»ºç«‹æ€§èƒ½ç›£æ§é«”ç³»\n",
    "   - å®šæœŸåŸ·è¡Œæ€§èƒ½è©•ä¼°\n",
    "   - æ ¹æ“šæ¥­å‹™éœ€æ±‚èª¿æ•´é…ç½®\n",
    "\n",
    "#### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°\n",
    "\n",
    "1. **ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²**\n",
    "   - æ‡‰ç”¨å­¸åˆ°çš„å„ªåŒ–é…ç½®\n",
    "   - å»ºç«‹æ€§èƒ½ç›£æ§å‘Šè­¦\n",
    "   - å¯¦æ–½è‡ªå‹•åŒ–å„ªåŒ–æµç¨‹\n",
    "\n",
    "2. **é«˜ç´šåŠŸèƒ½æ¢ç´¢**\n",
    "   - æ¨¡å‹ Ensemble å’Œ Pipeline\n",
    "   - è‡ªå®šç¾©å¾Œç«¯é–‹ç™¼\n",
    "   - åˆ†æ•£å¼æ¨ç†æ¶æ§‹\n",
    "\n",
    "3. **æŒçºŒå­¸ç¿’**\n",
    "   - é—œæ³¨ Triton æ–°åŠŸèƒ½ç™¼å¸ƒ\n",
    "   - åƒèˆ‡ç¤¾å€è¨è«–å’Œè²¢ç»\n",
    "   - æ¢ç´¢æ–°çš„å„ªåŒ–æŠ€è¡“\n",
    "\n",
    "### ğŸ“š ç›¸é—œè³‡æº\n",
    "\n",
    "- [Triton Performance Guide](https://github.com/triton-inference-server/tutorials)\n",
    "- [GPU Memory Optimization](https://developer.nvidia.com/blog/cuda-memory-optimization/)\n",
    "- [Multi-GPU Best Practices](https://docs.nvidia.com/deeplearning/frameworks/user-guide/)\n",
    "\n",
    "ğŸ‰ **æ­å–œï¼æ‚¨å·²ç¶“æŒæ¡äº† Triton Inference Server çš„æ¥µè‡´æ€§èƒ½å„ªåŒ–æŠ€è¡“ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}