# Lab-01: å–®ç¯€é»å¤š GPU åˆ†æ•£å¼è¨“ç·´ - DataParallel vs DistributedDataParallel

## ğŸ¯ å¯¦é©—ç›®æ¨™

æœ¬å¯¦é©—å°‡æ·±å…¥æ¢ç´¢åœ¨å–®ç¯€é»å¤š GPU ç’°å¢ƒä¸‹çš„å…©ç¨®ä¸»è¦æ•¸æ“šä¸¦è¡Œè¨“ç·´æ–¹æ³•ï¼š
- **DataParallel (DP)**: PyTorch çš„åŸºç¤å¤š GPU è¨“ç·´æ–¹æ³•
- **DistributedDataParallel (DDP)**: ç¾ä»£æ¨è–¦çš„åˆ†æ•£å¼è¨“ç·´æ–¹æ³•

é€šéå¯¦éš›å¯¦é©—å°æ¯”å…©è€…çš„æ€§èƒ½å·®ç•°ã€è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³å’Œæ“´å±•æ€§ï¼Œç†è§£ç‚ºä»€éº¼ DDP å·²æˆç‚ºæ¥­ç•Œæ¨™æº–ã€‚

## ğŸ“š ç†è«–èƒŒæ™¯

### DataParallel (DP) ç‰¹é»
- âœ… ä½¿ç”¨ç°¡å–®ï¼Œåªéœ€ä¸€è¡Œä»£ç¢¼åŒ…è£æ¨¡å‹
- âŒ ä¸» GPU è² è¼‰ä¸å‡ï¼Œå­˜åœ¨é€šä¿¡ç“¶é ¸
- âŒ å— Python GIL é™åˆ¶ï¼Œç„¡æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸ CPU
- âŒ é›£ä»¥æ“´å±•åˆ°å¤šç¯€é»

### DistributedDataParallel (DDP) ç‰¹é»
- âœ… ç„¡ä¸» GPU æ¦‚å¿µï¼Œè² è¼‰å®Œå…¨å‡è¡¡
- âœ… å¤šé€²ç¨‹æ¶æ§‹ï¼Œé¿å… GIL é™åˆ¶
- âœ… é«˜æ•ˆçš„ All-Reduce é€šä¿¡ï¼Œæ”¯æ´å¤šç¨®å¾Œç«¯
- âœ… åŸç”Ÿæ”¯æ´å¤šç¯€é»æ“´å±•
- âœ… èˆ‡è‡ªå‹•æ··åˆç²¾åº¦ (AMP) æ›´å¥½æ•´åˆ

## ğŸ› ï¸ ç’°å¢ƒæº–å‚™

### ç¡¬é«”éœ€æ±‚
- **GPU**: è‡³å°‘ 2 å¼µ GPUï¼ˆå»ºè­° 4 å¼µä»¥ä¸Šä»¥è§€å¯Ÿæ˜é¡¯å·®ç•°ï¼‰
- **è¨˜æ†¶é«”**: æ¯å¼µ GPU è‡³å°‘ 8GB VRAM
- **ç¶²è·¯**: å–®ç¯€é»å…§ GPUs é–“é«˜é€Ÿäº’é€£ï¼ˆPCIe/NVLinkï¼‰

### è»Ÿé«”ä¾è³´
```bash
# æ¿€æ´» Poetry ç’°å¢ƒ
cd 00-Course_Setup
source .venv/bin/activate

# æª¢æŸ¥å¿…è¦å¥—ä»¶
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU Count: {torch.cuda.device_count()}')"
```

## ğŸ”¬ å¯¦é©—æµç¨‹

### æ­¥é©Ÿ 1: ç’°å¢ƒæª¢æŸ¥èˆ‡åŸºæº–æ¸¬è©¦
```bash
python 01_environment_check.py
python 02_baseline_single_gpu.py --model resnet50 --batch-size 64 --epochs 5
```

### æ­¥é©Ÿ 2: DataParallel å¯¦é©—
```bash
python 03_dataparallel_training.py --model resnet50 --batch-size 256 --epochs 5 --gpus 0,1,2,3
```

### æ­¥é©Ÿ 3: DistributedDataParallel å¯¦é©—
```bash
torchrun --nproc_per_node=4 04_ddp_training.py --model resnet50 --batch-size 64 --epochs 5
```

### æ­¥é©Ÿ 4: æ€§èƒ½å°æ¯”åˆ†æ
```bash
python 05_performance_comparison.py --results-dir ./results --generate-plots --output-report comparison_report.html
```

## ğŸ“Š é æœŸå¯¦é©—çµæœ (4 x RTX 4090)

| è¨“ç·´æ–¹æ³• | ç¸½æ‰¹æ¬¡å¤§å° | è¨“ç·´æ™‚é–“/Epoch | GPU è¨˜æ†¶é«” | ååé‡ | æ“´å±•æ•ˆç‡ |
|---------|------------|---------------|----------|--------|---------|
| å–® GPU | 64 | 120.5s | 3.2GB | 425 samples/s | - |
| DP (4 GPU) | 256 | 85.2s | 7.8GB/4.1GB | 1,205 samples/s | 71% |
| DDP (4 GPU) | 256 | 32.1s | 3.4GB | 1,598 samples/s | 94% |

## âš¡ DDP æœ€ä½³å¯¦è¸

```python
# å„ªåŒ–çš„ DDP åˆå§‹åŒ–
model = torch.nn.parallel.DistributedDataParallel(
    model,
    device_ids=[local_rank],
    output_device=local_rank,
    find_unused_parameters=False,    # æ€§èƒ½å„ªåŒ–
    gradient_as_bucket_view=True,    # æ¸›å°‘è¨˜æ†¶é«”è¤‡è£½
    bucket_cap_mb=25,               # èª¿æ•´é€šä¿¡æ¡¶å¤§å°
    broadcast_buffers=False,        # ä¸åŒæ­¥ buffer
)

# DDP + æ··åˆç²¾åº¦
from torch.cuda.amp import autocast, GradScaler
model = DDP(model, device_ids=[local_rank])
scaler = GradScaler()

for batch in train_loader:
    optimizer.zero_grad()
    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

## ğŸ› å¸¸è¦‹å•é¡Œè§£æ±º

### NCCL é€šä¿¡è¶…æ™‚
```bash
export NCCL_DEBUG=INFO
export NCCL_BLOCKING_WAIT=1
export NCCL_ASYNC_ERROR_HANDLING=1
```

### ç«¯å£è¡çª
```bash
export MASTER_PORT=$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1]); s.close()')
```

### GPU è¨˜æ†¶é«”ä¸è¶³
```python
# æ¢¯åº¦ç´¯ç©æŠ€è¡“
accumulation_steps = 4
for i, batch in enumerate(train_loader):
    outputs = model(inputs)
    loss = criterion(outputs, targets) / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## ğŸ¯ å­¸ç¿’æª¢æ ¸é»

å®Œæˆæœ¬å¯¦é©—å¾Œï¼Œæ‚¨æ‡‰è©²èƒ½å¤ ï¼š
- [ ] ç†è§£ DataParallel å’Œ DistributedDataParallel çš„æ ¹æœ¬å·®ç•°
- [ ] ç¨ç«‹è¨­ç½®å’Œé‹è¡Œå¤š GPU åˆ†æ•£å¼è¨“ç·´
- [ ] è­˜åˆ¥å’Œè§£æ±ºå¸¸è¦‹çš„åˆ†æ•£å¼è¨“ç·´å•é¡Œ
- [ ] é‡å°ç‰¹å®šç¡¬é«”é…ç½®é¸æ“‡åˆé©çš„ä¸¦è¡Œç­–ç•¥
- [ ] å„ªåŒ–åˆ†æ•£å¼è¨“ç·´çš„æ€§èƒ½å’Œè³‡æºä½¿ç”¨

## ğŸ”— åƒè€ƒè³‡æ–™

- [PyTorch DistributedDataParallel å®˜æ–¹æ–‡æª”](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
- [PyTorch DDP æ•™å­¸](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [NVIDIA æ·±åº¦å­¸ç¿’æ€§èƒ½æŒ‡å—](https://docs.nvidia.com/deeplearning/performance/index.html)

---

**æ­å–œï¼æ‚¨å·²ç¶“æŒæ¡äº†å–®ç¯€é»å¤š GPU åˆ†æ•£å¼è¨“ç·´çš„æ ¸å¿ƒæŠ€è¡“ã€‚æ¥ä¸‹ä¾†å¯ä»¥é€²å…¥ Lab-02 æ¢ç´¢æ›´è¤‡é›œçš„æµæ°´ç·šä¸¦è¡ŒæŠ€è¡“ã€‚**