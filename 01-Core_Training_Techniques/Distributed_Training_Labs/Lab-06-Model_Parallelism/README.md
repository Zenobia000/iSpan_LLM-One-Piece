# Lab 6: 突破單卡限制 - 模型平行 (Model Parallelism)

## 概述

當深度學習模型（尤其是大型語言模型 LLM）的規模大到單張 GPU 无法容纳时，**模型平行 (Model Parallelism)** 成为一种必需的训练策略。本章節將深入探討模型平行的核心思想、主要實現方式（層間模型平行和張量平行），以及在 PyTorch 中的實作方法。

![模型平行示意圖](https://pic1.zhimg.com/v2-98b31a1964f5188f61536b139af558f4_1440w.jpg)

---

## 1. 為什麼需要模型平行？

**數據平行 (Data Parallelism)** 雖然能加速訓練，但它有一个前提：**每个 GPU 都必须能完整地载入整个模型**。随着模型参数量从数亿增长到数千亿，这个前提不再成立。

**模型平行的核心動機**：将单个过于庞大的模型**切分**到多个 GPU 上，使得每个 GPU 只负责模型的一部分计算和存储。

### 1.1 模型平行的主要挑战

- **通信开销**：不同 GPU 上的模型部分之间需要频繁交换激活值 (activations)，这会引入巨大的通信开销。
- **设备空闲 (Bubble)**：在前向和后向传播过程中，由于计算依赖关系，某些 GPU 可能会处于空闲等待状态，导致硬件利用率降低。
- **实现复杂**：相比数据平行，模型平行的实现更为复杂，通常需要对模型结构进行手动修改。

---

## 2. 層間模型平行 (Inter-layer Model Parallelism)

### 2.1 核心思想

这是最直观的模型平行方式，也称为 **流水线平行 (Pipeline Parallelism)**。其思想是**按顺序将模型的不同层放置在不同的 GPU 上**。

**工作流程**：
1.  **切分**：将模型（如一个 4 层的 Transformer）切分，例如，第 1-2 层放在 GPU 0，第 3-4 层放在 GPU 1。
2.  **前向传播**：
    -   数据首先在 GPU 0 上通过第 1-2 层。
    -   第 2 层的输出（激活值）被传输到 GPU 1。
    -   GPU 1 接收到数据后，继续通过第 3-4 层的计算。
3.  **后向传播**：过程相反，梯度从 GPU 1 传回 GPU 0。

### 2.2 流水线气泡 (Pipeline Bubble)

简单的流水线会产生大量的设备空闲时间，即“气泡”。例如，在 GPU 0 计算时，GPU 1 处于空闲；在 GPU 1 计算时，GPU 0 又处于空闲。

![流水线气泡示意图](https://pic3.zhimg.com/v2-b7e671231f28682a39223788a1077759_1440w.jpg)

### 2.3 GPipe 和 PipeDream：优化流水线

为了减少气泡，研究人员提出了 GPipe 和 PipeDream 等优化策略。
- **核心思想**：将一个大的批次 (Batch) 切分成多个微批次 (Micro-batches)，让 GPU 以流水线的方式处理这些微批次，从而让不同 GPU 的计算能够重叠起来。
- **GPipe**：在前向传播完成后，进行一次性的后向传播。
- **PipeDream (1F1B)**：采用“一个前向，一个后向” (1F1B) 的调度策略，进一步减少了设备空闲时间。

---

## 3. 張量平行 (Tensor Parallelism)

### 3.1 核心思想

張量平行，也称为 **层内模型平行 (Intra-layer Model Parallelism)**，是一种更细粒度的并行策略。它不是切分模型的层，而是**将单层内的巨大权重矩阵（如 `nn.Linear`）切分到多个 GPU 上**。

**工作流程 (以一个线性层为例)**：
假设一个线性层 `Y = XA`，其中 `A` 是一个巨大的权重矩阵。
1.  **按列切分**：将矩阵 `A` 按列切分成 `[A1, A2]`，分别存放在 GPU 0 和 GPU 1 上。
    -   GPU 0 计算 `Y1 = X * A1`。
    -   GPU 1 计算 `Y2 = X * A2`。
    -   最后，通过 `All-Gather` 操作将 `Y1` 和 `Y2` 拼接成完整的 `Y = [Y1, Y2]`。

2.  **按行切分**：将矩阵 `A` 按行切分，分别存放在 GPU 0 和 GPU 1 上。
    -   输入 `X` 也需要相应地切分。
    -   每个 GPU 计算一部分结果，最后通过 `All-Reduce` 操作将结果相加得到最终的 `Y`。

### 3.2 Megatron-LM

NVIDIA 的 Megatron-LM 项目是張量平行的杰出代表，它巧妙地将 Transformer 中的 `Self-Attention` 和 `MLP` 模块进行了切分，实现了高效的层内并行。
- **MLP 模块**：结合了按列切分和按行切分。
- **Attention 模块**：将 Q, K, V 的投影矩阵切分到多个 GPU 上。

![張量平行示意圖](https://pic1.zhimg.com/v2-53a5c2d7667f0237587822989c93f0b2_1440w.jpg)

---

## 4. 3D 平行：數據、張量與流水线的结合

为了训练最大规模的模型（如千亿、兆级参数），通常会将多种并行策略结合起来，形成所谓的 **3D 平行**：
1.  **數據平行 (Data Parallelism)**：在多个数据副本上并行处理数据，以加速训练。
2.  **張量平行 (Tensor Parallelism)**：在单个节点内部，使用張量平行将模型层切分到多张 GPU 上。
3.  **流水线平行 (Pipeline Parallelism)**：在多个节点之间，使用流水线平行将模型的不同阶段分配到不同的节点上。

**典型配置**：
- 一个拥有 8 张 GPU 的节点内部采用 **8 路張量平行**。
- 8 个这样的节点组成一个流水线阶段，采用 **8 路流水线平行**。
- 多个这样的流水线副本再进行**数据平行**。

---

## 5. 結論與實踐建議

- **模型大小是决定因素**：当模型无法放入单张 GPU 时，必须使用模型平行。
- **張量平行优先**：在单机多卡的环境下，張量平行通常比流水线平行更高效，因为它产生的“气泡”更少。
- **流水线平行用于跨节点**：流水线平行更适合用于跨越多台机器的超大規模模型训练，因为它的通信开销相对固定。
- **混合使用是趋势**：最先进的大模型训练框架（如 Megatron-LM, DeepSpeed, Colossal-AI）都支持将数据平行、張量平行和流水线平行结合使用。
- **框架是关键**：由于模型平行实现复杂，强烈建议使用成熟的开源框架，而不是从零开始实现。

理解不同模型平行策略的原理和优缺点，是您迈向大规模模型训练领域的重要一步。
