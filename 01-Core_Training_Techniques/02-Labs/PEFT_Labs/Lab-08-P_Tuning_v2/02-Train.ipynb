{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 8: P-Tuning v2 - Deep Prompt Tuning for Diverse Tasks\n",
        "---\n",
        "## Notebook 2: The Training Process\n",
        "\n",
        "**Goal:** In this notebook, you will fine-tune a model using **P-Tuning v2** on both understanding and generation tasks. P-Tuning v2 applies deep prompts to every transformer layer, providing universal effectiveness across tasks and scales.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Dataset and Preprocess\n",
        "\n",
        "We will use both classification and generation tasks to demonstrate P-Tuning v2's versatility. Let's start with the GLUE CoLA dataset for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load dataset - CoLA for linguistic acceptability classification\n",
        "dataset = load_dataset(\"glue\", \"cola\", split=\"train[:1000]\")\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Load tokenizer\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"âœ… Dataset loaded and preprocessed.\")\n",
        "print(f\"Train samples: {len(tokenized_datasets['train'])}\")\n",
        "print(f\"Test samples: {len(tokenized_datasets['test'])}\")\n",
        "print(tokenized_datasets[\"train\"][0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load the Base Model\n",
        "\n",
        "Next, we load the BERT model for sequence classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, \n",
        "    num_labels=2  # Binary classification for CoLA\n",
        ")\n",
        "\n",
        "print(\"âœ… Base BERT model loaded.\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Configure P-Tuning v2\n",
        "\n",
        "Now we configure P-Tuning v2. Unlike P-Tuning v1, P-Tuning v2 applies prompts to every transformer layer, providing deeper integration and better performance.\n",
        "\n",
        "#### Key Differences from P-Tuning v1:\n",
        "-   **Deep Prompts**: Prompts are added to every transformer layer, not just the input.\n",
        "-   **No MLP Encoder**: P-Tuning v2 uses direct prompt optimization without MLP.\n",
        "-   **Universal Effectiveness**: Works well across various tasks and model scales.\n",
        "-   **Simplified Design**: Easier to implement and more stable training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_model, PromptTuningConfig, TaskType\n",
        "\n",
        "# --- P-Tuning v2 Configuration ---\n",
        "# P-Tuning v2 uses PromptTuningConfig with specific settings for deep prompts\n",
        "ptuning_v2_config = PromptTuningConfig(\n",
        "    task_type=TaskType.SEQ_CLS,           # Sequence classification task\n",
        "    prompt_tuning_init=\"RANDOM\",          # Random initialization\n",
        "    num_virtual_tokens=100,               # Longer prompt sequence for P-Tuning v2\n",
        "    prompt_tuning_init_text=None,         # No text initialization\n",
        "    tokenizer_name_or_path=model_checkpoint\n",
        ")\n",
        "\n",
        "# --- Create PeftModel ---\n",
        "peft_model = get_peft_model(model, ptuning_v2_config)\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Set Up Training\n",
        "\n",
        "The final step is to configure and run the training process using the `transformers.Trainer`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
        "        \"matthews_correlation\": matthews_corrcoef(labels, predictions)\n",
        "    }\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-ptuning-v2-cola\",\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,                    # Higher learning rate for PEFT\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"matthews_correlation\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "# --- Data Collator ---\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# --- Create Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"ðŸš€ Starting training with P-Tuning v2...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
