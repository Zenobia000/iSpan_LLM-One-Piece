{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5.1 - Kubernetes åŸç”Ÿéƒ¨ç½²\n",
    "\n",
    "## ğŸ¯ å¯¦é©—ç›®æ¨™\n",
    "\n",
    "æœ¬å¯¦é©—å°‡æ•™æ‚¨å¦‚ä½•ï¼š\n",
    "1. è¨­è¨ˆé›²åŸç”Ÿ Triton éƒ¨ç½²æ¶æ§‹\n",
    "2. å¯¦ç¾ GPU è³‡æºèª¿åº¦èˆ‡ç®¡ç†\n",
    "3. é…ç½®æœå‹™ç™¼ç¾èˆ‡è² è¼‰å‡è¡¡\n",
    "4. å»ºç«‹è‡ªå‹•æ“´ç¸®å®¹æ©Ÿåˆ¶\n",
    "5. å¯¦æ–½æ»¾å‹•æ›´æ–°èˆ‡å›æ»¾ç­–ç•¥\n",
    "\n",
    "## ğŸ“‹ å‰ç½®éœ€æ±‚\n",
    "\n",
    "- Kubernetes å¢é›† (v1.24+)\n",
    "- NVIDIA GPU Operator æˆ– Device Plugin\n",
    "- Helm 3.x\n",
    "- kubectl å·²é…ç½®\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ç†è«–èƒŒæ™¯\n",
    "\n",
    "### Kubernetes ä¸­çš„ AI æ¨ç†æŒ‘æˆ°\n",
    "\n",
    "**1. GPU è³‡æºç®¡ç†**\n",
    "- GPU æ˜¯ç¨€ç¼ºä¸”æ˜‚è²´çš„è¨ˆç®—è³‡æº\n",
    "- éœ€è¦ç²¾ç¢ºçš„è³‡æºèª¿åº¦å’Œåˆ†é…\n",
    "- æ”¯æ´å¤šç§Ÿæˆ¶å’Œè³‡æºéš”é›¢\n",
    "\n",
    "**2. æ¨¡å‹ç”Ÿå‘½é€±æœŸç®¡ç†**\n",
    "- æ¨¡å‹æ–‡ä»¶é€šå¸¸é«”ç©é¾å¤§ (æ•¸ GB åˆ°æ•¸å GB)\n",
    "- éœ€è¦ç‰ˆæœ¬æ§åˆ¶å’Œå¿«é€Ÿéƒ¨ç½²\n",
    "- æ”¯æ´ A/B æ¸¬è©¦å’Œæ¼¸é€²å¼ç™¼å¸ƒ\n",
    "\n",
    "**3. æœå‹™é«˜å¯ç”¨æ€§**\n",
    "- é›¶åœæ©Ÿæ™‚é–“çš„æ¨¡å‹æ›´æ–°\n",
    "- è‡ªå‹•æ•…éšœè½‰ç§»å’Œæ¢å¾©\n",
    "- å½ˆæ€§æ“´ç¸®å®¹æ‡‰å°æµé‡è®ŠåŒ–\n",
    "\n",
    "### é›²åŸç”Ÿ AI æ¨ç†æ¶æ§‹\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Ingress Controller                       â”‚\n",
    "â”‚                  (NGINX/Traefik/Istio)                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚            Service Mesh (Optional)          â”‚\n",
    "      â”‚              (Istio/Linkerd)                â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              LoadBalancer Service             â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚                                 â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   Pod-1    â”‚                   â”‚   Pod-2    â”‚\n",
    "    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "    â”‚ â”‚ Triton â”‚ â”‚                   â”‚ â”‚ Triton â”‚ â”‚\n",
    "    â”‚ â”‚ Server â”‚ â”‚                   â”‚ â”‚ Server â”‚ â”‚\n",
    "    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "    â”‚ â”‚  GPU   â”‚ â”‚                   â”‚ â”‚  GPU   â”‚ â”‚\n",
    "    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚                                 â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚           Persistent Volume (Models)          â”‚\n",
    "    â”‚              (NFS/GlusterFS/S3)               â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "print(f\"Environment ready at {datetime.now()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­ç½® Kubernetes éƒ¨ç½²ç’°å¢ƒ\n",
    "K8S_DIR = \"/tmp/triton-k8s\"\n",
    "MANIFESTS_DIR = f\"{K8S_DIR}/manifests\"\n",
    "HELM_DIR = f\"{K8S_DIR}/helm\"\n",
    "CONFIGS_DIR = f\"{K8S_DIR}/configs\"\n",
    "\n",
    "# å‰µå»ºç›®éŒ„çµæ§‹\n",
    "directories = [\n",
    "    MANIFESTS_DIR,\n",
    "    HELM_DIR,\n",
    "    CONFIGS_DIR,\n",
    "    f\"{HELM_DIR}/triton-inference\",\n",
    "    f\"{HELM_DIR}/triton-inference/templates\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ Kubernetes éƒ¨ç½²ç›®éŒ„çµæ§‹:\")\n",
    "for directory in directories:\n",
    "    print(f\"   {directory}\")\n",
    "\n",
    "# å…¨åŸŸé…ç½®\n",
    "NAMESPACE = \"triton-inference\"\n",
    "APP_NAME = \"triton-server\"\n",
    "IMAGE = \"nvcr.io/nvidia/tritonserver:24.10-py3\"\n",
    "MODEL_REPOSITORY = \"/models\"\n",
    "\n",
    "print(f\"\\nâš™ï¸  éƒ¨ç½²é…ç½®:\")\n",
    "print(f\"   å‘½åç©ºé–“: {NAMESPACE}\")\n",
    "print(f\"   æ‡‰ç”¨åç¨±: {APP_NAME}\")\n",
    "print(f\"   å®¹å™¨æ˜ åƒ: {IMAGE}\")\n",
    "print(f\"   æ¨¡å‹å€‰åº«: {MODEL_REPOSITORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 1ï¼šé›†ç¾¤ç’°å¢ƒé©—è­‰èˆ‡æº–å‚™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Kubernetes é›†ç¾¤æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_kubernetes_cluster():\n",
    "    \"\"\"æª¢æŸ¥ Kubernetes é›†ç¾¤ç‹€æ…‹\"\"\"\n",
    "    try:\n",
    "        # è¼‰å…¥ kubeconfig\n",
    "        config.load_kube_config()\n",
    "        v1 = client.CoreV1Api()\n",
    "        \n",
    "        # æª¢æŸ¥ç¯€é»ç‹€æ…‹\n",
    "        print(\"ğŸ” æª¢æŸ¥é›†ç¾¤ç¯€é»:\")\n",
    "        nodes = v1.list_node()\n",
    "        for node in nodes.items:\n",
    "            name = node.metadata.name\n",
    "            status = \"Ready\" if any(condition.type == \"Ready\" and condition.status == \"True\" \n",
    "                                  for condition in node.status.conditions) else \"NotReady\"\n",
    "            \n",
    "            # æª¢æŸ¥ GPU è³‡æº\n",
    "            gpu_capacity = node.status.capacity.get('nvidia.com/gpu', '0')\n",
    "            gpu_allocatable = node.status.allocatable.get('nvidia.com/gpu', '0')\n",
    "            \n",
    "            print(f\"   ç¯€é»: {name}\")\n",
    "            print(f\"   ç‹€æ…‹: {status}\")\n",
    "            print(f\"   GPU å®¹é‡: {gpu_capacity}\")\n",
    "            print(f\"   GPU å¯åˆ†é…: {gpu_allocatable}\")\n",
    "            print()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é›†ç¾¤æª¢æŸ¥å¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "cluster_ready = check_kubernetes_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GPU Operator é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_operator():\n",
    "    \"\"\"æª¢æŸ¥ NVIDIA GPU Operator ç‹€æ…‹\"\"\"\n",
    "    try:\n",
    "        # æª¢æŸ¥ GPU Operator namespace\n",
    "        v1 = client.CoreV1Api()\n",
    "        apps_v1 = client.AppsV1Api()\n",
    "        \n",
    "        print(\"ğŸ” æª¢æŸ¥ GPU Operator:\")\n",
    "        \n",
    "        # æª¢æŸ¥ gpu-operator namespace\n",
    "        try:\n",
    "            namespace = v1.read_namespace(name=\"gpu-operator\")\n",
    "            print(f\"   âœ… GPU Operator namespace å­˜åœ¨\")\n",
    "        except ApiException as e:\n",
    "            if e.status == 404:\n",
    "                print(f\"   âš ï¸  GPU Operator namespace ä¸å­˜åœ¨\")\n",
    "                return False\n",
    "        \n",
    "        # æª¢æŸ¥ DaemonSet\n",
    "        daemonsets = apps_v1.list_namespaced_daemon_set(namespace=\"gpu-operator\")\n",
    "        for ds in daemonsets.items:\n",
    "            name = ds.metadata.name\n",
    "            desired = ds.status.desired_number_scheduled or 0\n",
    "            ready = ds.status.number_ready or 0\n",
    "            print(f\"   DaemonSet: {name} ({ready}/{desired} ready)\")\n",
    "        \n",
    "        # æª¢æŸ¥ Device Plugin\n",
    "        pods = v1.list_namespaced_pod(namespace=\"gpu-operator\", \n",
    "                                     label_selector=\"app=nvidia-device-plugin-daemonset\")\n",
    "        \n",
    "        if pods.items:\n",
    "            print(f\"   âœ… NVIDIA Device Plugin é‹è¡Œä¸­ ({len(pods.items)} pods)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   âš ï¸  NVIDIA Device Plugin æœªæ‰¾åˆ°\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPU Operator æª¢æŸ¥å¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "gpu_ready = check_gpu_operator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 å‰µå»ºå‘½åç©ºé–“å’ŒåŸºç¤è³‡æº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºå‘½åç©ºé–“ YAML\n",
    "namespace_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: {NAMESPACE}\n",
    "  labels:\n",
    "    name: {NAMESPACE}\n",
    "    purpose: ai-inference\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ResourceQuota\n",
    "metadata:\n",
    "  name: triton-quota\n",
    "  namespace: {NAMESPACE}\n",
    "spec:\n",
    "  hard:\n",
    "    requests.cpu: \"8\"\n",
    "    requests.memory: 32Gi\n",
    "    requests.nvidia.com/gpu: \"4\"\n",
    "    limits.cpu: \"16\"\n",
    "    limits.memory: 64Gi\n",
    "    limits.nvidia.com/gpu: \"4\"\n",
    "    persistentvolumeclaims: \"2\"\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/namespace.yaml\", \"w\") as f:\n",
    "    f.write(namespace_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»ºå‘½åç©ºé–“é…ç½®\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/namespace.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kubernetes_manifest(manifest_path: str) -> bool:\n",
    "    \"\"\"æ‡‰ç”¨ Kubernetes manifest\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"kubectl\", \"apply\", \"-f\", manifest_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"âœ… æˆåŠŸæ‡‰ç”¨: {manifest_path}\")\n",
    "        print(f\"   è¼¸å‡º: {result.stdout.strip()}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ æ‡‰ç”¨å¤±æ•—: {manifest_path}\")\n",
    "        print(f\"   éŒ¯èª¤: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "# æ‡‰ç”¨å‘½åç©ºé–“é…ç½®\n",
    "if cluster_ready:\n",
    "    apply_kubernetes_manifest(f\"{MANIFESTS_DIR}/namespace.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 2ï¼šæ¨¡å‹å„²å­˜èˆ‡ PV é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 è¨­è¨ˆæ¨¡å‹å„²å­˜ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PersistentVolume å’Œ PersistentVolumeClaim é…ç½®\n",
    "storage_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: triton-models-pv\n",
    "  labels:\n",
    "    type: models\n",
    "    app: triton-server\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 100Gi\n",
    "  accessModes:\n",
    "    - ReadOnlyMany\n",
    "  persistentVolumeReclaimPolicy: Retain\n",
    "  storageClassName: models-storage\n",
    "  hostPath:\n",
    "    path: /data/triton/models\n",
    "    type: DirectoryOrCreate\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: triton-models-pvc\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: storage\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadOnlyMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 100Gi\n",
    "  storageClassName: models-storage\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: models\n",
    "      app: triton-server\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: triton-cache-pv\n",
    "  labels:\n",
    "    type: cache\n",
    "    app: triton-server\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 50Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  persistentVolumeReclaimPolicy: Delete\n",
    "  storageClassName: cache-storage\n",
    "  hostPath:\n",
    "    path: /data/triton/cache\n",
    "    type: DirectoryOrCreate\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: triton-cache-pvc\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: cache\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 50Gi\n",
    "  storageClassName: cache-storage\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: cache\n",
    "      app: triton-server\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/storage.yaml\", \"w\") as f:\n",
    "    f.write(storage_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»ºå„²å­˜é…ç½®\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/storage.yaml\")\n",
    "print(\"\\nğŸ’¾ å„²å­˜ç­–ç•¥:\")\n",
    "print(\"   - æ¨¡å‹å„²å­˜: 100Gi (ReadOnlyMany)\")\n",
    "print(\"   - å¿«å–å„²å­˜: 50Gi (ReadWriteMany)\")\n",
    "print(\"   - å›æ”¶ç­–ç•¥: Retain (models), Delete (cache)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ConfigMap é…ç½®ç®¡ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfigMap ç”¨æ–¼ Triton é…ç½®\n",
    "configmap_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: triton-config\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: config\n",
    "data:\n",
    "  # Triton æœå‹™å™¨é…ç½®\n",
    "  triton-config.json: |\n",
    "    {{\n",
    "      \"backend_config\": {{\n",
    "        \"pytorch\": {{\n",
    "          \"cmdline\": {{\n",
    "            \"auto-complete-config\": \"true\",\n",
    "            \"backend-directory\": \"/opt/tritonserver/backends\",\n",
    "            \"min-compute-capability\": \"6.0\"\n",
    "          }}\n",
    "        }}\n",
    "      }},\n",
    "      \"model_config_name\": \"config.pbtxt\",\n",
    "      \"log_level\": 1,\n",
    "      \"log_verbose\": 1,\n",
    "      \"metrics\": {{\n",
    "        \"allow_metrics\": true,\n",
    "        \"allow_gpu_metrics\": true,\n",
    "        \"allow_cpu_metrics\": true,\n",
    "        \"metrics_interval_ms\": 1000\n",
    "      }}\n",
    "    }}\n",
    "  \n",
    "  # å¥åº·æª¢æŸ¥è…³æœ¬\n",
    "  health-check.sh: |\n",
    "    #!/bin/bash\n",
    "    set -e\n",
    "    \n",
    "    # æª¢æŸ¥ Triton æœå‹™å™¨ç‹€æ…‹\n",
    "    curl -f http://localhost:8000/v2/health/ready || exit 1\n",
    "    \n",
    "    # æª¢æŸ¥æ¨¡å‹ç‹€æ…‹\n",
    "    MODEL_COUNT=$(curl -s http://localhost:8000/v2/models | jq '.models | length')\n",
    "    if [ \"$MODEL_COUNT\" -eq 0 ]; then\n",
    "      echo \"No models loaded\"\n",
    "      exit 1\n",
    "    fi\n",
    "    \n",
    "    echo \"Health check passed: $MODEL_COUNT models loaded\"\n",
    "  \n",
    "  # æ¨¡å‹è¼‰å…¥è…³æœ¬\n",
    "  model-loader.sh: |\n",
    "    #!/bin/bash\n",
    "    set -e\n",
    "    \n",
    "    echo \"Starting model loader...\"\n",
    "    \n",
    "    # ç­‰å¾…æ¨¡å‹å„²å­˜æ›è¼‰\n",
    "    while [ ! -d \"{MODEL_REPOSITORY}\" ]; do\n",
    "      echo \"Waiting for model repository to be mounted...\"\n",
    "      sleep 5\n",
    "    done\n",
    "    \n",
    "    # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶\n",
    "    MODEL_COUNT=$(find {MODEL_REPOSITORY} -name \"config.pbtxt\" | wc -l)\n",
    "    echo \"Found $MODEL_COUNT models in repository\"\n",
    "    \n",
    "    if [ \"$MODEL_COUNT\" -eq 0 ]; then\n",
    "      echo \"Warning: No models found in repository\"\n",
    "    fi\n",
    "    \n",
    "    echo \"Model loader completed\"\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: triton-secrets\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: security\n",
    "type: Opaque\n",
    "data:\n",
    "  # Base64 ç·¨ç¢¼çš„æ†‘è­‰ (ç¯„ä¾‹)\n",
    "  model-access-key: bW9kZWwtYWNjZXNzLWtleQ==\n",
    "  registry-token: cmVnaXN0cnktdG9rZW4=\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/configmap.yaml\", \"w\") as f:\n",
    "    f.write(configmap_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»ºé…ç½®ç®¡ç†\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/configmap.yaml\")\n",
    "print(\"\\nâš™ï¸  é…ç½®å…§å®¹:\")\n",
    "print(\"   - Triton æœå‹™å™¨é…ç½®\")\n",
    "print(\"   - å¥åº·æª¢æŸ¥è…³æœ¬\")\n",
    "print(\"   - æ¨¡å‹è¼‰å…¥è…³æœ¬\")\n",
    "print(\"   - å®‰å…¨æ†‘è­‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 3ï¼šTriton Server Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 è¨­è¨ˆé«˜å¯ç”¨éƒ¨ç½²é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triton Server Deployment\n",
    "deployment_yaml = f\"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: {APP_NAME}\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    version: v1\n",
    "    component: inference-server\n",
    "spec:\n",
    "  replicas: 2\n",
    "  strategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxSurge: 1\n",
    "      maxUnavailable: 0\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: {APP_NAME}\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: {APP_NAME}\n",
    "        version: v1\n",
    "        component: inference-server\n",
    "      annotations:\n",
    "        prometheus.io/scrape: \"true\"\n",
    "        prometheus.io/port: \"8002\"\n",
    "        prometheus.io/path: \"/metrics\"\n",
    "    spec:\n",
    "      serviceAccountName: triton-service-account\n",
    "      securityContext:\n",
    "        runAsNonRoot: true\n",
    "        runAsUser: 1000\n",
    "        fsGroup: 1000\n",
    "      \n",
    "      # ç¯€é»é¸æ“‡å™¨ - é¸æ“‡æœ‰ GPU çš„ç¯€é»\n",
    "      nodeSelector:\n",
    "        accelerator: nvidia-tesla-gpu\n",
    "      \n",
    "      # å®¹å¿åº¦ - å…è¨±åœ¨æœ‰ GPU æ±¡é»çš„ç¯€é»ä¸Šèª¿åº¦\n",
    "      tolerations:\n",
    "      - key: nvidia.com/gpu\n",
    "        operator: Exists\n",
    "        effect: NoSchedule\n",
    "      \n",
    "      # åè¦ªå’Œæ€§ - é¿å…å¤šå€‹ Pod èª¿åº¦åˆ°åŒä¸€ç¯€é»\n",
    "      affinity:\n",
    "        podAntiAffinity:\n",
    "          preferredDuringSchedulingIgnoredDuringExecution:\n",
    "          - weight: 100\n",
    "            podAffinityTerm:\n",
    "              labelSelector:\n",
    "                matchLabels:\n",
    "                  app: {APP_NAME}\n",
    "              topologyKey: kubernetes.io/hostname\n",
    "      \n",
    "      # Init Container - æ¨¡å‹é è¼‰å…¥\n",
    "      initContainers:\n",
    "      - name: model-loader\n",
    "        image: curlimages/curl:latest\n",
    "        command: [\"/bin/sh\"]\n",
    "        args: [\"/scripts/model-loader.sh\"]\n",
    "        volumeMounts:\n",
    "        - name: triton-models\n",
    "          mountPath: {MODEL_REPOSITORY}\n",
    "          readOnly: true\n",
    "        - name: triton-config\n",
    "          mountPath: /scripts\n",
    "          readOnly: true\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 128Mi\n",
    "          limits:\n",
    "            cpu: 500m\n",
    "            memory: 512Mi\n",
    "      \n",
    "      containers:\n",
    "      - name: triton-server\n",
    "        image: {IMAGE}\n",
    "        imagePullPolicy: IfNotPresent\n",
    "        \n",
    "        command: [\"tritonserver\"]\n",
    "        args:\n",
    "        - --model-repository={MODEL_REPOSITORY}\n",
    "        - --allow-http=true\n",
    "        - --allow-grpc=true\n",
    "        - --allow-metrics=true\n",
    "        - --allow-gpu-metrics=true\n",
    "        - --strict-model-config=false\n",
    "        - --strict-readiness=false\n",
    "        - --http-port=8000\n",
    "        - --grpc-port=8001\n",
    "        - --metrics-port=8002\n",
    "        - --log-verbose=1\n",
    "        - --model-control-mode=poll\n",
    "        - --repository-poll-secs=30\n",
    "        \n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "          name: http\n",
    "          protocol: TCP\n",
    "        - containerPort: 8001\n",
    "          name: grpc\n",
    "          protocol: TCP\n",
    "        - containerPort: 8002\n",
    "          name: metrics\n",
    "          protocol: TCP\n",
    "        \n",
    "        # è³‡æºè«‹æ±‚å’Œé™åˆ¶\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 2\n",
    "            memory: 8Gi\n",
    "            nvidia.com/gpu: 1\n",
    "          limits:\n",
    "            cpu: 4\n",
    "            memory: 16Gi\n",
    "            nvidia.com/gpu: 1\n",
    "        \n",
    "        # å¥åº·æª¢æŸ¥\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /v2/health/live\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 30\n",
    "          timeoutSeconds: 10\n",
    "          failureThreshold: 3\n",
    "        \n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /v2/health/ready\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 15\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          failureThreshold: 3\n",
    "        \n",
    "        startupProbe:\n",
    "          httpGet:\n",
    "            path: /v2/health/ready\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          failureThreshold: 30\n",
    "        \n",
    "        # ç’°å¢ƒè®Šæ•¸\n",
    "        env:\n",
    "        - name: CUDA_VISIBLE_DEVICES\n",
    "          value: \"0\"\n",
    "        - name: TRITON_MODEL_REPOSITORY\n",
    "          value: {MODEL_REPOSITORY}\n",
    "        - name: NVIDIA_VISIBLE_DEVICES\n",
    "          value: all\n",
    "        - name: NVIDIA_DRIVER_CAPABILITIES\n",
    "          value: compute,utility\n",
    "        \n",
    "        # æ›è¼‰é»\n",
    "        volumeMounts:\n",
    "        - name: triton-models\n",
    "          mountPath: {MODEL_REPOSITORY}\n",
    "          readOnly: true\n",
    "        - name: triton-cache\n",
    "          mountPath: /opt/tritonserver/cache\n",
    "        - name: triton-config\n",
    "          mountPath: /opt/tritonserver/config\n",
    "          readOnly: true\n",
    "        - name: triton-secrets\n",
    "          mountPath: /opt/tritonserver/secrets\n",
    "          readOnly: true\n",
    "        - name: shm\n",
    "          mountPath: /dev/shm\n",
    "        \n",
    "        # å®‰å…¨ä¸Šä¸‹æ–‡\n",
    "        securityContext:\n",
    "          allowPrivilegeEscalation: false\n",
    "          readOnlyRootFilesystem: false\n",
    "          capabilities:\n",
    "            drop:\n",
    "            - ALL\n",
    "      \n",
    "      # å­˜å„²å·\n",
    "      volumes:\n",
    "      - name: triton-models\n",
    "        persistentVolumeClaim:\n",
    "          claimName: triton-models-pvc\n",
    "      - name: triton-cache\n",
    "        persistentVolumeClaim:\n",
    "          claimName: triton-cache-pvc\n",
    "      - name: triton-config\n",
    "        configMap:\n",
    "          name: triton-config\n",
    "          defaultMode: 0755\n",
    "      - name: triton-secrets\n",
    "        secret:\n",
    "          secretName: triton-secrets\n",
    "          defaultMode: 0600\n",
    "      - name: shm\n",
    "        emptyDir:\n",
    "          medium: Memory\n",
    "          sizeLimit: 1Gi\n",
    "      \n",
    "      # é‡å•Ÿç­–ç•¥\n",
    "      restartPolicy: Always\n",
    "      terminationGracePeriodSeconds: 30\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/deployment.yaml\", \"w\") as f:\n",
    "    f.write(deployment_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»ºéƒ¨ç½²é…ç½®\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/deployment.yaml\")\n",
    "print(\"\\nğŸš€ éƒ¨ç½²ç‰¹æ€§:\")\n",
    "print(\"   - é«˜å¯ç”¨æ€§: 2 å‰¯æœ¬ + åè¦ªå’Œæ€§\")\n",
    "print(\"   - æ»¾å‹•æ›´æ–°: é›¶åœæ©Ÿéƒ¨ç½²\")\n",
    "print(\"   - GPU èª¿åº¦: ç¯€é»é¸æ“‡å™¨ + å®¹å¿åº¦\")\n",
    "print(\"   - å¥åº·æª¢æŸ¥: å­˜æ´»/å°±ç·’/å•Ÿå‹•æ¢é‡\")\n",
    "print(\"   - è³‡æºç®¡ç†: CPU/Memory/GPU é™åˆ¶\")\n",
    "print(\"   - å®‰å…¨æ€§: érootç”¨æˆ¶ + å®‰å…¨ä¸Šä¸‹æ–‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æœå‹™å¸³æˆ¶å’Œ RBAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ServiceAccount å’Œ RBAC é…ç½®\n",
    "rbac_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: triton-service-account\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: security\n",
    "automountServiceAccountToken: true\n",
    "\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: Role\n",
    "metadata:\n",
    "  name: triton-role\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: security\n",
    "rules:\n",
    "# å…è¨±è®€å– ConfigMaps å’Œ Secrets\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"configmaps\", \"secrets\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "\n",
    "# å…è¨±è®€å– PVC ç‹€æ…‹\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"persistentvolumeclaims\"]\n",
    "  verbs: [\"get\", \"list\"]\n",
    "\n",
    "# å…è¨±è®€å–è‡ªå·±çš„ Pod è³‡è¨Š\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"pods\"]\n",
    "  verbs: [\"get\", \"list\"]\n",
    "\n",
    "# å…è¨±å‰µå»º Events\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"events\"]\n",
    "  verbs: [\"create\", \"patch\"]\n",
    "\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: RoleBinding\n",
    "metadata:\n",
    "  name: triton-role-binding\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: security\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: triton-service-account\n",
    "  namespace: {NAMESPACE}\n",
    "roleRef:\n",
    "  kind: Role\n",
    "  name: triton-role\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: triton-monitoring\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: monitoring\n",
    "\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRole\n",
    "metadata:\n",
    "  name: triton-monitoring-cluster-role\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: monitoring\n",
    "rules:\n",
    "# å…è¨±è®€å–ç¯€é»å’Œ Pod æŒ‡æ¨™\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"nodes\", \"nodes/metrics\", \"services\", \"endpoints\", \"pods\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "\n",
    "# å…è¨±è®€å– Deployment å’Œ ReplicaSet\n",
    "- apiGroups: [\"apps\"]\n",
    "  resources: [\"deployments\", \"replicasets\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "\n",
    "# å…è¨±è®€å–æŒ‡æ¨™\n",
    "- apiGroups: [\"metrics.k8s.io\"]\n",
    "  resources: [\"nodes\", \"pods\"]\n",
    "  verbs: [\"get\", \"list\"]\n",
    "\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: triton-monitoring-cluster-role-binding\n",
    "  labels:\n",
    "    app: triton-server\n",
    "    component: monitoring\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: triton-monitoring\n",
    "  namespace: {NAMESPACE}\n",
    "roleRef:\n",
    "  kind: ClusterRole\n",
    "  name: triton-monitoring-cluster-role\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/rbac.yaml\", \"w\") as f:\n",
    "    f.write(rbac_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»º RBAC é…ç½®\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/rbac.yaml\")\n",
    "print(\"\\nğŸ” å®‰å…¨é…ç½®:\")\n",
    "print(\"   - æœå‹™å¸³æˆ¶: triton-service-account\")\n",
    "print(\"   - æœ€å°æ¬Šé™åŸå‰‡: åƒ…å¿…è¦çš„è³‡æºè¨ªå•\")\n",
    "print(\"   - ç›£æ§æ¬Šé™: ç¨ç«‹çš„ç›£æ§æœå‹™å¸³æˆ¶\")\n",
    "print(\"   - å‘½åç©ºé–“éš”é›¢: æ¬Šé™é™åˆ¶åœ¨å‘½åç©ºé–“å…§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 4ï¼šæœå‹™ç™¼ç¾èˆ‡è² è¼‰å‡è¡¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 è¨­è¨ˆæœå‹™å±¤ç´š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service é…ç½®\n",
    "service_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: {APP_NAME}-service\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: load-balancer\n",
    "  annotations:\n",
    "    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n",
    "    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"tcp\"\n",
    "    prometheus.io/scrape: \"true\"\n",
    "    prometheus.io/port: \"8002\"\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: {APP_NAME}\n",
    "  ports:\n",
    "  - name: http\n",
    "    port: 8000\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "  - name: grpc\n",
    "    port: 8001\n",
    "    targetPort: 8001\n",
    "    protocol: TCP\n",
    "  - name: metrics\n",
    "    port: 8002\n",
    "    targetPort: 8002\n",
    "    protocol: TCP\n",
    "  sessionAffinity: None\n",
    "  loadBalancerSourceRanges:\n",
    "  - 10.0.0.0/8\n",
    "  - 172.16.0.0/12\n",
    "  - 192.168.0.0/16\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: {APP_NAME}-headless\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: discovery\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  clusterIP: None\n",
    "  selector:\n",
    "    app: {APP_NAME}\n",
    "  ports:\n",
    "  - name: http\n",
    "    port: 8000\n",
    "    targetPort: 8000\n",
    "  - name: grpc\n",
    "    port: 8001\n",
    "    targetPort: 8001\n",
    "  - name: metrics\n",
    "    port: 8002\n",
    "    targetPort: 8002\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: {APP_NAME}-internal\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: internal\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  selector:\n",
    "    app: {APP_NAME}\n",
    "  ports:\n",
    "  - name: http\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  - name: grpc\n",
    "    port: 8001\n",
    "    targetPort: 8001\n",
    "  sessionAffinity: ClientIP\n",
    "  sessionAffinityConfig:\n",
    "    clientIP:\n",
    "      timeoutSeconds: 300\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/service.yaml\", \"w\") as f:\n",
    "    f.write(service_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»ºæœå‹™é…ç½®\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/service.yaml\")\n",
    "print(\"\\nğŸŒ æœå‹™å±¤ç´š:\")\n",
    "print(\"   - LoadBalancer: å¤–éƒ¨è¨ªå• (HTTP/gRPC/Metrics)\")\n",
    "print(\"   - Headless: æœå‹™ç™¼ç¾ (DNS)\")\n",
    "print(\"   - ClusterIP: å…§éƒ¨è¨ªå• (æœƒè©±è¦ªå’Œæ€§)\")\n",
    "print(\"   - å®‰å…¨æ€§: ç§æœ‰ç¶²è·¯ä¾†æºé™åˆ¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ingress é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingress é…ç½®\n",
    "ingress_yaml = f\"\"\"\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: {APP_NAME}-ingress\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: ingress\n",
    "  annotations:\n",
    "    # NGINX Ingress Controller é…ç½®\n",
    "    kubernetes.io/ingress.class: \"nginx\"\n",
    "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
    "    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n",
    "    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n",
    "    \n",
    "    # é€Ÿç‡é™åˆ¶\n",
    "    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n",
    "    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\n",
    "    \n",
    "    # é€£æ¥å’Œè«‹æ±‚è¶…æ™‚\n",
    "    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"60\"\n",
    "    nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n",
    "    nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n",
    "    nginx.ingress.kubernetes.io/proxy-body-size: \"100m\"\n",
    "    \n",
    "    # å¥åº·æª¢æŸ¥\n",
    "    nginx.ingress.kubernetes.io/upstream-healthcheck-path: \"/v2/health/ready\"\n",
    "    nginx.ingress.kubernetes.io/upstream-healthcheck-interval: \"30s\"\n",
    "    \n",
    "    # è² è¼‰å‡è¡¡\n",
    "    nginx.ingress.kubernetes.io/load-balance: \"round_robin\"\n",
    "    nginx.ingress.kubernetes.io/upstream-hash-by: \"$remote_addr\"\n",
    "    \n",
    "    # å®‰å…¨æ¨™é ­\n",
    "    nginx.ingress.kubernetes.io/configuration-snippet: |\n",
    "      more_set_headers \"X-Content-Type-Options: nosniff\";\n",
    "      more_set_headers \"X-Frame-Options: DENY\";\n",
    "      more_set_headers \"X-XSS-Protection: 1; mode=block\";\n",
    "      more_set_headers \"Referrer-Policy: strict-origin-when-cross-origin\";\n",
    "      \n",
    "    # TLS é…ç½®\n",
    "    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n",
    "    \n",
    "spec:\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - triton.example.com\n",
    "    - api.triton.example.com\n",
    "    secretName: triton-tls-secret\n",
    "  \n",
    "  rules:\n",
    "  # ä¸»è¦ API ç«¯é»\n",
    "  - host: triton.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /v2\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: {APP_NAME}-service\n",
    "            port:\n",
    "              number: 8000\n",
    "      - path: /metrics\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: {APP_NAME}-service\n",
    "            port:\n",
    "              number: 8002\n",
    "  \n",
    "  # API å°ˆç”¨åŸŸå\n",
    "  - host: api.triton.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: {APP_NAME}-service\n",
    "            port:\n",
    "              number: 8000\n",
    "\n",
    "---\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: {APP_NAME}-network-policy\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: security\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app: {APP_NAME}\n",
    "  \n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  - Egress\n",
    "  \n",
    "  ingress:\n",
    "  # å…è¨±ä¾†è‡ª Ingress Controller çš„æµé‡\n",
    "  - from:\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          name: ingress-nginx\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 8000\n",
    "    - protocol: TCP\n",
    "      port: 8001\n",
    "    - protocol: TCP\n",
    "      port: 8002\n",
    "  \n",
    "  # å…è¨±ä¾†è‡ªç›¸åŒå‘½åç©ºé–“çš„æµé‡\n",
    "  - from:\n",
    "    - podSelector: {{}}\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 8000\n",
    "    - protocol: TCP\n",
    "      port: 8001\n",
    "    - protocol: TCP\n",
    "      port: 8002\n",
    "  \n",
    "  # å…è¨±ä¾†è‡ªç›£æ§å‘½åç©ºé–“çš„æµé‡\n",
    "  - from:\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          name: monitoring\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 8002\n",
    "  \n",
    "  egress:\n",
    "  # å…è¨± DNS æŸ¥è©¢\n",
    "  - to: []\n",
    "    ports:\n",
    "    - protocol: UDP\n",
    "      port: 53\n",
    "    - protocol: TCP\n",
    "      port: 53\n",
    "  \n",
    "  # å…è¨± HTTPS å‡ºç«™ (æ¨¡å‹ä¸‹è¼‰)\n",
    "  - to: []\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 443\n",
    "  \n",
    "  # å…è¨±èˆ‡ Kubernetes API é€šä¿¡\n",
    "  - to:\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          name: kube-system\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 6443\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/ingress.yaml\", \"w\") as f:\n",
    "    f.write(ingress_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»º Ingress é…ç½®\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/ingress.yaml\")\n",
    "print(\"\\nğŸŒ Ingress ç‰¹æ€§:\")\n",
    "print(\"   - TLS çµ‚æ­¢: Let's Encrypt è‡ªå‹•è­‰æ›¸\")\n",
    "print(\"   - é€Ÿç‡é™åˆ¶: 100 req/min\")\n",
    "print(\"   - å¥åº·æª¢æŸ¥: è‡ªå‹•å¾Œç«¯æª¢æ¸¬\")\n",
    "print(\"   - å®‰å…¨æ¨™é ­: é˜²è­·å¸¸è¦‹æ”»æ“Š\")\n",
    "print(\"   - ç¶²è·¯æ”¿ç­–: ç²¾ç´°æµé‡æ§åˆ¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 5ï¼šè‡ªå‹•æ“´ç¸®å®¹é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 HPA (Horizontal Pod Autoscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPA é…ç½®\n",
    "hpa_yaml = f\"\"\"\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: {APP_NAME}-hpa\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: autoscaling\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: {APP_NAME}\n",
    "  \n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  \n",
    "  metrics:\n",
    "  # CPU ä½¿ç”¨ç‡\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  \n",
    "  # è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  \n",
    "  # GPU ä½¿ç”¨ç‡ (è‡ªå®šç¾©æŒ‡æ¨™)\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: nvidia_gpu_utilization\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"75\"\n",
    "  \n",
    "  # è«‹æ±‚é€Ÿç‡ (è‡ªå®šç¾©æŒ‡æ¨™)\n",
    "  - type: Object\n",
    "    object:\n",
    "      metric:\n",
    "        name: triton_request_rate\n",
    "      describedObject:\n",
    "        apiVersion: v1\n",
    "        kind: Service\n",
    "        name: {APP_NAME}-service\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"100\"\n",
    "  \n",
    "  behavior:\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 300\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100\n",
    "        periodSeconds: 60\n",
    "      - type: Pods\n",
    "        value: 2\n",
    "        periodSeconds: 60\n",
    "      selectPolicy: Max\n",
    "    \n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 600\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50\n",
    "        periodSeconds: 300\n",
    "      - type: Pods\n",
    "        value: 1\n",
    "        periodSeconds: 300\n",
    "      selectPolicy: Min\n",
    "\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: VerticalPodAutoscaler\n",
    "metadata:\n",
    "  name: {APP_NAME}-vpa\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: autoscaling\n",
    "spec:\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: {APP_NAME}\n",
    "  \n",
    "  updatePolicy:\n",
    "    updateMode: \"Off\"  # åƒ…å»ºè­°ï¼Œä¸è‡ªå‹•æ‡‰ç”¨\n",
    "  \n",
    "  resourcePolicy:\n",
    "    containerPolicies:\n",
    "    - containerName: triton-server\n",
    "      minAllowed:\n",
    "        cpu: 1\n",
    "        memory: 4Gi\n",
    "      maxAllowed:\n",
    "        cpu: 8\n",
    "        memory: 32Gi\n",
    "      controlledResources: [\"cpu\", \"memory\"]\n",
    "      controlledValues: RequestsAndLimits\n",
    "\n",
    "---\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: {APP_NAME}-pdb\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: availability\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: {APP_NAME}\n",
    "  \n",
    "  # è‡³å°‘ä¿æŒ 50% çš„ Pod å¯ç”¨\n",
    "  minAvailable: 50%\n",
    "  \n",
    "  # æˆ–è€…æœ€å¤šå…è¨± 1 å€‹ Pod ä¸å¯ç”¨\n",
    "  # maxUnavailable: 1\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/autoscaling.yaml\", \"w\") as f:\n",
    "    f.write(hpa_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»ºè‡ªå‹•æ“´ç¸®å®¹é…ç½®\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/autoscaling.yaml\")\n",
    "print(\"\\nğŸ“ˆ æ“´ç¸®å®¹ç­–ç•¥:\")\n",
    "print(\"   - HPA: CPU/Memory/GPU/RPS å¤šæŒ‡æ¨™\")\n",
    "print(\"   - VPA: å‚ç›´æ“´å±•å»ºè­° (æ‰‹å‹•æ‡‰ç”¨)\")\n",
    "print(\"   - PDB: 50% æœ€å°å¯ç”¨æ€§ä¿è­‰\")\n",
    "print(\"   - æ“´å±•è¡Œç‚º: å¿«é€Ÿæ“´å±• + ç·©æ…¢æ”¶ç¸®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Custom Metrics é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus Adapter é…ç½®\n",
    "metrics_config_yaml = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: adapter-config\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: prometheus-adapter\n",
    "    component: metrics\n",
    "data:\n",
    "  config.yaml: |\n",
    "    rules:\n",
    "    # Triton è«‹æ±‚é€Ÿç‡æŒ‡æ¨™\n",
    "    - seriesQuery: 'nv_inference_request_success_rate{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^nv_inference_request_success_rate\"\n",
    "        as: \"triton_request_rate\"\n",
    "      metricsQuery: 'sum(rate(<<.Series>>{{<<.LabelMatchers>>}}[2m])) by (<<.GroupBy>>)'\n",
    "    \n",
    "    # GPU ä½¿ç”¨ç‡æŒ‡æ¨™\n",
    "    - seriesQuery: 'DCGM_FI_DEV_GPU_UTIL{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^DCGM_FI_DEV_GPU_UTIL\"\n",
    "        as: \"nvidia_gpu_utilization\"\n",
    "      metricsQuery: 'avg(<<.Series>>{{<<.LabelMatchers>>}}) by (<<.GroupBy>>)'\n",
    "    \n",
    "    # æ¨¡å‹æ¨ç†å»¶é²\n",
    "    - seriesQuery: 'nv_inference_request_duration_us{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^nv_inference_request_duration_us\"\n",
    "        as: \"triton_inference_latency\"\n",
    "      metricsQuery: 'avg(rate(<<.Series>>{{<<.LabelMatchers>>}}[2m])) by (<<.GroupBy>>)'\n",
    "    \n",
    "    # æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨é‡\n",
    "    - seriesQuery: 'nv_gpu_memory_used_bytes{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^nv_gpu_memory_used_bytes\"\n",
    "        as: \"triton_gpu_memory_usage\"\n",
    "      metricsQuery: 'avg(<<.Series>>{{<<.LabelMatchers>>}}) by (<<.GroupBy>>)'\n",
    "    \n",
    "    # ä½‡åˆ—é•·åº¦\n",
    "    - seriesQuery: 'nv_inference_queue_duration_us{{namespace!=\"\",pod!=\"\"}}'\n",
    "      resources:\n",
    "        overrides:\n",
    "          namespace: {{resource: \"namespace\"}}\n",
    "          pod: {{resource: \"pod\"}}\n",
    "      name:\n",
    "        matches: \"^nv_inference_queue_duration_us\"\n",
    "        as: \"triton_queue_time\"\n",
    "      metricsQuery: 'avg(rate(<<.Series>>{{<<.LabelMatchers>>}}[2m])) by (<<.GroupBy>>)'\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: {APP_NAME}-metrics\n",
    "  namespace: {NAMESPACE}\n",
    "  labels:\n",
    "    app: {APP_NAME}\n",
    "    component: monitoring\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: {APP_NAME}\n",
    "  \n",
    "  endpoints:\n",
    "  - port: metrics\n",
    "    path: /metrics\n",
    "    interval: 30s\n",
    "    scrapeTimeout: 10s\n",
    "    honorLabels: true\n",
    "    \n",
    "    # æŒ‡æ¨™é‡æ–°æ¨™è¨˜\n",
    "    metricRelabelings:\n",
    "    - sourceLabels: [__name__]\n",
    "      regex: 'nv_(.*)'\n",
    "      targetLabel: triton_metric\n",
    "      replacement: '${{1}}'\n",
    "    \n",
    "    # æ¨£æœ¬é™åˆ¶\n",
    "    sampleLimit: 10000\n",
    "    \n",
    "  namespaceSelector:\n",
    "    matchNames:\n",
    "    - {NAMESPACE}\n",
    "\"\"\"\n",
    "\n",
    "# å¯«å…¥æ–‡ä»¶\n",
    "with open(f\"{MANIFESTS_DIR}/metrics-config.yaml\", \"w\") as f:\n",
    "    f.write(metrics_config_yaml)\n",
    "\n",
    "print(\"ğŸ“ å·²å‰µå»ºè‡ªå®šç¾©æŒ‡æ¨™é…ç½®\")\n",
    "print(f\"   æ–‡ä»¶ä½ç½®: {MANIFESTS_DIR}/metrics-config.yaml\")\n",
    "print(\"\\nğŸ“Š è‡ªå®šç¾©æŒ‡æ¨™:\")\n",
    "print(\"   - triton_request_rate: è«‹æ±‚é€Ÿç‡\")\n",
    "print(\"   - nvidia_gpu_utilization: GPU ä½¿ç”¨ç‡\")\n",
    "print(\"   - triton_inference_latency: æ¨ç†å»¶é²\")\n",
    "print(\"   - triton_gpu_memory_usage: GPU è¨˜æ†¶é«”\")\n",
    "print(\"   - triton_queue_time: ä½‡åˆ—ç­‰å¾…æ™‚é–“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¯¦é©— 6ï¼šéƒ¨ç½²é©—è­‰èˆ‡æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 æ‰¹é‡éƒ¨ç½²æ‰€æœ‰è³‡æº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_all_manifests():\n",
    "    \"\"\"æ‰¹é‡éƒ¨ç½²æ‰€æœ‰ Kubernetes è³‡æº\"\"\"\n",
    "    \n",
    "    # å®šç¾©éƒ¨ç½²é †åº (ä¾è³´é—œä¿‚)\n",
    "    deployment_order = [\n",
    "        \"namespace.yaml\",\n",
    "        \"rbac.yaml\", \n",
    "        \"storage.yaml\",\n",
    "        \"configmap.yaml\",\n",
    "        \"deployment.yaml\",\n",
    "        \"service.yaml\",\n",
    "        \"ingress.yaml\",\n",
    "        \"autoscaling.yaml\",\n",
    "        \"metrics-config.yaml\"\n",
    "    ]\n",
    "    \n",
    "    deployment_results = []\n",
    "    \n",
    "    print(\"ğŸš€ é–‹å§‹æ‰¹é‡éƒ¨ç½²...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, manifest in enumerate(deployment_order, 1):\n",
    "        manifest_path = f\"{MANIFESTS_DIR}/{manifest}\"\n",
    "        \n",
    "        if not os.path.exists(manifest_path):\n",
    "            print(f\"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {manifest}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n[{i}/{len(deployment_order)}] éƒ¨ç½² {manifest}...\")\n",
    "        \n",
    "        success = apply_kubernetes_manifest(manifest_path)\n",
    "        deployment_results.append((manifest, success))\n",
    "        \n",
    "        if success:\n",
    "            print(f\"âœ… {manifest} éƒ¨ç½²æˆåŠŸ\")\n",
    "            # ç­‰å¾…è³‡æºå‰µå»º\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(f\"âŒ {manifest} éƒ¨ç½²å¤±æ•—\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ“Š éƒ¨ç½²çµæœç¸½çµ:\")\n",
    "    \n",
    "    success_count = 0\n",
    "    for manifest, success in deployment_results:\n",
    "        status = \"âœ… æˆåŠŸ\" if success else \"âŒ å¤±æ•—\"\n",
    "        print(f\"   {manifest:<25} {status}\")\n",
    "        if success:\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"\\nç¸½è¨ˆ: {success_count}/{len(deployment_results)} æˆåŠŸ\")\n",
    "    \n",
    "    return deployment_results\n",
    "\n",
    "# åŸ·è¡Œéƒ¨ç½² (å¦‚æœé›†ç¾¤å¯ç”¨)\n",
    "if cluster_ready:\n",
    "    deployment_results = deploy_all_manifests()\n",
    "else:\n",
    "    print(\"âš ï¸  è·³ééƒ¨ç½² - Kubernetes é›†ç¾¤ä¸å¯ç”¨\")\n",
    "    print(\"\\nğŸ“ ç”Ÿæˆçš„é…ç½®æ–‡ä»¶:\")\n",
    "    for file in os.listdir(MANIFESTS_DIR):\n",
    "        if file.endswith('.yaml'):\n",
    "            print(f\"   {MANIFESTS_DIR}/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 éƒ¨ç½²ç‹€æ…‹æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_deployment_status():\n",
    "    \"\"\"æª¢æŸ¥éƒ¨ç½²ç‹€æ…‹\"\"\"\n",
    "    try:\n",
    "        v1 = client.CoreV1Api()\n",
    "        apps_v1 = client.AppsV1Api()\n",
    "        \n",
    "        print(\"ğŸ” æª¢æŸ¥éƒ¨ç½²ç‹€æ…‹...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # æª¢æŸ¥å‘½åç©ºé–“\n",
    "        try:\n",
    "            namespace = v1.read_namespace(name=NAMESPACE)\n",
    "            print(f\"âœ… å‘½åç©ºé–“: {NAMESPACE} (æ´»èº)\")\n",
    "        except ApiException as e:\n",
    "            print(f\"âŒ å‘½åç©ºé–“: {NAMESPACE} (ä¸å­˜åœ¨)\")\n",
    "            return False\n",
    "        \n",
    "        # æª¢æŸ¥ PVC\n",
    "        print(\"\\nğŸ“¦ æŒä¹…åŒ–å­˜å„²:\")\n",
    "        pvcs = v1.list_namespaced_persistent_volume_claim(namespace=NAMESPACE)\n",
    "        for pvc in pvcs.items:\n",
    "            name = pvc.metadata.name\n",
    "            status = pvc.status.phase\n",
    "            capacity = pvc.status.capacity.get('storage', 'Unknown') if pvc.status.capacity else 'Unknown'\n",
    "            print(f\"   {name}: {status} ({capacity})\")\n",
    "        \n",
    "        # æª¢æŸ¥ Deployment\n",
    "        print(\"\\nğŸš€ éƒ¨ç½²ç‹€æ…‹:\")\n",
    "        deployments = apps_v1.list_namespaced_deployment(namespace=NAMESPACE)\n",
    "        for deployment in deployments.items:\n",
    "            name = deployment.metadata.name\n",
    "            replicas = deployment.spec.replicas\n",
    "            ready_replicas = deployment.status.ready_replicas or 0\n",
    "            available_replicas = deployment.status.available_replicas or 0\n",
    "            \n",
    "            print(f\"   {name}:\")\n",
    "            print(f\"     æœŸæœ›å‰¯æœ¬: {replicas}\")\n",
    "            print(f\"     å°±ç·’å‰¯æœ¬: {ready_replicas}\")\n",
    "            print(f\"     å¯ç”¨å‰¯æœ¬: {available_replicas}\")\n",
    "            \n",
    "            # æª¢æŸ¥éƒ¨ç½²æ¢ä»¶\n",
    "            if deployment.status.conditions:\n",
    "                for condition in deployment.status.conditions:\n",
    "                    if condition.type == \"Available\":\n",
    "                        status_icon = \"âœ…\" if condition.status == \"True\" else \"âŒ\"\n",
    "                        print(f\"     {status_icon} å¯ç”¨æ€§: {condition.status}\")\n",
    "        \n",
    "        # æª¢æŸ¥ Pod\n",
    "        print(\"\\nğŸƒ Pod ç‹€æ…‹:\")\n",
    "        pods = v1.list_namespaced_pod(namespace=NAMESPACE)\n",
    "        for pod in pods.items:\n",
    "            name = pod.metadata.name\n",
    "            phase = pod.status.phase\n",
    "            node = pod.spec.node_name or \"æœªèª¿åº¦\"\n",
    "            \n",
    "            # æª¢æŸ¥å®¹å™¨ç‹€æ…‹\n",
    "            ready_containers = 0\n",
    "            total_containers = len(pod.spec.containers)\n",
    "            \n",
    "            if pod.status.container_statuses:\n",
    "                ready_containers = sum(1 for status in pod.status.container_statuses if status.ready)\n",
    "            \n",
    "            status_icon = \"âœ…\" if phase == \"Running\" and ready_containers == total_containers else \"âš ï¸\"\n",
    "            print(f\"   {status_icon} {name}:\")\n",
    "            print(f\"     éšæ®µ: {phase}\")\n",
    "            print(f\"     ç¯€é»: {node}\")\n",
    "            print(f\"     å®¹å™¨: {ready_containers}/{total_containers} å°±ç·’\")\n",
    "        \n",
    "        # æª¢æŸ¥ Service\n",
    "        print(\"\\nğŸŒ æœå‹™ç‹€æ…‹:\")\n",
    "        services = v1.list_namespaced_service(namespace=NAMESPACE)\n",
    "        for service in services.items:\n",
    "            name = service.metadata.name\n",
    "            service_type = service.spec.type\n",
    "            cluster_ip = service.spec.cluster_ip\n",
    "            \n",
    "            print(f\"   {name}:\")\n",
    "            print(f\"     é¡å‹: {service_type}\")\n",
    "            print(f\"     Cluster IP: {cluster_ip}\")\n",
    "            \n",
    "            if service_type == \"LoadBalancer\":\n",
    "                if service.status.load_balancer.ingress:\n",
    "                    external_ip = service.status.load_balancer.ingress[0].ip\n",
    "                    print(f\"     å¤–éƒ¨ IP: {external_ip}\")\n",
    "                else:\n",
    "                    print(f\"     å¤–éƒ¨ IP: <å¾…åˆ†é…>\")\n",
    "        \n",
    "        # æª¢æŸ¥ HPA\n",
    "        print(\"\\nğŸ“ˆ è‡ªå‹•æ“´ç¸®å®¹:\")\n",
    "        try:\n",
    "            autoscaling_v2 = client.AutoscalingV2Api()\n",
    "            hpas = autoscaling_v2.list_namespaced_horizontal_pod_autoscaler(namespace=NAMESPACE)\n",
    "            for hpa in hpas.items:\n",
    "                name = hpa.metadata.name\n",
    "                current_replicas = hpa.status.current_replicas or 0\n",
    "                desired_replicas = hpa.status.desired_replicas or 0\n",
    "                min_replicas = hpa.spec.min_replicas\n",
    "                max_replicas = hpa.spec.max_replicas\n",
    "                \n",
    "                print(f\"   {name}:\")\n",
    "                print(f\"     ç•¶å‰å‰¯æœ¬: {current_replicas}\")\n",
    "                print(f\"     ç›®æ¨™å‰¯æœ¬: {desired_replicas}\")\n",
    "                print(f\"     ç¯„åœ: {min_replicas}-{max_replicas}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  ç„¡æ³•æª¢æŸ¥ HPA: {e}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "# æª¢æŸ¥éƒ¨ç½²ç‹€æ…‹\n",
    "if cluster_ready:\n",
    "    check_deployment_status()\n",
    "else:\n",
    "    print(\"âš ï¸  è·³éç‹€æ…‹æª¢æŸ¥ - Kubernetes é›†ç¾¤ä¸å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 åŠŸèƒ½æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_triton_deployment():\n",
    "    \"\"\"æ¸¬è©¦ Triton éƒ¨ç½²åŠŸèƒ½\"\"\"\n",
    "    print(\"ğŸ§ª é–‹å§‹åŠŸèƒ½æ¸¬è©¦...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ¸¬è©¦é…ç½®\n",
    "    test_results = []\n",
    "    \n",
    "    try:\n",
    "        v1 = client.CoreV1Api()\n",
    "        \n",
    "        # ç²å–æœå‹™ç«¯é»\n",
    "        services = v1.list_namespaced_service(namespace=NAMESPACE)\n",
    "        service_endpoint = None\n",
    "        \n",
    "        for service in services.items:\n",
    "            if service.metadata.name == f\"{APP_NAME}-service\":\n",
    "                cluster_ip = service.spec.cluster_ip\n",
    "                http_port = None\n",
    "                \n",
    "                for port in service.spec.ports:\n",
    "                    if port.name == \"http\":\n",
    "                        http_port = port.port\n",
    "                        break\n",
    "                \n",
    "                if cluster_ip and http_port:\n",
    "                    service_endpoint = f\"http://{cluster_ip}:{http_port}\"\n",
    "                break\n",
    "        \n",
    "        if not service_endpoint:\n",
    "            print(\"âŒ ç„¡æ³•ç²å–æœå‹™ç«¯é»\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"ğŸ¯ æ¸¬è©¦ç«¯é»: {service_endpoint}\")\n",
    "        \n",
    "        # æ¸¬è©¦ 1: å¥åº·æª¢æŸ¥\n",
    "        print(\"\\n1ï¸âƒ£ å¥åº·æª¢æŸ¥æ¸¬è©¦:\")\n",
    "        try:\n",
    "            response = requests.get(f\"{service_endpoint}/v2/health/live\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                print(\"   âœ… å­˜æ´»æª¢æŸ¥: é€šé\")\n",
    "                test_results.append(('health_live', True))\n",
    "            else:\n",
    "                print(f\"   âŒ å­˜æ´»æª¢æŸ¥: å¤±æ•— ({response.status_code})\")\n",
    "                test_results.append(('health_live', False))\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ å­˜æ´»æª¢æŸ¥: é€£æ¥å¤±æ•— ({e})\")\n",
    "            test_results.append(('health_live', False))\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(f\"{service_endpoint}/v2/health/ready\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                print(\"   âœ… å°±ç·’æª¢æŸ¥: é€šé\")\n",
    "                test_results.append(('health_ready', True))\n",
    "            else:\n",
    "                print(f\"   âŒ å°±ç·’æª¢æŸ¥: å¤±æ•— ({response.status_code})\")\n",
    "                test_results.append(('health_ready', False))\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ å°±ç·’æª¢æŸ¥: é€£æ¥å¤±æ•— ({e})\")\n",
    "            test_results.append(('health_ready', False))\n",
    "        \n",
    "        # æ¸¬è©¦ 2: æ¨¡å‹åº«æª¢æŸ¥\n",
    "        print(\"\\n2ï¸âƒ£ æ¨¡å‹åº«æ¸¬è©¦:\")\n",
    "        try:\n",
    "            response = requests.get(f\"{service_endpoint}/v2/models\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                models_data = response.json()\n",
    "                model_count = len(models_data.get('models', []))\n",
    "                print(f\"   âœ… æ¨¡å‹åº«: å¯è¨ªå• ({model_count} å€‹æ¨¡å‹)\")\n",
    "                test_results.append(('models_api', True))\n",
    "                \n",
    "                if model_count > 0:\n",
    "                    print(\"   ğŸ“‹ å¯ç”¨æ¨¡å‹:\")\n",
    "                    for model in models_data['models'][:5]:  # åªé¡¯ç¤ºå‰5å€‹\n",
    "                        model_name = model.get('name', 'Unknown')\n",
    "                        model_version = model.get('version', 'Unknown')\n",
    "                        print(f\"      - {model_name} (v{model_version})\")\n",
    "                else:\n",
    "                    print(\"   âš ï¸  æ²’æœ‰å¯ç”¨çš„æ¨¡å‹\")\n",
    "            else:\n",
    "                print(f\"   âŒ æ¨¡å‹åº«: API éŒ¯èª¤ ({response.status_code})\")\n",
    "                test_results.append(('models_api', False))\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ æ¨¡å‹åº«: é€£æ¥å¤±æ•— ({e})\")\n",
    "            test_results.append(('models_api', False))\n",
    "        \n",
    "        # æ¸¬è©¦ 3: æŒ‡æ¨™æª¢æŸ¥\n",
    "        print(\"\\n3ï¸âƒ£ æŒ‡æ¨™æ¸¬è©¦:\")\n",
    "        try:\n",
    "            # å˜—è©¦ç²å–æŒ‡æ¨™ç«¯é»\n",
    "            metrics_endpoint = service_endpoint.replace(':8000', ':8002')\n",
    "            response = requests.get(f\"{metrics_endpoint}/metrics\", timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                metrics_text = response.text\n",
    "                # æª¢æŸ¥é—œéµæŒ‡æ¨™\n",
    "                key_metrics = [\n",
    "                    'nv_inference_request_success',\n",
    "                    'nv_inference_request_failure', \n",
    "                    'nv_inference_queue_duration_us',\n",
    "                    'nv_gpu_utilization'\n",
    "                ]\n",
    "                \n",
    "                found_metrics = []\n",
    "                for metric in key_metrics:\n",
    "                    if metric in metrics_text:\n",
    "                        found_metrics.append(metric)\n",
    "                \n",
    "                print(f\"   âœ… æŒ‡æ¨™ç«¯é»: å¯è¨ªå•\")\n",
    "                print(f\"   ğŸ“Š æ‰¾åˆ°æŒ‡æ¨™: {len(found_metrics)}/{len(key_metrics)}\")\n",
    "                test_results.append(('metrics_api', True))\n",
    "                \n",
    "                if found_metrics:\n",
    "                    print(\"   ğŸ“ˆ å¯ç”¨æŒ‡æ¨™:\")\n",
    "                    for metric in found_metrics:\n",
    "                        print(f\"      - {metric}\")\n",
    "            else:\n",
    "                print(f\"   âŒ æŒ‡æ¨™ç«¯é»: HTTP éŒ¯èª¤ ({response.status_code})\")\n",
    "                test_results.append(('metrics_api', False))\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ æŒ‡æ¨™ç«¯é»: é€£æ¥å¤±æ•— ({e})\")\n",
    "            test_results.append(('metrics_api', False))\n",
    "        \n",
    "        # æ¸¬è©¦ 4: è² è¼‰å‡è¡¡æ¸¬è©¦\n",
    "        print(\"\\n4ï¸âƒ£ è² è¼‰å‡è¡¡æ¸¬è©¦:\")\n",
    "        try:\n",
    "            # å¤šæ¬¡è«‹æ±‚æª¢æŸ¥è² è¼‰åˆ†é…\n",
    "            server_ids = set()\n",
    "            success_count = 0\n",
    "            \n",
    "            for i in range(5):\n",
    "                response = requests.get(f\"{service_endpoint}/v2/health/live\", timeout=5)\n",
    "                if response.status_code == 200:\n",
    "                    success_count += 1\n",
    "                    # å˜—è©¦å¾éŸ¿æ‡‰æ¨™é ­ç²å–æœå‹™å™¨è³‡è¨Š\n",
    "                    server_id = response.headers.get('Server', f\"unknown-{i}\")\n",
    "                    server_ids.add(server_id)\n",
    "            \n",
    "            print(f\"   âœ… è² è¼‰å‡è¡¡: {success_count}/5 è«‹æ±‚æˆåŠŸ\")\n",
    "            print(f\"   ğŸ”„ æœå‹™å™¨å¯¦ä¾‹: {len(server_ids)} å€‹\")\n",
    "            test_results.append(('load_balancing', success_count >= 4))\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ è² è¼‰å‡è¡¡: æ¸¬è©¦å¤±æ•— ({e})\")\n",
    "            test_results.append(('load_balancing', False))\n",
    "        \n",
    "        # ç¸½çµæ¸¬è©¦çµæœ\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ğŸ“Š æ¸¬è©¦çµæœç¸½çµ:\")\n",
    "        \n",
    "        success_count = 0\n",
    "        for test_name, success in test_results:\n",
    "            status = \"âœ… é€šé\" if success else \"âŒ å¤±æ•—\"\n",
    "            test_display_names = {\n",
    "                'health_live': 'å­˜æ´»æª¢æŸ¥',\n",
    "                'health_ready': 'å°±ç·’æª¢æŸ¥', \n",
    "                'models_api': 'æ¨¡å‹åº« API',\n",
    "                'metrics_api': 'æŒ‡æ¨™ç«¯é»',\n",
    "                'load_balancing': 'è² è¼‰å‡è¡¡'\n",
    "            }\n",
    "            display_name = test_display_names.get(test_name, test_name)\n",
    "            print(f\"   {display_name:<15} {status}\")\n",
    "            if success:\n",
    "                success_count += 1\n",
    "        \n",
    "        overall_success = success_count >= len(test_results) * 0.8  # 80% é€šéç‡\n",
    "        overall_status = \"âœ… æ•´é«”é€šé\" if overall_success else \"âŒ æ•´é«”å¤±æ•—\"\n",
    "        print(f\"\\n{overall_status} ({success_count}/{len(test_results)})\")\n",
    "        \n",
    "        return overall_success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "# åŸ·è¡ŒåŠŸèƒ½æ¸¬è©¦\n",
    "if cluster_ready:\n",
    "    test_success = test_triton_deployment()\n",
    "else:\n",
    "    print(\"âš ï¸  è·³éåŠŸèƒ½æ¸¬è©¦ - Kubernetes é›†ç¾¤ä¸å¯ç”¨\")\n",
    "    test_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š å¯¦é©—ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆéƒ¨ç½²å ±å‘Š\n",
    "def generate_deployment_report():\n",
    "    \"\"\"ç”Ÿæˆéƒ¨ç½²å ±å‘Š\"\"\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# Kubernetes éƒ¨ç½²å ±å‘Š\n",
    "\n",
    "## ğŸ“‹ éƒ¨ç½²æ¦‚è¦\n",
    "\n",
    "**æ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**å‘½åç©ºé–“**: {NAMESPACE}\n",
    "**æ‡‰ç”¨**: {APP_NAME}\n",
    "**æ˜ åƒ**: {IMAGE}\n",
    "\n",
    "## ğŸ—ï¸ è³‡æºé…ç½®\n",
    "\n",
    "### è¨ˆç®—è³‡æº\n",
    "- **CPU**: 2-4 cores per pod\n",
    "- **è¨˜æ†¶é«”**: 8-16Gi per pod\n",
    "- **GPU**: 1x NVIDIA GPU per pod\n",
    "- **å‰¯æœ¬æ•¸**: 2 (min) - 10 (max)\n",
    "\n",
    "### å­˜å„²è³‡æº\n",
    "- **æ¨¡å‹å­˜å„²**: 100Gi (ReadOnlyMany)\n",
    "- **å¿«å–å­˜å„²**: 50Gi (ReadWriteMany)\n",
    "- **å…±äº«è¨˜æ†¶é«”**: 1Gi per pod\n",
    "\n",
    "### ç¶²è·¯é…ç½®\n",
    "- **HTTP ç«¯å£**: 8000\n",
    "- **gRPC ç«¯å£**: 8001 \n",
    "- **æŒ‡æ¨™ç«¯å£**: 8002\n",
    "- **TLS**: æ”¯æ´ (Let's Encrypt)\n",
    "\n",
    "## ğŸ”§ åŠŸèƒ½ç‰¹æ€§\n",
    "\n",
    "### é«˜å¯ç”¨æ€§\n",
    "- âœ… å¤šå‰¯æœ¬éƒ¨ç½² (åè¦ªå’Œæ€§)\n",
    "- âœ… æ»¾å‹•æ›´æ–° (é›¶åœæ©Ÿ)\n",
    "- âœ… å¥åº·æª¢æŸ¥ (å­˜æ´»/å°±ç·’/å•Ÿå‹•)\n",
    "- âœ… Pod ä¸­æ–·é ç®— (50% æœ€å°å¯ç”¨)\n",
    "\n",
    "### è‡ªå‹•æ“´ç¸®å®¹\n",
    "- âœ… HPA (CPU/Memory/GPU/RPS)\n",
    "- âœ… VPA å»ºè­°æ¨¡å¼\n",
    "- âœ… è‡ªå®šç¾©æŒ‡æ¨™æ”¯æ´\n",
    "- âœ… æ™ºèƒ½æ“´ç¸®ç­–ç•¥\n",
    "\n",
    "### å®‰å…¨æ€§\n",
    "- âœ… RBAC æœ€å°æ¬Šé™\n",
    "- âœ… ç¶²è·¯æ”¿ç­–\n",
    "- âœ… é root ç”¨æˆ¶\n",
    "- âœ… å®‰å…¨ä¸Šä¸‹æ–‡\n",
    "\n",
    "### ç›£æ§èˆ‡å¯è§€æ¸¬æ€§\n",
    "- âœ… Prometheus æŒ‡æ¨™\n",
    "- âœ… è‡ªå®šç¾©æŒ‡æ¨™\n",
    "- âœ… ServiceMonitor\n",
    "- âœ… æŒ‡æ¨™é‡æ–°æ¨™è¨˜\n",
    "\n",
    "## ğŸ“ ç”Ÿæˆçš„é…ç½®æ–‡ä»¶\n",
    "\"\"\"\n",
    "    \n",
    "    # åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶\n",
    "    manifest_files = []\n",
    "    for file in os.listdir(MANIFESTS_DIR):\n",
    "        if file.endswith('.yaml'):\n",
    "            file_path = f\"{MANIFESTS_DIR}/{file}\"\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            manifest_files.append((file, file_size))\n",
    "    \n",
    "    for file, size in sorted(manifest_files):\n",
    "        report += f\"\\n- `{file}` ({size:,} bytes)\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "## ğŸš€ éƒ¨ç½²å‘½ä»¤\n",
    "\n",
    "```bash\n",
    "# ä¸€éµéƒ¨ç½²æ‰€æœ‰è³‡æº\n",
    "kubectl apply -f {MANIFESTS_DIR}/\n",
    "\n",
    "# æª¢æŸ¥éƒ¨ç½²ç‹€æ…‹\n",
    "kubectl get all -n {NAMESPACE}\n",
    "\n",
    "# æŸ¥çœ‹ Pod æ—¥èªŒ\n",
    "kubectl logs -f deployment/{APP_NAME} -n {NAMESPACE}\n",
    "\n",
    "# ç«¯å£è½‰ç™¼ (æ¸¬è©¦ç”¨)\n",
    "kubectl port-forward svc/{APP_NAME}-service 8000:8000 -n {NAMESPACE}\n",
    "```\n",
    "\n",
    "## ğŸ§ª é©—è­‰æ­¥é©Ÿ\n",
    "\n",
    "```bash\n",
    "# å¥åº·æª¢æŸ¥\n",
    "curl http://localhost:8000/v2/health/live\n",
    "curl http://localhost:8000/v2/health/ready\n",
    "\n",
    "# æ¨¡å‹åˆ—è¡¨\n",
    "curl http://localhost:8000/v2/models\n",
    "\n",
    "# Prometheus æŒ‡æ¨™\n",
    "curl http://localhost:8002/metrics\n",
    "```\n",
    "\n",
    "## ğŸ“š ä¸‹ä¸€æ­¥\n",
    "\n",
    "1. **æ¨¡å‹éƒ¨ç½²**: ä¸Šå‚³æ¨¡å‹åˆ° PV ä¸¦é…ç½® config.pbtxt\n",
    "2. **ç›£æ§è¨­ç½®**: é…ç½® Prometheus å’Œ Grafana\n",
    "3. **CI/CD æ•´åˆ**: è¨­ç½®è‡ªå‹•åŒ–éƒ¨ç½²æµç¨‹\n",
    "4. **å£“åŠ›æ¸¬è©¦**: é©—è­‰è‡ªå‹•æ“´ç¸®å®¹åŠŸèƒ½\n",
    "5. **ç½é›£æ¢å¾©**: æ¸¬è©¦æ•…éšœè½‰ç§»æ©Ÿåˆ¶\n",
    "\n",
    "---\n",
    "**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**å¯¦é©—å®¤**: Lab 2.5.1 - Kubernetes åŸç”Ÿéƒ¨ç½²\n",
    "\"\"\"\n",
    "    \n",
    "    # ä¿å­˜å ±å‘Š\n",
    "    report_path = f\"{K8S_DIR}/deployment-report.md\"\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"ğŸ“Š éƒ¨ç½²å ±å‘Šå·²ç”Ÿæˆ\")\n",
    "    print(f\"   æ–‡ä»¶ä½ç½®: {report_path}\")\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "# ç”Ÿæˆå ±å‘Š\n",
    "report_path = generate_deployment_report()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ Lab 2.5.1 å¯¦é©—å®Œæˆï¼\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nâœ… å®Œæˆé …ç›®:\")\n",
    "print(\"   - Kubernetes é›†ç¾¤ç’°å¢ƒæº–å‚™\")\n",
    "print(\"   - GPU è³‡æºèª¿åº¦é…ç½®\")\n",
    "print(\"   - é«˜å¯ç”¨ Triton éƒ¨ç½²\")\n",
    "print(\"   - æœå‹™ç™¼ç¾èˆ‡è² è¼‰å‡è¡¡\")\n",
    "print(\"   - è‡ªå‹•æ“´ç¸®å®¹æ©Ÿåˆ¶\")\n",
    "print(\"   - å®‰å…¨èˆ‡ç¶²è·¯æ”¿ç­–\")\n",
    "print(\"   - ç›£æ§æŒ‡æ¨™æ•´åˆ\")\n",
    "print(\"\\nğŸ“ è¼¸å‡ºæ–‡ä»¶:\")\n",
    "print(f\"   - é…ç½®æ–‡ä»¶: {MANIFESTS_DIR}/\")\n",
    "print(f\"   - éƒ¨ç½²å ±å‘Š: {report_path}\")\n",
    "print(\"\\nğŸš€ æº–å‚™é€²å…¥ä¸‹ä¸€éšæ®µ: CI/CD èˆ‡ MLOps æ•´åˆ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}