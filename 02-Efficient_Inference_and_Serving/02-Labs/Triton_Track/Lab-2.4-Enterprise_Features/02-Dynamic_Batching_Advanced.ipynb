{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4.2 - 高級動態批次處理與智能調度\n",
    "\n",
    "## 🎯 實驗目標\n",
    "\n",
    "本實驗將教您如何：\n",
    "1. 實現智能動態批次調度算法\n",
    "2. 設計優先級請求處理機制\n",
    "3. 優化延遲與吞吐量的平衡\n",
    "4. 實現自適應批次大小調整\n",
    "5. 構建請求排隊和負載均衡策略\n",
    "\n",
    "## 📋 前置需求\n",
    "\n",
    "- 完成 Lab 2.1（Triton 基礎設置）\n",
    "- 了解批次處理基本概念\n",
    "- 熟悉性能監控和調優\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 理論背景\n",
    "\n",
    "### 動態批次處理挑戰\n",
    "\n",
    "**1. 延遲 vs 吞吐量權衡**\n",
    "- 大批次：高吞吐量，高延遲\n",
    "- 小批次：低延遲，低吞吐量\n",
    "- 動態調整：根據負載智能平衡\n",
    "\n",
    "**2. 請求優先級管理**\n",
    "- VIP 用戶優先處理\n",
    "- 緊急請求快速通道\n",
    "- 批次任務低優先級\n",
    "\n",
    "**3. 資源利用率最佳化**\n",
    "- GPU 記憶體有效利用\n",
    "- 計算資源動態分配\n",
    "- 多模型並行處理\n",
    "\n",
    "### 智能調度架構\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[請求接收] --> B[優先級分類]\n",
    "    B --> C{負載檢測}\n",
    "    C -->|高負載| D[大批次策略]\n",
    "    C -->|低負載| E[小批次策略]\n",
    "    C -->|中負載| F[動態調整]\n",
    "    \n",
    "    D --> G[批次調度器]\n",
    "    E --> G\n",
    "    F --> G\n",
    "    \n",
    "    G --> H[GPU 執行]\n",
    "    H --> I[結果返回]\n",
    "    I --> J[性能監控]\n",
    "    J --> C\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import asyncio\n",
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import deque, defaultdict\n",
    "import heapq\n",
    "\n",
    "# 性能監控\n",
    "import psutil\n",
    "import threading\n",
    "from threading import Lock, Event\n",
    "\n",
    "# Triton 客戶端\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "# 可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 設置樣式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"🚀 Dynamic Batching Lab initialized at {datetime.now()}\")\n",
    "print(f\"📊 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置實驗環境\n",
    "BASE_DIR = \"/opt/tritonserver\"\n",
    "MODEL_REPO = f\"{BASE_DIR}/models\"\n",
    "EXPERIMENT_DIR = f\"{BASE_DIR}/experiments/dynamic_batching\"\n",
    "\n",
    "# 創建實驗目錄\n",
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/metrics\", exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/configs\", exist_ok=True)\n",
    "os.makedirs(f\"{EXPERIMENT_DIR}/logs\", exist_ok=True)\n",
    "\n",
    "print(f\"📁 實驗目錄: {EXPERIMENT_DIR}\")\n",
    "print(f\"📁 模型倉庫: {MODEL_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 1：智能批次調度器設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 請求和批次數據結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceRequest:\n",
    "    \"\"\"推理請求數據結構\"\"\"\n",
    "    request_id: str\n",
    "    user_id: str\n",
    "    priority: int  # 1=最高, 5=最低\n",
    "    data: Any\n",
    "    created_at: datetime\n",
    "    timeout: float  # 秒\n",
    "    callback: Optional[callable] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        \"\"\"優先級比較（用於優先佇列）\"\"\"\n",
    "        if self.priority != other.priority:\n",
    "            return self.priority < other.priority\n",
    "        return self.created_at < other.created_at\n",
    "    \n",
    "    @property\n",
    "    def age(self) -> float:\n",
    "        \"\"\"請求年齡（秒）\"\"\"\n",
    "        return (datetime.now() - self.created_at).total_seconds()\n",
    "    \n",
    "    @property\n",
    "    def is_expired(self) -> bool:\n",
    "        \"\"\"是否已過期\"\"\"\n",
    "        return self.age > self.timeout\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BatchConfig:\n",
    "    \"\"\"批次配置\"\"\"\n",
    "    min_batch_size: int = 1\n",
    "    max_batch_size: int = 32\n",
    "    max_wait_time: float = 0.1  # 秒\n",
    "    target_latency: float = 0.05  # 秒\n",
    "    priority_boost: Dict[int, float] = field(default_factory=lambda: {\n",
    "        1: 0.8,  # VIP 用戶降低 80% 等待時間\n",
    "        2: 0.6,  # 高優先級降低 60%\n",
    "        3: 1.0,  # 普通優先級\n",
    "        4: 1.2,  # 低優先級增加 20%\n",
    "        5: 1.5   # 最低優先級增加 50%\n",
    "    })\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BatchMetrics:\n",
    "    \"\"\"批次性能指標\"\"\"\n",
    "    batch_id: str\n",
    "    batch_size: int\n",
    "    processing_time: float\n",
    "    wait_time: float\n",
    "    total_latency: float\n",
    "    throughput: float\n",
    "    priority_distribution: Dict[int, int]\n",
    "    timestamp: datetime\n",
    "    gpu_utilization: float = 0.0\n",
    "    memory_usage: float = 0.0\n",
    "\n",
    "\n",
    "print(\"✅ 數據結構定義完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 智能批次調度器實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartBatchScheduler:\n",
    "    \"\"\"智能批次調度器\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BatchConfig, model_name: str = \"text_classifier\"):\n",
    "        self.config = config\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # 請求佇列（按優先級排序）\n",
    "        self.priority_queue = []\n",
    "        self.queue_lock = Lock()\n",
    "        \n",
    "        # 性能統計\n",
    "        self.metrics_history: List[BatchMetrics] = []\n",
    "        self.metrics_lock = Lock()\n",
    "        \n",
    "        # 調度狀態\n",
    "        self.running = False\n",
    "        self.scheduler_thread = None\n",
    "        self.stop_event = Event()\n",
    "        \n",
    "        # 自適應參數\n",
    "        self.adaptive_config = {\n",
    "            \"current_batch_size\": config.min_batch_size,\n",
    "            \"avg_latency\": 0.0,\n",
    "            \"avg_throughput\": 0.0,\n",
    "            \"load_factor\": 0.0\n",
    "        }\n",
    "        \n",
    "        # 統計計數器\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"processed_requests\": 0,\n",
    "            \"expired_requests\": 0,\n",
    "            \"total_batches\": 0,\n",
    "            \"avg_batch_size\": 0.0\n",
    "        }\n",
    "    \n",
    "    def add_request(self, request: InferenceRequest) -> bool:\n",
    "        \"\"\"添加推理請求\"\"\"\n",
    "        if request.is_expired:\n",
    "            self.stats[\"expired_requests\"] += 1\n",
    "            return False\n",
    "        \n",
    "        with self.queue_lock:\n",
    "            heapq.heappush(self.priority_queue, request)\n",
    "            self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_queue_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"獲取佇列狀態\"\"\"\n",
    "        with self.queue_lock:\n",
    "            queue_size = len(self.priority_queue)\n",
    "            priority_counts = defaultdict(int)\n",
    "            \n",
    "            for req in self.priority_queue:\n",
    "                priority_counts[req.priority] += 1\n",
    "            \n",
    "            return {\n",
    "                \"queue_size\": queue_size,\n",
    "                \"priority_distribution\": dict(priority_counts),\n",
    "                \"oldest_request_age\": self.priority_queue[0].age if queue_size > 0 else 0\n",
    "            }\n",
    "    \n",
    "    def _calculate_optimal_batch_size(self) -> int:\n",
    "        \"\"\"計算最佳批次大小\"\"\"\n",
    "        with self.queue_lock:\n",
    "            queue_size = len(self.priority_queue)\n",
    "        \n",
    "        if queue_size == 0:\n",
    "            return self.config.min_batch_size\n",
    "        \n",
    "        # 基於歷史性能數據的自適應調整\n",
    "        recent_metrics = self.metrics_history[-10:] if self.metrics_history else []\n",
    "        \n",
    "        if recent_metrics:\n",
    "            avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "            avg_throughput = np.mean([m.throughput for m in recent_metrics])\n",
    "            \n",
    "            # 如果延遲過高，減少批次大小\n",
    "            if avg_latency > self.config.target_latency * 1.5:\n",
    "                target_size = max(self.config.min_batch_size, \n",
    "                                self.adaptive_config[\"current_batch_size\"] - 2)\n",
    "            # 如果延遲合理且佇列較長，增加批次大小\n",
    "            elif avg_latency <= self.config.target_latency and queue_size > 10:\n",
    "                target_size = min(self.config.max_batch_size,\n",
    "                                self.adaptive_config[\"current_batch_size\"] + 1)\n",
    "            else:\n",
    "                target_size = self.adaptive_config[\"current_batch_size\"]\n",
    "        else:\n",
    "            # 初始階段基於佇列長度\n",
    "            if queue_size >= self.config.max_batch_size:\n",
    "                target_size = self.config.max_batch_size\n",
    "            elif queue_size >= self.config.min_batch_size:\n",
    "                target_size = min(queue_size, self.config.max_batch_size)\n",
    "            else:\n",
    "                target_size = self.config.min_batch_size\n",
    "        \n",
    "        self.adaptive_config[\"current_batch_size\"] = target_size\n",
    "        return target_size\n",
    "    \n",
    "    def _calculate_wait_time(self, priority: int) -> float:\n",
    "        \"\"\"計算等待時間（基於優先級）\"\"\"\n",
    "        base_wait = self.config.max_wait_time\n",
    "        priority_factor = self.config.priority_boost.get(priority, 1.0)\n",
    "        return base_wait * priority_factor\n",
    "    \n",
    "    def _form_batch(self) -> List[InferenceRequest]:\n",
    "        \"\"\"組成批次\"\"\"\n",
    "        batch = []\n",
    "        target_size = self._calculate_optimal_batch_size()\n",
    "        \n",
    "        with self.queue_lock:\n",
    "            # 移除過期請求\n",
    "            expired_count = 0\n",
    "            while self.priority_queue and self.priority_queue[0].is_expired:\n",
    "                heapq.heappop(self.priority_queue)\n",
    "                expired_count += 1\n",
    "            \n",
    "            self.stats[\"expired_requests\"] += expired_count\n",
    "            \n",
    "            # 組成批次\n",
    "            while len(batch) < target_size and self.priority_queue:\n",
    "                request = heapq.heappop(self.priority_queue)\n",
    "                if not request.is_expired:\n",
    "                    batch.append(request)\n",
    "                else:\n",
    "                    self.stats[\"expired_requests\"] += 1\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def _should_process_batch(self, batch: List[InferenceRequest]) -> bool:\n",
    "        \"\"\"判斷是否應該處理批次\"\"\"\n",
    "        if not batch:\n",
    "            return False\n",
    "        \n",
    "        # 如果達到最小批次大小\n",
    "        if len(batch) >= self.config.min_batch_size:\n",
    "            return True\n",
    "        \n",
    "        # 如果有高優先級請求等待時間過長\n",
    "        for request in batch:\n",
    "            wait_threshold = self._calculate_wait_time(request.priority)\n",
    "            if request.age >= wait_threshold:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _process_batch(self, batch: List[InferenceRequest]) -> BatchMetrics:\n",
    "        \"\"\"處理批次（模擬）\"\"\"\n",
    "        if not batch:\n",
    "            return None\n",
    "        \n",
    "        batch_id = f\"batch_{int(time.time() * 1000)}\"\n",
    "        batch_size = len(batch)\n",
    "        \n",
    "        # 記錄開始時間\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 計算等待時間（最老請求的等待時間）\n",
    "        wait_time = max(req.age for req in batch)\n",
    "        \n",
    "        # 模擬處理時間（基於批次大小和複雜度）\n",
    "        base_processing_time = 0.02  # 20ms 基礎處理時間\n",
    "        batch_overhead = batch_size * 0.001  # 每個樣本增加 1ms\n",
    "        processing_time = base_processing_time + batch_overhead + random.uniform(0, 0.01)\n",
    "        \n",
    "        # 模擬實際處理\n",
    "        time.sleep(processing_time)\n",
    "        \n",
    "        # 計算指標\n",
    "        total_latency = time.time() - start_time + wait_time\n",
    "        throughput = batch_size / processing_time\n",
    "        \n",
    "        # 優先級分布\n",
    "        priority_dist = defaultdict(int)\n",
    "        for req in batch:\n",
    "            priority_dist[req.priority] += 1\n",
    "        \n",
    "        # 創建批次指標\n",
    "        metrics = BatchMetrics(\n",
    "            batch_id=batch_id,\n",
    "            batch_size=batch_size,\n",
    "            processing_time=processing_time,\n",
    "            wait_time=wait_time,\n",
    "            total_latency=total_latency,\n",
    "            throughput=throughput,\n",
    "            priority_distribution=dict(priority_dist),\n",
    "            timestamp=datetime.now(),\n",
    "            gpu_utilization=random.uniform(0.7, 0.95),  # 模擬 GPU 使用率\n",
    "            memory_usage=random.uniform(0.4, 0.8)        # 模擬記憶體使用率\n",
    "        )\n",
    "        \n",
    "        # 更新統計\n",
    "        with self.metrics_lock:\n",
    "            self.metrics_history.append(metrics)\n",
    "            self.stats[\"processed_requests\"] += batch_size\n",
    "            self.stats[\"total_batches\"] += 1\n",
    "            \n",
    "            # 保留最近 1000 條記錄\n",
    "            if len(self.metrics_history) > 1000:\n",
    "                self.metrics_history = self.metrics_history[-1000:]\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _scheduler_loop(self):\n",
    "        \"\"\"調度器主循環\"\"\"\n",
    "        print(\"🚀 智能批次調度器已啟動\")\n",
    "        \n",
    "        while not self.stop_event.is_set():\n",
    "            try:\n",
    "                # 組成批次\n",
    "                batch = self._form_batch()\n",
    "                \n",
    "                # 檢查是否需要處理\n",
    "                if self._should_process_batch(batch):\n",
    "                    metrics = self._process_batch(batch)\n",
    "                    if metrics:\n",
    "                        print(f\"📊 處理批次 {metrics.batch_id}: \"\n",
    "                              f\"大小={metrics.batch_size}, \"\n",
    "                              f\"延遲={metrics.total_latency:.3f}s, \"\n",
    "                              f\"吞吐量={metrics.throughput:.1f} req/s\")\n",
    "                else:\n",
    "                    # 如果有批次但不需要立即處理，將請求放回佇列\n",
    "                    if batch:\n",
    "                        with self.queue_lock:\n",
    "                            for req in batch:\n",
    "                                heapq.heappush(self.priority_queue, req)\n",
    "                \n",
    "                # 短暫等待\n",
    "                time.sleep(0.001)  # 1ms\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ 調度器錯誤: {str(e)}\")\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        print(\"⏹️  智能批次調度器已停止\")\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"啟動調度器\"\"\"\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.stop_event.clear()\n",
    "            self.scheduler_thread = threading.Thread(target=self._scheduler_loop)\n",
    "            self.scheduler_thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"停止調度器\"\"\"\n",
    "        if self.running:\n",
    "            self.running = False\n",
    "            self.stop_event.set()\n",
    "            if self.scheduler_thread:\n",
    "                self.scheduler_thread.join()\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"獲取性能摘要\"\"\"\n",
    "        with self.metrics_lock:\n",
    "            if not self.metrics_history:\n",
    "                return {\"error\": \"沒有性能數據\"}\n",
    "            \n",
    "            recent_metrics = self.metrics_history[-100:]  # 最近 100 個批次\n",
    "            \n",
    "            avg_batch_size = np.mean([m.batch_size for m in recent_metrics])\n",
    "            avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "            avg_throughput = np.mean([m.throughput for m in recent_metrics])\n",
    "            avg_gpu_util = np.mean([m.gpu_utilization for m in recent_metrics])\n",
    "            \n",
    "            return {\n",
    "                \"total_requests\": self.stats[\"total_requests\"],\n",
    "                \"processed_requests\": self.stats[\"processed_requests\"],\n",
    "                \"expired_requests\": self.stats[\"expired_requests\"],\n",
    "                \"total_batches\": self.stats[\"total_batches\"],\n",
    "                \"avg_batch_size\": avg_batch_size,\n",
    "                \"avg_latency\": avg_latency,\n",
    "                \"avg_throughput\": avg_throughput,\n",
    "                \"avg_gpu_utilization\": avg_gpu_util,\n",
    "                \"queue_status\": self.get_queue_status()\n",
    "            }\n",
    "\n",
    "\n",
    "print(\"✅ 智能批次調度器實現完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 2：調度器測試與性能分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 創建調度器實例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建批次配置\n",
    "batch_config = BatchConfig(\n",
    "    min_batch_size=2,\n",
    "    max_batch_size=16,\n",
    "    max_wait_time=0.05,  # 50ms\n",
    "    target_latency=0.03,  # 30ms\n",
    "    priority_boost={\n",
    "        1: 0.5,  # VIP 用戶等待時間減半\n",
    "        2: 0.7,  # 高優先級用戶\n",
    "        3: 1.0,  # 普通用戶\n",
    "        4: 1.3,  # 低優先級用戶\n",
    "        5: 1.8   # 批次處理用戶\n",
    "    }\n",
    ")\n",
    "\n",
    "# 創建調度器\n",
    "scheduler = SmartBatchScheduler(batch_config)\n",
    "\n",
    "print(\"✅ 智能批次調度器創建完成\")\n",
    "print(f\"📊 配置: min_batch={batch_config.min_batch_size}, \"\n",
    "      f\"max_batch={batch_config.max_batch_size}, \"\n",
    "      f\"max_wait={batch_config.max_wait_time}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 請求生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestGenerator:\n",
    "    \"\"\"請求生成器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_types = {\n",
    "            \"vip\": {\"priority\": 1, \"ratio\": 0.05, \"timeout\": 1.0},\n",
    "            \"premium\": {\"priority\": 2, \"ratio\": 0.15, \"timeout\": 2.0},\n",
    "            \"regular\": {\"priority\": 3, \"ratio\": 0.70, \"timeout\": 5.0},\n",
    "            \"batch\": {\"priority\": 4, \"ratio\": 0.08, \"timeout\": 30.0},\n",
    "            \"background\": {\"priority\": 5, \"ratio\": 0.02, \"timeout\": 60.0}\n",
    "        }\n",
    "        self.request_counter = 0\n",
    "    \n",
    "    def generate_request(self) -> InferenceRequest:\n",
    "        \"\"\"生成單個請求\"\"\"\n",
    "        self.request_counter += 1\n",
    "        \n",
    "        # 隨機選擇用戶類型\n",
    "        rand = random.random()\n",
    "        cumulative = 0\n",
    "        selected_type = \"regular\"\n",
    "        \n",
    "        for user_type, config in self.user_types.items():\n",
    "            cumulative += config[\"ratio\"]\n",
    "            if rand <= cumulative:\n",
    "                selected_type = user_type\n",
    "                break\n",
    "        \n",
    "        user_config = self.user_types[selected_type]\n",
    "        \n",
    "        # 創建請求\n",
    "        request = InferenceRequest(\n",
    "            request_id=f\"req_{self.request_counter:06d}\",\n",
    "            user_id=f\"{selected_type}_user_{random.randint(1000, 9999)}\",\n",
    "            priority=user_config[\"priority\"],\n",
    "            data=np.random.randn(224, 224, 3),  # 模擬圖像數據\n",
    "            created_at=datetime.now(),\n",
    "            timeout=user_config[\"timeout\"],\n",
    "            metadata={\"user_type\": selected_type}\n",
    "        )\n",
    "        \n",
    "        return request\n",
    "    \n",
    "    def generate_burst_requests(self, count: int, \n",
    "                              priority_bias: Optional[int] = None) -> List[InferenceRequest]:\n",
    "        \"\"\"生成突發請求\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        for _ in range(count):\n",
    "            request = self.generate_request()\n",
    "            \n",
    "            # 如果指定了優先級偏向，調整優先級\n",
    "            if priority_bias is not None:\n",
    "                request.priority = priority_bias\n",
    "            \n",
    "            requests.append(request)\n",
    "        \n",
    "        return requests\n",
    "    \n",
    "    def simulate_traffic_pattern(self, duration: int, \n",
    "                                pattern: str = \"normal\") -> List[InferenceRequest]:\n",
    "        \"\"\"模擬不同的流量模式\"\"\"\n",
    "        requests = []\n",
    "        \n",
    "        if pattern == \"normal\":\n",
    "            # 正常流量：每秒 10-20 個請求\n",
    "            for _ in range(duration):\n",
    "                count = random.randint(10, 20)\n",
    "                requests.extend(self.generate_burst_requests(count))\n",
    "        \n",
    "        elif pattern == \"spike\":\n",
    "            # 突發流量：短時間內大量請求\n",
    "            for second in range(duration):\n",
    "                if second % 10 == 0:  # 每 10 秒一個峰值\n",
    "                    count = random.randint(50, 100)\n",
    "                else:\n",
    "                    count = random.randint(5, 15)\n",
    "                requests.extend(self.generate_burst_requests(count))\n",
    "        \n",
    "        elif pattern == \"priority_flood\":\n",
    "            # 高優先級請求突增\n",
    "            for second in range(duration):\n",
    "                if second < duration // 2:\n",
    "                    # 前半段正常流量\n",
    "                    count = random.randint(10, 20)\n",
    "                    requests.extend(self.generate_burst_requests(count))\n",
    "                else:\n",
    "                    # 後半段高優先級突增\n",
    "                    normal_count = random.randint(10, 20)\n",
    "                    priority_count = random.randint(20, 40)\n",
    "                    requests.extend(self.generate_burst_requests(normal_count))\n",
    "                    requests.extend(self.generate_burst_requests(priority_count, priority_bias=1))\n",
    "        \n",
    "        return requests\n",
    "\n",
    "\n",
    "# 創建請求生成器\n",
    "request_generator = RequestGenerator()\n",
    "print(\"✅ 請求生成器創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 啟動調度器並進行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 啟動調度器\n",
    "scheduler.start()\n",
    "\n",
    "print(\"🚀 開始性能測試...\")\n",
    "\n",
    "# 測試 1: 正常流量模式\n",
    "print(\"\\n📊 測試 1: 正常流量模式（30秒）\")\n",
    "normal_requests = request_generator.simulate_traffic_pattern(30, \"normal\")\n",
    "\n",
    "# 分批添加請求（模擬實時到達）\n",
    "for i, request in enumerate(normal_requests):\n",
    "    scheduler.add_request(request)\n",
    "    \n",
    "    # 每 50 個請求顯示一次進度\n",
    "    if (i + 1) % 50 == 0:\n",
    "        status = scheduler.get_queue_status()\n",
    "        print(f\"   📈 已添加 {i + 1} 個請求，佇列長度: {status['queue_size']}\")\n",
    "    \n",
    "    # 模擬請求間隔\n",
    "    time.sleep(0.001)\n",
    "\n",
    "# 等待處理完成\n",
    "time.sleep(2)\n",
    "performance_summary = scheduler.get_performance_summary()\n",
    "print(f\"✅ 正常流量測試完成:\")\n",
    "print(f\"   📊 處理請求: {performance_summary['processed_requests']}\")\n",
    "print(f\"   📊 平均延遲: {performance_summary['avg_latency']:.3f}s\")\n",
    "print(f\"   📊 平均吞吐量: {performance_summary['avg_throughput']:.1f} req/s\")\n",
    "print(f\"   📊 平均批次大小: {performance_summary['avg_batch_size']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 2: 突發流量模式\n",
    "print(\"\\n📊 測試 2: 突發流量模式（20秒）\")\n",
    "spike_requests = request_generator.simulate_traffic_pattern(20, \"spike\")\n",
    "\n",
    "start_time = time.time()\n",
    "for request in spike_requests:\n",
    "    scheduler.add_request(request)\n",
    "    time.sleep(0.0001)  # 更快的到達率\n",
    "\n",
    "# 等待處理完成\n",
    "time.sleep(3)\n",
    "spike_performance = scheduler.get_performance_summary()\n",
    "print(f\"✅ 突發流量測試完成:\")\n",
    "print(f\"   📊 處理請求: {spike_performance['processed_requests'] - performance_summary['processed_requests']}\")\n",
    "print(f\"   📊 當前平均延遲: {spike_performance['avg_latency']:.3f}s\")\n",
    "print(f\"   📊 當前平均吞吐量: {spike_performance['avg_throughput']:.1f} req/s\")\n",
    "print(f\"   📊 當前平均批次大小: {spike_performance['avg_batch_size']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 3: 優先級壓測\n",
    "print(\"\\n📊 測試 3: 高優先級請求突增（15秒）\")\n",
    "priority_requests = request_generator.simulate_traffic_pattern(15, \"priority_flood\")\n",
    "\n",
    "for request in priority_requests:\n",
    "    scheduler.add_request(request)\n",
    "    time.sleep(0.0005)\n",
    "\n",
    "# 等待處理完成\n",
    "time.sleep(2)\n",
    "priority_performance = scheduler.get_performance_summary()\n",
    "print(f\"✅ 優先級測試完成:\")\n",
    "print(f\"   📊 總處理請求: {priority_performance['processed_requests']}\")\n",
    "print(f\"   📊 過期請求: {priority_performance['expired_requests']}\")\n",
    "print(f\"   📊 總批次數: {priority_performance['total_batches']}\")\n",
    "\n",
    "# 最終狀態\n",
    "final_status = scheduler.get_queue_status()\n",
    "print(f\"\\n📈 最終佇列狀態:\")\n",
    "print(f\"   📊 剩餘請求: {final_status['queue_size']}\")\n",
    "print(f\"   📊 優先級分布: {final_status['priority_distribution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 3：性能監控與可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 實時性能監控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scheduler_performance(scheduler: SmartBatchScheduler, window_size: int = 50):\n",
    "    \"\"\"可視化調度器性能\"\"\"\n",
    "    metrics = scheduler.metrics_history[-window_size:] if scheduler.metrics_history else []\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"❌ 沒有性能數據可供可視化\")\n",
    "        return\n",
    "    \n",
    "    # 準備數據\n",
    "    timestamps = [m.timestamp for m in metrics]\n",
    "    batch_sizes = [m.batch_size for m in metrics]\n",
    "    latencies = [m.total_latency * 1000 for m in metrics]  # 轉換為毫秒\n",
    "    throughputs = [m.throughput for m in metrics]\n",
    "    wait_times = [m.wait_time * 1000 for m in metrics]  # 轉換為毫秒\n",
    "    gpu_utils = [m.gpu_utilization * 100 for m in metrics]  # 轉換為百分比\n",
    "    \n",
    "    # 創建子圖\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('智能批次調度器性能監控', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 批次大小趨勢\n",
    "    axes[0, 0].plot(timestamps, batch_sizes, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "    axes[0, 0].set_title('批次大小變化', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('批次大小')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axhline(y=np.mean(batch_sizes), color='r', linestyle='--', alpha=0.7, label=f'平均: {np.mean(batch_sizes):.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 延遲分布\n",
    "    axes[0, 1].plot(timestamps, latencies, 'g-', linewidth=2, marker='s', markersize=4)\n",
    "    axes[0, 1].set_title('總延遲變化', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('延遲 (ms)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].axhline(y=scheduler.config.target_latency * 1000, color='r', linestyle='--', alpha=0.7, label='目標延遲')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 吞吐量趨勢\n",
    "    axes[0, 2].plot(timestamps, throughputs, 'purple', linewidth=2, marker='^', markersize=4)\n",
    "    axes[0, 2].set_title('吞吐量變化', fontweight='bold')\n",
    "    axes[0, 2].set_ylabel('吞吐量 (req/s)')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].axhline(y=np.mean(throughputs), color='r', linestyle='--', alpha=0.7, label=f'平均: {np.mean(throughputs):.1f}')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # 等待時間分析\n",
    "    axes[1, 0].plot(timestamps, wait_times, 'orange', linewidth=2, marker='d', markersize=4)\n",
    "    axes[1, 0].set_title('請求等待時間', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('等待時間 (ms)')\n",
    "    axes[1, 0].set_xlabel('時間')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # GPU 利用率\n",
    "    axes[1, 1].plot(timestamps, gpu_utils, 'red', linewidth=2, marker='h', markersize=4)\n",
    "    axes[1, 1].set_title('GPU 利用率', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('利用率 (%)')\n",
    "    axes[1, 1].set_xlabel('時間')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_ylim(0, 100)\n",
    "    \n",
    "    # 延遲 vs 批次大小散點圖\n",
    "    scatter = axes[1, 2].scatter(batch_sizes, latencies, c=throughputs, \n",
    "                                cmap='viridis', s=50, alpha=0.7)\n",
    "    axes[1, 2].set_title('延遲 vs 批次大小', fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('批次大小')\n",
    "    axes[1, 2].set_ylabel('延遲 (ms)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加顏色條\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 2])\n",
    "    cbar.set_label('吞吐量 (req/s)')\n",
    "    \n",
    "    # 調整時間軸標籤\n",
    "    for ax in axes.flat:\n",
    "        if 'time' in ax.get_xlabel().lower() or len(ax.get_xticklabels()) > 10:\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 統計摘要\n",
    "    print(f\"\\n📊 性能統計摘要（最近 {len(metrics)} 個批次）:\")\n",
    "    print(f\"   📈 平均批次大小: {np.mean(batch_sizes):.2f} ± {np.std(batch_sizes):.2f}\")\n",
    "    print(f\"   ⏱️  平均延遲: {np.mean(latencies):.2f}ms ± {np.std(latencies):.2f}ms\")\n",
    "    print(f\"   🚀 平均吞吐量: {np.mean(throughputs):.1f} ± {np.std(throughputs):.1f} req/s\")\n",
    "    print(f\"   ⏳ 平均等待時間: {np.mean(wait_times):.2f}ms ± {np.std(wait_times):.2f}ms\")\n",
    "    print(f\"   💻 平均 GPU 利用率: {np.mean(gpu_utils):.1f}% ± {np.std(gpu_utils):.1f}%\")\n",
    "\n",
    "\n",
    "# 可視化性能\n",
    "plot_scheduler_performance(scheduler, window_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 優先級處理效能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_priority_performance(scheduler: SmartBatchScheduler):\n",
    "    \"\"\"分析優先級處理效能\"\"\"\n",
    "    metrics = scheduler.metrics_history\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"❌ 沒有數據可供分析\")\n",
    "        return\n",
    "    \n",
    "    # 收集優先級統計\n",
    "    priority_stats = defaultdict(lambda: {\n",
    "        'count': 0,\n",
    "        'total_latency': 0,\n",
    "        'total_wait_time': 0,\n",
    "        'batches': []\n",
    "    })\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for priority, count in metric.priority_distribution.items():\n",
    "            priority_stats[priority]['count'] += count\n",
    "            priority_stats[priority]['total_latency'] += metric.total_latency * count\n",
    "            priority_stats[priority]['total_wait_time'] += metric.wait_time * count\n",
    "            priority_stats[priority]['batches'].append(metric)\n",
    "    \n",
    "    # 計算平均值\n",
    "    priority_names = {1: 'VIP', 2: 'Premium', 3: 'Regular', 4: 'Batch', 5: 'Background'}\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('優先級處理效能分析', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 準備數據\n",
    "    priorities = sorted(priority_stats.keys())\n",
    "    priority_labels = [priority_names.get(p, f'P{p}') for p in priorities]\n",
    "    request_counts = [priority_stats[p]['count'] for p in priorities]\n",
    "    avg_latencies = [priority_stats[p]['total_latency'] / max(priority_stats[p]['count'], 1) * 1000 \n",
    "                    for p in priorities]  # 轉換為毫秒\n",
    "    avg_wait_times = [priority_stats[p]['total_wait_time'] / max(priority_stats[p]['count'], 1) * 1000 \n",
    "                     for p in priorities]  # 轉換為毫秒\n",
    "    \n",
    "    # 請求數量分布\n",
    "    colors = ['gold', 'lightcoral', 'lightblue', 'lightgreen', 'lightgray']\n",
    "    bars1 = ax1.bar(priority_labels, request_counts, color=colors[:len(priorities)], alpha=0.8)\n",
    "    ax1.set_title('各優先級請求數量', fontweight='bold')\n",
    "    ax1.set_ylabel('請求數量')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加數值標籤\n",
    "    for bar, count in zip(bars1, request_counts):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(request_counts)*0.01,\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 平均延遲對比\n",
    "    bars2 = ax2.bar(priority_labels, avg_latencies, color=colors[:len(priorities)], alpha=0.8)\n",
    "    ax2.set_title('各優先級平均延遲', fontweight='bold')\n",
    "    ax2.set_ylabel('平均延遲 (ms)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, latency in zip(bars2, avg_latencies):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_latencies)*0.01,\n",
    "                f'{latency:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 平均等待時間對比\n",
    "    bars3 = ax3.bar(priority_labels, avg_wait_times, color=colors[:len(priorities)], alpha=0.8)\n",
    "    ax3.set_title('各優先級平均等待時間', fontweight='bold')\n",
    "    ax3.set_ylabel('平均等待時間 (ms)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, wait_time in zip(bars3, avg_wait_times):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_wait_times)*0.01,\n",
    "                f'{wait_time:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 優先級效能比較（延遲降低比例）\n",
    "    if len(avg_latencies) > 1:\n",
    "        baseline_latency = avg_latencies[2] if len(avg_latencies) > 2 else avg_latencies[-1]  # 使用 Regular 或最後一個作為基準\n",
    "        latency_improvements = [(baseline_latency - lat) / baseline_latency * 100 for lat in avg_latencies]\n",
    "        \n",
    "        bars4 = ax4.bar(priority_labels, latency_improvements, \n",
    "                       color=['green' if x > 0 else 'red' for x in latency_improvements], alpha=0.8)\n",
    "        ax4.set_title('優先級延遲改善 (vs Regular)', fontweight='bold')\n",
    "        ax4.set_ylabel('延遲改善 (%)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        for bar, improvement in zip(bars4, latency_improvements):\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2, \n",
    "                    bar.get_height() + (5 if improvement > 0 else -8),\n",
    "                    f'{improvement:+.1f}%', ha='center', va='bottom' if improvement > 0 else 'top', \n",
    "                    fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 詳細統計報告\n",
    "    print(f\"\\n📊 優先級處理效能報告:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, priority in enumerate(priorities):\n",
    "        stats = priority_stats[priority]\n",
    "        name = priority_names.get(priority, f'Priority {priority}')\n",
    "        \n",
    "        print(f\"\\n🏷️  {name} (優先級 {priority}):\")\n",
    "        print(f\"   📊 處理請求數: {stats['count']}\")\n",
    "        print(f\"   ⏱️  平均延遲: {avg_latencies[i]:.2f}ms\")\n",
    "        print(f\"   ⏳ 平均等待時間: {avg_wait_times[i]:.2f}ms\")\n",
    "        print(f\"   📈 參與批次數: {len(stats['batches'])}\")\n",
    "        \n",
    "        if i > 0:  # 與前一個優先級比較\n",
    "            latency_diff = avg_latencies[i] - avg_latencies[i-1]\n",
    "            wait_diff = avg_wait_times[i] - avg_wait_times[i-1]\n",
    "            print(f\"   📉 vs 更高優先級: 延遲 {latency_diff:+.2f}ms, 等待時間 {wait_diff:+.2f}ms\")\n",
    "\n",
    "\n",
    "# 分析優先級性能\n",
    "analyze_priority_performance(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 4：自適應批次大小優化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 自適應調優算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveBatchOptimizer:\n",
    "    \"\"\"自適應批次大小優化器\"\"\"\n",
    "    \n",
    "    def __init__(self, scheduler: SmartBatchScheduler):\n",
    "        self.scheduler = scheduler\n",
    "        self.optimization_history = []\n",
    "        self.best_config = None\n",
    "        self.best_score = float('-inf')\n",
    "        \n",
    "        # 優化參數\n",
    "        self.param_ranges = {\n",
    "            'min_batch_size': (1, 8),\n",
    "            'max_batch_size': (8, 64),\n",
    "            'max_wait_time': (0.01, 0.2),\n",
    "            'target_latency': (0.01, 0.1)\n",
    "        }\n",
    "        \n",
    "        # 評分權重\n",
    "        self.score_weights = {\n",
    "            'throughput': 0.4,      # 吞吐量權重\n",
    "            'latency': 0.3,         # 延遲權重（負相關）\n",
    "            'utilization': 0.2,     # 資源利用率權重\n",
    "            'fairness': 0.1         # 公平性權重\n",
    "        }\n",
    "    \n",
    "    def calculate_performance_score(self, window_size: int = 20) -> float:\n",
    "        \"\"\"計算性能評分\"\"\"\n",
    "        recent_metrics = self.scheduler.metrics_history[-window_size:]\n",
    "        \n",
    "        if not recent_metrics:\n",
    "            return 0.0\n",
    "        \n",
    "        # 計算各項指標\n",
    "        avg_throughput = np.mean([m.throughput for m in recent_metrics])\n",
    "        avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "        avg_utilization = np.mean([m.gpu_utilization for m in recent_metrics])\n",
    "        \n",
    "        # 計算公平性（優先級間延遲的標準差，越小越公平）\n",
    "        priority_latencies = defaultdict(list)\n",
    "        for metric in recent_metrics:\n",
    "            for priority in metric.priority_distribution.keys():\n",
    "                priority_latencies[priority].append(metric.total_latency)\n",
    "        \n",
    "        if len(priority_latencies) > 1:\n",
    "            priority_avg_latencies = [np.mean(latencies) for latencies in priority_latencies.values()]\n",
    "            fairness_score = 1.0 / (1.0 + np.std(priority_avg_latencies))\n",
    "        else:\n",
    "            fairness_score = 1.0\n",
    "        \n",
    "        # 標準化分數 (0-1)\n",
    "        throughput_score = min(avg_throughput / 1000.0, 1.0)  # 假設最大吞吐量 1000 req/s\n",
    "        latency_score = max(0, 1.0 - avg_latency / 0.2)  # 延遲超過 200ms 得 0 分\n",
    "        utilization_score = avg_utilization  # GPU 利用率已經是 0-1\n",
    "        \n",
    "        # 加權平均\n",
    "        total_score = (\n",
    "            self.score_weights['throughput'] * throughput_score +\n",
    "            self.score_weights['latency'] * latency_score +\n",
    "            self.score_weights['utilization'] * utilization_score +\n",
    "            self.score_weights['fairness'] * fairness_score\n",
    "        )\n",
    "        \n",
    "        return total_score\n",
    "    \n",
    "    def generate_config_variant(self, base_config: BatchConfig, mutation_rate: float = 0.2) -> BatchConfig:\n",
    "        \"\"\"生成配置變體\"\"\"\n",
    "        new_config = BatchConfig(\n",
    "            min_batch_size=base_config.min_batch_size,\n",
    "            max_batch_size=base_config.max_batch_size,\n",
    "            max_wait_time=base_config.max_wait_time,\n",
    "            target_latency=base_config.target_latency,\n",
    "            priority_boost=base_config.priority_boost.copy()\n",
    "        )\n",
    "        \n",
    "        # 隨機變異參數\n",
    "        if random.random() < mutation_rate:\n",
    "            min_val, max_val = self.param_ranges['min_batch_size']\n",
    "            new_config.min_batch_size = random.randint(min_val, max_val)\n",
    "        \n",
    "        if random.random() < mutation_rate:\n",
    "            min_val, max_val = self.param_ranges['max_batch_size']\n",
    "            new_config.max_batch_size = random.randint(min_val, max_val)\n",
    "            # 確保 max >= min\n",
    "            new_config.max_batch_size = max(new_config.max_batch_size, new_config.min_batch_size)\n",
    "        \n",
    "        if random.random() < mutation_rate:\n",
    "            min_val, max_val = self.param_ranges['max_wait_time']\n",
    "            new_config.max_wait_time = random.uniform(min_val, max_val)\n",
    "        \n",
    "        if random.random() < mutation_rate:\n",
    "            min_val, max_val = self.param_ranges['target_latency']\n",
    "            new_config.target_latency = random.uniform(min_val, max_val)\n",
    "        \n",
    "        return new_config\n",
    "    \n",
    "    def optimize_batch_config(self, iterations: int = 5, test_duration: int = 10):\n",
    "        \"\"\"優化批次配置\"\"\"\n",
    "        print(f\"🔧 開始自適應批次配置優化（{iterations} 次迭代）\")\n",
    "        \n",
    "        current_config = self.scheduler.config\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            print(f\"\\n🔄 迭代 {iteration + 1}/{iterations}\")\n",
    "            \n",
    "            # 生成新配置\n",
    "            if iteration == 0:\n",
    "                test_config = current_config\n",
    "            else:\n",
    "                test_config = self.generate_config_variant(current_config)\n",
    "            \n",
    "            print(f\"   🔧 測試配置: min_batch={test_config.min_batch_size}, \"\n",
    "                  f\"max_batch={test_config.max_batch_size}, \"\n",
    "                  f\"max_wait={test_config.max_wait_time:.3f}s, \"\n",
    "                  f\"target_latency={test_config.target_latency:.3f}s\")\n",
    "            \n",
    "            # 應用新配置\n",
    "            old_config = self.scheduler.config\n",
    "            self.scheduler.config = test_config\n",
    "            \n",
    "            # 清空歷史記錄以獲得純淨測試\n",
    "            metrics_backup = self.scheduler.metrics_history.copy()\n",
    "            self.scheduler.metrics_history.clear()\n",
    "            \n",
    "            try:\n",
    "                # 生成測試流量\n",
    "                test_requests = request_generator.simulate_traffic_pattern(test_duration, \"normal\")\n",
    "                \n",
    "                # 添加請求\n",
    "                for request in test_requests:\n",
    "                    self.scheduler.add_request(request)\n",
    "                    time.sleep(0.001)\n",
    "                \n",
    "                # 等待處理完成\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # 計算性能分數\n",
    "                score = self.calculate_performance_score()\n",
    "                \n",
    "                print(f\"   📊 性能分數: {score:.4f}\")\n",
    "                \n",
    "                # 記錄結果\n",
    "                self.optimization_history.append({\n",
    "                    'iteration': iteration + 1,\n",
    "                    'config': test_config,\n",
    "                    'score': score,\n",
    "                    'metrics_count': len(self.scheduler.metrics_history)\n",
    "                })\n",
    "                \n",
    "                # 更新最佳配置\n",
    "                if score > self.best_score:\n",
    "                    self.best_score = score\n",
    "                    self.best_config = test_config\n",
    "                    current_config = test_config\n",
    "                    print(f\"   ✅ 發現更佳配置！新最佳分數: {score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   📉 配置未改善，保持當前配置\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ 測試失敗: {str(e)}\")\n",
    "                # 恢復原配置\n",
    "                self.scheduler.config = old_config\n",
    "            \n",
    "            # 恢復部分歷史記錄\n",
    "            self.scheduler.metrics_history = metrics_backup + self.scheduler.metrics_history\n",
    "        \n",
    "        # 應用最佳配置\n",
    "        if self.best_config:\n",
    "            self.scheduler.config = self.best_config\n",
    "            print(f\"\\n🎯 優化完成！應用最佳配置:\")\n",
    "            print(f\"   📊 最佳分數: {self.best_score:.4f}\")\n",
    "            print(f\"   🔧 最佳配置: min_batch={self.best_config.min_batch_size}, \"\n",
    "                  f\"max_batch={self.best_config.max_batch_size}, \"\n",
    "                  f\"max_wait={self.best_config.max_wait_time:.3f}s, \"\n",
    "                  f\"target_latency={self.best_config.target_latency:.3f}s\")\n",
    "    \n",
    "    def plot_optimization_history(self):\n",
    "        \"\"\"可視化優化歷史\"\"\"\n",
    "        if not self.optimization_history:\n",
    "            print(\"❌ 沒有優化歷史數據\")\n",
    "            return\n",
    "        \n",
    "        iterations = [h['iteration'] for h in self.optimization_history]\n",
    "        scores = [h['score'] for h in self.optimization_history]\n",
    "        min_batches = [h['config'].min_batch_size for h in self.optimization_history]\n",
    "        max_batches = [h['config'].max_batch_size for h in self.optimization_history]\n",
    "        wait_times = [h['config'].max_wait_time * 1000 for h in self.optimization_history]  # 轉換為毫秒\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('自適應批次配置優化歷史', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 性能分數趨勢\n",
    "        ax1.plot(iterations, scores, 'b-o', linewidth=2, markersize=8)\n",
    "        ax1.set_title('性能分數變化', fontweight='bold')\n",
    "        ax1.set_xlabel('迭代次數')\n",
    "        ax1.set_ylabel('性能分數')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 標記最佳點\n",
    "        best_idx = scores.index(max(scores))\n",
    "        ax1.scatter(iterations[best_idx], scores[best_idx], color='red', s=100, zorder=5)\n",
    "        ax1.annotate(f'最佳: {scores[best_idx]:.4f}', \n",
    "                    xy=(iterations[best_idx], scores[best_idx]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "        \n",
    "        # 批次大小變化\n",
    "        ax2.plot(iterations, min_batches, 'g-s', label='最小批次', linewidth=2, markersize=6)\n",
    "        ax2.plot(iterations, max_batches, 'r-^', label='最大批次', linewidth=2, markersize=6)\n",
    "        ax2.set_title('批次大小參數變化', fontweight='bold')\n",
    "        ax2.set_xlabel('迭代次數')\n",
    "        ax2.set_ylabel('批次大小')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 等待時間變化\n",
    "        ax3.plot(iterations, wait_times, 'purple', marker='d', linewidth=2, markersize=6)\n",
    "        ax3.set_title('最大等待時間變化', fontweight='bold')\n",
    "        ax3.set_xlabel('迭代次數')\n",
    "        ax3.set_ylabel('等待時間 (ms)')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 參數相關性熱圖\n",
    "        param_data = pd.DataFrame({\n",
    "            'score': scores,\n",
    "            'min_batch': min_batches,\n",
    "            'max_batch': max_batches,\n",
    "            'wait_time': wait_times\n",
    "        })\n",
    "        \n",
    "        correlation_matrix = param_data.corr()\n",
    "        im = ax4.imshow(correlation_matrix, cmap='RdYlBu', aspect='auto', vmin=-1, vmax=1)\n",
    "        ax4.set_title('參數相關性', fontweight='bold')\n",
    "        ax4.set_xticks(range(len(correlation_matrix.columns)))\n",
    "        ax4.set_yticks(range(len(correlation_matrix.columns)))\n",
    "        ax4.set_xticklabels(correlation_matrix.columns, rotation=45)\n",
    "        ax4.set_yticklabels(correlation_matrix.columns)\n",
    "        \n",
    "        # 添加相關係數文字\n",
    "        for i in range(len(correlation_matrix)):\n",
    "            for j in range(len(correlation_matrix.columns)):\n",
    "                text = ax4.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax4)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 創建優化器\n",
    "optimizer = AdaptiveBatchOptimizer(scheduler)\n",
    "print(\"✅ 自適應批次優化器創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 執行自適應優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行優化\n",
    "optimizer.optimize_batch_config(iterations=6, test_duration=8)\n",
    "\n",
    "# 可視化優化過程\n",
    "optimizer.plot_optimization_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 優化後性能驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用優化後的配置進行最終驗證測試\n",
    "print(\"🧪 優化後配置驗證測試\")\n",
    "\n",
    "# 生成大量測試請求\n",
    "validation_requests = request_generator.simulate_traffic_pattern(30, \"spike\")\n",
    "\n",
    "print(f\"📊 驗證測試開始，共 {len(validation_requests)} 個請求\")\n",
    "\n",
    "# 清空之前的指標\n",
    "validation_start_metrics = len(scheduler.metrics_history)\n",
    "\n",
    "# 添加驗證請求\n",
    "for i, request in enumerate(validation_requests):\n",
    "    scheduler.add_request(request)\n",
    "    time.sleep(0.0005)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"   📈 已添加 {i + 1}/{len(validation_requests)} 個請求\")\n",
    "\n",
    "# 等待處理完成\n",
    "print(\"⏳ 等待處理完成...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# 獲取驗證結果\n",
    "final_performance = scheduler.get_performance_summary()\n",
    "validation_metrics = scheduler.metrics_history[validation_start_metrics:]\n",
    "\n",
    "print(f\"\\n✅ 優化後性能驗證結果:\")\n",
    "print(f\"   📊 處理請求: {len(validation_requests)}\")\n",
    "print(f\"   📊 實際處理: {final_performance['processed_requests'] - (validation_start_metrics * np.mean([m.batch_size for m in scheduler.metrics_history[:validation_start_metrics]] if validation_start_metrics > 0 else [1]))}\")\n",
    "print(f\"   📊 過期請求: {final_performance['expired_requests']}\")\n",
    "print(f\"   ⏱️  平均延遲: {final_performance['avg_latency']:.3f}s\")\n",
    "print(f\"   🚀 平均吞吐量: {final_performance['avg_throughput']:.1f} req/s\")\n",
    "print(f\"   📏 平均批次大小: {final_performance['avg_batch_size']:.1f}\")\n",
    "print(f\"   💻 平均 GPU 利用率: {final_performance['avg_gpu_utilization']:.1f}%\")\n",
    "\n",
    "# 計算最終性能分數\n",
    "final_score = optimizer.calculate_performance_score(50)\n",
    "print(f\"   🎯 最終性能分數: {final_score:.4f}\")\n",
    "\n",
    "# 比較優化前後\n",
    "if optimizer.optimization_history:\n",
    "    initial_score = optimizer.optimization_history[0]['score']\n",
    "    improvement = ((final_score - initial_score) / initial_score) * 100\n",
    "    print(f\"   📈 性能改善: {improvement:+.1f}%\")\n",
    "\n",
    "# 可視化最終性能\n",
    "plot_scheduler_performance(scheduler, window_size=min(50, len(validation_metrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 實驗 5：負載均衡與故障恢復"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 多調度器負載均衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadBalancer:\n",
    "    \"\"\"負載均衡器\"\"\"\n",
    "    \n",
    "    def __init__(self, schedulers: List[SmartBatchScheduler]):\n",
    "        self.schedulers = schedulers\n",
    "        self.scheduler_weights = [1.0] * len(schedulers)  # 初始權重相等\n",
    "        self.request_counts = [0] * len(schedulers)\n",
    "        self.health_status = [True] * len(schedulers)\n",
    "        \n",
    "        # 負載均衡策略\n",
    "        self.strategies = {\n",
    "            'round_robin': self._round_robin,\n",
    "            'least_connections': self._least_connections,\n",
    "            'weighted_performance': self._weighted_performance,\n",
    "            'priority_aware': self._priority_aware\n",
    "        }\n",
    "        \n",
    "        self.current_strategy = 'weighted_performance'\n",
    "        self.round_robin_index = 0\n",
    "    \n",
    "    def _round_robin(self, request: InferenceRequest) -> int:\n",
    "        \"\"\"輪詢策略\"\"\"\n",
    "        # 跳過不健康的調度器\n",
    "        attempts = 0\n",
    "        while attempts < len(self.schedulers):\n",
    "            if self.health_status[self.round_robin_index]:\n",
    "                selected = self.round_robin_index\n",
    "                self.round_robin_index = (self.round_robin_index + 1) % len(self.schedulers)\n",
    "                return selected\n",
    "            self.round_robin_index = (self.round_robin_index + 1) % len(self.schedulers)\n",
    "            attempts += 1\n",
    "        return 0  # 備用方案\n",
    "    \n",
    "    def _least_connections(self, request: InferenceRequest) -> int:\n",
    "        \"\"\"最少連接策略\"\"\"\n",
    "        min_connections = float('inf')\n",
    "        selected_idx = 0\n",
    "        \n",
    "        for i, scheduler in enumerate(self.schedulers):\n",
    "            if not self.health_status[i]:\n",
    "                continue\n",
    "            \n",
    "            queue_size = scheduler.get_queue_status()['queue_size']\n",
    "            if queue_size < min_connections:\n",
    "                min_connections = queue_size\n",
    "                selected_idx = i\n",
    "        \n",
    "        return selected_idx\n",
    "    \n",
    "    def _weighted_performance(self, request: InferenceRequest) -> int:\n",
    "        \"\"\"基於性能權重的策略\"\"\"\n",
    "        # 更新權重基於最近性能\n",
    "        self._update_weights()\n",
    "        \n",
    "        # 加權隨機選擇\n",
    "        healthy_indices = [i for i, healthy in enumerate(self.health_status) if healthy]\n",
    "        if not healthy_indices:\n",
    "            return 0\n",
    "        \n",
    "        healthy_weights = [self.scheduler_weights[i] for i in healthy_indices]\n",
    "        total_weight = sum(healthy_weights)\n",
    "        \n",
    "        if total_weight == 0:\n",
    "            return random.choice(healthy_indices)\n",
    "        \n",
    "        rand_val = random.uniform(0, total_weight)\n",
    "        cumulative = 0\n",
    "        \n",
    "        for i, idx in enumerate(healthy_indices):\n",
    "            cumulative += healthy_weights[i]\n",
    "            if rand_val <= cumulative:\n",
    "                return idx\n",
    "        \n",
    "        return healthy_indices[-1]\n",
    "    \n",
    "    def _priority_aware(self, request: InferenceRequest) -> int:\n",
    "        \"\"\"優先級感知策略\"\"\"\n",
    "        # 高優先級請求選擇性能最好的調度器\n",
    "        if request.priority <= 2:\n",
    "            best_scheduler = -1\n",
    "            best_score = -1\n",
    "            \n",
    "            for i, scheduler in enumerate(self.schedulers):\n",
    "                if not self.health_status[i]:\n",
    "                    continue\n",
    "                \n",
    "                # 計算調度器分數（低延遲 + 低佇列長度）\n",
    "                recent_metrics = scheduler.metrics_history[-5:]\n",
    "                if recent_metrics:\n",
    "                    avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "                    queue_size = scheduler.get_queue_status()['queue_size']\n",
    "                    score = 1.0 / (avg_latency + 0.001) - queue_size * 0.01\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_scheduler = i\n",
    "            \n",
    "            return best_scheduler if best_scheduler >= 0 else 0\n",
    "        else:\n",
    "            # 低優先級請求使用負載均衡\n",
    "            return self._least_connections(request)\n",
    "    \n",
    "    def _update_weights(self):\n",
    "        \"\"\"更新調度器權重\"\"\"\n",
    "        for i, scheduler in enumerate(self.schedulers):\n",
    "            if not self.health_status[i]:\n",
    "                self.scheduler_weights[i] = 0.0\n",
    "                continue\n",
    "            \n",
    "            recent_metrics = scheduler.metrics_history[-10:]\n",
    "            if recent_metrics:\n",
    "                avg_throughput = np.mean([m.throughput for m in recent_metrics])\n",
    "                avg_latency = np.mean([m.total_latency for m in recent_metrics])\n",
    "                \n",
    "                # 權重 = 吞吐量 / 延遲\n",
    "                weight = avg_throughput / max(avg_latency, 0.001)\n",
    "                self.scheduler_weights[i] = weight\n",
    "            else:\n",
    "                self.scheduler_weights[i] = 1.0\n",
    "    \n",
    "    def route_request(self, request: InferenceRequest) -> bool:\n",
    "        \"\"\"路由請求到合適的調度器\"\"\"\n",
    "        strategy_func = self.strategies.get(self.current_strategy, self._round_robin)\n",
    "        selected_idx = strategy_func(request)\n",
    "        \n",
    "        if 0 <= selected_idx < len(self.schedulers) and self.health_status[selected_idx]:\n",
    "            success = self.schedulers[selected_idx].add_request(request)\n",
    "            if success:\n",
    "                self.request_counts[selected_idx] += 1\n",
    "            return success\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def check_health(self):\n",
    "        \"\"\"檢查調度器健康狀態\"\"\"\n",
    "        for i, scheduler in enumerate(self.schedulers):\n",
    "            try:\n",
    "                # 簡單健康檢查：檢查佇列狀態\n",
    "                status = scheduler.get_queue_status()\n",
    "                \n",
    "                # 如果佇列過長或最老請求過久，標記為不健康\n",
    "                queue_too_long = status['queue_size'] > 1000\n",
    "                oldest_too_old = status['oldest_request_age'] > 60  # 60秒\n",
    "                \n",
    "                self.health_status[i] = not (queue_too_long or oldest_too_old)\n",
    "                \n",
    "            except Exception:\n",
    "                self.health_status[i] = False\n",
    "    \n",
    "    def get_load_distribution(self) -> Dict[str, Any]:\n",
    "        \"\"\"獲取負載分布狀態\"\"\"\n",
    "        total_requests = sum(self.request_counts)\n",
    "        \n",
    "        distribution = []\n",
    "        for i, scheduler in enumerate(self.schedulers):\n",
    "            queue_status = scheduler.get_queue_status()\n",
    "            performance = scheduler.get_performance_summary()\n",
    "            \n",
    "            distribution.append({\n",
    "                'scheduler_id': i,\n",
    "                'healthy': self.health_status[i],\n",
    "                'weight': self.scheduler_weights[i],\n",
    "                'requests_routed': self.request_counts[i],\n",
    "                'request_percentage': (self.request_counts[i] / max(total_requests, 1)) * 100,\n",
    "                'queue_size': queue_status['queue_size'],\n",
    "                'avg_latency': performance.get('avg_latency', 0),\n",
    "                'avg_throughput': performance.get('avg_throughput', 0)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'strategy': self.current_strategy,\n",
    "            'total_requests': total_requests,\n",
    "            'healthy_schedulers': sum(self.health_status),\n",
    "            'distribution': distribution\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✅ 負載均衡器實現完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 創建多調度器環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止原調度器\n",
    "scheduler.stop()\n",
    "\n",
    "# 創建多個調度器實例\n",
    "print(\"🔧 創建多調度器環境...\")\n",
    "\n",
    "# 調度器配置（每個略有不同以模擬真實環境）\n",
    "scheduler_configs = [\n",
    "    BatchConfig(min_batch_size=1, max_batch_size=8, max_wait_time=0.03, target_latency=0.025),\n",
    "    BatchConfig(min_batch_size=2, max_batch_size=16, max_wait_time=0.05, target_latency=0.035),\n",
    "    BatchConfig(min_batch_size=1, max_batch_size=12, max_wait_time=0.04, target_latency=0.030),\n",
    "]\n",
    "\n",
    "# 創建調度器實例\n",
    "schedulers = []\n",
    "for i, config in enumerate(scheduler_configs):\n",
    "    sched = SmartBatchScheduler(config, model_name=f\"text_classifier_replica_{i}\")\n",
    "    sched.start()\n",
    "    schedulers.append(sched)\n",
    "    print(f\"   ✅ 調度器 {i} 已啟動 (max_batch={config.max_batch_size})\")\n",
    "\n",
    "# 創建負載均衡器\n",
    "load_balancer = LoadBalancer(schedulers)\n",
    "print(f\"✅ 負載均衡器已創建，管理 {len(schedulers)} 個調度器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 負載均衡測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試不同負載均衡策略\n",
    "strategies_to_test = ['round_robin', 'least_connections', 'weighted_performance', 'priority_aware']\n",
    "\n",
    "strategy_results = {}\n",
    "\n",
    "for strategy in strategies_to_test:\n",
    "    print(f\"\\n🧪 測試負載均衡策略: {strategy}\")\n",
    "    \n",
    "    # 重置統計\n",
    "    load_balancer.current_strategy = strategy\n",
    "    load_balancer.request_counts = [0] * len(schedulers)\n",
    "    \n",
    "    # 生成測試請求\n",
    "    test_requests = request_generator.simulate_traffic_pattern(20, \"normal\")\n",
    "    \n",
    "    # 路由請求\n",
    "    successful_routes = 0\n",
    "    for request in test_requests:\n",
    "        if load_balancer.route_request(request):\n",
    "            successful_routes += 1\n",
    "        time.sleep(0.001)\n",
    "    \n",
    "    # 等待處理\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # 檢查健康狀態\n",
    "    load_balancer.check_health()\n",
    "    \n",
    "    # 獲取分布狀態\n",
    "    distribution = load_balancer.get_load_distribution()\n",
    "    strategy_results[strategy] = distribution\n",
    "    \n",
    "    print(f\"   📊 成功路由: {successful_routes}/{len(test_requests)} 個請求\")\n",
    "    print(f\"   🏥 健康調度器: {distribution['healthy_schedulers']}/{len(schedulers)}\")\n",
    "    \n",
    "    for i, sched_info in enumerate(distribution['distribution']):\n",
    "        print(f\"   📈 調度器 {i}: {sched_info['requests_routed']} 請求 \"\n",
    "              f\"({sched_info['request_percentage']:.1f}%), \"\n",
    "              f\"佇列: {sched_info['queue_size']}, \"\n",
    "              f\"健康: {'✅' if sched_info['healthy'] else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 可視化負載均衡效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_load_balancing_results(strategy_results: Dict[str, Dict]):\n",
    "    \"\"\"可視化負載均衡結果\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('負載均衡策略效果對比', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    strategies = list(strategy_results.keys())\n",
    "    num_schedulers = len(strategy_results[strategies[0]]['distribution'])\n",
    "    \n",
    "    # 請求分布對比\n",
    "    ax1 = axes[0, 0]\n",
    "    x = np.arange(num_schedulers)\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, strategy in enumerate(strategies):\n",
    "        percentages = [sched['request_percentage'] for sched in strategy_results[strategy]['distribution']]\n",
    "        ax1.bar(x + i * width, percentages, width, label=strategy, alpha=0.8)\n",
    "    \n",
    "    ax1.set_title('請求分布比例 (%)', fontweight='bold')\n",
    "    ax1.set_xlabel('調度器 ID')\n",
    "    ax1.set_ylabel('請求比例 (%)')\n",
    "    ax1.set_xticks(x + width * 1.5)\n",
    "    ax1.set_xticklabels([f'Sched {i}' for i in range(num_schedulers)])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 平均延遲對比\n",
    "    ax2 = axes[0, 1]\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        latencies = [sched['avg_latency'] * 1000 for sched in strategy_results[strategy]['distribution']]\n",
    "        ax2.bar(x + i * width, latencies, width, label=strategy, alpha=0.8)\n",
    "    \n",
    "    ax2.set_title('平均延遲對比 (ms)', fontweight='bold')\n",
    "    ax2.set_xlabel('調度器 ID')\n",
    "    ax2.set_ylabel('平均延遲 (ms)')\n",
    "    ax2.set_xticks(x + width * 1.5)\n",
    "    ax2.set_xticklabels([f'Sched {i}' for i in range(num_schedulers)])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 吞吐量對比\n",
    "    ax3 = axes[1, 0]\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        throughputs = [sched['avg_throughput'] for sched in strategy_results[strategy]['distribution']]\n",
    "        ax3.bar(x + i * width, throughputs, width, label=strategy, alpha=0.8)\n",
    "    \n",
    "    ax3.set_title('平均吞吐量對比 (req/s)', fontweight='bold')\n",
    "    ax3.set_xlabel('調度器 ID')\n",
    "    ax3.set_ylabel('吞吐量 (req/s)')\n",
    "    ax3.set_xticks(x + width * 1.5)\n",
    "    ax3.set_xticklabels([f'Sched {i}' for i in range(num_schedulers)])\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 負載均衡度分析（標準差）\n",
    "    ax4 = axes[1, 1]\n",
    "    balance_scores = []\n",
    "    strategy_names = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        percentages = [sched['request_percentage'] for sched in strategy_results[strategy]['distribution']]\n",
    "        # 計算分布的標準差（越小越均衡）\n",
    "        balance_score = np.std(percentages)\n",
    "        balance_scores.append(balance_score)\n",
    "        strategy_names.append(strategy)\n",
    "    \n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "    bars = ax4.bar(strategy_names, balance_scores, color=colors[:len(strategies)], alpha=0.8)\n",
    "    ax4.set_title('負載均衡度 (標準差)', fontweight='bold')\n",
    "    ax4.set_ylabel('分布標準差 (越小越均衡)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加數值標籤\n",
    "    for bar, score in zip(bars, balance_scores):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(balance_scores)*0.01,\n",
    "                f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 策略效果摘要\n",
    "    print(f\"\\n📊 負載均衡策略效果摘要:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        result = strategy_results[strategy]\n",
    "        percentages = [sched['request_percentage'] for sched in result['distribution']]\n",
    "        latencies = [sched['avg_latency'] * 1000 for sched in result['distribution']]\n",
    "        throughputs = [sched['avg_throughput'] for sched in result['distribution']]\n",
    "        \n",
    "        balance_score = np.std(percentages)\n",
    "        avg_latency = np.mean(latencies)\n",
    "        total_throughput = np.sum(throughputs)\n",
    "        \n",
    "        print(f\"\\n🔧 {strategy.replace('_', ' ').title()}:\")\n",
    "        print(f\"   ⚖️  均衡度: {balance_score:.2f} (越小越好)\")\n",
    "        print(f\"   ⏱️  平均延遲: {avg_latency:.2f}ms\")\n",
    "        print(f\"   🚀 總吞吐量: {total_throughput:.1f} req/s\")\n",
    "        print(f\"   🏥 健康率: {result['healthy_schedulers']}/{len(schedulers)}\")\n",
    "\n",
    "\n",
    "# 可視化結果\n",
    "plot_load_balancing_results(strategy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 實驗總結與最佳實踐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止所有調度器\n",
    "for i, sched in enumerate(schedulers):\n",
    "    sched.stop()\n",
    "    print(f\"⏹️  調度器 {i} 已停止\")\n",
    "\n",
    "# 導出實驗結果\n",
    "def export_dynamic_batching_results():\n",
    "    \"\"\"導出動態批次處理實驗結果\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 綜合結果\n",
    "    experiment_results = {\n",
    "        \"experiment_name\": \"Dynamic Batching Advanced\",\n",
    "        \"timestamp\": timestamp,\n",
    "        \"optimization_history\": optimizer.optimization_history,\n",
    "        \"best_config\": {\n",
    "            \"min_batch_size\": optimizer.best_config.min_batch_size if optimizer.best_config else None,\n",
    "            \"max_batch_size\": optimizer.best_config.max_batch_size if optimizer.best_config else None,\n",
    "            \"max_wait_time\": optimizer.best_config.max_wait_time if optimizer.best_config else None,\n",
    "            \"target_latency\": optimizer.best_config.target_latency if optimizer.best_config else None,\n",
    "        },\n",
    "        \"best_score\": optimizer.best_score,\n",
    "        \"load_balancing_results\": strategy_results,\n",
    "        \"scheduler_metrics\": [\n",
    "            {\n",
    "                \"scheduler_id\": i,\n",
    "                \"total_batches\": len(sched.metrics_history),\n",
    "                \"performance_summary\": sched.get_performance_summary()\n",
    "            }\n",
    "            for i, sched in enumerate(schedulers)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 導出 JSON 文件\n",
    "    results_file = f\"{EXPERIMENT_DIR}/dynamic_batching_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(experiment_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"📄 實驗結果已導出: {results_file}\")\n",
    "    return results_file\n",
    "\n",
    "\n",
    "# 導出結果\n",
    "results_file = export_dynamic_batching_results()\n",
    "\n",
    "# 最佳實踐總結\n",
    "best_practices_summary = \"\"\"\n",
    "🎯 高級動態批次處理最佳實踐總結\n",
    "\n",
    "🔧 智能調度策略:\n",
    "   ✅ 自適應批次大小調整\n",
    "   ✅ 優先級感知的等待時間策略\n",
    "   ✅ 實時性能監控和反饋調整\n",
    "   ✅ 基於負載的動態配置優化\n",
    "\n",
    "⚖️ 負載均衡策略選擇:\n",
    "   📊 輪詢策略: 簡單均勻，適合同質化環境\n",
    "   📊 最少連接: 動態均衡，適合異質化負載\n",
    "   📊 權重性能: 智能分配，適合性能差異化\n",
    "   📊 優先級感知: 服務質量保證，適合SLA要求\n",
    "\n",
    "🎯 關鍵性能指標:\n",
    "   ⏱️  延遲優化: 目標 < 50ms P99 延遲\n",
    "   🚀 吞吐量: 最大化 GPU 利用率\n",
    "   ⚖️  公平性: 優先級間合理的延遲差異\n",
    "   💻 資源效率: GPU 利用率 > 80%\n",
    "\n",
    "🛡️ 故障恢復機制:\n",
    "   ✅ 健康檢查和自動故障轉移\n",
    "   ✅ 請求超時和過期處理\n",
    "   ✅ 佇列長度監控和限制\n",
    "   ✅ 性能退化檢測和恢復\n",
    "\n",
    "📈 優化建議:\n",
    "   🔄 定期重新評估和調優批次參數\n",
    "   📊 基於業務模式調整優先級策略\n",
    "   🎯 根據SLA要求設定目標延遲\n",
    "   💡 使用A/B測試驗證優化效果\n",
    "\n",
    "🚀 生產部署考慮:\n",
    "   📦 容器化部署支持水平擴展\n",
    "   📊 完整的監控和告警體系\n",
    "   🔧 配置熱更新能力\n",
    "   📝 詳細的性能日誌記錄\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices_summary)\n",
    "\n",
    "print(f\"\\n✅ Lab 2.4.2 高級動態批次處理實驗完成！\")\n",
    "print(f\"📊 實驗數據已保存至: {EXPERIMENT_DIR}\")\n",
    "print(f\"🎯 最佳配置已確定並可用於生產環境\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📖 總結\n",
    "\n",
    "本實驗完成了高級動態批次處理與智能調度的完整實現：\n",
    "\n",
    "### 🎯 實驗成果\n",
    "1. **智能調度器** - 實現了自適應批次大小和優先級感知調度\n",
    "2. **性能優化** - 通過自動調優實現延遲與吞吐量的最佳平衡\n",
    "3. **負載均衡** - 支援多種策略的智能請求路由\n",
    "4. **故障恢復** - 具備健康檢查和自動故障轉移能力\n",
    "\n",
    "### 🔧 關鍵技術特點\n",
    "- 基於優先級的動態等待時間調整\n",
    "- 實時性能監控和反饋式參數調優\n",
    "- 多調度器協同工作和負載分散\n",
    "- 自動化故障檢測和恢復機制\n",
    "\n",
    "### 🚀 實際應用價值\n",
    "1. **提升系統吞吐量** - 智能批次處理提高GPU利用率\n",
    "2. **降低服務延遲** - 優先級調度保證關鍵請求響應時間\n",
    "3. **增強系統穩定性** - 負載均衡和故障恢復提高可用性\n",
    "4. **簡化運維管理** - 自動化調優減少人工干預\n",
    "\n",
    "### 💡 學習要點\n",
    "- 動態批次處理需要平衡延遲與吞吐量\n",
    "- 優先級調度是企業級服務的關鍵特性\n",
    "- 負載均衡策略的選擇影響整體性能\n",
    "- 監控和可觀測性是優化的基礎\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 恭喜完成 Lab 2.4.2！**\n",
    "\n",
    "您已經掌握了高級動態批次處理技術，可以構建高效、可靠的智能推理調度系統。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}