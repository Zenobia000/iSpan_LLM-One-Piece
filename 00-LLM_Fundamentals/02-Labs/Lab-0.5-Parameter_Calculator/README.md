# Lab 0.5: LLMåƒæ•¸é‡èˆ‡è³‡æºè¨ˆç®—å™¨

## å¯¦é©—ç›®æ¨™

é–‹ç™¼ä¸€å€‹å®Œæ•´çš„LLMè³‡æºä¼°ç®—å·¥å…·ï¼Œå¯¦è¸åƒæ•¸é‡è¨ˆç®—ã€FLOPsä¼°ç®—å’Œéƒ¨ç½²è³‡æºè¦åŠƒï¼Œå»ºç«‹ç²¾ç¢ºçš„è³‡æºè©•ä¼°èƒ½åŠ›ã€‚

## å­¸ç¿’æˆæœ

å®Œæˆæœ¬å¯¦é©—å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š
- ç²¾ç¢ºè¨ˆç®—Transformeræ¨¡å‹çš„åƒæ•¸é‡
- ä¼°ç®—è¨“ç·´å’Œæ¨ç†çš„è¨ˆç®—è¤‡é›œåº¦
- é€²è¡Œè¨˜æ†¶é«”éœ€æ±‚å’Œæˆæœ¬åˆ†æ
- æ‡‰ç”¨ç¸®æ”¾æ³•å‰‡é€²è¡Œæ€§èƒ½é æ¸¬

## å¯¦é©—ç’°å¢ƒè¦æ±‚

### ç¡¬é«”è¦æ±‚
- RAMï¼š8GB+ç³»çµ±è¨˜æ†¶é«”
- å­˜å„²ï¼š5GBå¯ç”¨ç©ºé–“

### è»Ÿé«”è¦æ±‚
- Python 3.8+
- å·²æ¿€æ´»çš„poetryè™›æ“¬ç’°å¢ƒ

## ä¸»è¦å¯¦é©—å…§å®¹

### LLMè³‡æºè¨ˆç®—å™¨å¯¦ç¾

```python
# llm_resource_calculator.py
"""
LLMè³‡æºè¨ˆç®—å™¨
æä¾›åƒæ•¸é‡ã€FLOPsã€è¨˜æ†¶é«”éœ€æ±‚ç­‰å…¨é¢è¨ˆç®—åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple
import streamlit as st
from datetime import datetime

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®é¡"""
    name: str
    vocab_size: int
    max_seq_len: int
    d_model: int
    n_layers: int
    n_heads: int
    d_ff: Optional[int] = None
    use_bias: bool = False
    tie_embeddings: bool = True
    attention_type: str = "mha"  # mha, mqa, gqa
    ffn_type: str = "standard"  # standard, gated, moe

    def __post_init__(self):
        if self.d_ff is None:
            self.d_ff = 4 * self.d_model

class LLMResourceCalculator:
    """LLMè³‡æºè¨ˆç®—å™¨"""

    def __init__(self):
        self.precision_bytes = {
            'fp32': 4, 'fp16': 2, 'bf16': 2,
            'int8': 1, 'int4': 0.5, 'nf4': 0.5
        }

    def calculate_parameters(self, config: ModelConfig) -> Dict:
        """è¨ˆç®—æ¨¡å‹åƒæ•¸é‡"""

        # 1. Embeddingå±¤åƒæ•¸
        token_embedding = config.vocab_size * config.d_model
        position_embedding = config.max_seq_len * config.d_model

        if config.tie_embeddings:
            embedding_params = token_embedding + position_embedding
            output_params = 0
        else:
            embedding_params = token_embedding + position_embedding
            output_params = config.vocab_size * config.d_model

        # 2. Attentionå±¤åƒæ•¸
        if config.attention_type == "mha":
            attention_params = 4 * config.d_model * config.d_model  # Q,K,V,O
        elif config.attention_type == "mqa":
            d_head = config.d_model // config.n_heads
            attention_params = 3 * config.d_model * config.d_model + 2 * config.d_model * d_head
        elif config.attention_type == "gqa":
            d_head = config.d_model // config.n_heads
            n_kv_heads = max(1, config.n_heads // 4)  # å‡è¨­KVé ­æ•¸ç‚ºQueryé ­æ•¸çš„1/4
            attention_params = 3 * config.d_model * config.d_model + 2 * config.d_model * n_kv_heads * d_head
        else:
            raise ValueError(f"Unsupported attention type: {config.attention_type}")

        # 3. FFNå±¤åƒæ•¸
        if config.ffn_type == "standard":
            ffn_params = 2 * config.d_model * config.d_ff
        elif config.ffn_type == "gated":
            ffn_params = 3 * config.d_model * config.d_ff
        elif config.ffn_type == "moe":
            n_experts = 8  # å‡è¨­8å€‹å°ˆå®¶
            routing_params = config.d_model * n_experts
            expert_params = n_experts * 2 * config.d_model * config.d_ff
            ffn_params = routing_params + expert_params
        else:
            raise ValueError(f"Unsupported FFN type: {config.ffn_type}")

        # 4. LayerNormåƒæ•¸
        layernorm_params = 2 * 2 * config.d_model  # æ¯å±¤2å€‹LayerNormï¼Œæ¯å€‹2å€‹åƒæ•¸

        # 5. åç½®é …ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        bias_params = 0
        if config.use_bias:
            bias_params = 4 * config.d_model + 2 * config.d_ff  # æ³¨æ„åŠ›å’ŒFFNçš„åç½®

        # 6. æ¯å±¤ç¸½åƒæ•¸
        per_layer_params = attention_params + ffn_params + layernorm_params + bias_params

        # 7. ç¸½åƒæ•¸
        total_params = (
            embedding_params +
            config.n_layers * per_layer_params +
            output_params +
            2 * config.d_model  # æœ€çµ‚LayerNorm
        )

        return {
            'total_parameters': total_params,
            'embedding_parameters': embedding_params,
            'attention_parameters_per_layer': attention_params,
            'ffn_parameters_per_layer': ffn_params,
            'layernorm_parameters_per_layer': layernorm_params,
            'per_layer_parameters': per_layer_params,
            'all_layers_parameters': config.n_layers * per_layer_params,
            'output_parameters': output_params,
            'parameter_breakdown': {
                'embedding_ratio': embedding_params / total_params * 100,
                'attention_ratio': (config.n_layers * attention_params) / total_params * 100,
                'ffn_ratio': (config.n_layers * ffn_params) / total_params * 100,
                'layernorm_ratio': (config.n_layers * layernorm_params + 2 * config.d_model) / total_params * 100,
                'output_ratio': output_params / total_params * 100
            }
        }

    def calculate_flops(self, config: ModelConfig, sequence_length: int,
                       batch_size: int = 1, generation_length: int = 0) -> Dict:
        """è¨ˆç®—FLOPs"""

        # å‰å‘å‚³æ’­FLOPsè¨ˆç®—
        def forward_flops(seq_len: int) -> int:
            # Attention FLOPs
            attention_flops = (
                4 * seq_len * config.d_model * config.d_model +  # QKVè®Šæ› + Outputè®Šæ›
                2 * config.n_heads * seq_len * seq_len * (config.d_model // config.n_heads)  # æ³¨æ„åŠ›è¨ˆç®—
            )

            # FFN FLOPs
            ffn_flops = 2 * seq_len * config.d_model * config.d_ff

            # æ¯å±¤FLOPs
            layer_flops = attention_flops + ffn_flops

            # æ‰€æœ‰å±¤FLOPs
            model_flops = config.n_layers * layer_flops

            # è¼¸å‡ºå±¤FLOPsï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not config.tie_embeddings:
                output_flops = seq_len * config.d_model * config.vocab_size
                model_flops += output_flops

            return model_flops * batch_size

        # æ¨ç†éšæ®µè¨ˆç®—
        if generation_length == 0:
            # åƒ…é å¡«å……
            total_flops = forward_flops(sequence_length)
            return {
                'total_flops': total_flops,
                'prefill_flops': total_flops,
                'decode_flops': 0
            }
        else:
            # é å¡«å…… + è§£ç¢¼
            prefill_flops = forward_flops(sequence_length)

            # è§£ç¢¼éšæ®µï¼ˆç°¡åŒ–è¨ˆç®—ï¼‰
            decode_flops = generation_length * forward_flops(1)  # æ¯æ­¥ç”Ÿæˆä¸€å€‹token

            total_flops = prefill_flops + decode_flops

            return {
                'total_flops': total_flops,
                'prefill_flops': prefill_flops,
                'decode_flops': decode_flops,
                'flops_breakdown': {
                    'prefill_ratio': prefill_flops / total_flops * 100,
                    'decode_ratio': decode_flops / total_flops * 100
                }
            }

    def calculate_memory(self, config: ModelConfig, batch_size: int,
                        sequence_length: int, precision: str = 'fp16',
                        training: bool = False) -> Dict:
        """è¨ˆç®—è¨˜æ†¶é«”éœ€æ±‚"""

        param_result = self.calculate_parameters(config)
        total_params = param_result['total_parameters']
        bytes_per_param = self.precision_bytes[precision]

        # 1. æ¨¡å‹åƒæ•¸è¨˜æ†¶é«”
        model_memory = total_params * bytes_per_param

        # 2. æ¿€æ´»å€¼è¨˜æ†¶é«”
        activation_memory = batch_size * sequence_length * config.d_model * bytes_per_param

        if training:
            # è¨“ç·´æ¨¡å¼éœ€è¦æ›´å¤šè¨˜æ†¶é«”
            # å„ªåŒ–å™¨ç‹€æ…‹ï¼ˆAdaméœ€è¦2å€åƒæ•¸è¨˜æ†¶é«”ï¼‰
            optimizer_memory = total_params * 4 * 2  # FP32ç²¾åº¦å­˜å„²

            # æ¢¯åº¦è¨˜æ†¶é«”
            gradient_memory = total_params * bytes_per_param

            # æ¿€æ´»å€¼éœ€è¦ä¿å­˜ç”¨æ–¼åå‘å‚³æ’­
            activation_memory *= config.n_layers

            total_memory = model_memory + optimizer_memory + gradient_memory + activation_memory

            return {
                'total_memory_gb': total_memory / (1024**3),
                'model_memory_gb': model_memory / (1024**3),
                'optimizer_memory_gb': optimizer_memory / (1024**3),
                'gradient_memory_gb': gradient_memory / (1024**3),
                'activation_memory_gb': activation_memory / (1024**3),
                'breakdown': {
                    'model_ratio': model_memory / total_memory * 100,
                    'optimizer_ratio': optimizer_memory / total_memory * 100,
                    'gradient_ratio': gradient_memory / total_memory * 100,
                    'activation_ratio': activation_memory / total_memory * 100
                }
            }
        else:
            # æ¨ç†æ¨¡å¼
            # KV Cacheè¨˜æ†¶é«”
            kv_cache_memory = (
                2 * config.n_layers * batch_size * config.n_heads *
                sequence_length * (config.d_model // config.n_heads) * bytes_per_param
            )

            total_memory = model_memory + activation_memory + kv_cache_memory

            return {
                'total_memory_gb': total_memory / (1024**3),
                'model_memory_gb': model_memory / (1024**3),
                'activation_memory_gb': activation_memory / (1024**3),
                'kv_cache_memory_gb': kv_cache_memory / (1024**3),
                'breakdown': {
                    'model_ratio': model_memory / total_memory * 100,
                    'activation_ratio': activation_memory / total_memory * 100,
                    'kv_cache_ratio': kv_cache_memory / total_memory * 100
                }
            }

    def estimate_training_cost(self, config: ModelConfig, training_tokens: int,
                              gpu_type: str = 'A100', efficiency: float = 0.5) -> Dict:
        """ä¼°ç®—è¨“ç·´æˆæœ¬"""

        # GPUè¦æ ¼ï¼ˆç°¡åŒ–ï¼‰
        gpu_specs = {
            'V100': {'flops_per_second': 125e12, 'cost_per_hour': 3.0},
            'A100': {'flops_per_second': 312e12, 'cost_per_hour': 4.0},
            'H100': {'flops_per_second': 1000e12, 'cost_per_hour': 8.0}
        }

        if gpu_type not in gpu_specs:
            raise ValueError(f"Unsupported GPU type: {gpu_type}")

        gpu_spec = gpu_specs[gpu_type]

        # è¨ˆç®—ç¸½FLOPsï¼ˆç°¡åŒ–ï¼š6*N*Dï¼ŒNç‚ºåƒæ•¸æ•¸ï¼ŒDç‚ºæ•¸æ“šé‡ï¼‰
        param_result = self.calculate_parameters(config)
        total_params = param_result['total_parameters']
        total_flops = 6 * total_params * training_tokens  # å‰å‘+åå‘

        # è¨ˆç®—è¨“ç·´æ™‚é–“
        effective_flops_per_second = gpu_spec['flops_per_second'] * efficiency
        training_time_seconds = total_flops / effective_flops_per_second
        training_time_hours = training_time_seconds / 3600
        training_time_days = training_time_hours / 24

        # è¨ˆç®—æˆæœ¬
        total_cost = training_time_hours * gpu_spec['cost_per_hour']

        return {
            'total_flops': total_flops,
            'training_time_hours': training_time_hours,
            'training_time_days': training_time_days,
            'total_cost_usd': total_cost,
            'cost_per_billion_params': total_cost / (total_params / 1e9),
            'gpu_type': gpu_type,
            'efficiency': efficiency
        }

    def scaling_laws_prediction(self, compute_budget: float, law_type: str = 'chinchilla') -> Dict:
        """åŸºæ–¼ç¸®æ”¾æ³•å‰‡çš„æ€§èƒ½é æ¸¬"""

        if law_type == 'chinchilla':
            # Chinchillaç¸®æ”¾æ³•å‰‡åƒæ•¸
            alpha = 0.34
            beta = 0.28
            A = 406.4
            B = 410.7
            E = 1.69

            # æœ€å„ªåˆ†é…
            a = alpha / (alpha + beta)
            b = beta / (alpha + beta)

            optimal_params = (compute_budget / 6) ** a
            optimal_tokens = (compute_budget / 6) ** b

            # é æ¸¬æå¤±
            loss_param = A / (optimal_params ** alpha)
            loss_data = B / (optimal_tokens ** beta)
            predicted_loss = E + loss_param + loss_data

            return {
                'optimal_parameters': optimal_params,
                'optimal_tokens': optimal_tokens,
                'predicted_loss': predicted_loss,
                'compute_budget': compute_budget,
                'params_ratio': a,
                'tokens_ratio': b
            }
        else:
            raise ValueError(f"Unsupported scaling law: {law_type}")

    def generate_comparison_table(self, configs: List[ModelConfig]) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡å‹å°æ¯”è¡¨æ ¼"""

        comparison_data = []

        for config in configs:
            param_result = self.calculate_parameters(config)

            row = {
                'æ¨¡å‹åç¨±': config.name,
                'åƒæ•¸é‡(B)': f"{param_result['total_parameters'] / 1e9:.1f}",
                'è©è¡¨å¤§å°': f"{config.vocab_size:,}",
                'æ¨¡å‹ç¶­åº¦': config.d_model,
                'å±¤æ•¸': config.n_layers,
                'æ³¨æ„åŠ›é ­æ•¸': config.n_heads,
                'FFNç¶­åº¦': config.d_ff,
                'æ³¨æ„åŠ›æ¯”ä¾‹(%)': f"{param_result['parameter_breakdown']['attention_ratio']:.1f}",
                'FFNæ¯”ä¾‹(%)': f"{param_result['parameter_breakdown']['ffn_ratio']:.1f}"
            }

            comparison_data.append(row)

        return pd.DataFrame(comparison_data)

class LLMCalculatorUI:
    """è¨ˆç®—å™¨ç”¨æˆ¶ç•Œé¢"""

    def __init__(self):
        self.calculator = LLMResourceCalculator()

    def run_streamlit_app(self):
        """é‹è¡ŒStreamlitæ‡‰ç”¨"""

        st.title("ğŸ§® LLMè³‡æºè¨ˆç®—å™¨")
        st.markdown("ç²¾ç¢ºè¨ˆç®—Transformeræ¨¡å‹çš„åƒæ•¸é‡ã€FLOPså’Œè¨˜æ†¶é«”éœ€æ±‚")

        # å´é‚Šæ¬„é…ç½®
        st.sidebar.header("æ¨¡å‹é…ç½®")

        # é è¨­é…ç½®é¸é …
        preset_configs = {
            "è‡ªå®šç¾©": None,
            "GPT-2 Small": ModelConfig("GPT-2 Small", 50257, 1024, 768, 12, 12, 3072),
            "LLaMA-7B": ModelConfig("LLaMA-7B", 32000, 2048, 4096, 32, 32, 11008),
            "LLaMA-13B": ModelConfig("LLaMA-13B", 32000, 2048, 5120, 40, 40, 13824)
        }

        selected_preset = st.sidebar.selectbox("é¸æ“‡é è¨­é…ç½®", list(preset_configs.keys()))

        if selected_preset != "è‡ªå®šç¾©":
            config = preset_configs[selected_preset]
        else:
            # è‡ªå®šç¾©é…ç½®
            config = ModelConfig(
                name=st.sidebar.text_input("æ¨¡å‹åç¨±", "Custom Model"),
                vocab_size=st.sidebar.number_input("è©è¡¨å¤§å°", 1000, 100000, 50000),
                max_seq_len=st.sidebar.number_input("æœ€å¤§åºåˆ—é•·åº¦", 512, 8192, 2048),
                d_model=st.sidebar.number_input("æ¨¡å‹ç¶­åº¦", 256, 8192, 512, step=64),
                n_layers=st.sidebar.number_input("å±¤æ•¸", 1, 100, 6),
                n_heads=st.sidebar.number_input("æ³¨æ„åŠ›é ­æ•¸", 1, 64, 8),
                d_ff=st.sidebar.number_input("FFNç¶­åº¦", 256, 32768, 2048, step=64),
            )

        # ä¸»ç•Œé¢
        tab1, tab2, tab3, tab4 = st.tabs(["åƒæ•¸è¨ˆç®—", "æ€§èƒ½åˆ†æ", "æˆæœ¬ä¼°ç®—", "æ¨¡å‹å°æ¯”"])

        with tab1:
            st.header("ğŸ“Š åƒæ•¸é‡è¨ˆç®—")

            param_result = self.calculator.calculate_parameters(config)

            col1, col2 = st.columns(2)

            with col1:
                st.metric("ç¸½åƒæ•¸é‡", f"{param_result['total_parameters']:,}")
                st.metric("Embeddingåƒæ•¸", f"{param_result['embedding_parameters']:,}")
                st.metric("æ¯å±¤åƒæ•¸", f"{param_result['per_layer_parameters']:,}")

            with col2:
                st.metric("ç¸½åƒæ•¸(B)", f"{param_result['total_parameters'] / 1e9:.2f}")
                st.metric("Attentionåƒæ•¸/å±¤", f"{param_result['attention_parameters_per_layer']:,}")
                st.metric("FFNåƒæ•¸/å±¤", f"{param_result['ffn_parameters_per_layer']:,}")

            # åƒæ•¸åˆ†ä½ˆé¤…åœ–
            breakdown = param_result['parameter_breakdown']
            fig, ax = plt.subplots()
            sizes = [breakdown['embedding_ratio'], breakdown['attention_ratio'],
                    breakdown['ffn_ratio'], breakdown['layernorm_ratio']]
            labels = ['Embedding', 'Attention', 'FFN', 'LayerNorm']
            ax.pie(sizes, labels=labels, autopct='%1.1f%%')
            ax.set_title('åƒæ•¸åˆ†ä½ˆ')
            st.pyplot(fig)

        with tab2:
            st.header("âš¡ æ€§èƒ½åˆ†æ")

            col1, col2 = st.columns(2)

            with col1:
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", 1, 32, 1)
                sequence_length = st.number_input("åºåˆ—é•·åº¦", 128, 4096, 512)

            with col2:
                precision = st.selectbox("ç²¾åº¦", ["fp32", "fp16", "int8", "int4"])
                mode = st.selectbox("æ¨¡å¼", ["æ¨ç†", "è¨“ç·´"])

            # FLOPsè¨ˆç®—
            flops_result = self.calculator.calculate_flops(config, sequence_length, batch_size)

            st.subheader("è¨ˆç®—è¤‡é›œåº¦ (FLOPs)")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç¸½FLOPs", f"{flops_result['total_flops']:.2e}")
            with col2:
                if 'prefill_flops' in flops_result:
                    st.metric("é å¡«å……FLOPs", f"{flops_result['prefill_flops']:.2e}")
            with col3:
                if 'decode_flops' in flops_result:
                    st.metric("è§£ç¢¼FLOPs", f"{flops_result['decode_flops']:.2e}")

            # è¨˜æ†¶é«”è¨ˆç®—
            memory_result = self.calculator.calculate_memory(
                config, batch_size, sequence_length, precision, mode == "è¨“ç·´"
            )

            st.subheader("è¨˜æ†¶é«”éœ€æ±‚")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç¸½è¨˜æ†¶é«”", f"{memory_result['total_memory_gb']:.2f} GB")
            with col2:
                st.metric("æ¨¡å‹è¨˜æ†¶é«”", f"{memory_result['model_memory_gb']:.2f} GB")
            with col3:
                if mode == "è¨“ç·´":
                    st.metric("å„ªåŒ–å™¨è¨˜æ†¶é«”", f"{memory_result['optimizer_memory_gb']:.2f} GB")
                else:
                    st.metric("KV Cache", f"{memory_result['kv_cache_memory_gb']:.2f} GB")

        with tab3:
            st.header("ğŸ’° æˆæœ¬ä¼°ç®—")

            col1, col2 = st.columns(2)

            with col1:
                training_tokens = st.number_input("è¨“ç·´Tokenæ•¸(B)", 1, 10000, 100) * 1e9
                gpu_type = st.selectbox("GPUé¡å‹", ["V100", "A100", "H100"])

            with col2:
                efficiency = st.slider("ç¡¬é«”æ•ˆç‡", 0.1, 1.0, 0.5, 0.1)

            if st.button("è¨ˆç®—è¨“ç·´æˆæœ¬"):
                cost_result = self.calculator.estimate_training_cost(
                    config, training_tokens, gpu_type, efficiency
                )

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("è¨“ç·´æ™‚é–“(å¤©)", f"{cost_result['training_time_days']:.1f}")
                with col2:
                    st.metric("ç¸½æˆæœ¬(USD)", f"${cost_result['total_cost_usd']:,.0f}")
                with col3:
                    st.metric("å–®ä½æˆæœ¬($/Båƒæ•¸)", f"${cost_result['cost_per_billion_params']:.0f}")

        with tab4:
            st.header("ğŸ” æ¨¡å‹å°æ¯”")

            # å‰µå»ºå°æ¯”æ¨¡å‹åˆ—è¡¨
            comparison_configs = [
                ModelConfig("GPT-2 Small", 50257, 1024, 768, 12, 12, 3072),
                ModelConfig("GPT-2 Medium", 50257, 1024, 1024, 24, 16, 4096),
                ModelConfig("GPT-2 Large", 50257, 1024, 1280, 36, 20, 5120),
                ModelConfig("LLaMA-7B", 32000, 2048, 4096, 32, 32, 11008),
                ModelConfig("LLaMA-13B", 32000, 2048, 5120, 40, 40, 13824)
            ]

            comparison_df = self.calculator.generate_comparison_table(comparison_configs)
            st.dataframe(comparison_df)

            # ä¸‹è¼‰å°æ¯”çµæœ
            csv = comparison_df.to_csv(index=False)
            st.download_button(
                label="ä¸‹è¼‰å°æ¯”çµæœ",
                data=csv,
                file_name=f"llm_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

def main():
    """ä¸»å‡½æ•¸"""

    print("=== LLMè³‡æºè¨ˆç®—å™¨ ===\\n")

    # å¦‚æœåœ¨Jupyterç’°å¢ƒä¸­ï¼Œé‹è¡Œäº¤äº’å¼è¨ˆç®—
    calculator = LLMResourceCalculator()

    # ç¤ºä¾‹è¨ˆç®—
    llama_7b = ModelConfig(
        name="LLaMA-7B",
        vocab_size=32000,
        max_seq_len=2048,
        d_model=4096,
        n_layers=32,
        n_heads=32,
        d_ff=11008
    )

    print("1. åƒæ•¸é‡è¨ˆç®—:")
    param_result = calculator.calculate_parameters(llama_7b)
    print(f"ç¸½åƒæ•¸é‡: {param_result['total_parameters']:,} ({param_result['total_parameters']/1e9:.1f}B)")

    print("\\n2. FLOPsè¨ˆç®—:")
    flops_result = calculator.calculate_flops(llama_7b, sequence_length=512, batch_size=1)
    print(f"æ¨ç†FLOPs: {flops_result['total_flops']:.2e}")

    print("\\n3. è¨˜æ†¶é«”éœ€æ±‚:")
    memory_result = calculator.calculate_memory(llama_7b, batch_size=1, sequence_length=512)
    print(f"æ¨ç†è¨˜æ†¶é«”: {memory_result['total_memory_gb']:.2f} GB")

    print("\\n4. è¨“ç·´æˆæœ¬ä¼°ç®—:")
    cost_result = calculator.estimate_training_cost(llama_7b, training_tokens=1000e9)
    print(f"è¨“ç·´æˆæœ¬: ${cost_result['total_cost_usd']:,.0f}")

    print("\\n=== è¨ˆç®—å®Œæˆ ===")
    print("é‹è¡Œ 'streamlit run llm_resource_calculator.py' å•Ÿå‹•Webç•Œé¢")

if __name__ == "__main__":
    # æª¢æŸ¥æ˜¯å¦åœ¨Streamlitç’°å¢ƒ
    try:
        import streamlit as st
        ui = LLMCalculatorUI()
        ui.run_streamlit_app()
    except ImportError:
        print("Streamlitæœªå®‰è£ï¼Œé‹è¡ŒåŸºç¤è¨ˆç®—æ¨¡å¼")
        main()
```

## ä½¿ç”¨æŒ‡å—

### åŸºç¤æ¨¡å¼é‹è¡Œ
```bash
python llm_resource_calculator.py
```

### Webç•Œé¢æ¨¡å¼ï¼ˆæ¨è–¦ï¼‰
```bash
# å®‰è£Streamlit
pip install streamlit

# å•Ÿå‹•Webæ‡‰ç”¨
streamlit run llm_resource_calculator.py
```

## è¨ˆç®—å™¨åŠŸèƒ½ç‰¹é»

### 1. ç²¾ç¢ºåƒæ•¸è¨ˆç®—
- æ”¯æŒå¤šç¨®Transformerè®Šé«”ï¼ˆMHAã€MQAã€GQAï¼‰
- è€ƒæ…®ä¸åŒFFNé¡å‹ï¼ˆæ¨™æº–ã€é–€æ§ã€MoEï¼‰
- è©³ç´°çš„åƒæ•¸åˆ†è§£å’Œæ¯”ä¾‹åˆ†æ

### 2. å…¨é¢æ€§èƒ½åˆ†æ
- FLOPsè¨ˆç®—ï¼ˆè¨“ç·´å’Œæ¨ç†ï¼‰
- è¨˜æ†¶é«”éœ€æ±‚ä¼°ç®—ï¼ˆä¸åŒç²¾åº¦ï¼‰
- KV Cacheå’Œæ¿€æ´»å€¼è¨˜æ†¶é«”åˆ†æ

### 3. æˆæœ¬ä¼°ç®—åŠŸèƒ½
- åŸºæ–¼å¯¦éš›GPUè¦æ ¼çš„è¨“ç·´æˆæœ¬è¨ˆç®—
- è€ƒæ…®ç¡¬é«”æ•ˆç‡å’Œå¯¦éš›åˆ©ç”¨ç‡
- æ”¯æŒå¤šç¨®GPUé¡å‹å°æ¯”

### 4. ç¸®æ”¾æ³•å‰‡æ‡‰ç”¨
- Chinchillaç¸®æ”¾æ³•å‰‡å¯¦ç¾
- æœ€å„ªè³‡æºåˆ†é…è¨ˆç®—
- æ€§èƒ½é æ¸¬åŠŸèƒ½

### 5. ç›´è§€å¯è¦–åŒ–ç•Œé¢
- åƒæ•¸åˆ†ä½ˆé¤…åœ–
- æ¨¡å‹å°æ¯”è¡¨æ ¼
- äº¤äº’å¼Webç•Œé¢

## å¯¦é©—ä»»å‹™

### ä»»å‹™1ï¼šæ¨¡å‹è¦æ¨¡åˆ†æ
æ¯”è¼ƒä¸åŒè¦æ¨¡æ¨¡å‹çš„è³‡æºéœ€æ±‚å·®ç•°

### ä»»å‹™2ï¼šç²¾åº¦å½±éŸ¿è©•ä¼°
åˆ†æä¸åŒé‡åŒ–ç²¾åº¦å°è³‡æºéœ€æ±‚çš„å½±éŸ¿

### ä»»å‹™3ï¼šéƒ¨ç½²æ–¹æ¡ˆè¨­è¨ˆ
ç‚ºç‰¹å®šæ‡‰ç”¨å ´æ™¯è¨­è¨ˆæœ€å„ªéƒ¨ç½²æ–¹æ¡ˆ

### ä»»å‹™4ï¼šæˆæœ¬æ•ˆç›Šåˆ†æ
é€²è¡Œè¨“ç·´å’Œæ¨ç†çš„å®Œæ•´æˆæœ¬åˆ†æ

## å¯¦é©—å ±å‘Šè¦æ±‚

### å¿…ç­”å•é¡Œ
1. **åƒæ•¸åˆ†ä½ˆ**ï¼šåˆ†æä¸åŒè¦æ¨¡æ¨¡å‹çš„åƒæ•¸åˆ†ä½ˆç‰¹é»
2. **è³‡æºç¸®æ”¾**ï¼šæ¢è¨æ¨¡å‹è¦æ¨¡èˆ‡è³‡æºéœ€æ±‚çš„é—œä¿‚
3. **æˆæœ¬å„ªåŒ–**ï¼šæå‡ºé™ä½è¨“ç·´å’Œéƒ¨ç½²æˆæœ¬çš„ç­–ç•¥
4. **å¯¦éš›æ‡‰ç”¨**ï¼šç‚ºå…·é«”å ´æ™¯æ¨è–¦åˆé©çš„æ¨¡å‹é…ç½®

### å»¶ä¼¸æ€è€ƒ
1. å¦‚ä½•åœ¨è³‡æºç´„æŸä¸‹æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ï¼Ÿ
2. ç¸®æ”¾æ³•å‰‡åœ¨å¯¦éš›æ‡‰ç”¨ä¸­çš„å±€é™æ€§æ˜¯ä»€éº¼ï¼Ÿ
3. æœªä¾†æ¨¡å‹æ¶æ§‹å¯èƒ½å¦‚ä½•å½±éŸ¿è³‡æºéœ€æ±‚ï¼Ÿ

é€™å€‹Labæä¾›äº†å®Œæ•´çš„LLMè³‡æºä¼°ç®—å·¥å…·ï¼Œå¹«åŠ©å­¸å“¡å»ºç«‹ç²¾ç¢ºçš„è³‡æºè©•ä¼°å’Œè¦åŠƒèƒ½åŠ›ã€‚