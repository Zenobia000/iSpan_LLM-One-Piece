{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashAttention ç²¾åº¦å½±éŸ¿åˆ†æ\n",
    "## Precision Impact Analysis for Training vs Inference\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "- ç†è§£ FlashAttention åœ¨ä¸åŒç²¾åº¦ä¸‹çš„è¡Œç‚ºå·®ç•°\n",
    "- è­˜åˆ¥è¨“ç·´å’Œæ¨è«–ä¸­çš„é—œéµç²¾åº¦æ•æ„Ÿå±¤\n",
    "- æŒæ¡ç²¾åº¦è™•ç†çš„æœ€ä½³å¯¦è¸\n",
    "- å»ºç«‹ç²¾åº¦å…¼å®¹æ€§æª¢æ¸¬æ©Ÿåˆ¶\n",
    "\n",
    "**é‡è¦æ€§**: FlashAttention åªæ”¯æ´ FP16/BF16ï¼Œèˆ‡å‚³çµ± FP32 æ¨¡å‹æ··åˆä½¿ç”¨æ™‚éœ€è¦ç‰¹åˆ¥æ³¨æ„ç²¾åº¦è½‰æ›çš„å½±éŸ¿ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­ç½®å’ŒåŸºç¤å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# FlashAttention\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"âœ… FlashAttention å¯ç”¨\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"âŒ FlashAttention æœªå®‰è£\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# è¨­ç½®éš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ç²¾åº¦åˆ†æå·¥å…·\n",
    "class PrecisionAnalyzer:\n",
    "    \"\"\"ç²¾åº¦åˆ†æå·¥å…·\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_numerical_error(tensor1, tensor2, metric='mse'):\n",
    "        \"\"\"è¨ˆç®—æ•¸å€¼èª¤å·®\"\"\"\n",
    "        if tensor1.shape != tensor2.shape:\n",
    "            raise ValueError(f\"å¼µé‡å½¢ç‹€ä¸åŒ¹é…: {tensor1.shape} vs {tensor2.shape}\")\n",
    "        \n",
    "        # ç¢ºä¿ç›¸åŒæ•¸æ“šé¡å‹\n",
    "        if tensor1.dtype != tensor2.dtype:\n",
    "            tensor2 = tensor2.to(tensor1.dtype)\n",
    "        \n",
    "        diff = tensor1 - tensor2\n",
    "        \n",
    "        if metric == 'mse':\n",
    "            return torch.mean(diff ** 2).item()\n",
    "        elif metric == 'mae':\n",
    "            return torch.mean(torch.abs(diff)).item()\n",
    "        elif metric == 'max':\n",
    "            return torch.max(torch.abs(diff)).item()\n",
    "        elif metric == 'relative':\n",
    "            return torch.mean(torch.abs(diff) / (torch.abs(tensor1) + 1e-8)).item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_gradient_precision(gradients):\n",
    "        \"\"\"åˆ†ææ¢¯åº¦ç²¾åº¦\"\"\"\n",
    "        results = {}\n",
    "        for name, grad in gradients.items():\n",
    "            if grad is not None:\n",
    "                results[name] = {\n",
    "                    'dtype': str(grad.dtype),\n",
    "                    'mean': grad.mean().item(),\n",
    "                    'std': grad.std().item(),\n",
    "                    'max': grad.max().item(),\n",
    "                    'min': grad.min().item(),\n",
    "                    'norm': grad.norm().item()\n",
    "                }\n",
    "        return results\n",
    "\n",
    "print(\"âœ… ç²¾åº¦åˆ†æå·¥å…·æº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é—œéµå±¤ç²¾åº¦å½±éŸ¿åˆ†æ\n",
    "\n",
    "### ğŸ¯ é—œéµç™¼ç¾ï¼šå“ªäº›å±¤æœ€å®¹æ˜“å—åˆ°ç²¾åº¦å½±éŸ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºæ¸¬è©¦æ¨¡å‹ä¾†åˆ†æå„å±¤ç²¾åº¦æ•æ„Ÿæ€§\n",
    "class PrecisionSensitiveTransformer(nn.Module):\n",
    "    \"\"\"ç”¨æ–¼ç²¾åº¦åˆ†æçš„ Transformer æ¨¡å‹\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=768, n_heads=12, seq_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # é—œéµå±¤å®šç¾©\n",
    "        self.embedding = nn.Embedding(50257, d_model)  # GPT-2 vocab size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        \n",
    "        # LayerNorm - ç²¾åº¦æ•æ„Ÿåº¦ï¼šHIGH âš ï¸\n",
    "        self.ln_1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.ln_2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.ln_final = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        \n",
    "        # Attention æŠ•å½±å±¤ - ç²¾åº¦æ•æ„Ÿåº¦ï¼šMEDIUM\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.o_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # MLP å±¤ - ç²¾åº¦æ•æ„Ÿåº¦ï¼šMEDIUM\n",
    "        self.mlp_up = nn.Linear(d_model, 4 * d_model)\n",
    "        self.mlp_down = nn.Linear(4 * d_model, d_model)\n",
    "        \n",
    "        # è¼¸å‡ºå±¤ - ç²¾åº¦æ•æ„Ÿåº¦ï¼šHIGH âš ï¸\n",
    "        self.lm_head = nn.Linear(d_model, 50257, bias=False)\n",
    "        \n",
    "        # Dropout - ç²¾åº¦æ•æ„Ÿåº¦ï¼šLOW\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def standard_attention(self, x):\n",
    "        \"\"\"æ¨™æº–æ³¨æ„åŠ›è¨ˆç®—\"\"\"\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # æ³¨æ„åŠ›è¨ˆç®— - ç²¾åº¦æ•æ„Ÿåº¦ï¼šHIGH âš ï¸\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # Softmax - ç²¾åº¦æ•æ„Ÿåº¦ï¼šCRITICAL âš ï¸âš ï¸\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(attn_output)\n",
    "    \n",
    "    def flash_attention(self, x):\n",
    "        \"\"\"FlashAttention è¨ˆç®—\"\"\"\n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            return self.standard_attention(x)\n",
    "            \n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # ç¢ºä¿ FP16 ç²¾åº¦\n",
    "        original_dtype = x.dtype\n",
    "        if x.dtype == torch.float32:\n",
    "            x = x.half()\n",
    "            \n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # FlashAttention è¨ˆç®—\n",
    "        attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "        attn_output = attn_output.view(B, T, C)\n",
    "        \n",
    "        # è½‰æ›å›åŸå§‹ç²¾åº¦\n",
    "        if original_dtype == torch.float32:\n",
    "            attn_output = attn_output.float()\n",
    "            \n",
    "        return self.o_proj(attn_output)\n",
    "    \n",
    "    def forward(self, input_ids, use_flash=False):\n",
    "        B, T = input_ids.shape\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x + self.pos_embedding[:, :T, :]\n",
    "        \n",
    "        # Pre-attention LayerNorm\n",
    "        x_norm = self.ln_1(x)\n",
    "        \n",
    "        # Attention\n",
    "        if use_flash:\n",
    "            attn_out = self.flash_attention(x_norm)\n",
    "        else:\n",
    "            attn_out = self.standard_attention(x_norm)\n",
    "        \n",
    "        # Residual connection\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # Pre-MLP LayerNorm\n",
    "        x_norm2 = self.ln_2(x)\n",
    "        \n",
    "        # MLP\n",
    "        mlp_out = self.mlp_down(F.gelu(self.mlp_up(x_norm2)))\n",
    "        \n",
    "        # Residual connection\n",
    "        x = x + self.dropout(mlp_out)\n",
    "        \n",
    "        # Final LayerNorm\n",
    "        x = self.ln_final(x)\n",
    "        \n",
    "        # Language modeling head\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"âœ… ç²¾åº¦æ•æ„Ÿæ€§æ¸¬è©¦æ¨¡å‹æº–å‚™å®Œæˆ\")\n",
    "\n",
    "# ç²¾åº¦æ•æ„Ÿåº¦ç¸½çµ\n",
    "precision_sensitivity = {\n",
    "    \"CRITICAL (éœ€è¦ç‰¹åˆ¥æ³¨æ„)\": [\n",
    "        \"Softmax è¨ˆç®—\",\n",
    "        \"Attention åˆ†æ•¸è¨ˆç®—\",\n",
    "        \"æº«åº¦ç¸®æ”¾ (Temperature scaling)\"\n",
    "    ],\n",
    "    \"HIGH (é«˜æ•æ„Ÿåº¦)\": [\n",
    "        \"LayerNorm\",\n",
    "        \"è¼¸å‡ºæŠ•å½±å±¤ (lm_head)\",\n",
    "        \"æ®˜å·®é€£æ¥ç´¯ç©\",\n",
    "        \"Loss è¨ˆç®—\"\n",
    "    ],\n",
    "    \"MEDIUM (ä¸­ç­‰æ•æ„Ÿåº¦)\": [\n",
    "        \"Linear æŠ•å½±å±¤ (QKV, MLP)\",\n",
    "        \"GELU æ¿€æ´»å‡½æ•¸\",\n",
    "        \"Embedding å±¤\"\n",
    "    ],\n",
    "    \"LOW (ä½æ•æ„Ÿåº¦)\": [\n",
    "        \"Dropout\",\n",
    "        \"ä½ç½®ç·¨ç¢¼åŠ æ³•\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ¯ ç²¾åº¦æ•æ„Ÿåº¦å±¤ç´šåˆ†æï¼š\")\n",
    "for level, layers in precision_sensitivity.items():\n",
    "    print(f\"\\n{level}:\")\n",
    "    for layer in layers:\n",
    "        print(f\"  â€¢ {layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¨“ç·´ vs æ¨è«–ç²¾åº¦å·®ç•°å¯¦é©—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_precision_comparison_experiment():\n",
    "    \"\"\"é‹è¡Œç²¾åº¦æ¯”è¼ƒå¯¦é©—\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"è¨“ç·´ vs æ¨è«–ç²¾åº¦å·®ç•°å¯¦é©—\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # å‰µå»ºæ¨¡å‹\n",
    "    model = PrecisionSensitiveTransformer(d_model=768, n_heads=12, seq_len=128).to(device)\n",
    "    \n",
    "    # æ¸¬è©¦æ•¸æ“š\n",
    "    batch_size = 4\n",
    "    seq_len = 128\n",
    "    input_ids = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
    "    target_ids = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. ç´” FP32 åŸºæº–\n",
    "    print(\"\\n1. FP32 åŸºæº–æ¸¬è©¦...\")\n",
    "    model.float()\n",
    "    with torch.no_grad():\n",
    "        fp32_output = model(input_ids, use_flash=False)\n",
    "    results['fp32_inference'] = fp32_output.clone()\n",
    "    \n",
    "    # 2. FP32 + FlashAttention (å…§éƒ¨è½‰æ›)\n",
    "    print(\"\\n2. FP32 + FlashAttention æ¸¬è©¦...\")\n",
    "    if FLASH_ATTN_AVAILABLE:\n",
    "        with torch.no_grad():\n",
    "            fp32_flash_output = model(input_ids, use_flash=True)\n",
    "        results['fp32_flash_inference'] = fp32_flash_output.clone()\n",
    "    \n",
    "    # 3. ç´” FP16 æ¨è«–\n",
    "    print(\"\\n3. FP16 æ¨è«–æ¸¬è©¦...\")\n",
    "    model.half()\n",
    "    input_ids_fp16 = input_ids\n",
    "    with torch.no_grad():\n",
    "        with autocast(dtype=torch.float16):\n",
    "            fp16_output = model(input_ids_fp16, use_flash=False)\n",
    "    results['fp16_inference'] = fp16_output.clone()\n",
    "    \n",
    "    # 4. FP16 + FlashAttention æ¨è«–\n",
    "    print(\"\\n4. FP16 + FlashAttention æ¨è«–æ¸¬è©¦...\")\n",
    "    if FLASH_ATTN_AVAILABLE:\n",
    "        with torch.no_grad():\n",
    "            with autocast(dtype=torch.float16):\n",
    "                fp16_flash_output = model(input_ids_fp16, use_flash=True)\n",
    "        results['fp16_flash_inference'] = fp16_flash_output.clone()\n",
    "    \n",
    "    # 5. æ··åˆç²¾åº¦è¨“ç·´æ¨¡æ“¬\n",
    "    print(\"\\n5. æ··åˆç²¾åº¦è¨“ç·´æ¸¬è©¦...\")\n",
    "    model.float()  # æ¬Šé‡ä¿æŒ FP32\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scaler = GradScaler()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(dtype=torch.float16):\n",
    "        mixed_output = model(input_ids, use_flash=True)\n",
    "        loss = criterion(mixed_output.view(-1, mixed_output.size(-1)), target_ids.view(-1))\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    results['mixed_precision_training'] = mixed_output.clone().detach()\n",
    "    \n",
    "    # æ¢¯åº¦åˆ†æ\n",
    "    gradients = {name: param.grad for name, param in model.named_parameters() if param.grad is not None}\n",
    "    grad_analysis = PrecisionAnalyzer.analyze_gradient_precision(gradients)\n",
    "    \n",
    "    return results, grad_analysis\n",
    "\n",
    "# é‹è¡Œå¯¦é©—\n",
    "if FLASH_ATTN_AVAILABLE:\n",
    "    precision_results, gradient_analysis = run_precision_comparison_experiment()\n",
    "    print(\"\\nâœ… ç²¾åº¦æ¯”è¼ƒå¯¦é©—å®Œæˆ\")\n",
    "else:\n",
    "    print(\"âš ï¸  FlashAttention æœªå®‰è£ï¼Œè·³éå¯¦é©—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç²¾åº¦å·®ç•°æ•¸å€¼åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLASH_ATTN_AVAILABLE and 'precision_results' in locals():\n",
    "    print(\"=\"*80)\n",
    "    print(\"ç²¾åº¦å·®ç•°æ•¸å€¼åˆ†æ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    analyzer = PrecisionAnalyzer()\n",
    "    \n",
    "    # åŸºæº–ï¼šFP32 æ¨™æº–æ³¨æ„åŠ›\n",
    "    baseline = precision_results['fp32_inference'].float()\n",
    "    \n",
    "    comparison_pairs = [\n",
    "        ('fp32_flash_inference', 'FP32 + FlashAttention'),\n",
    "        ('fp16_inference', 'FP16 æ¨™æº–æ³¨æ„åŠ›'),\n",
    "        ('fp16_flash_inference', 'FP16 + FlashAttention'),\n",
    "        ('mixed_precision_training', 'æ··åˆç²¾åº¦è¨“ç·´')\n",
    "    ]\n",
    "    \n",
    "    error_analysis = {}\n",
    "    \n",
    "    print(f\"\\n{'é…ç½®':<25} {'MSE':<12} {'MAE':<12} {'æœ€å¤§èª¤å·®':<12} {'ç›¸å°èª¤å·®':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for key, name in comparison_pairs:\n",
    "        if key in precision_results:\n",
    "            target = precision_results[key].float()\n",
    "            \n",
    "            mse = analyzer.compute_numerical_error(baseline, target, 'mse')\n",
    "            mae = analyzer.compute_numerical_error(baseline, target, 'mae')\n",
    "            max_err = analyzer.compute_numerical_error(baseline, target, 'max')\n",
    "            rel_err = analyzer.compute_numerical_error(baseline, target, 'relative')\n",
    "            \n",
    "            error_analysis[key] = {\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'max_error': max_err,\n",
    "                'relative_error': rel_err\n",
    "            }\n",
    "            \n",
    "            print(f\"{name:<25} {mse:<12.2e} {mae:<12.2e} {max_err:<12.2e} {rel_err:<12.4f}\")\n",
    "    \n",
    "    # æ¢¯åº¦ç²¾åº¦åˆ†æ\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"æ¢¯åº¦ç²¾åº¦åˆ†æ (æ··åˆç²¾åº¦è¨“ç·´)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n{'å±¤åç¨±':<30} {'æ•¸æ“šé¡å‹':<10} {'å‡å€¼':<12} {'æ¨™æº–å·®':<12} {'æ¢¯åº¦ç¯„æ•¸':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, stats in list(gradient_analysis.items())[:10]:  # é¡¯ç¤ºå‰10å±¤\n",
    "        print(f\"{name[-28:]:<30} {stats['dtype']:<10} {stats['mean']:<12.2e} {stats['std']:<12.2e} {stats['norm']:<12.2e}\")\n",
    "    \n",
    "    # è¦–è¦ºåŒ–èª¤å·®åˆ†ä½ˆ\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # å­åœ–1ï¼šä¸åŒé…ç½®çš„èª¤å·®å°æ¯”\n",
    "    plt.subplot(1, 3, 1)\n",
    "    configs = [name for _, name in comparison_pairs if _ in error_analysis]\n",
    "    mse_values = [error_analysis[key]['mse'] for key, _ in comparison_pairs if key in error_analysis]\n",
    "    \n",
    "    bars = plt.bar(range(len(configs)), mse_values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(configs)])\n",
    "    plt.xticks(range(len(configs)), [c.replace(' ', '\\n') for c in configs], rotation=0)\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('MSE (log scale)')\n",
    "    plt.title('ä¸åŒé…ç½®çš„ MSE æ¯”è¼ƒ')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # å­åœ–2ï¼šç›¸å°èª¤å·®\n",
    "    plt.subplot(1, 3, 2)\n",
    "    rel_errors = [error_analysis[key]['relative_error'] for key, _ in comparison_pairs if key in error_analysis]\n",
    "    \n",
    "    bars = plt.bar(range(len(configs)), rel_errors, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(configs)])\n",
    "    plt.xticks(range(len(configs)), [c.replace(' ', '\\n') for c in configs], rotation=0)\n",
    "    plt.ylabel('ç›¸å°èª¤å·®')\n",
    "    plt.title('ç›¸å°èª¤å·®æ¯”è¼ƒ')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # å­åœ–3ï¼šèª¤å·®åˆ†ä½ˆç†±åœ–\n",
    "    plt.subplot(1, 3, 3)\n",
    "    \n",
    "    # è¨ˆç®—ç¬¬ä¸€å€‹ token çš„èª¤å·®åˆ†ä½ˆ\n",
    "    if 'fp16_flash_inference' in precision_results:\n",
    "        diff = (baseline[0, 0, :] - precision_results['fp16_flash_inference'][0, 0, :].float()).abs().cpu().numpy()\n",
    "        plt.hist(diff, bins=50, alpha=0.7, color='#e74c3c')\n",
    "        plt.xlabel('çµ•å°èª¤å·®')\n",
    "        plt.ylabel('é »æ¬¡')\n",
    "        plt.title('FP16+FlashAttention\\nèª¤å·®åˆ†ä½ˆ')\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  ç„¡ç²¾åº¦çµæœå¯åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. é—œéµå±¤ç²¾åº¦è™•ç†æœ€ä½³å¯¦è¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionAwareFlashAttention(nn.Module):\n",
    "    \"\"\"ç²¾åº¦æ„ŸçŸ¥çš„ FlashAttention å¯¦ç¾\"\"\"\n",
    "    \n",
    "    def __init__(self, config, precision_policy='mixed'):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        self.precision_policy = precision_policy\n",
    "        \n",
    "        # ç·šæ€§å±¤\n",
    "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd, bias=True)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=True)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(getattr(config, 'resid_pdrop', 0.1))\n",
    "        \n",
    "        # ç²¾åº¦ç­–ç•¥é…ç½®\n",
    "        self.setup_precision_policy()\n",
    "    \n",
    "    def setup_precision_policy(self):\n",
    "        \"\"\"è¨­ç½®ç²¾åº¦ç­–ç•¥\"\"\"\n",
    "        if self.precision_policy == 'conservative':\n",
    "            # ä¿å®ˆç­–ç•¥ï¼šé—œéµè¨ˆç®—ä¿æŒé«˜ç²¾åº¦\n",
    "            self.attention_dtype = torch.float32\n",
    "            self.output_dtype = torch.float32\n",
    "            print(\"ğŸ›¡ï¸ ä½¿ç”¨ä¿å®ˆç²¾åº¦ç­–ç•¥ (æ›´é«˜ç²¾åº¦ï¼Œè¼ƒæ…¢é€Ÿåº¦)\")\n",
    "            \n",
    "        elif self.precision_policy == 'aggressive':\n",
    "            # æ¿€é€²ç­–ç•¥ï¼šå…¨ç¨‹ FP16\n",
    "            self.attention_dtype = torch.float16\n",
    "            self.output_dtype = torch.float16\n",
    "            print(\"âš¡ ä½¿ç”¨æ¿€é€²ç²¾åº¦ç­–ç•¥ (æ›´å¿«é€Ÿåº¦ï¼Œè¼ƒä½ç²¾åº¦)\")\n",
    "            \n",
    "        else:  # mixed\n",
    "            # æ··åˆç­–ç•¥ï¼šå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦\n",
    "            self.attention_dtype = torch.float16\n",
    "            self.output_dtype = torch.float32\n",
    "            print(\"âš–ï¸ ä½¿ç”¨æ··åˆç²¾åº¦ç­–ç•¥ (å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦)\")\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None, layer_past=None,\n",
    "                head_mask=None, use_cache=False, output_attentions=False, **kwargs):\n",
    "        \n",
    "        B, T, C = hidden_states.size()\n",
    "        original_dtype = hidden_states.dtype\n",
    "        \n",
    "        # 1. è¼¸å…¥ç²¾åº¦è™•ç†\n",
    "        if hidden_states.dtype != self.attention_dtype:\n",
    "            hidden_states = hidden_states.to(self.attention_dtype)\n",
    "        \n",
    "        # 2. ç¢ºä¿æ¬Šé‡ç²¾åº¦åŒ¹é…\n",
    "        if self.c_attn.weight.dtype != hidden_states.dtype:\n",
    "            self.c_attn = self.c_attn.to(hidden_states.dtype)\n",
    "        \n",
    "        # 3. QKV æŠ•å½± - ä½¿ç”¨æŒ‡å®šç²¾åº¦\n",
    "        qkv = self.c_attn(hidden_states)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # 4. è™•ç† KV cacheï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "        present = None\n",
    "        if layer_past is not None:\n",
    "            try:\n",
    "                if isinstance(layer_past, (tuple, list)) and len(layer_past) >= 2:\n",
    "                    past_key, past_value = layer_past[0], layer_past[1]\n",
    "                    # ç¢ºä¿ KV cache ç²¾åº¦ä¸€è‡´\n",
    "                    if past_key.dtype != k.dtype:\n",
    "                        past_key = past_key.to(k.dtype)\n",
    "                        past_value = past_value.to(v.dtype)\n",
    "                    k = torch.cat((past_key, k), dim=1)\n",
    "                    v = torch.cat((past_value, v), dim=1)\n",
    "            except (ValueError, IndexError):\n",
    "                layer_past = None\n",
    "        \n",
    "        if use_cache:\n",
    "            present = (k, v)\n",
    "        \n",
    "        # 5. é‡å¡‘ç‚º FlashAttention æ ¼å¼\n",
    "        current_seq_len = k.size(1)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim)\n",
    "        k = k.view(B, current_seq_len, self.n_head, self.head_dim)\n",
    "        v = v.view(B, current_seq_len, self.n_head, self.head_dim)\n",
    "        \n",
    "        # 6. FlashAttention è¨ˆç®—ï¼ˆé—œéµï¼šç¢ºä¿æ•¸å€¼ç©©å®šæ€§ï¼‰\n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            # é™ç´šåˆ°æ¨™æº–æ³¨æ„åŠ›\n",
    "            q = q.transpose(1, 2)\n",
    "            k = k.transpose(1, 2)\n",
    "            v = v.transpose(1, 2)\n",
    "            \n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "            \n",
    "            # ç²¾åº¦æ•æ„Ÿçš„ Softmax - ä½¿ç”¨é«˜ç²¾åº¦è¨ˆç®—\n",
    "            if self.precision_policy == 'conservative':\n",
    "                scores = scores.float()\n",
    "                attn_weights = F.softmax(scores, dim=-1).to(v.dtype)\n",
    "            else:\n",
    "                attn_weights = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            attn_output = torch.matmul(attn_weights, v)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        else:\n",
    "            # ä½¿ç”¨ FlashAttention\n",
    "            if layer_past is None:\n",
    "                try:\n",
    "                    from flash_attn import flash_attn_qkvpacked_func\n",
    "                    qkv_packed = torch.stack([q, k, v], dim=2)\n",
    "                    attn_output = flash_attn_qkvpacked_func(\n",
    "                        qkv_packed,\n",
    "                        dropout_p=0.0,\n",
    "                        causal=True\n",
    "                    )\n",
    "                except ImportError:\n",
    "                    attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "            else:\n",
    "                attn_output = flash_attn_func(q, k, v, causal=True)\n",
    "            \n",
    "            attn_output = attn_output.contiguous().view(B, T, C)\n",
    "        \n",
    "        # 7. è¼¸å‡ºæŠ•å½± - ç²¾åº¦è™•ç†\n",
    "        if self.c_proj.weight.dtype != attn_output.dtype:\n",
    "            self.c_proj = self.c_proj.to(attn_output.dtype)\n",
    "        \n",
    "        output = self.c_proj(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # 8. æœ€çµ‚è¼¸å‡ºç²¾åº¦è½‰æ›\n",
    "        if self.output_dtype != output.dtype and self.output_dtype == original_dtype:\n",
    "            output = output.to(original_dtype)\n",
    "        \n",
    "        # 9. è¿”å›æ ¼å¼\n",
    "        outputs = (output, present)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (None,)  # FlashAttention ä¸è¿”å›æ³¨æ„åŠ›æ¬Šé‡\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"âœ… ç²¾åº¦æ„ŸçŸ¥ FlashAttention å¯¦ç¾å®Œæˆ\")\n",
    "\n",
    "# å±•ç¤ºä¸åŒç²¾åº¦ç­–ç•¥çš„é…ç½®å»ºè­°\n",
    "precision_strategies = {\n",
    "    \"ç ”ç™¼/èª¿è©¦éšæ®µ\": {\n",
    "        \"ç­–ç•¥\": \"conservative\",\n",
    "        \"æè¿°\": \"ä¿æŒé—œéµè¨ˆç®—çš„é«˜ç²¾åº¦ï¼Œç¢ºä¿æ•¸å€¼ç©©å®šæ€§\",\n",
    "        \"é©ç”¨å ´æ™¯\": \"æ¨¡å‹é–‹ç™¼ã€èª¿è©¦ã€ç²¾åº¦æ•æ„Ÿä»»å‹™\",\n",
    "        \"æ€§èƒ½\": \"è¼ƒæ…¢ï¼Œä½†ç²¾åº¦æœ€é«˜\"\n",
    "    },\n",
    "    \"ç”Ÿç”¢éƒ¨ç½²éšæ®µ\": {\n",
    "        \"ç­–ç•¥\": \"mixed\",\n",
    "        \"æè¿°\": \"å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½ï¼Œæ¨è–¦çš„é»˜èªç­–ç•¥\",\n",
    "        \"é©ç”¨å ´æ™¯\": \"å¤§éƒ¨åˆ†ç”Ÿç”¢ç’°å¢ƒ\",\n",
    "        \"æ€§èƒ½\": \"è¼ƒå¥½çš„ç²¾åº¦-é€Ÿåº¦å¹³è¡¡\"\n",
    "    },\n",
    "    \"é«˜æ€§èƒ½æ¨è«–\": {\n",
    "        \"ç­–ç•¥\": \"aggressive\",\n",
    "        \"æè¿°\": \"æœ€å¤§åŒ–æ€§èƒ½ï¼Œæ¥å—ç²¾åº¦æå¤±\",\n",
    "        \"é©ç”¨å ´æ™¯\": \"å°å»¶é²æ¥µå…¶æ•æ„Ÿçš„å ´æ™¯\",\n",
    "        \"æ€§èƒ½\": \"æœ€å¿«ï¼Œä½†ç²¾åº¦æœ€ä½\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ¯ ç²¾åº¦ç­–ç•¥å»ºè­°ï¼š\")\n",
    "for scenario, config in precision_strategies.items():\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"  ç­–ç•¥: {config['ç­–ç•¥']}\")\n",
    "    print(f\"  æè¿°: {config['æè¿°']}\")\n",
    "    print(f\"  é©ç”¨: {config['é©ç”¨å ´æ™¯']}\")\n",
    "    print(f\"  æ€§èƒ½: {config['æ€§èƒ½']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LayerNorm ç²¾åº¦ç‰¹æ®Šè™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionAwareLayerNorm(nn.Module):\n",
    "    \"\"\"ç²¾åº¦æ„ŸçŸ¥çš„ LayerNorm å¯¦ç¾\"\"\"\n",
    "    \n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True, \n",
    "                 dtype=None, device=None, force_fp32_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LayerNorm åƒæ•¸\n",
    "        factory_kwargs = {'dtype': dtype, 'device': device}\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        self.force_fp32_stats = force_fp32_stats  # å¼·åˆ¶çµ±è¨ˆè¨ˆç®—ä½¿ç”¨ FP32\n",
    "        \n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.empty(normalized_shape, **factory_kwargs))\n",
    "            self.bias = nn.Parameter(torch.empty(normalized_shape, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        if self.elementwise_affine:\n",
    "            nn.init.ones_(self.weight)\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        original_dtype = input.dtype\n",
    "        \n",
    "        # é—œéµï¼šLayerNorm çµ±è¨ˆè¨ˆç®—ä½¿ç”¨ FP32 ä»¥æé«˜æ•¸å€¼ç©©å®šæ€§\n",
    "        if self.force_fp32_stats and input.dtype != torch.float32:\n",
    "            input_fp32 = input.float()\n",
    "            \n",
    "            # è¨ˆç®—çµ±è¨ˆé‡ï¼ˆFP32ï¼‰\n",
    "            mean = input_fp32.mean(-1, keepdim=True)\n",
    "            var = input_fp32.var(-1, keepdim=True, unbiased=False)\n",
    "            \n",
    "            # æ¨™æº–åŒ–ï¼ˆFP32ï¼‰\n",
    "            normalized = (input_fp32 - mean) / torch.sqrt(var + self.eps)\n",
    "            \n",
    "            # è½‰æ›å›åŸå§‹ç²¾åº¦\n",
    "            normalized = normalized.to(original_dtype)\n",
    "        else:\n",
    "            # æ¨™æº– LayerNorm\n",
    "            normalized = F.layer_norm(input, self.normalized_shape, eps=self.eps)\n",
    "        \n",
    "        # æ‡‰ç”¨å¯å­¸ç¿’åƒæ•¸\n",
    "        if self.elementwise_affine:\n",
    "            # ç¢ºä¿æ¬Šé‡ç²¾åº¦åŒ¹é…\n",
    "            weight = self.weight.to(normalized.dtype)\n",
    "            bias = self.bias.to(normalized.dtype)\n",
    "            normalized = normalized * weight + bias\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "print(\"âœ… ç²¾åº¦æ„ŸçŸ¥ LayerNorm å¯¦ç¾å®Œæˆ\")\n",
    "\n",
    "# LayerNorm ç²¾åº¦å½±éŸ¿æ¸¬è©¦\n",
    "def test_layernorm_precision():\n",
    "    \"\"\"æ¸¬è©¦ LayerNorm ç²¾åº¦å½±éŸ¿\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LayerNorm ç²¾åº¦å½±éŸ¿æ¸¬è©¦\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æ¸¬è©¦æ•¸æ“š\n",
    "    hidden_dim = 768\n",
    "    batch_size = 4\n",
    "    seq_len = 128\n",
    "    \n",
    "    # å‰µå»ºæ¸¬è©¦è¼¸å…¥ï¼ˆæ¨¡æ“¬æ¥µç«¯æƒ…æ³ï¼‰\n",
    "    x = torch.randn(batch_size, seq_len, hidden_dim, device=device, dtype=torch.float32)\n",
    "    x = x * 100 + 1000  # å¢åŠ æ•¸å€¼ç¯„åœä»¥æ¸¬è©¦ç²¾åº¦æ•æ„Ÿæ€§\n",
    "    \n",
    "    # ä¸åŒ LayerNorm å¯¦ç¾\n",
    "    ln_standard = nn.LayerNorm(hidden_dim).to(device)\n",
    "    ln_precision_aware = PrecisionAwareLayerNorm(hidden_dim, force_fp32_stats=True).to(device)\n",
    "    ln_precision_aware_false = PrecisionAwareLayerNorm(hidden_dim, force_fp32_stats=False).to(device)\n",
    "    \n",
    "    # è¤‡è£½æ¬Šé‡ä»¥ç¢ºä¿å…¬å¹³æ¯”è¼ƒ\n",
    "    with torch.no_grad():\n",
    "        ln_precision_aware.weight.copy_(ln_standard.weight)\n",
    "        ln_precision_aware.bias.copy_(ln_standard.bias)\n",
    "        ln_precision_aware_false.weight.copy_(ln_standard.weight)\n",
    "        ln_precision_aware_false.bias.copy_(ln_standard.bias)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. FP32 åŸºæº–\n",
    "    with torch.no_grad():\n",
    "        fp32_output = ln_standard(x)\n",
    "    results['fp32_baseline'] = fp32_output\n",
    "    \n",
    "    # 2. FP16 æ¨™æº– LayerNorm\n",
    "    ln_standard.half()\n",
    "    with torch.no_grad():\n",
    "        fp16_output = ln_standard(x.half())\n",
    "    results['fp16_standard'] = fp16_output.float()\n",
    "    \n",
    "    # 3. FP16 + ç²¾åº¦æ„ŸçŸ¥ LayerNorm (FP32 çµ±è¨ˆ)\n",
    "    ln_precision_aware.half()\n",
    "    with torch.no_grad():\n",
    "        fp16_aware_output = ln_precision_aware(x.half())\n",
    "    results['fp16_precision_aware'] = fp16_aware_output.float()\n",
    "    \n",
    "    # 4. FP16 + ç²¾åº¦æ„ŸçŸ¥ LayerNorm (FP16 çµ±è¨ˆ)\n",
    "    ln_precision_aware_false.half()\n",
    "    with torch.no_grad():\n",
    "        fp16_aware_false_output = ln_precision_aware_false(x.half())\n",
    "    results['fp16_no_fp32_stats'] = fp16_aware_false_output.float()\n",
    "    \n",
    "    # åˆ†æèª¤å·®\n",
    "    analyzer = PrecisionAnalyzer()\n",
    "    baseline = results['fp32_baseline']\n",
    "    \n",
    "    print(f\"\\n{'LayerNorm é…ç½®':<30} {'MSE':<12} {'æœ€å¤§èª¤å·®':<12} {'ç›¸å°èª¤å·®':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for key, name in [('fp16_standard', 'FP16 æ¨™æº–'),\n",
    "                      ('fp16_precision_aware', 'FP16 + FP32çµ±è¨ˆ'),\n",
    "                      ('fp16_no_fp32_stats', 'FP16 + FP16çµ±è¨ˆ')]:\n",
    "        mse = analyzer.compute_numerical_error(baseline, results[key], 'mse')\n",
    "        max_err = analyzer.compute_numerical_error(baseline, results[key], 'max')\n",
    "        rel_err = analyzer.compute_numerical_error(baseline, results[key], 'relative')\n",
    "        \n",
    "        print(f\"{name:<30} {mse:<12.2e} {max_err:<12.2e} {rel_err:<12.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# é‹è¡Œ LayerNorm æ¸¬è©¦\n",
    "layernorm_results = test_layernorm_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¯¦éš›è¨“ç·´å ´æ™¯ç²¾åº¦ç›£æ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionMonitor:\n",
    "    \"\"\"è¨“ç·´éç¨‹ä¸­çš„ç²¾åº¦ç›£æ§å·¥å…·\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor_layers=None, log_frequency=100):\n",
    "        self.monitor_layers = monitor_layers or ['attention', 'layernorm', 'output']\n",
    "        self.log_frequency = log_frequency\n",
    "        self.precision_history = defaultdict(list)\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"è¨»å†Šç›£æ§é‰¤å­\"\"\"\n",
    "        hooks = []\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if any(layer_type in name.lower() for layer_type in self.monitor_layers):\n",
    "                hook = module.register_forward_hook(\n",
    "                    lambda module, input, output, name=name: self._log_precision(name, output)\n",
    "                )\n",
    "                hooks.append((name, hook))\n",
    "        \n",
    "        return hooks\n",
    "    \n",
    "    def _log_precision(self, layer_name, output):\n",
    "        \"\"\"è¨˜éŒ„ç²¾åº¦ä¿¡æ¯\"\"\"\n",
    "        if self.step_count % self.log_frequency == 0:\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]  # å–ç¬¬ä¸€å€‹è¼¸å‡º\n",
    "            \n",
    "            if torch.is_tensor(output):\n",
    "                info = {\n",
    "                    'step': self.step_count,\n",
    "                    'dtype': str(output.dtype),\n",
    "                    'mean': output.mean().item(),\n",
    "                    'std': output.std().item(),\n",
    "                    'min': output.min().item(),\n",
    "                    'max': output.max().item(),\n",
    "                    'has_nan': torch.isnan(output).any().item(),\n",
    "                    'has_inf': torch.isinf(output).any().item()\n",
    "                }\n",
    "                self.precision_history[layer_name].append(info)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"å¢åŠ æ­¥æ•¸è¨ˆæ•¸\"\"\"\n",
    "        self.step_count += 1\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"ç²å–ç²¾åº¦ç›£æ§æ‘˜è¦\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            if history:\n",
    "                latest = history[-1]\n",
    "                summary[layer_name] = {\n",
    "                    'latest_dtype': latest['dtype'],\n",
    "                    'nan_detected': any(h['has_nan'] for h in history),\n",
    "                    'inf_detected': any(h['has_inf'] for h in history),\n",
    "                    'value_range': (min(h['min'] for h in history), max(h['max'] for h in history)),\n",
    "                    'records_count': len(history)\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_precision_trends(self):\n",
    "        \"\"\"ç¹ªè£½ç²¾åº¦è¶¨å‹¢åœ–\"\"\"\n",
    "        if not self.precision_history:\n",
    "            print(\"âš ï¸  ç„¡ç²¾åº¦æ•¸æ“šå¯ç¹ªè£½\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('è¨“ç·´éç¨‹ç²¾åº¦ç›£æ§', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # å­åœ–1ï¼šå‡å€¼è¶¨å‹¢\n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            steps = [h['step'] for h in history]\n",
    "            means = [h['mean'] for h in history]\n",
    "            axes[0, 0].plot(steps, means, label=layer_name.split('.')[-1], marker='o', markersize=4)\n",
    "        \n",
    "        axes[0, 0].set_xlabel('è¨“ç·´æ­¥æ•¸')\n",
    "        axes[0, 0].set_ylabel('è¼¸å‡ºå‡å€¼')\n",
    "        axes[0, 0].set_title('è¼¸å‡ºå‡å€¼è¶¨å‹¢')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # å­åœ–2ï¼šæ¨™æº–å·®è¶¨å‹¢\n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            steps = [h['step'] for h in history]\n",
    "            stds = [h['std'] for h in history]\n",
    "            axes[0, 1].plot(steps, stds, label=layer_name.split('.')[-1], marker='s', markersize=4)\n",
    "        \n",
    "        axes[0, 1].set_xlabel('è¨“ç·´æ­¥æ•¸')\n",
    "        axes[0, 1].set_ylabel('è¼¸å‡ºæ¨™æº–å·®')\n",
    "        axes[0, 1].set_title('è¼¸å‡ºæ¨™æº–å·®è¶¨å‹¢')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # å­åœ–3ï¼šæ•¸å€¼ç¯„åœ\n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            steps = [h['step'] for h in history]\n",
    "            ranges = [h['max'] - h['min'] for h in history]\n",
    "            axes[1, 0].semilogy(steps, ranges, label=layer_name.split('.')[-1], marker='^', markersize=4)\n",
    "        \n",
    "        axes[1, 0].set_xlabel('è¨“ç·´æ­¥æ•¸')\n",
    "        axes[1, 0].set_ylabel('æ•¸å€¼ç¯„åœ (log scale)')\n",
    "        axes[1, 0].set_title('æ•¸å€¼ç¯„åœè®ŠåŒ–')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # å­åœ–4ï¼šç•°å¸¸æª¢æ¸¬\n",
    "        anomaly_counts = {}\n",
    "        for layer_name, history in self.precision_history.items():\n",
    "            nan_count = sum(h['has_nan'] for h in history)\n",
    "            inf_count = sum(h['has_inf'] for h in history)\n",
    "            anomaly_counts[layer_name.split('.')[-1]] = nan_count + inf_count\n",
    "        \n",
    "        if anomaly_counts:\n",
    "            layers = list(anomaly_counts.keys())\n",
    "            counts = list(anomaly_counts.values())\n",
    "            bars = axes[1, 1].bar(layers, counts, color='red', alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('å±¤')\n",
    "            axes[1, 1].set_ylabel('ç•°å¸¸æª¢æ¸¬æ¬¡æ•¸')\n",
    "            axes[1, 1].set_title('NaN/Inf æª¢æ¸¬çµ±è¨ˆ')\n",
    "            axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "            for bar, count in zip(bars, counts):\n",
    "                if count > 0:\n",
    "                    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                                   str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"âœ… ç²¾åº¦ç›£æ§å·¥å…·æº–å‚™å®Œæˆ\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "print(\"\\nğŸ“– ç²¾åº¦ç›£æ§ä½¿ç”¨ç¤ºä¾‹:\")\n",
    "print(\"\"\"\n",
    "# 1. å‰µå»ºç›£æ§å™¨\n",
    "monitor = PrecisionMonitor(monitor_layers=['attention', 'layernorm'], log_frequency=50)\n",
    "\n",
    "# 2. è¨»å†Šç›£æ§é‰¤å­\n",
    "hooks = monitor.register_hooks(model)\n",
    "\n",
    "# 3. è¨“ç·´å¾ªç’°\n",
    "for batch in dataloader:\n",
    "    output = model(batch)\n",
    "    loss = compute_loss(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    monitor.step()  # æ›´æ–°æ­¥æ•¸\n",
    "\n",
    "# 4. æŸ¥çœ‹çµæœ\n",
    "summary = monitor.get_summary()\n",
    "monitor.plot_precision_trends()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç²¾åº¦è™•ç†æª¢æŸ¥æ¸…å–®å’Œæœ€ä½³å¯¦è¸ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæˆç²¾åº¦åˆ†æä»»å‹™\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ FlashAttention ç²¾åº¦è™•ç†å®Œæ•´æŒ‡å—\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "precision_checklist = {\n",
    "    \"ğŸš¨ CRITICAL å±¤ (å¿…é ˆç‰¹æ®Šè™•ç†)\": {\n",
    "        \"Softmax è¨ˆç®—\": {\n",
    "            \"å•é¡Œ\": \"FP16 ç²¾åº¦ä¸‹å®¹æ˜“æ•¸å€¼æº¢å‡ºï¼Œå°è‡´ NaN\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"ä½¿ç”¨ FP32 è¨ˆç®— softmaxï¼Œå†è½‰æ›å› FP16\",\n",
    "            \"ä»£ç¢¼\": \"scores = scores.float(); attn = F.softmax(scores, dim=-1).half()\"\n",
    "        },\n",
    "        \"æ³¨æ„åŠ›åˆ†æ•¸è¨ˆç®—\": {\n",
    "            \"å•é¡Œ\": \"å¤§åºåˆ—é•·åº¦æ™‚åˆ†æ•¸ç¯„åœæ¥µå¤§ï¼ŒFP16 è¡¨ç¤ºç¯„åœä¸è¶³\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"ç¢ºä¿ç¸®æ”¾å› å­æ­£ç¢ºï¼Œè€ƒæ…®ä½¿ç”¨ FP32 è¨ˆç®—\",\n",
    "            \"ä»£ç¢¼\": \"scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(head_dim)\"\n",
    "        },\n",
    "        \"æº«åº¦ç¸®æ”¾\": {\n",
    "            \"å•é¡Œ\": \"ç”Ÿæˆæ™‚çš„æº«åº¦åƒæ•¸æœƒæ”¾å¤§ç²¾åº¦èª¤å·®\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"æº«åº¦ç¸®æ”¾ä½¿ç”¨ FP32 è¨ˆç®—\",\n",
    "            \"ä»£ç¢¼\": \"logits = logits.float() / temperature\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"âš ï¸ HIGH å±¤ (å»ºè­°ç‰¹æ®Šè™•ç†)\": {\n",
    "        \"LayerNorm\": {\n",
    "            \"å•é¡Œ\": \"çµ±è¨ˆè¨ˆç®—ï¼ˆå‡å€¼ã€æ–¹å·®ï¼‰åœ¨ FP16 ä¸‹ç²¾åº¦ä¸è¶³\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"çµ±è¨ˆè¨ˆç®—ä½¿ç”¨ FP32ï¼Œè¼¸å‡ºè½‰å›åŸç²¾åº¦\",\n",
    "            \"ä»£ç¢¼\": \"ä½¿ç”¨ PrecisionAwareLayerNorm æˆ– force_fp32_stats=True\"\n",
    "        },\n",
    "        \"è¼¸å‡ºæŠ•å½±å±¤ (lm_head)\": {\n",
    "            \"å•é¡Œ\": \"è©å½™è¡¨å¤§å°å°è‡´æ¬Šé‡çŸ©é™£å·¨å¤§ï¼Œç´¯ç©èª¤å·®\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"ä¿æŒ FP32 æ¬Šé‡ï¼Œæˆ–ä½¿ç”¨é«˜ç²¾åº¦ç´¯ç©\",\n",
    "            \"ä»£ç¢¼\": \"ä¿æŒ lm_head.weight ç‚º FP32 dtype\"\n",
    "        },\n",
    "        \"æ®˜å·®é€£æ¥ç´¯ç©\": {\n",
    "            \"å•é¡Œ\": \"å¤šå±¤ç´¯ç©å°è‡´ç²¾åº¦æå¤±\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"å®šæœŸæª¢æŸ¥æ•¸å€¼ç¯„åœï¼Œé¿å…ç²¾åº¦é™ç´š\",\n",
    "            \"ä»£ç¢¼\": \"x = x + residual  # ç¢ºä¿ dtype ä¸€è‡´\"\n",
    "        },\n",
    "        \"Loss è¨ˆç®—\": {\n",
    "            \"å•é¡Œ\": \"CrossEntropyLoss åœ¨ FP16 ä¸‹å¯èƒ½ä¸ç©©å®š\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"Loss è¨ˆç®—ä½¿ç”¨ FP32\",\n",
    "            \"ä»£ç¢¼\": \"loss = F.cross_entropy(logits.float(), targets)\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"ğŸ”§ MEDIUM å±¤ (éœ€è¦æ³¨æ„)\": {\n",
    "        \"Linear æŠ•å½±å±¤\": {\n",
    "            \"å•é¡Œ\": \"æ¬Šé‡ç²¾åº¦èˆ‡è¼¸å…¥ç²¾åº¦ä¸åŒ¹é…\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"å‹•æ…‹èª¿æ•´æ¬Šé‡ç²¾åº¦æˆ–ä½¿ç”¨ autocast\",\n",
    "            \"ä»£ç¢¼\": \"ç¢ºä¿ linear.weight.dtype == input.dtype\"\n",
    "        },\n",
    "        \"GELU æ¿€æ´»å‡½æ•¸\": {\n",
    "            \"å•é¡Œ\": \"è¤‡é›œçš„æ•¸å­¸é‹ç®—åœ¨ FP16 ä¸‹ç²¾åº¦æå¤±\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"ä½¿ç”¨ PyTorch å„ªåŒ–çš„ GELU å¯¦ç¾\",\n",
    "            \"ä»£ç¢¼\": \"F.gelu(x, approximate='tanh') æˆ– nn.GELU()\"\n",
    "        },\n",
    "        \"Embedding å±¤\": {\n",
    "            \"å•é¡Œ\": \"Embedding æŸ¥æ‰¾å¾Œçš„ç²¾åº¦è½‰æ›\",\n",
    "            \"è§£æ±ºæ–¹æ¡ˆ\": \"ç¢ºä¿ embedding æ¬Šé‡ç²¾åº¦æ­£ç¢º\",\n",
    "            \"ä»£ç¢¼\": \"embedding = embedding.to(target_dtype)\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, layers in precision_checklist.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for layer_name, details in layers.items():\n",
    "        print(f\"\\nğŸ“ {layer_name}:\")\n",
    "        print(f\"   å•é¡Œ: {details['å•é¡Œ']}\")\n",
    "        print(f\"   è§£æ±º: {details['è§£æ±ºæ–¹æ¡ˆ']}\")\n",
    "        print(f\"   ä»£ç¢¼: {details['ä»£ç¢¼']}\")\n",
    "\n",
    "# éƒ¨ç½²å‰æª¢æŸ¥æ¸…å–®\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ éƒ¨ç½²å‰ç²¾åº¦æª¢æŸ¥æ¸…å–®\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "deployment_checklist = [\n",
    "    \"âœ… ç¢ºèª FlashAttention ç‰ˆæœ¬èˆ‡ PyTorch ç‰ˆæœ¬å…¼å®¹\",\n",
    "    \"âœ… æ¸¬è©¦ FP16 vs FP32 è¼¸å‡ºä¸€è‡´æ€§ï¼ˆç›¸å°èª¤å·® < 1e-2ï¼‰\",\n",
    "    \"âœ… é©—è­‰æ¢¯åº¦ç¸®æ”¾ç­–ç•¥æ­£ç¢ºé…ç½®\",\n",
    "    \"âœ… æª¢æŸ¥ LayerNorm ç²¾åº¦è¨­ç½®\",\n",
    "    \"âœ… ç¢ºèªé—œéµå±¤ï¼ˆSoftmaxã€Lossï¼‰ä½¿ç”¨é«˜ç²¾åº¦è¨ˆç®—\",\n",
    "    \"âœ… æ¸¬è©¦ä¸åŒåºåˆ—é•·åº¦ä¸‹çš„æ•¸å€¼ç©©å®šæ€§\",\n",
    "    \"âœ… ç›£æ§è¨“ç·´éç¨‹ä¸­æ˜¯å¦å‡ºç¾ NaN/Inf\",\n",
    "    \"âœ… é©—è­‰ KV cache ç²¾åº¦ä¸€è‡´æ€§ï¼ˆæ¨è«–æ™‚ï¼‰\",\n",
    "    \"âœ… æ¸¬è©¦æ‰¹æ¬¡å¤§å°è®ŠåŒ–å°ç²¾åº¦çš„å½±éŸ¿\",\n",
    "    \"âœ… ç¢ºèªç”Ÿç”¢ç’°å¢ƒæ€§èƒ½æ»¿è¶³è¦æ±‚\"\n",
    "]\n",
    "\n",
    "for item in deployment_checklist:\n",
    "    print(item)\n",
    "\n",
    "# æ•…éšœæ’é™¤æŒ‡å—\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš¨ å¸¸è¦‹ç²¾åº¦å•é¡Œæ•…éšœæ’é™¤\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "troubleshooting = {\n",
    "    \"Loss è®Šæˆ NaN\": [\n",
    "        \"æª¢æŸ¥ Softmax è¨ˆç®—æ˜¯å¦æº¢å‡º\",\n",
    "        \"é™ä½å­¸ç¿’ç‡æˆ–ä½¿ç”¨æ¢¯åº¦è£å‰ª\",\n",
    "        \"ç¢ºä¿ Loss è¨ˆç®—ä½¿ç”¨ FP32\",\n",
    "        \"æª¢æŸ¥è¼¸å…¥æ•¸æ“šæ˜¯å¦åŒ…å«æ¥µå€¼\"\n",
    "    ],\n",
    "    \"è¨“ç·´ä¸æ”¶æ–‚\": [\n",
    "        \"æ¯”è¼ƒ FP32 vs FP16 è¨“ç·´æ›²ç·š\",\n",
    "        \"æª¢æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸ç´¯ç©\",\n",
    "        \"èª¿æ•´ GradScaler åƒæ•¸\",\n",
    "        \"å¢åŠ é—œéµå±¤çš„ç²¾åº¦\"\n",
    "    ],\n",
    "    \"æ¨è«–çµæœä¸ä¸€è‡´\": [\n",
    "        \"æª¢æŸ¥æ¨¡å‹åŠ è¼‰æ™‚çš„ç²¾åº¦è½‰æ›\",\n",
    "        \"é©—è­‰ KV cache ç²¾åº¦ä¸€è‡´æ€§\",\n",
    "        \"ç¢ºèª tokenizer ç²¾åº¦è¨­ç½®\",\n",
    "        \"æ¸¬è©¦ç›¸åŒè¼¸å…¥çš„å¯é‡ç¾æ€§\"\n",
    "    ],\n",
    "    \"è¨˜æ†¶é«”ä½¿ç”¨ç•°å¸¸\": [\n",
    "        \"æª¢æŸ¥æ˜¯å¦æœ‰ç²¾åº¦è½‰æ›å°è‡´çš„å…§å­˜æ´©æ¼\",\n",
    "        \"ç¢ºèªæ¬Šé‡å…±äº«æ­£ç¢º\",\n",
    "        \"ç›£æ§æ¢¯åº¦ç´¯ç©éç¨‹\",\n",
    "        \"ä½¿ç”¨ torch.cuda.empty_cache() æ¸…ç†\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for problem, solutions in troubleshooting.items():\n",
    "    print(f\"\\nâŒ {problem}:\")\n",
    "    for solution in solutions:\n",
    "        print(f\"   â€¢ {solution}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ FlashAttention ç²¾åº¦åˆ†æå®Œæˆï¼\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸ”‘ é—œéµè¦é»:\")\n",
    "print(\"â€¢ FlashAttention æœ¬èº«æ˜¯æ•¸å­¸ç­‰åƒ¹çš„ï¼Œç²¾åº¦å•é¡Œä¸»è¦ä¾†è‡ª FP16 é™åˆ¶\")\n",
    "print(\"â€¢ é—œéµæ˜¯è­˜åˆ¥ç²¾åº¦æ•æ„Ÿå±¤ä¸¦æ¡ç”¨é©ç•¶çš„è™•ç†ç­–ç•¥\")\n",
    "print(\"â€¢ æ··åˆç²¾åº¦ç­–ç•¥å¯ä»¥å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦\")\n",
    "print(\"â€¢ æŒçºŒç›£æ§å’Œæ¸¬è©¦æ˜¯ç¢ºä¿ç²¾åº¦çš„é—œéµ\")\n",
    "print(\"\\nğŸ’¡ å»ºè­°:\")\n",
    "print(\"â€¢ é–‹ç™¼éšæ®µä½¿ç”¨ä¿å®ˆç²¾åº¦ç­–ç•¥\")\n",
    "print(\"â€¢ ç”Ÿç”¢éƒ¨ç½²ä½¿ç”¨æ··åˆç²¾åº¦ç­–ç•¥\")\n",
    "print(\"â€¢ é«˜æ€§èƒ½å ´æ™¯å¯è€ƒæ…®æ¿€é€²ç²¾åº¦ç­–ç•¥\")\n",
    "print(\"â€¢ å»ºç«‹å®Œæ•´çš„ç²¾åº¦æ¸¬è©¦å’Œç›£æ§é«”ç³»\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}