[tool.poetry]
name = "llm-engineering-course"
version = "0.3.0"
description = "Advanced LLM Engineering Course - Training, Optimization, Inference & Serving"
authors = ["LLM Teaching Project Team <noreply@example.com>"]
readme = "README.md"
package-mode = false

[tool.poetry.dependencies]
python = ">=3.10,<3.13"

# PyTorch with CUDA 12.4 (>= 2.6 required)
torch = {version = ">=2.6.0", source = "pytorch-cu124"}
torchvision = {version = ">=0.21.0", source = "pytorch-cu124"}
torchaudio = {version = ">=2.6.0", source = "pytorch-cu124"}

# Core Libraries
datasets = ">=2.14.0"
accelerate = ">=0.24.0"

# Inference & Serving (Chapter 2)
fastapi = ">=0.104.0"
uvicorn = {version = ">=0.24.0", extras = ["standard"]}
prometheus-client = ">=0.19.0"
aiohttp = ">=3.9.0"
websockets = ">=12.0"

# Data & Utility
numpy = ">=1.24.0"
pandas = ">=2.0.0"
scikit-learn = ">=1.3.0"

# Development tools
jupyterlab = "^4.4.9"
ipywidgets = "^8.1.7"
matplotlib = "^3.10.6"
seaborn = "^0.13.2"
tqdm = "^4.67.1"

# Transformers and PEFT
transformers = "^4.57.0"
peft = ">=0.7.0"
bitsandbytes = ">=0.48.1"
sentencepiece = ">=0.2.1"
protobuf = ">=3.20.0"

# Jupyter kernel
ipykernel = "^6.29.0"

# Inference engines (Chapter 2)
vllm = ">=0.6.0"

# flash-attn 需要從源碼編譯，不通過 Poetry 管理
# 安裝方式: pip install flash-attn --no-build-isolation
auto-gptq = "^0.7.1"
autoawq = "^0.2.9"

[[tool.poetry.source]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
priority = "explicit"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
