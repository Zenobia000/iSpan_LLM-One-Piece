{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.1 - OpenCompass è©•ä¼°å¯¦æˆ°\n",
    "## Notebook 02: åŸ·è¡Œæ¨¡å‹è©•ä¼°\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™**:\n",
    "1. é…ç½®è©•ä¼°åƒæ•¸ (batch size, æ¨ç†æ¨¡å¼)\n",
    "2. åŸ·è¡Œ C-Eval è©•ä¼° (STEM, ç¤¾æœƒç§‘å­¸, äººæ–‡å­¸ç§‘)\n",
    "3. æ”¶é›†è©•ä¼°æ—¥èªŒèˆ‡ä¸­é–“çµæœ\n",
    "4. è™•ç†è©•ä¼°éç¨‹ä¸­çš„å¸¸è¦‹å•é¡Œ\n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 1-2 å°æ™‚ (å–æ±ºæ–¼ç¡¬é«”)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥é…ç½®èˆ‡æ¨¡å‹\n",
    "\n",
    "å¾ 01-Setup.ipynb è¼‰å…¥é…ç½®å’Œæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# æ•¸æ“šç›®éŒ„\n",
    "DATA_DIR = Path(\"./data\")\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# è¼‰å…¥é…ç½®\n",
    "config_path = DATA_DIR / \"eval_config.json\"\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    eval_config = json.load(f)\n",
    "\n",
    "print(\"âœ… é…ç½®å·²è¼‰å…¥\")\n",
    "print(json.dumps(eval_config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é‡æ–°è¼‰å…¥æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡åŒ–é…ç½®\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    \"\"\"è¼‰å…¥æ¨¡å‹å’Œ tokenizer\"\"\"\n",
    "    print(f\"ğŸ“¥ è¼‰å…¥æ¨¡å‹: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config if torch.cuda.is_available() else None,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for model_key, model_info in eval_config[\"models\"].items():\n",
    "    if model_info[\"loaded\"]:\n",
    "        try:\n",
    "            model, tokenizer = load_model(model_info[\"name\"])\n",
    "            models[model_key] = model\n",
    "            tokenizers[model_key] = tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¼‰å…¥ {model_key} å¤±æ•—: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥è©•ä¼°æ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ C-Eval æ•¸æ“šé›†\n",
    "ceval_subjects = eval_config[\"datasets\"][\"ceval\"][\"subjects\"]\n",
    "\n",
    "print(\"ğŸ“¥ è¼‰å…¥ C-Eval æ•¸æ“šé›†...\")\n",
    "ceval_data = {}\n",
    "\n",
    "for subject in ceval_subjects:\n",
    "    dataset = load_dataset(\n",
    "        \"ceval/ceval-exam\",\n",
    "        subject,\n",
    "        split=\"val\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    ceval_data[subject] = dataset\n",
    "    print(f\"  âœ… {subject}: {len(dataset)} é¡Œ\")\n",
    "\n",
    "print(f\"\\nç¸½è¨ˆ: {sum(len(d) for d in ceval_data.values())} é¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¯¦ä½œè©•ä¼°å‡½æ•¸\n",
    "\n",
    "å¯¦ä½œå¤šé¸é¡Œè©•ä¼°é‚è¼¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ceval_prompt(question: str, choices: dict) -> str:\n",
    "    \"\"\"\n",
    "    æ ¼å¼åŒ– C-Eval å•é¡Œç‚ºæç¤º\n",
    "    \n",
    "    Args:\n",
    "        question: å•é¡Œæ–‡æœ¬\n",
    "        choices: é¸é …å­—å…¸ {\"A\": \"...\", \"B\": \"...\", ...}\n",
    "    \n",
    "    Returns:\n",
    "        æ ¼å¼åŒ–çš„æç¤ºæ–‡æœ¬\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"ä»¥ä¸‹æ˜¯ä¸€é“é¸æ“‡é¡Œ,è«‹é¸å‡ºæ­£ç¢ºç­”æ¡ˆã€‚\n",
    "\n",
    "å•é¡Œ: {question}\n",
    "\n",
    "A. {choices['A']}\n",
    "B. {choices['B']}\n",
    "C. {choices['C']}\n",
    "D. {choices['D']}\n",
    "\n",
    "ç­”æ¡ˆ:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def evaluate_multiple_choice(model, tokenizer, question: str, choices: dict, correct_answer: str):\n",
    "    \"\"\"\n",
    "    è©•ä¼°å¤šé¸é¡Œ\n",
    "    \n",
    "    ä½¿ç”¨å°æ•¸ä¼¼ç„¶æ³•é¸æ“‡ç­”æ¡ˆ:\n",
    "    1. è¨ˆç®—æ¯å€‹é¸é …çš„å°æ•¸ä¼¼ç„¶\n",
    "    2. é¸æ“‡ä¼¼ç„¶æœ€é«˜çš„é¸é …\n",
    "    \n",
    "    Args:\n",
    "        model: æ¨¡å‹\n",
    "        tokenizer: Tokenizer\n",
    "        question: å•é¡Œæ–‡æœ¬\n",
    "        choices: é¸é …å­—å…¸\n",
    "        correct_answer: æ­£ç¢ºç­”æ¡ˆ (A/B/C/D)\n",
    "    \n",
    "    Returns:\n",
    "        (predicted_answer, is_correct, logits_dict)\n",
    "    \"\"\"\n",
    "    prompt = format_ceval_prompt(question, choices)\n",
    "    \n",
    "    # ç·¨ç¢¼æç¤º\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹é¸é …çš„å°æ•¸ä¼¼ç„¶\n",
    "    option_logits = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            # ç·¨ç¢¼é¸é …\n",
    "            option_ids = tokenizer.encode(f\" {option}\", add_special_tokens=False)\n",
    "            \n",
    "            # ç²å–æ¨¡å‹è¼¸å‡º\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # ç²å–æœ€å¾Œä¸€å€‹ token çš„ logits\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # ç²å–é¸é …çš„ logit\n",
    "            option_logit = last_token_logits[option_ids[0]].item()\n",
    "            option_logits[option] = option_logit\n",
    "    \n",
    "    # é¸æ“‡ logit æœ€é«˜çš„é¸é …\n",
    "    predicted_answer = max(option_logits, key=option_logits.get)\n",
    "    is_correct = (predicted_answer == correct_answer)\n",
    "    \n",
    "    return predicted_answer, is_correct, option_logits\n",
    "\n",
    "\n",
    "# æ¸¬è©¦è©•ä¼°å‡½æ•¸\n",
    "if ceval_data and models:\n",
    "    test_subject = list(ceval_data.keys())[0]\n",
    "    test_sample = ceval_data[test_subject][0]\n",
    "    test_model = list(models.values())[0]\n",
    "    test_tokenizer = list(tokenizers.values())[0]\n",
    "    \n",
    "    print(\"ğŸ§ª æ¸¬è©¦è©•ä¼°å‡½æ•¸...\")\n",
    "    print(f\"å•é¡Œ: {test_sample['question']}\")\n",
    "    \n",
    "    pred, correct, logits = evaluate_multiple_choice(\n",
    "        test_model,\n",
    "        test_tokenizer,\n",
    "        test_sample['question'],\n",
    "        {k: test_sample[k] for k in ['A', 'B', 'C', 'D']},\n",
    "        test_sample['answer']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\né æ¸¬ç­”æ¡ˆ: {pred}\")\n",
    "    print(f\"æ­£ç¢ºç­”æ¡ˆ: {test_sample['answer']}\")\n",
    "    print(f\"æ˜¯å¦æ­£ç¢º: {correct}\")\n",
    "    print(f\"\\nLogits: {logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åŸ·è¡Œå®Œæ•´è©•ä¼°\n",
    "\n",
    "å°æ‰€æœ‰æ¨¡å‹å’Œå­¸ç§‘é€²è¡Œè©•ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_subject(model, tokenizer, dataset, subject_name: str):\n",
    "    \"\"\"\n",
    "    è©•ä¼°å–®å€‹å­¸ç§‘\n",
    "    \n",
    "    Args:\n",
    "        model: æ¨¡å‹\n",
    "        tokenizer: Tokenizer\n",
    "        dataset: æ•¸æ“šé›†\n",
    "        subject_name: å­¸ç§‘åç¨±\n",
    "    \n",
    "    Returns:\n",
    "        è©•ä¼°çµæœå­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"\\nè©•ä¼°å­¸ç§‘: {subject_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for sample in tqdm(dataset, desc=subject_name):\n",
    "        question = sample['question']\n",
    "        choices = {k: sample[k] for k in ['A', 'B', 'C', 'D']}\n",
    "        correct_answer = sample['answer']\n",
    "        \n",
    "        pred, is_correct, logits = evaluate_multiple_choice(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            question,\n",
    "            choices,\n",
    "            correct_answer\n",
    "        )\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'choices': choices,\n",
    "            'correct_answer': correct_answer,\n",
    "            'predicted_answer': pred,\n",
    "            'is_correct': is_correct,\n",
    "            'logits': logits\n",
    "        })\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    \n",
    "    print(f\"\\næº–ç¢ºç‡: {accuracy:.2%} ({correct_count}/{total_count})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'subject': subject_name,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct_count,\n",
    "        'total': total_count,\n",
    "        'details': results\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model_on_ceval(model, tokenizer, model_name: str, ceval_data: dict):\n",
    "    \"\"\"\n",
    "    è©•ä¼°æ¨¡å‹åœ¨ C-Eval ä¸Šçš„è¡¨ç¾\n",
    "    \n",
    "    Args:\n",
    "        model: æ¨¡å‹\n",
    "        tokenizer: Tokenizer\n",
    "        model_name: æ¨¡å‹åç¨±\n",
    "        ceval_data: C-Eval æ•¸æ“šé›†å­—å…¸\n",
    "    \n",
    "    Returns:\n",
    "        è©•ä¼°çµæœ\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"è©•ä¼°æ¨¡å‹: {model_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    subject_results = []\n",
    "    \n",
    "    for subject, dataset in ceval_data.items():\n",
    "        result = evaluate_subject(model, tokenizer, dataset, subject)\n",
    "        subject_results.append(result)\n",
    "    \n",
    "    # è¨ˆç®—ç¸½é«”æº–ç¢ºç‡\n",
    "    total_correct = sum(r['correct'] for r in subject_results)\n",
    "    total_questions = sum(r['total'] for r in subject_results)\n",
    "    overall_accuracy = total_correct / total_questions\n",
    "    \n",
    "    # æŒ‰å­¸ç§‘åˆ†é¡è¨ˆç®—æº–ç¢ºç‡\n",
    "    subject_categories = {\n",
    "        'STEM': ['computer_science', 'physics', 'mathematics'],\n",
    "        'Humanities': ['chinese_language_and_literature', 'history'],\n",
    "        'Social Science': ['law', 'economics']\n",
    "    }\n",
    "    \n",
    "    category_accuracies = {}\n",
    "    for category, subjects in subject_categories.items():\n",
    "        category_results = [r for r in subject_results if r['subject'] in subjects]\n",
    "        if category_results:\n",
    "            cat_correct = sum(r['correct'] for r in category_results)\n",
    "            cat_total = sum(r['total'] for r in category_results)\n",
    "            category_accuracies[category] = cat_correct / cat_total\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'category_accuracies': category_accuracies,\n",
    "        'subject_results': subject_results,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŸ·è¡Œè©•ä¼° (Llama-2-7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama-2-7b' in models:\n",
    "    llama_results = evaluate_model_on_ceval(\n",
    "        models['llama-2-7b'],\n",
    "        tokenizers['llama-2-7b'],\n",
    "        'Llama-2-7B',\n",
    "        ceval_data\n",
    "    )\n",
    "    \n",
    "    # ä¿å­˜çµæœ\n",
    "    results_path = RESULTS_DIR / \"llama2_7b_results.json\"\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(llama_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    print(f\"\\nâœ… çµæœå·²ä¿å­˜: {results_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Llama-2-7B æœªè¼‰å…¥,è·³éè©•ä¼°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŸ·è¡Œè©•ä¼° (Qwen-7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'qwen-7b' in models:\n",
    "    qwen_results = evaluate_model_on_ceval(\n",
    "        models['qwen-7b'],\n",
    "        tokenizers['qwen-7b'],\n",
    "        'Qwen-7B',\n",
    "        ceval_data\n",
    "    )\n",
    "    \n",
    "    # ä¿å­˜çµæœ\n",
    "    results_path = RESULTS_DIR / \"qwen_7b_results.json\"\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qwen_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    print(f\"\\nâœ… çµæœå·²ä¿å­˜: {results_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Qwen-7B æœªè¼‰å…¥,è·³éè©•ä¼°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åˆæ­¥çµæœæŸ¥çœ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results_summary(results: dict):\n",
    "    \"\"\"\n",
    "    é¡¯ç¤ºè©•ä¼°çµæœæ‘˜è¦\n",
    "    \n",
    "    Args:\n",
    "        results: è©•ä¼°çµæœå­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"æ¨¡å‹: {results['model_name']}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    print(f\"\\næ•´é«”æº–ç¢ºç‡: {results['overall_accuracy']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nåˆ†é¡æº–ç¢ºç‡:\")\n",
    "    for category, accuracy in results['category_accuracies'].items():\n",
    "        print(f\"  {category:20s}: {accuracy:.2%}\")\n",
    "    \n",
    "    print(f\"\\nå­¸ç§‘æº–ç¢ºç‡:\")\n",
    "    for subject_result in results['subject_results']:\n",
    "        print(f\"  {subject_result['subject']:35s}: {subject_result['accuracy']:.2%}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "if 'llama_results' in locals():\n",
    "    display_results_summary(llama_results)\n",
    "\n",
    "if 'qwen_results' in locals():\n",
    "    display_results_summary(qwen_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama_results' in locals() and 'qwen_results' in locals():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"æ¨¡å‹å°æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ•´é«”å°æ¯”\n",
    "    print(f\"\\næ•´é«”æº–ç¢ºç‡:\")\n",
    "    print(f\"  Llama-2-7B: {llama_results['overall_accuracy']:.2%}\")\n",
    "    print(f\"  Qwen-7B:    {qwen_results['overall_accuracy']:.2%}\")\n",
    "    print(f\"  å·®è·:       {(qwen_results['overall_accuracy'] - llama_results['overall_accuracy']):.2%}\")\n",
    "    \n",
    "    # åˆ†é¡å°æ¯”\n",
    "    print(f\"\\nåˆ†é¡æº–ç¢ºç‡å°æ¯”:\")\n",
    "    for category in llama_results['category_accuracies'].keys():\n",
    "        llama_acc = llama_results['category_accuracies'][category]\n",
    "        qwen_acc = qwen_results['category_accuracies'][category]\n",
    "        diff = qwen_acc - llama_acc\n",
    "        print(f\"  {category:20s}: Llama {llama_acc:.2%} | Qwen {qwen_acc:.2%} | Î” {diff:+.2%}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸ éœ€è¦å…©å€‹æ¨¡å‹çš„çµæœæ‰èƒ½é€²è¡Œå°æ¯”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ç¸½çµ\n",
    "\n",
    "åœ¨æœ¬ notebook ä¸­,æˆ‘å€‘å®Œæˆäº†:\n",
    "\n",
    "1. âœ… é…ç½®è©•ä¼°åƒæ•¸\n",
    "2. âœ… å¯¦ä½œå¤šé¸é¡Œè©•ä¼°é‚è¼¯ (å°æ•¸ä¼¼ç„¶æ³•)\n",
    "3. âœ… åŸ·è¡Œ C-Eval è©•ä¼°\n",
    "4. âœ… ä¿å­˜è©•ä¼°çµæœ\n",
    "5. âœ… åˆæ­¥çµæœå°æ¯”\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ **03-Analyze.ipynb** é€²è¡Œæ·±å…¥çš„çµæœåˆ†æã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**è§€å¯Ÿ**:\n",
    "- Qwen-7B é€šå¸¸åœ¨ä¸­æ–‡ä»»å‹™ä¸Šè¡¨ç¾æ›´å¥½\n",
    "- STEM é¡åˆ¥é€šå¸¸æ¯”äººæ–‡é¡åˆ¥æ›´å…·æŒ‘æˆ°æ€§\n",
    "- 4-bit é‡åŒ–å¯èƒ½ç•¥å¾®é™ä½æº–ç¢ºç‡ (ç´„ 1-2%)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
