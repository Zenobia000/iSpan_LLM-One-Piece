# 大模型推理技術 2.2 模型服務與優化 (Model Serving & Optimization)

本教學模組深入探討大型語言模型 (LLM) 服務架構與推理優化技術，涵蓋從服務設計到生產部署的完整知識體系。

| 層次 | 學習目標 | 核心內容 | 產出 |
| :--- | :--- | :--- | :--- |
| **Fundamentals** | 理解服務架構與優化目標 | API 設計、延遲與吞吐量 | 掌握服務設計原則 |
| **First Principles** | 掌握優化理論與數學模型 | 記憶體、吞吐量、延遲優化 | 理解優化演算法原理 |
| **Body of Knowledge** | 精通生產部署最佳實踐 | FastAPI、Triton、監控 | 具備生產部署能力 |

---

## 1. Fundamentals (基礎)

### 1.1 推理服務架構

**多層架構設計**:
```
┌──────────────────────────┐
│   API Gateway / LB       │  負載均衡、路由
├──────────────────────────┤
│   Application Layer      │  業務邏輯、認證
│   (FastAPI / Flask)      │
├──────────────────────────┤
│   Model Serving Layer    │  推理引擎
│   (vLLM / TensorRT)      │
├──────────────────────────┤
│   Monitoring & Logging   │  可觀測性
│   (Prometheus / Grafana) │
└──────────────────────────┘
```

### 1.2 推理優化三大目標

**1. 延遲優化 (Latency)**:
- TTFT (Time to First Token): 首個 token 延遲 (<500ms)
- ITL (Inter-Token Latency): Token 間延遲 (<50ms)
- E2E Latency: 端到端請求延遲

**2. 吞吐量優化 (Throughput)**:
- Tokens/second: 每秒生成 token 數 (>2000)
- Requests/second: 每秒處理請求數 (>100)
- GPU 利用率: 目標 >85%

**3. 成本優化 (Cost)**:
- 記憶體效率: KV Cache 管理
- 批次大小: 最大化並發
- 資源利用: Auto-scaling

### 1.3 服務協議與接口設計

#### 1.3.1 RESTful API 設計

**標準 API 端點**:
```
POST /v1/completions          # 文本完成
POST /v1/chat/completions     # 對話完成
POST /v1/embeddings          # 向量嵌入
GET  /v1/models              # 模型列表
GET  /health                 # 健康檢查
GET  /metrics                # 監控指標
```

**OpenAI 兼容格式**:
```json
{
  "model": "llama-2-7b",
  "messages": [
    {"role": "user", "content": "Hello world"}
  ],
  "temperature": 0.7,
  "max_tokens": 100,
  "stream": true
}
```

#### 1.3.2 gRPC 高性能協議

**Protocol Buffer 定義**:
```protobuf
service LLMService {
  rpc Generate(GenerateRequest) returns (stream GenerateResponse);
  rpc Embed(EmbedRequest) returns (EmbedResponse);
}

message GenerateRequest {
  string model = 1;
  string prompt = 2;
  GenerationConfig config = 3;
}

message GenerateResponse {
  string text = 1;
  bool finished = 2;
  TokenUsage usage = 3;
}
```

**性能優勢**:
- HTTP/2 多路復用
- Protocol Buffer 序列化效率
- 雙向流式傳輸
- 減少 50% 網路開銷

#### 1.3.3 WebSocket 實時通信

**適用場景**:
- 實時對話系統
- 長時間生成任務
- 多輪交互應用

```javascript
// WebSocket 客戶端範例
const ws = new WebSocket('ws://localhost:8080/v1/chat');

ws.onmessage = function(event) {
  const response = JSON.parse(event.data);
  if (response.type === 'token') {
    appendToken(response.content);
  }
};

ws.send(JSON.stringify({
  type: 'generate',
  prompt: 'Hello world',
  config: { max_tokens: 100 }
}));
```

---

## 2. 模型服務架構深度解析

### 2.1 Triton Inference Server 企業級架構

#### 2.1.1 多模型管理架構

```
Triton Server 架構:
┌─────────────────────────────────────┐
│            HTTP/gRPC               │
│         C++ API Frontend           │
├─────────────────────────────────────┤
│          Model Repository          │
│      ┌─────────┬─────────┬─────────┐│
│      │ Model A │ Model B │ Model C ││
│      │ (v1.0)  │ (v2.1)  │ (v1.5)  ││
│      └─────────┴─────────┴─────────┘│
├─────────────────────────────────────┤
│         Backend Engines            │
│  ┌──────────┬──────────┬──────────┐ │
│  │ PyTorch  │ TensorRT │ Python   │ │
│  │ Backend  │ Backend  │ Backend  │ │
│  └──────────┴──────────┴──────────┘ │
└─────────────────────────────────────┘
```

#### 2.1.2 動態批次與模型調度

**Triton 動態批次配置**:
```json
{
  "name": "llama-2-7b",
  "platform": "pytorch_libtorch",
  "max_batch_size": 32,
  "dynamic_batching": {
    "preferred_batch_size": [4, 8, 16],
    "max_queue_delay_microseconds": 5000,
    "preserve_ordering": true
  }
}
```

**批次調度策略**:
- **Earliest Deadline First (EDF)**: 優先處理截止時間最早的請求
- **Shortest Job First (SJF)**: 優先處理預估時間最短的請求
- **Round Robin**: 輪詢調度，確保公平性

#### 2.1.3 Backend 架構與擴展性

**自定義 Python Backend**:
```python
import triton_python_backend_utils as pb_utils

class TritonPythonModel:
    def initialize(self, args):
        self.model = load_vllm_model(args['model_config'])

    def execute(self, requests):
        responses = []
        batch_requests = [pb_utils.get_input_tensor_by_name(r, "input")
                         for r in requests]

        # 批次推理
        outputs = self.model.generate(batch_requests)

        for output in outputs:
            response = pb_utils.InferenceResponse(
                output_tensors=[pb_utils.Tensor("output", output)]
            )
            responses.append(response)

        return responses
```

### 2.2 負載均衡與容錯機制

#### 2.2.1 多級負載均衡

```
負載均衡架構:
Internet → CDN/WAF → L7 LB → L4 LB → GPU Nodes
           Cloudflare  ALB      NLB     Pod 1-N
```

**負載均衡演算法對比**:

| 演算法 | 適用場景 | 優勢 | 劣勢 |
|--------|----------|------|------|
| Round Robin | 同質化節點 | 簡單、公平 | 忽略節點負載 |
| Weighted Round Robin | 異質化節點 | 考慮節點能力 | 靜態權重 |
| Least Connections | 長連接 | 動態負載感知 | 計算開銷 |
| **Consistent Hashing** | 有狀態服務 | Cache 親和性 | 複雜實現 |

#### 2.2.2 健康檢查與故障轉移

**多層健康檢查**:
```python
# 1. 基礎健康檢查
@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": time.time()}

# 2. 深度健康檢查
@app.get("/health/deep")
async def deep_health_check():
    try:
        # GPU 狀態檢查
        if not torch.cuda.is_available():
            raise HealthCheckError("GPU unavailable")

        # 模型推理測試
        test_output = model.generate("test")
        if not test_output:
            raise HealthCheckError("Model inference failed")

        return {"status": "healthy", "gpu_memory": get_gpu_memory()}

    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}, 503
```

#### 2.2.3 斷路器模式 (Circuit Breaker)

**實現範例**:
```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
        self.last_failure_time = None

    async def call(self, func, *args, **kwargs):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitBreakerOpenError()

        try:
            result = await func(*args, **kwargs)
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failure_count = 0
            return result

        except Exception as e:
            self.failure_count += 1
            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
                self.last_failure_time = time.time()
            raise e
```

### 2.3 API 版本管理與相容性

#### 2.3.1 語義化版本控制

**版本策略**:
```
API Version: v{major}.{minor}.{patch}
- Major: 不相容的變更 (破壞性更新)
- Minor: 向後相容的功能新增
- Patch: 向後相容的問題修復

範例:
v1.0.0 → v1.1.0: 新增 stream 參數
v1.1.0 → v2.0.0: 改變回應格式
```

#### 2.3.2 多版本並行支援

**API Gateway 路由配置**:
```yaml
# Kong/Nginx 配置
routes:
  - path: /v1/*
    upstream: llm-service-v1
    plugins:
      - rate-limiting: 100/min

  - path: /v2/*
    upstream: llm-service-v2
    plugins:
      - rate-limiting: 200/min
      - jwt-auth: enabled
```

---

## 3. 推理性能優化技術

### 3.1 記憶體優化策略

#### 3.1.1 KV Cache 高效管理

**記憶體占用分析**:
```
Llama-2-7B KV Cache 計算:
- 層數: 32
- 注意力頭: 32
- 頭維度: 128
- 序列長度: 2048

單個 token KV Cache:
32 layers × 32 heads × 128 dim × 2 (K,V) × 2 bytes (FP16) = 524KB

完整序列 KV Cache:
524KB × 2048 tokens = 1.07GB (單個請求)
```

**動態記憶體管理**:
```python
class KVCacheManager:
    def __init__(self, max_blocks=1000, block_size=64):
        self.max_blocks = max_blocks
        self.block_size = block_size
        self.free_blocks = list(range(max_blocks))
        self.allocated_blocks = {}

    def allocate_sequence(self, seq_id, seq_len):
        """為序列分配 KV Cache 塊"""
        required_blocks = math.ceil(seq_len / self.block_size)

        if len(self.free_blocks) < required_blocks:
            # 觸發 LRU 清理
            self.evict_least_recently_used()

        blocks = []
        for _ in range(required_blocks):
            if self.free_blocks:
                blocks.append(self.free_blocks.pop(0))

        self.allocated_blocks[seq_id] = blocks
        return blocks

    def free_sequence(self, seq_id):
        """釋放序列的 KV Cache"""
        if seq_id in self.allocated_blocks:
            blocks = self.allocated_blocks.pop(seq_id)
            self.free_blocks.extend(blocks)
```

#### 3.1.2 CPU Offload 技術

**分層卸載策略**:
```python
class LayeredOffload:
    def __init__(self, model, offload_layers=None):
        self.model = model
        self.offload_layers = offload_layers or []
        self.cpu_layers = {}

    def setup_offload(self):
        """設置層級卸載"""
        for layer_idx in self.offload_layers:
            layer = self.model.layers[layer_idx]
            # 移動到 CPU
            layer.cpu()
            self.cpu_layers[layer_idx] = layer

    def forward_with_offload(self, hidden_states, layer_idx):
        """帶卸載的前向傳播"""
        if layer_idx in self.cpu_layers:
            # CPU 層計算
            hidden_states = hidden_states.cpu()
            output = self.cpu_layers[layer_idx](hidden_states)
            return output.cuda()
        else:
            # GPU 層計算
            return self.model.layers[layer_idx](hidden_states)
```

#### 3.1.3 量化推理優化

**動態量化實現**:
```python
class DynamicQuantization:
    def __init__(self, model, quant_config):
        self.model = model
        self.quant_config = quant_config

    def quantize_activations(self, tensor):
        """動態量化激活值"""
        if self.quant_config.activation_dtype == "int8":
            # 計算量化參數
            scale = tensor.abs().max() / 127.0
            zero_point = 0

            # 量化
            quantized = torch.round(tensor / scale).clamp(-128, 127)
            return quantized.to(torch.int8), scale, zero_point

    def dequantize_activations(self, quantized_tensor, scale, zero_point):
        """反量化"""
        return quantized_tensor.float() * scale + zero_point
```

### 3.2 吞吐量優化技術

#### 3.2.1 Continuous Batching 深度優化

**請求調度算法**:
```python
class AdvancedScheduler:
    def __init__(self, max_batch_size=32, max_tokens=8192):
        self.max_batch_size = max_batch_size
        self.max_tokens = max_tokens
        self.waiting_queue = PriorityQueue()
        self.running_requests = {}

    def schedule_requests(self):
        """智能調度算法"""
        # 1. 計算當前資源使用
        current_batch_size = len(self.running_requests)
        current_tokens = sum(req.current_length for req in self.running_requests.values())

        # 2. 預估可用容量
        available_batch = self.max_batch_size - current_batch_size
        available_tokens = self.max_tokens - current_tokens

        # 3. 選擇最佳請求組合
        selected_requests = []
        estimated_tokens = 0

        while (len(selected_requests) < available_batch and
               estimated_tokens < available_tokens and
               not self.waiting_queue.empty()):

            priority, request = self.waiting_queue.get()

            # 預估該請求的 token 消耗
            estimated_length = self.estimate_generation_length(request)

            if estimated_tokens + estimated_length <= available_tokens:
                selected_requests.append(request)
                estimated_tokens += estimated_length
            else:
                # 放回隊列
                self.waiting_queue.put((priority, request))
                break

        return selected_requests

    def estimate_generation_length(self, request):
        """預估生成長度"""
        # 基於歷史數據的啟發式估算
        prompt_length = len(request.prompt_tokens)
        max_new_tokens = request.max_tokens

        # 經驗公式：生成長度通常為 prompt 的 0.3-0.8 倍
        estimated = min(prompt_length * 0.6, max_new_tokens)
        return int(estimated)
```

#### 3.2.2 Speculative Decoding 實現

**投機解碼核心邏輯**:
```python
class SpeculativeDecoding:
    def __init__(self, draft_model, target_model, gamma=5):
        self.draft_model = draft_model      # 小模型
        self.target_model = target_model    # 大模型
        self.gamma = gamma                  # 投機步數

    async def speculative_generate(self, prompt_tokens):
        """投機解碼生成"""
        current_tokens = prompt_tokens.copy()

        while len(current_tokens) < self.max_length:
            # 1. Draft model 投機生成
            draft_tokens = await self.draft_phase(current_tokens)

            # 2. Target model 驗證
            accepted_tokens = await self.verification_phase(
                current_tokens, draft_tokens
            )

            # 3. 更新序列
            current_tokens.extend(accepted_tokens)

            # 4. 早停條件
            if self.should_stop(current_tokens):
                break

        return current_tokens

    async def draft_phase(self, context_tokens):
        """投機生成階段"""
        draft_tokens = []
        current_context = context_tokens

        for _ in range(self.gamma):
            # 使用小模型快速生成
            logits = self.draft_model(current_context)
            next_token = self.sample_token(logits)

            draft_tokens.append(next_token)
            current_context = current_context + [next_token]

        return draft_tokens

    async def verification_phase(self, context_tokens, draft_tokens):
        """驗證階段"""
        # 大模型並行計算所有位置的 logits
        full_context = context_tokens + draft_tokens
        target_logits = self.target_model(full_context)

        accepted_tokens = []

        for i, draft_token in enumerate(draft_tokens):
            pos = len(context_tokens) + i
            target_probs = F.softmax(target_logits[pos], dim=-1)
            draft_probs = F.softmax(
                self.draft_model(full_context[:pos+1])[-1], dim=-1
            )

            # 接受概率計算
            accept_prob = min(1.0, target_probs[draft_token] / draft_probs[draft_token])

            if random.random() < accept_prob:
                accepted_tokens.append(draft_token)
            else:
                # 拒絕後重新採樣
                adjusted_probs = torch.clamp(target_probs - draft_probs, min=0)
                adjusted_probs = F.normalize(adjusted_probs, p=1, dim=0)
                new_token = torch.multinomial(adjusted_probs, 1).item()
                accepted_tokens.append(new_token)
                break

        return accepted_tokens
```

#### 3.2.3 Parallel Sampling 並行採樣

**多候選並行生成**:
```python
class ParallelSampling:
    def __init__(self, model, num_candidates=4):
        self.model = model
        self.num_candidates = num_candidates

    async def parallel_generate(self, prompt, sampling_params):
        """並行生成多個候選"""
        # 1. 並行生成多個候選序列
        candidates = await asyncio.gather(*[
            self.generate_candidate(prompt, sampling_params)
            for _ in range(self.num_candidates)
        ])

        # 2. 評估和選擇最佳候選
        best_candidate = self.select_best_candidate(candidates, prompt)

        return best_candidate

    async def generate_candidate(self, prompt, sampling_params):
        """生成單個候選序列"""
        tokens = prompt.copy()

        for _ in range(sampling_params.max_new_tokens):
            logits = self.model(tokens)

            # 隨機採樣
            probs = F.softmax(logits[-1] / sampling_params.temperature, dim=-1)
            next_token = torch.multinomial(probs, 1).item()

            tokens.append(next_token)

            if next_token == self.eos_token:
                break

        return tokens

    def select_best_candidate(self, candidates, prompt):
        """選擇最佳候選"""
        scores = []

        for candidate in candidates:
            # 計算對數概率評分
            score = self.calculate_sequence_score(candidate, prompt)
            scores.append(score)

        best_idx = np.argmax(scores)
        return candidates[best_idx]
```

### 3.3 延遲優化技術

#### 3.3.1 Prefill 優化

**KV Cache 預計算**:
```python
class PrefillOptimizer:
    def __init__(self, model):
        self.model = model
        self.cache_store = {}

    def precompute_common_prefixes(self, common_prompts):
        """預計算常見前綴的 KV Cache"""
        for prompt in common_prompts:
            tokens = self.tokenizer.encode(prompt)

            # 計算 KV Cache
            with torch.no_grad():
                kv_cache = self.model.compute_kv_cache(tokens)

            # 存儲到快取
            self.cache_store[prompt] = {
                'tokens': tokens,
                'kv_cache': kv_cache,
                'timestamp': time.time()
            }

    def fast_prefill(self, prompt):
        """快速 prefill"""
        # 查找最長匹配前綴
        best_match = self.find_longest_prefix_match(prompt)

        if best_match:
            # 使用預計算的 KV Cache
            cached_data = self.cache_store[best_match]
            remaining_tokens = self.get_remaining_tokens(prompt, best_match)

            # 增量計算剩餘部分
            incremental_cache = self.model.incremental_compute(
                remaining_tokens, cached_data['kv_cache']
            )

            return incremental_cache
        else:
            # 全新計算
            return self.model.compute_kv_cache(prompt)
```

#### 3.3.2 First Token Latency 優化

**推理流水線優化**:
```python
class PipelinedInference:
    def __init__(self, model, pipeline_stages=4):
        self.model = model
        self.pipeline_stages = pipeline_stages
        self.stage_queues = [asyncio.Queue() for _ in range(pipeline_stages)]

    async def pipelined_forward(self, input_tokens):
        """流水線推理"""
        # 啟動所有流水線階段
        tasks = []
        for stage_id in range(self.pipeline_stages):
            task = asyncio.create_task(
                self.process_stage(stage_id)
            )
            tasks.append(task)

        # 將輸入放入第一階段
        await self.stage_queues[0].put(input_tokens)

        # 等待最後階段輸出
        result = await self.stage_queues[-1].get()

        # 清理任務
        for task in tasks:
            task.cancel()

        return result

    async def process_stage(self, stage_id):
        """處理特定流水線階段"""
        while True:
            try:
                # 從上一階段獲取數據
                data = await self.stage_queues[stage_id].get()

                # 處理數據
                if stage_id == 0:
                    # Embedding + Position Encoding
                    processed = self.model.embed_tokens(data)
                elif stage_id < self.pipeline_stages - 1:
                    # Transformer Layers
                    layer_start = stage_id * (self.model.num_layers // self.pipeline_stages)
                    layer_end = (stage_id + 1) * (self.model.num_layers // self.pipeline_stages)

                    for layer_idx in range(layer_start, layer_end):
                        processed = self.model.layers[layer_idx](data)
                else:
                    # Output Layer
                    processed = self.model.lm_head(data)

                # 傳遞到下一階段
                if stage_id < self.pipeline_stages - 1:
                    await self.stage_queues[stage_id + 1].put(processed)
                else:
                    # 最後階段，返回結果
                    return processed

            except asyncio.CancelledError:
                break
```

---

## 4. 特殊場景優化技術

### 4.1 結構化生成優化

#### 4.1.1 JSON/YAML 格式約束

**語法制導生成**:
```python
class ConstrainedGeneration:
    def __init__(self, model, grammar_rules):
        self.model = model
        self.grammar_rules = grammar_rules
        self.parser = ConstraintParser(grammar_rules)

    def generate_json(self, prompt, schema):
        """生成符合 JSON Schema 的文本"""
        tokens = self.tokenizer.encode(prompt)
        generated_tokens = []

        while len(generated_tokens) < self.max_tokens:
            # 獲取模型輸出分佈
            logits = self.model(tokens + generated_tokens)

            # 應用語法約束
            constrained_logits = self.apply_json_constraints(
                logits, generated_tokens, schema
            )

            # 採樣下一個 token
            next_token = self.sample_with_constraints(constrained_logits)
            generated_tokens.append(next_token)

            # 檢查是否完成有效 JSON
            if self.is_valid_json_complete(generated_tokens, schema):
                break

        return self.tokenizer.decode(generated_tokens)

    def apply_json_constraints(self, logits, current_tokens, schema):
        """應用 JSON 語法約束"""
        current_text = self.tokenizer.decode(current_tokens)
        parser_state = self.parser.get_current_state(current_text, schema)

        # 獲取允許的下一個 tokens
        allowed_tokens = self.parser.get_allowed_tokens(parser_state)

        # 創建約束後的 logits
        constrained_logits = logits.clone()
        mask = torch.zeros_like(logits, dtype=torch.bool)
        mask[allowed_tokens] = True

        # 將不允許的 tokens 設為負無窮
        constrained_logits[~mask] = float('-inf')

        return constrained_logits
```

#### 4.1.2 程式碼生成優化

**語法感知生成**:
```python
class CodeGeneration:
    def __init__(self, model, language="python"):
        self.model = model
        self.language = language
        self.syntax_checker = SyntaxChecker(language)

    def generate_code(self, prompt, constraints=None):
        """生成語法正確的程式碼"""
        tokens = self.tokenizer.encode(prompt)
        generated_tokens = []

        for step in range(self.max_tokens):
            logits = self.model(tokens + generated_tokens)

            # 語法制導約束
            if constraints:
                logits = self.apply_syntax_constraints(
                    logits, generated_tokens, constraints
                )

            next_token = self.sample_token(logits)
            generated_tokens.append(next_token)

            # 實時語法檢查
            current_code = self.tokenizer.decode(generated_tokens)
            if not self.syntax_checker.is_valid_partial(current_code):
                # 回退並重新採樣
                generated_tokens = self.backtrack_and_resample(
                    generated_tokens, logits
                )

        return self.tokenizer.decode(generated_tokens)
```

### 4.2 長文本處理優化

#### 4.2.1 滑動窗口注意力

**長序列記憶體管理**:
```python
class SlidingWindowOptimizer:
    def __init__(self, model, window_size=4096, overlap=512):
        self.model = model
        self.window_size = window_size
        self.overlap = overlap

    def process_long_sequence(self, input_tokens):
        """處理超長序列"""
        if len(input_tokens) <= self.window_size:
            return self.model(input_tokens)

        # 分段處理
        segments = []
        start = 0

        while start < len(input_tokens):
            end = min(start + self.window_size, len(input_tokens))
            segment = input_tokens[start:end]

            # 處理當前段
            segment_output = self.model(segment)
            segments.append(segment_output)

            # 滑動窗口
            start += self.window_size - self.overlap

        # 合併段落結果
        return self.merge_segments(segments)

    def merge_segments(self, segments):
        """合併多個段落的輸出"""
        # 使用注意力機制合併
        merged_representation = self.attention_merge(segments)
        return merged_representation
```

#### 4.2.2 漸進式生成

**分段生成策略**:
```python
class ProgressiveGeneration:
    def __init__(self, model, chunk_size=1024):
        self.model = model
        self.chunk_size = chunk_size

    async def generate_long_text(self, prompt, target_length):
        """漸進式生成長文本"""
        current_text = prompt
        progress_callback = self.setup_progress_tracking()

        while len(current_text) < target_length:
            # 生成下一段
            chunk = await self.generate_chunk(current_text)
            current_text += chunk

            # 更新進度
            progress = len(current_text) / target_length
            await progress_callback(progress, chunk)

            # 檢查品質與一致性
            if not self.check_coherence(current_text):
                current_text = self.correct_coherence(current_text)

        return current_text

    async def generate_chunk(self, context):
        """生成單個文本段落"""
        # 控制生成長度
        sampling_params = SamplingParams(
            max_tokens=self.chunk_size,
            temperature=0.7,
            top_p=0.9
        )

        return await self.model.generate(context, sampling_params)
```

### 4.3 多輪對話優化

#### 4.3.1 對話狀態管理

**上下文壓縮技術**:
```python
class DialogueStateManager:
    def __init__(self, model, max_context_length=8192):
        self.model = model
        self.max_context_length = max_context_length
        self.context_compressor = ContextCompressor()

    def manage_dialogue_context(self, dialogue_history):
        """管理對話上下文"""
        total_length = sum(len(turn) for turn in dialogue_history)

        if total_length <= self.max_context_length:
            return dialogue_history

        # 上下文壓縮策略
        compressed_history = self.compress_context(dialogue_history)
        return compressed_history

    def compress_context(self, dialogue_history):
        """壓縮對話上下文"""
        # 1. 保留最近的 N 輪對話
        recent_turns = dialogue_history[-10:]

        # 2. 壓縮早期對話
        early_turns = dialogue_history[:-10]
        if early_turns:
            compressed_early = self.context_compressor.summarize(early_turns)
            return [compressed_early] + recent_turns

        return recent_turns

    def extract_key_information(self, dialogue_turn):
        """提取關鍵資訊"""
        # 使用小型模型提取關鍵資訊
        key_info = self.information_extractor.extract(dialogue_turn)
        return key_info
```

#### 4.3.2 流式對話優化

**實時響應生成**:
```python
class StreamingDialogue:
    def __init__(self, model):
        self.model = model
        self.response_buffer = []

    async def stream_response(self, user_input, websocket):
        """流式生成對話響應"""
        async for token in self.model.stream_generate(user_input):
            # 實時語義檢查
            if self.should_send_token(token):
                await websocket.send_text(token)

            # 緩衝管理
            self.response_buffer.append(token)

            # 句子邊界檢測
            if self.is_sentence_boundary(token):
                sentence = self.extract_complete_sentence()
                await self.process_complete_sentence(sentence)

    def should_send_token(self, token):
        """判斷是否應該發送 token"""
        # 過濾特殊 tokens 和不完整單詞
        if token.startswith('##') or token in ['<unk>', '<pad>']:
            return False

        # 檢查是否形成完整單詞
        current_word = self.get_current_word()
        return self.is_complete_word(current_word + token)
```

### 4.4 監控與可觀測性

#### 4.4.1 性能指標收集

**全面監控系統**:
```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'latency': LatencyTracker(),
            'throughput': ThroughputTracker(),
            'gpu_utilization': GPUMonitor(),
            'memory_usage': MemoryMonitor(),
            'request_count': RequestCounter()
        }

    async def monitor_request(self, request_handler):
        """監控單個請求"""
        start_time = time.time()

        try:
            # 開始監控
            self.start_request_monitoring()

            # 執行請求
            result = await request_handler()

            # 記錄成功指標
            self.record_success_metrics(start_time)

            return result

        except Exception as e:
            # 記錄錯誤指標
            self.record_error_metrics(start_time, e)
            raise

        finally:
            # 清理監控資源
            self.cleanup_monitoring()

    def record_success_metrics(self, start_time):
        """記錄成功請求指標"""
        end_time = time.time()
        latency = end_time - start_time

        # 更新指標
        self.metrics['latency'].record(latency)
        self.metrics['throughput'].increment()
        self.metrics['request_count'].increment('success')

        # GPU 使用率
        gpu_util = self.get_gpu_utilization()
        self.metrics['gpu_utilization'].record(gpu_util)
```

#### 4.4.2 自動告警與調優

**智能告警系統**:
```python
class IntelligentAlerting:
    def __init__(self, thresholds):
        self.thresholds = thresholds
        self.alert_manager = AlertManager()

    def check_performance_anomalies(self, metrics):
        """檢查性能異常"""
        alerts = []

        # 延遲異常檢測
        if metrics['p95_latency'] > self.thresholds['latency_p95']:
            alerts.append(LatencyAlert(
                current=metrics['p95_latency'],
                threshold=self.thresholds['latency_p95'],
                severity='warning'
            ))

        # GPU 利用率異常
        if metrics['gpu_utilization'] < self.thresholds['min_gpu_util']:
            alerts.append(UtilizationAlert(
                current=metrics['gpu_utilization'],
                threshold=self.thresholds['min_gpu_util'],
                severity='info',
                suggestion='考慮增加批次大小'
            ))

        # 記憶體洩漏檢測
        if self.detect_memory_leak(metrics['memory_usage']):
            alerts.append(MemoryLeakAlert(severity='critical'))

        return alerts

    def auto_tune_parameters(self, metrics):
        """自動調優參數"""
        suggestions = []

        # 批次大小調優
        if metrics['gpu_utilization'] < 70:
            new_batch_size = min(
                self.current_batch_size * 1.2,
                self.max_batch_size
            )
            suggestions.append(('batch_size', new_batch_size))

        # 內存利用率調優
        if metrics['memory_utilization'] > 90:
            suggestions.append(('gpu_memory_utilization', 0.85))

        return suggestions
```

---

## 5. 延伸學習與實踐指引

### 5.1 最佳實踐總結

**服務架構設計**:
- 採用微服務架構，模型服務與業務邏輯分離
- 使用 API Gateway 統一管理路由與認證
- 實施多層負載均衡與容錯機制

**性能優化策略**:
- 優先實施 vLLM + PagedAttention 提升基礎性能
- 根據場景選擇 Speculative Decoding 或 Parallel Sampling
- 部署全面監控系統，基於數據驅動優化

**生產部署要點**:
- 建立完整的 CI/CD 流程
- 實施金絲雀部署與藍綠部署
- 準備詳細的災難恢復計劃

### 5.2 進階學習資源

**推薦論文**:
- Efficient Memory Management for Large Language Model Serving with PagedAttention (SOSP 2023)
- Fast and Memory-Efficient Exact Attention with IO-Awareness (NeurIPS 2022)
- Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation (ICLR 2023)

**開源專案**:
- vLLM: https://github.com/vllm-project/vllm
- Triton Inference Server: https://github.com/triton-inference-server/server
- SGLang: https://github.com/sgl-project/sglang

**下一步**: 通過 Lab-2.2, Lab-2.3 進行實作練習

---

**版本**: v2.0
**制定日期**: 2025-10-09
**維護者**: LLM 教學專案團隊
