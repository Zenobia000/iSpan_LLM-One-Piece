{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5-01: 監控系統建置\n",
    "\n",
    "## 實驗目標\n",
    "\n",
    "本節將建立完整的 vLLM 性能監控基礎設施，包括：\n",
    "- Prometheus 時間序列資料庫配置\n",
    "- vLLM 服務的 metrics 端點設置\n",
    "- Grafana 視覺化儀表板創建\n",
    "- 基礎監控指標驗證\n",
    "\n",
    "## 監控架構概覽\n",
    "\n",
    "```\n",
    "┌─────────────────┐    HTTP /metrics    ┌─────────────────┐\n",
    "│   vLLM Server   │◄────────────────────│   Prometheus    │\n",
    "│  (Port 8000)    │                     │   (Port 9090)   │\n",
    "│  /metrics       │                     │                 │\n",
    "└─────────────────┘                     └─────────────────┘\n",
    "                                                  │\n",
    "                                         PromQL Queries\n",
    "                                                  │\n",
    "                                                  ▼\n",
    "                                        ┌─────────────────┐\n",
    "                                        │    Grafana      │\n",
    "                                        │   (Port 3000)   │\n",
    "                                        │   Dashboard     │\n",
    "                                        └─────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境準備與依賴安裝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝監控相關依賴套件\n",
    "!pip install prometheus-client\n",
    "!pip install psutil\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install requests\n",
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Prometheus 客戶端\n",
    "from prometheus_client import start_http_server, Gauge, Counter, Histogram, CollectorRegistry\n",
    "from prometheus_client.core import REGISTRY\n",
    "\n",
    "# 系統監控\n",
    "import psutil\n",
    "try:\n",
    "    import pynvml\n",
    "    NVIDIA_GPU_AVAILABLE = True\n",
    "    pynvml.nvmlInit()\n",
    "except ImportError:\n",
    "    NVIDIA_GPU_AVAILABLE = False\n",
    "    print(\"警告: NVIDIA GPU 監控不可用\")\n",
    "\n",
    "# 數據處理與視覺化\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✅ 所有依賴套件已載入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. vLLM 服務配置與 Metrics 端點設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置 vLLM 服務的監控參數\n",
    "VLLM_CONFIG = {\n",
    "    \"model\": \"microsoft/DialoGPT-medium\",  # 使用較小的模型進行測試\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": 8000,\n",
    "    \"metrics_port\": 8001,  # 專用的 metrics 端點\n",
    "    \"max_model_len\": 1024,\n",
    "    \"gpu_memory_utilization\": 0.7\n",
    "}\n",
    "\n",
    "print(f\"vLLM 配置:\")\n",
    "for key, value in VLLM_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 vLLM 啟動腳本\n",
    "vllm_script = f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# 啟動 vLLM 服務並啟用監控\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model {VLLM_CONFIG['model']} \\\n",
    "    --host {VLLM_CONFIG['host']} \\\n",
    "    --port {VLLM_CONFIG['port']} \\\n",
    "    --max-model-len {VLLM_CONFIG['max_model_len']} \\\n",
    "    --gpu-memory-utilization {VLLM_CONFIG['gpu_memory_utilization']} \\\n",
    "    --enable-metrics \\\n",
    "    --metrics-port {VLLM_CONFIG['metrics_port']}\n",
    "\"\"\"\n",
    "\n",
    "# 儲存啟動腳本\n",
    "script_path = \"start_vllm_with_metrics.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(vllm_script)\n",
    "\n",
    "# 賦予執行權限\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"✅ vLLM 啟動腳本已建立: {script_path}\")\n",
    "print(\"\\n腳本內容:\")\n",
    "print(vllm_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 自定義 Metrics 收集器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLLMMetricsCollector:\n",
    "    \"\"\"vLLM 自定義指標收集器\"\"\"\n",
    "    \n",
    "    def __init__(self, vllm_metrics_url=\"http://127.0.0.1:8001/metrics\"):\n",
    "        self.vllm_metrics_url = vllm_metrics_url\n",
    "        self.registry = CollectorRegistry()\n",
    "        \n",
    "        # 定義自定義指標\n",
    "        self.request_latency = Histogram(\n",
    "            'vllm_request_latency_seconds',\n",
    "            'Request latency in seconds',\n",
    "            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "            registry=self.registry\n",
    "        )\n",
    "        \n",
    "        self.active_requests = Gauge(\n",
    "            'vllm_active_requests',\n",
    "            'Number of active requests',\n",
    "            registry=self.registry\n",
    "        )\n",
    "        \n",
    "        self.tokens_generated = Counter(\n",
    "            'vllm_tokens_generated_total',\n",
    "            'Total number of tokens generated',\n",
    "            registry=self.registry\n",
    "        )\n",
    "        \n",
    "        self.gpu_memory_usage = Gauge(\n",
    "            'vllm_gpu_memory_usage_bytes',\n",
    "            'GPU memory usage in bytes',\n",
    "            ['gpu_id'],\n",
    "            registry=self.registry\n",
    "        )\n",
    "        \n",
    "        self.cpu_usage = Gauge(\n",
    "            'vllm_cpu_usage_percent',\n",
    "            'CPU usage percentage',\n",
    "            registry=self.registry\n",
    "        )\n",
    "    \n",
    "    def collect_system_metrics(self):\n",
    "        \"\"\"收集系統級指標\"\"\"\n",
    "        # CPU 使用率\n",
    "        cpu_percent = psutil.cpu_percent(interval=1)\n",
    "        self.cpu_usage.set(cpu_percent)\n",
    "        \n",
    "        # GPU 指標 (如果可用)\n",
    "        if NVIDIA_GPU_AVAILABLE:\n",
    "            try:\n",
    "                device_count = pynvml.nvmlDeviceGetCount()\n",
    "                for i in range(device_count):\n",
    "                    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                    self.gpu_memory_usage.labels(gpu_id=str(i)).set(mem_info.used)\n",
    "            except Exception as e:\n",
    "                print(f\"GPU 指標收集失敗: {e}\")\n",
    "    \n",
    "    def get_vllm_native_metrics(self):\n",
    "        \"\"\"獲取 vLLM 原生指標\"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.vllm_metrics_url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                return None\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "    \n",
    "    def start_metrics_server(self, port=8002):\n",
    "        \"\"\"啟動 Prometheus metrics 伺服器\"\"\"\n",
    "        start_http_server(port, registry=self.registry)\n",
    "        print(f\"✅ Metrics 伺服器已啟動在 http://127.0.0.1:{port}/metrics\")\n",
    "    \n",
    "    def simulate_request_metrics(self, latency=1.5, tokens=150):\n",
    "        \"\"\"模擬請求指標 (用於測試)\"\"\"\n",
    "        self.request_latency.observe(latency)\n",
    "        self.tokens_generated.inc(tokens)\n",
    "        self.active_requests.inc()\n",
    "        \n",
    "        # 模擬請求完成\n",
    "        threading.Timer(latency, lambda: self.active_requests.dec()).start()\n",
    "\n",
    "# 初始化 metrics 收集器\n",
    "metrics_collector = VLLMMetricsCollector()\n",
    "print(\"✅ Metrics 收集器已初始化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prometheus 配置檔案生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Prometheus 配置檔案\n",
    "prometheus_config = {\n",
    "    \"global\": {\n",
    "        \"scrape_interval\": \"15s\",\n",
    "        \"evaluation_interval\": \"15s\"\n",
    "    },\n",
    "    \"scrape_configs\": [\n",
    "        {\n",
    "            \"job_name\": \"vllm-native\",\n",
    "            \"static_configs\": [\n",
    "                {\n",
    "                    \"targets\": [f\"127.0.0.1:{VLLM_CONFIG['metrics_port']}\"]\n",
    "                }\n",
    "            ],\n",
    "            \"scrape_interval\": \"5s\",\n",
    "            \"metrics_path\": \"/metrics\"\n",
    "        },\n",
    "        {\n",
    "            \"job_name\": \"vllm-custom\",\n",
    "            \"static_configs\": [\n",
    "                {\n",
    "                    \"targets\": [\"127.0.0.1:8002\"]\n",
    "                }\n",
    "            ],\n",
    "            \"scrape_interval\": \"5s\",\n",
    "            \"metrics_path\": \"/metrics\"\n",
    "        },\n",
    "        {\n",
    "            \"job_name\": \"prometheus\",\n",
    "            \"static_configs\": [\n",
    "                {\n",
    "                    \"targets\": [\"127.0.0.1:9090\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 將配置轉換為 YAML 格式\n",
    "import yaml\n",
    "\n",
    "prometheus_yaml = yaml.dump(prometheus_config, default_flow_style=False)\n",
    "\n",
    "# 儲存 Prometheus 配置檔案\n",
    "with open(\"prometheus.yml\", \"w\") as f:\n",
    "    f.write(prometheus_yaml)\n",
    "\n",
    "print(\"✅ Prometheus 配置檔案已生成: prometheus.yml\")\n",
    "print(\"\\n配置內容:\")\n",
    "print(prometheus_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grafana 儀表板配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Grafana 儀表板 JSON 配置\n",
    "grafana_dashboard = {\n",
    "    \"dashboard\": {\n",
    "        \"id\": None,\n",
    "        \"title\": \"vLLM Performance Monitoring\",\n",
    "        \"tags\": [\"vllm\", \"llm\", \"performance\"],\n",
    "        \"timezone\": \"browser\",\n",
    "        \"refresh\": \"5s\",\n",
    "        \"time\": {\n",
    "            \"from\": \"now-30m\",\n",
    "            \"to\": \"now\"\n",
    "        },\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Request Latency\",\n",
    "                \"type\": \"graph\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": \"histogram_quantile(0.95, rate(vllm_request_latency_seconds_bucket[5m]))\",\n",
    "                        \"legendFormat\": \"P95 Latency\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expr\": \"histogram_quantile(0.50, rate(vllm_request_latency_seconds_bucket[5m]))\",\n",
    "                        \"legendFormat\": \"P50 Latency\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0},\n",
    "                \"yAxes\": [\n",
    "                    {\n",
    "                        \"label\": \"Seconds\",\n",
    "                        \"min\": 0\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"Active Requests\",\n",
    "                \"type\": \"singlestat\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": \"vllm_active_requests\",\n",
    "                        \"legendFormat\": \"Active Requests\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 0}\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"Token Generation Rate\",\n",
    "                \"type\": \"graph\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": \"rate(vllm_tokens_generated_total[5m])\",\n",
    "                        \"legendFormat\": \"Tokens/sec\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 8},\n",
    "                \"yAxes\": [\n",
    "                    {\n",
    "                        \"label\": \"Tokens per second\",\n",
    "                        \"min\": 0\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"GPU Memory Usage\",\n",
    "                \"type\": \"graph\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"expr\": \"vllm_gpu_memory_usage_bytes\",\n",
    "                        \"legendFormat\": \"GPU {{gpu_id}} Memory\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 8},\n",
    "                \"yAxes\": [\n",
    "                    {\n",
    "                        \"label\": \"Bytes\",\n",
    "                        \"min\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"overwrite\": True\n",
    "}\n",
    "\n",
    "# 儲存 Grafana 儀表板配置\n",
    "with open(\"vllm_dashboard.json\", \"w\") as f:\n",
    "    json.dump(grafana_dashboard, f, indent=2)\n",
    "\n",
    "print(\"✅ Grafana 儀表板配置已生成: vllm_dashboard.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 監控服務啟動與驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 啟動自定義 metrics 伺服器\n",
    "metrics_collector.start_metrics_server(port=8002)\n",
    "\n",
    "# 開始收集系統指標\n",
    "def collect_metrics_loop():\n",
    "    while True:\n",
    "        metrics_collector.collect_system_metrics()\n",
    "        time.sleep(5)\n",
    "\n",
    "# 在背景執行指標收集\n",
    "metrics_thread = threading.Thread(target=collect_metrics_loop, daemon=True)\n",
    "metrics_thread.start()\n",
    "\n",
    "print(\"✅ 系統指標收集已啟動\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 驗證 metrics 端點\n",
    "def verify_metrics_endpoint(url, name):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✅ {name} metrics 端點正常: {url}\")\n",
    "            # 顯示部分 metrics 內容\n",
    "            lines = response.text.split('\\n')[:10]\n",
    "            print(f\"   前 10 行 metrics:\")\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    print(f\"     {line}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ {name} metrics 端點異常: {url} (狀態碼: {response.status_code})\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ {name} metrics 端點無法連接: {url} ({e})\")\n",
    "        return False\n",
    "\n",
    "# 驗證自定義 metrics\n",
    "custom_metrics_ok = verify_metrics_endpoint(\"http://127.0.0.1:8002/metrics\", \"自定義\")\n",
    "\n",
    "# 嘗試驗證 vLLM 原生 metrics (如果服務在運行)\n",
    "vllm_metrics_ok = verify_metrics_endpoint(f\"http://127.0.0.1:{VLLM_CONFIG['metrics_port']}/metrics\", \"vLLM 原生\")\n",
    "\n",
    "if not vllm_metrics_ok:\n",
    "    print(\"\\n💡 提示: vLLM 服務尚未啟動，請使用以下命令啟動:\")\n",
    "    print(f\"   bash {script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 模擬監控數據生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 模擬生成監控數據\n",
    "def simulate_monitoring_data(duration_minutes=5):\n",
    "    \"\"\"模擬生成監控數據\"\"\"\n",
    "    print(f\"開始模擬 {duration_minutes} 分鐘的監控數據...\")\n",
    "    \n",
    "    end_time = time.time() + duration_minutes * 60\n",
    "    request_count = 0\n",
    "    \n",
    "    while time.time() < end_time:\n",
    "        # 模擬請求延遲 (正態分佈)\n",
    "        latency = max(0.1, np.random.normal(1.5, 0.5))\n",
    "        \n",
    "        # 模擬生成的 token 數量\n",
    "        tokens = random.randint(50, 300)\n",
    "        \n",
    "        # 記錄 metrics\n",
    "        metrics_collector.simulate_request_metrics(latency, tokens)\n",
    "        \n",
    "        request_count += 1\n",
    "        \n",
    "        # 隨機間隔\n",
    "        time.sleep(random.uniform(0.5, 3.0))\n",
    "        \n",
    "        if request_count % 10 == 0:\n",
    "            print(f\"已模擬 {request_count} 個請求...\")\n",
    "    \n",
    "    print(f\"✅ 模擬完成，總共生成 {request_count} 個請求的監控數據\")\n",
    "\n",
    "# 在背景執行模擬數據生成\n",
    "simulation_thread = threading.Thread(\n",
    "    target=simulate_monitoring_data, \n",
    "    args=(2,),  # 模擬 2 分鐘\n",
    "    daemon=True\n",
    ")\n",
    "simulation_thread.start()\n",
    "\n",
    "print(\"📊 監控數據模擬已開始...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 監控指標即時視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_current_metrics():\n",
    "    \"\"\"視覺化當前監控指標\"\"\"\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('vLLM 即時監控指標', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 獲取當前指標值\n",
    "    try:\n",
    "        response = requests.get(\"http://127.0.0.1:8002/metrics\", timeout=5)\n",
    "        metrics_text = response.text\n",
    "        \n",
    "        # 解析 metrics (簡化版)\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        \n",
    "        # CPU 使用率\n",
    "        axes[0, 0].bar(['CPU'], [cpu_usage], color='skyblue')\n",
    "        axes[0, 0].set_title('CPU 使用率 (%)')\n",
    "        axes[0, 0].set_ylim(0, 100)\n",
    "        axes[0, 0].set_ylabel('使用率 (%)')\n",
    "        \n",
    "        # 記憶體使用率\n",
    "        axes[0, 1].bar(['Memory'], [memory_usage], color='lightcoral')\n",
    "        axes[0, 1].set_title('記憶體使用率 (%)')\n",
    "        axes[0, 1].set_ylim(0, 100)\n",
    "        axes[0, 1].set_ylabel('使用率 (%)')\n",
    "        \n",
    "        # GPU 資訊 (如果可用)\n",
    "        if NVIDIA_GPU_AVAILABLE:\n",
    "            try:\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                gpu_mem_percent = (mem_info.used / mem_info.total) * 100\n",
    "                \n",
    "                axes[1, 0].bar(['GPU Compute', 'GPU Memory'], \n",
    "                              [gpu_util.gpu, gpu_mem_percent], \n",
    "                              color=['orange', 'purple'])\n",
    "                axes[1, 0].set_title('GPU 使用率 (%)')\n",
    "                axes[1, 0].set_ylim(0, 100)\n",
    "                axes[1, 0].set_ylabel('使用率 (%)')\n",
    "            except:\n",
    "                axes[1, 0].text(0.5, 0.5, 'GPU 資訊\\n無法取得', \n",
    "                               ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "                axes[1, 0].set_title('GPU 使用率 (N/A)')\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'GPU 不可用', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('GPU 使用率 (N/A)')\n",
    "        \n",
    "        # Metrics 可用性狀態\n",
    "        endpoints = [\n",
    "            ('自定義 Metrics', 'http://127.0.0.1:8002/metrics'),\n",
    "            ('vLLM Metrics', f'http://127.0.0.1:{VLLM_CONFIG[\"metrics_port\"]}/metrics')\n",
    "        ]\n",
    "        \n",
    "        statuses = []\n",
    "        labels = []\n",
    "        colors = []\n",
    "        \n",
    "        for name, url in endpoints:\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=2)\n",
    "                if resp.status_code == 200:\n",
    "                    statuses.append(1)\n",
    "                    colors.append('green')\n",
    "                else:\n",
    "                    statuses.append(0)\n",
    "                    colors.append('red')\n",
    "            except:\n",
    "                statuses.append(0)\n",
    "                colors.append('red')\n",
    "            labels.append(name)\n",
    "        \n",
    "        axes[1, 1].bar(labels, statuses, color=colors)\n",
    "        axes[1, 1].set_title('Metrics 端點狀態')\n",
    "        axes[1, 1].set_ylim(0, 1.2)\n",
    "        axes[1, 1].set_ylabel('狀態 (1=正常, 0=異常)')\n",
    "        axes[1, 1].set_xticklabels(labels, rotation=45)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"視覺化錯誤: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 生成即時監控視覺化\n",
    "visualize_current_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 部署腳本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成完整的部署腳本\n",
    "deployment_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# vLLM 監控系統部署腳本\n",
    "echo \"🚀 開始部署 vLLM 監控系統...\"\n",
    "\n",
    "# 檢查依賴\n",
    "echo \"📋 檢查系統依賴...\"\n",
    "command -v docker >/dev/null 2>&1 || { echo \"❌ Docker 未安裝\"; exit 1; }\n",
    "command -v docker-compose >/dev/null 2>&1 || { echo \"❌ Docker Compose 未安裝\"; exit 1; }\n",
    "\n",
    "# 建立目錄結構\n",
    "mkdir -p monitoring/{prometheus,grafana}\n",
    "cp prometheus.yml monitoring/prometheus/\n",
    "cp vllm_dashboard.json monitoring/grafana/\n",
    "\n",
    "# 生成 Docker Compose 檔案\n",
    "cat > monitoring/docker-compose.yml << 'EOF'\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    container_name: vllm-prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "      - '--storage.tsdb.retention.time=200h'\n",
    "      - '--web.enable-lifecycle'\n",
    "    networks:\n",
    "      - monitoring\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    container_name: vllm-grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana-storage:/var/lib/grafana\n",
    "      - ./grafana:/etc/grafana/provisioning/dashboards\n",
    "    networks:\n",
    "      - monitoring\n",
    "\n",
    "networks:\n",
    "  monitoring:\n",
    "    driver: bridge\n",
    "\n",
    "volumes:\n",
    "  grafana-storage:\n",
    "EOF\n",
    "\n",
    "echo \"✅ 部署檔案已生成\"\n",
    "echo \"📍 執行以下命令啟動監控服務:\"\n",
    "echo \"   cd monitoring\"\n",
    "echo \"   docker-compose up -d\"\n",
    "echo \"\"\n",
    "echo \"🌐 服務端點:\"\n",
    "echo \"   Prometheus: http://localhost:9090\"\n",
    "echo \"   Grafana: http://localhost:3000 (admin/admin)\"\n",
    "echo \"   vLLM API: http://localhost:8000\"\n",
    "echo \"   vLLM Metrics: http://localhost:8001/metrics\"\n",
    "echo \"   Custom Metrics: http://localhost:8002/metrics\"\n",
    "\"\"\"\n",
    "\n",
    "# 儲存部署腳本\n",
    "with open(\"deploy_monitoring.sh\", \"w\") as f:\n",
    "    f.write(deployment_script)\n",
    "\n",
    "os.chmod(\"deploy_monitoring.sh\", 0o755)\n",
    "\n",
    "print(\"✅ 部署腳本已生成: deploy_monitoring.sh\")\n",
    "print(\"\\n執行 `bash deploy_monitoring.sh` 來部署完整的監控系統\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 監控設置驗證總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 監控設置總結報告\n",
    "def generate_setup_summary():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 vLLM 監控系統建置完成總結\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 檢查生成的檔案\n",
    "    files_to_check = [\n",
    "        \"start_vllm_with_metrics.sh\",\n",
    "        \"prometheus.yml\",\n",
    "        \"vllm_dashboard.json\",\n",
    "        \"deploy_monitoring.sh\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n📁 生成的設定檔案:\")\n",
    "    for filename in files_to_check:\n",
    "        if os.path.exists(filename):\n",
    "            size = os.path.getsize(filename)\n",
    "            print(f\"   ✅ {filename} ({size} bytes)\")\n",
    "        else:\n",
    "            print(f\"   ❌ {filename} (遺失)\")\n",
    "    \n",
    "    # 檢查服務狀態\n",
    "    print(\"\\n🌐 服務端點狀態:\")\n",
    "    endpoints = {\n",
    "        \"自定義 Metrics\": \"http://127.0.0.1:8002/metrics\",\n",
    "        \"vLLM Metrics\": f\"http://127.0.0.1:{VLLM_CONFIG['metrics_port']}/metrics\",\n",
    "        \"vLLM API\": f\"http://127.0.0.1:{VLLM_CONFIG['port']}/v1/models\"\n",
    "    }\n",
    "    \n",
    "    for name, url in endpoints.items():\n",
    "        try:\n",
    "            response = requests.get(url, timeout=3)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"   ✅ {name}: {url}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  {name}: {url} (狀態: {response.status_code})\")\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"   ❌ {name}: {url} (無法連接)\")\n",
    "    \n",
    "    print(\"\\n🚀 後續步驟:\")\n",
    "    print(\"   1. 啟動 vLLM 服務: bash start_vllm_with_metrics.sh\")\n",
    "    print(\"   2. 部署監控服務: bash deploy_monitoring.sh\")\n",
    "    print(\"   3. 訪問 Grafana: http://localhost:3000 (admin/admin)\")\n",
    "    print(\"   4. 導入儀表板: vllm_dashboard.json\")\n",
    "    print(\"   5. 繼續進行 02-Real_Time_Metrics.ipynb\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "generate_setup_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實驗總結\n",
    "\n",
    "本實驗成功建立了完整的 vLLM 監控基礎設施，包括：\n",
    "\n",
    "### ✅ 完成項目\n",
    "1. **監控架構設計**: 建立了 Prometheus + Grafana 的監控技術棧\n",
    "2. **Metrics 收集器**: 開發了自定義的 vLLM 指標收集器\n",
    "3. **配置檔案生成**: 自動生成了 Prometheus 和 Grafana 的配置\n",
    "4. **部署腳本**: 建立了一鍵部署監控系統的腳本\n",
    "5. **即時視覺化**: 實現了基礎的監控指標視覺化\n",
    "\n",
    "### 🎯 核心成果\n",
    "- **自定義 Metrics 伺服器**: 在 port 8002 提供系統級監控指標\n",
    "- **vLLM 整合**: 準備好與 vLLM 原生 metrics 的整合\n",
    "- **Grafana 儀表板**: 預配置的效能監控儀表板\n",
    "- **Docker 化部署**: 容器化的監控服務部署方案\n",
    "\n",
    "### 📋 下一步\n",
    "繼續進行 **02-Real_Time_Metrics.ipynb**，學習實時監控指標的收集與分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}