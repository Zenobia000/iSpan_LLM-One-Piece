{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Adapter Layers - Fine-Tuning BERT for Classification\n",
    "---\n",
    "\n",
    "## Notebook 2: The Training Process\n",
    "\n",
    "**Goal:** In this notebook, you will fine-tune a `bert-base-uncased` model on a sequence classification task using **Traditional Adapter Layers**.\n",
    "\n",
    "**Key Concept:** We will insert small, trainable \"Adapter\" modules into a pre-trained BERT model. By only training the adapters and keeping the base model frozen, we can achieve efficient fine-tuning with far fewer trainable parameters.\n",
    "\n",
    "**You will learn to:**\n",
    "-   Load a dataset for sequence classification and preprocess it with a tokenizer.\n",
    "-   Load a pre-trained BERT model for `SequenceClassification`.\n",
    "-   Implement traditional Adapter layers.\n",
    "-   Insert adapters into the BERT model and freeze the base model's weights.\n",
    "-   Fine-tune the adapter layers using the Hugging Face `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Load Dataset ---\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# --- Preprocessing Function ---\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# The Trainer expects columns named 'labels', but the dataset has 'label'. Let's rename it.\n",
    "encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# We only need a few columns for training.\n",
    "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded and tokenized.\n",
      "Train samples: 3668\n",
      "Test samples: 1725\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Dataset loaded and tokenized.\")\n",
    "print(f\"Train samples: {len(encoded_dataset['train'])}\")\n",
    "print(f\"Test samples: {len(encoded_dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Base Model\n",
    "\n",
    "Next, we load the `bert-base-uncased` model. This will be the base model that we will enhance with adapter layers.\n",
    "\n",
    "#### Key Hugging Face Components:\n",
    "- `transformers.AutoModelForSequenceClassification`: Loads a pre-trained model with a classification head.\n",
    "    - `num_labels`: Number of classes (2 for paraphrase detection: 0=not paraphrase, 1=paraphrase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BERT model loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# --- Device Setup ---\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Model ---\n",
    "\n",
    "num_labels = 2\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=num_labels,\n",
    ").to(device)\n",
    "\n",
    "print(\"‚úÖ BERT model loaded.\")\n",
    "# print(model) # Uncomment to see the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure Traditional Adapters\n",
    "\n",
    "Now we implement traditional Adapter layers and insert them into our base model.\n",
    "\n",
    "#### Traditional Adapter Components:\n",
    "\n",
    "-   **Down-projection Layer**: Compresses features to a smaller bottleneck dimension.\n",
    "-   **Non-linear Activation**: ReLU activation for expressiveness.\n",
    "-   **Up-projection Layer**: Projects features back to their original dimension.\n",
    "-   **Skip Connection**: A critical residual connection that adds the adapter's output to the original input: `output = input + adapter(input)`.\n",
    "\n",
    "By freezing the base model and only training these small adapter layers, we can adapt the model to a new task efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 894,528 || all params: 110,378,306 || trainable%: 0.8104\n",
      "üîß Device Information:\n",
      "Base model device: cuda:0\n",
      "Adapter[0] device: cuda:0\n",
      "Model is on CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AdapterLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Traditional Adapter Layer implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, reduction_factor=16, non_linearity=\"relu\"):\n",
    "        super(AdapterLayer, self).__init__()\n",
    "        \n",
    "        # Calculate bottleneck dimension\n",
    "        bottleneck_dim = input_dim // reduction_factor\n",
    "        \n",
    "        # Adapter components\n",
    "        self.down_project = nn.Linear(input_dim, bottleneck_dim)\n",
    "        self.up_project = nn.Linear(bottleneck_dim, input_dim)\n",
    "        \n",
    "        # Activation function\n",
    "        if non_linearity == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif non_linearity == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {non_linearity}\")\n",
    "        \n",
    "        # Initialize weights for stable training\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights close to identity mapping\"\"\"\n",
    "        nn.init.normal_(self.down_project.weight, std=1e-3)\n",
    "        nn.init.zeros_(self.down_project.bias)\n",
    "        nn.init.normal_(self.up_project.weight, std=1e-3)\n",
    "        nn.init.zeros_(self.up_project.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with skip connection.\"\"\"\n",
    "        adapter_output = self.down_project(x)\n",
    "        adapter_output = self.activation(adapter_output)\n",
    "        adapter_output = self.up_project(adapter_output)\n",
    "        return x + adapter_output\n",
    "\n",
    "class BertWithAdapters(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for a BERT model to insert adapter layers into each transformer layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, reduction_factor=16):\n",
    "        super(BertWithAdapters, self).__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.config = base_model.config\n",
    "        \n",
    "        # Freeze all base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get the device of the base model\n",
    "        base_device = next(self.base_model.parameters()).device\n",
    "        \n",
    "        # Create adapters for each layer and move to correct device\n",
    "        self.adapters = nn.ModuleList()\n",
    "        hidden_size = self.config.hidden_size\n",
    "        \n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            adapter = AdapterLayer(hidden_size, reduction_factor, \"relu\")\n",
    "            adapter.to(base_device)  # Ensure adapter is on same device as base model\n",
    "            self.adapters.append(adapter)\n",
    "            \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        \"\"\"Use the base model directly with manual adapter injection\"\"\"\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeddings = self.base_model.bert.embeddings(\n",
    "            input_ids=input_ids, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Process through encoder layers with adapter injection\n",
    "        hidden_states = embeddings\n",
    "        \n",
    "        # Create proper attention mask format\n",
    "        if attention_mask is not None:\n",
    "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            # Match the dtype of hidden states\n",
    "            extended_attention_mask = extended_attention_mask.to(dtype=hidden_states.dtype)\n",
    "            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "        \n",
    "        # Process each layer\n",
    "        for i, layer in enumerate(self.base_model.bert.encoder.layer):\n",
    "            layer_output = layer(hidden_states, extended_attention_mask)\n",
    "            hidden_states = layer_output[0]\n",
    "            \n",
    "            # Apply adapter after each layer (device/precision handled automatically)\n",
    "            hidden_states = self.adapters[i](hidden_states)\n",
    "        \n",
    "        # Pooling and classification\n",
    "        pooled_output = self.base_model.bert.pooler(hidden_states)\n",
    "        logits = self.base_model.classifier(pooled_output)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        \n",
    "        return {\"logits\": logits}\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        \"\"\"Print number of trainable parameters\"\"\"\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        \n",
    "        for _, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "                \n",
    "        print(f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.4f}\")\n",
    "\n",
    "# Create adapter-enhanced model\n",
    "adapter_model = BertWithAdapters(\n",
    "    base_model=model,\n",
    "    reduction_factor=16  # Bottleneck reduction factor\n",
    ")\n",
    "\n",
    "# Print parameter information\n",
    "adapter_model.print_trainable_parameters()\n",
    "\n",
    "# Print device information\n",
    "print(f\"üîß Device Information:\")\n",
    "print(f\"Base model device: {next(adapter_model.base_model.parameters()).device}\")\n",
    "print(f\"Adapter[0] device: {adapter_model.adapters[0].down_project.weight.device}\")\n",
    "print(f\"Model is on CUDA: {next(adapter_model.parameters()).is_cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set Up Training\n",
    "\n",
    "Now we'll train our adapter-enhanced model. We will use the standard Hugging Face `Trainer`.\n",
    "\n",
    "#### Key Training Concepts:\n",
    "\n",
    "-   **Parameter-Efficient Tuning**: We only train the adapter weights, which is a small fraction of the total model parameters. This saves time and computational resources.\n",
    "-   **Frozen Base Model**: The original BERT model weights are not updated during training. It acts as a powerful feature extractor.\n",
    "-   **Standard Training Loop**: The process is compatible with the standard `Trainer` from the `transformers` library, making it easy to set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# --- Metrics Calculation Function ---\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    # Calculate metrics using sklearn\n    accuracy = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"weighted\")\n    \n    return {\n        \"accuracy\": accuracy,\n        \"f1\": f1,\n    }\n\n# --- Create Trainer ---\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-adapters-mrpc\",\n    learning_rate=1e-3,  # Higher learning rate for adapters\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    eval_strategy=\"steps\",        # ÊîπÁÇ∫ steps ‰ª•‰æøÊõ¥È†ªÁπÅÈ°ØÁ§∫ÊåáÊ®ô\n    eval_steps=50,               # ÊØè50Ê≠•Ë©ï‰º∞‰∏ÄÊ¨°\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    logging_steps=25,            # Êõ¥È†ªÁπÅÁöÑÊó•Ë™åË®òÈåÑ\n    logging_first_step=True,     # Ë®òÈåÑÁ¨¨‰∏ÄÊ≠•\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    report_to=None,              # ÈÅøÂÖçÂ§ñÈÉ®Â†±ÂëäÂπ≤Êìæ\n)\n\n# --- Create Trainer ---\n\ntrainer = Trainer(\n    model=adapter_model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# --- Start Training ---\n\nprint(\"üöÄ Starting training with Traditional Adapters...\")\nprint(\"üìä Training configuration:\")\nprint(f\"   ‚Ä¢ Base model: BERT (frozen)\")\nprint(f\"   ‚Ä¢ Adapter layers: Trainable\")\nprint(f\"   ‚Ä¢ Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   ‚Ä¢ Learning rate: {training_args.learning_rate}\")\nprint(f\"   ‚Ä¢ Epochs: {training_args.num_train_epochs}\")\n\ntrainer.train()\n\n# --- Final Evaluation ---\n\nprint(\"üìä Final Evaluation Results:\")\nfinal_metrics = trainer.evaluate()\nprint(f\"Final Accuracy: {final_metrics['eval_accuracy']:.4f}\")\nprint(f\"Final F1 Score: {final_metrics['eval_f1']:.4f}\")\nprint(f\"Final Loss: {final_metrics['eval_loss']:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory has been released.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Â¶ÇÊûúÊÇ®Êúâ‰∏çÂÜçÈúÄË¶ÅÁöÑÊ®°ÂûãÊàñË®ìÁ∑¥Âô®Á≠âËÆäÊï∏ÔºåÂèØ‰ª•ÂèñÊ∂à‰ª•‰∏ãË®ªËß£‰æÜÂà™Èô§ÂÆÉÂÄë\n",
    "# del model\n",
    "# del trainer\n",
    "\n",
    "# Âü∑Ë°åÂûÉÂúæÂõûÊî∂\n",
    "gc.collect()\n",
    "\n",
    "# Ê∏ÖÁ©∫ PyTorch ÁöÑ CUDA Âø´Âèñ\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"GPU memory has been released.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Course (Poetry)",
   "language": "python",
   "name": "llm-course-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}