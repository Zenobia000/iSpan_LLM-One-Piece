{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.7: DPO å°é½Šæ•ˆæœè©•ä¼°èˆ‡å°æ¯”\n",
    "\n",
    "**å¯¦é©—ç›®æ¨™**: è©•ä¼° DPO å°é½Šæ¨¡å‹çš„æ•ˆæœä¸¦èˆ‡åŸºç·šé€²è¡Œå°æ¯”\n",
    "\n",
    "é€™å€‹ notebook å°‡å…¨é¢è©•ä¼° DPO è¨“ç·´å¾Œçš„æ¨¡å‹æ•ˆæœï¼ŒåŒ…æ‹¬ï¼š\n",
    "- è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™\n",
    "- äººé¡åå¥½æ¨¡æ“¬\n",
    "- å®‰å…¨æ€§è©•ä¼°\n",
    "- èˆ‡ SFT åŸºç·šçš„å°æ¯”\n",
    "\n",
    "## è©•ä¼°ç¶­åº¦\n",
    "\n",
    "1. **å°é½Šæ•ˆæœ**: æ¨¡å‹æ˜¯å¦æ›´ç¬¦åˆäººé¡åå¥½\n",
    "2. **ç”Ÿæˆè³ªé‡**: å›æ‡‰çš„ç›¸é—œæ€§ã€å¹«åŠ©æ€§ã€é€£è²«æ€§\n",
    "3. **å®‰å…¨æ€§**: æœ‰å®³å…§å®¹é˜²è­·ã€åè¦‹æª¢æ¸¬\n",
    "4. **æ•ˆç‡**: æ¨ç†é€Ÿåº¦ã€è¨˜æ†¶é«”å ç”¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1: ç’°å¢ƒæº–å‚™èˆ‡æ¨¡å‹è¼‰å…¥\n",
    "\n",
    "è¼‰å…¥è¨“ç·´å¥½çš„ DPO æ¨¡å‹å’Œ SFT åŸºç·šæ¨¡å‹é€²è¡Œå°æ¯”è©•ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "\n",
    "# è¨­ç½® matplotlib ä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­ç½®éš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('ğŸ§ª DPO å°é½Šæ•ˆæœè©•ä¼°é–‹å§‹')\n",
    "print(f'GPU å¯ç”¨: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'ç•¶å‰ GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è·¯å¾‘é…ç½®\n",
    "BASE_MODEL_NAME = 'microsoft/DialoGPT-medium'\n",
    "SFT_MODEL_PATH = './sft_model_output'\n",
    "DPO_MODEL_PATH = './dpo_model_output'\n",
    "\n",
    "print('ğŸ“¦ è¼‰å…¥æ¨¡å‹é€²è¡Œè©•ä¼°...')\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# è¼‰å…¥åŸºç¤æ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "models = {'Base': base_model}\n",
    "\n",
    "# è¼‰å…¥ SFT æ¨¡å‹\n",
    "if Path(SFT_MODEL_PATH).exists():\n",
    "    try:\n",
    "        sft_model = PeftModel.from_pretrained(base_model, SFT_MODEL_PATH)\n",
    "        models['SFT'] = sft_model\n",
    "        print(f'âœ… SFT æ¨¡å‹è¼‰å…¥æˆåŠŸ')\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸  SFT æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}')\n",
    "        models['SFT'] = base_model  # ä½¿ç”¨åŸºç¤æ¨¡å‹æ›¿ä»£\n",
    "else:\n",
    "    print('âš ï¸  æœªæ‰¾åˆ° SFT æ¨¡å‹')\n",
    "    models['SFT'] = base_model\n",
    "\n",
    "# è¼‰å…¥ DPO æ¨¡å‹\n",
    "if Path(DPO_MODEL_PATH).exists():\n",
    "    try:\n",
    "        dpo_model = PeftModel.from_pretrained(base_model, DPO_MODEL_PATH)\n",
    "        models['DPO'] = dpo_model\n",
    "        print(f'âœ… DPO æ¨¡å‹è¼‰å…¥æˆåŠŸ')\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸  DPO æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}')\n",
    "        # å˜—è©¦è¼‰å…¥æ‰‹å‹•ä¿å­˜çš„æ¨¡å‹\n",
    "        try:\n",
    "            checkpoint = torch.load(Path(DPO_MODEL_PATH) / 'dpo_model_manual.pth')\n",
    "            dpo_model = base_model\n",
    "            dpo_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            models['DPO'] = dpo_model\n",
    "            print(f'âœ… DPO æ‰‹å‹•æª¢æŸ¥é»è¼‰å…¥æˆåŠŸ')\n",
    "        except:\n",
    "            print('âš ï¸  DPO æ¨¡å‹å®Œå…¨è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨ SFT æ¨¡å‹æ›¿ä»£')\n",
    "            models['DPO'] = models['SFT']\n",
    "else:\n",
    "    print('âš ï¸  æœªæ‰¾åˆ° DPO æ¨¡å‹')\n",
    "    models['DPO'] = models['SFT']\n",
    "\n",
    "print(f'\\nå¯ç”¨æ¨¡å‹: {list(models.keys())}')\n",
    "for name, model in models.items():\n",
    "    print(f'{name}: {model.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2: è©•ä¼°æ•¸æ“šé›†æº–å‚™\n",
    "\n",
    "æº–å‚™è©•ä¼°ç”¨çš„æ¸¬è©¦é›†ï¼ŒåŒ…æ‹¬å¤šæ¨£åŒ–çš„æç¤ºå’Œé æœŸå›æ‡‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºå¤šæ¨£åŒ–çš„è©•ä¼°æ•¸æ“šé›†\n",
    "evaluation_prompts = [\n",
    "    # æŠ€è¡“è§£é‡‹é¡\n",
    "    {\n",
    "        'prompt': 'è«‹è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?',\n",
    "        'category': 'technical_explanation',\n",
    "        'expected_quality': 'æ‡‰è©²æä¾›æ¸…æ™°ã€æº–ç¢ºçš„æŠ€è¡“è§£é‡‹'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'æ·±åº¦å­¸ç¿’å’Œæ©Ÿå™¨å­¸ç¿’æœ‰ä»€éº¼å€åˆ¥?',\n",
    "        'category': 'technical_comparison',\n",
    "        'expected_quality': 'æ‡‰è©²æ¸…æ¥šå€åˆ†å…©å€‹æ¦‚å¿µçš„é—œä¿‚å’Œå·®ç•°'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'å¦‚ä½•é–‹å§‹å­¸ç¿’ç¨‹å¼è¨­è¨ˆ?',\n",
    "        'category': 'learning_guidance',\n",
    "        'expected_quality': 'æ‡‰è©²æä¾›çµæ§‹åŒ–ã€å¯¦ç”¨çš„å­¸ç¿’å»ºè­°'\n",
    "    },\n",
    "    \n",
    "    # å¯¦ç”¨å»ºè­°é¡\n",
    "    {\n",
    "        'prompt': 'å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡?',\n",
    "        'category': 'productivity_advice',\n",
    "        'expected_quality': 'æ‡‰è©²æä¾›å…·é«”ã€å¯åŸ·è¡Œçš„å»ºè­°'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'å¥åº·é£²é£Ÿçš„åŸºæœ¬åŸå‰‡æ˜¯ä»€éº¼?',\n",
    "        'category': 'health_advice',\n",
    "        'expected_quality': 'æ‡‰è©²æä¾›ç§‘å­¸ã€å¹³è¡¡çš„å¥åº·å»ºè­°'\n",
    "    },\n",
    "    \n",
    "    # å‰µæ„ä»»å‹™é¡\n",
    "    {\n",
    "        'prompt': 'è«‹å¯«ä¸€å€‹é—œæ–¼å‹èª¼çš„çŸ­æ•…äº‹',\n",
    "        'category': 'creative_writing',\n",
    "        'expected_quality': 'æ‡‰è©²æœ‰æƒ…ç¯€ã€äººç‰©å’Œæƒ…æ„Ÿæ·±åº¦'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'ç‚ºä¸€å€‹æ–°çš„å’–å•¡åº—æƒ³ä¸€å€‹å‰µæ„åå­—',\n",
    "        'category': 'creative_naming',\n",
    "        'expected_quality': 'æ‡‰è©²æœ‰å‰µæ„ä¸”é©åˆå•†æ¥­ç”¨é€”'\n",
    "    },\n",
    "    \n",
    "    # å•é¡Œè§£æ±ºé¡\n",
    "    {\n",
    "        'prompt': 'æˆ‘åœ¨å·¥ä½œä¸­é‡åˆ°å›°é›£çš„åŒäº‹ï¼Œè©²å¦‚ä½•è™•ç†?',\n",
    "        'category': 'interpersonal_advice',\n",
    "        'expected_quality': 'æ‡‰è©²æä¾›å¹³è¡¡ã€å»ºè¨­æ€§çš„å»ºè­°'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'å¦‚ä½•åœ¨æœ‰é™é ç®—ä¸‹è¦åŠƒä¸€æ¬¡æ—…è¡Œ?',\n",
    "        'category': 'practical_planning',\n",
    "        'expected_quality': 'æ‡‰è©²æä¾›å…·é«”çš„çœéŒ¢ç­–ç•¥å’Œè¦åŠƒå»ºè­°'\n",
    "    },\n",
    "    \n",
    "    # è¤‡é›œæ¨ç†é¡\n",
    "    {\n",
    "        'prompt': 'åˆ†æäººå·¥æ™ºèƒ½å°æœªä¾†å°±æ¥­å¸‚å ´çš„å½±éŸ¿',\n",
    "        'category': 'complex_analysis',\n",
    "        'expected_quality': 'æ‡‰è©²å¾å¤šè§’åº¦åˆ†æï¼Œå¹³è¡¡æ­£è² é¢å½±éŸ¿'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f'ğŸ“Š æº–å‚™äº† {len(evaluation_prompts)} å€‹è©•ä¼°æç¤º')\n",
    "print('è©•ä¼°é¡åˆ¥:')\n",
    "categories = {}\n",
    "for prompt_data in evaluation_prompts:\n",
    "    category = prompt_data['category']\n",
    "    categories[category] = categories.get(category, 0) + 1\n",
    "\n",
    "for category, count in categories.items():\n",
    "    print(f'  {category}: {count} å€‹')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3: æ¨¡å‹å›æ‡‰ç”Ÿæˆ\n",
    "\n",
    "ç‚ºæ¯å€‹æ¨¡å‹ç”Ÿæˆè©•ä¼°æç¤ºçš„å›æ‡‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=256, temperature=0.7):\n",
    "    \"\"\"ç”Ÿæˆæ¨¡å‹å›æ‡‰\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # æ ¼å¼åŒ–æç¤º\n",
    "    formatted_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "    \n",
    "    # Tokenization\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # ç”Ÿæˆ\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # è§£ç¢¼å›æ‡‰\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(formatted_prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# ç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„å›æ‡‰\n",
    "print('ğŸ”„ ç”Ÿæˆæ¨¡å‹å›æ‡‰...')\n",
    "results = []\n",
    "\n",
    "for prompt_data in tqdm(evaluation_prompts, desc='ç”Ÿæˆå›æ‡‰'):\n",
    "    prompt = prompt_data['prompt']\n",
    "    \n",
    "    result = {\n",
    "        'prompt': prompt,\n",
    "        'category': prompt_data['category'],\n",
    "        'expected_quality': prompt_data['expected_quality']\n",
    "    }\n",
    "    \n",
    "    # ç‚ºæ¯å€‹æ¨¡å‹ç”Ÿæˆå›æ‡‰\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            # è¨˜éŒ„ç”Ÿæˆæ™‚é–“\n",
    "            start_time = time.time()\n",
    "            response = generate_response(model, tokenizer, prompt)\n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            result[f'{model_name}_response'] = response\n",
    "            result[f'{model_name}_time'] = generation_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'âš ï¸  {model_name} ç”Ÿæˆå¤±æ•—: {e}')\n",
    "            result[f'{model_name}_response'] = '[ç”Ÿæˆå¤±æ•—]'\n",
    "            result[f'{model_name}_time'] = 0\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "print(f'âœ… å®Œæˆ {len(results)} å€‹æç¤ºçš„å›æ‡‰ç”Ÿæˆ')\n",
    "\n",
    "# è½‰æ›ç‚º DataFrame ä¾¿æ–¼åˆ†æ\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f'çµæœè¡¨æ ¼å½¢ç‹€: {results_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4: è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™\n",
    "\n",
    "å¯¦ç¾å¤šç¨®è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™ä¾†é‡åŒ–æ¨¡å‹è¡¨ç¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_response_length(response):\n",
    "    \"\"\"è¨ˆç®—å›æ‡‰é•·åº¦ï¼ˆå­—ç¬¦æ•¸å’Œè©æ•¸ï¼‰\"\"\"\n",
    "    if response == '[ç”Ÿæˆå¤±æ•—]':\n",
    "        return 0, 0\n",
    "    \n",
    "    char_count = len(response)\n",
    "    word_count = len(response.split())\n",
    "    return char_count, word_count\n",
    "\n",
    "\n",
    "def compute_diversity_score(response):\n",
    "    \"\"\"è¨ˆç®—å›æ‡‰çš„è©å½™å¤šæ¨£æ€§ï¼ˆunique words / total wordsï¼‰\"\"\"\n",
    "    if response == '[ç”Ÿæˆå¤±æ•—]' or not response.strip():\n",
    "        return 0\n",
    "    \n",
    "    words = response.lower().split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    unique_words = set(words)\n",
    "    diversity = len(unique_words) / len(words)\n",
    "    return diversity\n",
    "\n",
    "\n",
    "def compute_coherence_score(response):\n",
    "    \"\"\"ç°¡å–®çš„é€£è²«æ€§è©•åˆ†ï¼ˆåŸºæ–¼å¥å­çµæ§‹ï¼‰\"\"\"\n",
    "    if response == '[ç”Ÿæˆå¤±æ•—]' or not response.strip():\n",
    "        return 0\n",
    "    \n",
    "    # åŸºæœ¬æª¢æŸ¥\n",
    "    sentences = response.split('ã€‚')\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # ç°¡å–®çš„é€£è²«æ€§æŒ‡æ¨™\n",
    "    score = 0\n",
    "    \n",
    "    # 1. å¥å­æ•¸é‡åˆç† (1-5å¥å¾—åˆ†è¼ƒé«˜)\n",
    "    if 1 <= len(sentences) <= 5:\n",
    "        score += 0.3\n",
    "    \n",
    "    # 2. å¹³å‡å¥å­é•·åº¦åˆç† (10-50å­—)\n",
    "    avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)\n",
    "    if 10 <= avg_sentence_length <= 50:\n",
    "        score += 0.3\n",
    "    \n",
    "    # 3. åŒ…å«é—œéµè©å½™\n",
    "    if any(keyword in response for keyword in ['æ˜¯', 'å¯ä»¥', 'éœ€è¦', 'æ‡‰è©²', 'å› ç‚º']):\n",
    "        score += 0.2\n",
    "    \n",
    "    # 4. æ²’æœ‰æ˜é¡¯çš„é‡è¤‡\n",
    "    words = response.split()\n",
    "    if len(set(words)) / len(words) > 0.7:  # è©å½™é‡è¤‡ç‡ä½\n",
    "        score += 0.2\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def compute_helpfulness_score(prompt, response):\n",
    "    \"\"\"è¨ˆç®—å›æ‡‰çš„å¹«åŠ©æ€§ï¼ˆåŸºæ–¼æç¤ºç›¸é—œæ€§ï¼‰\"\"\"\n",
    "    if response == '[ç”Ÿæˆå¤±æ•—]' or not response.strip():\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    prompt_lower = prompt.lower()\n",
    "    response_lower = response.lower()\n",
    "    \n",
    "    # 1. é•·åº¦é©ä¸­ (50-300å­—ç¬¦)\n",
    "    if 50 <= len(response) <= 300:\n",
    "        score += 0.25\n",
    "    elif len(response) > 20:  # è‡³å°‘æœ‰ä¸€äº›å…§å®¹\n",
    "        score += 0.1\n",
    "    \n",
    "    # 2. åŒ…å«å•é¡Œç›¸é—œè©å½™\n",
    "    prompt_words = set(prompt_lower.split())\n",
    "    response_words = set(response_lower.split())\n",
    "    overlap = len(prompt_words & response_words) / len(prompt_words) if prompt_words else 0\n",
    "    score += min(overlap * 0.3, 0.3)\n",
    "    \n",
    "    # 3. çµæ§‹åŒ–å›æ‡‰ï¼ˆåŒ…å«è§£é‡‹æ€§è©å½™ï¼‰\n",
    "    explanation_words = ['å› ç‚º', 'æ‰€ä»¥', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å¾Œ', 'ä¾‹å¦‚', 'æ¯”å¦‚', 'åŒ…æ‹¬']\n",
    "    if any(word in response_lower for word in explanation_words):\n",
    "        score += 0.25\n",
    "    \n",
    "    # 4. ç©æ¥µèªèª¿\n",
    "    positive_words = ['å¯ä»¥', 'èƒ½å¤ ', 'å»ºè­°', 'æ¨è–¦', 'æœ‰æ•ˆ', 'å¹«åŠ©']\n",
    "    if any(word in response_lower for word in positive_words):\n",
    "        score += 0.2\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "# è¨ˆç®—æ‰€æœ‰è©•ä¼°æŒ‡æ¨™\n",
    "print('ğŸ“Š è¨ˆç®—è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™...')\n",
    "\n",
    "for model_name in models.keys():\n",
    "    response_col = f'{model_name}_response'\n",
    "    time_col = f'{model_name}_time'\n",
    "    \n",
    "    if response_col not in results_df.columns:\n",
    "        continue\n",
    "        \n",
    "    # é•·åº¦æŒ‡æ¨™\n",
    "    length_data = results_df[response_col].apply(compute_response_length)\n",
    "    results_df[f'{model_name}_char_count'] = [x[0] for x in length_data]\n",
    "    results_df[f'{model_name}_word_count'] = [x[1] for x in length_data]\n",
    "    \n",
    "    # å¤šæ¨£æ€§æŒ‡æ¨™\n",
    "    results_df[f'{model_name}_diversity'] = results_df[response_col].apply(compute_diversity_score)\n",
    "    \n",
    "    # é€£è²«æ€§æŒ‡æ¨™\n",
    "    results_df[f'{model_name}_coherence'] = results_df[response_col].apply(compute_coherence_score)\n",
    "    \n",
    "    # å¹«åŠ©æ€§æŒ‡æ¨™\n",
    "    results_df[f'{model_name}_helpfulness'] = results_df.apply(\n",
    "        lambda row: compute_helpfulness_score(row['prompt'], row[response_col]), axis=1\n",
    "    )\n",
    "\n",
    "print('âœ… è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™è¨ˆç®—å®Œæˆ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5: æ¨¡å‹å°æ¯”åˆ†æ\n",
    "\n",
    "åˆ†æä¸åŒæ¨¡å‹åœ¨å„é …æŒ‡æ¨™ä¸Šçš„è¡¨ç¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºæ¨¡å‹æ€§èƒ½å°æ¯”è¡¨\n",
    "def create_model_comparison():\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        if f'{model_name}_response' not in results_df.columns:\n",
    "            continue\n",
    "            \n",
    "        model_stats = {\n",
    "            'Model': model_name,\n",
    "            'Avg_Char_Count': results_df[f'{model_name}_char_count'].mean(),\n",
    "            'Avg_Word_Count': results_df[f'{model_name}_word_count'].mean(),\n",
    "            'Avg_Diversity': results_df[f'{model_name}_diversity'].mean(),\n",
    "            'Avg_Coherence': results_df[f'{model_name}_coherence'].mean(),\n",
    "            'Avg_Helpfulness': results_df[f'{model_name}_helpfulness'].mean(),\n",
    "            'Avg_Generation_Time': results_df[f'{model_name}_time'].mean()\n",
    "        }\n",
    "        comparison_data.append(model_stats)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "comparison_df = create_model_comparison()\n",
    "print('ğŸ“Š æ¨¡å‹æ€§èƒ½å°æ¯”:')\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# è¦–è¦ºåŒ–å°æ¯”\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('æ¨¡å‹æ€§èƒ½å°æ¯”', fontsize=16)\n",
    "\n",
    "metrics = ['Avg_Char_Count', 'Avg_Diversity', 'Avg_Coherence', \n",
    "          'Avg_Helpfulness', 'Avg_Generation_Time']\n",
    "metric_names = ['å¹³å‡å­—ç¬¦æ•¸', 'è©å½™å¤šæ¨£æ€§', 'é€£è²«æ€§è©•åˆ†', 'å¹«åŠ©æ€§è©•åˆ†', 'ç”Ÿæˆæ™‚é–“(ç§’)']\n",
    "\n",
    "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "    row, col = i // 3, i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    comparison_df.plot(x='Model', y=metric, kind='bar', ax=ax, \n",
    "                      color=['skyblue', 'lightgreen', 'salmon'][:len(comparison_df)])\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend().set_visible(False)\n",
    "\n",
    "# éš±è—å¤šé¤˜çš„å­åœ–\n",
    "axes[1, 2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# è¨ˆç®—ç›¸å°æ”¹é€²\n",
    "if len(comparison_df) >= 2:\n",
    "    print('\\nğŸ“ˆ ç›¸å°æ–¼åŸºç·šçš„æ”¹é€²:')\n",
    "    baseline = comparison_df[comparison_df['Model'] == 'Base'].iloc[0] if 'Base' in comparison_df['Model'].values else comparison_df.iloc[0]\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        if row['Model'] == baseline['Model']:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{row['Model']} vs {baseline['Model']}:\")\n",
    "        print(f\"  è©å½™å¤šæ¨£æ€§: {((row['Avg_Diversity'] - baseline['Avg_Diversity']) / baseline['Avg_Diversity'] * 100):+.1f}%\")\n",
    "        print(f\"  é€£è²«æ€§: {((row['Avg_Coherence'] - baseline['Avg_Coherence']) / baseline['Avg_Coherence'] * 100):+.1f}%\")\n",
    "        print(f\"  å¹«åŠ©æ€§: {((row['Avg_Helpfulness'] - baseline['Avg_Helpfulness']) / baseline['Avg_Helpfulness'] * 100):+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 6: é¡åˆ¥åˆ†æ\n",
    "\n",
    "åˆ†æä¸åŒé¡å‹æç¤ºä¸‹çš„æ¨¡å‹è¡¨ç¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‰é¡åˆ¥åˆ†ææ¨¡å‹è¡¨ç¾\n",
    "def analyze_by_category():\n",
    "    category_analysis = []\n",
    "    \n",
    "    for category in results_df['category'].unique():\n",
    "        category_data = results_df[results_df['category'] == category]\n",
    "        \n",
    "        for model_name in models.keys():\n",
    "            if f'{model_name}_helpfulness' not in category_data.columns:\n",
    "                continue\n",
    "                \n",
    "            analysis = {\n",
    "                'Category': category,\n",
    "                'Model': model_name,\n",
    "                'Helpfulness': category_data[f'{model_name}_helpfulness'].mean(),\n",
    "                'Coherence': category_data[f'{model_name}_coherence'].mean(),\n",
    "                'Diversity': category_data[f'{model_name}_diversity'].mean(),\n",
    "                'Count': len(category_data)\n",
    "            }\n",
    "            category_analysis.append(analysis)\n",
    "    \n",
    "    return pd.DataFrame(category_analysis)\n",
    "\n",
    "category_df = analyze_by_category()\n",
    "\n",
    "# å‰µå»ºé¡åˆ¥åˆ†æç†±åœ–\n",
    "if not category_df.empty:\n",
    "    pivot_helpfulness = category_df.pivot(index='Category', columns='Model', values='Helpfulness')\n",
    "    pivot_coherence = category_df.pivot(index='Category', columns='Model', values='Coherence')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # å¹«åŠ©æ€§ç†±åœ–\n",
    "    sns.heatmap(pivot_helpfulness, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax1)\n",
    "    ax1.set_title('å„é¡åˆ¥å¹«åŠ©æ€§è©•åˆ†')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Category')\n",
    "    \n",
    "    # é€£è²«æ€§ç†±åœ–\n",
    "    sns.heatmap(pivot_coherence, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax2)\n",
    "    ax2.set_title('å„é¡åˆ¥é€£è²«æ€§è©•åˆ†')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Category')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('ğŸ“Š é¡åˆ¥åˆ†æçµæœ:')\n",
    "    print(category_df.round(3))\n",
    "else:\n",
    "    print('âš ï¸  é¡åˆ¥åˆ†ææ•¸æ“šä¸è¶³')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 7: è³ªæ€§åˆ†æ - å¯¦éš›å›æ‡‰å°æ¯”\n",
    "\n",
    "å±•ç¤ºå…·é«”çš„å›æ‡‰ç¯„ä¾‹ä¾†é€²è¡Œè³ªæ€§æ¯”è¼ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å±•ç¤ºå…·é«”å›æ‡‰ç¯„ä¾‹\n",
    "def display_response_examples(num_examples=3):\n",
    "    print('ğŸ“ å›æ‡‰ç¯„ä¾‹å°æ¯”\\n')\n",
    "    print('=' * 100)\n",
    "    \n",
    "    for i in range(min(num_examples, len(results_df))):\n",
    "        row = results_df.iloc[i]\n",
    "        \n",
    "        print(f\"\\nç¯„ä¾‹ {i+1}: {row['category']}\")\n",
    "        print(f\"æç¤º: {row['prompt']}\")\n",
    "        print('-' * 100)\n",
    "        \n",
    "        for model_name in models.keys():\n",
    "            response_col = f'{model_name}_response'\n",
    "            helpfulness_col = f'{model_name}_helpfulness'\n",
    "            coherence_col = f'{model_name}_coherence'\n",
    "            \n",
    "            if response_col in row and helpfulness_col in row:\n",
    "                print(f\"\\nã€{model_name} æ¨¡å‹ã€‘\")\n",
    "                print(f\"å›æ‡‰: {row[response_col]}\")\n",
    "                print(f\"è©•åˆ† - å¹«åŠ©æ€§: {row[helpfulness_col]:.3f}, é€£è²«æ€§: {row[coherence_col]:.3f}\")\n",
    "        \n",
    "        print('\\n' + '=' * 100)\n",
    "\n",
    "display_response_examples(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 8: Win Rate åˆ†æ\n",
    "\n",
    "è¨ˆç®—æ¨¡å‹é–“çš„å‹ç‡ï¼Œæ¨¡æ“¬äººé¡åå¥½åˆ¤æ–·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_win_rate(model_a_scores, model_b_scores):\n",
    "    \"\"\"è¨ˆç®—æ¨¡å‹ A å°æ¨¡å‹ B çš„å‹ç‡\"\"\"\n",
    "    wins = sum(1 for a, b in zip(model_a_scores, model_b_scores) if a > b)\n",
    "    ties = sum(1 for a, b in zip(model_a_scores, model_b_scores) if abs(a - b) < 0.01)\n",
    "    total = len(model_a_scores)\n",
    "    \n",
    "    win_rate = wins / total\n",
    "    tie_rate = ties / total\n",
    "    \n",
    "    return win_rate, tie_rate\n",
    "\n",
    "\n",
    "# è¨ˆç®—æ‰€æœ‰æ¨¡å‹å°çš„ Win Rate\n",
    "model_names = list(models.keys())\n",
    "win_rates = {}\n",
    "\n",
    "print('ğŸ† Win Rate åˆ†æ (åŸºæ–¼å¹«åŠ©æ€§è©•åˆ†)\\n')\n",
    "\n",
    "for i, model_a in enumerate(model_names):\n",
    "    for j, model_b in enumerate(model_names):\n",
    "        if i != j:\n",
    "            helpfulness_a_col = f'{model_a}_helpfulness'\n",
    "            helpfulness_b_col = f'{model_b}_helpfulness'\n",
    "            \n",
    "            if helpfulness_a_col in results_df.columns and helpfulness_b_col in results_df.columns:\n",
    "                scores_a = results_df[helpfulness_a_col].values\n",
    "                scores_b = results_df[helpfulness_b_col].values\n",
    "                \n",
    "                win_rate, tie_rate = compute_win_rate(scores_a, scores_b)\n",
    "                win_rates[f'{model_a}_vs_{model_b}'] = win_rate\n",
    "                \n",
    "                print(f\"{model_a} vs {model_b}: {win_rate:.1%} å‹ç‡ (å¹³æ‰‹: {tie_rate:.1%})\")\n",
    "\n",
    "# å‰µå»º Win Rate çŸ©é™£\n",
    "if len(model_names) >= 2:\n",
    "    win_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "    \n",
    "    for i, model_a in enumerate(model_names):\n",
    "        for j, model_b in enumerate(model_names):\n",
    "            if i != j:\n",
    "                key = f'{model_a}_vs_{model_b}'\n",
    "                if key in win_rates:\n",
    "                    win_matrix[i, j] = win_rates[key]\n",
    "            else:\n",
    "                win_matrix[i, j] = 0.5  # è‡ªå·±å°è‡ªå·±æ˜¯ 50%\n",
    "    \n",
    "    # ç¹ªè£½ Win Rate ç†±åœ–\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(win_matrix, \n",
    "                xticklabels=model_names, \n",
    "                yticklabels=model_names,\n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='RdYlBu_r',\n",
    "                center=0.5,\n",
    "                vmin=0, vmax=1)\n",
    "    plt.title('æ¨¡å‹ Win Rate çŸ©é™£\\n(è¡Œå‹éåˆ—çš„æ©Ÿç‡)')\n",
    "    plt.xlabel('å°æ‰‹æ¨¡å‹')\n",
    "    plt.ylabel('è©•ä¼°æ¨¡å‹')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 9: æ•ˆç‡åˆ†æ\n",
    "\n",
    "åˆ†ææ¨¡å‹çš„æ¨ç†æ•ˆç‡å’Œè³‡æºå ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•ˆç‡åˆ†æ\n",
    "def analyze_efficiency():\n",
    "    efficiency_data = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        time_col = f'{model_name}_time'\n",
    "        char_col = f'{model_name}_char_count'\n",
    "        \n",
    "        if time_col in results_df.columns:\n",
    "            avg_time = results_df[time_col].mean()\n",
    "            avg_chars = results_df[char_col].mean()\n",
    "            \n",
    "            # è¨ˆç®—æ¯ç§’å­—ç¬¦æ•¸\n",
    "            chars_per_second = avg_chars / avg_time if avg_time > 0 else 0\n",
    "            \n",
    "            # ä¼°ç®—æ¨¡å‹åƒæ•¸é‡\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            \n",
    "            efficiency_data.append({\n",
    "                'Model': model_name,\n",
    "                'Avg_Generation_Time': avg_time,\n",
    "                'Chars_Per_Second': chars_per_second,\n",
    "                'Total_Parameters': total_params,\n",
    "                'Trainable_Parameters': trainable_params,\n",
    "                'Param_Efficiency': trainable_params / total_params * 100\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(efficiency_data)\n",
    "\n",
    "efficiency_df = analyze_efficiency()\n",
    "\n",
    "print('âš¡ æ•ˆç‡åˆ†æçµæœ:')\n",
    "print(efficiency_df.round(3))\n",
    "\n",
    "# ç¹ªè£½æ•ˆç‡å°æ¯”åœ–\n",
    "if not efficiency_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # ç”Ÿæˆé€Ÿåº¦å°æ¯”\n",
    "    efficiency_df.plot(x='Model', y='Chars_Per_Second', kind='bar', ax=ax1, color='lightcoral')\n",
    "    ax1.set_title('ç”Ÿæˆé€Ÿåº¦ (å­—ç¬¦/ç§’)')\n",
    "    ax1.set_ylabel('å­—ç¬¦/ç§’')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.legend().set_visible(False)\n",
    "    \n",
    "    # åƒæ•¸æ•ˆç‡å°æ¯”\n",
    "    efficiency_df.plot(x='Model', y='Param_Efficiency', kind='bar', ax=ax2, color='lightgreen')\n",
    "    ax2.set_title('åƒæ•¸æ•ˆç‡ (%)')\n",
    "    ax2.set_ylabel('å¯è¨“ç·´åƒæ•¸æ¯”ä¾‹ (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.legend().set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 10: è©•ä¼°å ±å‘Šç¸½çµ\n",
    "\n",
    "ç”Ÿæˆå®Œæ•´çš„è©•ä¼°å ±å‘Šå’Œå»ºè­°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆè©•ä¼°å ±å‘Š\n",
    "def generate_evaluation_report():\n",
    "    report = {\n",
    "        'evaluation_summary': {\n",
    "            'total_prompts': len(results_df),\n",
    "            'categories': list(results_df['category'].unique()),\n",
    "            'models_evaluated': list(models.keys())\n",
    "        },\n",
    "        'performance_metrics': comparison_df.to_dict('records') if not comparison_df.empty else [],\n",
    "        'efficiency_metrics': efficiency_df.to_dict('records') if not efficiency_df.empty else [],\n",
    "        'win_rates': win_rates,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # ç”Ÿæˆå»ºè­°\n",
    "    if not comparison_df.empty:\n",
    "        best_helpfulness = comparison_df.loc[comparison_df['Avg_Helpfulness'].idxmax(), 'Model']\n",
    "        best_coherence = comparison_df.loc[comparison_df['Avg_Coherence'].idxmax(), 'Model']\n",
    "        fastest_model = comparison_df.loc[comparison_df['Avg_Generation_Time'].idxmin(), 'Model']\n",
    "        \n",
    "        report['recommendations'].extend([\n",
    "            f\"æœ€ä½³å¹«åŠ©æ€§: {best_helpfulness}\",\n",
    "            f\"æœ€ä½³é€£è²«æ€§: {best_coherence}\",\n",
    "            f\"æœ€å¿«æ¨ç†: {fastest_model}\"\n",
    "        ])\n",
    "        \n",
    "        # DPO æ•ˆæœåˆ†æ\n",
    "        if 'DPO' in comparison_df['Model'].values and 'SFT' in comparison_df['Model'].values:\n",
    "            dpo_row = comparison_df[comparison_df['Model'] == 'DPO'].iloc[0]\n",
    "            sft_row = comparison_df[comparison_df['Model'] == 'SFT'].iloc[0]\n",
    "            \n",
    "            helpfulness_improvement = (dpo_row['Avg_Helpfulness'] - sft_row['Avg_Helpfulness']) / sft_row['Avg_Helpfulness'] * 100\n",
    "            coherence_improvement = (dpo_row['Avg_Coherence'] - sft_row['Avg_Coherence']) / sft_row['Avg_Coherence'] * 100\n",
    "            \n",
    "            report['recommendations'].extend([\n",
    "                f\"DPO ç›¸å° SFT å¹«åŠ©æ€§æ”¹é€²: {helpfulness_improvement:+.1f}%\",\n",
    "                f\"DPO ç›¸å° SFT é€£è²«æ€§æ”¹é€²: {coherence_improvement:+.1f}%\"\n",
    "            ])\n",
    "    \n",
    "    return report\n",
    "\n",
    "report = generate_evaluation_report()\n",
    "\n",
    "print('ğŸ“‹ DPO å°é½Šæ•ˆæœè©•ä¼°å ±å‘Š\\n')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\nğŸ“Š è©•ä¼°æ¦‚è¦½:')\n",
    "print(f\"  æ¸¬è©¦æç¤ºæ•¸: {report['evaluation_summary']['total_prompts']}\")\n",
    "print(f\"  æ¸¬è©¦é¡åˆ¥: {len(report['evaluation_summary']['categories'])} å€‹\")\n",
    "print(f\"  è©•ä¼°æ¨¡å‹: {', '.join(report['evaluation_summary']['models_evaluated'])}\")\n",
    "\n",
    "print('\\nğŸ¯ é—œéµå»ºè­°:')\n",
    "for recommendation in report['recommendations']:\n",
    "    print(f\"  â€¢ {recommendation}\")\n",
    "\n",
    "# ä¿å­˜è©•ä¼°çµæœ\n",
    "results_df.to_csv('./dpo_evaluation_results.csv', index=False)\n",
    "with open('./dpo_evaluation_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('\\nğŸ’¾ è©•ä¼°çµæœå·²ä¿å­˜:')\n",
    "print('  â€¢ dpo_evaluation_results.csv - è©³ç´°çµæœæ•¸æ“š')\n",
    "print('  â€¢ dpo_evaluation_report.json - è©•ä¼°å ±å‘Šæ‘˜è¦')\n",
    "\n",
    "print('\\nğŸ“ è©•ä¼°ç¸½çµ:')\n",
    "print('  âœ… å®Œæˆå¤šç¶­åº¦è‡ªå‹•åŒ–è©•ä¼°')\n",
    "print('  âœ… æ¨¡å‹æ€§èƒ½å°æ¯”åˆ†æ')\n",
    "print('  âœ… Win Rate ç«¶çˆ­åˆ†æ')\n",
    "print('  âœ… æ•ˆç‡èˆ‡è³‡æºåˆ†æ')\n",
    "print('  âœ… è³ªæ€§èˆ‡é‡åŒ–çµåˆè©•ä¼°')\n",
    "\n",
    "print('\\nğŸ”¬ DPO å°é½ŠæŠ€è¡“æ ¸å¿ƒç™¼ç¾:')\n",
    "print('  â€¢ DPO èƒ½æœ‰æ•ˆæå‡æ¨¡å‹å°äººé¡åå¥½çš„å°é½Šåº¦')\n",
    "print('  â€¢ ç›¸æ¯”å‚³çµ± RLHFï¼ŒDPO è¨“ç·´æ›´åŠ ç©©å®š')\n",
    "print('  â€¢ å–®éšæ®µå°é½Šå¯ä»¥é”åˆ°åª²ç¾å¤šéšæ®µçš„æ•ˆæœ')\n",
    "print('  â€¢ åå¥½æ•¸æ“šçš„è³ªé‡å°å°é½Šæ•ˆæœè‡³é—œé‡è¦')\n",
    "\n",
    "# æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\nğŸ’¾ GPU è¨˜æ†¶é«”å·²æ¸…ç†ï¼Œç•¶å‰ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f} GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}