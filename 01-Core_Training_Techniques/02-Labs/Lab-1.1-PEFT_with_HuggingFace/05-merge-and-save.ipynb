{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. 合併 LoRA 權重並儲存模型\n",
        "\n",
        "在本實驗的最後一個環節，我們將學習 `peft` 函式庫的一個非常實用的功能：將訓練好的 LoRA adapter 權重合併回原始的基礎模型中。\n",
        "\n",
        "這樣做的好處是，我們可以得到一個**單一的、標準的 Hugging Face 模型**，它已經包含了微調後的所有知識。這個合併後的模型可以像任何普通的 `transformers` 模型一樣被載入和使用，不再需要 `peft` 函式庫，這對於簡化部署流程非常方便。\n",
        "\n",
        "## 步驟 1: 載入基礎模型與 LoRA Adapter\n",
        "\n",
        "和上一個筆記本一樣，我們首先需要載入量化後的基礎模型，以及我們訓練好的 LoRA adapter。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "# --- 重新載入基礎模型與 Tokenizer ---\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 載入 LoRA Adapter ---\n",
        "output_dir = \"./lora-llama2-7b-chat\"\n",
        "latest_checkpoint = max([os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")], key=os.path.getmtime)\n",
        "peft_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "print(\"模型與 Adapter 載入完成。\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 2: 合併權重\n",
        "\n",
        "`peft` 提供了一個非常簡單的方法來合併權重：`merge_and_unload()`。\n",
        "\n",
        "這個方法會執行以下操作：\n",
        "1.  將 LoRA adapter (矩陣 A 和 B) 的權重計算出來 (`B * A`)。\n",
        "2.  將計算出的權重加回到原始的 `target_modules` (例如 \"q_proj\", \"v_proj\") 上。\n",
        "3.  從模型中移除 LoRA adapter，並返回一個合併後的標準 `transformers` 模型。\n",
        "\n",
        "**注意**: 由於我們使用了量化模型，`merge_and_unload()` 目前可能無法直接在量化層上運作。在理想情況下（非量化模型），這個步驟會非常直接。對於量化模型，`peft` 函式庫正在持續改進支援。這裡我們展示標準流程。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 合併權重\n",
        "# 在非量化模型上，這會直接返回一個 AutoModelForCausalLM 物件\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "print(\"權重合併完成！\")\n",
        "# 合併後，模型類型變回原本的 LlamaForCausalLM\n",
        "print(type(merged_model))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步驟 3: 儲存完整模型與 Tokenizer\n",
        "\n",
        "合併後的模型現在是一個完整的、獨立的模型。我們可以使用 `save_pretrained()` 方法將它和它的 Tokenizer 一起儲存到一個新的目錄中。\n",
        "\n",
        "這個儲存下來的模型未來可以直接使用 `AutoModelForCausalLM.from_pretrained()` 載入，不再需要 `peft` 函式庫。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定義儲存目錄\n",
        "save_directory = \"./llama2-7b-chat-guanaco-merged\"\n",
        "\n",
        "# 儲存模型\n",
        "merged_model.save_pretrained(save_directory)\n",
        "\n",
        "# 儲存 Tokenizer\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"合併後的模型已儲存至: {save_directory}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 實驗總結\n",
        "\n",
        "恭喜您完成了整個 PEFT 微調實驗！\n",
        "\n",
        "您已經學會了：\n",
        "- **環境設定**: 準備進行 PEFT 的開發環境。\n",
        "- **載入與量化**: 如何載入大型語言模型並使用 4-bit 量化來節省資源。\n",
        "- **應用 LoRA**: 如何使用 `peft` 函式庫設定並應用 LoRA。\n",
        "- **訓練**: 如何使用 `Trainer` 進行微調。\n",
        "- **推理**: 如何使用微調後的模型生成文本。\n",
        "- **合併與儲存**: 如何將 LoRA 權重合併回主模型，以便於部署。\n",
        "\n",
        "希望這個實驗能幫助您掌握參數高效微調的核心技術，並將其應用到您自己的專案中！\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
