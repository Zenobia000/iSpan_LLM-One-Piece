# å¤§æ¨¡å‹æ¨ç†æŠ€è¡“ 2.1 æ¨ç†å¼•æ“ (Inference Engines)

æœ¬æ•™å­¸æ¨¡çµ„æ·±å…¥æ¢è¨å¤§å‹èªè¨€æ¨¡å‹ (LLM) æ¨ç†å¼•æ“çš„æ ¸å¿ƒæŠ€è¡“ï¼Œæ¶µè“‹å¾åŸºç¤åŸç†åˆ°ç”Ÿç”¢æ‡‰ç”¨çš„å®Œæ•´çŸ¥è­˜é«”ç³»ã€‚

**ğŸš¨ é‡è¦èªªæ˜**: æœ¬èª²ç¨‹æä¾›å®Œæ•´ç†è«–æ•™å­¸èˆ‡æ¶æ§‹åˆ†æï¼Œé©åˆå–®GPUç’°å¢ƒå­¸ç¿’èˆ‡å°è¦æ¨¡å¯¦é©—ã€‚

| å±¤æ¬¡ | å­¸ç¿’ç›®æ¨™ | æ ¸å¿ƒå…§å®¹ | ç”¢å‡º |
| :--- | :--- | :--- | :--- |
| **Fundamentals** | ç†è§£LLMæ¨ç†æŒ‘æˆ°èˆ‡å¼•æ“è¨­è¨ˆ | è‡ªå›æ­¸ç”Ÿæˆã€KV Cacheã€è¨˜æ†¶é«”ç“¶é ¸ | æŒæ¡æ¨ç†åŸºæœ¬æ¦‚å¿µ |
| **First Principles** | æŒæ¡PagedAttentionèˆ‡Continuous BatchingåŸç† | è™›æ“¬è¨˜æ†¶é«”ç®¡ç†ã€å‹•æ…‹æ‰¹æ¬¡èª¿åº¦ | ç†è§£æ ¸å¿ƒæ¼”ç®—æ³•è¨­è¨ˆ |
| **Body of Knowledge** | ç²¾é€šä¸»æµæ¨ç†å¼•æ“æ‡‰ç”¨ | vLLMã€TensorRT-LLMã€å¼•æ“é¸å‹ | å…·å‚™ç”Ÿç”¢éƒ¨ç½²èƒ½åŠ› |

---

## 1. Fundamentals (åŸºç¤)

### 1.1 LLM æ¨ç†çš„æ ¸å¿ƒæŒ‘æˆ°

#### 1.1.1 è‡ªå›æ­¸ç”Ÿæˆç‰¹æ€§

LLM æ¡ç”¨è‡ªå›æ­¸ (Autoregressive) æ–¹å¼é€å€‹ç”Ÿæˆ tokenï¼Œæ¯å€‹ token çš„ç”Ÿæˆä¾è³´æ–¼ä¹‹å‰æ‰€æœ‰ tokenã€‚

**ç”Ÿæˆæµç¨‹**:
```
ç”Ÿæˆ "Hello World"
Step 1: Input = ""      â†’ Output = "Hello"
Step 2: Input = "Hello" â†’ Output = " World"
```

**æ¨ç†ç“¶é ¸**:
- **åºåˆ—ä¾è³´**: ç„¡æ³•ä¸¦è¡Œç”Ÿæˆï¼Œå¿…é ˆé€å€‹ç”¢ç”Ÿ
- **KV Cache å¢é•·**: æ¯ç”Ÿæˆä¸€å€‹ tokenï¼ŒKV Cache ç·šæ€§å¢é•·
- **è¨˜æ†¶é«”é »å¯¬å—é™**: æ¨ç†éç¨‹ç‚º Memory-Bound è€Œé Compute-Bound

#### 1.1.2 è¨˜æ†¶é«”é »å¯¬ç“¶é ¸

**æ¡ˆä¾‹åˆ†æ**: Llama-2-7B æ¨ç†

```
æ¨¡å‹åƒæ•¸: 7B Ã— 2 bytes (FP16) = 14GB
å–®æ¬¡å‰å‘å‚³æ’­:
  - è¨ˆç®—é‡: ç´„ 45M FLOPs (çŸ©é™£ä¹˜æ³•)
  - è¨˜æ†¶é«”è¨ªå•: 14GB (è®€å–æ‰€æœ‰åƒæ•¸)

GPU æ€§èƒ½ (A100):
  - è¨ˆç®—èƒ½åŠ›: 312 TFLOPS (FP16)
  - è¨˜æ†¶é«”é »å¯¬: 1.5 TB/s
  
æ™‚é–“åˆ†æ:
  - è¨ˆç®—æ™‚é–“: 45M / 312T â‰ˆ 0.14ms
  - è¨˜æ†¶é«”è¨ªå•æ™‚é–“: 14GB / 1.5TB/s â‰ˆ 9ms
  
çµè«–: è¨˜æ†¶é«”è¨ªå•æ™‚é–“ >> è¨ˆç®—æ™‚é–“ (Memory-Bound)
```

#### 1.1.3 KV Cache è¨˜æ†¶é«”å ç”¨

**KV Cache è¨ˆç®—** (Llama-2-7B ç¯„ä¾‹):

```python
num_layers = 32      # Transformer å±¤æ•¸
num_heads = 32       # æ³¨æ„åŠ›é ­æ•¸
head_dim = 128       # æ¯å€‹é ­çš„ç¶­åº¦
batch_size = 16      # æ‰¹æ¬¡å¤§å°
seq_len = 2048       # ç”Ÿæˆåºåˆ—é•·åº¦
precision = 2        # FP16 = 2 bytes

kv_cache_size = (
    2                 # K å’Œ V
    Ã— batch_size
    Ã— num_layers
    Ã— num_heads
    Ã— seq_len
    Ã— head_dim
    Ã— precision
) / (1024**3)  # è½‰ç‚º GB

# çµæœ: ç´„ 16GB (åƒ… KV Cache!)
```

**é—œéµæ´å¯Ÿ**:
- KV Cache éš¨åºåˆ—é•·åº¦ç·šæ€§å¢é•·
- æ‰¹æ¬¡æ¨ç†æ™‚è¨˜æ†¶é«”å ç”¨å·¨å¤§
- æˆç‚ºæ¨ç†æ“´å±•çš„ä¸»è¦ç“¶é ¸

---

## 2. First Principles (åŸç†)

### 2.1 PagedAttention: è™›æ“¬è¨˜æ†¶é«”ç®¡ç†

#### 2.1.1 å‚³çµ± KV Cache çš„è¨˜æ†¶é«”æµªè²»

**å•é¡Œ**: éœæ…‹è¨˜æ†¶é«”åˆ†é…å°è‡´ç¢ç‰‡åŒ–

```
å‚³çµ±æ–¹æ¡ˆ (é åˆ†é…æœ€å¤§é•·åº¦):
Request 1 (seq_len=1024): åˆ†é… 2048 ç©ºé–“ï¼Œæµªè²» 50%
Request 2 (seq_len=512):  åˆ†é… 2048 ç©ºé–“ï¼Œæµªè²» 75%

ç¸½è¨˜æ†¶é«”åˆ©ç”¨ç‡: ç´„ 30-50%
```

#### 2.1.2 PagedAttention æ ¸å¿ƒè¨­è¨ˆ

**éˆæ„Ÿ**: å€Ÿé‘‘ä½œæ¥­ç³»çµ±è™›æ“¬è¨˜æ†¶é«”ç®¡ç†

**æ ¸å¿ƒæ¦‚å¿µ**:
1. **ç‰©ç†å¡Š (Physical Block)**: å›ºå®šå¤§å°çš„ KV Cache å–®å…ƒ (å¦‚ 64 tokens)
2. **é‚è¼¯é  (Logical Page)**: è«‹æ±‚çš„é€£çºŒä½å€ç©ºé–“
3. **é è¡¨ (Page Table)**: é‚è¼¯é åˆ°ç‰©ç†å¡Šçš„æ˜ å°„

**è¨˜æ†¶é«”ç¯€çœ**:
```
PagedAttention:
Request 1 (1024 tokens): åˆ†é… 16 å€‹ block (1024/64)
Request 2 (512 tokens):  åˆ†é… 8 å€‹ block

è¨˜æ†¶é«”åˆ©ç”¨ç‡: >95%
ç¯€çœ: (50% - 5%) / 50% â‰ˆ 88% è¨˜æ†¶é«”ç¯€çœ
```

**Python å¯¦ç¾æ¦‚å¿µ**:
```python
class BlockManager:
    def __init__(self, num_blocks, block_size):
        self.num_blocks = num_blocks
        self.block_size = block_size  # æ¯å€‹ block çš„ tokens æ•¸
        self.free_blocks = list(range(num_blocks))
        
    def allocate(self, num_required):
        """åˆ†é…æŒ‡å®šæ•¸é‡çš„ blocks"""
        blocks = []
        for _ in range(num_required):
            if not self.free_blocks:
                raise OutOfMemoryError("KV Cache å·²æ»¿")
            blocks.append(self.free_blocks.pop(0))
        return blocks
    
    def free(self, blocks):
        """é‡‹æ”¾ blocks"""
        self.free_blocks.extend(blocks)
```

### 2.2 Continuous Batching: å‹•æ…‹æ‰¹æ¬¡èª¿åº¦

#### 2.2.1 Static Batching çš„å±€é™

**å•é¡Œ**: æ‰¹æ¬¡å…§æ‰€æœ‰è«‹æ±‚å¿…é ˆåŒæ™‚å®Œæˆ

```
Static Batching:
Batch = [Req1(é•·), Req2(çŸ­), Req3(çŸ­), Req4(çŸ­)]

æ™‚é–“è»¸:
Req1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (800ms, 100 tokens)
Req2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            (400ms, 50 tokens) - é–’ç½® 400ms
Req3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            (400ms, 50 tokens) - é–’ç½® 400ms
Req4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            (400ms, 50 tokens) - é–’ç½® 400ms

ç¸½æ™‚é–“: 800ms
GPU åˆ©ç”¨ç‡: 400ms / 800ms = 50%
```

#### 2.2.2 Continuous Batching å„ªåŒ–

**æ ¸å¿ƒæ€æƒ³**: è«‹æ±‚å®Œæˆå¾Œç«‹å³ç§»å‡ºæ‰¹æ¬¡ï¼Œè£œå……æ–°è«‹æ±‚

```
Continuous Batching:
æ™‚é–“: 0     200ms  400ms  600ms  800ms
Req1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Req2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (å®Œæˆï¼Œç§»å‡º)
Req5:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (è£œå……é€²å…¥)
Req3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (å®Œæˆï¼Œç§»å‡º)
Req6:                 â–ˆâ–ˆâ–ˆâ–ˆ (è£œå……é€²å…¥)

ååé‡æå‡: 2-3x
GPU åˆ©ç”¨ç‡: 85-95%
```

---

## 3. Body of Knowledge (å¯¦å‹™)

### 3.1 vLLM: é«˜æ•ˆæ¨ç†å¼•æ“

vLLM (UC Berkeley) é€é PagedAttention èˆ‡ Continuous Batching å¯¦ç¾æ¥­ç•Œé ˜å…ˆçš„æ¨ç†æ€§èƒ½ã€‚

#### 3.1.1 æ ¸å¿ƒæ¶æ§‹

```
vLLM æ¶æ§‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       API Server (OpenAIå…¼å®¹)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Scheduler               â”‚
â”‚  - Continuous Batching          â”‚
â”‚  - Request å„ªå…ˆç´šæ’ç¨‹            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Block Manager              â”‚
â”‚  - PagedAttention è¨˜æ†¶é«”ç®¡ç†     â”‚
â”‚  - ç‰©ç†å¡Šåˆ†é…èˆ‡å›æ”¶              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Model Executor            â”‚
â”‚  - PagedAttention è¨ˆç®—          â”‚
â”‚  - CUDA Kernel å„ªåŒ–             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.1.2 ä½¿ç”¨ç¯„ä¾‹

**åŸºç¤æ¨ç†**:
```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–å¼•æ“
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    tensor_parallel_size=1,      # å–®GPU
    gpu_memory_utilization=0.9,  # ä½¿ç”¨ 90% GPU è¨˜æ†¶é«”
    max_num_seqs=32,             # æœ€å¤§ä¸¦ç™¼è«‹æ±‚æ•¸
)

# é…ç½®ç”Ÿæˆåƒæ•¸
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100
)

# æ‰¹æ¬¡æ¨ç†
prompts = ["Hello, ", "Explain AI: ", "Code example:"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

**æ€§èƒ½å°æ¯”** (Llama-2-7B, A100):

| å¼•æ“ | ååé‡ (tokens/s) | TTFT (ms) | è¨˜æ†¶é«”åˆ©ç”¨ç‡ |
|------|------------------|-----------|-------------|
| HuggingFace | 250 | 450 | 60% |
| **vLLM** | **2,800** | **120** | **95%** |
| åŠ é€Ÿæ¯” | **11.2x** | **3.8x** | **1.6x** |

---

### 3.2 TensorRT-LLM: NVIDIA å„ªåŒ–å¼•æ“

TensorRT-LLM æ¡ç”¨ AOT (Ahead-of-Time) ç·¨è­¯å„ªåŒ–ï¼Œæä¾›æ¥µè‡´æ€§èƒ½ã€‚

#### 3.2.1 ç·¨è­¯å„ªåŒ–æµç¨‹

```
TensorRT-LLM æµç¨‹:
HF Model â†’ Convert â†’ Build Engine â†’ Runtime
   â†“           â†“          â†“            â†“
 PyTorch   TRT-LLM    å„ªåŒ–ç·¨è­¯      æ¨ç†åŸ·è¡Œ
            æ ¼å¼      (kernelé¸æ“‡)   (CUDAåŸ·è¡Œ)
```

#### 3.2.2 é‡åŒ–æ”¯æ´

| ç²¾åº¦ | è¨˜æ†¶é«”å ç”¨ | é€Ÿåº¦ | ç²¾åº¦æå¤± | é©ç”¨å ´æ™¯ |
|------|----------|------|---------|---------|
| FP16 | 100% | 1.0x | ç„¡ | æ¨™æº–æ¨ç† |
| INT8 | 50% | 1.5-2x | <1% | å¹³è¡¡æ€§èƒ½ |
| FP8 (H100) | 50% | 2-3x | <0.5% | æ¥µè‡´æ€§èƒ½ |
| INT4 | 25% | 3-4x | 2-3% | è³‡æºå—é™ |

---

### 3.3 å¼•æ“é¸å‹æŒ‡å—

| å¼•æ“ | ååé‡ | å»¶é² | æ˜“ç”¨æ€§ | éƒ¨ç½² | æ¨è–¦å ´æ™¯ |
|------|-------|------|-------|------|---------|
| **vLLM** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | é€šç”¨æ¨è–¦ |
| **TensorRT-LLM** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­ | NVIDIA GPU æ¥µè‡´æ€§èƒ½ |
| **SGLang** | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | çµæ§‹åŒ–ç”Ÿæˆ |
| **TGI** | â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | HuggingFace ç”Ÿæ…‹ |

**é¸æ“‡å»ºè­°**:
- **é€šç”¨å ´æ™¯**: vLLM (æœ€ä½³å¹³è¡¡)
- **æ¥µè‡´æ€§èƒ½**: TensorRT-LLM (NVIDIA GPU)
- **çµæ§‹åŒ–è¼¸å‡º**: SGLang (JSON/YAML)
- **å¿«é€ŸåŸå‹**: TGI (HuggingFace å…¼å®¹)

---

## å»¶ä¼¸å­¸ç¿’

**è«–æ–‡**:
- vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention (SOSP 2023)
- FlashAttention: Fast and Memory-Efficient Exact Attention (NeurIPS 2022)

**è³‡æº**:
- vLLM å®˜æ–¹æ–‡æª”: https://docs.vllm.ai/
- TensorRT-LLM GitHub: https://github.com/NVIDIA/TensorRT-LLM
- SGLang å°ˆæ¡ˆ: https://github.com/sgl-project/sglang

**ä¸‹ä¸€æ­¥**: å­¸ç¿’ Lab-2.1 (vLLM Deployment) é€²è¡Œå¯¦ä½œç·´ç¿’

---

**ç‰ˆæœ¬**: v1.0  
**åˆ¶å®šæ—¥æœŸ**: 2025-10-09  
**ç¶­è­·è€…**: LLM æ•™å­¸å°ˆæ¡ˆåœ˜éšŠ
