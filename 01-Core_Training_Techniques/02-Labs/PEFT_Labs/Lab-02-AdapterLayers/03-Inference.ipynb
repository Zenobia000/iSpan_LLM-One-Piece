{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Adapter Layers - Fine-Tuning a BERT Model for Classification\n",
        "---\n",
        "## Notebook 3: Inference\n",
        "\n",
        "**Goal:** In this notebook, you will load the trained Adapter weights and use the fine-tuned model to make predictions on new sentence pairs.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Reload the base model and tokenizer.\n",
        "-   Load the trained Adapter weights from a checkpoint using `peft.PeftModel`.\n",
        "-   Write a function to perform inference on a pair of sentences.\n",
        "-   Interpret the model's output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Reload Model and Adapter\n",
        "\n",
        "First, we load the base model (`bert-base-uncased`) and the tokenizer, just as we did in the training notebook. Then, we use `PeftModel.from_pretrained` to load our saved adapter weights from the best checkpoint.\n",
        "\n",
        "#### Key Hugging Face Components:\n",
        "\n",
        "-   `transformers.AutoModelForSequenceClassification`: Loads the base model structure.\n",
        "-   `peft.PeftModel.from_pretrained`: Loads the trained adapter weights and correctly attaches them to the base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- Load Base Model and Tokenizer ---\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "num_labels = 2\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, \n",
        "    num_labels=num_labels\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "# --- Load PEFT Adapter ---\n",
        "output_dir = \"./bert-adapters-mrpc\"\n",
        "# Find the latest checkpoint\n",
        "latest_checkpoint = max(\n",
        "    [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")],\n",
        "    key=os.path.getmtime\n",
        ")\n",
        "print(f\"Loading adapter from: {latest_checkpoint}\")\n",
        "\n",
        "inference_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inference_model.to(device)\n",
        "inference_model.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"âœ… Inference model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Perform Inference\n",
        "\n",
        "Now, let's create a simple function to test our model. The function will take two sentences, tokenize them, pass them through the model, and return the predicted label.\n",
        "\n",
        "The MRPC dataset labels are:\n",
        "-   `0`: Not a paraphrase\n",
        "-   `1`: Is a paraphrase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the labels\n",
        "id2label = {0: \"Not a Paraphrase\", 1: \"Is a Paraphrase\"}\n",
        "\n",
        "def predict_paraphrase(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Takes two sentences and predicts if they are paraphrases.\n",
        "    \"\"\"\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(sentence1, sentence2, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    \n",
        "    # Move inputs to the correct device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Get model output\n",
        "    with torch.no_grad():\n",
        "        outputs = inference_model(**inputs)\n",
        "    \n",
        "    # Get probabilities and prediction\n",
        "    logits = outputs.logits\n",
        "    probabilities = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "    prediction = torch.argmax(logits, dim=-1).cpu().item()\n",
        "    \n",
        "    print(f\"Sentence 1: '{sentence1}'\")\n",
        "    print(f\"Sentence 2: '{sentence2}'\")\n",
        "    print(f\"Prediction: {id2label[prediction]}\")\n",
        "    print(f\"Probabilities:\")\n",
        "    print(f\"  - {id2label[0]}: {probabilities[0]:.4f}\")\n",
        "    print(f\"  - {id2label[1]}: {probabilities[1]:.4f}\")\n",
        "\n",
        "# --- Test Cases ---\n",
        "print(\"--- Test Case 1 (Should be a paraphrase) ---\")\n",
        "predict_paraphrase(\n",
        "    \"The company said the merger was subject to the approval of its shareholders.\",\n",
        "    \"The company said the deal was subject to the approval of its shareholders.\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- Test Case 2 (Should NOT be a paraphrase) ---\")\n",
        "predict_paraphrase(\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog played in the garden.\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
