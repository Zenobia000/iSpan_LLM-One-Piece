{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.2.1: 企業級多模型倉庫架構設計\n",
    "\n",
    "## 🎯 學習目標\n",
    "\n",
    "- 理解企業級多模型倉庫的設計原則\n",
    "- 實現模型間依賴關係管理\n",
    "- 掌握資源分配與隔離策略\n",
    "- 建立並發部署與版本衝突處理機制\n",
    "\n",
    "## 🏢 企業案例: Netflix 推薦系統多模型架構\n",
    "\n",
    "Netflix 同時部署超過 20 個模型：\n",
    "- **推薦模型**: 個人化推薦、相似內容推薦\n",
    "- **搜索模型**: 查詢理解、內容檢索\n",
    "- **內容模型**: 縮圖生成、字幕翻譯\n",
    "- **分析模型**: 觀看行為分析、流失預測\n",
    "\n",
    "每個模型都有多個版本在生產環境中運行，需要統一的管理架構。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ 環境準備與依賴檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# 設定日誌\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"🚀 企業級多模型倉庫設計 - 環境檢查\")\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "print(f\"工作目錄: {os.getcwd()}\")\n",
    "\n",
    "# 檢查 Triton 相關工具\n",
    "def check_triton_tools():\n",
    "    tools = {\n",
    "        'curl': 'HTTP 客戶端測試',\n",
    "        'docker': 'Triton 容器管理',\n",
    "        'nvidia-smi': 'GPU 監控',\n",
    "    }\n",
    "    \n",
    "    for tool, desc in tools.items():\n",
    "        try:\n",
    "            result = subprocess.run([tool, '--version'], \n",
    "                                  capture_output=True, text=True, timeout=5)\n",
    "            print(f\"✅ {tool}: {desc} - 可用\")\n",
    "        except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "            print(f\"⚠️ {tool}: {desc} - 未安裝或不可用\")\n",
    "\n",
    "check_triton_tools()\n",
    "print(\"\\n✅ 環境檢查完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ 企業級模型倉庫架構設計\n",
    "\n",
    "### 1. 模型分類與組織策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelMetadata:\n",
    "    \"\"\"模型元數據定義\"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "    model_type: str  # 'classification', 'generation', 'embedding', 'ensemble'\n",
    "    framework: str   # 'pytorch', 'tensorflow', 'onnx', 'python'\n",
    "    domain: str      # 'recommendation', 'search', 'content', 'analytics'\n",
    "    owner_team: str\n",
    "    description: str\n",
    "    created_at: str\n",
    "    dependencies: List[str] = None\n",
    "    resource_requirements: Dict[str, Any] = None\n",
    "    sla_requirements: Dict[str, Any] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.dependencies is None:\n",
    "            self.dependencies = []\n",
    "        if self.resource_requirements is None:\n",
    "            self.resource_requirements = {\n",
    "                'memory_mb': 2048,\n",
    "                'gpu_memory_mb': 4096,\n",
    "                'cpu_cores': 2,\n",
    "                'max_batch_size': 32\n",
    "            }\n",
    "        if self.sla_requirements is None:\n",
    "            self.sla_requirements = {\n",
    "                'max_latency_p99_ms': 100,\n",
    "                'min_throughput_rps': 10,\n",
    "                'availability': 0.999\n",
    "            }\n",
    "\n",
    "class EnterpriseModelRepository:\n",
    "    \"\"\"企業級模型倉庫管理器\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"./enterprise_model_repository\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.metadata_file = self.base_path / \"repository_metadata.json\"\n",
    "        self.models = {}\n",
    "        self._initialize_repository()\n",
    "    \n",
    "    def _initialize_repository(self):\n",
    "        \"\"\"初始化倉庫結構\"\"\"\n",
    "        self.base_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # 創建企業級目錄結構\n",
    "        domains = ['recommendation', 'search', 'content', 'analytics', 'shared']\n",
    "        for domain in domains:\n",
    "            (self.base_path / domain).mkdir(exist_ok=True)\n",
    "        \n",
    "        # 載入現有元數據\n",
    "        if self.metadata_file.exists():\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                self.models = {k: ModelMetadata(**v) for k, v in data.items()}\n",
    "        \n",
    "        logger.info(f\"企業模型倉庫初始化完成: {self.base_path}\")\n",
    "    \n",
    "    def register_model(self, metadata: ModelMetadata) -> bool:\n",
    "        \"\"\"註冊新模型到倉庫\"\"\"\n",
    "        model_id = f\"{metadata.domain}/{metadata.name}\"\n",
    "        \n",
    "        # 檢查依賴關係\n",
    "        if not self._validate_dependencies(metadata.dependencies):\n",
    "            logger.error(f\"模型 {model_id} 依賴檢查失敗\")\n",
    "            return False\n",
    "        \n",
    "        # 檢查資源衝突\n",
    "        if not self._check_resource_conflicts(metadata):\n",
    "            logger.warning(f\"模型 {model_id} 可能存在資源衝突\")\n",
    "        \n",
    "        # 創建模型目錄結構\n",
    "        model_path = self.base_path / metadata.domain / metadata.name\n",
    "        version_path = model_path / metadata.version\n",
    "        version_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 生成 Triton 配置\n",
    "        config = self._generate_triton_config(metadata)\n",
    "        config_path = version_path / \"config.pbtxt\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            f.write(config)\n",
    "        \n",
    "        # 更新元數據\n",
    "        self.models[model_id] = metadata\n",
    "        self._save_metadata()\n",
    "        \n",
    "        logger.info(f\"✅ 模型 {model_id} v{metadata.version} 註冊成功\")\n",
    "        return True\n",
    "    \n",
    "    def _validate_dependencies(self, dependencies: List[str]) -> bool:\n",
    "        \"\"\"驗證模型依賴關係\"\"\"\n",
    "        for dep in dependencies:\n",
    "            if dep not in self.models:\n",
    "                logger.error(f\"依賴模型 {dep} 不存在\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _check_resource_conflicts(self, metadata: ModelMetadata) -> bool:\n",
    "        \"\"\"檢查資源衝突\"\"\"\n",
    "        total_gpu_memory = sum(\n",
    "            model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "            for model in self.models.values()\n",
    "        )\n",
    "        \n",
    "        new_gpu_memory = metadata.resource_requirements.get('gpu_memory_mb', 0)\n",
    "        \n",
    "        # 假設總 GPU 記憶體為 24GB\n",
    "        max_gpu_memory = 24 * 1024  # MB\n",
    "        \n",
    "        if total_gpu_memory + new_gpu_memory > max_gpu_memory:\n",
    "            logger.warning(f\"GPU 記憶體可能不足: {total_gpu_memory + new_gpu_memory}MB > {max_gpu_memory}MB\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _generate_triton_config(self, metadata: ModelMetadata) -> str:\n",
    "        \"\"\"生成 Triton 配置檔案\"\"\"\n",
    "        backend = {\n",
    "            'pytorch': 'pytorch',\n",
    "            'tensorflow': 'tensorflow',\n",
    "            'onnx': 'onnxruntime',\n",
    "            'python': 'python'\n",
    "        }.get(metadata.framework, 'python')\n",
    "        \n",
    "        config = f'''\n",
    "name: \"{metadata.name}\"\n",
    "backend: \"{backend}\"\n",
    "max_batch_size: {metadata.resource_requirements['max_batch_size']}\n",
    "\n",
    "# 輸入配置 (示例)\n",
    "input [\n",
    "  {{\n",
    "    name: \"INPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "# 輸出配置 (示例)\n",
    "output [\n",
    "  {{\n",
    "    name: \"OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "# 動態批處理配置\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: {metadata.sla_requirements['max_latency_p99_ms'] * 1000}\n",
    "}}\n",
    "\n",
    "# 實例群組配置\n",
    "instance_group [\n",
    "  {{\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus: [ 0 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "# 模型元數據\n",
    "parameters: {{\n",
    "  key: \"domain\"\n",
    "  value: {{ string_value: \"{metadata.domain}\" }}\n",
    "}}\n",
    "parameters: {{\n",
    "  key: \"owner_team\"\n",
    "  value: {{ string_value: \"{metadata.owner_team}\" }}\n",
    "}}\n",
    "parameters: {{\n",
    "  key: \"sla_latency_p99_ms\"\n",
    "  value: {{ string_value: \"{metadata.sla_requirements['max_latency_p99_ms']}\" }}\n",
    "}}\n",
    "'''.strip()\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"保存元數據到檔案\"\"\"\n",
    "        data = {k: asdict(v) for k, v in self.models.items()}\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def list_models(self, domain: Optional[str] = None) -> List[ModelMetadata]:\n",
    "        \"\"\"列出模型\"\"\"\n",
    "        if domain:\n",
    "            return [model for model_id, model in self.models.items() \n",
    "                   if model.domain == domain]\n",
    "        return list(self.models.values())\n",
    "    \n",
    "    def get_model_dependencies(self, model_id: str) -> List[str]:\n",
    "        \"\"\"獲取模型依賴圖\"\"\"\n",
    "        if model_id not in self.models:\n",
    "            return []\n",
    "        \n",
    "        def get_deps(mid: str, visited: set) -> List[str]:\n",
    "            if mid in visited:\n",
    "                return []  # 避免循環依賴\n",
    "            \n",
    "            visited.add(mid)\n",
    "            deps = []\n",
    "            \n",
    "            if mid in self.models:\n",
    "                for dep in self.models[mid].dependencies:\n",
    "                    deps.append(dep)\n",
    "                    deps.extend(get_deps(dep, visited.copy()))\n",
    "            \n",
    "            return deps\n",
    "        \n",
    "        return get_deps(model_id, set())\n",
    "\n",
    "# 初始化企業模型倉庫\n",
    "repo = EnterpriseModelRepository()\n",
    "print(\"\\n✅ 企業級模型倉庫初始化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Netflix 案例：建立推薦系統模型群組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 Netflix 推薦系統模型群組\n",
    "netflix_models = [\n",
    "    ModelMetadata(\n",
    "        name=\"user_embedding\",\n",
    "        version=\"1.0\",\n",
    "        model_type=\"embedding\",\n",
    "        framework=\"pytorch\",\n",
    "        domain=\"recommendation\",\n",
    "        owner_team=\"recommendation_platform\",\n",
    "        description=\"用戶向量化模型，將用戶行為轉換為稠密向量\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 4096,\n",
    "            'gpu_memory_mb': 6144,\n",
    "            'cpu_cores': 4,\n",
    "            'max_batch_size': 128\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 50,\n",
    "            'min_throughput_rps': 100,\n",
    "            'availability': 0.9999\n",
    "        }\n",
    "    ),\n",
    "    ModelMetadata(\n",
    "        name=\"content_embedding\",\n",
    "        version=\"2.1\",\n",
    "        model_type=\"embedding\",\n",
    "        framework=\"pytorch\",\n",
    "        domain=\"recommendation\",\n",
    "        owner_team=\"content_intelligence\",\n",
    "        description=\"內容向量化模型，提取影視內容特徵\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 6144,\n",
    "            'gpu_memory_mb': 8192,\n",
    "            'cpu_cores': 6,\n",
    "            'max_batch_size': 64\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 75,\n",
    "            'min_throughput_rps': 50,\n",
    "            'availability': 0.9995\n",
    "        }\n",
    "    ),\n",
    "    ModelMetadata(\n",
    "        name=\"recommendation_ranker\",\n",
    "        version=\"3.0\",\n",
    "        model_type=\"classification\",\n",
    "        framework=\"pytorch\",\n",
    "        domain=\"recommendation\",\n",
    "        owner_team=\"recommendation_platform\",\n",
    "        description=\"推薦排序模型，基於用戶和內容特徵進行個人化排序\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[\"recommendation/user_embedding\", \"recommendation/content_embedding\"],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 8192,\n",
    "            'gpu_memory_mb': 12288,\n",
    "            'cpu_cores': 8,\n",
    "            'max_batch_size': 32\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 100,\n",
    "            'min_throughput_rps': 25,\n",
    "            'availability': 0.9999\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# 註冊模型到倉庫\n",
    "print(\"📊 註冊 Netflix 推薦系統模型群組...\")\n",
    "for model in netflix_models:\n",
    "    success = repo.register_model(model)\n",
    "    if success:\n",
    "        print(f\"✅ {model.domain}/{model.name} v{model.version} 註冊成功\")\n",
    "    else:\n",
    "        print(f\"❌ {model.domain}/{model.name} v{model.version} 註冊失敗\")\n",
    "\n",
    "print(f\"\\n📈 倉庫統計:\")\n",
    "print(f\"- 總模型數量: {len(repo.models)}\")\n",
    "print(f\"- 推薦領域模型: {len(repo.list_models('recommendation'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 搜索領域模型添加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加搜索領域模型\n",
    "search_models = [\n",
    "    ModelMetadata(\n",
    "        name=\"query_understanding\",\n",
    "        version=\"1.5\",\n",
    "        model_type=\"classification\",\n",
    "        framework=\"pytorch\",\n",
    "        domain=\"search\",\n",
    "        owner_team=\"search_experience\",\n",
    "        description=\"查詢理解模型，解析用戶搜索意圖和實體識別\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 3072,\n",
    "            'gpu_memory_mb': 4096,\n",
    "            'cpu_cores': 4,\n",
    "            'max_batch_size': 64\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 80,\n",
    "            'min_throughput_rps': 200,\n",
    "            'availability': 0.9995\n",
    "        }\n",
    "    ),\n",
    "    ModelMetadata(\n",
    "        name=\"content_retrieval\",\n",
    "        version=\"2.0\",\n",
    "        model_type=\"embedding\",\n",
    "        framework=\"onnx\",\n",
    "        domain=\"search\",\n",
    "        owner_team=\"search_experience\",\n",
    "        description=\"內容檢索模型，基於語義相似度檢索相關內容\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[\"search/query_understanding\"],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 2048,\n",
    "            'gpu_memory_mb': 3072,\n",
    "            'cpu_cores': 2,\n",
    "            'max_batch_size': 128\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 60,\n",
    "            'min_throughput_rps': 150,\n",
    "            'availability': 0.999\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# 註冊搜索模型\n",
    "print(\"🔍 註冊搜索領域模型...\")\n",
    "for model in search_models:\n",
    "    success = repo.register_model(model)\n",
    "    if success:\n",
    "        print(f\"✅ {model.domain}/{model.name} v{model.version} 註冊成功\")\n",
    "    else:\n",
    "        print(f\"❌ {model.domain}/{model.name} v{model.version} 註冊失敗\")\n",
    "\n",
    "# 驗證依賴關係\n",
    "print(\"\\n🔗 依賴關係分析:\")\n",
    "for model_id in repo.models.keys():\n",
    "    deps = repo.get_model_dependencies(model_id)\n",
    "    if deps:\n",
    "        print(f\"- {model_id}: 依賴 {', '.join(deps)}\")\n",
    "    else:\n",
    "        print(f\"- {model_id}: 無依賴\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 倉庫結構視覺化與分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_repository_structure():\n",
    "    \"\"\"視覺化倉庫結構\"\"\"\n",
    "    print(\"🏗️ 企業級模型倉庫結構:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 按領域分組顯示\n",
    "    domains = {}\n",
    "    for model_id, model in repo.models.items():\n",
    "        if model.domain not in domains:\n",
    "            domains[model.domain] = []\n",
    "        domains[model.domain].append(model)\n",
    "    \n",
    "    for domain, models in domains.items():\n",
    "        print(f\"\\n📁 {domain.upper()} 領域 ({len(models)} 個模型)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for model in models:\n",
    "            deps_info = f\" (依賴: {len(model.dependencies)} 個)\" if model.dependencies else \" (無依賴)\"\n",
    "            gpu_mb = model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "            latency = model.sla_requirements.get('max_latency_p99_ms', 0)\n",
    "            \n",
    "            print(f\"  📦 {model.name} v{model.version}\")\n",
    "            print(f\"     框架: {model.framework} | GPU: {gpu_mb}MB | 延遲: {latency}ms{deps_info}\")\n",
    "            print(f\"     團隊: {model.owner_team}\")\n",
    "            print(f\"     描述: {model.description[:50]}...\")\n",
    "            print()\n",
    "\n",
    "def analyze_resource_usage():\n",
    "    \"\"\"分析資源使用情況\"\"\"\n",
    "    print(\"\\n📈 資源使用分析:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_memory = 0\n",
    "    total_gpu_memory = 0\n",
    "    total_cpu_cores = 0\n",
    "    \n",
    "    framework_count = {}\n",
    "    domain_resources = {}\n",
    "    \n",
    "    for model in repo.models.values():\n",
    "        # 累計資源\n",
    "        total_memory += model.resource_requirements.get('memory_mb', 0)\n",
    "        total_gpu_memory += model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "        total_cpu_cores += model.resource_requirements.get('cpu_cores', 0)\n",
    "        \n",
    "        # 框架統計\n",
    "        framework_count[model.framework] = framework_count.get(model.framework, 0) + 1\n",
    "        \n",
    "        # 領域資源統計\n",
    "        if model.domain not in domain_resources:\n",
    "            domain_resources[model.domain] = {'memory': 0, 'gpu_memory': 0, 'models': 0}\n",
    "        \n",
    "        domain_resources[model.domain]['memory'] += model.resource_requirements.get('memory_mb', 0)\n",
    "        domain_resources[model.domain]['gpu_memory'] += model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "        domain_resources[model.domain]['models'] += 1\n",
    "    \n",
    "    print(f\"💾 總記憶體需求: {total_memory/1024:.1f} GB\")\n",
    "    print(f\"🎮 總 GPU 記憶體需求: {total_gpu_memory/1024:.1f} GB\")\n",
    "    print(f\"⚡ 總 CPU 核心需求: {total_cpu_cores} 核\")\n",
    "    \n",
    "    print(f\"\\n🔧 框架分佈:\")\n",
    "    for framework, count in framework_count.items():\n",
    "        percentage = (count / len(repo.models)) * 100\n",
    "        print(f\"  - {framework}: {count} 個模型 ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n📊 領域資源分佈:\")\n",
    "    for domain, resources in domain_resources.items():\n",
    "        print(f\"  - {domain}:\")\n",
    "        print(f\"    模型數量: {resources['models']}\")\n",
    "        print(f\"    記憶體: {resources['memory']/1024:.1f} GB\")\n",
    "        print(f\"    GPU 記憶體: {resources['gpu_memory']/1024:.1f} GB\")\n",
    "\n",
    "def analyze_sla_requirements():\n",
    "    \"\"\"分析 SLA 需求\"\"\"\n",
    "    print(f\"\\n⏱️ SLA 需求分析:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "    availabilities = []\n",
    "    \n",
    "    print(\"模型名稱\\t\\t延遲(ms)\\t吞吐量(RPS)\\t可用性\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model in repo.models.values():\n",
    "        latency = model.sla_requirements.get('max_latency_p99_ms', 0)\n",
    "        throughput = model.sla_requirements.get('min_throughput_rps', 0)\n",
    "        availability = model.sla_requirements.get('availability', 0)\n",
    "        \n",
    "        latencies.append(latency)\n",
    "        throughputs.append(throughput)\n",
    "        availabilities.append(availability)\n",
    "        \n",
    "        print(f\"{model.name[:15]:<15}\\t{latency:<8}\\t{throughput:<12}\\t{availability*100:.2f}%\")\n",
    "    \n",
    "    if latencies:\n",
    "        print(f\"\\n📊 統計摘要:\")\n",
    "        print(f\"  平均延遲: {sum(latencies)/len(latencies):.1f} ms\")\n",
    "        print(f\"  平均吞吐量: {sum(throughputs)/len(throughputs):.1f} RPS\")\n",
    "        print(f\"  平均可用性: {(sum(availabilities)/len(availabilities))*100:.3f}%\")\n",
    "        print(f\"  最嚴格延遲要求: {min(latencies)} ms\")\n",
    "        print(f\"  最高吞吐量要求: {max(throughputs)} RPS\")\n",
    "\n",
    "# 執行分析\n",
    "visualize_repository_structure()\n",
    "analyze_resource_usage()\n",
    "analyze_sla_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 模型部署順序規劃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentPlanner:\n",
    "    \"\"\"模型部署順序規劃器\"\"\"\n",
    "    \n",
    "    def __init__(self, repository: EnterpriseModelRepository):\n",
    "        self.repo = repository\n",
    "    \n",
    "    def create_deployment_plan(self) -> List[List[str]]:\n",
    "        \"\"\"創建部署計劃 - 考慮依賴關係的拓撲排序\"\"\"\n",
    "        # 建立依賴圖\n",
    "        graph = {}\n",
    "        in_degree = {}\n",
    "        \n",
    "        for model_id in self.repo.models.keys():\n",
    "            graph[model_id] = []\n",
    "            in_degree[model_id] = 0\n",
    "        \n",
    "        # 建立邊和入度\n",
    "        for model_id, model in self.repo.models.items():\n",
    "            for dep in model.dependencies:\n",
    "                if dep in graph:\n",
    "                    graph[dep].append(model_id)\n",
    "                    in_degree[model_id] += 1\n",
    "        \n",
    "        # Kahn's 算法進行拓撲排序\n",
    "        deployment_waves = []\n",
    "        queue = [model_id for model_id, degree in in_degree.items() if degree == 0]\n",
    "        \n",
    "        while queue:\n",
    "            current_wave = queue.copy()\n",
    "            deployment_waves.append(current_wave)\n",
    "            queue.clear()\n",
    "            \n",
    "            for model_id in current_wave:\n",
    "                for neighbor in graph[model_id]:\n",
    "                    in_degree[neighbor] -= 1\n",
    "                    if in_degree[neighbor] == 0:\n",
    "                        queue.append(neighbor)\n",
    "        \n",
    "        return deployment_waves\n",
    "    \n",
    "    def estimate_deployment_time(self, deployment_waves: List[List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"估算部署時間\"\"\"\n",
    "        # 假設部署時間與模型大小相關\n",
    "        base_deployment_time = 2  # 基礎部署時間（分鐘）\n",
    "        \n",
    "        wave_times = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for i, wave in enumerate(deployment_waves):\n",
    "            wave_time = 0\n",
    "            for model_id in wave:\n",
    "                model = self.repo.models[model_id]\n",
    "                # 根據 GPU 記憶體需求估算部署時間\n",
    "                gpu_memory_gb = model.resource_requirements.get('gpu_memory_mb', 0) / 1024\n",
    "                deployment_time = base_deployment_time + (gpu_memory_gb * 0.5)\n",
    "                wave_time = max(wave_time, deployment_time)  # 並行部署取最長時間\n",
    "            \n",
    "            wave_times.append(wave_time)\n",
    "            total_time += wave_time\n",
    "        \n",
    "        return {\n",
    "            'total_time_minutes': total_time,\n",
    "            'wave_times': wave_times,\n",
    "            'total_waves': len(deployment_waves)\n",
    "        }\n",
    "    \n",
    "    def generate_deployment_script(self, deployment_waves: List[List[str]]) -> str:\n",
    "        \"\"\"生成部署腳本\"\"\"\n",
    "        script_lines = [\n",
    "            \"#!/bin/bash\",\n",
    "            \"# 企業級多模型部署腳本\",\n",
    "            \"# 自動生成 - 請勿手動修改\",\n",
    "            \"\",\n",
    "            \"set -e  # 發生錯誤時立即退出\",\n",
    "            \"\",\n",
    "            \"echo '🚀 開始企業級多模型部署'\",\n",
    "            \"echo '==============================================='\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        for i, wave in enumerate(deployment_waves, 1):\n",
    "            script_lines.extend([\n",
    "                f\"echo '📦 第 {i} 波部署: {len(wave)} 個模型'\",\n",
    "                \"echo '-----------------------------------------------'\",\n",
    "                \"\"\n",
    "            ])\n",
    "            \n",
    "            # 並行部署同一波的模型\n",
    "            for model_id in wave:\n",
    "                domain, name = model_id.split('/')\n",
    "                model = self.repo.models[model_id]\n",
    "                \n",
    "                script_lines.extend([\n",
    "                    f\"echo '  🔄 部署 {model_id} v{model.version}'\",\n",
    "                    f\"# 健康檢查\",\n",
    "                    f\"curl -f http://localhost:8000/v2/models/{name}/ready || {{\",\n",
    "                    f\"  echo '❌ 模型 {name} 部署失敗'\",\n",
    "                    f\"  exit 1\",\n",
    "                    f\"}}\",\n",
    "                    f\"echo '✅ 模型 {name} 部署成功'\",\n",
    "                    \"\"\n",
    "                ])\n",
    "            \n",
    "            if i < len(deployment_waves):\n",
    "                script_lines.extend([\n",
    "                    \"echo '⏳ 等待當前波部署完成...'\",\n",
    "                    \"sleep 30\",\n",
    "                    \"\"\n",
    "                ])\n",
    "        \n",
    "        script_lines.extend([\n",
    "            \"echo '✅ 所有模型部署完成！'\",\n",
    "            \"echo '📊 最終狀態檢查...'\",\n",
    "            \"curl -s http://localhost:8000/v2/models | jq .\",\n",
    "            \"echo '🎉 企業級多模型平台部署成功！'\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(script_lines)\n",
    "\n",
    "# 創建部署計劃\n",
    "planner = DeploymentPlanner(repo)\n",
    "deployment_waves = planner.create_deployment_plan()\n",
    "time_estimate = planner.estimate_deployment_time(deployment_waves)\n",
    "\n",
    "print(\"📅 智能部署計劃:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, wave in enumerate(deployment_waves, 1):\n",
    "    print(f\"\\n🌊 第 {i} 波部署 (預估 {time_estimate['wave_times'][i-1]:.1f} 分鐘):\")\n",
    "    for model_id in wave:\n",
    "        model = repo.models[model_id]\n",
    "        deps_info = f\" (依賴: {', '.join(model.dependencies)})\" if model.dependencies else \" (無依賴)\"\n",
    "        print(f\"  📦 {model_id} v{model.version}{deps_info}\")\n",
    "\n",
    "print(f\"\\n⏱️ 總部署時間估算: {time_estimate['total_time_minutes']:.1f} 分鐘\")\n",
    "print(f\"📊 總部署波數: {time_estimate['total_waves']}\")\n",
    "\n",
    "# 生成部署腳本\n",
    "deployment_script = planner.generate_deployment_script(deployment_waves)\n",
    "script_path = repo.base_path / \"deploy_models.sh\"\n",
    "\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(deployment_script)\n",
    "\n",
    "# 設定執行權限\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"\\n📜 部署腳本已生成: {script_path}\")\n",
    "print(\"執行方式: ./deploy_models.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 倉庫驗證與健康檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepositoryValidator:\n",
    "    \"\"\"倉庫驗證器\"\"\"\n",
    "    \n",
    "    def __init__(self, repository: EnterpriseModelRepository):\n",
    "        self.repo = repository\n",
    "        self.issues = []\n",
    "        self.warnings = []\n",
    "    \n",
    "    def validate_all(self) -> Dict[str, Any]:\n",
    "        \"\"\"執行完整驗證\"\"\"\n",
    "        self.issues.clear()\n",
    "        self.warnings.clear()\n",
    "        \n",
    "        print(\"🔍 執行倉庫健康檢查...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 各項檢查\n",
    "        self._check_dependency_cycles()\n",
    "        self._check_resource_constraints()\n",
    "        self._check_sla_consistency()\n",
    "        self._check_file_structure()\n",
    "        self._check_naming_conventions()\n",
    "        \n",
    "        return {\n",
    "            'total_issues': len(self.issues),\n",
    "            'total_warnings': len(self.warnings),\n",
    "            'issues': self.issues,\n",
    "            'warnings': self.warnings,\n",
    "            'status': 'healthy' if len(self.issues) == 0 else 'needs_attention'\n",
    "        }\n",
    "    \n",
    "    def _check_dependency_cycles(self):\n",
    "        \"\"\"檢查循環依賴\"\"\"\n",
    "        print(\"🔄 檢查循環依賴...\")\n",
    "        \n",
    "        def has_cycle(model_id: str, visited: set, rec_stack: set) -> bool:\n",
    "            visited.add(model_id)\n",
    "            rec_stack.add(model_id)\n",
    "            \n",
    "            if model_id in self.repo.models:\n",
    "                for dep in self.repo.models[model_id].dependencies:\n",
    "                    if dep not in visited:\n",
    "                        if has_cycle(dep, visited, rec_stack):\n",
    "                            return True\n",
    "                    elif dep in rec_stack:\n",
    "                        return True\n",
    "            \n",
    "            rec_stack.remove(model_id)\n",
    "            return False\n",
    "        \n",
    "        visited = set()\n",
    "        for model_id in self.repo.models.keys():\n",
    "            if model_id not in visited:\n",
    "                if has_cycle(model_id, visited, set()):\n",
    "                    self.issues.append(f\"檢測到循環依賴，涉及模型: {model_id}\")\n",
    "        \n",
    "        if not any(\"循環依賴\" in issue for issue in self.issues):\n",
    "            print(\"  ✅ 無循環依賴\")\n",
    "    \n",
    "    def _check_resource_constraints(self):\n",
    "        \"\"\"檢查資源約束\"\"\"\n",
    "        print(\"💾 檢查資源約束...\")\n",
    "        \n",
    "        total_gpu_memory = sum(\n",
    "            model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "            for model in self.repo.models.values()\n",
    "        )\n",
    "        \n",
    "        # 假設系統限制\n",
    "        max_gpu_memory = 32 * 1024  # 32GB\n",
    "        max_cpu_cores = 64\n",
    "        \n",
    "        if total_gpu_memory > max_gpu_memory:\n",
    "            self.issues.append(\n",
    "                f\"GPU 記憶體需求超出限制: {total_gpu_memory/1024:.1f}GB > {max_gpu_memory/1024}GB\"\n",
    "            )\n",
    "        elif total_gpu_memory > max_gpu_memory * 0.8:\n",
    "            self.warnings.append(\n",
    "                f\"GPU 記憶體使用率較高: {total_gpu_memory/1024:.1f}GB (80%+ 容量)\"\n",
    "            )\n",
    "        \n",
    "        total_cpu_cores = sum(\n",
    "            model.resource_requirements.get('cpu_cores', 0)\n",
    "            for model in self.repo.models.values()\n",
    "        )\n",
    "        \n",
    "        if total_cpu_cores > max_cpu_cores:\n",
    "            self.issues.append(\n",
    "                f\"CPU 核心需求超出限制: {total_cpu_cores} > {max_cpu_cores}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"  📊 GPU 記憶體使用: {total_gpu_memory/1024:.1f}GB / {max_gpu_memory/1024}GB\")\n",
    "        print(f\"  ⚡ CPU 核心使用: {total_cpu_cores} / {max_cpu_cores}\")\n",
    "    \n",
    "    def _check_sla_consistency(self):\n",
    "        \"\"\"檢查 SLA 一致性\"\"\"\n",
    "        print(\"⏱️ 檢查 SLA 一致性...\")\n",
    "        \n",
    "        for model_id, model in self.repo.models.items():\n",
    "            sla = model.sla_requirements\n",
    "            \n",
    "            # 檢查依賴鏈的 SLA 一致性\n",
    "            for dep_id in model.dependencies:\n",
    "                if dep_id in self.repo.models:\n",
    "                    dep_model = self.repo.models[dep_id]\n",
    "                    dep_sla = dep_model.sla_requirements\n",
    "                    \n",
    "                    # 依賴模型的延遲應該更嚴格\n",
    "                    if dep_sla['max_latency_p99_ms'] >= sla['max_latency_p99_ms']:\n",
    "                        self.warnings.append(\n",
    "                            f\"SLA 不一致: {model_id} 依賴 {dep_id}，但依賴延遲要求不夠嚴格\"\n",
    "                        )\n",
    "            \n",
    "            # 檢查不合理的 SLA 設定\n",
    "            if sla['max_latency_p99_ms'] < 10:\n",
    "                self.warnings.append(\n",
    "                    f\"SLA 可能過於嚴格: {model_id} 延遲要求 {sla['max_latency_p99_ms']}ms\"\n",
    "                )\n",
    "            \n",
    "            if sla['availability'] > 0.9999:\n",
    "                self.warnings.append(\n",
    "                    f\"可用性要求極高: {model_id} 要求 {sla['availability']*100:.3f}% 可用性\"\n",
    "                )\n",
    "    \n",
    "    def _check_file_structure(self):\n",
    "        \"\"\"檢查檔案結構\"\"\"\n",
    "        print(\"📁 檢查檔案結構...\")\n",
    "        \n",
    "        for model_id, model in self.repo.models.items():\n",
    "            domain, name = model_id.split('/')\n",
    "            model_path = self.repo.base_path / domain / name / model.version\n",
    "            config_path = model_path / \"config.pbtxt\"\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                self.issues.append(f\"模型目錄不存在: {model_path}\")\n",
    "            elif not config_path.exists():\n",
    "                self.issues.append(f\"配置檔案不存在: {config_path}\")\n",
    "    \n",
    "    def _check_naming_conventions(self):\n",
    "        \"\"\"檢查命名規範\"\"\"\n",
    "        print(\"📝 檢查命名規範...\")\n",
    "        \n",
    "        import re\n",
    "        \n",
    "        # 模型名稱應該使用小寫和底線\n",
    "        name_pattern = re.compile(r'^[a-z][a-z0-9_]*$')\n",
    "        \n",
    "        for model_id, model in self.repo.models.items():\n",
    "            if not name_pattern.match(model.name):\n",
    "                self.warnings.append(\n",
    "                    f\"命名不規範: {model.name} (建議使用小寫字母、數字和底線)\"\n",
    "                )\n",
    "            \n",
    "            # 版本號應該遵循語義化版本\n",
    "            version_pattern = re.compile(r'^\\d+\\.\\d+(\\..+)?$')\n",
    "            if not version_pattern.match(model.version):\n",
    "                self.warnings.append(\n",
    "                    f\"版本號不規範: {model.version} (建議使用語義化版本如 1.0, 2.1.3)\"\n",
    "                )\n",
    "\n",
    "# 執行驗證\n",
    "validator = RepositoryValidator(repo)\n",
    "validation_result = validator.validate_all()\n",
    "\n",
    "print(f\"\\n📋 驗證結果摘要:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"🔴 嚴重問題: {validation_result['total_issues']} 個\")\n",
    "print(f\"🟡 警告: {validation_result['total_warnings']} 個\")\n",
    "print(f\"📊 整體狀態: {validation_result['status'].upper()}\")\n",
    "\n",
    "if validation_result['issues']:\n",
    "    print(f\"\\n🔴 需要修復的問題:\")\n",
    "    for i, issue in enumerate(validation_result['issues'], 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "\n",
    "if validation_result['warnings']:\n",
    "    print(f\"\\n🟡 建議改進的警告:\")\n",
    "    for i, warning in enumerate(validation_result['warnings'], 1):\n",
    "        print(f\"  {i}. {warning}\")\n",
    "\n",
    "if validation_result['status'] == 'healthy':\n",
    "    print(f\"\\n✅ 恭喜！倉庫結構健康，可以開始部署\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ 建議修復上述問題後再進行部署\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 實際目錄結構展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_directory_tree(path: Path, prefix: str = \"\", max_depth: int = 3, current_depth: int = 0):\n",
    "    \"\"\"顯示目錄樹結構\"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    if path.is_dir():\n",
    "        print(f\"{prefix}📁 {path.name}/\")\n",
    "        \n",
    "        try:\n",
    "            children = sorted(path.iterdir())\n",
    "            for i, child in enumerate(children):\n",
    "                is_last = i == len(children) - 1\n",
    "                child_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "                print(f\"{prefix}{'└── ' if is_last else '├── '}\", end=\"\")\n",
    "                \n",
    "                if child.is_dir():\n",
    "                    print(f\"📁 {child.name}/\")\n",
    "                    if current_depth < max_depth:\n",
    "                        for j, grandchild in enumerate(sorted(child.iterdir())):\n",
    "                            is_last_grand = j == len(list(child.iterdir())) - 1\n",
    "                            grand_prefix = child_prefix + (\"    \" if is_last_grand else \"│   \")\n",
    "                            icon = \"📄\" if grandchild.is_file() else \"📁\"\n",
    "                            suffix = \"\" if grandchild.is_file() else \"/\"\n",
    "                            print(f\"{child_prefix}{'└── ' if is_last_grand else '├── '}{icon} {grandchild.name}{suffix}\")\n",
    "                else:\n",
    "                    file_size = child.stat().st_size\n",
    "                    size_str = f\" ({file_size} bytes)\" if file_size < 1024 else f\" ({file_size/1024:.1f} KB)\"\n",
    "                    print(f\"📄 {child.name}{size_str}\")\n",
    "        except PermissionError:\n",
    "            print(f\"{prefix}    ❌ 權限拒絕\")\n",
    "\n",
    "print(\"🏗️ 企業級模型倉庫實際結構:\")\n",
    "print(\"=\"*80)\n",
    "show_directory_tree(repo.base_path)\n",
    "\n",
    "# 顯示配置檔案範例\n",
    "print(\"\\n📋 配置檔案範例 (recommendation/user_embedding/1.0/config.pbtxt):\")\n",
    "print(\"=\"*80)\n",
    "config_path = repo.base_path / \"recommendation\" / \"user_embedding\" / \"1.0\" / \"config.pbtxt\"\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_content = f.read()\n",
    "        print(config_content)\n",
    "else:\n",
    "    print(\"❌ 配置檔案不存在\")\n",
    "\n",
    "# 顯示元數據檔案\n",
    "print(\"\\n📊 倉庫元數據 (repository_metadata.json):\")\n",
    "print(\"=\"*80)\n",
    "if repo.metadata_file.exists():\n",
    "    with open(repo.metadata_file, 'r', encoding='utf-8') as f:\n",
    "        metadata_content = f.read()\n",
    "        # 只顯示前500字元避免過長\n",
    "        if len(metadata_content) > 500:\n",
    "            print(metadata_content[:500] + \"\\n... (內容過長，已截斷)\")\n",
    "        else:\n",
    "            print(metadata_content)\n",
    "else:\n",
    "    print(\"❌ 元數據檔案不存在\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 實驗總結與下一步\n",
    "\n",
    "### 🎯 本實驗完成的學習目標\n",
    "\n",
    "✅ **企業級模型倉庫架構設計**\n",
    "- 建立了分層式的模型組織結構\n",
    "- 實現了完整的模型元數據管理\n",
    "- 設計了資源分配與隔離策略\n",
    "\n",
    "✅ **模型依賴關係管理**\n",
    "- 實現了依賴關係驗證機制\n",
    "- 建立了智能部署順序規劃\n",
    "- 提供了循環依賴檢測功能\n",
    "\n",
    "✅ **Netflix 級別的實際案例**\n",
    "- 模擬了推薦系統多模型架構\n",
    "- 展示了搜索領域模型整合\n",
    "- 實現了企業級 SLA 管理\n",
    "\n",
    "### 🚀 核心技術成果\n",
    "\n",
    "1. **EnterpriseModelRepository**: 企業級模型倉庫管理系統\n",
    "2. **DeploymentPlanner**: 智能部署順序規劃器\n",
    "3. **RepositoryValidator**: 全面的倉庫健康檢查工具\n",
    "4. **自動化配置生成**: Triton 配置檔案自動生成\n",
    "5. **部署腳本生成**: 完整的自動化部署流程\n",
    "\n",
    "### 📊 企業級特性\n",
    "\n",
    "- **資源管理**: GPU/CPU 資源分配與衝突檢測\n",
    "- **SLA 監控**: 延遲、吞吐量、可用性要求管理\n",
    "- **依賴管理**: 拓撲排序確保正確部署順序\n",
    "- **健康檢查**: 多層次的倉庫驗證機制\n",
    "- **企業規範**: 命名規範、版本控制等最佳實踐\n",
    "\n",
    "### 🎓 下一步學習路徑\n",
    "\n",
    "準備好進入 **Lab-2.2.2: A/B 測試與版本控制**，我們將學習：\n",
    "- 實現智能流量分配機制\n",
    "- 建立統計顯著性測試\n",
    "- 掌握漸進式部署策略 (Canary/Blue-Green)\n",
    "- 設計模型性能比較框架\n",
    "\n",
    "### 💡 延伸思考\n",
    "\n",
    "1. 如何在現有企業環境中逐步導入這套模型管理架構？\n",
    "2. 面對 100+ 模型的超大規模場景，需要哪些額外的設計考量？\n",
    "3. 如何整合現有的 MLOps 工具鏈 (Kubeflow, MLflow 等)？\n",
    "4. 在多雲環境下，如何確保模型倉庫的一致性和可移植性？\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 恭喜完成企業級多模型倉庫架構設計！您已經掌握了 Netflix 級別的模型管理能力！**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}