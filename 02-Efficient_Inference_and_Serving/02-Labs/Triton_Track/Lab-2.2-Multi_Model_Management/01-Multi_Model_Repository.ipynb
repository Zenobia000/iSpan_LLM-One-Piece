{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.2.1: ä¼æ¥­ç´šå¤šæ¨¡å‹å€‰åº«æ¶æ§‹è¨­è¨ˆ\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "- ç†è§£ä¼æ¥­ç´šå¤šæ¨¡å‹å€‰åº«çš„è¨­è¨ˆåŸå‰‡\n",
    "- å¯¦ç¾æ¨¡å‹é–“ä¾è³´é—œä¿‚ç®¡ç†\n",
    "- æŒæ¡è³‡æºåˆ†é…èˆ‡éš”é›¢ç­–ç•¥\n",
    "- å»ºç«‹ä¸¦ç™¼éƒ¨ç½²èˆ‡ç‰ˆæœ¬è¡çªè™•ç†æ©Ÿåˆ¶\n",
    "\n",
    "## ğŸ¢ ä¼æ¥­æ¡ˆä¾‹: Netflix æ¨è–¦ç³»çµ±å¤šæ¨¡å‹æ¶æ§‹\n",
    "\n",
    "Netflix åŒæ™‚éƒ¨ç½²è¶…é 20 å€‹æ¨¡å‹ï¼š\n",
    "- **æ¨è–¦æ¨¡å‹**: å€‹äººåŒ–æ¨è–¦ã€ç›¸ä¼¼å…§å®¹æ¨è–¦\n",
    "- **æœç´¢æ¨¡å‹**: æŸ¥è©¢ç†è§£ã€å…§å®¹æª¢ç´¢\n",
    "- **å…§å®¹æ¨¡å‹**: ç¸®åœ–ç”Ÿæˆã€å­—å¹•ç¿»è­¯\n",
    "- **åˆ†ææ¨¡å‹**: è§€çœ‹è¡Œç‚ºåˆ†æã€æµå¤±é æ¸¬\n",
    "\n",
    "æ¯å€‹æ¨¡å‹éƒ½æœ‰å¤šå€‹ç‰ˆæœ¬åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­é‹è¡Œï¼Œéœ€è¦çµ±ä¸€çš„ç®¡ç†æ¶æ§‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç’°å¢ƒæº–å‚™èˆ‡ä¾è³´æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# è¨­å®šæ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ğŸš€ ä¼æ¥­ç´šå¤šæ¨¡å‹å€‰åº«è¨­è¨ˆ - ç’°å¢ƒæª¢æŸ¥\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"å·¥ä½œç›®éŒ„: {os.getcwd()}\")\n",
    "\n",
    "# æª¢æŸ¥ Triton ç›¸é—œå·¥å…·\n",
    "def check_triton_tools():\n",
    "    tools = {\n",
    "        'curl': 'HTTP å®¢æˆ¶ç«¯æ¸¬è©¦',\n",
    "        'docker': 'Triton å®¹å™¨ç®¡ç†',\n",
    "        'nvidia-smi': 'GPU ç›£æ§',\n",
    "    }\n",
    "    \n",
    "    for tool, desc in tools.items():\n",
    "        try:\n",
    "            result = subprocess.run([tool, '--version'], \n",
    "                                  capture_output=True, text=True, timeout=5)\n",
    "            print(f\"âœ… {tool}: {desc} - å¯ç”¨\")\n",
    "        except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "            print(f\"âš ï¸ {tool}: {desc} - æœªå®‰è£æˆ–ä¸å¯ç”¨\")\n",
    "\n",
    "check_triton_tools()\n",
    "print(\"\\nâœ… ç’°å¢ƒæª¢æŸ¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ ä¼æ¥­ç´šæ¨¡å‹å€‰åº«æ¶æ§‹è¨­è¨ˆ\n",
    "\n",
    "### 1. æ¨¡å‹åˆ†é¡èˆ‡çµ„ç¹”ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelMetadata:\n",
    "    \"\"\"æ¨¡å‹å…ƒæ•¸æ“šå®šç¾©\"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "    model_type: str  # 'classification', 'generation', 'embedding', 'ensemble'\n",
    "    framework: str   # 'pytorch', 'tensorflow', 'onnx', 'python'\n",
    "    domain: str      # 'recommendation', 'search', 'content', 'analytics'\n",
    "    owner_team: str\n",
    "    description: str\n",
    "    created_at: str\n",
    "    dependencies: List[str] = None\n",
    "    resource_requirements: Dict[str, Any] = None\n",
    "    sla_requirements: Dict[str, Any] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.dependencies is None:\n",
    "            self.dependencies = []\n",
    "        if self.resource_requirements is None:\n",
    "            self.resource_requirements = {\n",
    "                'memory_mb': 2048,\n",
    "                'gpu_memory_mb': 4096,\n",
    "                'cpu_cores': 2,\n",
    "                'max_batch_size': 32\n",
    "            }\n",
    "        if self.sla_requirements is None:\n",
    "            self.sla_requirements = {\n",
    "                'max_latency_p99_ms': 100,\n",
    "                'min_throughput_rps': 10,\n",
    "                'availability': 0.999\n",
    "            }\n",
    "\n",
    "class EnterpriseModelRepository:\n",
    "    \"\"\"ä¼æ¥­ç´šæ¨¡å‹å€‰åº«ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"./enterprise_model_repository\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.metadata_file = self.base_path / \"repository_metadata.json\"\n",
    "        self.models = {}\n",
    "        self._initialize_repository()\n",
    "    \n",
    "    def _initialize_repository(self):\n",
    "        \"\"\"åˆå§‹åŒ–å€‰åº«çµæ§‹\"\"\"\n",
    "        self.base_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # å‰µå»ºä¼æ¥­ç´šç›®éŒ„çµæ§‹\n",
    "        domains = ['recommendation', 'search', 'content', 'analytics', 'shared']\n",
    "        for domain in domains:\n",
    "            (self.base_path / domain).mkdir(exist_ok=True)\n",
    "        \n",
    "        # è¼‰å…¥ç¾æœ‰å…ƒæ•¸æ“š\n",
    "        if self.metadata_file.exists():\n",
    "            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                self.models = {k: ModelMetadata(**v) for k, v in data.items()}\n",
    "        \n",
    "        logger.info(f\"ä¼æ¥­æ¨¡å‹å€‰åº«åˆå§‹åŒ–å®Œæˆ: {self.base_path}\")\n",
    "    \n",
    "    def register_model(self, metadata: ModelMetadata) -> bool:\n",
    "        \"\"\"è¨»å†Šæ–°æ¨¡å‹åˆ°å€‰åº«\"\"\"\n",
    "        model_id = f\"{metadata.domain}/{metadata.name}\"\n",
    "        \n",
    "        # æª¢æŸ¥ä¾è³´é—œä¿‚\n",
    "        if not self._validate_dependencies(metadata.dependencies):\n",
    "            logger.error(f\"æ¨¡å‹ {model_id} ä¾è³´æª¢æŸ¥å¤±æ•—\")\n",
    "            return False\n",
    "        \n",
    "        # æª¢æŸ¥è³‡æºè¡çª\n",
    "        if not self._check_resource_conflicts(metadata):\n",
    "            logger.warning(f\"æ¨¡å‹ {model_id} å¯èƒ½å­˜åœ¨è³‡æºè¡çª\")\n",
    "        \n",
    "        # å‰µå»ºæ¨¡å‹ç›®éŒ„çµæ§‹\n",
    "        model_path = self.base_path / metadata.domain / metadata.name\n",
    "        version_path = model_path / metadata.version\n",
    "        version_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # ç”Ÿæˆ Triton é…ç½®\n",
    "        config = self._generate_triton_config(metadata)\n",
    "        config_path = version_path / \"config.pbtxt\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            f.write(config)\n",
    "        \n",
    "        # æ›´æ–°å…ƒæ•¸æ“š\n",
    "        self.models[model_id] = metadata\n",
    "        self._save_metadata()\n",
    "        \n",
    "        logger.info(f\"âœ… æ¨¡å‹ {model_id} v{metadata.version} è¨»å†ŠæˆåŠŸ\")\n",
    "        return True\n",
    "    \n",
    "    def _validate_dependencies(self, dependencies: List[str]) -> bool:\n",
    "        \"\"\"é©—è­‰æ¨¡å‹ä¾è³´é—œä¿‚\"\"\"\n",
    "        for dep in dependencies:\n",
    "            if dep not in self.models:\n",
    "                logger.error(f\"ä¾è³´æ¨¡å‹ {dep} ä¸å­˜åœ¨\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _check_resource_conflicts(self, metadata: ModelMetadata) -> bool:\n",
    "        \"\"\"æª¢æŸ¥è³‡æºè¡çª\"\"\"\n",
    "        total_gpu_memory = sum(\n",
    "            model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "            for model in self.models.values()\n",
    "        )\n",
    "        \n",
    "        new_gpu_memory = metadata.resource_requirements.get('gpu_memory_mb', 0)\n",
    "        \n",
    "        # å‡è¨­ç¸½ GPU è¨˜æ†¶é«”ç‚º 24GB\n",
    "        max_gpu_memory = 24 * 1024  # MB\n",
    "        \n",
    "        if total_gpu_memory + new_gpu_memory > max_gpu_memory:\n",
    "            logger.warning(f\"GPU è¨˜æ†¶é«”å¯èƒ½ä¸è¶³: {total_gpu_memory + new_gpu_memory}MB > {max_gpu_memory}MB\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _generate_triton_config(self, metadata: ModelMetadata) -> str:\n",
    "        \"\"\"ç”Ÿæˆ Triton é…ç½®æª”æ¡ˆ\"\"\"\n",
    "        backend = {\n",
    "            'pytorch': 'pytorch',\n",
    "            'tensorflow': 'tensorflow',\n",
    "            'onnx': 'onnxruntime',\n",
    "            'python': 'python'\n",
    "        }.get(metadata.framework, 'python')\n",
    "        \n",
    "        config = f'''\n",
    "name: \"{metadata.name}\"\n",
    "backend: \"{backend}\"\n",
    "max_batch_size: {metadata.resource_requirements['max_batch_size']}\n",
    "\n",
    "# è¼¸å…¥é…ç½® (ç¤ºä¾‹)\n",
    "input [\n",
    "  {{\n",
    "    name: \"INPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "# è¼¸å‡ºé…ç½® (ç¤ºä¾‹)\n",
    "output [\n",
    "  {{\n",
    "    name: \"OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "# å‹•æ…‹æ‰¹è™•ç†é…ç½®\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: {metadata.sla_requirements['max_latency_p99_ms'] * 1000}\n",
    "}}\n",
    "\n",
    "# å¯¦ä¾‹ç¾¤çµ„é…ç½®\n",
    "instance_group [\n",
    "  {{\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus: [ 0 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "# æ¨¡å‹å…ƒæ•¸æ“š\n",
    "parameters: {{\n",
    "  key: \"domain\"\n",
    "  value: {{ string_value: \"{metadata.domain}\" }}\n",
    "}}\n",
    "parameters: {{\n",
    "  key: \"owner_team\"\n",
    "  value: {{ string_value: \"{metadata.owner_team}\" }}\n",
    "}}\n",
    "parameters: {{\n",
    "  key: \"sla_latency_p99_ms\"\n",
    "  value: {{ string_value: \"{metadata.sla_requirements['max_latency_p99_ms']}\" }}\n",
    "}}\n",
    "'''.strip()\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"ä¿å­˜å…ƒæ•¸æ“šåˆ°æª”æ¡ˆ\"\"\"\n",
    "        data = {k: asdict(v) for k, v in self.models.items()}\n",
    "        with open(self.metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def list_models(self, domain: Optional[str] = None) -> List[ModelMetadata]:\n",
    "        \"\"\"åˆ—å‡ºæ¨¡å‹\"\"\"\n",
    "        if domain:\n",
    "            return [model for model_id, model in self.models.items() \n",
    "                   if model.domain == domain]\n",
    "        return list(self.models.values())\n",
    "    \n",
    "    def get_model_dependencies(self, model_id: str) -> List[str]:\n",
    "        \"\"\"ç²å–æ¨¡å‹ä¾è³´åœ–\"\"\"\n",
    "        if model_id not in self.models:\n",
    "            return []\n",
    "        \n",
    "        def get_deps(mid: str, visited: set) -> List[str]:\n",
    "            if mid in visited:\n",
    "                return []  # é¿å…å¾ªç’°ä¾è³´\n",
    "            \n",
    "            visited.add(mid)\n",
    "            deps = []\n",
    "            \n",
    "            if mid in self.models:\n",
    "                for dep in self.models[mid].dependencies:\n",
    "                    deps.append(dep)\n",
    "                    deps.extend(get_deps(dep, visited.copy()))\n",
    "            \n",
    "            return deps\n",
    "        \n",
    "        return get_deps(model_id, set())\n",
    "\n",
    "# åˆå§‹åŒ–ä¼æ¥­æ¨¡å‹å€‰åº«\n",
    "repo = EnterpriseModelRepository()\n",
    "print(\"\\nâœ… ä¼æ¥­ç´šæ¨¡å‹å€‰åº«åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Netflix æ¡ˆä¾‹ï¼šå»ºç«‹æ¨è–¦ç³»çµ±æ¨¡å‹ç¾¤çµ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾© Netflix æ¨è–¦ç³»çµ±æ¨¡å‹ç¾¤çµ„\n",
    "netflix_models = [\n",
    "    ModelMetadata(\n",
    "        name=\"user_embedding\",\n",
    "        version=\"1.0\",\n",
    "        model_type=\"embedding\",\n",
    "        framework=\"pytorch\",\n",
    "        domain=\"recommendation\",\n",
    "        owner_team=\"recommendation_platform\",\n",
    "        description=\"ç”¨æˆ¶å‘é‡åŒ–æ¨¡å‹ï¼Œå°‡ç”¨æˆ¶è¡Œç‚ºè½‰æ›ç‚ºç¨ å¯†å‘é‡\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 4096,\n",
    "            'gpu_memory_mb': 6144,\n",
    "            'cpu_cores': 4,\n",
    "            'max_batch_size': 128\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 50,\n",
    "            'min_throughput_rps': 100,\n",
    "            'availability': 0.9999\n",
    "        }\n",
    "    ),\n",
    "    ModelMetadata(\n",
    "        name=\"content_embedding\",\n",
    "        version=\"2.1\",\n",
    "        model_type=\"embedding\",\n",
    "        framework=\"pytorch\",\n",
    "        domain=\"recommendation\",\n",
    "        owner_team=\"content_intelligence\",\n",
    "        description=\"å…§å®¹å‘é‡åŒ–æ¨¡å‹ï¼Œæå–å½±è¦–å…§å®¹ç‰¹å¾µ\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 6144,\n",
    "            'gpu_memory_mb': 8192,\n",
    "            'cpu_cores': 6,\n",
    "            'max_batch_size': 64\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 75,\n",
    "            'min_throughput_rps': 50,\n",
    "            'availability': 0.9995\n",
    "        }\n",
    "    ),\n",
    "    ModelMetadata(\n",
    "        name=\"recommendation_ranker\",\n",
    "        version=\"3.0\",\n",
    "        model_type=\"classification\",\n",
    "        framework=\"pytorch\",\n",
    "        domain=\"recommendation\",\n",
    "        owner_team=\"recommendation_platform\",\n",
    "        description=\"æ¨è–¦æ’åºæ¨¡å‹ï¼ŒåŸºæ–¼ç”¨æˆ¶å’Œå…§å®¹ç‰¹å¾µé€²è¡Œå€‹äººåŒ–æ’åº\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[\"recommendation/user_embedding\", \"recommendation/content_embedding\"],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 8192,\n",
    "            'gpu_memory_mb': 12288,\n",
    "            'cpu_cores': 8,\n",
    "            'max_batch_size': 32\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 100,\n",
    "            'min_throughput_rps': 25,\n",
    "            'availability': 0.9999\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# è¨»å†Šæ¨¡å‹åˆ°å€‰åº«\n",
    "print(\"ğŸ“Š è¨»å†Š Netflix æ¨è–¦ç³»çµ±æ¨¡å‹ç¾¤çµ„...\")\n",
    "for model in netflix_models:\n",
    "    success = repo.register_model(model)\n",
    "    if success:\n",
    "        print(f\"âœ… {model.domain}/{model.name} v{model.version} è¨»å†ŠæˆåŠŸ\")\n",
    "    else:\n",
    "        print(f\"âŒ {model.domain}/{model.name} v{model.version} è¨»å†Šå¤±æ•—\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ å€‰åº«çµ±è¨ˆ:\")\n",
    "print(f\"- ç¸½æ¨¡å‹æ•¸é‡: {len(repo.models)}\")\n",
    "print(f\"- æ¨è–¦é ˜åŸŸæ¨¡å‹: {len(repo.list_models('recommendation'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. æœç´¢é ˜åŸŸæ¨¡å‹æ·»åŠ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·»åŠ æœç´¢é ˜åŸŸæ¨¡å‹\n",
    "search_models = [\n",
    "    ModelMetadata(\n",
    "        name=\"query_understanding\",\n",
    "        version=\"1.5\",\n",
    "        model_type=\"classification\",\n",
    "        framework=\"pytorch\",\n",
    "        domain=\"search\",\n",
    "        owner_team=\"search_experience\",\n",
    "        description=\"æŸ¥è©¢ç†è§£æ¨¡å‹ï¼Œè§£æç”¨æˆ¶æœç´¢æ„åœ–å’Œå¯¦é«”è­˜åˆ¥\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 3072,\n",
    "            'gpu_memory_mb': 4096,\n",
    "            'cpu_cores': 4,\n",
    "            'max_batch_size': 64\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 80,\n",
    "            'min_throughput_rps': 200,\n",
    "            'availability': 0.9995\n",
    "        }\n",
    "    ),\n",
    "    ModelMetadata(\n",
    "        name=\"content_retrieval\",\n",
    "        version=\"2.0\",\n",
    "        model_type=\"embedding\",\n",
    "        framework=\"onnx\",\n",
    "        domain=\"search\",\n",
    "        owner_team=\"search_experience\",\n",
    "        description=\"å…§å®¹æª¢ç´¢æ¨¡å‹ï¼ŒåŸºæ–¼èªç¾©ç›¸ä¼¼åº¦æª¢ç´¢ç›¸é—œå…§å®¹\",\n",
    "        created_at=datetime.now().isoformat(),\n",
    "        dependencies=[\"search/query_understanding\"],\n",
    "        resource_requirements={\n",
    "            'memory_mb': 2048,\n",
    "            'gpu_memory_mb': 3072,\n",
    "            'cpu_cores': 2,\n",
    "            'max_batch_size': 128\n",
    "        },\n",
    "        sla_requirements={\n",
    "            'max_latency_p99_ms': 60,\n",
    "            'min_throughput_rps': 150,\n",
    "            'availability': 0.999\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# è¨»å†Šæœç´¢æ¨¡å‹\n",
    "print(\"ğŸ” è¨»å†Šæœç´¢é ˜åŸŸæ¨¡å‹...\")\n",
    "for model in search_models:\n",
    "    success = repo.register_model(model)\n",
    "    if success:\n",
    "        print(f\"âœ… {model.domain}/{model.name} v{model.version} è¨»å†ŠæˆåŠŸ\")\n",
    "    else:\n",
    "        print(f\"âŒ {model.domain}/{model.name} v{model.version} è¨»å†Šå¤±æ•—\")\n",
    "\n",
    "# é©—è­‰ä¾è³´é—œä¿‚\n",
    "print(\"\\nğŸ”— ä¾è³´é—œä¿‚åˆ†æ:\")\n",
    "for model_id in repo.models.keys():\n",
    "    deps = repo.get_model_dependencies(model_id)\n",
    "    if deps:\n",
    "        print(f\"- {model_id}: ä¾è³´ {', '.join(deps)}\")\n",
    "    else:\n",
    "        print(f\"- {model_id}: ç„¡ä¾è³´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š å€‰åº«çµæ§‹è¦–è¦ºåŒ–èˆ‡åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_repository_structure():\n",
    "    \"\"\"è¦–è¦ºåŒ–å€‰åº«çµæ§‹\"\"\"\n",
    "    print(\"ğŸ—ï¸ ä¼æ¥­ç´šæ¨¡å‹å€‰åº«çµæ§‹:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æŒ‰é ˜åŸŸåˆ†çµ„é¡¯ç¤º\n",
    "    domains = {}\n",
    "    for model_id, model in repo.models.items():\n",
    "        if model.domain not in domains:\n",
    "            domains[model.domain] = []\n",
    "        domains[model.domain].append(model)\n",
    "    \n",
    "    for domain, models in domains.items():\n",
    "        print(f\"\\nğŸ“ {domain.upper()} é ˜åŸŸ ({len(models)} å€‹æ¨¡å‹)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for model in models:\n",
    "            deps_info = f\" (ä¾è³´: {len(model.dependencies)} å€‹)\" if model.dependencies else \" (ç„¡ä¾è³´)\"\n",
    "            gpu_mb = model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "            latency = model.sla_requirements.get('max_latency_p99_ms', 0)\n",
    "            \n",
    "            print(f\"  ğŸ“¦ {model.name} v{model.version}\")\n",
    "            print(f\"     æ¡†æ¶: {model.framework} | GPU: {gpu_mb}MB | å»¶é²: {latency}ms{deps_info}\")\n",
    "            print(f\"     åœ˜éšŠ: {model.owner_team}\")\n",
    "            print(f\"     æè¿°: {model.description[:50]}...\")\n",
    "            print()\n",
    "\n",
    "def analyze_resource_usage():\n",
    "    \"\"\"åˆ†æè³‡æºä½¿ç”¨æƒ…æ³\"\"\"\n",
    "    print(\"\\nğŸ“ˆ è³‡æºä½¿ç”¨åˆ†æ:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_memory = 0\n",
    "    total_gpu_memory = 0\n",
    "    total_cpu_cores = 0\n",
    "    \n",
    "    framework_count = {}\n",
    "    domain_resources = {}\n",
    "    \n",
    "    for model in repo.models.values():\n",
    "        # ç´¯è¨ˆè³‡æº\n",
    "        total_memory += model.resource_requirements.get('memory_mb', 0)\n",
    "        total_gpu_memory += model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "        total_cpu_cores += model.resource_requirements.get('cpu_cores', 0)\n",
    "        \n",
    "        # æ¡†æ¶çµ±è¨ˆ\n",
    "        framework_count[model.framework] = framework_count.get(model.framework, 0) + 1\n",
    "        \n",
    "        # é ˜åŸŸè³‡æºçµ±è¨ˆ\n",
    "        if model.domain not in domain_resources:\n",
    "            domain_resources[model.domain] = {'memory': 0, 'gpu_memory': 0, 'models': 0}\n",
    "        \n",
    "        domain_resources[model.domain]['memory'] += model.resource_requirements.get('memory_mb', 0)\n",
    "        domain_resources[model.domain]['gpu_memory'] += model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "        domain_resources[model.domain]['models'] += 1\n",
    "    \n",
    "    print(f\"ğŸ’¾ ç¸½è¨˜æ†¶é«”éœ€æ±‚: {total_memory/1024:.1f} GB\")\n",
    "    print(f\"ğŸ® ç¸½ GPU è¨˜æ†¶é«”éœ€æ±‚: {total_gpu_memory/1024:.1f} GB\")\n",
    "    print(f\"âš¡ ç¸½ CPU æ ¸å¿ƒéœ€æ±‚: {total_cpu_cores} æ ¸\")\n",
    "    \n",
    "    print(f\"\\nğŸ”§ æ¡†æ¶åˆ†ä½ˆ:\")\n",
    "    for framework, count in framework_count.items():\n",
    "        percentage = (count / len(repo.models)) * 100\n",
    "        print(f\"  - {framework}: {count} å€‹æ¨¡å‹ ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š é ˜åŸŸè³‡æºåˆ†ä½ˆ:\")\n",
    "    for domain, resources in domain_resources.items():\n",
    "        print(f\"  - {domain}:\")\n",
    "        print(f\"    æ¨¡å‹æ•¸é‡: {resources['models']}\")\n",
    "        print(f\"    è¨˜æ†¶é«”: {resources['memory']/1024:.1f} GB\")\n",
    "        print(f\"    GPU è¨˜æ†¶é«”: {resources['gpu_memory']/1024:.1f} GB\")\n",
    "\n",
    "def analyze_sla_requirements():\n",
    "    \"\"\"åˆ†æ SLA éœ€æ±‚\"\"\"\n",
    "    print(f\"\\nâ±ï¸ SLA éœ€æ±‚åˆ†æ:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "    availabilities = []\n",
    "    \n",
    "    print(\"æ¨¡å‹åç¨±\\t\\tå»¶é²(ms)\\tååé‡(RPS)\\tå¯ç”¨æ€§\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model in repo.models.values():\n",
    "        latency = model.sla_requirements.get('max_latency_p99_ms', 0)\n",
    "        throughput = model.sla_requirements.get('min_throughput_rps', 0)\n",
    "        availability = model.sla_requirements.get('availability', 0)\n",
    "        \n",
    "        latencies.append(latency)\n",
    "        throughputs.append(throughput)\n",
    "        availabilities.append(availability)\n",
    "        \n",
    "        print(f\"{model.name[:15]:<15}\\t{latency:<8}\\t{throughput:<12}\\t{availability*100:.2f}%\")\n",
    "    \n",
    "    if latencies:\n",
    "        print(f\"\\nğŸ“Š çµ±è¨ˆæ‘˜è¦:\")\n",
    "        print(f\"  å¹³å‡å»¶é²: {sum(latencies)/len(latencies):.1f} ms\")\n",
    "        print(f\"  å¹³å‡ååé‡: {sum(throughputs)/len(throughputs):.1f} RPS\")\n",
    "        print(f\"  å¹³å‡å¯ç”¨æ€§: {(sum(availabilities)/len(availabilities))*100:.3f}%\")\n",
    "        print(f\"  æœ€åš´æ ¼å»¶é²è¦æ±‚: {min(latencies)} ms\")\n",
    "        print(f\"  æœ€é«˜ååé‡è¦æ±‚: {max(throughputs)} RPS\")\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "visualize_repository_structure()\n",
    "analyze_resource_usage()\n",
    "analyze_sla_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ¨¡å‹éƒ¨ç½²é †åºè¦åŠƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentPlanner:\n",
    "    \"\"\"æ¨¡å‹éƒ¨ç½²é †åºè¦åŠƒå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, repository: EnterpriseModelRepository):\n",
    "        self.repo = repository\n",
    "    \n",
    "    def create_deployment_plan(self) -> List[List[str]]:\n",
    "        \"\"\"å‰µå»ºéƒ¨ç½²è¨ˆåŠƒ - è€ƒæ…®ä¾è³´é—œä¿‚çš„æ‹“æ’²æ’åº\"\"\"\n",
    "        # å»ºç«‹ä¾è³´åœ–\n",
    "        graph = {}\n",
    "        in_degree = {}\n",
    "        \n",
    "        for model_id in self.repo.models.keys():\n",
    "            graph[model_id] = []\n",
    "            in_degree[model_id] = 0\n",
    "        \n",
    "        # å»ºç«‹é‚Šå’Œå…¥åº¦\n",
    "        for model_id, model in self.repo.models.items():\n",
    "            for dep in model.dependencies:\n",
    "                if dep in graph:\n",
    "                    graph[dep].append(model_id)\n",
    "                    in_degree[model_id] += 1\n",
    "        \n",
    "        # Kahn's ç®—æ³•é€²è¡Œæ‹“æ’²æ’åº\n",
    "        deployment_waves = []\n",
    "        queue = [model_id for model_id, degree in in_degree.items() if degree == 0]\n",
    "        \n",
    "        while queue:\n",
    "            current_wave = queue.copy()\n",
    "            deployment_waves.append(current_wave)\n",
    "            queue.clear()\n",
    "            \n",
    "            for model_id in current_wave:\n",
    "                for neighbor in graph[model_id]:\n",
    "                    in_degree[neighbor] -= 1\n",
    "                    if in_degree[neighbor] == 0:\n",
    "                        queue.append(neighbor)\n",
    "        \n",
    "        return deployment_waves\n",
    "    \n",
    "    def estimate_deployment_time(self, deployment_waves: List[List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"ä¼°ç®—éƒ¨ç½²æ™‚é–“\"\"\"\n",
    "        # å‡è¨­éƒ¨ç½²æ™‚é–“èˆ‡æ¨¡å‹å¤§å°ç›¸é—œ\n",
    "        base_deployment_time = 2  # åŸºç¤éƒ¨ç½²æ™‚é–“ï¼ˆåˆ†é˜ï¼‰\n",
    "        \n",
    "        wave_times = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for i, wave in enumerate(deployment_waves):\n",
    "            wave_time = 0\n",
    "            for model_id in wave:\n",
    "                model = self.repo.models[model_id]\n",
    "                # æ ¹æ“š GPU è¨˜æ†¶é«”éœ€æ±‚ä¼°ç®—éƒ¨ç½²æ™‚é–“\n",
    "                gpu_memory_gb = model.resource_requirements.get('gpu_memory_mb', 0) / 1024\n",
    "                deployment_time = base_deployment_time + (gpu_memory_gb * 0.5)\n",
    "                wave_time = max(wave_time, deployment_time)  # ä¸¦è¡Œéƒ¨ç½²å–æœ€é•·æ™‚é–“\n",
    "            \n",
    "            wave_times.append(wave_time)\n",
    "            total_time += wave_time\n",
    "        \n",
    "        return {\n",
    "            'total_time_minutes': total_time,\n",
    "            'wave_times': wave_times,\n",
    "            'total_waves': len(deployment_waves)\n",
    "        }\n",
    "    \n",
    "    def generate_deployment_script(self, deployment_waves: List[List[str]]) -> str:\n",
    "        \"\"\"ç”Ÿæˆéƒ¨ç½²è…³æœ¬\"\"\"\n",
    "        script_lines = [\n",
    "            \"#!/bin/bash\",\n",
    "            \"# ä¼æ¥­ç´šå¤šæ¨¡å‹éƒ¨ç½²è…³æœ¬\",\n",
    "            \"# è‡ªå‹•ç”Ÿæˆ - è«‹å‹¿æ‰‹å‹•ä¿®æ”¹\",\n",
    "            \"\",\n",
    "            \"set -e  # ç™¼ç”ŸéŒ¯èª¤æ™‚ç«‹å³é€€å‡º\",\n",
    "            \"\",\n",
    "            \"echo 'ğŸš€ é–‹å§‹ä¼æ¥­ç´šå¤šæ¨¡å‹éƒ¨ç½²'\",\n",
    "            \"echo '==============================================='\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        for i, wave in enumerate(deployment_waves, 1):\n",
    "            script_lines.extend([\n",
    "                f\"echo 'ğŸ“¦ ç¬¬ {i} æ³¢éƒ¨ç½²: {len(wave)} å€‹æ¨¡å‹'\",\n",
    "                \"echo '-----------------------------------------------'\",\n",
    "                \"\"\n",
    "            ])\n",
    "            \n",
    "            # ä¸¦è¡Œéƒ¨ç½²åŒä¸€æ³¢çš„æ¨¡å‹\n",
    "            for model_id in wave:\n",
    "                domain, name = model_id.split('/')\n",
    "                model = self.repo.models[model_id]\n",
    "                \n",
    "                script_lines.extend([\n",
    "                    f\"echo '  ğŸ”„ éƒ¨ç½² {model_id} v{model.version}'\",\n",
    "                    f\"# å¥åº·æª¢æŸ¥\",\n",
    "                    f\"curl -f http://localhost:8000/v2/models/{name}/ready || {{\",\n",
    "                    f\"  echo 'âŒ æ¨¡å‹ {name} éƒ¨ç½²å¤±æ•—'\",\n",
    "                    f\"  exit 1\",\n",
    "                    f\"}}\",\n",
    "                    f\"echo 'âœ… æ¨¡å‹ {name} éƒ¨ç½²æˆåŠŸ'\",\n",
    "                    \"\"\n",
    "                ])\n",
    "            \n",
    "            if i < len(deployment_waves):\n",
    "                script_lines.extend([\n",
    "                    \"echo 'â³ ç­‰å¾…ç•¶å‰æ³¢éƒ¨ç½²å®Œæˆ...'\",\n",
    "                    \"sleep 30\",\n",
    "                    \"\"\n",
    "                ])\n",
    "        \n",
    "        script_lines.extend([\n",
    "            \"echo 'âœ… æ‰€æœ‰æ¨¡å‹éƒ¨ç½²å®Œæˆï¼'\",\n",
    "            \"echo 'ğŸ“Š æœ€çµ‚ç‹€æ…‹æª¢æŸ¥...'\",\n",
    "            \"curl -s http://localhost:8000/v2/models | jq .\",\n",
    "            \"echo 'ğŸ‰ ä¼æ¥­ç´šå¤šæ¨¡å‹å¹³å°éƒ¨ç½²æˆåŠŸï¼'\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(script_lines)\n",
    "\n",
    "# å‰µå»ºéƒ¨ç½²è¨ˆåŠƒ\n",
    "planner = DeploymentPlanner(repo)\n",
    "deployment_waves = planner.create_deployment_plan()\n",
    "time_estimate = planner.estimate_deployment_time(deployment_waves)\n",
    "\n",
    "print(\"ğŸ“… æ™ºèƒ½éƒ¨ç½²è¨ˆåŠƒ:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, wave in enumerate(deployment_waves, 1):\n",
    "    print(f\"\\nğŸŒŠ ç¬¬ {i} æ³¢éƒ¨ç½² (é ä¼° {time_estimate['wave_times'][i-1]:.1f} åˆ†é˜):\")\n",
    "    for model_id in wave:\n",
    "        model = repo.models[model_id]\n",
    "        deps_info = f\" (ä¾è³´: {', '.join(model.dependencies)})\" if model.dependencies else \" (ç„¡ä¾è³´)\"\n",
    "        print(f\"  ğŸ“¦ {model_id} v{model.version}{deps_info}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸ ç¸½éƒ¨ç½²æ™‚é–“ä¼°ç®—: {time_estimate['total_time_minutes']:.1f} åˆ†é˜\")\n",
    "print(f\"ğŸ“Š ç¸½éƒ¨ç½²æ³¢æ•¸: {time_estimate['total_waves']}\")\n",
    "\n",
    "# ç”Ÿæˆéƒ¨ç½²è…³æœ¬\n",
    "deployment_script = planner.generate_deployment_script(deployment_waves)\n",
    "script_path = repo.base_path / \"deploy_models.sh\"\n",
    "\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(deployment_script)\n",
    "\n",
    "# è¨­å®šåŸ·è¡Œæ¬Šé™\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"\\nğŸ“œ éƒ¨ç½²è…³æœ¬å·²ç”Ÿæˆ: {script_path}\")\n",
    "print(\"åŸ·è¡Œæ–¹å¼: ./deploy_models.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” å€‰åº«é©—è­‰èˆ‡å¥åº·æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepositoryValidator:\n",
    "    \"\"\"å€‰åº«é©—è­‰å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, repository: EnterpriseModelRepository):\n",
    "        self.repo = repository\n",
    "        self.issues = []\n",
    "        self.warnings = []\n",
    "    \n",
    "    def validate_all(self) -> Dict[str, Any]:\n",
    "        \"\"\"åŸ·è¡Œå®Œæ•´é©—è­‰\"\"\"\n",
    "        self.issues.clear()\n",
    "        self.warnings.clear()\n",
    "        \n",
    "        print(\"ğŸ” åŸ·è¡Œå€‰åº«å¥åº·æª¢æŸ¥...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # å„é …æª¢æŸ¥\n",
    "        self._check_dependency_cycles()\n",
    "        self._check_resource_constraints()\n",
    "        self._check_sla_consistency()\n",
    "        self._check_file_structure()\n",
    "        self._check_naming_conventions()\n",
    "        \n",
    "        return {\n",
    "            'total_issues': len(self.issues),\n",
    "            'total_warnings': len(self.warnings),\n",
    "            'issues': self.issues,\n",
    "            'warnings': self.warnings,\n",
    "            'status': 'healthy' if len(self.issues) == 0 else 'needs_attention'\n",
    "        }\n",
    "    \n",
    "    def _check_dependency_cycles(self):\n",
    "        \"\"\"æª¢æŸ¥å¾ªç’°ä¾è³´\"\"\"\n",
    "        print(\"ğŸ”„ æª¢æŸ¥å¾ªç’°ä¾è³´...\")\n",
    "        \n",
    "        def has_cycle(model_id: str, visited: set, rec_stack: set) -> bool:\n",
    "            visited.add(model_id)\n",
    "            rec_stack.add(model_id)\n",
    "            \n",
    "            if model_id in self.repo.models:\n",
    "                for dep in self.repo.models[model_id].dependencies:\n",
    "                    if dep not in visited:\n",
    "                        if has_cycle(dep, visited, rec_stack):\n",
    "                            return True\n",
    "                    elif dep in rec_stack:\n",
    "                        return True\n",
    "            \n",
    "            rec_stack.remove(model_id)\n",
    "            return False\n",
    "        \n",
    "        visited = set()\n",
    "        for model_id in self.repo.models.keys():\n",
    "            if model_id not in visited:\n",
    "                if has_cycle(model_id, visited, set()):\n",
    "                    self.issues.append(f\"æª¢æ¸¬åˆ°å¾ªç’°ä¾è³´ï¼Œæ¶‰åŠæ¨¡å‹: {model_id}\")\n",
    "        \n",
    "        if not any(\"å¾ªç’°ä¾è³´\" in issue for issue in self.issues):\n",
    "            print(\"  âœ… ç„¡å¾ªç’°ä¾è³´\")\n",
    "    \n",
    "    def _check_resource_constraints(self):\n",
    "        \"\"\"æª¢æŸ¥è³‡æºç´„æŸ\"\"\"\n",
    "        print(\"ğŸ’¾ æª¢æŸ¥è³‡æºç´„æŸ...\")\n",
    "        \n",
    "        total_gpu_memory = sum(\n",
    "            model.resource_requirements.get('gpu_memory_mb', 0)\n",
    "            for model in self.repo.models.values()\n",
    "        )\n",
    "        \n",
    "        # å‡è¨­ç³»çµ±é™åˆ¶\n",
    "        max_gpu_memory = 32 * 1024  # 32GB\n",
    "        max_cpu_cores = 64\n",
    "        \n",
    "        if total_gpu_memory > max_gpu_memory:\n",
    "            self.issues.append(\n",
    "                f\"GPU è¨˜æ†¶é«”éœ€æ±‚è¶…å‡ºé™åˆ¶: {total_gpu_memory/1024:.1f}GB > {max_gpu_memory/1024}GB\"\n",
    "            )\n",
    "        elif total_gpu_memory > max_gpu_memory * 0.8:\n",
    "            self.warnings.append(\n",
    "                f\"GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡è¼ƒé«˜: {total_gpu_memory/1024:.1f}GB (80%+ å®¹é‡)\"\n",
    "            )\n",
    "        \n",
    "        total_cpu_cores = sum(\n",
    "            model.resource_requirements.get('cpu_cores', 0)\n",
    "            for model in self.repo.models.values()\n",
    "        )\n",
    "        \n",
    "        if total_cpu_cores > max_cpu_cores:\n",
    "            self.issues.append(\n",
    "                f\"CPU æ ¸å¿ƒéœ€æ±‚è¶…å‡ºé™åˆ¶: {total_cpu_cores} > {max_cpu_cores}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"  ğŸ“Š GPU è¨˜æ†¶é«”ä½¿ç”¨: {total_gpu_memory/1024:.1f}GB / {max_gpu_memory/1024}GB\")\n",
    "        print(f\"  âš¡ CPU æ ¸å¿ƒä½¿ç”¨: {total_cpu_cores} / {max_cpu_cores}\")\n",
    "    \n",
    "    def _check_sla_consistency(self):\n",
    "        \"\"\"æª¢æŸ¥ SLA ä¸€è‡´æ€§\"\"\"\n",
    "        print(\"â±ï¸ æª¢æŸ¥ SLA ä¸€è‡´æ€§...\")\n",
    "        \n",
    "        for model_id, model in self.repo.models.items():\n",
    "            sla = model.sla_requirements\n",
    "            \n",
    "            # æª¢æŸ¥ä¾è³´éˆçš„ SLA ä¸€è‡´æ€§\n",
    "            for dep_id in model.dependencies:\n",
    "                if dep_id in self.repo.models:\n",
    "                    dep_model = self.repo.models[dep_id]\n",
    "                    dep_sla = dep_model.sla_requirements\n",
    "                    \n",
    "                    # ä¾è³´æ¨¡å‹çš„å»¶é²æ‡‰è©²æ›´åš´æ ¼\n",
    "                    if dep_sla['max_latency_p99_ms'] >= sla['max_latency_p99_ms']:\n",
    "                        self.warnings.append(\n",
    "                            f\"SLA ä¸ä¸€è‡´: {model_id} ä¾è³´ {dep_id}ï¼Œä½†ä¾è³´å»¶é²è¦æ±‚ä¸å¤ åš´æ ¼\"\n",
    "                        )\n",
    "            \n",
    "            # æª¢æŸ¥ä¸åˆç†çš„ SLA è¨­å®š\n",
    "            if sla['max_latency_p99_ms'] < 10:\n",
    "                self.warnings.append(\n",
    "                    f\"SLA å¯èƒ½éæ–¼åš´æ ¼: {model_id} å»¶é²è¦æ±‚ {sla['max_latency_p99_ms']}ms\"\n",
    "                )\n",
    "            \n",
    "            if sla['availability'] > 0.9999:\n",
    "                self.warnings.append(\n",
    "                    f\"å¯ç”¨æ€§è¦æ±‚æ¥µé«˜: {model_id} è¦æ±‚ {sla['availability']*100:.3f}% å¯ç”¨æ€§\"\n",
    "                )\n",
    "    \n",
    "    def _check_file_structure(self):\n",
    "        \"\"\"æª¢æŸ¥æª”æ¡ˆçµæ§‹\"\"\"\n",
    "        print(\"ğŸ“ æª¢æŸ¥æª”æ¡ˆçµæ§‹...\")\n",
    "        \n",
    "        for model_id, model in self.repo.models.items():\n",
    "            domain, name = model_id.split('/')\n",
    "            model_path = self.repo.base_path / domain / name / model.version\n",
    "            config_path = model_path / \"config.pbtxt\"\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                self.issues.append(f\"æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: {model_path}\")\n",
    "            elif not config_path.exists():\n",
    "                self.issues.append(f\"é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {config_path}\")\n",
    "    \n",
    "    def _check_naming_conventions(self):\n",
    "        \"\"\"æª¢æŸ¥å‘½åè¦ç¯„\"\"\"\n",
    "        print(\"ğŸ“ æª¢æŸ¥å‘½åè¦ç¯„...\")\n",
    "        \n",
    "        import re\n",
    "        \n",
    "        # æ¨¡å‹åç¨±æ‡‰è©²ä½¿ç”¨å°å¯«å’Œåº•ç·š\n",
    "        name_pattern = re.compile(r'^[a-z][a-z0-9_]*$')\n",
    "        \n",
    "        for model_id, model in self.repo.models.items():\n",
    "            if not name_pattern.match(model.name):\n",
    "                self.warnings.append(\n",
    "                    f\"å‘½åä¸è¦ç¯„: {model.name} (å»ºè­°ä½¿ç”¨å°å¯«å­—æ¯ã€æ•¸å­—å’Œåº•ç·š)\"\n",
    "                )\n",
    "            \n",
    "            # ç‰ˆæœ¬è™Ÿæ‡‰è©²éµå¾ªèªç¾©åŒ–ç‰ˆæœ¬\n",
    "            version_pattern = re.compile(r'^\\d+\\.\\d+(\\..+)?$')\n",
    "            if not version_pattern.match(model.version):\n",
    "                self.warnings.append(\n",
    "                    f\"ç‰ˆæœ¬è™Ÿä¸è¦ç¯„: {model.version} (å»ºè­°ä½¿ç”¨èªç¾©åŒ–ç‰ˆæœ¬å¦‚ 1.0, 2.1.3)\"\n",
    "                )\n",
    "\n",
    "# åŸ·è¡Œé©—è­‰\n",
    "validator = RepositoryValidator(repo)\n",
    "validation_result = validator.validate_all()\n",
    "\n",
    "print(f\"\\nğŸ“‹ é©—è­‰çµæœæ‘˜è¦:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ”´ åš´é‡å•é¡Œ: {validation_result['total_issues']} å€‹\")\n",
    "print(f\"ğŸŸ¡ è­¦å‘Š: {validation_result['total_warnings']} å€‹\")\n",
    "print(f\"ğŸ“Š æ•´é«”ç‹€æ…‹: {validation_result['status'].upper()}\")\n",
    "\n",
    "if validation_result['issues']:\n",
    "    print(f\"\\nğŸ”´ éœ€è¦ä¿®å¾©çš„å•é¡Œ:\")\n",
    "    for i, issue in enumerate(validation_result['issues'], 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "\n",
    "if validation_result['warnings']:\n",
    "    print(f\"\\nğŸŸ¡ å»ºè­°æ”¹é€²çš„è­¦å‘Š:\")\n",
    "    for i, warning in enumerate(validation_result['warnings'], 1):\n",
    "        print(f\"  {i}. {warning}\")\n",
    "\n",
    "if validation_result['status'] == 'healthy':\n",
    "    print(f\"\\nâœ… æ­å–œï¼å€‰åº«çµæ§‹å¥åº·ï¼Œå¯ä»¥é–‹å§‹éƒ¨ç½²\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ å»ºè­°ä¿®å¾©ä¸Šè¿°å•é¡Œå¾Œå†é€²è¡Œéƒ¨ç½²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ å¯¦éš›ç›®éŒ„çµæ§‹å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_directory_tree(path: Path, prefix: str = \"\", max_depth: int = 3, current_depth: int = 0):\n",
    "    \"\"\"é¡¯ç¤ºç›®éŒ„æ¨¹çµæ§‹\"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    if path.is_dir():\n",
    "        print(f\"{prefix}ğŸ“ {path.name}/\")\n",
    "        \n",
    "        try:\n",
    "            children = sorted(path.iterdir())\n",
    "            for i, child in enumerate(children):\n",
    "                is_last = i == len(children) - 1\n",
    "                child_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
    "                print(f\"{prefix}{'â””â”€â”€ ' if is_last else 'â”œâ”€â”€ '}\", end=\"\")\n",
    "                \n",
    "                if child.is_dir():\n",
    "                    print(f\"ğŸ“ {child.name}/\")\n",
    "                    if current_depth < max_depth:\n",
    "                        for j, grandchild in enumerate(sorted(child.iterdir())):\n",
    "                            is_last_grand = j == len(list(child.iterdir())) - 1\n",
    "                            grand_prefix = child_prefix + (\"    \" if is_last_grand else \"â”‚   \")\n",
    "                            icon = \"ğŸ“„\" if grandchild.is_file() else \"ğŸ“\"\n",
    "                            suffix = \"\" if grandchild.is_file() else \"/\"\n",
    "                            print(f\"{child_prefix}{'â””â”€â”€ ' if is_last_grand else 'â”œâ”€â”€ '}{icon} {grandchild.name}{suffix}\")\n",
    "                else:\n",
    "                    file_size = child.stat().st_size\n",
    "                    size_str = f\" ({file_size} bytes)\" if file_size < 1024 else f\" ({file_size/1024:.1f} KB)\"\n",
    "                    print(f\"ğŸ“„ {child.name}{size_str}\")\n",
    "        except PermissionError:\n",
    "            print(f\"{prefix}    âŒ æ¬Šé™æ‹’çµ•\")\n",
    "\n",
    "print(\"ğŸ—ï¸ ä¼æ¥­ç´šæ¨¡å‹å€‰åº«å¯¦éš›çµæ§‹:\")\n",
    "print(\"=\"*80)\n",
    "show_directory_tree(repo.base_path)\n",
    "\n",
    "# é¡¯ç¤ºé…ç½®æª”æ¡ˆç¯„ä¾‹\n",
    "print(\"\\nğŸ“‹ é…ç½®æª”æ¡ˆç¯„ä¾‹ (recommendation/user_embedding/1.0/config.pbtxt):\")\n",
    "print(\"=\"*80)\n",
    "config_path = repo.base_path / \"recommendation\" / \"user_embedding\" / \"1.0\" / \"config.pbtxt\"\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_content = f.read()\n",
    "        print(config_content)\n",
    "else:\n",
    "    print(\"âŒ é…ç½®æª”æ¡ˆä¸å­˜åœ¨\")\n",
    "\n",
    "# é¡¯ç¤ºå…ƒæ•¸æ“šæª”æ¡ˆ\n",
    "print(\"\\nğŸ“Š å€‰åº«å…ƒæ•¸æ“š (repository_metadata.json):\")\n",
    "print(\"=\"*80)\n",
    "if repo.metadata_file.exists():\n",
    "    with open(repo.metadata_file, 'r', encoding='utf-8') as f:\n",
    "        metadata_content = f.read()\n",
    "        # åªé¡¯ç¤ºå‰500å­—å…ƒé¿å…éé•·\n",
    "        if len(metadata_content) > 500:\n",
    "            print(metadata_content[:500] + \"\\n... (å…§å®¹éé•·ï¼Œå·²æˆªæ–·)\")\n",
    "        else:\n",
    "            print(metadata_content)\n",
    "else:\n",
    "    print(\"âŒ å…ƒæ•¸æ“šæª”æ¡ˆä¸å­˜åœ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ å¯¦é©—ç¸½çµèˆ‡ä¸‹ä¸€æ­¥\n",
    "\n",
    "### ğŸ¯ æœ¬å¯¦é©—å®Œæˆçš„å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "âœ… **ä¼æ¥­ç´šæ¨¡å‹å€‰åº«æ¶æ§‹è¨­è¨ˆ**\n",
    "- å»ºç«‹äº†åˆ†å±¤å¼çš„æ¨¡å‹çµ„ç¹”çµæ§‹\n",
    "- å¯¦ç¾äº†å®Œæ•´çš„æ¨¡å‹å…ƒæ•¸æ“šç®¡ç†\n",
    "- è¨­è¨ˆäº†è³‡æºåˆ†é…èˆ‡éš”é›¢ç­–ç•¥\n",
    "\n",
    "âœ… **æ¨¡å‹ä¾è³´é—œä¿‚ç®¡ç†**\n",
    "- å¯¦ç¾äº†ä¾è³´é—œä¿‚é©—è­‰æ©Ÿåˆ¶\n",
    "- å»ºç«‹äº†æ™ºèƒ½éƒ¨ç½²é †åºè¦åŠƒ\n",
    "- æä¾›äº†å¾ªç’°ä¾è³´æª¢æ¸¬åŠŸèƒ½\n",
    "\n",
    "âœ… **Netflix ç´šåˆ¥çš„å¯¦éš›æ¡ˆä¾‹**\n",
    "- æ¨¡æ“¬äº†æ¨è–¦ç³»çµ±å¤šæ¨¡å‹æ¶æ§‹\n",
    "- å±•ç¤ºäº†æœç´¢é ˜åŸŸæ¨¡å‹æ•´åˆ\n",
    "- å¯¦ç¾äº†ä¼æ¥­ç´š SLA ç®¡ç†\n",
    "\n",
    "### ğŸš€ æ ¸å¿ƒæŠ€è¡“æˆæœ\n",
    "\n",
    "1. **EnterpriseModelRepository**: ä¼æ¥­ç´šæ¨¡å‹å€‰åº«ç®¡ç†ç³»çµ±\n",
    "2. **DeploymentPlanner**: æ™ºèƒ½éƒ¨ç½²é †åºè¦åŠƒå™¨\n",
    "3. **RepositoryValidator**: å…¨é¢çš„å€‰åº«å¥åº·æª¢æŸ¥å·¥å…·\n",
    "4. **è‡ªå‹•åŒ–é…ç½®ç”Ÿæˆ**: Triton é…ç½®æª”æ¡ˆè‡ªå‹•ç”Ÿæˆ\n",
    "5. **éƒ¨ç½²è…³æœ¬ç”Ÿæˆ**: å®Œæ•´çš„è‡ªå‹•åŒ–éƒ¨ç½²æµç¨‹\n",
    "\n",
    "### ğŸ“Š ä¼æ¥­ç´šç‰¹æ€§\n",
    "\n",
    "- **è³‡æºç®¡ç†**: GPU/CPU è³‡æºåˆ†é…èˆ‡è¡çªæª¢æ¸¬\n",
    "- **SLA ç›£æ§**: å»¶é²ã€ååé‡ã€å¯ç”¨æ€§è¦æ±‚ç®¡ç†\n",
    "- **ä¾è³´ç®¡ç†**: æ‹“æ’²æ’åºç¢ºä¿æ­£ç¢ºéƒ¨ç½²é †åº\n",
    "- **å¥åº·æª¢æŸ¥**: å¤šå±¤æ¬¡çš„å€‰åº«é©—è­‰æ©Ÿåˆ¶\n",
    "- **ä¼æ¥­è¦ç¯„**: å‘½åè¦ç¯„ã€ç‰ˆæœ¬æ§åˆ¶ç­‰æœ€ä½³å¯¦è¸\n",
    "\n",
    "### ğŸ“ ä¸‹ä¸€æ­¥å­¸ç¿’è·¯å¾‘\n",
    "\n",
    "æº–å‚™å¥½é€²å…¥ **Lab-2.2.2: A/B æ¸¬è©¦èˆ‡ç‰ˆæœ¬æ§åˆ¶**ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ï¼š\n",
    "- å¯¦ç¾æ™ºèƒ½æµé‡åˆ†é…æ©Ÿåˆ¶\n",
    "- å»ºç«‹çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦\n",
    "- æŒæ¡æ¼¸é€²å¼éƒ¨ç½²ç­–ç•¥ (Canary/Blue-Green)\n",
    "- è¨­è¨ˆæ¨¡å‹æ€§èƒ½æ¯”è¼ƒæ¡†æ¶\n",
    "\n",
    "### ğŸ’¡ å»¶ä¼¸æ€è€ƒ\n",
    "\n",
    "1. å¦‚ä½•åœ¨ç¾æœ‰ä¼æ¥­ç’°å¢ƒä¸­é€æ­¥å°å…¥é€™å¥—æ¨¡å‹ç®¡ç†æ¶æ§‹ï¼Ÿ\n",
    "2. é¢å° 100+ æ¨¡å‹çš„è¶…å¤§è¦æ¨¡å ´æ™¯ï¼Œéœ€è¦å“ªäº›é¡å¤–çš„è¨­è¨ˆè€ƒé‡ï¼Ÿ\n",
    "3. å¦‚ä½•æ•´åˆç¾æœ‰çš„ MLOps å·¥å…·éˆ (Kubeflow, MLflow ç­‰)ï¼Ÿ\n",
    "4. åœ¨å¤šé›²ç’°å¢ƒä¸‹ï¼Œå¦‚ä½•ç¢ºä¿æ¨¡å‹å€‰åº«çš„ä¸€è‡´æ€§å’Œå¯ç§»æ¤æ€§ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆä¼æ¥­ç´šå¤šæ¨¡å‹å€‰åº«æ¶æ§‹è¨­è¨ˆï¼æ‚¨å·²ç¶“æŒæ¡äº† Netflix ç´šåˆ¥çš„æ¨¡å‹ç®¡ç†èƒ½åŠ›ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}