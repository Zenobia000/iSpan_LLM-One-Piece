{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1.8: ORPO 環境設置與數據準備\n",
    "\n",
    "**實驗目標**: 準備 ORPO (Odds Ratio Preference Optimization) 訓練環境\n",
    "\n",
    "**ORPO** 是比 DPO 更先進的對齊技術，主要創新點：\n",
    "- **單階段訓練**: 無需預先 SFT，直接從偏好數據進行對齊\n",
    "- **統一目標**: 同時優化 instruction following 和 preference alignment\n",
    "- **效率提升**: 訓練時間和記憶體使用都大幅減少\n",
    "\n",
    "## ORPO vs DPO\n",
    "\n",
    "| 特性 | DPO | ORPO |\n",
    "|------|-----|------|\n",
    "| 訓練階段 | 2 (SFT + DPO) | 1 (ORPO) |\n",
    "| 參考模型 | 需要 | 不需要 |\n",
    "| 記憶體占用 | 2x | 1x |\n",
    "| 訓練時間 | 100% | 50-60% |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 1: 環境檢查與安裝\n",
    "\n",
    "檢查 ORPO 所需的環境和套件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "print(f'Python 版本: {sys.version}')\n",
    "print(f'PyTorch 版本: {torch.__version__}')\n",
    "print(f'CUDA 可用: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU 設備: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU 記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "    print(f'當前記憶體使用: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "# 設置隨機種子\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print('\\n🚀 ORPO 環境檢查完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查並安裝必要套件\n",
    "def install_and_import(package, import_name=None):\n",
    "    if import_name is None:\n",
    "        import_name = package\n",
    "    \n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f'✅ {package} 已安裝')\n",
    "    except ImportError:\n",
    "        print(f'⚠️  安裝 {package}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f'✅ {package} 安裝完成')\n",
    "\n",
    "# 檢查必要套件\n",
    "required_packages = [\n",
    "    'transformers',\n",
    "    'peft', \n",
    "    'trl',\n",
    "    'datasets',\n",
    "    'bitsandbytes',\n",
    "    'accelerate',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn'\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    install_and_import(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入所需庫並檢查版本\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import datasets\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('=== 套件版本資訊 ===')\n",
    "print(f'🤗 Transformers: {transformers.__version__}')\n",
    "print(f'🎯 PEFT: {peft.__version__}')\n",
    "print(f'🚀 TRL: {trl.__version__}')\n",
    "print(f'📊 Datasets: {datasets.__version__}')\n",
    "print(f'⚡ Accelerate: {accelerate.__version__}')\n",
    "\n",
    "# 檢查 TRL 是否支援 ORPO\n",
    "try:\n",
    "    from trl import ORPOTrainer, ORPOConfig\n",
    "    print('✅ TRL 支援 ORPO')\n",
    "except ImportError:\n",
    "    print('⚠️  TRL 版本可能不支援 ORPO，將使用自定義實現')\n",
    "\n",
    "print('\\n✅ ORPO 環境準備完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 2: 偏好數據集準備\n",
    "\n",
    "ORPO 直接使用偏好數據進行訓練，無需額外的 instruction dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# 載入偏好數據集\n",
    "print('📥 載入偏好數據集...')\n",
    "\n",
    "try:\n",
    "    # 載入 Anthropic HH-RLHF 數據集\n",
    "    dataset = load_dataset('Anthropic/hh-rlhf', split='train[:500]')  # 載入較大樣本\n",
    "    print(f'✅ 成功載入 HH-RLHF 數據集，樣本數: {len(dataset)}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ HH-RLHF 載入失敗: {e}')\n",
    "    print('使用增強版模擬數據...')\n",
    "    \n",
    "    # 創建更豐富的模擬偏好數據\n",
    "    mock_data = {\n",
    "        'chosen': [\n",
    "            'Human: 請解釋什麼是機器學習?\\n\\nAssistant: 機器學習是人工智能的一個重要分支，它讓計算機能夠從數據中自動學習並改進性能，而無需明確編程。核心概念包括：\\n\\n1. **學習過程**: 通過分析大量數據來識別模式和規律\\n2. **算法類型**: 包括監督學習、無監督學習和強化學習\\n3. **應用領域**: 廣泛應用於圖像識別、自然語言處理、推薦系統等\\n\\n機器學習的價值在於能夠處理複雜的數據模式，為決策提供智能支持。',\n",
    "            \n",
    "            'Human: 如何開始學習程式設計?\\n\\nAssistant: 學習程式設計是一個循序漸進的過程，建議按以下步驟進行：\\n\\n**基礎階段**：\\n1. 選擇適合的入門語言（如 Python）\\n2. 學習基本語法和概念\\n3. 練習簡單的邏輯問題\\n\\n**進階階段**：\\n1. 學習數據結構和算法\\n2. 做實際項目練習\\n3. 閱讀他人代碼\\n\\n**持續成長**：\\n1. 參與開源專案\\n2. 學習軟體工程實踐\\n3. 保持學習新技術的習慣\\n\\n記住，程式設計重在實作，多寫多練是關鍵。',\n",
    "            \n",
    "            'Human: 健康飲食的基本原則是什麼?\\n\\nAssistant: 健康飲食應遵循以下科學原則：\\n\\n**營養均衡**：\\n- 碳水化合物：提供能量，選擇全穀類\\n- 蛋白質：維持肌肉，選擇優質來源\\n- 脂肪：選擇不飽和脂肪，避免反式脂肪\\n- 維生素礦物質：多吃蔬果補充\\n\\n**飲食習慣**：\\n- 規律用餐，避免暴飲暴食\\n- 控制份量，適量攝取\\n- 多喝水，少喝含糖飲料\\n- 減少加工食品\\n\\n**個人化調整**：\\n根據年齡、性別、活動量調整飲食計劃。建議諮詢營養師制定個人化方案。',\n",
    "            \n",
    "            'Human: 如何提高工作效率?\\n\\nAssistant: 提高工作效率需要系統性方法：\\n\\n**時間管理**：\\n1. 使用番茄工作法：25分鐘專注 + 5分鐘休息\\n2. 優先級排序：先做重要且緊急的任務\\n3. 時間塊規劃：為不同任務分配固定時間\\n\\n**環境優化**：\\n1. 整理工作空間，減少干擾\\n2. 使用高效工具和軟體\\n3. 建立標準化流程\\n\\n**個人習慣**：\\n1. 保持健康作息\\n2. 定期休息和運動\\n3. 持續學習新技能\\n\\n**團隊協作**：\\n1. 清晰溝通，減少誤解\\n2. 有效授權和分工\\n3. 定期回顧和改進\\n\\n關鍵是找到適合自己的方法並持續執行。',\n",
    "            \n",
    "            'Human: 分析人工智能對未來就業的影響\\n\\nAssistant: 人工智能對就業市場的影響是複雜且多面向的：\\n\\n**潛在挑戰**：\\n1. **自動化替代**：重複性工作面臨被取代風險\\n2. **技能鴻溝**：需要新技能以適應變化\\n3. **過渡期痛苦**：短期內可能造成失業\\n\\n**新機遇**：\\n1. **新職業創造**：AI工程師、數據科學家等\\n2. **工作增強**：AI作為工具提升人類能力\\n3. **創意領域擴展**：更多時間專注創新工作\\n\\n**應對策略**：\\n1. **終身學習**：持續更新技能\\n2. **跨領域能力**：培養AI無法替代的軟技能\\n3. **政策支持**：政府需制定轉型政策\\n\\n總體而言，AI更可能是工作轉型而非完全替代，關鍵在於主動適應。'\n",
    "        ],\n",
    "        'rejected': [\n",
    "            'Human: 請解釋什麼是機器學習?\\n\\nAssistant: 機器學習就是讓機器變聰明，能夠自己學東西。',\n",
    "            \n",
    "            'Human: 如何開始學習程式設計?\\n\\nAssistant: 學程式就是寫code，買本書看看就會了。',\n",
    "            \n",
    "            'Human: 健康飲食的基本原則是什麼?\\n\\nAssistant: 健康飲食就是少吃多動，不要吃垃圾食品。',\n",
    "            \n",
    "            'Human: 如何提高工作效率?\\n\\nAssistant: 工作效率就是要快一點，多做一些事情。',\n",
    "            \n",
    "            'Human: 分析人工智能對未來就業的影響\\n\\nAssistant: AI會搶走很多工作，大家要小心。'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(mock_data)\n",
    "    print(f'✅ 創建模擬數據集，樣本數: {len(dataset)}')\n",
    "\n",
    "print(f'\\n數據集欄位: {list(dataset.features.keys())}')\n",
    "print(f'數據集大小: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索數據集內容\n",
    "print('=== 偏好數據樣本探索 ===')\n",
    "\n",
    "for i in range(min(2, len(dataset))):\n",
    "    sample = dataset[i]\n",
    "    print(f'\\n📝 樣本 {i+1}:')\n",
    "    \n",
    "    if 'chosen' in sample:\n",
    "        print('✅ 偏好回應 (chosen):')\n",
    "        chosen_text = sample['chosen']\n",
    "        print(chosen_text[:300] + '...' if len(chosen_text) > 300 else chosen_text)\n",
    "        print()\n",
    "        \n",
    "    if 'rejected' in sample:\n",
    "        print('❌ 非偏好回應 (rejected):')\n",
    "        rejected_text = sample['rejected']\n",
    "        print(rejected_text[:300] + '...' if len(rejected_text) > 300 else rejected_text)\n",
    "        print()\n",
    "    \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 3: 數據預處理與格式化\n",
    "\n",
    "將數據轉換為 ORPO 訓練所需的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_and_response(text):\n",
    "    \"\"\"從對話文本中提取提示和回應\"\"\"\n",
    "    if 'Human:' in text and 'Assistant:' in text:\n",
    "        parts = text.split('Assistant:')\n",
    "        if len(parts) >= 2:\n",
    "            prompt = parts[0].replace('Human:', '').strip()\n",
    "            response = parts[1].strip()\n",
    "            return prompt, response\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def preprocess_orpo_dataset(dataset):\n",
    "    \"\"\"預處理數據集為 ORPO 格式\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # 提取 chosen 和 rejected 的 prompt 和 response\n",
    "        chosen_prompt, chosen_response = extract_prompt_and_response(sample['chosen'])\n",
    "        rejected_prompt, rejected_response = extract_prompt_and_response(sample['rejected'])\n",
    "        \n",
    "        # 確保兩個回應使用相同的 prompt\n",
    "        if chosen_prompt and chosen_response and rejected_response:\n",
    "            # ORPO 格式: prompt, chosen, rejected\n",
    "            processed_data.append({\n",
    "                'prompt': chosen_prompt,\n",
    "                'chosen': chosen_response,\n",
    "                'rejected': rejected_response\n",
    "            })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# 預處理數據\n",
    "print('🔄 預處理 ORPO 數據...')\n",
    "processed_data = preprocess_orpo_dataset(dataset)\n",
    "\n",
    "print(f'✅ 處理完成，有效樣本數: {len(processed_data)}')\n",
    "\n",
    "# 顯示處理後的樣本\n",
    "if processed_data:\n",
    "    print('\\n=== 處理後樣本示例 ===')\n",
    "    sample = processed_data[0]\n",
    "    print(f'📝 Prompt: {sample[\"prompt\"]}\\n')\n",
    "    print(f'✅ Chosen: {sample[\"chosen\"][:200]}...')\n",
    "    print(f'❌ Rejected: {sample[\"rejected\"][:200]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 4: ORPO 損失函數預覽\n",
    "\n",
    "實現並測試 ORPO 核心損失函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_log_probs(model, input_ids, attention_mask, labels, tokenizer):\n",
    "    \"\"\"計算序列的對數概率\"\"\"\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # 計算每個 token 的對數概率\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # 收集目標 token 的對數概率\n",
    "    target_log_probs = torch.gather(log_probs[:, :-1], dim=-1, \n",
    "                                   index=labels[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # 僅計算非 padding token 的平均對數概率\n",
    "    mask = (labels[:, 1:] != tokenizer.pad_token_id).float()\n",
    "    sequence_log_prob = (target_log_probs * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "    \n",
    "    return sequence_log_prob\n",
    "\n",
    "\n",
    "def compute_odds_ratio_loss(chosen_log_probs, rejected_log_probs):\n",
    "    \"\"\"計算 ORPO 的 Odds Ratio 損失\"\"\"\n",
    "    # 計算 log odds\n",
    "    chosen_log_odds = chosen_log_probs - torch.log1p(-torch.exp(chosen_log_probs.clamp(max=0)))\n",
    "    rejected_log_odds = rejected_log_probs - torch.log1p(-torch.exp(rejected_log_probs.clamp(max=0)))\n",
    "    \n",
    "    # Odds Ratio 損失\n",
    "    log_odds_ratio = chosen_log_odds - rejected_log_odds\n",
    "    loss = -F.logsigmoid(log_odds_ratio).mean()\n",
    "    \n",
    "    return loss, log_odds_ratio.mean()\n",
    "\n",
    "\n",
    "def orpo_loss(model, batch, tokenizer, lambda_or=0.5):\n",
    "    \"\"\"\n",
    "    ORPO 損失函數\n",
    "    \n",
    "    Args:\n",
    "        model: 訓練中的模型\n",
    "        batch: 包含 prompt, chosen, rejected 的批次\n",
    "        tokenizer: tokenizer\n",
    "        lambda_or: Odds Ratio 損失權重\n",
    "    \n",
    "    Returns:\n",
    "        total_loss, metrics\n",
    "    \"\"\"\n",
    "    # 1. SFT 損失 (標準語言模型損失，僅在 chosen 上)\n",
    "    chosen_input_ids = batch['chosen_input_ids']\n",
    "    chosen_attention_mask = batch['chosen_attention_mask']\n",
    "    \n",
    "    outputs = model(input_ids=chosen_input_ids, attention_mask=chosen_attention_mask, \n",
    "                   labels=chosen_input_ids)\n",
    "    sft_loss = outputs.loss\n",
    "    \n",
    "    # 2. Odds Ratio 損失\n",
    "    chosen_log_probs = compute_log_probs(model, batch['chosen_input_ids'], \n",
    "                                        batch['chosen_attention_mask'], \n",
    "                                        batch['chosen_input_ids'], tokenizer)\n",
    "    \n",
    "    rejected_log_probs = compute_log_probs(model, batch['rejected_input_ids'],\n",
    "                                          batch['rejected_attention_mask'],\n",
    "                                          batch['rejected_input_ids'], tokenizer)\n",
    "    \n",
    "    or_loss, log_odds_ratio = compute_odds_ratio_loss(chosen_log_probs, rejected_log_probs)\n",
    "    \n",
    "    # 3. 總損失\n",
    "    total_loss = sft_loss + lambda_or * or_loss\n",
    "    \n",
    "    metrics = {\n",
    "        'total_loss': total_loss.item(),\n",
    "        'sft_loss': sft_loss.item(),\n",
    "        'or_loss': or_loss.item(),\n",
    "        'log_odds_ratio': log_odds_ratio.item(),\n",
    "        'lambda_or': lambda_or\n",
    "    }\n",
    "    \n",
    "    return total_loss, metrics\n",
    "\n",
    "\n",
    "print('✅ ORPO 損失函數實現完成')\n",
    "print('\\n🔬 ORPO 核心概念:')\n",
    "print('• SFT Loss: 確保模型基本的指令遵循能力')\n",
    "print('• Odds Ratio Loss: 優化偏好對齊，提升 chosen 相對 rejected 的機率')\n",
    "print('• Lambda 平衡: 控制兩個損失的相對重要性')\n",
    "print('• 單階段訓練: 同時優化兩個目標，無需分階段')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟 5: 實驗設置總結\n",
    "\n",
    "總結 ORPO 環境設置完成情況。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實驗設置總結\n",
    "print('=== ORPO 環境設置總結 ===')\n",
    "print(f'✅ PyTorch 版本: {torch.__version__}')\n",
    "print(f'✅ GPU 可用: {torch.cuda.is_available()}')\n",
    "print(f'✅ 數據樣本: {len(processed_data) if processed_data else 0}')\n",
    "print(f'✅ ORPO 損失函數: 已實現')\n",
    "\n",
    "if processed_data:\n",
    "    # 保存數據集\n",
    "    output_dir = './orpo_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 轉換為 Dataset 並保存\n",
    "    orpo_dataset = Dataset.from_list(processed_data)\n",
    "    orpo_dataset.save_to_disk(output_dir)\n",
    "    \n",
    "    print(f'💾 數據集已保存至: {output_dir}')\n",
    "\n",
    "print('\\n🚀 ORPO 環境設置完成！')\n",
    "print('下一步: 執行 02-ORPO_Training.ipynb 開始單階段對齊訓練')\n",
    "\n",
    "print('\\n🔬 ORPO 技術優勢:')\n",
    "print('• 單階段訓練：無需 SFT + DPO 兩階段')\n",
    "print('• 記憶體高效：無需參考模型，記憶體使用減半')\n",
    "print('• 訓練簡化：統一損失函數，超參數調優更容易')\n",
    "print('• 效果更佳：理論上比 DPO 有更好的對齊效果')\n",
    "\n",
    "# 清理 GPU 記憶體\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\n💾 GPU 記憶體已清理，當前使用: {torch.cuda.memory_allocated() / 1e9:.2f} GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}