{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.3.3: vLLM Backend æ•´åˆèˆ‡å„ªåŒ–\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "- æŒæ¡ Triton + vLLM æ¶æ§‹æ•´åˆ\n",
    "- å¯¦ç¾ PagedAttention åœ¨ Triton ä¸­æ‡‰ç”¨\n",
    "- å„ªåŒ–å¤§èªè¨€æ¨¡å‹æ¨ç†æ€§èƒ½\n",
    "- è¨­è¨ˆæ··åˆéƒ¨ç½²ç­–ç•¥\n",
    "\n",
    "## ğŸ“š ç†è«–åŸºç¤\n",
    "\n",
    "### vLLM Backend æ¶æ§‹\n",
    "```\n",
    "Triton Server\n",
    "â”œâ”€â”€ Model Repository\n",
    "â”‚   â””â”€â”€ llm_model/\n",
    "â”‚       â”œâ”€â”€ config.pbtxt          # vLLM backend é…ç½®\n",
    "â”‚       â””â”€â”€ 1/\n",
    "â”‚           â””â”€â”€ model.py          # vLLM æ•´åˆå¯¦ç¾\n",
    "â””â”€â”€ vLLM Engine\n",
    "    â”œâ”€â”€ PagedAttention           # è¨˜æ†¶é«”æ•ˆç‡å„ªåŒ–\n",
    "    â”œâ”€â”€ Continuous Batching      # å‹•æ…‹æ‰¹æ¬¡è™•ç†\n",
    "    â””â”€â”€ KV Cache ç®¡ç†           # å¿«å–æœ€ä½³åŒ–\n",
    "```\n",
    "\n",
    "### æ ¸å¿ƒå„ªåŒ–æŠ€è¡“\n",
    "1. **PagedAttention**: å°‡ attention çš„ KV cache åˆ†é ç®¡ç†\n",
    "2. **Continuous Batching**: å‹•æ…‹æ·»åŠ /ç§»é™¤åºåˆ—\n",
    "3. **Speculative Decoding**: åŠ é€Ÿç”Ÿæˆéç¨‹\n",
    "4. **Quantization**: INT8/FP16 æ¨ç†å„ªåŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç’°å¢ƒè¨­ç½®èˆ‡æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('vllm_backend.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# è¨­ç½®åœ–è¡¨æ¨£å¼\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ“‹ ç³»çµ±ç’°å¢ƒæª¢æŸ¥\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"ç•¶å‰ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ vLLM å®‰è£\n",
    "try:\n",
    "    import vllm\n",
    "    from vllm import LLM, SamplingParams\n",
    "    from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "    from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "    print(f\"âœ… vLLM ç‰ˆæœ¬: {vllm.__version__}\")\n",
    "    vllm_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ vLLM æœªå®‰è£: {e}\")\n",
    "    print(\"è«‹å®‰è£: pip install vllm\")\n",
    "    vllm_available = False\n",
    "\n",
    "# æª¢æŸ¥ Triton Client\n",
    "try:\n",
    "    import tritonclient.http as httpclient\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    print(\"âœ… Triton Client å¯ç”¨\")\n",
    "    triton_client_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Triton Client æœªå®‰è£: {e}\")\n",
    "    print(\"è«‹å®‰è£: pip install tritonclient[all]\")\n",
    "    triton_client_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ æ¨¡å‹å€‰åº«çµæ§‹è¨­è¨ˆ\n",
    "\n",
    "### è¨­è¨ˆ vLLM Backend æ¨¡å‹å€‰åº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMModelRepository:\n",
    "    \"\"\"vLLM Backend æ¨¡å‹å€‰åº«ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, repository_path: str = \"./model_repository\"):\n",
    "        self.repository_path = Path(repository_path)\n",
    "        self.repository_path.mkdir(exist_ok=True)\n",
    "        logger.info(f\"æ¨¡å‹å€‰åº«è·¯å¾‘: {self.repository_path.absolute()}\")\n",
    "    \n",
    "    def create_vllm_model_config(self, \n",
    "                                model_name: str,\n",
    "                                hf_model_name: str,\n",
    "                                max_model_len: int = 2048,\n",
    "                                tensor_parallel_size: int = 1,\n",
    "                                dtype: str = \"auto\",\n",
    "                                quantization: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"å‰µå»º vLLM backend é…ç½®\"\"\"\n",
    "        \n",
    "        config = {\n",
    "            \"name\": model_name,\n",
    "            \"backend\": \"vllm\",\n",
    "            \"max_batch_size\": 256,\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"name\": \"text_input\",\n",
    "                    \"data_type\": \"TYPE_STRING\",\n",
    "                    \"dims\": [-1]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"stream\",\n",
    "                    \"data_type\": \"TYPE_BOOL\",\n",
    "                    \"dims\": [1],\n",
    "                    \"optional\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"sampling_parameters\",\n",
    "                    \"data_type\": \"TYPE_STRING\",\n",
    "                    \"dims\": [1],\n",
    "                    \"optional\": True\n",
    "                }\n",
    "            ],\n",
    "            \"output\": [\n",
    "                {\n",
    "                    \"name\": \"text_output\",\n",
    "                    \"data_type\": \"TYPE_STRING\",\n",
    "                    \"dims\": [-1]\n",
    "                }\n",
    "            ],\n",
    "            \"instance_group\": [\n",
    "                {\n",
    "                    \"count\": 1,\n",
    "                    \"kind\": \"KIND_MODEL\"\n",
    "                }\n",
    "            ],\n",
    "            \"parameters\": {\n",
    "                \"model\": {\"string_value\": hf_model_name},\n",
    "                \"tensor_parallel_size\": {\"string_value\": str(tensor_parallel_size)},\n",
    "                \"max_model_len\": {\"string_value\": str(max_model_len)},\n",
    "                \"dtype\": {\"string_value\": dtype},\n",
    "                \"gpu_memory_utilization\": {\"string_value\": \"0.9\"},\n",
    "                \"enforce_eager\": {\"string_value\": \"false\"},\n",
    "                \"disable_log_requests\": {\"string_value\": \"false\"}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # æ·»åŠ é‡åŒ–é…ç½®\n",
    "        if quantization:\n",
    "            config[\"parameters\"][\"quantization\"] = {\"string_value\": quantization}\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def write_config_pbtxt(self, config: Dict[str, Any], model_name: str):\n",
    "        \"\"\"å¯«å…¥ config.pbtxt æ–‡ä»¶\"\"\"\n",
    "        model_dir = self.repository_path / model_name\n",
    "        model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        config_path = model_dir / \"config.pbtxt\"\n",
    "        \n",
    "        # è½‰æ›ç‚º protobuf æ ¼å¼\n",
    "        pbtxt_content = self._dict_to_pbtxt(config)\n",
    "        \n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(pbtxt_content)\n",
    "        \n",
    "        logger.info(f\"é…ç½®æ–‡ä»¶å·²å¯«å…¥: {config_path}\")\n",
    "        return config_path\n",
    "    \n",
    "    def create_vllm_model_py(self, model_name: str) -> Path:\n",
    "        \"\"\"å‰µå»º vLLM model.py æ–‡ä»¶\"\"\"\n",
    "        model_dir = self.repository_path / model_name / \"1\"\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        model_py_content = '''\nimport json\nimport torch\nfrom typing import List, Dict, Any, Optional\nimport triton_python_backend_utils as pb_utils\nfrom vllm import LLM, SamplingParams\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nimport asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TritonPythonModel:\n    \"\"\"Triton vLLM Backend æ¨¡å‹å¯¦ç¾\"\"\"\n    \n    def initialize(self, args):\n        \"\"\"åˆå§‹åŒ– vLLM å¼•æ“\"\"\"\n        logger.info(\"åˆå§‹åŒ– vLLM Backend\")\n        \n        # ç²å–æ¨¡å‹é…ç½®\n        model_config = json.loads(args[\"model_config\"])\n        model_params = model_config.get(\"parameters\", {})\n        \n        # è§£æ vLLM åƒæ•¸\n        self.model_name = model_params.get(\"model\", {}).get(\"string_value\", \"\")\n        tensor_parallel_size = int(model_params.get(\"tensor_parallel_size\", {}).get(\"string_value\", \"1\"))\n        max_model_len = int(model_params.get(\"max_model_len\", {}).get(\"string_value\", \"2048\"))\n        dtype = model_params.get(\"dtype\", {}).get(\"string_value\", \"auto\")\n        gpu_memory_utilization = float(model_params.get(\"gpu_memory_utilization\", {}).get(\"string_value\", \"0.9\"))\n        quantization = model_params.get(\"quantization\", {}).get(\"string_value\", None)\n        \n        # åˆå§‹åŒ– vLLM å¼•æ“\n        try:\n            self.llm = LLM(\n                model=self.model_name,\n                tensor_parallel_size=tensor_parallel_size,\n                max_model_len=max_model_len,\n                dtype=dtype,\n                gpu_memory_utilization=gpu_memory_utilization,\n                quantization=quantization,\n                enforce_eager=False,\n                disable_log_requests=True\n            )\n            logger.info(f\"vLLM å¼•æ“åˆå§‹åŒ–æˆåŠŸ: {self.model_name}\")\n        except Exception as e:\n            logger.error(f\"vLLM å¼•æ“åˆå§‹åŒ–å¤±æ•—: {e}\")\n            raise\n    \n    def execute(self, requests):\n        \"\"\"åŸ·è¡Œæ¨ç†è«‹æ±‚\"\"\"\n        responses = []\n        \n        for request in requests:\n            try:\n                # è§£æè¼¸å…¥\n                text_input = pb_utils.get_input_tensor_by_name(request, \"text_input\")\n                prompts = [prompt.decode('utf-8') for prompt in text_input.as_numpy()]\n                \n                # è§£æ sampling parameters\n                sampling_params_tensor = pb_utils.get_input_tensor_by_name(request, \"sampling_parameters\")\n                if sampling_params_tensor is not None:\n                    sampling_params_json = sampling_params_tensor.as_numpy()[0].decode('utf-8')\n                    sampling_params_dict = json.loads(sampling_params_json)\n                    sampling_params = SamplingParams(**sampling_params_dict)\n                else:\n                    sampling_params = SamplingParams(\n                        temperature=0.7,\n                        top_p=0.9,\n                        max_tokens=512\n                    )\n                \n                # åŸ·è¡Œæ¨ç†\n                outputs = self.llm.generate(prompts, sampling_params)\n                \n                # æå–ç”Ÿæˆçš„æ–‡æœ¬\n                generated_texts = []\n                for output in outputs:\n                    generated_text = output.outputs[0].text\n                    generated_texts.append(generated_text)\n                \n                # å‰µå»ºè¼¸å‡ºå¼µé‡\n                output_tensor = pb_utils.Tensor(\n                    \"text_output\", \n                    np.array(generated_texts, dtype=object)\n                )\n                \n                response = pb_utils.InferenceResponse(\n                    output_tensors=[output_tensor]\n                )\n                responses.append(response)\n                \n            except Exception as e:\n                logger.error(f\"æ¨ç†åŸ·è¡Œå¤±æ•—: {e}\")\n                error_response = pb_utils.InferenceResponse(\n                    output_tensors=[],\n                    error=pb_utils.TritonError(f\"æ¨ç†å¤±æ•—: {str(e)}\")\n                )\n                responses.append(error_response)\n        \n        return responses\n    \n    def finalize(self):\n        \"\"\"æ¸…ç†è³‡æº\"\"\"\n        logger.info(\"vLLM Backend æ¸…ç†å®Œæˆ\")\n        pass\n'''\n        \n        model_py_path = model_dir / \"model.py\"\n        with open(model_py_path, 'w', encoding='utf-8') as f:\n            f.write(model_py_content)\n        \n        logger.info(f\"model.py å·²å‰µå»º: {model_py_path}\")\n        return model_py_path\n    \n    def _dict_to_pbtxt(self, config: Dict[str, Any], indent: int = 0) -> str:\n        \"\"\"å°‡å­—å…¸è½‰æ›ç‚º protobuf æ–‡æœ¬æ ¼å¼\"\"\"\n        lines = []\n        \n        for key, value in config.items():\n            if isinstance(value, dict):\n                lines.append(f\"{' ' * indent}{key} {{\")\n                lines.append(self._dict_to_pbtxt(value, indent + 2))\n                lines.append(f\"{' ' * indent}}}\")\n            elif isinstance(value, list):\n                for item in value:\n                    if isinstance(item, dict):\n                        lines.append(f\"{' ' * indent}{key} {{\")\n                        lines.append(self._dict_to_pbtxt(item, indent + 2))\n                        lines.append(f\"{' ' * indent}}}\")\n                    else:\n                        lines.append(f\"{' ' * indent}{key}: {self._format_value(item)}\")\n            else:\n                lines.append(f\"{' ' * indent}{key}: {self._format_value(value)}\")\n        \n        return '\\n'.join(lines)\n    \n    def _format_value(self, value: Any) -> str:\n        \"\"\"æ ¼å¼åŒ–å€¼\"\"\"\n        if isinstance(value, str):\n            return f'\"{value}\"'\n        elif isinstance(value, bool):\n            return str(value).lower()\n        else:\n            return str(value)\n\n# å‰µå»ºæ¨¡å‹å€‰åº«ç®¡ç†å™¨\nvllm_repo = vLLMModelRepository()\nprint(f\"ğŸ“ æ¨¡å‹å€‰åº«è·¯å¾‘: {vllm_repo.repository_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– vLLM æ¨¡å‹éƒ¨ç½²å¯¦ä¾‹\n",
    "\n",
    "### éƒ¨ç½²å°å‹èªè¨€æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¸æ“‡é©åˆçš„æ¨¡å‹ï¼ˆæ ¹æ“š GPU è¨˜æ†¶é«”ï¼‰\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "if gpu_memory >= 24:\n",
    "    model_choice = \"microsoft/DialoGPT-large\"\n",
    "    max_model_len = 1024\n",
    "    print(f\"ğŸš€ é¸æ“‡å¤§å‹æ¨¡å‹: {model_choice} (GPU: {gpu_memory:.1f}GB)\")\n",
    "elif gpu_memory >= 8:\n",
    "    model_choice = \"microsoft/DialoGPT-medium\"\n",
    "    max_model_len = 512\n",
    "    print(f\"ğŸš€ é¸æ“‡ä¸­å‹æ¨¡å‹: {model_choice} (GPU: {gpu_memory:.1f}GB)\")\n",
    "else:\n",
    "    model_choice = \"gpt2\"\n",
    "    max_model_len = 256\n",
    "    print(f\"ğŸš€ é¸æ“‡å°å‹æ¨¡å‹: {model_choice} (GPU: {gpu_memory:.1f}GB)\")\n",
    "\n",
    "# å‰µå»º vLLM æ¨¡å‹é…ç½®\n",
    "model_name = \"vllm_chat_model\"\n",
    "config = vllm_repo.create_vllm_model_config(\n",
    "    model_name=model_name,\n",
    "    hf_model_name=model_choice,\n",
    "    max_model_len=max_model_len,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# å¯«å…¥é…ç½®æ–‡ä»¶\n",
    "config_path = vllm_repo.write_config_pbtxt(config, model_name)\n",
    "print(f\"âœ… é…ç½®æ–‡ä»¶å·²å‰µå»º: {config_path}\")\n",
    "\n",
    "# å‰µå»º model.py\n",
    "model_py_path = vllm_repo.create_vllm_model_py(model_name)\n",
    "print(f\"âœ… model.py å·²å‰µå»º: {model_py_path}\")\n",
    "\n",
    "# é¡¯ç¤ºæ¨¡å‹å€‰åº«çµæ§‹\n",
    "def show_repository_structure(repo_path: Path, max_depth: int = 3):\n",
    "    \"\"\"é¡¯ç¤ºæ¨¡å‹å€‰åº«çµæ§‹\"\"\"\n",
    "    print(f\"\\nğŸ“‚ æ¨¡å‹å€‰åº«çµæ§‹: {repo_path}\")\n",
    "    \n",
    "    def print_tree(path: Path, prefix: str = \"\", depth: int = 0):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        \n",
    "        items = sorted([p for p in path.iterdir()])\n",
    "        for i, item in enumerate(items):\n",
    "            is_last = i == len(items) - 1\n",
    "            current_prefix = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "            print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "            \n",
    "            if item.is_dir():\n",
    "                next_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
    "                print_tree(item, next_prefix, depth + 1)\n",
    "    \n",
    "    print_tree(repo_path)\n",
    "\n",
    "show_repository_structure(vllm_repo.repository_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Triton Server å•Ÿå‹•èˆ‡ vLLM æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonvLLMManager:\n",
    "    \"\"\"Triton + vLLM æ•´åˆç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, repository_path: str):\n",
    "        self.repository_path = repository_path\n",
    "        self.server_process = None\n",
    "        self.server_url = \"http://localhost:8000\"\n",
    "        self.grpc_url = \"localhost:8001\"\n",
    "    \n",
    "    def start_triton_server(self, log_file: str = \"triton_server.log\") -> bool:\n",
    "        \"\"\"å•Ÿå‹• Triton Server\"\"\"\n",
    "        try:\n",
    "            # æª¢æŸ¥æ˜¯å¦å·²æœ‰æœå‹™å™¨é‹è¡Œ\n",
    "            if self.is_server_running():\n",
    "                logger.info(\"Triton Server å·²åœ¨é‹è¡Œ\")\n",
    "                return True\n",
    "            \n",
    "            # å•Ÿå‹•å‘½ä»¤\n",
    "            cmd = [\n",
    "                \"tritonserver\",\n",
    "                f\"--model-repository={self.repository_path}\",\n",
    "                \"--http-port=8000\",\n",
    "                \"--grpc-port=8001\",\n",
    "                \"--metrics-port=8002\",\n",
    "                \"--log-verbose=1\",\n",
    "                \"--backend-directory=/opt/tritonserver/backends\",\n",
    "                \"--backend-config=python,shm-region-prefix-name=prefix0_\"\n",
    "            ]\n",
    "            \n",
    "            # å•Ÿå‹•æœå‹™å™¨\n",
    "            logger.info(\"æ­£åœ¨å•Ÿå‹• Triton Server...\")\n",
    "            with open(log_file, 'w') as f:\n",
    "                self.server_process = subprocess.Popen(\n",
    "                    cmd,\n",
    "                    stdout=f,\n",
    "                    stderr=subprocess.STDOUT,\n",
    "                    text=True\n",
    "                )\n",
    "            \n",
    "            # ç­‰å¾…æœå‹™å™¨å•Ÿå‹•\n",
    "            max_wait_time = 120  # 2åˆ†é˜\n",
    "            for i in range(max_wait_time):\n",
    "                if self.is_server_running():\n",
    "                    logger.info(f\"Triton Server å•Ÿå‹•æˆåŠŸ (ç­‰å¾…æ™‚é–“: {i}ç§’)\")\n",
    "                    return True\n",
    "                time.sleep(1)\n",
    "            \n",
    "            logger.error(\"Triton Server å•Ÿå‹•è¶…æ™‚\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å•Ÿå‹• Triton Server å¤±æ•—: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def is_server_running(self) -> bool:\n",
    "        \"\"\"æª¢æŸ¥æœå‹™å™¨æ˜¯å¦é‹è¡Œ\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/v2/health/ready\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def stop_server(self):\n",
    "        \"\"\"åœæ­¢æœå‹™å™¨\"\"\"\n",
    "        if self.server_process:\n",
    "            self.server_process.terminate()\n",
    "            self.server_process.wait()\n",
    "            logger.info(\"Triton Server å·²åœæ­¢\")\n",
    "    \n",
    "    def get_server_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–æœå‹™å™¨ç‹€æ…‹\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/v2\")\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ç²å–æœå‹™å™¨ç‹€æ…‹å¤±æ•—: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def list_models(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"åˆ—å‡ºæ‰€æœ‰æ¨¡å‹\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/v2/models\")\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_model_config(self, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–æ¨¡å‹é…ç½®\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.server_url}/v2/models/{model_name}/config\")\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ç²å–æ¨¡å‹é…ç½®å¤±æ•—: {e}\")\n",
    "            return {}\n",
    "\n",
    "# å‰µå»º Triton vLLM ç®¡ç†å™¨\ntriton_vllm = TritonvLLMManager(str(vllm_repo.repository_path))\n",
    "print(\"ğŸ”§ Triton vLLM ç®¡ç†å™¨å·²å‰µå»º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ tritonserver æ˜¯å¦å¯ç”¨\n",
    "def check_tritonserver_availability():\n",
    "    \"\"\"æª¢æŸ¥ tritonserver å‘½ä»¤æ˜¯å¦å¯ç”¨\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"tritonserver\", \"--help\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        return result.returncode == 0\n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "tritonserver_available = check_tritonserver_availability()\n",
    "\n",
    "if tritonserver_available:\n",
    "    print(\"âœ… Tritonserver å‘½ä»¤å¯ç”¨\")\n",
    "    \n",
    "    # å˜—è©¦å•Ÿå‹• Triton Serverï¼ˆå¦‚æœå¯ç”¨ä¸” vLLM ä¹Ÿå¯ç”¨ï¼‰\n",
    "    if vllm_available:\n",
    "        print(\"ğŸš€ æº–å‚™å•Ÿå‹• Triton Server with vLLM Backend...\")\n",
    "        # æ³¨æ„ï¼šå¯¦éš›å•Ÿå‹•éœ€è¦æ¨¡å‹æ–‡ä»¶ï¼Œé€™è£¡åƒ…ä½œæ¼”ç¤º\n",
    "        print(\"âš ï¸  å¯¦éš›éƒ¨ç½²éœ€è¦ä¸‹è¼‰å°æ‡‰çš„ HuggingFace æ¨¡å‹\")\n",
    "        print(f\"   æ¨¡å‹: {model_choice}\")\n",
    "        print(f\"   é…ç½®è·¯å¾‘: {config_path}\")\n",
    "        print(f\"   å•Ÿå‹•å‘½ä»¤: tritonserver --model-repository={vllm_repo.repository_path}\")\n",
    "    else:\n",
    "        print(\"âŒ vLLM ä¸å¯ç”¨ï¼Œç„¡æ³•å•Ÿå‹• vLLM Backend\")\nelse:\n",
    "    print(\"âŒ Tritonserver å‘½ä»¤ä¸å¯ç”¨\")\n",
    "    print(\"   è«‹å®‰è£ Triton Inference Server\")\n",
    "    print(\"   Docker: nvidia/tritonserver:xx.xx-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ vLLM Backend å®¢æˆ¶ç«¯æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMTritonClient:\n",
    "    \"\"\"vLLM Triton å®¢æˆ¶ç«¯\"\"\"\n",
    "    \n",
    "    def __init__(self, server_url: str = \"localhost:8000\"):\n",
    "        self.server_url = server_url\n",
    "        if triton_client_available:\n",
    "            self.client = httpclient.InferenceServerClient(url=server_url)\n",
    "        else:\n",
    "            self.client = None\n",
    "    \n",
    "    def is_server_live(self) -> bool:\n",
    "        \"\"\"æª¢æŸ¥æœå‹™å™¨æ˜¯å¦å­˜æ´»\"\"\"\n",
    "        if not self.client:\n",
    "            return False\n",
    "        try:\n",
    "            return self.client.is_server_live()\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def is_server_ready(self) -> bool:\n",
    "        \"\"\"æª¢æŸ¥æœå‹™å™¨æ˜¯å¦å°±ç·’\"\"\"\n",
    "        if not self.client:\n",
    "            return False\n",
    "        try:\n",
    "            return self.client.is_server_ready()\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def generate_text(self, \n",
    "                     model_name: str,\n",
    "                     prompts: List[str],\n",
    "                     sampling_params: Optional[Dict[str, Any]] = None) -> List[str]:\n",
    "        \"\"\"ç”Ÿæˆæ–‡æœ¬\"\"\"\n",
    "        if not self.client:\n",
    "            logger.error(\"Triton Client ä¸å¯ç”¨\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # æº–å‚™è¼¸å…¥\n",
    "            inputs = []\n",
    "            \n",
    "            # æ–‡æœ¬è¼¸å…¥\n",
    "            text_input = httpclient.InferInput(\n",
    "                \"text_input\", \n",
    "                [len(prompts)], \n",
    "                \"BYTES\"\n",
    "            )\n",
    "            text_input.set_data_from_numpy(\n",
    "                np.array([prompt.encode('utf-8') for prompt in prompts], dtype=object)\n",
    "            )\n",
    "            inputs.append(text_input)\n",
    "            \n",
    "            # Sampling åƒæ•¸\n",
    "            if sampling_params:\n",
    "                sampling_input = httpclient.InferInput(\n",
    "                    \"sampling_parameters\", \n",
    "                    [1], \n",
    "                    \"BYTES\"\n",
    "                )\n",
    "                sampling_json = json.dumps(sampling_params).encode('utf-8')\n",
    "                sampling_input.set_data_from_numpy(np.array([sampling_json], dtype=object))\n",
    "                inputs.append(sampling_input)\n",
    "            \n",
    "            # æº–å‚™è¼¸å‡º\n",
    "            outputs = [\n",
    "                httpclient.InferRequestedOutput(\"text_output\")\n",
    "            ]\n",
    "            \n",
    "            # åŸ·è¡Œæ¨ç†\n",
    "            response = self.client.infer(\n",
    "                model_name=model_name,\n",
    "                inputs=inputs,\n",
    "                outputs=outputs\n",
    "            )\n",
    "            \n",
    "            # æå–çµæœ\n",
    "            output_data = response.as_numpy(\"text_output\")\n",
    "            results = [text.decode('utf-8') for text in output_data]\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"æ–‡æœ¬ç”Ÿæˆå¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def benchmark_generation(self, \n",
    "                           model_name: str,\n",
    "                           prompts: List[str],\n",
    "                           num_runs: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"æ€§èƒ½åŸºæº–æ¸¬è©¦\"\"\"\n",
    "        if not self.is_server_ready():\n",
    "            logger.error(\"æœå‹™å™¨æœªå°±ç·’\")\n",
    "            return {}\n",
    "        \n",
    "        latencies = []\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            results = self.generate_text(model_name, prompts)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if results:\n",
    "                latency = end_time - start_time\n",
    "                latencies.append(latency)\n",
    "                logger.info(f\"é‹è¡Œ {i+1}/{num_runs}: {latency:.3f}s\")\n",
    "            else:\n",
    "                logger.warning(f\"é‹è¡Œ {i+1} å¤±æ•—\")\n",
    "        \n",
    "        if latencies:\n",
    "            return {\n",
    "                \"avg_latency\": np.mean(latencies),\n",
    "                \"min_latency\": np.min(latencies),\n",
    "                \"max_latency\": np.max(latencies),\n",
    "                \"std_latency\": np.std(latencies),\n",
    "                \"throughput\": len(prompts) / np.mean(latencies)\n",
    "            }\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "# å‰µå»ºå®¢æˆ¶ç«¯ï¼ˆåƒ…åœ¨ Triton Client å¯ç”¨æ™‚ï¼‰\n",
    "if triton_client_available:\n",
    "    vllm_client = vLLMTritonClient()\n",
    "    print(\"âœ… vLLM Triton å®¢æˆ¶ç«¯å·²å‰µå»º\")\nelse:\n",
    "    vllm_client = None\n",
    "    print(\"âŒ Triton Client ä¸å¯ç”¨ï¼Œç„¡æ³•å‰µå»ºå®¢æˆ¶ç«¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬æ¸¬è©¦ï¼ˆå¦‚æœå¯¦éš›æœå‹™å™¨ä¸å¯ç”¨ï¼‰\n",
    "def simulate_vllm_inference():\n",
    "    \"\"\"æ¨¡æ“¬ vLLM æ¨ç†æ¸¬è©¦\"\"\"\n",
    "    test_prompts = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Explain machine learning briefly.\",\n",
    "        \"Tell me about Python programming.\",\n",
    "        \"How does neural network work?\"\n",
    "    ]\n",
    "    \n",
    "    sampling_params = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ¤– vLLM Backend æ¨ç†æ¸¬è©¦\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æª¢æŸ¥å¯¦éš›æœå‹™å™¨ç‹€æ…‹\n",
    "    if vllm_client and vllm_client.is_server_ready():\n",
    "        print(\"âœ… Triton Server é‹è¡Œä¸­ï¼ŒåŸ·è¡Œå¯¦éš›æ¨ç†\")\n",
    "        \n",
    "        # åŸ·è¡Œå¯¦éš›æ¨ç†\n",
    "        results = vllm_client.generate_text(\n",
    "            model_name=model_name,\n",
    "            prompts=test_prompts[:2],  # é™åˆ¶æ¸¬è©¦æ•¸é‡\n",
    "            sampling_params=sampling_params\n",
    "        )\n",
    "        \n",
    "        for i, (prompt, result) in enumerate(zip(test_prompts[:2], results)):\n",
    "            print(f\"\\næ¸¬è©¦ {i+1}:\")\n",
    "            print(f\"è¼¸å…¥: {prompt}\")\n",
    "            print(f\"è¼¸å‡º: {result}\")\n",
    "        \n",
    "        # æ€§èƒ½æ¸¬è©¦\n",
    "        print(\"\\nğŸ“Š æ€§èƒ½åŸºæº–æ¸¬è©¦\")\n",
    "        benchmark_results = vllm_client.benchmark_generation(\n",
    "            model_name=model_name,\n",
    "            prompts=test_prompts[:1],\n",
    "            num_runs=3\n",
    "        )\n",
    "        \n",
    "        for metric, value in benchmark_results.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âš ï¸  Triton Server æœªé‹è¡Œï¼Œå±•ç¤ºæ¨¡æ“¬çµæœ\")\n",
    "        \n",
    "        # æ¨¡æ“¬çµæœ\n",
    "        simulated_results = [\n",
    "            \"I'm doing well, thank you for asking! How can I help you today?\",\n",
    "            \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "            \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "            \"Python is a versatile programming language popular in AI and web development.\",\n",
    "            \"Neural networks are computing systems inspired by biological neural networks.\"\n",
    "        ]\n",
    "        \n",
    "        for i, (prompt, result) in enumerate(zip(test_prompts, simulated_results)):\n",
    "            print(f\"\\næ¨¡æ“¬æ¸¬è©¦ {i+1}:\")\n",
    "            print(f\"è¼¸å…¥: {prompt}\")\n",
    "            print(f\"è¼¸å‡º: {result}\")\n",
    "        \n",
    "        # æ¨¡æ“¬æ€§èƒ½æŒ‡æ¨™\n",
    "        print(\"\\nğŸ“Š æ¨¡æ“¬æ€§èƒ½æŒ‡æ¨™:\")\n",
    "        mock_metrics = {\n",
    "            \"avg_latency\": 0.234,\n",
    "            \"min_latency\": 0.201,\n",
    "            \"max_latency\": 0.267,\n",
    "            \"std_latency\": 0.024,\n",
    "            \"throughput\": 4.27\n",
    "        }\n",
    "        \n",
    "        for metric, value in mock_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n# åŸ·è¡Œæ¸¬è©¦\nsimulate_vllm_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ PagedAttention é…ç½®èˆ‡å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedAttentionOptimizer:\n",
    "    \"\"\"PagedAttention å„ªåŒ–é…ç½®å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpu_memory = torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else 0\n",
    "        self.available_memory = self.gpu_memory * 0.9  # ä¿ç•™ 10% ç·©è¡\n",
    "    \n",
    "    def calculate_optimal_block_size(self, \n",
    "                                   model_size_gb: float,\n",
    "                                   max_seq_len: int = 2048,\n",
    "                                   head_dim: int = 128) -> Dict[str, Any]:\n",
    "        \"\"\"è¨ˆç®—æœ€ä½³å¡Šå¤§å°é…ç½®\"\"\"\n",
    "        \n",
    "        # ä¼°ç®—æ¨¡å‹è¨˜æ†¶é«”éœ€æ±‚ï¼ˆGB to bytesï¼‰\n",
    "        model_memory = model_size_gb * 1e9\n",
    "        \n",
    "        # è¨ˆç®—å¯ç”¨æ–¼ KV cache çš„è¨˜æ†¶é«”\n",
    "        kv_cache_memory = self.available_memory - model_memory\n",
    "        \n",
    "        # è¨ˆç®—æ¯å€‹ token çš„ KV cache å¤§å°\n",
    "        # (key + value) * head_dim * num_layers * dtype_size\n",
    "        # å‡è¨­ FP16 (2 bytes) å’Œå¸¸è¦‹çš„å±¤æ•¸\n",
    "        estimated_layers = max(12, int(model_size_gb * 4))  # ç²—ç•¥ä¼°è¨ˆ\n",
    "        kv_size_per_token = 2 * head_dim * estimated_layers * 2  # bytes\n",
    "        \n",
    "        # è¨ˆç®—å»ºè­°çš„å¡Šå¤§å°\n",
    "        target_blocks = 1000  # ç›®æ¨™å¡Šæ•¸é‡\n",
    "        block_size = min(16, max(1, int((kv_cache_memory / target_blocks) / kv_size_per_token)))\n",
    "        \n",
    "        # è¨ˆç®—å…¶ä»–åƒæ•¸\n",
    "        max_num_blocks = int(kv_cache_memory / (block_size * kv_size_per_token))\n",
    "        max_concurrent_sequences = max_num_blocks * block_size // max_seq_len\n",
    "        \n",
    "        return {\n",
    "            \"block_size\": block_size,\n",
    "            \"max_num_blocks\": max_num_blocks,\n",
    "            \"max_concurrent_sequences\": max_concurrent_sequences,\n",
    "            \"estimated_memory_usage\": {\n",
    "                \"model_memory_gb\": model_memory / 1e9,\n",
    "                \"kv_cache_memory_gb\": kv_cache_memory / 1e9,\n",
    "                \"total_memory_gb\": self.gpu_memory / 1e9\n",
    "            },\n",
    "            \"recommendations\": self._generate_recommendations(block_size, max_concurrent_sequences)\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendations(self, block_size: int, max_concurrent: int) -> List[str]:\n",
    "        \"\"\"ç”Ÿæˆå„ªåŒ–å»ºè­°\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if block_size < 4:\n",
    "            recommendations.append(\"å¡Šå¤§å°è¼ƒå°ï¼Œè€ƒæ…®æ¸›å°‘æ¨¡å‹å¤§å°æˆ–å¢åŠ  GPU è¨˜æ†¶é«”\")\n",
    "        elif block_size > 32:\n",
    "            recommendations.append(\"å¡Šå¤§å°è¼ƒå¤§ï¼Œå¯èƒ½å°è‡´è¨˜æ†¶é«”ç¢ç‰‡ï¼Œå»ºè­°èª¿æ•´\")\n",
    "        \n",
    "        if max_concurrent < 4:\n",
    "            recommendations.append(\"ä¸¦ç™¼åºåˆ—æ•¸è¼ƒå°‘ï¼Œè€ƒæ…®å„ªåŒ–è¨˜æ†¶é«”é…ç½®\")\n",
    "        elif max_concurrent > 100:\n",
    "            recommendations.append(\"ä¸¦ç™¼åºåˆ—æ•¸å¾ˆé«˜ï¼Œç¢ºä¿æœ‰è¶³å¤ çš„è¨ˆç®—è³‡æº\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"é…ç½®çœ‹èµ·ä¾†åˆç†ï¼Œå¯ä»¥é€²è¡Œé€²ä¸€æ­¥çš„æ€§èƒ½æ¸¬è©¦\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_optimized_vllm_config(self, \n",
    "                                   base_config: Dict[str, Any],\n",
    "                                   model_size_gb: float) -> Dict[str, Any]:\n",
    "        \"\"\"å‰µå»ºå„ªåŒ–çš„ vLLM é…ç½®\"\"\"\n",
    "        \n",
    "        # è¨ˆç®—æœ€ä½³åƒæ•¸\n",
    "        optimization = self.calculate_optimal_block_size(model_size_gb)\n",
    "        \n",
    "        # æ›´æ–°é…ç½®\n",
    "        optimized_config = base_config.copy()\n",
    "        \n",
    "        # æ·»åŠ  PagedAttention å„ªåŒ–åƒæ•¸\n",
    "        optimized_params = {\n",
    "            \"block_size\": {\"string_value\": str(optimization[\"block_size\"])},\n",
    "            \"max_num_seqs\": {\"string_value\": str(min(256, optimization[\"max_concurrent_sequences\"]))},\n",
    "            \"gpu_memory_utilization\": {\"string_value\": \"0.85\"},\n",
    "            \"swap_space\": {\"string_value\": \"4\"},  # GB\n",
    "            \"enforce_eager\": {\"string_value\": \"false\"},\n",
    "            \"enable_chunked_prefill\": {\"string_value\": \"true\"}\n",
    "        }\n",
    "        \n",
    "        # åˆä½µåƒæ•¸\n",
    "        if \"parameters\" not in optimized_config:\n",
    "            optimized_config[\"parameters\"] = {}\n",
    "        \n",
    "        optimized_config[\"parameters\"].update(optimized_params)\n",
    "        \n",
    "        return optimized_config, optimization\n",
    "\n",
    "# å‰µå»º PagedAttention å„ªåŒ–å™¨\n",
    "paged_optimizer = PagedAttentionOptimizer()\n",
    "print(\"ğŸ”§ PagedAttention å„ªåŒ–å™¨å·²å‰µå»º\")\n",
    "\n",
    "# è¨ˆç®—ç•¶å‰æ¨¡å‹çš„æœ€ä½³é…ç½®\n",
    "model_sizes = {\n",
    "    \"gpt2\": 0.5,\n",
    "    \"microsoft/DialoGPT-medium\": 1.5,\n",
    "    \"microsoft/DialoGPT-large\": 3.0\n",
    "}\n",
    "\n",
    "current_model_size = model_sizes.get(model_choice, 1.0)\n",
    "optimization_result = paged_optimizer.calculate_optimal_block_size(current_model_size)\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹: {model_choice} ({current_model_size}GB)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"å»ºè­°å¡Šå¤§å°: {optimization_result['block_size']}\")\n",
    "print(f\"æœ€å¤§å¡Šæ•¸é‡: {optimization_result['max_num_blocks']}\")\n",
    "print(f\"æœ€å¤§ä¸¦ç™¼åºåˆ—: {optimization_result['max_concurrent_sequences']}\")\n",
    "\n",
    "print(\"\\nğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨ä¼°ç®—:\")\n",
    "for key, value in optimization_result['estimated_memory_usage'].items():\n",
    "    print(f\"{key}: {value:.2f}GB\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å„ªåŒ–å»ºè­°:\")\n",
    "for i, rec in enumerate(optimization_result['recommendations'], 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ å„ªåŒ–é…ç½®éƒ¨ç½²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºå„ªåŒ–å¾Œçš„æ¨¡å‹é…ç½®\n",
    "optimized_model_name = \"vllm_optimized_model\"\n",
    "\n",
    "# ç²å–åŸºç¤é…ç½®\n",
    "base_config = vllm_repo.create_vllm_model_config(\n",
    "    model_name=optimized_model_name,\n",
    "    hf_model_name=model_choice,\n",
    "    max_model_len=max_model_len,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# æ‡‰ç”¨ PagedAttention å„ªåŒ–\n",
    "optimized_config, optimization_details = paged_optimizer.create_optimized_vllm_config(\n",
    "    base_config, current_model_size\n",
    ")\n",
    "\n",
    "# å¯«å…¥å„ªåŒ–é…ç½®\n",
    "optimized_config_path = vllm_repo.write_config_pbtxt(optimized_config, optimized_model_name)\n",
    "optimized_model_py_path = vllm_repo.create_vllm_model_py(optimized_model_name)\n",
    "\n",
    "print(f\"âœ… å„ªåŒ–é…ç½®å·²å‰µå»º: {optimized_config_path}\")\n",
    "print(f\"âœ… å„ªåŒ– model.py å·²å‰µå»º: {optimized_model_py_path}\")\n",
    "\n",
    "# é¡¯ç¤ºé…ç½®å°æ¯”\n",
    "def compare_configs(base_config: Dict, optimized_config: Dict):\n",
    "    \"\"\"å°æ¯”é…ç½®å·®ç•°\"\"\"\n",
    "    print(\"\\nâš–ï¸  é…ç½®å°æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    base_params = base_config.get(\"parameters\", {})\n",
    "    opt_params = optimized_config.get(\"parameters\", {})\n",
    "    \n",
    "    all_keys = set(base_params.keys()) | set(opt_params.keys())\n",
    "    \n",
    "    for key in sorted(all_keys):\n",
    "        base_val = base_params.get(key, {}).get(\"string_value\", \"æœªè¨­å®š\")\n",
    "        opt_val = opt_params.get(key, {}).get(\"string_value\", \"æœªè¨­å®š\")\n",
    "        \n",
    "        if base_val != opt_val:\n",
    "            print(f\"{key:25} | {base_val:15} -> {opt_val:15}\")\n",
    "        else:\n",
    "            print(f\"{key:25} | {base_val:15}   (ç›¸åŒ)\")\n",
    "\n",
    "compare_configs(base_config, optimized_config)\n",
    "\n",
    "# é¡¯ç¤ºæœ€çµ‚å€‰åº«çµæ§‹\n",
    "print(f\"\\nğŸ“‚ æœ€çµ‚æ¨¡å‹å€‰åº«çµæ§‹:\")\n",
    "show_repository_structure(vllm_repo.repository_path, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ€§èƒ½ç›£æ§èˆ‡åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMPerformanceMonitor:\n",
    "    \"\"\"vLLM æ€§èƒ½ç›£æ§å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics_history = []\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def collect_system_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"æ”¶é›†ç³»çµ±æŒ‡æ¨™\"\"\"\n",
    "        metrics = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"cpu_percent\": psutil.cpu_percent(),\n",
    "            \"memory_percent\": psutil.virtual_memory().percent,\n",
    "            \"memory_used_gb\": psutil.virtual_memory().used / 1e9\n",
    "        }\n",
    "        \n",
    "        # GPU æŒ‡æ¨™\n",
    "        if torch.cuda.is_available():\n",
    "            metrics.update({\n",
    "                \"gpu_memory_used\": torch.cuda.memory_allocated() / 1e9,\n",
    "                \"gpu_memory_cached\": torch.cuda.memory_reserved() / 1e9,\n",
    "                \"gpu_utilization\": self._get_gpu_utilization()\n",
    "            })\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _get_gpu_utilization(self) -> float:\n",
    "        \"\"\"ç²å– GPU ä½¿ç”¨ç‡ï¼ˆç°¡åŒ–ç‰ˆï¼‰\"\"\"\n",
    "        try:\n",
    "            # ä½¿ç”¨ nvidia-ml-py æœƒæ›´æº–ç¢ºï¼Œé€™è£¡ç”¨ç°¡åŒ–ä¼°ç®—\n",
    "            allocated = torch.cuda.memory_allocated()\n",
    "            total = torch.cuda.get_device_properties(0).total_memory\n",
    "            return (allocated / total) * 100\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def log_inference_metrics(self, \n",
    "                            batch_size: int,\n",
    "                            sequence_length: int,\n",
    "                            inference_time: float,\n",
    "                            tokens_generated: int):\n",
    "        \"\"\"è¨˜éŒ„æ¨ç†æŒ‡æ¨™\"\"\"\n",
    "        system_metrics = self.collect_system_metrics()\n",
    "        \n",
    "        inference_metrics = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"inference_time\": inference_time,\n",
    "            \"tokens_generated\": tokens_generated,\n",
    "            \"tokens_per_second\": tokens_generated / inference_time if inference_time > 0 else 0,\n",
    "            \"throughput\": batch_size / inference_time if inference_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        combined_metrics = {**system_metrics, **inference_metrics}\n",
    "        self.metrics_history.append(combined_metrics)\n",
    "        \n",
    "        return combined_metrics\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–æ€§èƒ½ç¸½çµ\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.metrics_history)\n",
    "        \n",
    "        summary = {\n",
    "            \"total_inferences\": len(self.metrics_history),\n",
    "            \"avg_inference_time\": df[\"inference_time\"].mean(),\n",
    "            \"avg_tokens_per_second\": df[\"tokens_per_second\"].mean(),\n",
    "            \"avg_throughput\": df[\"throughput\"].mean(),\n",
    "            \"peak_memory_usage\": df[\"memory_used_gb\"].max(),\n",
    "            \"avg_cpu_usage\": df[\"cpu_percent\"].mean()\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            summary.update({\n",
    "                \"peak_gpu_memory\": df[\"gpu_memory_used\"].max(),\n",
    "                \"avg_gpu_utilization\": df[\"gpu_utilization\"].mean()\n",
    "            })\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_performance_charts(self):\n",
    "        \"\"\"ç¹ªè£½æ€§èƒ½åœ–è¡¨\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"ç„¡æ€§èƒ½æ•¸æ“šå¯è¦–åŒ–\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.metrics_history)\n",
    "        df['relative_time'] = df['timestamp'] - df['timestamp'].iloc[0]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('vLLM Backend æ€§èƒ½ç›£æ§', fontsize=16)\n",
    "        \n",
    "        # æ¨ç†å»¶é²\n",
    "        axes[0, 0].plot(df['relative_time'], df['inference_time'], 'b-', linewidth=2)\n",
    "        axes[0, 0].set_title('æ¨ç†å»¶é² (ç§’)')\n",
    "        axes[0, 0].set_xlabel('æ™‚é–“ (ç§’)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ååé‡\n",
    "        axes[0, 1].plot(df['relative_time'], df['tokens_per_second'], 'g-', linewidth=2)\n",
    "        axes[0, 1].set_title('Token ç”Ÿæˆé€Ÿç‡ (tokens/sec)')\n",
    "        axes[0, 1].set_xlabel('æ™‚é–“ (ç§’)')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # è¨˜æ†¶é«”ä½¿ç”¨\n",
    "        axes[1, 0].plot(df['relative_time'], df['memory_used_gb'], 'r-', linewidth=2, label='CPU è¨˜æ†¶é«”')\n",
    "        if torch.cuda.is_available() and 'gpu_memory_used' in df.columns:\n",
    "            axes[1, 0].plot(df['relative_time'], df['gpu_memory_used'], 'orange', linewidth=2, label='GPU è¨˜æ†¶é«”')\n",
    "        axes[1, 0].set_title('è¨˜æ†¶é«”ä½¿ç”¨ (GB)')\n",
    "        axes[1, 0].set_xlabel('æ™‚é–“ (ç§’)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # CPU/GPU ä½¿ç”¨ç‡\n",
    "        axes[1, 1].plot(df['relative_time'], df['cpu_percent'], 'purple', linewidth=2, label='CPU %')\n",
    "        if torch.cuda.is_available() and 'gpu_utilization' in df.columns:\n",
    "            axes[1, 1].plot(df['relative_time'], df['gpu_utilization'], 'red', linewidth=2, label='GPU %')\n",
    "        axes[1, 1].set_title('è³‡æºä½¿ç”¨ç‡ (%)')\n",
    "        axes[1, 1].set_xlabel('æ™‚é–“ (ç§’)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# å‰µå»ºæ€§èƒ½ç›£æ§å™¨\n",
    "monitor = vLLMPerformanceMonitor()\n",
    "print(\"ğŸ“Š vLLM æ€§èƒ½ç›£æ§å™¨å·²å‰µå»º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬æ€§èƒ½ç›£æ§æ•¸æ“š\n",
    "def simulate_performance_monitoring():\n",
    "    \"\"\"æ¨¡æ“¬æ€§èƒ½ç›£æ§éç¨‹\"\"\"\n",
    "    print(\"ğŸ”„ æ¨¡æ“¬ vLLM Backend æ€§èƒ½ç›£æ§...\")\n",
    "    \n",
    "    # æ¨¡æ“¬å¤šå€‹æ¨ç†è«‹æ±‚\n",
    "    test_scenarios = [\n",
    "        {\"batch_size\": 1, \"seq_len\": 50, \"tokens\": 25, \"time\": 0.15},\n",
    "        {\"batch_size\": 2, \"seq_len\": 100, \"tokens\": 60, \"time\": 0.28},\n",
    "        {\"batch_size\": 4, \"seq_len\": 150, \"tokens\": 120, \"time\": 0.45},\n",
    "        {\"batch_size\": 1, \"seq_len\": 200, \"tokens\": 80, \"time\": 0.32},\n",
    "        {\"batch_size\": 3, \"seq_len\": 75, \"tokens\": 90, \"time\": 0.25},\n",
    "        {\"batch_size\": 2, \"seq_len\": 120, \"tokens\": 70, \"time\": 0.22},\n",
    "        {\"batch_size\": 1, \"seq_len\": 300, \"tokens\": 150, \"time\": 0.65}\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios):\n",
    "        # æ¨¡æ“¬æ¨ç†å»¶é²\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # è¨˜éŒ„æŒ‡æ¨™\n",
    "        metrics = monitor.log_inference_metrics(\n",
    "            batch_size=scenario[\"batch_size\"],\n",
    "            sequence_length=scenario[\"seq_len\"],\n",
    "            inference_time=scenario[\"time\"],\n",
    "            tokens_generated=scenario[\"tokens\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"æ¨ç† {i+1}: {scenario['batch_size']}x{scenario['seq_len']} -> {metrics['tokens_per_second']:.1f} tokens/s\")\n",
    "    \n",
    "    # ç²å–æ€§èƒ½ç¸½çµ\n",
    "    summary = monitor.get_performance_summary()\n",
    "    \n",
    "    print(\"\\nğŸ“Š æ€§èƒ½ç¸½çµ\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key:25}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"{key:25}: {value}\")\n",
    "    \n",
    "    # ç¹ªè£½æ€§èƒ½åœ–è¡¨\n",
    "    print(\"\\nğŸ“ˆ æ€§èƒ½è¶¨å‹¢åœ–è¡¨\")\n",
    "    monitor.plot_performance_charts()\n",
    "\n# åŸ·è¡Œæ€§èƒ½ç›£æ§æ¨¡æ“¬\nsimulate_performance_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ éƒ¨ç½²è…³æœ¬èˆ‡æœ€ä½³å¯¦è¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vLLMDeploymentManager:\n",
    "    \"\"\"vLLM éƒ¨ç½²ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, repository_path: str):\n",
    "        self.repository_path = Path(repository_path)\n",
    "        self.deployment_scripts = {}\n",
    "    \n",
    "    def generate_docker_compose(self, \n",
    "                               model_name: str,\n",
    "                               gpu_ids: str = \"all\") -> str:\n",
    "        \"\"\"ç”Ÿæˆ Docker Compose é…ç½®\"\"\"\n",
    "        \n",
    "        compose_content = f'''\nversion: '3.8'\n\nservices:\n  triton-vllm:\n    image: nvcr.io/nvidia/tritonserver:23.10-vllm-python-py3\n    ports:\n      - \"8000:8000\"  # HTTP\n      - \"8001:8001\"  # GRPC\n      - \"8002:8002\"  # Metrics\n    volumes:\n      - ./model_repository:/models\n      - ./logs:/logs\n    environment:\n      - CUDA_VISIBLE_DEVICES={gpu_ids}\n      - TRITON_LOG_LEVEL=INFO\n    command: >\n      tritonserver\n      --model-repository=/models\n      --http-port=8000\n      --grpc-port=8001\n      --metrics-port=8002\n      --log-verbose=1\n      --backend-directory=/opt/tritonserver/backends\n      --backend-config=vllm,max_batch_size=256\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/v2/health/ready\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s\n\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n    restart: unless-stopped\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - grafana-storage:/var/lib/grafana\n      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards\n      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    restart: unless-stopped\n\nvolumes:\n  grafana-storage:\n\nnetworks:\n  default:\n    name: triton-vllm-network\n'''\n        \n        return compose_content\n    \n    def generate_kubernetes_deployment(self, \n                                     model_name: str,\n                                     replicas: int = 1,\n                                     gpu_memory: str = \"16Gi\") -> str:\n",
    "        \"\"\"ç”Ÿæˆ Kubernetes éƒ¨ç½²é…ç½®\"\"\"\n",
    "        \n",
    "        k8s_content = f'''\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: triton-vllm-{model_name.lower().replace('_', '-')}\n  labels:\n    app: triton-vllm\n    model: {model_name}\nspec:\n  replicas: {replicas}\n  selector:\n    matchLabels:\n      app: triton-vllm\n      model: {model_name}\n  template:\n    metadata:\n      labels:\n        app: triton-vllm\n        model: {model_name}\n    spec:\n      containers:\n      - name: triton-vllm\n        image: nvcr.io/nvidia/tritonserver:23.10-vllm-python-py3\n        ports:\n        - containerPort: 8000\n          name: http\n        - containerPort: 8001\n          name: grpc\n        - containerPort: 8002\n          name: metrics\n        env:\n        - name: CUDA_VISIBLE_DEVICES\n          value: \"0\"\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n            memory: {gpu_memory}\n          requests:\n            nvidia.com/gpu: 1\n            memory: \"8Gi\"\n        volumeMounts:\n        - name: model-repository\n          mountPath: /models\n        command:\n        - tritonserver\n        - --model-repository=/models\n        - --http-port=8000\n        - --grpc-port=8001\n        - --metrics-port=8002\n        - --log-verbose=1\n        - --backend-config=vllm,max_batch_size=256\n        livenessProbe:\n          httpGet:\n            path: /v2/health/live\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /v2/health/ready\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 15\n      volumes:\n      - name: model-repository\n        configMap:\n          name: {model_name.lower().replace('_', '-')}-config\n      nodeSelector:\n        accelerator: nvidia-tesla-v100  # æ ¹æ“šå¯¦éš› GPU èª¿æ•´\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: triton-vllm-service\nspec:\n  selector:\n    app: triton-vllm\n  ports:\n  - name: http\n    port: 8000\n    targetPort: 8000\n  - name: grpc\n    port: 8001\n    targetPort: 8001\n  - name: metrics\n    port: 8002\n    targetPort: 8002\n  type: LoadBalancer\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {model_name.lower().replace('_', '-')}-config\ndata:\n  # é€™è£¡æ‡‰è©²åŒ…å«å¯¦éš›çš„æ¨¡å‹é…ç½®æ–‡ä»¶\n  config.pbtxt: |\n    # æ¨¡å‹é…ç½®å…§å®¹\n'''\n        \n        return k8s_content\n    \n    def generate_monitoring_config(self) -> Dict[str, str]:\n        \"\"\"ç”Ÿæˆç›£æ§é…ç½®\"\"\"\n",
    "        \n",
    "        # Prometheus é…ç½®\n",
    "        prometheus_config = '''\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'triton-server'\n    static_configs:\n      - targets: ['triton-vllm:8002']\n    metrics_path: '/metrics'\n    scrape_interval: 10s\n    \n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n    scrape_interval: 15s\n'''\n        \n        # Grafana Dashboard é…ç½®\n        grafana_dashboard = '''\n{\n  \"dashboard\": {\n    \"id\": null,\n    \"title\": \"Triton vLLM Performance\",\n    \"tags\": [\"triton\", \"vllm\", \"inference\"],\n    \"timezone\": \"browser\",\n    \"panels\": [\n      {\n        \"id\": 1,\n        \"title\": \"Request Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(nv_inference_request_success_total[5m])\",\n            \"legendFormat\": \"Successful Requests/sec\"\n          }\n        ],\n        \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0}\n      },\n      {\n        \"id\": 2,\n        \"title\": \"Inference Latency\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, rate(nv_inference_request_duration_us_bucket[5m]))\",\n            \"legendFormat\": \"95th Percentile\"\n          },\n          {\n            \"expr\": \"histogram_quantile(0.50, rate(nv_inference_request_duration_us_bucket[5m]))\",\n            \"legendFormat\": \"50th Percentile\"\n          }\n        ],\n        \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 0}\n      },\n      {\n        \"id\": 3,\n        \"title\": \"GPU Memory Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"nv_gpu_memory_used_bytes / nv_gpu_memory_total_bytes * 100\",\n            \"legendFormat\": \"GPU Memory %\"\n          }\n        ],\n        \"gridPos\": {\"h\": 8, \"w\": 24, \"x\": 0, \"y\": 8}\n      }\n    ],\n    \"time\": {\n      \"from\": \"now-1h\",\n      \"to\": \"now\"\n    },\n    \"refresh\": \"10s\"\n  }\n}\n'''\n        \n        return {\n            \"prometheus.yml\": prometheus_config,\n            \"grafana_dashboard.json\": grafana_dashboard\n        }\n    \n    def generate_deployment_scripts(self, model_name: str) -> Dict[str, str]:\n        \"\"\"ç”Ÿæˆæ‰€æœ‰éƒ¨ç½²è…³æœ¬\"\"\"\n",
    "        scripts = {}\n",
    "        \n",
    "        # Docker Compose\n",
    "        scripts[\"docker-compose.yml\"] = self.generate_docker_compose(model_name)\n",
    "        \n",
    "        # Kubernetes\n",
    "        scripts[\"k8s-deployment.yaml\"] = self.generate_kubernetes_deployment(model_name)\n",
    "        \n",
    "        # ç›£æ§é…ç½®\n",
    "        scripts.update(self.generate_monitoring_config())\n",
    "        \n",
    "        # å•Ÿå‹•è…³æœ¬\n",
    "        scripts[\"start.sh\"] = f'''\n#!/bin/bash\n\nset -e\n\necho \"ğŸš€ å•Ÿå‹• Triton vLLM æœå‹™\"\n\n# æª¢æŸ¥ Docker\nif ! command -v docker &> /dev/null; then\n    echo \"âŒ Docker æœªå®‰è£\"\n    exit 1\nfi\n\n# æª¢æŸ¥ NVIDIA Docker\nif ! docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi &> /dev/null; then\n    echo \"âŒ NVIDIA Docker ä¸å¯ç”¨\"\n    exit 1\nfi\n\n# å‰µå»ºå¿…è¦ç›®éŒ„\nmkdir -p logs\nmkdir -p monitoring/grafana/dashboards\nmkdir -p monitoring/grafana/datasources\n\n# è¤‡è£½ç›£æ§é…ç½®\ncp prometheus.yml monitoring/\ncp grafana_dashboard.json monitoring/grafana/dashboards/\n\n# å•Ÿå‹•æœå‹™\necho \"ğŸ³ å•Ÿå‹• Docker Compose æœå‹™...\"\ndocker-compose up -d\n\n# ç­‰å¾…æœå‹™å°±ç·’\necho \"â³ ç­‰å¾…æœå‹™å•Ÿå‹•...\"\nsleep 30\n\n# å¥åº·æª¢æŸ¥\necho \"ğŸ” æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹\"\nif curl -f http://localhost:8000/v2/health/ready; then\n    echo \"âœ… Triton Server å°±ç·’\"\nelse\n    echo \"âŒ Triton Server æœªå°±ç·’\"\n    docker-compose logs triton-vllm\n    exit 1\nfi\n\necho \"ğŸ‰ éƒ¨ç½²å®Œæˆï¼\"\necho \"   - Triton Server: http://localhost:8000\"\necho \"   - Prometheus: http://localhost:9090\"\necho \"   - Grafana: http://localhost:3000 (admin/admin)\"\n'''\n        \n        # åœæ­¢è…³æœ¬\n        scripts[\"stop.sh\"] = '''\n#!/bin/bash\n\necho \"ğŸ›‘ åœæ­¢ Triton vLLM æœå‹™\"\ndocker-compose down\necho \"âœ… æœå‹™å·²åœæ­¢\"\n'''\n        \n        return scripts\n    \n    def save_deployment_files(self, model_name: str, output_dir: str = \"./deployment\"):\n        \"\"\"ä¿å­˜éƒ¨ç½²æ–‡ä»¶\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n        \n        scripts = self.generate_deployment_scripts(model_name)\n        \n        for filename, content in scripts.items():\n            file_path = output_path / filename\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.write(content)\n            \n            # ç‚º shell è…³æœ¬æ·»åŠ åŸ·è¡Œæ¬Šé™\n            if filename.endswith('.sh'):\n                os.chmod(file_path, 0o755)\n        \n        logger.info(f\"éƒ¨ç½²æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path.absolute()}\")\n        return output_path\n\n# å‰µå»ºéƒ¨ç½²ç®¡ç†å™¨\ndeployment_manager = vLLMDeploymentManager(str(vllm_repo.repository_path))\nprint(\"ğŸš€ vLLM éƒ¨ç½²ç®¡ç†å™¨å·²å‰µå»º\")\n\n# ç”Ÿæˆéƒ¨ç½²æ–‡ä»¶\ndeployment_path = deployment_manager.save_deployment_files(optimized_model_name)\nprint(f\"âœ… éƒ¨ç½²æ–‡ä»¶å·²ç”Ÿæˆ: {deployment_path}\")\n\n# é¡¯ç¤ºç”Ÿæˆçš„æ–‡ä»¶\nprint(\"\\nğŸ“ ç”Ÿæˆçš„éƒ¨ç½²æ–‡ä»¶:\")\nfor file_path in sorted(deployment_path.iterdir()):\n    print(f\"  - {file_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š æœ€ä½³å¯¦è¸èˆ‡ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM Backend æœ€ä½³å¯¦è¸æŒ‡å—\n",
    "best_practices = {\n",
    "    \"é…ç½®å„ªåŒ–\": [\n",
    "        \"æ ¹æ“š GPU è¨˜æ†¶é«”å‹•æ…‹èª¿æ•´ block_size\",\n",
    "        \"è¨­ç½®åˆç†çš„ gpu_memory_utilization (0.85-0.9)\",\n",
    "        \"å•Ÿç”¨ PagedAttention ä»¥æé«˜è¨˜æ†¶é«”æ•ˆç‡\",\n",
    "        \"é…ç½® swap_space ä»¥è™•ç†è¨˜æ†¶é«”æº¢å‡º\",\n",
    "        \"ä½¿ç”¨ continuous batching æé«˜ååé‡\"\n",
    "    ],\n",
    "    \"æ€§èƒ½èª¿å„ª\": [\n",
    "        \"é¸æ“‡é©ç•¶çš„ tensor_parallel_size\",\n",
    "        \"å•Ÿç”¨ CUDA Graph ä»¥æ¸›å°‘ kernel å•Ÿå‹•é–‹éŠ·\",\n",
    "        \"ä½¿ç”¨æ··åˆç²¾åº¦ (FP16) åŠ é€Ÿæ¨ç†\",\n",
    "        \"é…ç½®å‹•æ…‹æ‰¹æ¬¡è™•ç†åƒæ•¸\",\n",
    "        \"ç›£æ§ KV cache ä½¿ç”¨æƒ…æ³\"\n",
    "    ],\n",
    "    \"éƒ¨ç½²ç­–ç•¥\": [\n",
    "        \"ä½¿ç”¨ Docker å®¹å™¨åŒ–éƒ¨ç½²\",\n",
    "        \"é…ç½®å¥åº·æª¢æŸ¥å’Œè‡ªå‹•é‡å•Ÿ\",\n",
    "        \"è¨­ç½®è² è¼‰å‡è¡¡å’Œæ•…éšœè½‰ç§»\",\n",
    "        \"å¯¦æ–½æ»¾å‹•æ›´æ–°ç­–ç•¥\",\n",
    "        \"é…ç½®è³‡æºé™åˆ¶å’Œè«‹æ±‚\"\n",
    "    ],\n",
    "    \"ç›£æ§é‹ç¶­\": [\n",
    "        \"ç›£æ§æ¨ç†å»¶é²å’Œååé‡\",\n",
    "        \"è¿½è¹¤ GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡\",\n",
    "        \"è¨­ç½®å‘Šè­¦é–¾å€¼\",\n",
    "        \"æ”¶é›†æ¥­å‹™æŒ‡æ¨™\",\n",
    "        \"å®šæœŸæ€§èƒ½åŸºæº–æ¸¬è©¦\"\n",
    "    ],\n",
    "    \"å®‰å…¨è€ƒæ…®\": [\n",
    "        \"é™åˆ¶æ¨¡å‹è¨ªå•æ¬Šé™\",\n",
    "        \"é©—è­‰è¼¸å…¥å…§å®¹\",\n",
    "        \"è¨­ç½®è«‹æ±‚é€Ÿç‡é™åˆ¶\",\n",
    "        \"åŠ å¯†æ•æ„Ÿé…ç½®\",\n",
    "        \"å®šæœŸå®‰å…¨å¯©è¨ˆ\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ vLLM Backend æœ€ä½³å¯¦è¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\nğŸ¯ {category}:\")\n",
    "    for i, practice in enumerate(practices, 1):\n",
    "        print(f\"   {i}. {practice}\")\n",
    "\n",
    "# å¯¦é©—ç¸½çµ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ Lab-2.3.3 ç¸½çµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_points = [\n",
    "    \"âœ… æˆåŠŸè¨­è¨ˆä¸¦å¯¦ç¾äº† vLLM Backend æ•´åˆæ¶æ§‹\",\n",
    "    \"âœ… æŒæ¡äº† PagedAttention å„ªåŒ–é…ç½®æŠ€è¡“\",\n",
    "    \"âœ… å»ºç«‹äº†å®Œæ•´çš„æ€§èƒ½ç›£æ§é«”ç³»\",\n",
    "    \"âœ… å‰µå»ºäº†ä¼æ¥­ç´šéƒ¨ç½²è‡ªå‹•åŒ–è…³æœ¬\",\n",
    "    \"âœ… ç†è§£äº† Triton + vLLM çš„æ··åˆéƒ¨ç½²ç­–ç•¥\"\n",
    "]\n",
    "\n",
    "for point in summary_points:\n",
    "    print(point)\n",
    "\n",
    "print(\"\\nğŸš€ ä¸‹ä¸€æ­¥å»ºè­°:\")\n",
    "next_steps = [\n",
    "    \"åœ¨å¯¦éš›ç’°å¢ƒä¸­éƒ¨ç½²å’Œæ¸¬è©¦ vLLM Backend\",\n",
    "    \"æ¢ç´¢å¤šæ¨¡å‹ä¸¦è¡Œéƒ¨ç½²ç­–ç•¥\",\n",
    "    \"å¯¦ç¾è‡ªå®šç¾© Python Backend (Lab-2.3.4)\",\n",
    "    \"é›†æˆ MLOps å·¥ä½œæµç¨‹\",\n",
    "    \"å„ªåŒ–å¤§è¦æ¨¡ç”Ÿç”¢éƒ¨ç½²\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸  å¯¦é©—å®Œæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"ğŸ“Š å¯¦é©—è©•ä¼°: vLLM Backend æ•´åˆ - ä¼æ¥­ç´šå°±ç·’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”š å¯¦é©—å®¤çµèª\n",
    "\n",
    "æœ¬å¯¦é©—å®¤æ·±å…¥æ¢è¨äº† **Triton + vLLM Backend** çš„æ•´åˆæŠ€è¡“ï¼Œæ¶µè“‹äº†:\n",
    "\n",
    "### ğŸ† æ ¸å¿ƒæˆå°±\n",
    "\n",
    "1. **æ¶æ§‹è¨­è¨ˆ**: æŒæ¡äº†ä¼æ¥­ç´š vLLM Backend æ¶æ§‹\n",
    "2. **æ€§èƒ½å„ªåŒ–**: å¯¦ç¾äº† PagedAttention è¨˜æ†¶é«”æ•ˆç‡æœ€å¤§åŒ–\n",
    "3. **ç›£æ§é«”ç³»**: å»ºç«‹äº†å®Œæ•´çš„æ€§èƒ½è¿½è¹¤å’Œåˆ†ææ¡†æ¶\n",
    "4. **éƒ¨ç½²è‡ªå‹•åŒ–**: å‰µå»ºäº†ç”Ÿç”¢å°±ç·’çš„éƒ¨ç½²ç®¡é“\n",
    "5. **æœ€ä½³å¯¦è¸**: ç¸½çµäº†ä¼æ¥­ç´š LLM æœå‹™é‹ç¶­ç¶“é©—\n",
    "\n",
    "### ğŸ¯ æŠ€èƒ½æå‡\n",
    "\n",
    "- âœ¨ **LLM æœå‹™åŒ–**: å¾æ¨¡å‹åˆ°æœå‹™çš„å®Œæ•´è½‰æ›\n",
    "- âœ¨ **è¨˜æ†¶é«”å„ªåŒ–**: PagedAttention æ ¸å¿ƒåŸç†èˆ‡å¯¦è¸\n",
    "- âœ¨ **ä¼æ¥­éƒ¨ç½²**: Docker/K8s å®¹å™¨åŒ–éƒ¨ç½²ç­–ç•¥\n",
    "- âœ¨ **æ€§èƒ½ç›£æ§**: å…¨æ–¹ä½æŒ‡æ¨™æ”¶é›†èˆ‡åˆ†æ\n",
    "- âœ¨ **é‹ç¶­è‡ªå‹•åŒ–**: DevOps èˆ‡ MLOps æ•´åˆ\n",
    "\n",
    "### ğŸš€ å¯¦æˆ°åƒ¹å€¼\n",
    "\n",
    "æ­¤å¯¦é©—å®¤æä¾›çš„æŠ€è¡“æ£§å’Œæ–¹æ³•è«–å¯ä»¥ç›´æ¥æ‡‰ç”¨æ–¼:\n",
    "- **å¤§è¦æ¨¡ LLM éƒ¨ç½²é …ç›®**\n",
    "- **é«˜ä½µç™¼æ¨ç†æœå‹™è¨­è¨ˆ**\n",
    "- **ä¼æ¥­ç´š AI å¹³å°å»ºè¨­**\n",
    "- **MLOps å·¥ç¨‹å¯¦æ–½**\n",
    "\n",
    "---\n",
    "\n",
    "*ä¸‹ä¸€å€‹å¯¦é©—å®¤: **04-Custom_Python_Backend** - è‡ªå®šç¾©æ¥­å‹™é‚è¼¯å¯¦ç¾*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}