# 大模型訓練技術 1.3 訓練優化與對齊 (Training Optimization & Alignment)

本教學模組深入探討大型語言模型 (LLM) 訓練優化與對齊技術，涵蓋從效率提升到模型對齊的完整知識體系。

**🚨 重要限制**: 本課程強調理論教學與演算法分析，適合單GPU環境學習。

| 層次 | 時間 | 學習目標 | 核心內容 | 產出 |
| :--- | :--- | :--- | :--- | :--- |
| **Fundamentals** | 基礎 | 掌握訓練優化基本技術與對齊原理 | 注意力優化與基礎對齊 | 掌握基本概念與實現原理 |
| **First Principles** | 原理 | 理解數學原理與計算複雜度分析 | 通訊複雜度與對齊理論 | 理解底層演算法設計原理 |
| **Body of Knowledge** | 實務 | 優化 LLM 訓練與對齊策略精通 | 主流技術 FlashAttention & RLHF | 具備演算法實現與調優能力 |

---

## 1. Fundamentals (基礎)

### 1.1 訓練優化動機

大型語言模型面臨的關鍵挑戰：

1. **記憶體瓶頸** Transformer 架構的注意力複雜度 $O(n^2)$ 造成記憶體瓶頸
2. **計算效率** 激活值、優化器狀態佔用大量記憶體
3. **收斂速度** 大模型訓練需要高效學習率調度
4. **模型對齊** 確保模型輸出符合人類價值觀與預期

**訓練優化**是解決LLM訓練效率問題的核心技術，包含記憶體優化、計算加速和收斂改進。

**模型對齊**是確保模型行為與人類意圖一致的重要技術，包含價值對齊和行為對齊。

### 1.2 訓練優化技術

#### 1.2.1 注意力優化

##### FlashAttention: 記憶體高效注意力

**問題分析**：標準注意力需要儲存 $n \times n$ 的注意力矩陣，當 $n$ 很大時會導致記憶體瓶頸。

**數學基礎**
標準注意力：
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**FlashAttention 核心思想**
1. **分塊 (Tiling)** 將 $Q$、$K$、$V$ 分割成小塊
2. **在線算法** 逐塊計算並更新結果
3. **記憶體層次** 充分利用 GPU 快速記憶體

**算法框架**
```python
# FlashAttention 偽代碼
def flash_attention(Q, K, V, block_size):
    N, d = Q.shape
    O = zeros_like(Q)
    l = zeros(N)  # 正規化常數
    m = full(-inf, N)  # 最大值

    for i in range(0, N, block_size):
        # 載入 Q 塊到 SRAM
        Qi = Q[i:i+block_size]
        for j in range(0, N, block_size):
            # 載入 K, V 塊到 SRAM
            Kj = K[j:j+block_size]
            Vj = V[j:j+block_size]

            # 計算分數
            Sij = Qi @ Kj.T / sqrt(d)

            # 在線 softmax 更新
            # ... (省略具體實現)

    return O
```

**效果提升**
- 記憶體從 $O(n^2)$ 降至 $O(n)$
- 速度提升2-4倍
- 支援長序列訓練

##### FlashAttention-2: 進一步優化

**主要改進**
1. **工作量分配優化** 更好利用 GPU 的平行計算單元
2. **並行策略改進** 針對不同序列長度的並行優化
3. **backward pass 優化** 反向傳播的記憶體與計算優化

#### 1.2.2 混合精度訓練

##### 混合精度 (Mixed Precision Training)

**基本概念**：結合 FP16 和 FP32 精度進行訓練，平衡速度與穩定性。

**實現方式**
```python
# PyTorch 混合精度實例
from torch.cuda.amp import autocast, GradScaler

model = MyModel().cuda()
optimizer = torch.optim.Adam(model.parameters())
scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()

    # 前向傳播使用 FP16
    with autocast():
        output = model(data)
        loss = criterion(output, target)

    # 損失縮放防止梯度下溢
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**技術要點**
- **動態損失縮放** 自動調整縮放因子防止梯度下溢
- **選擇性精度** 關鍵操作使用 FP32 精度
- **記憶體節省** 減少約50%記憶體使用，同時避免 inf/nan 問題

##### 梯度累積 (Gradient Accumulation)

**應用場景** 當GPU記憶體不足以支持大批次訓練時

**實現策略**
```python
# 梯度累積範例
accumulation_steps = 4
model.zero_grad()

for i, (data, target) in enumerate(dataloader):
    output = model(data)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        model.zero_grad()
```

**有效批次大小計算**
$$ \text{batch\_size}_{\text{effective}} = \text{batch\_size}_{\text{micro}} \times \text{accumulation\_steps} $$

##### 梯度檢查點 (Gradient Checkpointing)

**基本原理** 在前向傳播時不儲存所有中間激活值，在反向傳播時重新計算。

**策略權衡**
- **記憶體節省** 以時間換空間的經典策略
- **計算開銷** 需要重新計算丟棄的中間激活值
- **實現策略** 根據層數決定檢查點位置

**理論分析**
- 記憶體從 $O(L)$ 降至 $O(\sqrt{L})$，其中 $L$ 為層數
- 增加約 20-30% 的計算時間

#### 1.2.3 注意力改進

##### Multi-Query Attention (MQA)

**核心思想** 減少 KV Cache 記憶體使用，提升推理效率。

**架構對比**
```python
# 標準 Multi-Head Attention
class StandardMHA(nn.Module):
    def __init__(self, d_model, n_heads):
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.W_q = nn.Linear(d_model, d_model)  # n_heads 個 query
        self.W_k = nn.Linear(d_model, d_model)  # n_heads 個 key
        self.W_v = nn.Linear(d_model, d_model)  # n_heads 個 value

# Multi-Query Attention
class MQA(nn.Module):
    def __init__(self, d_model, n_heads):
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.W_q = nn.Linear(d_model, d_model)  # n_heads 個 query
        self.W_k = nn.Linear(d_model, self.d_k)  # 1 個共享 key
        self.W_v = nn.Linear(d_model, self.d_k)  # 1 個共享 value
```

**效能改善**
- KV Cache 減少 $\frac{n_{\text{heads}} - 1}{n_{\text{heads}}}$
- 推理速度提升 10-20%
- 對模型品質影響最小

##### Grouped-Query Attention (GQA)

**核心思想** MQA 與 MHA 的折衷方案

**技術特點**
- 查詢頭分組處理
- 每組共享一個 Key 和 Value
- 平衡記憶體與品質

**配置範例**
```python
# GQA 配置
n_heads = 32
n_kv_heads = 8  # 4 個查詢頭共享一組 KV
group_size = n_heads // n_kv_heads  # = 4
```

---

## 2. First Principles (原理)

### 2.1 計算複雜度分析

#### 注意力機制計算複雜度

**標準注意力複雜度**
- **時間複雜度** $O(n^2 d)$，其中 $n$ 為序列長度，$d$ 為隱向量維度
- **空間複雜度** $O(n^2 + nd)$
- **記憶體瓶頸** 當 $n \gg d$ 時，$n^2$ 項主導複雜度

**數學分析**
對於輸入 $X \in \mathbb{R}^{n \times d}$：
1. 計算 QKV：$3nd^2$ 次操作
2. 注意力分數：$n^2d$ 次操作
3. 加權平均：$n^2d$ 次操作

總計算量：$O(3nd^2 + 2n^2d)$

**記憶體瓶頸分析**
當 $n = 8192, d = 4096$ 時：
- 注意力矩陣：$8192^2 \times 4 \text{bytes} = 268 \text{GB}$
- 超出常見 GPU 記憶體容量

#### 記憶體層次優化原理

**GPU 記憶體層次**
1. **暫存器** 1-2KB，1 cycle 存取
2. **共享記憶體** 48-164KB，1-32 cycles
3. **L2 快取** 40-80MB，200-400 cycles
4. **全域記憶體** 16-80GB，400-800 cycles

**最佳化策略**
- **資料重用** 最大化暫存器和共享記憶體的使用
- **合併存取** 分塊演算法優化記憶體存取模式
- **計算隱藏** 利用記憶體層次隱藏計算延遲

### 2.2 數值精度理論

#### 浮點數表示分析

**IEEE 754 標準**
- **FP32**：1 符號位 + 8 指數位 + 23 尾數位
- **FP16**：1 符號位 + 5 指數位 + 10 尾數位
- **BF16**：1 符號位 + 8 指數位 + 7 尾數位

**精度比較**
| 格式 | 範圍 | 精度 | 特性 |
|------|------|------|------|
| FP32 | $\pm 3.4 \times 10^{38}$ | ~7 位 | 標準精度 |
| FP16 | $\pm 6.5 \times 10^4$ | ~3 位 | 高速推理 |
| BF16 | $\pm 3.4 \times 10^{38}$ | ~2 位 | 訓練友好 |

#### 梯度下溢問題

**問題描述** FP16 最小可表示正數約為 $6 \times 10^{-5}$，小於此值的梯度會被截斷為 0

**解決方案** 損失縮放
$$L_{\text{scaled}} = \text{scale} \times L_{\text{original}}$$
$$\nabla_{\text{scaled}} = \text{scale} \times \nabla_{\text{original}}$$

**動態縮放演算法**
```python
def dynamic_loss_scaling(loss, scale, growth_factor=2.0,
                        backoff_factor=0.5, growth_interval=2000):
    # 檢測梯度是否包含 inf/nan
    if has_inf_or_nan(gradients):
        scale *= backoff_factor
        skip_update = True
    else:
        if steps_since_last_unscale > growth_interval:
            scale *= growth_factor
        skip_update = False

    return scale, skip_update
```

---

## 3. Body of Knowledge (實務知識)

### 3.1 FlashAttention 進階實務

#### 3.1.1 FlashAttention 生產環境優化

##### 實際部署考量

**硬體適配策略**
```python
# FlashAttention 硬體檢測與配置
import torch
import torch.nn.functional as F
from flash_attn import flash_attn_func, flash_attn_varlen_func

def configure_flash_attention(device_capability):
    """根據GPU算力配置FlashAttention參數"""
    if device_capability >= (8, 0):  # A100, H100
        return {
            'block_size_m': 128,
            'block_size_n': 128,
            'num_warps': 8,
            'num_stages': 2
        }
    elif device_capability >= (7, 5):  # RTX 20/30 series
        return {
            'block_size_m': 64,
            'block_size_n': 64,
            'num_warps': 4,
            'num_stages': 1
        }
    else:
        # 回退到標準注意力
        return None

class OptimizedFlashAttention(torch.nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.0):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.dropout = dropout

        # 檢測硬體能力
        device_cap = torch.cuda.get_device_capability()
        self.flash_config = configure_flash_attention(device_cap)

        self.qkv_proj = torch.nn.Linear(d_model, 3 * d_model, bias=False)
        self.out_proj = torch.nn.Linear(d_model, d_model)

    def forward(self, x, attention_mask=None):
        batch_size, seq_len, _ = x.shape

        # 投影到 QKV
        qkv = self.qkv_proj(x)
        q, k, v = qkv.chunk(3, dim=-1)

        # 重塑為多頭格式
        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)

        if self.flash_config is not None:
            # 使用 FlashAttention
            output = flash_attn_func(
                q, k, v,
                dropout_p=self.dropout if self.training else 0.0,
                softmax_scale=1.0 / (self.head_dim ** 0.5),
                causal=True
            )
        else:
            # 回退到標準注意力
            output = self._standard_attention(q, k, v, attention_mask)

        # 重塑輸出
        output = output.view(batch_size, seq_len, self.d_model)
        return self.out_proj(output)
```

##### 記憶體使用優化

**動態序列長度處理**
```python
class VariableLengthFlashAttention:
    """處理不同序列長度的批次推理"""

    def __init__(self, max_seq_len=4096):
        self.max_seq_len = max_seq_len

    def packed_attention(self, q, k, v, cu_seqlens):
        """
        使用 packed 格式處理變長序列
        Args:
            q, k, v: (total_tokens, n_heads, head_dim)
            cu_seqlens: 累積序列長度 [0, len1, len1+len2, ...]
        """
        return flash_attn_varlen_func(
            q, k, v, cu_seqlens, cu_seqlens,
            max_seqlen_q=self.max_seq_len,
            max_seqlen_k=self.max_seq_len,
            dropout_p=0.0,
            softmax_scale=1.0 / (q.shape[-1] ** 0.5),
            causal=True
        )

    def batch_inference(self, inputs):
        """批次推理優化"""
        # 將不同長度序列打包
        packed_inputs, cu_seqlens = self._pack_sequences(inputs)

        # 執行注意力計算
        output = self.packed_attention(
            packed_inputs['q'],
            packed_inputs['k'],
            packed_inputs['v'],
            cu_seqlens
        )

        # 解包回原始格式
        return self._unpack_sequences(output, cu_seqlens)
```

##### 推理服務整合

**與 vLLM 整合**
```python
# vLLM 中的 FlashAttention 配置
class FlashAttentionBackend:
    def __init__(self, model_config):
        self.model_config = model_config
        self.use_flash_attn = self._check_flash_support()

    def _check_flash_support(self):
        """檢查環境是否支持 FlashAttention"""
        try:
            import flash_attn
            device_cap = torch.cuda.get_device_capability()
            return device_cap >= (7, 5)
        except ImportError:
            return False

    def attention_forward(self, query, key, value, attention_mask):
        if self.use_flash_attn:
            return self._flash_attention_forward(query, key, value)
        else:
            return self._standard_attention_forward(
                query, key, value, attention_mask
            )
```

#### 3.1.2 FlashAttention 性能調優

##### 模型架構適配

**長序列優化**
```python
class LongSequenceOptimizer:
    """長序列場景的特殊優化"""

    def __init__(self, window_size=2048, stride=1024):
        self.window_size = window_size
        self.stride = stride

    def sliding_window_attention(self, q, k, v, seq_len):
        """滑動窗口注意力"""
        if seq_len <= self.window_size:
            return flash_attn_func(q, k, v, causal=True)

        outputs = []
        for start in range(0, seq_len, self.stride):
            end = min(start + self.window_size, seq_len)

            # 窗口內的注意力計算
            window_q = q[:, start:end]
            window_k = k[:, :end]  # 可以看到之前的所有內容
            window_v = v[:, :end]

            window_out = flash_attn_func(
                window_q, window_k, window_v, causal=True
            )
            outputs.append(window_out)

        return torch.cat(outputs, dim=1)
```

##### 量化與精度優化

**混合精度 FlashAttention**
```python
class MixedPrecisionFlashAttention:
    """混合精度優化的 FlashAttention"""

    def __init__(self, dtype=torch.float16):
        self.compute_dtype = dtype
        self.storage_dtype = torch.float32

    def forward(self, q, k, v):
        # 確保計算精度
        q = q.to(self.compute_dtype)
        k = k.to(self.compute_dtype)
        v = v.to(self.compute_dtype)

        # FlashAttention 計算
        with torch.cuda.amp.autocast(dtype=self.compute_dtype):
            output = flash_attn_func(q, k, v, causal=True)

        # 轉回儲存精度
        return output.to(self.storage_dtype)
```

##### 性能監控與調試

**性能分析工具**
```python
class FlashAttentionProfiler:
    """FlashAttention 性能分析"""

    def __init__(self):
        self.metrics = {}

    def profile_attention(self, q, k, v, num_runs=100):
        """比較 FlashAttention 與標準注意力性能"""
        batch_size, seq_len, n_heads, head_dim = q.shape

        # FlashAttention 性能測試
        flash_times = []
        flash_memory = []

        for _ in range(num_runs):
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            start_memory = torch.cuda.memory_allocated()
            start_time = time.time()

            _ = flash_attn_func(q, k, v, causal=True)

            torch.cuda.synchronize()
            end_time = time.time()
            end_memory = torch.cuda.max_memory_allocated()

            flash_times.append(end_time - start_time)
            flash_memory.append(end_memory - start_memory)

        # 標準注意力性能測試
        standard_times = []
        standard_memory = []

        for _ in range(num_runs):
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            start_memory = torch.cuda.memory_allocated()
            start_time = time.time()

            _ = self._standard_attention(q, k, v)

            torch.cuda.synchronize()
            end_time = time.time()
            end_memory = torch.cuda.max_memory_allocated()

            standard_times.append(end_time - start_time)
            standard_memory.append(end_memory - start_memory)

        return {
            'flash_attn': {
                'avg_time': np.mean(flash_times),
                'avg_memory': np.mean(flash_memory)
            },
            'standard_attn': {
                'avg_time': np.mean(standard_times),
                'avg_memory': np.mean(standard_memory)
            },
            'speedup': np.mean(standard_times) / np.mean(flash_times),
            'memory_reduction': 1 - np.mean(flash_memory) / np.mean(standard_memory)
        }
```

### 3.2 模型對齊技術

#### 3.2.1 強化學習對齊 (RLHF)

##### 近端策略優化 (Proximal Policy Optimization, PPO) 在 LLM 對齊中的應用

**PPO 核心思想**
PPO 透過限制策略更新幅度來避免不穩定的大幅策略變化，保持訓練的穩定性

**目標函數**
$$L^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$

其中：
- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ 為重要性比值
- $\hat{A}_t$ 為優勢函數估計
- $\epsilon$ 為裁切參數（通常為 0.2）

**RLHF 三階段流程**

**階段 1：監督微調 (SFT)**
```python
# 監督微調範例
def sft_loss(model, batch):
    input_ids, labels = batch
    logits = model(input_ids).logits
    loss = cross_entropy(logits.view(-1, vocab_size),
                        labels.view(-1))
    return loss
```

**階段 2：獎勵模型訓練**
```python
# 獎勵模型訓練
def reward_model_loss(reward_model, chosen, rejected):
    reward_chosen = reward_model(chosen)
    reward_rejected = reward_model(rejected)

    # Bradley-Terry 模型
    loss = -log_sigmoid(reward_chosen - reward_rejected)
    return loss.mean()
```

**階段 3：PPO (Proximal Policy Optimization) 強化學習**
```python
# PPO 訓練範例
def ppo_loss(policy_model, value_model, ref_model, batch):
    states, actions, old_log_probs, rewards, values = batch

    # 計算重要性比值
    new_log_probs = policy_model.log_prob(states, actions)
    ratio = torch.exp(new_log_probs - old_log_probs)

    # 優勢估計
    advantages = rewards - values

    # PPO 裁切損失
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1-eps, 1+eps) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()

    # 價值函數損失
    value_loss = F.mse_loss(value_model(states), rewards)

    # KL 散度正則化項
    kl_div = kl_divergence(policy_model(states), ref_model(states))

    total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_div
    return total_loss
```

##### RLHF 的挑戰與解決方案

**收斂性問題**
- **問題** 策略梯度方法容易發散
- **解決** KL 散度約束限制策略變化幅度

**獎勵函數設計**
- **問題** 人類標註的獎勵模型可能不準確
- **解決** 獎勵模型的魯棒性訓練

**計算複雜性**
- **問題** 需要同時維護多個模型
- **解決** 模型並行和記憶體優化

#### 3.2.2 直接偏好優化

##### 直接偏好優化 (Direct Preference Optimization, DPO)

**核心思想** 繞過獎勵模型，直接從偏好資料進行優化

**理論基礎**
根據 Bradley-Terry 偏好模型，最優策略滿足：
$$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{r(x,y)}{\beta}\right)$$

**DPO (Direct Preference Optimization) 目標函數**
$$L_{\text{DPO}}(\pi_\theta) = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]$$

其中：
- $y_w$ 為偏好回應
- $y_l$ 為非偏好回應
- $\beta$ 為溫度參數

**實現代碼**
```python
def dpo_loss(model, ref_model, chosen, rejected, beta=0.1):
    # 計算策略模型對數機率
    chosen_logprobs = model.log_prob(chosen)
    rejected_logprobs = model.log_prob(rejected)

    # 計算參考模型對數機率
    with torch.no_grad():
        chosen_ref_logprobs = ref_model.log_prob(chosen)
        rejected_ref_logprobs = ref_model.log_prob(rejected)

    # 計算比值
    chosen_ratio = chosen_logprobs - chosen_ref_logprobs
    rejected_ratio = rejected_logprobs - rejected_ref_logprobs

    # DPO 損失
    loss = -F.logsigmoid(beta * (chosen_ratio - rejected_ratio))
    return loss.mean()
```

**DPO 優勢**
- 無需獨立的獎勵模型
- 訓練更加穩定簡潔
- 計算資源需求較低

##### Odds Ratio Preference Optimization (ORPO)

**基本概念** 在監督微調的同時進行偏好優化

**目標函數**
$$L_{\text{ORPO}} = L_{\text{SFT}} + \lambda \cdot L_{\text{OR}}$$

其中 Odds Ratio 損失：
$$L_{\text{OR}} = -\mathbb{E}\left[\log \sigma\left(\log \frac{\text{odds}_\theta(y_w|x)}{\text{odds}_\theta(y_l|x)}\right)\right]$$

**技術優勢**
- 單階段訓練
- 無需獎勵模型
- 更高的訓練效率

### 3.3 優化器進階技術

#### 3.3.1 現代優化器設計

##### AdamW 與 Weight Decay

**核心問題** 傳統 Adam 的L2正則化會與自適應學習率產生交互作用

**AdamW 解決方案**
將權重衰減與梯度更新分離：
$$\theta_{t+1} = \theta_t - \alpha \cdot \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t\right)$$

**實現對比**
```python
# 傳統 Adam + L2 (不正確的做法)
loss = mse_loss + l2_reg * sum(p**2 for p in parameters)

# AdamW (正確的做法)
optimizer = torch.optim.AdamW(parameters, lr=1e-3, weight_decay=0.01)
```

##### Lion 優化器

**核心思想** 使用符號函數簡化梯度更新，提高記憶體效率

**更新規則**
$$\theta_{t+1} = \theta_t - \eta \cdot \text{sign}(\text{interp}(m_t, \nabla_t, \beta_1))$$
$$m_{t+1} = \beta_2 m_t + (1-\beta_2) \nabla_t$$

**優勢特點**
- 記憶體使用比 Adam 節省 50%
- 大型模型訓練效果更佳
- 計算簡潔高效

#### 3.3.2 學習率調度策略

##### Cosine Annealing with Warmup

**最佳實踐組合**
```python
class CosineAnnealingWarmup:
    def __init__(self, optimizer, warmup_steps, total_steps,
                 min_lr_ratio=0.1):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.min_lr_ratio = min_lr_ratio
        self.base_lrs = [group['lr'] for group in optimizer.param_groups]

    def get_lr(self, step):
        if step < self.warmup_steps:
            # 線性 warmup
            lr_scale = step / self.warmup_steps
        else:
            # Cosine annealing
            progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr_scale = self.min_lr_ratio + (1 - self.min_lr_ratio) * \
                      0.5 * (1 + math.cos(math.pi * progress))

        return [base_lr * lr_scale for base_lr in self.base_lrs]
```

---

## 總結

訓練優化與對齊技術是 LLM 實用化的關鍵技術群組，解決了大模型訓練效率和安全性的核心問題。通過記憶體優化、計算加速、數值穩定和對齊技術的系統性應用，現代LLM能夠高效且安全地為各種應用場景提供服務。

### 技術要點總結

1. **記憶體優化** FlashAttention等技術顯著降低記憶體需求，實現長序列訓練
2. **計算加速** 混合精度、梯度累積、動態損失縮放有效提升訓練效率
3. **對齊技術** 從 RLHF 到 DPO 的演進，使模型更好地符合人類價值觀
4. **優化器創新** 從關注收斂速度到強調記憶體效率和穩定性

### 未來發展方向

- **記憶體效率** 持續探索更高效的記憶體優化技術
- **對齊魯棒性** 開發更強健的對齊技術框架
- **硬體適配** 針對新一代硬體的優化演算法
- **對齊創新** 憲法AI等新興對齊技術的研究和應用

**🚨 單GPU環境限制說明**
1. 理解GPU記憶體優化的核心演算法
2. 掌握 FlashAttention 等關鍵技術原理
3. 理解 DPO 等對齊技術的理論基礎
4. 建立優化技術的全域視角
5. 為實際Constitution AI等新興技術奠定基礎

透過系統性的理論學習，您將掌握 LLM 訓練優化的核心技能，為進一步的實踐應用和技術創新打下堅實基礎。