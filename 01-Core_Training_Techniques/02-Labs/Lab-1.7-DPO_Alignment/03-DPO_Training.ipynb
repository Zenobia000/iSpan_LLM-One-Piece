{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab-1.7: DPO (Direct Preference Optimization) è¨“ç·´å¯¦ä½œ\n",
    "\n",
    "**å¯¦é©—ç›®æ¨™**: å¯¦ç¾ä¸¦è¨“ç·´ DPO å°é½Šæ¨¡å‹\n",
    "\n",
    "é€™å€‹ notebook å°‡å¯¦ç¾ DPO çš„æ ¸å¿ƒè¨“ç·´æµç¨‹ã€‚DPO é€šéç›´æ¥å„ªåŒ–åå¥½æ•¸æ“šä¾†æ”¹å–„æ¨¡å‹è¡Œç‚ºï¼Œç›¸æ¯”å‚³çµ± RLHF æ›´åŠ ç©©å®šä¸”é«˜æ•ˆã€‚\n",
    "\n",
    "## DPO æ ¸å¿ƒåŸç†\n",
    "\n",
    "DPO çš„æ ¸å¿ƒæƒ³æ³•æ˜¯ç›´æ¥å¾åå¥½æ•¸æ“šä¸­å­¸ç¿’æœ€å„ªç­–ç•¥ï¼Œè€Œä¸éœ€è¦é¡¯å¼çš„çå‹µæ¨¡å‹ï¼š\n",
    "\n",
    "**DPO æå¤±å‡½æ•¸**:\n",
    "```\n",
    "L_DPO = -E[(x,y_w,y_l)~D][log Ïƒ(Î² log Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x) - Î² log Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x))]\n",
    "```\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- `Ï€_Î¸`: æ­£åœ¨è¨“ç·´çš„ç­–ç•¥æ¨¡å‹\n",
    "- `Ï€_ref`: åƒè€ƒæ¨¡å‹ (SFT æ¨¡å‹)\n",
    "- `Î²`: æº«åº¦åƒæ•¸ï¼Œæ§åˆ¶åå¥½å¼·åº¦\n",
    "- `y_w`: åå¥½å›æ‡‰ (chosen)\n",
    "- `y_l`: éåå¥½å›æ‡‰ (rejected)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1: ç’°å¢ƒæº–å‚™èˆ‡å°å…¥\n",
    "\n",
    "è¼‰å…¥å¿…è¦çš„åº«ä¸¦æº–å‚™ DPO è¨“ç·´ç’°å¢ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from datasets import load_from_disk, Dataset\n",
    "from trl import DPOTrainer\n",
    "\n",
    "# è¨­ç½®éš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('ğŸš€ é–‹å§‹ DPO è¨“ç·´å¯¦ä½œ')\n",
    "print(f'GPU å¯ç”¨: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'ç•¶å‰ GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2: è¼‰å…¥ SFT åŸºç·šæ¨¡å‹\n",
    "\n",
    "è¼‰å…¥ä¹‹å‰è¨“ç·´çš„ SFT æ¨¡å‹ä½œç‚º DPO çš„èµ·å§‹é»å’Œåƒè€ƒæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è·¯å¾‘é…ç½®\n",
    "BASE_MODEL_NAME = 'microsoft/DialoGPT-medium'\n",
    "SFT_MODEL_PATH = './sft_model_output'\n",
    "\n",
    "print(f'ğŸ“¦ è¼‰å…¥ SFT åŸºç·šæ¨¡å‹...')\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# è¼‰å…¥åŸºç¤æ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f'âœ… åŸºç¤æ¨¡å‹è¼‰å…¥æˆåŠŸ')\n",
    "\n",
    "# æª¢æŸ¥ SFT æ¨¡å‹æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(SFT_MODEL_PATH):\n",
    "    try:\n",
    "        # è¼‰å…¥ SFT æ¨¡å‹ä½œç‚ºç­–ç•¥æ¨¡å‹\n",
    "        policy_model = PeftModel.from_pretrained(base_model, SFT_MODEL_PATH)\n",
    "        print(f'âœ… SFT ç­–ç•¥æ¨¡å‹è¼‰å…¥æˆåŠŸ: {SFT_MODEL_PATH}')\n",
    "        \n",
    "        # åƒè€ƒæ¨¡å‹ä½¿ç”¨åŸå§‹åŸºç¤æ¨¡å‹\n",
    "        reference_model = deepcopy(base_model)\n",
    "        print(f'âœ… åƒè€ƒæ¨¡å‹æº–å‚™å®Œæˆ')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸  SFT æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}')\n",
    "        print('ä½¿ç”¨åŸºç¤æ¨¡å‹é€²è¡Œ DPO è¨“ç·´')\n",
    "        policy_model = base_model\n",
    "        reference_model = deepcopy(base_model)\n",
    "else:\n",
    "    print('âš ï¸  æœªæ‰¾åˆ° SFT æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¤æ¨¡å‹')\n",
    "    policy_model = base_model\n",
    "    reference_model = deepcopy(base_model)\n",
    "\n",
    "print(f'æ¨¡å‹åƒæ•¸é‡: {policy_model.num_parameters():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3: æº–å‚™ DPO è¨“ç·´æ•¸æ“š\n",
    "\n",
    "è¼‰å…¥åå¥½æ•¸æ“šé›†ä¸¦ç¢ºä¿æ ¼å¼æ­£ç¢ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ DPO æ•¸æ“šé›†\n",
    "try:\n",
    "    dpo_dataset = load_from_disk('./dpo_data')\n",
    "    print(f'âœ… è¼‰å…¥ DPO æ•¸æ“šé›†ï¼Œæ¨£æœ¬æ•¸: {len(dpo_dataset)}')\n",
    "except:\n",
    "    print('âš ï¸  ç„¡æ³•è¼‰å…¥å·²ä¿å­˜çš„æ•¸æ“šï¼Œå‰µå»ºæ¨¡æ“¬æ•¸æ“š')\n",
    "    # å‰µå»ºæ¨¡æ“¬ DPO æ•¸æ“š\n",
    "    mock_data = [\n",
    "        {\n",
    "            'prompt': 'è«‹è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?',\n",
    "            'chosen': 'æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒè®“è¨ˆç®—æ©Ÿèƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’ä¸¦åšå‡ºæ±ºç­–ï¼Œè€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚é€šéç®—æ³•åˆ†æå¤§é‡æ•¸æ“šï¼Œç³»çµ±å¯ä»¥è­˜åˆ¥æ¨¡å¼ä¸¦æé«˜æ€§èƒ½ã€‚',\n",
    "            'rejected': 'æ©Ÿå™¨å­¸ç¿’å°±æ˜¯è®“æ©Ÿå™¨è®Šè°æ˜ã€‚'\n",
    "        },\n",
    "        {\n",
    "            'prompt': 'å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆ?',\n",
    "            'chosen': 'å­¸ç¿’ç¨‹å¼è¨­è¨ˆå»ºè­°å¾åŸºç¤èªæ³•é–‹å§‹ï¼Œé¸æ“‡ä¸€é–€é©åˆçš„èªè¨€å¦‚Pythonï¼Œå¤šåšç·´ç¿’é …ç›®ï¼Œåƒèˆ‡é–‹æºå°ˆæ¡ˆï¼Œä¸¦æŒçºŒå­¸ç¿’æ–°æŠ€è¡“ã€‚å¯¦ä½œæ˜¯æœ€é‡è¦çš„å­¸ç¿’æ–¹å¼ã€‚',\n",
    "            'rejected': 'å­¸ç¨‹å¼å°±æ˜¯å¯«codeã€‚'\n",
    "        },\n",
    "        {\n",
    "            'prompt': 'ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’?',\n",
    "            'chosen': 'æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹å­é ˜åŸŸï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’æ•¸æ“šçš„è¤‡é›œæ¨¡å¼ã€‚å®ƒåœ¨åœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†å’ŒèªéŸ³è­˜åˆ¥ç­‰é ˜åŸŸå–å¾—äº†çªç ´æ€§é€²å±•ã€‚',\n",
    "            'rejected': 'æ·±åº¦å­¸ç¿’å°±æ˜¯å¾ˆæ·±çš„å­¸ç¿’ã€‚'\n",
    "        }\n",
    "    ]\n",
    "    dpo_dataset = Dataset.from_list(mock_data)\n",
    "\n",
    "print(f'æ•¸æ“šé›†å¤§å°: {len(dpo_dataset)}')\n",
    "print(f'æ•¸æ“šæ¬„ä½: {list(dpo_dataset.features.keys())}')\n",
    "\n",
    "# æª¢æŸ¥æ•¸æ“šæ ¼å¼\n",
    "sample = dpo_dataset[0]\n",
    "print(f'\\nğŸ“ æ•¸æ“šæ¨£æœ¬:')\n",
    "print(f'Prompt: {sample[\"prompt\"]}')\n",
    "print(f'Chosen: {sample[\"chosen\"][:100]}...')\n",
    "print(f'Rejected: {sample[\"rejected\"][:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4: DPO æå¤±å‡½æ•¸å¯¦ç¾\n",
    "\n",
    "å¯¦ç¾ DPO çš„æ ¸å¿ƒæå¤±å‡½æ•¸ï¼Œç†è§£å…¶æ•¸å­¸åŸç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(model, input_ids, attention_mask, labels):\n",
    "    \"\"\"è¨ˆç®—åºåˆ—çš„å°æ•¸æ¦‚ç‡\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # è¨ˆç®—æ¯å€‹ token çš„å°æ•¸æ¦‚ç‡\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # æ”¶é›†ç›®æ¨™ token çš„å°æ•¸æ¦‚ç‡\n",
    "        target_log_probs = torch.gather(log_probs[:, :-1], dim=-1, \n",
    "                                       index=labels[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # åƒ…è¨ˆç®—é padding token çš„å¹³å‡å°æ•¸æ¦‚ç‡\n",
    "        mask = (labels[:, 1:] != tokenizer.pad_token_id).float()\n",
    "        sequence_log_prob = (target_log_probs * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        \n",
    "    return sequence_log_prob\n",
    "\n",
    "\n",
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps, \n",
    "             reference_chosen_logps, reference_rejected_logps, beta=0.1):\n",
    "    \"\"\"DPO æå¤±å‡½æ•¸å¯¦ç¾\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: ç­–ç•¥æ¨¡å‹å° chosen å›æ‡‰çš„å°æ•¸æ¦‚ç‡\n",
    "        policy_rejected_logps: ç­–ç•¥æ¨¡å‹å° rejected å›æ‡‰çš„å°æ•¸æ¦‚ç‡  \n",
    "        reference_chosen_logps: åƒè€ƒæ¨¡å‹å° chosen å›æ‡‰çš„å°æ•¸æ¦‚ç‡\n",
    "        reference_rejected_logps: åƒè€ƒæ¨¡å‹å° rejected å›æ‡‰çš„å°æ•¸æ¦‚ç‡\n",
    "        beta: æº«åº¦åƒæ•¸\n",
    "    \"\"\"\n",
    "    # è¨ˆç®—ç›¸å°æ–¼åƒè€ƒæ¨¡å‹çš„çå‹µ\n",
    "    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps)\n",
    "    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps)\n",
    "    \n",
    "    # DPO æå¤±: æœ€å¤§åŒ– chosen ç›¸å°æ–¼ rejected çš„åå¥½\n",
    "    loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "    \n",
    "    # è¨ˆç®—æº–ç¢ºç‡ (chosen çå‹µæ˜¯å¦é«˜æ–¼ rejected)\n",
    "    accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "    \n",
    "    return loss, accuracy, chosen_rewards.mean(), rejected_rewards.mean()\n",
    "\n",
    "\n",
    "print('âœ… DPO æå¤±å‡½æ•¸å¯¦ç¾å®Œæˆ')\n",
    "\n",
    "# æ¸¬è©¦æå¤±å‡½æ•¸\n",
    "print('\\nğŸ§ª æ¸¬è©¦ DPO æå¤±å‡½æ•¸:')\n",
    "dummy_chosen = torch.tensor([-2.0, -1.5])\n",
    "dummy_rejected = torch.tensor([-3.0, -2.5])\n",
    "dummy_ref_chosen = torch.tensor([-2.2, -1.7])\n",
    "dummy_ref_rejected = torch.tensor([-2.8, -2.3])\n",
    "\n",
    "loss, acc, chosen_reward, rejected_reward = dpo_loss(\n",
    "    dummy_chosen, dummy_rejected, dummy_ref_chosen, dummy_ref_rejected, beta=0.1\n",
    ")\n",
    "\n",
    "print(f'Loss: {loss.item():.4f}')\n",
    "print(f'Accuracy: {acc.item():.4f}')\n",
    "print(f'Chosen Reward: {chosen_reward.item():.4f}')\n",
    "print(f'Rejected Reward: {rejected_reward.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5: ä½¿ç”¨ TRL DPOTrainer\n",
    "\n",
    "ä½¿ç”¨ TRL åº«çš„ DPOTrainer ä¾†ç°¡åŒ– DPO è¨“ç·´æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æº–å‚™ DPO è¨“ç·´æ•¸æ“šæ ¼å¼\n",
    "def format_dpo_dataset(dataset):\n",
    "    \"\"\"å°‡æ•¸æ“šé›†æ ¼å¼åŒ–ç‚º DPOTrainer éœ€è¦çš„æ ¼å¼\"\"\"\n",
    "    formatted_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        formatted_data.append({\n",
    "            'prompt': sample['prompt'],\n",
    "            'chosen': sample['chosen'],\n",
    "            'rejected': sample['rejected']\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(formatted_data)\n",
    "\n",
    "# æ ¼å¼åŒ–æ•¸æ“šé›†\n",
    "formatted_dataset = format_dpo_dataset(dpo_dataset)\n",
    "print(f'âœ… æ•¸æ“šé›†æ ¼å¼åŒ–å®Œæˆï¼Œæ¨£æœ¬æ•¸: {len(formatted_dataset)}')\n",
    "\n",
    "# é¡¯ç¤ºæ ¼å¼åŒ–å¾Œçš„æ¨£æœ¬\n",
    "sample = formatted_dataset[0]\n",
    "print(f'\\nğŸ“ æ ¼å¼åŒ–æ¨£æœ¬:')\n",
    "for key, value in sample.items():\n",
    "    print(f'{key}: {value[:50]}...' if len(str(value)) > 50 else f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 6: DPO è¨“ç·´é…ç½®\n",
    "\n",
    "è¨­ç½® DPO è¨“ç·´çš„åƒæ•¸å’Œé…ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO è¨“ç·´é…ç½®\n",
    "output_dir = './dpo_model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# è¨“ç·´åƒæ•¸\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # åŸºæœ¬è¨­ç½®\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-6,  # DPO é€šå¸¸ä½¿ç”¨è¼ƒå°çš„å­¸ç¿’ç‡\n",
    "    \n",
    "    # å„ªåŒ–è¨­ç½®\n",
    "    warmup_steps=50,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    \n",
    "    # è¨˜æ†¶é«”å„ªåŒ–\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # å…¶ä»–è¨­ç½®\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print('âš™ï¸  DPO è¨“ç·´é…ç½®:')\n",
    "print(f'è¼¸å‡ºç›®éŒ„: {output_dir}')\n",
    "print(f'è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}')\n",
    "print(f'æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}')\n",
    "print(f'å­¸ç¿’ç‡: {training_args.learning_rate}')\n",
    "print(f'ä½¿ç”¨ FP16: {training_args.fp16}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 7: é–‹å§‹ DPO è¨“ç·´\n",
    "\n",
    "ä½¿ç”¨ DPOTrainer é€²è¡Œåå¥½å„ªåŒ–è¨“ç·´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # å‰µå»º DPO Trainer\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=policy_model,\n",
    "        ref_model=reference_model,\n",
    "        args=training_args,\n",
    "        train_dataset=formatted_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        beta=0.1,  # DPO æº«åº¦åƒæ•¸\n",
    "        max_length=512,\n",
    "        max_prompt_length=256,\n",
    "    )\n",
    "    \n",
    "    print('âœ… DPOTrainer å‰µå»ºæˆåŠŸ')\n",
    "    \n",
    "    # é–‹å§‹è¨“ç·´\n",
    "    print('ğŸš€ é–‹å§‹ DPO è¨“ç·´...')\n",
    "    \n",
    "    # è¨“ç·´å‰çš„è¨˜æ†¶é«”ç‹€æ…‹\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'è¨“ç·´å‰ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "    \n",
    "    # åŸ·è¡Œè¨“ç·´\n",
    "    dpo_trainer.train()\n",
    "    \n",
    "    print('âœ… DPO è¨“ç·´å®Œæˆï¼')\n",
    "    \n",
    "    # è¨“ç·´å¾Œçš„è¨˜æ†¶é«”ç‹€æ…‹\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'è¨“ç·´å¾Œ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'âŒ DPO è¨“ç·´å¤±æ•—: {e}')\n",
    "    print('\\nğŸ”„ å˜—è©¦æ‰‹å‹• DPO è¨“ç·´å¯¦ç¾...')\n",
    "    \n",
    "    # æ‰‹å‹• DPO è¨“ç·´çš„ç°¡åŒ–ç‰ˆæœ¬\n",
    "    from torch.optim import AdamW\n",
    "    \n",
    "    # æº–å‚™å„ªåŒ–å™¨\n",
    "    optimizer = AdamW(policy_model.parameters(), lr=1e-6)\n",
    "    policy_model.train()\n",
    "    reference_model.eval()\n",
    "    \n",
    "    print('ğŸ”§ ä½¿ç”¨æ‰‹å‹•å¯¦ç¾é€²è¡Œ DPO è¨“ç·´...')\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, sample in enumerate(formatted_dataset):\n",
    "            # æº–å‚™è¼¸å…¥\n",
    "            prompt = sample['prompt']\n",
    "            chosen_text = f\"{prompt} {sample['chosen']}\"\n",
    "            rejected_text = f\"{prompt} {sample['rejected']}\"\n",
    "            \n",
    "            # Tokenization\n",
    "            chosen_tokens = tokenizer(chosen_text, return_tensors='pt', truncation=True, max_length=256)\n",
    "            rejected_tokens = tokenizer(rejected_text, return_tensors='pt', truncation=True, max_length=256)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                chosen_tokens = {k: v.cuda() for k, v in chosen_tokens.items()}\n",
    "                rejected_tokens = {k: v.cuda() for k, v in rejected_tokens.items()}\n",
    "            \n",
    "            # è¨ˆç®—å°æ•¸æ¦‚ç‡\n",
    "            policy_chosen_logps = compute_log_probs(policy_model, **chosen_tokens, labels=chosen_tokens['input_ids'])\n",
    "            policy_rejected_logps = compute_log_probs(policy_model, **rejected_tokens, labels=rejected_tokens['input_ids'])\n",
    "            \n",
    "            ref_chosen_logps = compute_log_probs(reference_model, **chosen_tokens, labels=chosen_tokens['input_ids'])\n",
    "            ref_rejected_logps = compute_log_probs(reference_model, **rejected_tokens, labels=rejected_tokens['input_ids'])\n",
    "            \n",
    "            # è¨ˆç®— DPO æå¤±\n",
    "            loss, acc, _, _ = dpo_loss(\n",
    "                policy_chosen_logps, policy_rejected_logps,\n",
    "                ref_chosen_logps, ref_rejected_logps\n",
    "            )\n",
    "            \n",
    "            # åå‘å‚³æ’­\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                print(f'Step {i}: Loss={loss.item():.4f}, Acc={acc.item():.4f}')\n",
    "        \n",
    "        print(f'Epoch {epoch}: Avg Loss={total_loss/len(formatted_dataset):.4f}, Avg Acc={total_acc/len(formatted_dataset):.4f}')\n",
    "    \n",
    "    print('âœ… æ‰‹å‹• DPO è¨“ç·´å®Œæˆï¼')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 8: ä¿å­˜ DPO æ¨¡å‹\n",
    "\n",
    "ä¿å­˜è¨“ç·´å®Œæˆçš„ DPO æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ DPO æ¨¡å‹\n",
    "print('ğŸ’¾ ä¿å­˜ DPO æ¨¡å‹...')\n",
    "\n",
    "try:\n",
    "    # å¦‚æœä½¿ç”¨ PEFT æ¨¡å‹\n",
    "    if hasattr(policy_model, 'save_pretrained'):\n",
    "        policy_model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f'âœ… DPO æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}')\n",
    "    else:\n",
    "        torch.save(policy_model.state_dict(), os.path.join(output_dir, 'dpo_model.pth'))\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f'âœ… DPO æ¨¡å‹ç‹€æ…‹å·²ä¿å­˜è‡³: {output_dir}')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'âš ï¸  æ¨¡å‹ä¿å­˜éç¨‹ä¸­å‡ºç¾å•é¡Œ: {e}')\n",
    "    print('å˜—è©¦åŸºæœ¬ä¿å­˜æ–¹å¼...')\n",
    "    \n",
    "    # åŸºæœ¬ä¿å­˜æ–¹å¼\n",
    "    torch.save({\n",
    "        'model_state_dict': policy_model.state_dict(),\n",
    "        'config': policy_model.config,\n",
    "    }, os.path.join(output_dir, 'dpo_model_manual.pth'))\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f'âœ… DPO æ¨¡å‹æ‰‹å‹•ä¿å­˜è‡³: {output_dir}')\n",
    "\n",
    "# åˆ—å‡ºä¿å­˜çš„æª”æ¡ˆ\n",
    "saved_files = list(Path(output_dir).glob('*'))\n",
    "print(f'ä¿å­˜çš„æª”æ¡ˆ: {[f.name for f in saved_files]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 9: DPO æ¨¡å‹æ¸¬è©¦\n",
    "\n",
    "æ¸¬è©¦ DPO è¨“ç·´å¾Œçš„æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ DPO æ¨¡å‹\n",
    "def test_dpo_model(model, tokenizer, prompt, max_length=150):\n",
    "    \"\"\"æ¸¬è©¦ DPO æ¨¡å‹ç”Ÿæˆ\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # æº–å‚™è¼¸å…¥\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # ç”Ÿæˆå›æ‡‰\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # è§£ç¢¼è¼¸å‡º\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print('ğŸ§ª æ¸¬è©¦ DPO æ¨¡å‹ç”Ÿæˆèƒ½åŠ›...')\n",
    "\n",
    "test_prompts = [\n",
    "    \"Human: è«‹è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’?\\n\\nAssistant:\",\n",
    "    \"Human: å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆ?\\n\\nAssistant:\",\n",
    "    \"Human: ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’?\\n\\nAssistant:\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f'\\nğŸ“ æ¸¬è©¦ {i+1}:')\n",
    "    print(f'Prompt: {prompt.split(\"Assistant:\")[0]}Assistant:')\n",
    "    \n",
    "    try:\n",
    "        response = test_dpo_model(policy_model, tokenizer, prompt)\n",
    "        print(f'DPO Generated: {response}')\n",
    "    except Exception as e:\n",
    "        print(f'ç”Ÿæˆå¤±æ•—: {e}')\n",
    "    \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 10: è¨“ç·´ç¸½çµ\n",
    "\n",
    "ç¸½çµ DPO è¨“ç·´çš„çµæœå’Œé—œéµæŒ‡æ¨™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO è¨“ç·´ç¸½çµ\n",
    "print('=== DPO è¨“ç·´ç¸½çµ ===')\n",
    "print(f'âœ… åŸºç¤æ¨¡å‹: {BASE_MODEL_NAME}')\n",
    "print(f'âœ… SFT æ¨¡å‹è·¯å¾‘: {SFT_MODEL_PATH}')\n",
    "print(f'âœ… DPO è¨“ç·´æ•¸æ“š: {len(formatted_dataset)} å€‹åå¥½å°')\n",
    "print(f'âœ… è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}')\n",
    "print(f'âœ… å­¸ç¿’ç‡: {training_args.learning_rate}')\n",
    "print(f'âœ… DPO æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}')\n",
    "\n",
    "# è¨ˆç®—æ¨¡å‹åƒæ•¸çµ±è¨ˆ\n",
    "total_params = sum(p.numel() for p in policy_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
    "print(f'âœ… ç¸½åƒæ•¸: {total_params:,}')\n",
    "print(f'âœ… å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)')\n",
    "\n",
    "print('\\nğŸ¯ DPO å°é½Šæ¨¡å‹è¨“ç·´å®Œæˆï¼')\n",
    "print('ä¸‹ä¸€æ­¥: åŸ·è¡Œ 04-Evaluation_and_Compare.ipynb é€²è¡Œæ¨¡å‹è©•ä¼°')\n",
    "\n",
    "# æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\nGPU è¨˜æ†¶é«”å·²æ¸…ç†ï¼Œç•¶å‰ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "print('\\nğŸ“š DPO æ ¸å¿ƒæ¦‚å¿µç¸½çµ:')\n",
    "print('â€¢ DPO ç›´æ¥å¾åå¥½æ•¸æ“šä¸­å­¸ç¿’ï¼Œç„¡éœ€çå‹µæ¨¡å‹')\n",
    "print('â€¢ ä½¿ç”¨ Bradley-Terry æ¨¡å‹ä¾†å»ºæ¨¡äººé¡åå¥½')\n",
    "print('â€¢ é€šéå°æ¯”å­¸ç¿’å„ªåŒ–æ¨¡å‹è¡Œç‚º')\n",
    "print('â€¢ ç›¸æ¯” RLHF æ›´åŠ ç©©å®šä¸”æ˜“æ–¼å¯¦ç¾')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}