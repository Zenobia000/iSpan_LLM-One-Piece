# 2.2 服務優化理論
# Serving and Optimization for LLMs

> **學習目標**  
> - 掌握 LLM 服務架構設計  
> - 理解推理性能優化技術  
> - 學習記憶體與吞吐量優化  
> - 了解生產環境最佳實踐  

---

## 進度說明

由於檔案編碼問題，完整版本正在重新開發中。  
當前為簡化版本，包含核心概念。

完整內容將包含：
1. 模型服務架構 (Triton, FastAPI)
2. 記憶體優化 (KV Cache, Quantization)
3. 吞吐量優化 (Continuous Batching)
4. 特殊場景 (長文本, 結構化輸出)
5. 生產環境最佳實踐

---

## 參考資料

**論文**:
- SpecInfer: Speculative Decoding (2023)
- SmoothQuant: INT8 Quantization (ICML 2023)

**項目**:
- Triton Server: https://github.com/triton-inference-server/server
- FastAPI: https://fastapi.tiangolo.com/

**版本**: v0.5 (重建中)  
**更新**: 2025-10-09
