# 4.2 LLM 數據工程全流程指南

**更新日期**: 2025-10-17
**適用對象**: LLM 工程師、數據科學家、研究員
**閱讀時間**: 45 分鐘

---

## 📋 目錄

1. [為何數據質量決定模型質量](#1-為何數據質量決定模型質量)
2. [數據工程生命週期](#2-數據工程生命週期)
3. [預訓練數據處理](#3-預訓練數據處理)
4. [微調數據優化](#4-微調數據優化)
5. [數據質量評估方法](#5-數據質量評估方法)
6. [IFD 算法原理與實作](#6-ifd-算法原理與實作)
7. [DEITA 評分系統](#7-deita-評分系統)
8. [LESS 梯度選擇法](#8-less-梯度選擇法)
9. [數據管道設計](#9-數據管道設計)
10. [數據倫理與合規](#10-數據倫理與合規)
11. [實踐建議](#11-實踐建議)

---

## 1. 為何數據質量決定模型質量

### 1.1 經典研究案例

**LIMA (Less Is More for Alignment)**：
- 僅用 1,000 條精心篩選的高質量數據
- 達到與 52,000 條數據訓練的模型相當的性能
- **核心洞察**：數據質量 > 數據數量

**Phi-1.5 (Microsoft Research)**：
- 1.3B 參數的小型模型
- 僅使用 7B tokens 的高質量「教科書級」數據
- 在推理任務上超越許多 10B+ 參數模型
- **核心洞察**：精選數據能顯著提升參數效率

**TinyStories (Microsoft Research)**：
- 證明合成數據的價值
- 使用 GPT-4 生成的高質量故事數據
- 小模型達到驚人的語言流暢度

### 1.2 數據質量的三個維度

**複雜度 (Complexity)**：
- 任務難度與推理深度
- 語義豐富度
- 指令遵循難度 (IFD)

**多樣性 (Diversity)**：
- 任務類型覆蓋
- 語言風格變化
- 領域分布廣度

**準確性 (Accuracy)**：
- 標註正確性
- 邏輯一致性
- 事實準確性

### 1.3 低質量數據的危害

**過擬合風險**：
- 模型記憶低質量模式
- 泛化能力下降

**有害偏見**：
- 放大社會偏見
- 產生有害內容
- 違反倫理規範

**訓練浪費**：
- 計算資源浪費
- 訓練時間延長
- 無效的參數更新

---

## 2. 數據工程生命週期

### 2.1 完整流程圖

```
1. 數據收集 (Data Collection)
   ↓
2. 數據清洗 (Data Cleaning)
   ↓
3. 去重 (Deduplication)
   ↓
4. 質量評估 (Quality Assessment)
   ↓
5. 數據過濾 (Data Filtering)
   ↓
6. 數據增強 (Data Augmentation)
   ↓
7. 格式化與打包 (Formatting & Packaging)
   ↓
8. 版本管理 (Versioning)
```

### 2.2 各階段核心任務

| 階段 | 核心任務 | 常用工具 |
|:---|:---|:---|
| **收集** | 網頁爬取、API 獲取、眾包標註 | Scrapy, BeautifulSoup, Selenium |
| **清洗** | 移除 HTML、標點正規化、編碼修復 | ftfy, cleantext, regex |
| **去重** | 精確去重、近似去重、跨語言去重 | MinHash, SimHash, Deduplicate |
| **評估** | 困惑度、IFD、多樣性、有害性檢測 | Perspective API, toxicity-classifier |
| **過濾** | 基於規則、基於模型、基於梯度 | DEITA, LESS, AlpaGasus |
| **增強** | 回譯、同義替換、合成數據 | GPT-4, Claude, nlpaug |
| **格式化** | JSONL、Parquet、Arrow | Datasets, Pandas |
| **版本管理** | 數據版本、血緣追蹤、實驗管理 | DVC, Git LFS, Weights & Biases |

---

## 3. 預訓練數據處理

### 3.1 數據來源

**網頁爬取數據**：
- Common Crawl (數百 TB 網頁數據)
- OpenWebText (Reddit 高分文章)
- C4 (Colossal Clean Crawled Corpus)

**書籍與學術**：
- BookCorpus
- arXiv 論文
- PubMed 醫學文獻

**代碼數據**：
- GitHub 公開代碼
- StackOverflow 問答
- Jupyter Notebooks

### 3.2 預訓練數據清洗流程

**第一步：基礎清洗**

```python
import ftfy
import re

def basic_cleaning(text):
    """基礎文本清洗"""
    # 修復 Unicode 編碼問題
    text = ftfy.fix_text(text)
    
    # 移除 HTML 標籤
    text = re.sub(r'<[^>]+>', '', text)
    
    # 移除過多空白
    text = re.sub(r'\s+', ' ', text)
    
    # 移除控制字元
    text = ''.join(char for char in text if ord(char) >= 32 or char == '\n')
    
    return text.strip()
```

**第二步：質量過濾**

```python
def quality_filter(text, min_words=50, max_words=100000):
    """基於規則的質量過濾"""
    words = text.split()
    word_count = len(words)
    
    # 過濾過短或過長的文本
    if word_count < min_words or word_count > max_words:
        return False
    
    # 計算平均詞長
    avg_word_length = sum(len(w) for w in words) / word_count
    if avg_word_length < 3 or avg_word_length > 15:
        return False
    
    # 計算符號比例
    symbol_ratio = len(re.findall(r'[^\w\s]', text)) / len(text)
    if symbol_ratio > 0.3:
        return False
    
    # 計算大寫字母比例
    upper_ratio = sum(1 for c in text if c.isupper()) / len(text)
    if upper_ratio > 0.3:
        return False
    
    return True
```

**第三步：去重 (MinHash)**

```python
from datasketch import MinHash, MinHashLSH

def create_minhash(text, num_perm=128):
    """創建 MinHash 簽名"""
    m = MinHash(num_perm=num_perm)
    for word in text.split():
        m.update(word.encode('utf8'))
    return m

def deduplicate_corpus(texts, threshold=0.8):
    """使用 MinHash LSH 進行去重"""
    lsh = MinHashLSH(threshold=threshold, num_perm=128)
    
    unique_texts = []
    seen_ids = set()
    
    for idx, text in enumerate(texts):
        minhash = create_minhash(text)
        
        # 查詢相似文本
        result = lsh.query(minhash)
        
        if not result:
            # 沒有相似文本,保留
            lsh.insert(f"doc_{idx}", minhash)
            unique_texts.append(text)
            seen_ids.add(idx)
    
    return unique_texts

# 使用範例
corpus = ["This is a sample text.", "This is a sample text.", "Different content."]
unique_corpus = deduplicate_corpus(corpus, threshold=0.8)
print(f"Original: {len(corpus)}, Unique: {len(unique_corpus)}")
```

### 3.3 困惑度過濾 (Perplexity Filtering)

**原理**：使用高質量語言模型計算文本的困惑度,過濾低質量文本。

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

class PerplexityFilter:
    def __init__(self, model_name='gpt2'):
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.model.eval()
    
    def calculate_perplexity(self, text):
        """計算文本困惑度"""
        encodings = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=1024)
        
        with torch.no_grad():
            outputs = self.model(**encodings, labels=encodings['input_ids'])
            loss = outputs.loss
            perplexity = torch.exp(loss).item()
        
        return perplexity
    
    def filter_by_perplexity(self, texts, max_ppl=1000):
        """根據困惑度過濾文本"""
        filtered = []
        for text in texts:
            ppl = self.calculate_perplexity(text)
            if ppl < max_ppl:
                filtered.append((text, ppl))
        return filtered

# 使用範例
ppl_filter = PerplexityFilter()
texts = ["This is a well-formed sentence.", "asdfjkl qwerty zxcvbn"]
filtered_texts = ppl_filter.filter_by_perplexity(texts, max_ppl=500)
```

---

## 4. 微調數據優化

### 4.1 指令微調數據格式

**標準格式 (Alpaca)**：

```json
{
  "instruction": "將以下英文翻譯成中文",
  "input": "The quick brown fox jumps over the lazy dog.",
  "output": "敏捷的棕色狐狸跳過懶狗。"
}
```

**對話格式 (ChatML)**：

```json
{
  "messages": [
    {"role": "system", "content": "你是一個有用的助手。"},
    {"role": "user", "content": "什麼是機器學習?"},
    {"role": "assistant", "content": "機器學習是人工智能的一個分支..."}
  ]
}
```

### 4.2 數據質量三要素

**複雜度 (Complexity)**：
- 任務難度
- 推理步驟數
- 語義深度

**多樣性 (Diversity)**：
- 任務類型分布
- 領域覆蓋
- 語言風格

**準確性 (Accuracy)**：
- 邏輯正確性
- 事實準確性
- 格式規範性

---

## 5. 數據質量評估方法

### 5.1 基於規則的評估

```python
def rule_based_quality_score(sample):
    """基於規則的質量評分"""
    instruction = sample['instruction']
    output = sample['output']
    
    score = 0.0
    
    # 1. 長度合理性 (0-20 分)
    if 10 <= len(instruction.split()) <= 100:
        score += 10
    if 20 <= len(output.split()) <= 500:
        score += 10
    
    # 2. 複雜度 (0-20 分)
    # 使用詞彙豐富度
    unique_ratio = len(set(output.split())) / len(output.split())
    score += unique_ratio * 20
    
    # 3. 格式完整性 (0-20 分)
    if instruction and output:
        score += 20
    
    # 4. 標點符號使用 (0-20 分)
    punct_count = len(re.findall(r'[.,!?;:]', output))
    if 2 <= punct_count <= 20:
        score += 20
    
    # 5. 無重複內容 (0-20 分)
    words = output.split()
    if len(words) > 0:
        duplicate_ratio = 1 - (len(set(words)) / len(words))
        score += (1 - duplicate_ratio) * 20
    
    return min(score, 100)  # 最高 100 分
```

### 5.2 基於模型的評估

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

class ModelBasedQualityScorer:
    def __init__(self, model_name='roberta-base'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=1
        )
        self.model.eval()
    
    def score_sample(self, instruction, output):
        """使用模型評估質量"""
        text = f"Instruction: {instruction}\nOutput: {output}"
        
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            score = torch.sigmoid(outputs.logits).item()
        
        return score * 100  # 轉換為 0-100 分

# 使用範例
scorer = ModelBasedQualityScorer()
score = scorer.score_sample(
    instruction="解釋什麼是量子計算",
    output="量子計算是一種利用量子力學原理進行信息處理的計算方式..."
)
```

---

## 6. IFD 算法原理與實作

### 6.1 IFD (Instruction Following Difficulty) 原理

**核心思想**：測量指令與回應之間的語義距離,距離越大表示任務越困難。

**假設**：
- 簡單任務的回應與指令高度相關 (如「翻譯」)
- 困難任務的回應需要推理,與指令語義距離較大 (如「分析」)

**數學表示**：

```
IFD = 1 - cosine_similarity(embedding(instruction), embedding(response))
```

### 6.2 IFD 完整實作

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class IFDCalculator:
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):
        """
        初始化 IFD 計算器
        
        Args:
            model_name: 句子嵌入模型名稱
        """
        self.model = SentenceTransformer(model_name)
    
    def calculate_ifd(self, instruction, response):
        """
        計算單個樣本的 IFD
        
        Args:
            instruction: 指令文本
            response: 回應文本
        
        Returns:
            float: IFD 分數 (0-1,越高越困難)
        """
        # 生成嵌入向量
        instr_emb = self.model.encode([instruction])
        resp_emb = self.model.encode([response])
        
        # 計算餘弦相似度
        similarity = cosine_similarity(instr_emb, resp_emb)[0][0]
        
        # IFD = 1 - similarity
        ifd = 1 - similarity
        
        return ifd
    
    def calculate_batch_ifd(self, samples):
        """
        批量計算 IFD
        
        Args:
            samples: List of dicts with 'instruction' and 'output' keys
        
        Returns:
            List of (sample, ifd_score) tuples
        """
        instructions = [s['instruction'] for s in samples]
        responses = [s['output'] for s in samples]
        
        # 批量編碼
        instr_embs = self.model.encode(instructions, show_progress_bar=True)
        resp_embs = self.model.encode(responses, show_progress_bar=True)
        
        # 計算相似度
        similarities = np.array([
            cosine_similarity([instr_embs[i]], [resp_embs[i]])[0][0]
            for i in range(len(samples))
        ])
        
        # 計算 IFD
        ifds = 1 - similarities
        
        return [(samples[i], ifds[i]) for i in range(len(samples))]
    
    def filter_by_ifd(self, samples, min_ifd=0.3, max_ifd=0.9):
        """
        根據 IFD 過濾樣本
        
        Args:
            samples: 樣本列表
            min_ifd: 最小 IFD (過濾太簡單的)
            max_ifd: 最大 IFD (過濾太困難/不相關的)
        
        Returns:
            List of filtered samples
        """
        samples_with_ifd = self.calculate_batch_ifd(samples)
        
        filtered = [
            sample for sample, ifd in samples_with_ifd
            if min_ifd <= ifd <= max_ifd
        ]
        
        return filtered

# 使用範例
ifd_calc = IFDCalculator()

samples = [
    {
        "instruction": "將 'hello' 翻譯成中文",
        "output": "你好"
    },
    {
        "instruction": "分析量子計算的未來發展趨勢",
        "output": "量子計算未來將在密碼學、藥物設計、材料科學等領域產生革命性影響..."
    }
]

# 計算 IFD
for sample in samples:
    ifd = ifd_calc.calculate_ifd(sample['instruction'], sample['output'])
    print(f"IFD: {ifd:.3f} - {sample['instruction'][:30]}...")

# 過濾樣本
filtered = ifd_calc.filter_by_ifd(samples, min_ifd=0.3, max_ifd=0.9)
print(f"\nFiltered: {len(filtered)} / {len(samples)}")
```

### 6.3 IFD 的優勢與限制

**優勢**：
- 無需訓練,直接計算
- 計算效率高
- 適用於各種語言

**限制**：
- 依賴嵌入模型質量
- 無法捕捉邏輯錯誤
- 對某些任務類型不準確 (如創意寫作)

---

## 7. DEITA 評分系統

### 7.1 DEITA 原理

**DEITA (Data-Efficient Instruction Tuning for Alignment)** 結合三個維度：

1. **Complexity**: 使用 LLM 評估任務複雜度
2. **Quality**: 使用 LLM 評估回應質量
3. **Diversity**: 使用嵌入向量評估多樣性

**最終分數**：

```
DEITA_score = α × Complexity + β × Quality + γ × Diversity
```

常用權重：α=0.4, β=0.4, γ=0.2

### 7.2 DEITA 完整實作

```python
import openai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class DEITAScorer:
    def __init__(self, 
                 openai_api_key,
                 model_name='gpt-4',
                 embedding_model='sentence-transformers/all-MiniLM-L6-v2',
                 alpha=0.4,
                 beta=0.4,
                 gamma=0.2):
        """
        初始化 DEITA 評分器
        
        Args:
            openai_api_key: OpenAI API 密鑰
            model_name: 用於評估的 LLM 模型
            embedding_model: 用於多樣性計算的嵌入模型
            alpha, beta, gamma: 三個維度的權重
        """
        openai.api_key = openai_api_key
        self.model_name = model_name
        self.embedding_model = SentenceTransformer(embedding_model)
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
    
    def evaluate_complexity(self, instruction):
        """使用 LLM 評估指令複雜度"""
        prompt = f"""請評估以下指令的複雜度,從 1 到 10 打分,其中:
1-3: 簡單 (如翻譯單詞、基礎計算)
4-6: 中等 (如段落翻譯、多步驟推理)
7-10: 困難 (如複雜分析、創意寫作、專業領域)

指令: {instruction}

請僅回覆一個 1-10 的數字。"""

        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        
        try:
            score = int(response.choices[0].message.content.strip())
            return min(max(score, 1), 10) / 10  # 正規化到 0-1
        except:
            return 0.5  # 預設中等複雜度
    
    def evaluate_quality(self, instruction, output):
        """使用 LLM 評估回應質量"""
        prompt = f"""請評估以下回應的質量,從 1 到 10 打分,考慮:
- 準確性: 回應是否正確
- 完整性: 是否充分回答問題
- 清晰度: 表達是否清晰
- 實用性: 是否有實際價值

指令: {instruction}
回應: {output}

請僅回覆一個 1-10 的數字。"""

        response = openai.ChatCompletion.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        
        try:
            score = int(response.choices[0].message.content.strip())
            return min(max(score, 1), 10) / 10
        except:
            return 0.5
    
    def evaluate_diversity(self, sample, reference_samples):
        """計算樣本與參考集的多樣性"""
        sample_text = f"{sample['instruction']} {sample['output']}"
        sample_emb = self.embedding_model.encode([sample_text])
        
        if not reference_samples:
            return 1.0  # 第一個樣本,最大多樣性
        
        ref_texts = [f"{s['instruction']} {s['output']}" for s in reference_samples]
        ref_embs = self.embedding_model.encode(ref_texts)
        
        # 計算與所有參考樣本的最大相似度
        similarities = cosine_similarity(sample_emb, ref_embs)[0]
        max_similarity = np.max(similarities)
        
        # 多樣性 = 1 - 最大相似度
        diversity = 1 - max_similarity
        
        return diversity
    
    def score_sample(self, sample, reference_samples=None):
        """計算樣本的 DEITA 分數"""
        instruction = sample['instruction']
        output = sample['output']
        
        # 計算三個維度
        complexity = self.evaluate_complexity(instruction)
        quality = self.evaluate_quality(instruction, output)
        diversity = self.evaluate_diversity(sample, reference_samples or [])
        
        # 加權求和
        deita_score = (
            self.alpha * complexity +
            self.beta * quality +
            self.gamma * diversity
        )
        
        return {
            'sample': sample,
            'complexity': complexity,
            'quality': quality,
            'diversity': diversity,
            'deita_score': deita_score
        }
    
    def select_top_k(self, samples, k):
        """選擇 DEITA 分數最高的 k 個樣本"""
        scored_samples = []
        selected = []
        
        for sample in samples:
            result = self.score_sample(sample, selected)
            scored_samples.append(result)
            selected.append(sample)
        
        # 排序並選擇 top-k
        scored_samples.sort(key=lambda x: x['deita_score'], reverse=True)
        
        return scored_samples[:k]

# 使用範例 (需要 OpenAI API Key)
# scorer = DEITAScorer(openai_api_key='your-key-here')
# top_samples = scorer.select_top_k(all_samples, k=1000)
```

### 7.3 DEITA 的優勢

- **全面性**: 同時考慮複雜度、質量、多樣性
- **有效性**: 研究顯示可用 6% 數據達到全量訓練 90% 效果
- **靈活性**: 權重可根據應用場景調整

---

## 8. LESS 梯度選擇法

### 8.1 LESS 原理

**LESS (Low-Effort Score Sampling)** 基於梯度信息選擇數據：

**核心思想**：
- 對模型影響大的樣本 = 高梯度範數
- 選擇梯度範數最大的 k 個樣本

**優勢**：
- 直接基於模型訓練信號
- 不依賴外部 LLM
- 計算效率高

### 8.2 LESS 實作

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader

class LESSSelector:
    def __init__(self, model_name, device='cuda'):
        """
        初始化 LESS 選擇器
        
        Args:
            model_name: 預訓練模型名稱
            device: 計算設備
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
        self.device = device
    
    def compute_gradient_norm(self, sample):
        """計算單個樣本的梯度範數"""
        text = f"{sample['instruction']}\n{sample['output']}"
        
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            max_length=512
        ).to(self.device)
        
        # 前向傳播
        outputs = self.model(**inputs, labels=inputs['input_ids'])
        loss = outputs.loss
        
        # 反向傳播
        self.model.zero_grad()
        loss.backward()
        
        # 計算梯度範數
        total_norm = 0.0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.detach().data.norm(2)
                total_norm += param_norm.item() ** 2
        
        total_norm = total_norm ** 0.5
        
        return total_norm
    
    def select_top_k(self, samples, k):
        """選擇梯度範數最大的 k 個樣本"""
        sample_scores = []
        
        self.model.eval()
        
        for sample in samples:
            grad_norm = self.compute_gradient_norm(sample)
            sample_scores.append((sample, grad_norm))
        
        # 排序
        sample_scores.sort(key=lambda x: x[1], reverse=True)
        
        # 返回 top-k
        return [s[0] for s in sample_scores[:k]]

# 使用範例
# selector = LESSSelector('gpt2')
# selected = selector.select_top_k(all_samples, k=1000)
```

### 8.3 方法對比

| 方法 | 計算成本 | 是否需要 LLM API | 效果 | 適用場景 |
|:---|:---:|:---:|:---:|:---|
| **IFD** | 低 | ❌ | 中 | 快速過濾,大規模數據 |
| **DEITA** | 高 | ✅ | 高 | 精選小數據集 |
| **LESS** | 中 | ❌ | 高 | 有預訓練模型可用 |

---

## 9. 數據管道設計

### 9.1 完整管道架構

```python
from typing import List, Dict, Callable
import json
from pathlib import Path

class DataPipeline:
    def __init__(self, output_dir='./processed_data'):
        """數據處理管道"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.processors = []
    
    def add_processor(self, name: str, func: Callable):
        """添加處理步驟"""
        self.processors.append((name, func))
    
    def run(self, input_data: List[Dict], save_intermediate=True):
        """執行完整管道"""
        data = input_data
        
        for step_num, (name, func) in enumerate(self.processors, 1):
            print(f"Step {step_num}: {name}")
            print(f"  Input size: {len(data)}")
            
            # 執行處理
            data = func(data)
            
            print(f"  Output size: {len(data)}")
            
            # 保存中間結果
            if save_intermediate:
                output_path = self.output_dir / f"step_{step_num}_{name}.jsonl"
                self._save_jsonl(data, output_path)
                print(f"  Saved to: {output_path}")
            
            print()
        
        return data
    
    def _save_jsonl(self, data, path):
        """保存為 JSONL 格式"""
        with open(path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

# 使用範例：完整數據處理管道
def example_pipeline():
    # 初始化管道
    pipeline = DataPipeline(output_dir='./processed_data')
    
    # 1. 清洗
    def clean_step(data):
        return [s for s in data if len(s.get('output', '').split()) > 10]
    
    # 2. 去重
    def dedup_step(data):
        seen = set()
        unique = []
        for s in data:
            key = s['instruction'] + s['output']
            if key not in seen:
                seen.add(key)
                unique.append(s)
        return unique
    
    # 3. IFD 過濾
    ifd_calc = IFDCalculator()
    def ifd_step(data):
        return ifd_calc.filter_by_ifd(data, min_ifd=0.3, max_ifd=0.9)
    
    # 4. DEITA 選擇 top 1000
    # scorer = DEITAScorer(openai_api_key='your-key')
    # def deita_step(data):
    #     return scorer.select_top_k(data, k=1000)
    
    # 添加處理步驟
    pipeline.add_processor('cleaning', clean_step)
    pipeline.add_processor('deduplication', dedup_step)
    pipeline.add_processor('ifd_filtering', ifd_step)
    # pipeline.add_processor('deita_selection', deita_step)
    
    # 執行管道
    raw_data = [...]  # 載入原始數據
    processed_data = pipeline.run(raw_data)
    
    return processed_data
```

### 9.2 數據版本管理

```bash
# 使用 DVC (Data Version Control) 管理數據版本
dvc init
dvc add datasets/alpaca_filtered.jsonl
git add datasets/alpaca_filtered.jsonl.dvc
git commit -m "Add filtered alpaca dataset v1.0"
```

---

## 10. 數據倫理與合規

### 10.1 數據來源合規

**必須檢查**：
- 數據授權協議 (CC-BY, MIT, Apache 2.0)
- 個人隱私保護 (GDPR, CCPA)
- 版權問題 (書籍、文章、代碼)

### 10.2 有害內容檢測

```python
from transformers import pipeline

class ToxicityDetector:
    def __init__(self):
        self.classifier = pipeline(
            "text-classification",
            model="unitary/toxic-bert"
        )
    
    def detect_toxicity(self, text, threshold=0.7):
        """檢測有害內容"""
        result = self.classifier(text)[0]
        
        is_toxic = (
            result['label'] == 'toxic' and 
            result['score'] > threshold
        )
        
        return is_toxic, result['score']
    
    def filter_toxic_samples(self, samples):
        """過濾有害樣本"""
        safe_samples = []
        
        for sample in samples:
            text = f"{sample['instruction']} {sample['output']}"
            is_toxic, score = self.detect_toxicity(text)
            
            if not is_toxic:
                safe_samples.append(sample)
        
        return safe_samples

# 使用範例
detector = ToxicityDetector()
safe_data = detector.filter_toxic_samples(raw_data)
```

### 10.3 偏見檢測

```python
def detect_bias(text, bias_keywords):
    """簡單的偏見檢測"""
    text_lower = text.lower()
    
    detected_biases = []
    for category, keywords in bias_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                detected_biases.append((category, keyword))
    
    return detected_biases

# 範例偏見關鍵字
bias_keywords = {
    'gender': ['女生不適合', '男生應該', '女性天生'],
    'race': ['某族裔天生', '種族優越'],
    'age': ['年輕人才能', '老年人不會']
}
```

---

## 11. 實踐建議

### 11.1 數據處理最佳實踐

**DO**：
- 保存所有中間結果
- 使用數據版本管理 (DVC)
- 記錄所有處理步驟與參數
- 定期審查數據質量

**DON'T**：
- 不要刪除原始數據
- 不要跳過去重步驟
- 不要忽視數據倫理
- 不要過度依賴自動化評估

### 11.2 實驗追蹤

```python
import wandb

# 初始化 Weights & Biases
wandb.init(project='data-filtering', name='alpaca-deita-v1')

# 記錄數據統計
wandb.log({
    'raw_samples': len(raw_data),
    'after_cleaning': len(cleaned_data),
    'after_dedup': len(deduped_data),
    'after_ifd': len(ifd_filtered),
    'final_samples': len(final_data),
    'avg_ifd': np.mean([s['ifd'] for s in final_data]),
    'avg_deita_score': np.mean([s['deita_score'] for s in final_data])
})
```

### 11.3 質量監控看板

```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_data_quality_dashboard(samples_with_scores):
    """繪製數據質量看板"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. IFD 分布
    ifds = [s['ifd'] for s in samples_with_scores]
    axes[0, 0].hist(ifds, bins=50, edgecolor='black')
    axes[0, 0].set_title('IFD Distribution')
    axes[0, 0].set_xlabel('IFD Score')
    
    # 2. DEITA 分數分布
    deita_scores = [s['deita_score'] for s in samples_with_scores]
    axes[0, 1].hist(deita_scores, bins=50, edgecolor='black')
    axes[0, 1].set_title('DEITA Score Distribution')
    axes[0, 1].set_xlabel('DEITA Score')
    
    # 3. 複雜度 vs 質量
    complexities = [s['complexity'] for s in samples_with_scores]
    qualities = [s['quality'] for s in samples_with_scores]
    axes[1, 0].scatter(complexities, qualities, alpha=0.5)
    axes[1, 0].set_title('Complexity vs Quality')
    axes[1, 0].set_xlabel('Complexity')
    axes[1, 0].set_ylabel('Quality')
    
    # 4. 數據長度分布
    lengths = [len(s['sample']['output'].split()) for s in samples_with_scores]
    axes[1, 1].hist(lengths, bins=50, edgecolor='black')
    axes[1, 1].set_title('Response Length Distribution')
    axes[1, 1].set_xlabel('Word Count')
    
    plt.tight_layout()
    plt.savefig('data_quality_dashboard.png', dpi=300)
    plt.show()
```

---

## 📚 總結

### 核心要點

1. **質量優於數量**：LIMA 和 Phi-1.5 證明少量高質量數據的價值
2. **三大評估維度**：複雜度、多樣性、準確性
3. **三大過濾方法**：
   - IFD (快速、無需 API)
   - DEITA (全面、需要 LLM)
   - LESS (基於梯度、需要模型)
4. **完整管道**：收集 → 清洗 → 去重 → 評估 → 過濾 → 版本管理
5. **倫理合規**：數據授權、隱私保護、有害內容檢測

### 推薦工作流

**快速實驗** (1-2 小時)：
```
原始數據 → 基礎清洗 → 規則過濾 → IFD 過濾 → 訓練
```

**標準流程** (1 天)：
```
原始數據 → 清洗 → 去重 → IFD 過濾 → DEITA 選擇 top-k → 訓練
```

**完整管道** (3-5 天)：
```
原始數據 → 清洗 → 去重 → 困惑度過濾 → IFD 評估 → 
DEITA 評分 → LESS 梯度選擇 → 多樣性平衡 → 
倫理審查 → 版本管理 → 訓練
```

---

**延伸閱讀**：

- [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)
- [DEITA: Data-Efficient Instruction Tuning](https://arxiv.org/abs/2312.15685)
- [Phi-1.5: Textbooks Are All You Need II](https://arxiv.org/abs/2309.05463)
- [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/abs/2402.04333)

**下一步**：進入 [Lab-4.2](../02-Labs/Lab-4.2-Efficient_Data_Filtering/) 實際操作數據過濾與評估。
