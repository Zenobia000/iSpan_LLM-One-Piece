{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Backend æ·±åº¦é…ç½®èˆ‡å„ªåŒ–\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "æœ¬å¯¦é©—å°‡æ·±å…¥æ¢è¨ Triton PyTorch Backend çš„é«˜ç´šé…ç½®å’Œå„ªåŒ–æŠ€è¡“ï¼Œå­¸ç¿’å¦‚ä½•æœ€å¤§åŒ–åˆ©ç”¨ PyTorch åœ¨ Triton ä¸­çš„éˆæ´»æ€§å’Œæ€§èƒ½ã€‚\n",
    "\n",
    "### æ ¸å¿ƒçŸ¥è­˜é»\n",
    "- âœ… PyTorch Backend é€²éšé…ç½®\n",
    "- âœ… å‹•æ…‹å½¢ç‹€ (Dynamic Shapes) è™•ç†\n",
    "- âœ… è¨˜æ†¶é«”æ± ç®¡ç†èˆ‡å„ªåŒ–\n",
    "- âœ… è‡ªå®šç¾©é‹ç®—å­æ•´åˆ\n",
    "- âœ… æ‰¹æ¬¡è™•ç†å„ªåŒ–\n",
    "- âœ… æ¨¡å‹ç†±æ›´æ–°æ©Ÿåˆ¶\n",
    "\n",
    "### æŠ€è¡“æ¶æ§‹\n",
    "```\n",
    "Triton Server\n",
    "â”œâ”€â”€ PyTorch Backend Engine\n",
    "â”‚   â”œâ”€â”€ Dynamic Shape Manager\n",
    "â”‚   â”œâ”€â”€ Memory Pool Allocator  \n",
    "â”‚   â”œâ”€â”€ Custom Operator Registry\n",
    "â”‚   â””â”€â”€ Batch Scheduler\n",
    "â”œâ”€â”€ Model Repository\n",
    "â”‚   â”œâ”€â”€ config.pbtxt (Advanced)\n",
    "â”‚   â”œâ”€â”€ model.py (Custom Logic)\n",
    "â”‚   â””â”€â”€ version_policy.json\n",
    "â””â”€â”€ Performance Monitor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™èˆ‡é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "import tritonclient.http as httpclient\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# è¨­å®šæ—¥èªŒ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ğŸ”§ ç’°å¢ƒè³‡è¨Šæª¢æŸ¥\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"ç•¶å‰ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šå·¥ä½œç›®éŒ„\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_REPO = BASE_DIR / \"model_repository_advanced\"\n",
    "CONFIGS_DIR = BASE_DIR / \"configs\"\n",
    "SCRIPTS_DIR = BASE_DIR / \"scripts\"\n",
    "\n",
    "# å‰µå»ºå¿…è¦ç›®éŒ„\n",
    "for dir_path in [MODEL_REPO, CONFIGS_DIR, SCRIPTS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "print(f\"ğŸ“ å·¥ä½œç›®éŒ„: {BASE_DIR}\")\n",
    "print(f\"ğŸ“ æ¨¡å‹å€‰åº«: {MODEL_REPO}\")\n",
    "print(f\"ğŸ“ é…ç½®ç›®éŒ„: {CONFIGS_DIR}\")\n",
    "print(f\"ğŸ“ è…³æœ¬ç›®éŒ„: {SCRIPTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å‹•æ…‹å½¢ç‹€è™•ç†æ¶æ§‹è¨­è¨ˆ\n",
    "\n",
    "### æ ¸å¿ƒæ¦‚å¿µ\n",
    "å‹•æ…‹å½¢ç‹€æ˜¯ç¾ä»£æ·±åº¦å­¸ç¿’æ‡‰ç”¨çš„é—œéµéœ€æ±‚ï¼Œç‰¹åˆ¥æ˜¯åœ¨è™•ç†è®Šé•·åºåˆ—ã€ä¸åŒå°ºå¯¸åœ–åƒç­‰å ´æ™¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DynamicShapeConfig:\n",
    "    \"\"\"å‹•æ…‹å½¢ç‹€é…ç½®é¡\"\"\"\n",
    "    input_name: str\n",
    "    min_shape: List[int]\n",
    "    opt_shape: List[int]\n",
    "    max_shape: List[int]\n",
    "    data_type: str = \"TYPE_FP32\"\n",
    "    \n",
    "class AdvancedModelConfig:\n",
    "    \"\"\"é«˜ç´šæ¨¡å‹é…ç½®ç”Ÿæˆå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.backend = \"pytorch\"\n",
    "        self.max_batch_size = 0  # å‹•æ…‹æ‰¹æ¬¡\n",
    "        self.dynamic_configs = []\n",
    "        \n",
    "    def add_dynamic_input(self, config: DynamicShapeConfig):\n",
    "        \"\"\"æ·»åŠ å‹•æ…‹è¼¸å…¥é…ç½®\"\"\"\n",
    "        self.dynamic_configs.append(config)\n",
    "        \n",
    "    def generate_config(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆé€²éš config.pbtxt\"\"\"\n",
    "        config_lines = [\n",
    "            f'name: \"{self.model_name}\"',\n",
    "            f'backend: \"{self.backend}\"',\n",
    "            f'max_batch_size: {self.max_batch_size}',\n",
    "            '',\n",
    "            '# å‹•æ…‹å½¢ç‹€è¼¸å…¥é…ç½®'\n",
    "        ]\n",
    "        \n",
    "        # æ·»åŠ å‹•æ…‹è¼¸å…¥\n",
    "        for i, config in enumerate(self.dynamic_configs):\n",
    "            config_lines.extend([\n",
    "                'input {',\n",
    "                f'  name: \"{config.input_name}\"',\n",
    "                f'  data_type: {config.data_type}',\n",
    "                f'  dims: [ -1 ]  # å‹•æ…‹ç¶­åº¦',\n",
    "                '}',\n",
    "                ''\n",
    "            ])\n",
    "            \n",
    "        # æ·»åŠ è¼¸å‡ºé…ç½®\n",
    "        config_lines.extend([\n",
    "            'output {',\n",
    "            '  name: \"output\"',\n",
    "            '  data_type: TYPE_FP32',\n",
    "            '  dims: [ -1 ]',\n",
    "            '}',\n",
    "            '',\n",
    "            '# å¯¦ä¾‹çµ„é…ç½®',\n",
    "            'instance_group {',\n",
    "            '  count: 2',\n",
    "            '  kind: KIND_GPU',\n",
    "            '}',\n",
    "            '',\n",
    "            '# å‹•æ…‹æ‰¹æ¬¡é…ç½®',\n",
    "            'dynamic_batching {',\n",
    "            '  preferred_batch_size: [ 4, 8, 16 ]',\n",
    "            '  max_queue_delay_microseconds: 100',\n",
    "            '  preserve_ordering: true',\n",
    "            '}',\n",
    "            '',\n",
    "            '# å„ªåŒ–é…ç½®',\n",
    "            'optimization {',\n",
    "            '  cuda {',\n",
    "            '    graphs: true',\n",
    "            '    graph_spec {',\n",
    "            '      batch_size: 1',\n",
    "            '      input {',\n",
    "            f'        key: \"{self.dynamic_configs[0].input_name if self.dynamic_configs else \"input\"}\"',\n",
    "            f'        value {{',\n",
    "            f'          dim: [ {\"16\" if self.dynamic_configs else \"224\"} ]',\n",
    "            '        }',\n",
    "            '      }',\n",
    "            '    }',\n",
    "            '  }',\n",
    "            '}'\n",
    "        ])\n",
    "        \n",
    "        return '\\n'.join(config_lines)\n",
    "\n",
    "# ç¤ºä¾‹ï¼šæ–‡æœ¬åˆ†é¡æ¨¡å‹å‹•æ…‹é…ç½®\n",
    "text_config = DynamicShapeConfig(\n",
    "    input_name=\"input_ids\",\n",
    "    min_shape=[1],      # æœ€å°åºåˆ—é•·åº¦\n",
    "    opt_shape=[128],    # æœ€å„ªåºåˆ—é•·åº¦\n",
    "    max_shape=[512],    # æœ€å¤§åºåˆ—é•·åº¦\n",
    "    data_type=\"TYPE_INT64\"\n",
    ")\n",
    "\n",
    "model_config = AdvancedModelConfig(\"text_classifier_dynamic\")\n",
    "model_config.add_dynamic_input(text_config)\n",
    "\n",
    "print(\"ğŸ”§ å‹•æ…‹å½¢ç‹€é…ç½®ç”Ÿæˆå™¨å·²å‰µå»º\")\n",
    "print(f\"ğŸ“ æ”¯æŒçš„é…ç½®é¡å‹: {len(model_config.dynamic_configs)} å€‹å‹•æ…‹è¼¸å…¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¨˜æ†¶é«”æ± ç®¡ç†èˆ‡å„ªåŒ–\n",
    "\n",
    "### è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥\n",
    "- **é åˆ†é…æ± **: é¿å…å‹•æ…‹åˆ†é…é–‹éŠ·\n",
    "- **åˆ†æ®µç®¡ç†**: ä¸åŒå¤§å°å¼µé‡åˆ†åˆ¥ç®¡ç†\n",
    "- **å›æ”¶ç­–ç•¥**: æ™ºèƒ½åƒåœ¾å›æ”¶æ©Ÿåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryPoolManager:\n",
    "    \"\"\"è¨˜æ†¶é«”æ± ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pools = {}\n",
    "        self.usage_stats = {}\n",
    "        self.peak_usage = 0\n",
    "        \n",
    "    def create_pool(self, name: str, size_mb: int):\n",
    "        \"\"\"å‰µå»ºè¨˜æ†¶é«”æ± \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            # é åˆ†é… GPU è¨˜æ†¶é«”\n",
    "            pool_size = size_mb * 1024 * 1024\n",
    "            dummy_tensor = torch.empty(pool_size // 4, dtype=torch.float32, device='cuda')\n",
    "            del dummy_tensor  # è§¸ç™¼è¨˜æ†¶é«”åˆ†é…ä½†ä¸ä¿ç•™å¼•ç”¨\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        self.pools[name] = {\n",
    "            'size_mb': size_mb,\n",
    "            'allocated': 0,\n",
    "            'peak': 0,\n",
    "            'tensors': []\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"å‰µå»ºè¨˜æ†¶é«”æ±  '{name}': {size_mb}MB\")\n",
    "        \n",
    "    def allocate_tensor(self, pool_name: str, shape: tuple, dtype=torch.float32) -> torch.Tensor:\n",
    "        \"\"\"å¾è¨˜æ†¶é«”æ± åˆ†é…å¼µé‡\"\"\"\n",
    "        if pool_name not in self.pools:\n",
    "            raise ValueError(f\"è¨˜æ†¶é«”æ±  '{pool_name}' ä¸å­˜åœ¨\")\n",
    "            \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        tensor = torch.empty(shape, dtype=dtype, device=device)\n",
    "        \n",
    "        # æ›´æ–°çµ±è¨ˆ\n",
    "        pool = self.pools[pool_name]\n",
    "        tensor_size = tensor.numel() * tensor.element_size()\n",
    "        pool['allocated'] += tensor_size\n",
    "        pool['peak'] = max(pool['peak'], pool['allocated'])\n",
    "        pool['tensors'].append(tensor)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def get_memory_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–è¨˜æ†¶é«”ä½¿ç”¨çµ±è¨ˆ\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_stats = {\n",
    "                'allocated_mb': torch.cuda.memory_allocated() / 1024**2,\n",
    "                'reserved_mb': torch.cuda.memory_reserved() / 1024**2,\n",
    "                'max_allocated_mb': torch.cuda.max_memory_allocated() / 1024**2\n",
    "            }\n",
    "            stats['gpu'] = gpu_stats\n",
    "            \n",
    "        # ç³»çµ±è¨˜æ†¶é«”\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "        stats['system'] = {\n",
    "            'rss_mb': memory_info.rss / 1024**2,\n",
    "            'vms_mb': memory_info.vms / 1024**2\n",
    "        }\n",
    "        \n",
    "        # è¨˜æ†¶é«”æ± çµ±è¨ˆ\n",
    "        pool_stats = {}\n",
    "        for name, pool in self.pools.items():\n",
    "            pool_stats[name] = {\n",
    "                'size_mb': pool['size_mb'],\n",
    "                'allocated_mb': pool['allocated'] / 1024**2,\n",
    "                'peak_mb': pool['peak'] / 1024**2,\n",
    "                'utilization': pool['allocated'] / (pool['size_mb'] * 1024**2) * 100\n",
    "            }\n",
    "        stats['pools'] = pool_stats\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def cleanup_pool(self, pool_name: str):\n",
    "        \"\"\"æ¸…ç†è¨˜æ†¶é«”æ± \"\"\"\n",
    "        if pool_name in self.pools:\n",
    "            pool = self.pools[pool_name]\n",
    "            # æ¸…ç†å¼µé‡å¼•ç”¨\n",
    "            pool['tensors'].clear()\n",
    "            pool['allocated'] = 0\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            logger.info(f\"å·²æ¸…ç†è¨˜æ†¶é«”æ±  '{pool_name}'\")\n",
    "\n",
    "# å‰µå»ºè¨˜æ†¶é«”ç®¡ç†å™¨\n",
    "memory_manager = MemoryPoolManager()\n",
    "\n",
    "# å‰µå»ºä¸åŒç”¨é€”çš„è¨˜æ†¶é«”æ± \n",
    "memory_manager.create_pool(\"inference\", 512)      # æ¨ç†å°ˆç”¨ 512MB\n",
    "memory_manager.create_pool(\"preprocessing\", 128)  # é è™•ç†å°ˆç”¨ 128MB\n",
    "memory_manager.create_pool(\"cache\", 256)          # ç·©å­˜å°ˆç”¨ 256MB\n",
    "\n",
    "print(\"ğŸ§  è¨˜æ†¶é«”æ± ç®¡ç†å™¨å·²åˆå§‹åŒ–\")\n",
    "print(f\"ğŸ“Š å‰µå»ºäº† {len(memory_manager.pools)} å€‹è¨˜æ†¶é«”æ± \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è‡ªå®šç¾© PyTorch æ¨¡å‹å¯¦ç¾\n",
    "\n",
    "### é«˜ç´šæ¨¡å‹é¡è¨­è¨ˆ\n",
    "å¯¦ç¾æ”¯æŒå‹•æ…‹å½¢ç‹€ã€è¨˜æ†¶é«”å„ªåŒ–çš„ Triton PyTorch æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºé«˜ç´šæ–‡æœ¬åˆ†é¡æ¨¡å‹\n",
    "model_dir = MODEL_REPO / \"text_classifier_advanced\" / \"1\"\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# é«˜ç´šæ¨¡å‹å¯¦ç¾\n",
    "model_code = '''\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "class AdvancedTextClassifier(nn.Module):\n",
    "    \"\"\"æ”¯æŒå‹•æ…‹å½¢ç‹€çš„é«˜ç´šæ–‡æœ¬åˆ†é¡å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=30000, embed_dim=256, hidden_dim=512, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, 8, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # æ€§èƒ½å„ªåŒ–ï¼šé ç·¨è­¯åœ–\n",
    "        self._compiled = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # å‹•æ…‹åºåˆ—é•·åº¦è™•ç†\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != 0).float()\n",
    "            \n",
    "        # åµŒå…¥å±¤\n",
    "        embeds = self.embedding(input_ids)\n",
    "        \n",
    "        # LSTM ç·¨ç¢¼\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        # æ³¨æ„åŠ›æ©Ÿåˆ¶\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out, \n",
    "                                   key_padding_mask=(attention_mask == 0))\n",
    "        \n",
    "        # æ± åŒ–ï¼šä½¿ç”¨æ³¨æ„åŠ›æ¬Šé‡å¹³å‡\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand_as(attn_out)\n",
    "        sum_embeddings = torch.sum(attn_out * mask_expanded, dim=1)\n",
    "        sum_mask = torch.sum(mask_expanded, dim=1)\n",
    "        pooled = sum_embeddings / (sum_mask + 1e-9)\n",
    "        \n",
    "        # åˆ†é¡\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"Triton PyTorch Backend é«˜ç´šæ¨¡å‹\"\"\"\n",
    "    \n",
    "    def initialize(self, args):\n",
    "        self.model_config = model_config = json.loads(args[\\'model_config\\'])\n",
    "        self.model_instance_name = args[\\'model_instance_name\\']\n",
    "        \n",
    "        # è¨­å®šæ—¥èªŒ\n",
    "        self.logger = logging.getLogger(f\"AdvancedModel-{self.model_instance_name}\")\n",
    "        \n",
    "        # è¨­å‚™é…ç½®\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(f\"cuda:{args.get(\\'model_instance_device_id\\', 0)}\")\n",
    "            self.logger.info(f\"ä½¿ç”¨ GPU: {self.device}\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.logger.info(\"ä½¿ç”¨ CPU\")\n",
    "            \n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        self.model = AdvancedTextClassifier()\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # æ€§èƒ½å„ªåŒ–ï¼šJIT ç·¨è­¯\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                # å‰µå»ºç¤ºä¾‹è¼¸å…¥é€²è¡Œ JIT ç·¨è­¯\n",
    "                example_input = torch.randint(1, 1000, (1, 128), device=self.device)\n",
    "                self.model = torch.jit.trace(self.model, example_input)\n",
    "                self.logger.info(\"JIT ç·¨è­¯æˆåŠŸ\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"JIT ç·¨è­¯å¤±æ•—: {e}\")\n",
    "        \n",
    "        # è¨˜æ†¶é«”æ± åˆå§‹åŒ–\n",
    "        self.memory_stats = {\n",
    "            \\'total_requests\\': 0,\n",
    "            \\'total_time\\': 0,\n",
    "            \\'peak_memory\\': 0\n",
    "        }\n",
    "        \n",
    "        # é ç†±æ¨¡å‹\n",
    "        self._warmup_model()\n",
    "        \n",
    "        self.logger.info(\"é«˜ç´š PyTorch æ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def _warmup_model(self):\n",
    "        \"\"\"æ¨¡å‹é ç†±\"\"\"\n",
    "        self.logger.info(\"é–‹å§‹æ¨¡å‹é ç†±...\")\n",
    "        \n",
    "        # ä¸åŒé•·åº¦çš„é ç†±è¼¸å…¥\n",
    "        warmup_lengths = [16, 64, 128, 256]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for length in warmup_lengths:\n",
    "                dummy_input = torch.randint(1, 1000, (2, length), device=self.device)\n",
    "                _ = self.model(dummy_input)\n",
    "                \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        self.logger.info(\"æ¨¡å‹é ç†±å®Œæˆ\")\n",
    "    \n",
    "    def execute(self, requests):\n",
    "        \"\"\"åŸ·è¡Œæ¨ç†è«‹æ±‚\"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # ç²å–è¼¸å…¥\n",
    "                input_ids = pb_utils.get_input_tensor_by_name(request, \"input_ids\")\n",
    "                input_data = input_ids.as_numpy()\n",
    "                \n",
    "                # è½‰æ›ç‚º PyTorch å¼µé‡\n",
    "                input_tensor = torch.from_numpy(input_data).to(self.device)\n",
    "                \n",
    "                # æ¨ç†\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_tensor)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # è½‰æ›å› numpy\n",
    "                output_data = probs.cpu().numpy()\n",
    "                \n",
    "                # å‰µå»ºè¼¸å‡ºå¼µé‡\n",
    "                output_tensor = pb_utils.Tensor(\"output\", output_data)\n",
    "                response = pb_utils.InferenceResponse(output_tensors=[output_tensor])\n",
    "                \n",
    "                # æ›´æ–°çµ±è¨ˆ\n",
    "                end_time = time.time()\n",
    "                self.memory_stats[\\'total_requests\\'] += 1\n",
    "                self.memory_stats[\\'total_time\\'] += (end_time - start_time)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    current_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "                    self.memory_stats[\\'peak_memory\\'] = max(\n",
    "                        self.memory_stats[\\'peak_memory\\'], current_memory\n",
    "                    )\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"æ¨ç†éŒ¯èª¤: {e}\")\n",
    "                error_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[],\n",
    "                    error=pb_utils.TritonError(f\"æ¨ç†å¤±æ•—: {str(e)}\")\n",
    "                )\n",
    "                responses.append(error_response)\n",
    "                continue\n",
    "                \n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"æ¸…ç†è³‡æº\"\"\"\n",
    "        self.logger.info(\"æ­£åœ¨æ¸…ç†æ¨¡å‹è³‡æº...\")\n",
    "        \n",
    "        # è¼¸å‡ºçµ±è¨ˆä¿¡æ¯\n",
    "        if self.memory_stats[\\'total_requests\\'] > 0:\n",
    "            avg_time = self.memory_stats[\\'total_time\\'] / self.memory_stats[\\'total_requests\\']\n",
    "            self.logger.info(f\"ç¸½è«‹æ±‚æ•¸: {self.memory_stats[\\'total_requests\\']}\") \n",
    "            self.logger.info(f\"å¹³å‡æ¨ç†æ™‚é–“: {avg_time:.4f}s\")\n",
    "            self.logger.info(f\"å³°å€¼è¨˜æ†¶é«”ä½¿ç”¨: {self.memory_stats[\\'peak_memory\\']:.2f}MB\")\n",
    "        \n",
    "        # æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        self.logger.info(\"æ¨¡å‹è³‡æºæ¸…ç†å®Œæˆ\")\n",
    "'''\n",
    "\n",
    "# å¯«å…¥æ¨¡å‹æ–‡ä»¶\n",
    "with open(model_dir / \"model.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(model_code)\n",
    "\n",
    "print(f\"âœ… é«˜ç´š PyTorch æ¨¡å‹å·²å‰µå»º: {model_dir / 'model.py'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. é€²éšé…ç½®æ–‡ä»¶ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆé«˜ç´šé…ç½®æ–‡ä»¶\n",
    "advanced_config = '''\n",
    "name: \"text_classifier_advanced\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 0\n",
    "\n",
    "# å‹•æ…‹è¼¸å…¥é…ç½®\n",
    "input {\n",
    "  name: \"input_ids\"\n",
    "  data_type: TYPE_INT64\n",
    "  dims: [ -1 ]  # å‹•æ…‹åºåˆ—é•·åº¦\n",
    "}\n",
    "\n",
    "output {\n",
    "  name: \"output\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: [ 10 ]  # 10 å€‹åˆ†é¡\n",
    "}\n",
    "\n",
    "# å¯¦ä¾‹çµ„é…ç½® - å¤šå¯¦ä¾‹æé«˜ååé‡\n",
    "instance_group {\n",
    "  count: 2\n",
    "  kind: KIND_GPU\n",
    "  gpus: [ 0 ]\n",
    "}\n",
    "\n",
    "# å‹•æ…‹æ‰¹æ¬¡é…ç½®\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 2, 4, 8, 16 ]\n",
    "  max_queue_delay_microseconds: 500\n",
    "  preserve_ordering: false\n",
    "  priority_levels: 2\n",
    "  default_priority_level: 0\n",
    "  \n",
    "  # æ‰¹æ¬¡å¤§å°æ§åˆ¶\n",
    "  default_queue_policy {\n",
    "    timeout_action: REJECT\n",
    "    default_timeout_microseconds: 1000000\n",
    "    allow_timeout_override: true\n",
    "    max_batch_size: 32\n",
    "  }\n",
    "}\n",
    "\n",
    "# æ¨¡å‹å„ªåŒ–é…ç½®\n",
    "optimization {\n",
    "  cuda {\n",
    "    graphs: true\n",
    "    busy_wait_events: true\n",
    "    graph_spec {\n",
    "      batch_size: 1\n",
    "      input {\n",
    "        key: \"input_ids\"\n",
    "        value {\n",
    "          dim: [ 128 ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    graph_spec {\n",
    "      batch_size: 4\n",
    "      input {\n",
    "        key: \"input_ids\"\n",
    "        value {\n",
    "          dim: [ 256 ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# æ¨¡å‹é ç†±é…ç½®\n",
    "model_warmup {\n",
    "  name: \"warmup_sample_1\"\n",
    "  batch_size: 1\n",
    "  inputs {\n",
    "    key: \"input_ids\"\n",
    "    value {\n",
    "      data_type: TYPE_INT64\n",
    "      dims: [ 128 ]\n",
    "      zero_data: true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "model_warmup {\n",
    "  name: \"warmup_sample_batch\"\n",
    "  batch_size: 8\n",
    "  inputs {\n",
    "    key: \"input_ids\"\n",
    "    value {\n",
    "      data_type: TYPE_INT64\n",
    "      dims: [ 256 ]\n",
    "      zero_data: true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# æ€§èƒ½åƒæ•¸\n",
    "parameters {\n",
    "  key: \"EXECUTION_ENV_PATH\"\n",
    "  value: { string_value: \"/opt/tritonserver/backends/python/triton_python_backend_stub\" }\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  key: \"shm_region_prefix_name\"\n",
    "  value: { string_value: \"triton_python_backend\" }\n",
    "}\n",
    "'''\n",
    "\n",
    "# å¯«å…¥é…ç½®æ–‡ä»¶\n",
    "config_path = MODEL_REPO / \"text_classifier_advanced\" / \"config.pbtxt\"\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(advanced_config.strip())\n",
    "\n",
    "print(f\"âœ… é«˜ç´šé…ç½®æ–‡ä»¶å·²å‰µå»º: {config_path}\")\n",
    "print(\"\\nğŸ”§ é…ç½®ç‰¹æ€§:\")\n",
    "print(\"- âœ… å‹•æ…‹åºåˆ—é•·åº¦æ”¯æŒ\")\n",
    "print(\"- âœ… æ™ºèƒ½æ‰¹æ¬¡èª¿åº¦\")\n",
    "print(\"- âœ… CUDA Graph å„ªåŒ–\")\n",
    "print(\"- âœ… å¤šå¯¦ä¾‹ä¸¦è¡Œ\")\n",
    "print(\"- âœ… æ¨¡å‹é ç†±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½ç›£æ§èˆ‡åˆ†æå·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"æ€§èƒ½ç›£æ§å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, triton_url=\"localhost:8000\"):\n",
    "        self.triton_url = triton_url\n",
    "        self.client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        self.metrics_history = []\n",
    "        \n",
    "    def get_server_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–æœå‹™å™¨æŒ‡æ¨™\"\"\"\n",
    "        try:\n",
    "            # ç²å–æœå‹™å™¨çµ±è¨ˆ\n",
    "            stats = self.client.get_inference_statistics()\n",
    "            \n",
    "            # ç²å–æ¨¡å‹ç‹€æ…‹\n",
    "            models = self.client.get_model_repository_index()\n",
    "            \n",
    "            metrics = {\n",
    "                'timestamp': time.time(),\n",
    "                'server_stats': stats,\n",
    "                'model_count': len(models),\n",
    "                'system_resources': self._get_system_resources()\n",
    "            }\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ç²å–æŒ‡æ¨™å¤±æ•—: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _get_system_resources(self) -> Dict[str, float]:\n",
    "        \"\"\"ç²å–ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³\"\"\"\n",
    "        resources = {\n",
    "            'cpu_percent': psutil.cpu_percent(),\n",
    "            'memory_percent': psutil.virtual_memory().percent,\n",
    "            'memory_used_gb': psutil.virtual_memory().used / 1024**3\n",
    "        }\n",
    "        \n",
    "        # GPU è³‡æº\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu = gpus[0]\n",
    "                resources.update({\n",
    "                    'gpu_utilization': gpu.load * 100,\n",
    "                    'gpu_memory_used': gpu.memoryUsed,\n",
    "                    'gpu_memory_total': gpu.memoryTotal,\n",
    "                    'gpu_memory_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100,\n",
    "                    'gpu_temperature': gpu.temperature\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return resources\n",
    "    \n",
    "    def benchmark_model(self, model_name: str, test_data: List[np.ndarray], \n",
    "                       concurrent_requests: int = 1, iterations: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"æ¨¡å‹åŸºæº–æ¸¬è©¦\"\"\"\n",
    "        \n",
    "        latencies = []\n",
    "        errors = 0\n",
    "        \n",
    "        print(f\"ğŸš€ é–‹å§‹åŸºæº–æ¸¬è©¦: {model_name}\")\n",
    "        print(f\"ğŸ“Š åƒæ•¸: {concurrent_requests} ä¸¦ç™¼, {iterations} æ¬¡è¿­ä»£\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # éš¨æ©Ÿé¸æ“‡æ¸¬è©¦æ•¸æ“š\n",
    "            test_input = test_data[i % len(test_data)]\n",
    "            \n",
    "            try:\n",
    "                # å‰µå»ºè¼¸å…¥\n",
    "                inputs = [httpclient.InferInput(\"input_ids\", test_input.shape, \"INT64\")]\n",
    "                inputs[0].set_data_from_numpy(test_input)\n",
    "                \n",
    "                # ç™¼é€è«‹æ±‚ä¸¦è¨ˆæ™‚\n",
    "                request_start = time.time()\n",
    "                response = self.client.infer(model_name, inputs)\n",
    "                request_end = time.time()\n",
    "                \n",
    "                latency = (request_end - request_start) * 1000  # è½‰æ›ç‚ºæ¯«ç§’\n",
    "                latencies.append(latency)\n",
    "                \n",
    "                if i % 20 == 0:\n",
    "                    print(f\"  é€²åº¦: {i}/{iterations}, ç•¶å‰å»¶é²: {latency:.2f}ms\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                logger.error(f\"è«‹æ±‚å¤±æ•—: {e}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š\n",
    "        if latencies:\n",
    "            stats = {\n",
    "                'total_requests': iterations,\n",
    "                'successful_requests': len(latencies),\n",
    "                'error_rate': errors / iterations * 100,\n",
    "                'total_time_seconds': total_time,\n",
    "                'throughput_rps': len(latencies) / total_time,\n",
    "                'latency_stats': {\n",
    "                    'mean_ms': np.mean(latencies),\n",
    "                    'median_ms': np.median(latencies),\n",
    "                    'p95_ms': np.percentile(latencies, 95),\n",
    "                    'p99_ms': np.percentile(latencies, 99),\n",
    "                    'min_ms': np.min(latencies),\n",
    "                    'max_ms': np.max(latencies),\n",
    "                    'std_ms': np.std(latencies)\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            stats = {\n",
    "                'total_requests': iterations,\n",
    "                'successful_requests': 0,\n",
    "                'error_rate': 100.0,\n",
    "                'total_time_seconds': total_time,\n",
    "                'throughput_rps': 0\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def plot_performance_trends(self, save_path: Optional[str] = None):\n",
    "        \"\"\"ç¹ªè£½æ€§èƒ½è¶¨å‹¢åœ–\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"âš ï¸  æ²’æœ‰æ­·å²æ•¸æ“šå¯ä»¥ç¹ªè£½\")\n",
    "            return\n",
    "            \n",
    "        # æå–æ™‚é–“åºåˆ—æ•¸æ“š\n",
    "        timestamps = [m['timestamp'] for m in self.metrics_history]\n",
    "        cpu_usage = [m['system_resources']['cpu_percent'] for m in self.metrics_history]\n",
    "        memory_usage = [m['system_resources']['memory_percent'] for m in self.metrics_history]\n",
    "        \n",
    "        # å‰µå»ºåœ–è¡¨\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Triton Server æ€§èƒ½ç›£æ§å„€è¡¨æ¿', fontsize=16)\n",
    "        \n",
    "        # CPU ä½¿ç”¨ç‡\n",
    "        axes[0, 0].plot(timestamps, cpu_usage, 'b-', linewidth=2)\n",
    "        axes[0, 0].set_title('CPU ä½¿ç”¨ç‡')\n",
    "        axes[0, 0].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "        axes[0, 1].plot(timestamps, memory_usage, 'r-', linewidth=2)\n",
    "        axes[0, 1].set_title('è¨˜æ†¶é«”ä½¿ç”¨ç‡')\n",
    "        axes[0, 1].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # GPU ä½¿ç”¨ç‡ (å¦‚æœå¯ç”¨)\n",
    "        gpu_usage = []\n",
    "        gpu_memory = []\n",
    "        for m in self.metrics_history:\n",
    "            resources = m['system_resources']\n",
    "            gpu_usage.append(resources.get('gpu_utilization', 0))\n",
    "            gpu_memory.append(resources.get('gpu_memory_percent', 0))\n",
    "            \n",
    "        axes[1, 0].plot(timestamps, gpu_usage, 'g-', linewidth=2, label='GPU åˆ©ç”¨ç‡')\n",
    "        axes[1, 0].plot(timestamps, gpu_memory, 'orange', linewidth=2, label='GPU è¨˜æ†¶é«”')\n",
    "        axes[1, 0].set_title('GPU è³‡æº')\n",
    "        axes[1, 0].set_ylabel('ä½¿ç”¨ç‡ (%)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ¨¡å‹æ•¸é‡\n",
    "        model_counts = [m['model_count'] for m in self.metrics_history]\n",
    "        axes[1, 1].plot(timestamps, model_counts, 'm-', linewidth=2)\n",
    "        axes[1, 1].set_title('è¼‰å…¥æ¨¡å‹æ•¸é‡')\n",
    "        axes[1, 1].set_ylabel('æ¨¡å‹æ•¸é‡')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"ğŸ“Š æ€§èƒ½åœ–è¡¨å·²å„²å­˜: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# å‰µå»ºæ€§èƒ½ç›£æ§å™¨\n",
    "monitor = PerformanceMonitor()\n",
    "print(\"ğŸ“Š æ€§èƒ½ç›£æ§å™¨å·²å‰µå»º\")\n",
    "\n",
    "# æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "def generate_test_data(num_samples: int = 50) -> List[np.ndarray]:\n",
    "    \"\"\"ç”Ÿæˆæ¸¬è©¦æ•¸æ“š\"\"\"\n",
    "    test_data = []\n",
    "    \n",
    "    # ä¸åŒé•·åº¦çš„åºåˆ—\n",
    "    lengths = [16, 32, 64, 128, 256]\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        length = np.random.choice(lengths)\n",
    "        # ç”Ÿæˆéš¨æ©Ÿåºåˆ—ï¼Œé¿å…å…¨é›¶\n",
    "        sequence = np.random.randint(1, 5000, size=(1, length), dtype=np.int64)\n",
    "        test_data.append(sequence)\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "test_data = generate_test_data()\n",
    "print(f\"ğŸ“‹ ç”Ÿæˆäº† {len(test_data)} å€‹æ¸¬è©¦æ¨£æœ¬\")\n",
    "print(f\"ğŸ“ åºåˆ—é•·åº¦ç¯„åœ: {[data.shape[1] for data in test_data[:5]]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Triton Server å•Ÿå‹•èˆ‡æ¨¡å‹è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»º Docker å•Ÿå‹•è…³æœ¬\n",
    "docker_script = f'''\n",
    "#!/bin/bash\n",
    "\n",
    "# Triton Server é«˜ç´šé…ç½®å•Ÿå‹•è…³æœ¬\n",
    "\n",
    "MODEL_REPO=\"{MODEL_REPO.absolute()}\"\n",
    "CONTAINER_NAME=\"triton-advanced-pytorch\"\n",
    "\n",
    "echo \"ğŸš€ å•Ÿå‹• Triton Server (PyTorch Backend é€²éš)\"\n",
    "echo \"ğŸ“ æ¨¡å‹å€‰åº«: $MODEL_REPO\"\n",
    "\n",
    "# åœæ­¢ç¾æœ‰å®¹å™¨\n",
    "docker stop $CONTAINER_NAME 2>/dev/null || true\n",
    "docker rm $CONTAINER_NAME 2>/dev/null || true\n",
    "\n",
    "# å•Ÿå‹• Triton Server\n",
    "docker run -d \\\n",
    "  --name $CONTAINER_NAME \\\n",
    "  --gpus all \\\n",
    "  -p 8000:8000 \\\n",
    "  -p 8001:8001 \\\n",
    "  -p 8002:8002 \\\n",
    "  -v \"$MODEL_REPO:/models\" \\\n",
    "  -e CUDA_VISIBLE_DEVICES=0 \\\n",
    "  nvcr.io/nvidia/tritonserver:24.01-py3 \\\n",
    "  tritonserver \\\n",
    "    --model-repository=/models \\\n",
    "    --backend-directory=/opt/tritonserver/backends \\\n",
    "    --model-control-mode=explicit \\\n",
    "    --strict-model-config=false \\\n",
    "    --log-verbose=1 \\\n",
    "    --log-info=true \\\n",
    "    --log-warning=true \\\n",
    "    --log-error=true \\\n",
    "    --exit-on-error=false \\\n",
    "    --exit-timeout-secs=120 \\\n",
    "    --buffer-manager-thread-count=2 \\\n",
    "    --model-load-thread-count=4\n",
    "\n",
    "echo \"â³ ç­‰å¾…æœå‹™å™¨å•Ÿå‹•...\"\n",
    "sleep 10\n",
    "\n",
    "# æª¢æŸ¥æœå‹™å™¨ç‹€æ…‹\n",
    "if curl -s http://localhost:8000/v2/health/ready > /dev/null; then\n",
    "    echo \"âœ… Triton Server å·²å°±ç·’\"\n",
    "    echo \"ğŸŒ HTTP: http://localhost:8000\"\n",
    "    echo \"ğŸ“Š æŒ‡æ¨™: http://localhost:8002/metrics\"\n",
    "    \n",
    "    # è¼‰å…¥é«˜ç´šæ¨¡å‹\n",
    "    echo \"ğŸ“¥ è¼‰å…¥é«˜ç´š PyTorch æ¨¡å‹...\"\n",
    "    curl -X POST http://localhost:8000/v2/repository/models/text_classifier_advanced/load\n",
    "    \n",
    "    echo \"ğŸ“‹ æª¢æŸ¥æ¨¡å‹ç‹€æ…‹:\"\n",
    "    curl -s http://localhost:8000/v2/models/text_classifier_advanced\n",
    "else\n",
    "    echo \"âŒ Triton Server å•Ÿå‹•å¤±æ•—\"\n",
    "    docker logs $CONTAINER_NAME\n",
    "    exit 1\n",
    "fi\n",
    "'''\n",
    "\n",
    "script_path = SCRIPTS_DIR / \"start_triton_advanced.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(docker_script.strip())\n",
    "\n",
    "# è¨­å®šåŸ·è¡Œæ¬Šé™\n",
    "os.chmod(script_path, 0o755)\n",
    "\n",
    "print(f\"ğŸ“œ Docker å•Ÿå‹•è…³æœ¬å·²å‰µå»º: {script_path}\")\n",
    "print(\"\\nğŸš€ è¦å•Ÿå‹• Triton Serverï¼Œè«‹åŸ·è¡Œ:\")\n",
    "print(f\"   bash {script_path}\")\n",
    "print(\"\\nâš ï¸  æ³¨æ„: ç¢ºä¿ Docker å’Œ NVIDIA Container Toolkit å·²å®‰è£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ€§èƒ½æ¸¬è©¦èˆ‡åŸºæº–è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_server(max_wait_time: int = 60) -> bool:\n",
    "    \"\"\"ç­‰å¾…æœå‹™å™¨å°±ç·’\"\"\"\n",
    "    print(\"â³ ç­‰å¾… Triton Server å°±ç·’...\")\n",
    "    \n",
    "    for i in range(max_wait_time):\n",
    "        try:\n",
    "            client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "            if client.is_server_ready():\n",
    "                print(\"âœ… Triton Server å·²å°±ç·’\")\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  ç­‰å¾…ä¸­... ({i}/{max_wait_time}s)\")\n",
    "    \n",
    "    print(\"âŒ æœå‹™å™¨å•Ÿå‹•è¶…æ™‚\")\n",
    "    return False\n",
    "\n",
    "def run_comprehensive_benchmark():\n",
    "    \"\"\"åŸ·è¡Œç¶œåˆåŸºæº–æ¸¬è©¦\"\"\"\n",
    "    \n",
    "    if not wait_for_server():\n",
    "        print(\"ç„¡æ³•é€£æ¥åˆ° Triton Serverï¼Œè«‹ç¢ºä¿æœå‹™å™¨å·²å•Ÿå‹•\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nğŸ¯ é–‹å§‹ç¶œåˆæ€§èƒ½åŸºæº–æ¸¬è©¦\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ¸¬è©¦é…ç½®\n",
    "    test_configs = [\n",
    "        {\"name\": \"å°æ‰¹æ¬¡ä½å»¶é²\", \"concurrent\": 1, \"iterations\": 50},\n",
    "        {\"name\": \"ä¸­æ‰¹æ¬¡å¹³è¡¡\", \"concurrent\": 4, \"iterations\": 100},\n",
    "        {\"name\": \"å¤§æ‰¹æ¬¡é«˜åå\", \"concurrent\": 8, \"iterations\": 200}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in test_configs:\n",
    "        print(f\"\\nğŸ“Š æ¸¬è©¦: {config['name']}\")\n",
    "        print(f\"   ä¸¦ç™¼: {config['concurrent']}, è¿­ä»£: {config['iterations']}\")\n",
    "        \n",
    "        try:\n",
    "            result = monitor.benchmark_model(\n",
    "                \"text_classifier_advanced\",\n",
    "                test_data,\n",
    "                concurrent_requests=config['concurrent'],\n",
    "                iterations=config['iterations']\n",
    "            )\n",
    "            results[config['name']] = result\n",
    "            \n",
    "            # é¡¯ç¤ºçµæœ\n",
    "            if 'latency_stats' in result:\n",
    "                stats = result['latency_stats']\n",
    "                print(f\"   âœ… ååé‡: {result['throughput_rps']:.2f} RPS\")\n",
    "                print(f\"   âš¡ å¹³å‡å»¶é²: {stats['mean_ms']:.2f}ms\")\n",
    "                print(f\"   ğŸ“ˆ P95 å»¶é²: {stats['p95_ms']:.2f}ms\")\n",
    "                print(f\"   âŒ éŒ¯èª¤ç‡: {result['error_rate']:.2f}%\")\n",
    "            else:\n",
    "                print(f\"   âŒ æ¸¬è©¦å¤±æ•—ï¼ŒéŒ¯èª¤ç‡: {result['error_rate']:.2f}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "            results[config['name']] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# åŸ·è¡Œæ€§èƒ½æ¸¬è©¦ï¼ˆå¦‚æœæœå‹™å™¨å¯ç”¨ï¼‰\n",
    "try:\n",
    "    client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "    if client.is_server_ready():\n",
    "        print(\"ğŸ¯ æª¢æ¸¬åˆ° Triton Serverï¼Œé–‹å§‹æ€§èƒ½æ¸¬è©¦\")\n",
    "        benchmark_results = run_comprehensive_benchmark()\n",
    "    else:\n",
    "        print(\"âš ï¸  Triton Server æœªå°±ç·’ï¼Œè·³éæ€§èƒ½æ¸¬è©¦\")\n",
    "        print(\"   è«‹å…ˆå•Ÿå‹•æœå‹™å™¨å¾Œå†é‹è¡Œæ­¤éƒ¨åˆ†\")\n",
    "        benchmark_results = None\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  ç„¡æ³•é€£æ¥åˆ° Triton Server: {e}\")\n",
    "    print(\"   è«‹ç¢ºä¿æœå‹™å™¨å·²å•Ÿå‹•ä¸¦å¯è¨ªå•\")\n",
    "    benchmark_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage():\n",
    "    \"\"\"åˆ†æè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§  è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # ç²å–è¨˜æ†¶é«”çµ±è¨ˆ\n",
    "    stats = memory_manager.get_memory_stats()\n",
    "    \n",
    "    # ç³»çµ±è¨˜æ†¶é«”\n",
    "    if 'system' in stats:\n",
    "        sys_stats = stats['system']\n",
    "        print(f\"ğŸ“Š ç³»çµ±è¨˜æ†¶é«”:\")\n",
    "        print(f\"   RSS: {sys_stats['rss_mb']:.2f} MB\")\n",
    "        print(f\"   VMS: {sys_stats['vms_mb']:.2f} MB\")\n",
    "    \n",
    "    # GPU è¨˜æ†¶é«”\n",
    "    if 'gpu' in stats:\n",
    "        gpu_stats = stats['gpu']\n",
    "        print(f\"\\nğŸ® GPU è¨˜æ†¶é«”:\")\n",
    "        print(f\"   å·²åˆ†é…: {gpu_stats['allocated_mb']:.2f} MB\")\n",
    "        print(f\"   å·²ä¿ç•™: {gpu_stats['reserved_mb']:.2f} MB\")\n",
    "        print(f\"   å³°å€¼: {gpu_stats['max_allocated_mb']:.2f} MB\")\n",
    "        \n",
    "        # è¨ˆç®—åˆ©ç”¨ç‡\n",
    "        if torch.cuda.is_available():\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "            utilization = gpu_stats['allocated_mb'] / total_memory * 100\n",
    "            print(f\"   åˆ©ç”¨ç‡: {utilization:.1f}%\")\n",
    "    \n",
    "    # è¨˜æ†¶é«”æ± çµ±è¨ˆ\n",
    "    if 'pools' in stats and stats['pools']:\n",
    "        print(f\"\\nğŸŠ è¨˜æ†¶é«”æ± :\")\n",
    "        for pool_name, pool_stats in stats['pools'].items():\n",
    "            print(f\"   {pool_name}:\")\n",
    "            print(f\"     å¤§å°: {pool_stats['size_mb']:.2f} MB\")\n",
    "            print(f\"     å·²ç”¨: {pool_stats['allocated_mb']:.2f} MB\")\n",
    "            print(f\"     å³°å€¼: {pool_stats['peak_mb']:.2f} MB\")\n",
    "            print(f\"     åˆ©ç”¨ç‡: {pool_stats['utilization']:.1f}%\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def create_memory_visualization(stats: Dict[str, Any]):\n",
    "    \"\"\"å‰µå»ºè¨˜æ†¶é«”ä½¿ç”¨è¦–è¦ºåŒ–\"\"\"\n",
    "    \n",
    "    if 'pools' not in stats or not stats['pools']:\n",
    "        print(\"âš ï¸  æ²’æœ‰è¨˜æ†¶é«”æ± æ•¸æ“šå¯è¦–è¦ºåŒ–\")\n",
    "        return\n",
    "    \n",
    "    # æº–å‚™æ•¸æ“š\n",
    "    pool_names = list(stats['pools'].keys())\n",
    "    pool_sizes = [stats['pools'][name]['size_mb'] for name in pool_names]\n",
    "    pool_used = [stats['pools'][name]['allocated_mb'] for name in pool_names]\n",
    "    pool_utilization = [stats['pools'][name]['utilization'] for name in pool_names]\n",
    "    \n",
    "    # å‰µå»ºåœ–è¡¨\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # è¨˜æ†¶é«”æ± å¤§å°å°æ¯”\n",
    "    axes[0].bar(pool_names, pool_sizes, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_title('è¨˜æ†¶é«”æ± å¤§å°', fontsize=14)\n",
    "    axes[0].set_ylabel('å¤§å° (MB)')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # è¨˜æ†¶é«”æ± ä½¿ç”¨æƒ…æ³\n",
    "    x = np.arange(len(pool_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1].bar(x - width/2, pool_sizes, width, label='ç¸½å®¹é‡', color='lightgray', alpha=0.7)\n",
    "    axes[1].bar(x + width/2, pool_used, width, label='å·²ä½¿ç”¨', color='orange', alpha=0.7)\n",
    "    axes[1].set_title('è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”', fontsize=14)\n",
    "    axes[1].set_ylabel('è¨˜æ†¶é«” (MB)')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(pool_names, rotation=45)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # åˆ©ç”¨ç‡åœ“é¤…åœ–\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "    axes[2].pie(pool_utilization, labels=pool_names, colors=colors[:len(pool_names)], \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "    axes[2].set_title('è¨˜æ†¶é«”æ± åˆ©ç”¨ç‡', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜åœ–è¡¨\n",
    "    chart_path = BASE_DIR / \"memory_analysis.png\"\n",
    "    plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š è¨˜æ†¶é«”åˆ†æåœ–è¡¨å·²å„²å­˜: {chart_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# åŸ·è¡Œè¨˜æ†¶é«”åˆ†æ\n",
    "print(\"ğŸ” åŸ·è¡Œè¨˜æ†¶é«”ä½¿ç”¨åˆ†æ...\")\n",
    "memory_stats = analyze_memory_usage()\n",
    "\n",
    "# å‰µå»ºè¦–è¦ºåŒ–ï¼ˆå¦‚æœæœ‰æ•¸æ“šï¼‰\n",
    "if memory_stats and 'pools' in memory_stats:\n",
    "    create_memory_visualization(memory_stats)\n",
    "else:\n",
    "    print(\"ğŸ“Š è¨˜æ†¶é«”æ± æ•¸æ“šä¸è¶³ï¼Œè·³éè¦–è¦ºåŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. è‡ªå®šç¾©é‹ç®—å­æ•´åˆç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºè‡ªå®šç¾©é‹ç®—å­æ¨¡å‹\n",
    "custom_model_dir = MODEL_REPO / \"custom_ops_model\" / \"1\"\n",
    "custom_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "custom_ops_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "class CustomAttentionLayer(nn.Module):\n",
    "    \"\"\"è‡ªå®šç¾©æ³¨æ„åŠ›å±¤ï¼Œå±•ç¤ºè‡ªå®šç¾©é‹ç®—å­æ•´åˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 256, n_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model å¿…é ˆèƒ½è¢« n_heads æ•´é™¤\"\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len = query.shape[1]\n",
    "        \n",
    "        # ç·šæ€§è®Šæ›\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key) \n",
    "        V = self.v_linear(value)\n",
    "        \n",
    "        # é‡å¡‘ç‚ºå¤šé ­\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # è‡ªå®šç¾©æ³¨æ„åŠ›è¨ˆç®—ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰\n",
    "        attention = self._custom_attention(Q, K, V, mask)\n",
    "        \n",
    "        # åˆä½µå¤šé ­\n",
    "        attention = attention.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # è¼¸å‡ºæŠ•å½±\n",
    "        output = self.out_linear(attention)\n",
    "        return output\n",
    "    \n",
    "    def _custom_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"è‡ªå®šç¾©æ³¨æ„åŠ›è¨ˆç®—ï¼Œå±•ç¤ºè‡ªå®šç¾©é‹ç®—å­\"\"\"\n",
    "        \n",
    "        # æ³¨æ„åŠ›åˆ†æ•¸è¨ˆç®—\n",
    "        if self.scale.device != Q.device:\n",
    "            self.scale = self.scale.to(Q.device)\n",
    "            \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # æ‡‰ç”¨æ©ç¢¼\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, seq_len]\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax + Dropout\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # æ‡‰ç”¨æ¬Šé‡\n",
    "        attention = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attention\n",
    "\n",
    "class CustomOpsModel(nn.Module):\n",
    "    \"\"\"åŒ…å«è‡ªå®šç¾©é‹ç®—å­çš„æ¨¡å‹\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000, d_model=256, n_heads=8, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))\n",
    "        \n",
    "        # å¤šå±¤è‡ªå®šç¾©æ³¨æ„åŠ›\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            CustomAttentionLayer(d_model, n_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        \n",
    "        self.output_norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 5)  # 5 é¡åˆ†é¡\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        seq_len = input_ids.shape[1]\n",
    "        \n",
    "        # åµŒå…¥ + ä½ç½®ç·¨ç¢¼\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # å‰µå»ºæ³¨æ„åŠ›æ©ç¢¼\n",
    "        attention_mask = (input_ids != 0).float()\n",
    "        \n",
    "        # å¤šå±¤ Transformer\n",
    "        for attention_layer, layer_norm in zip(self.attention_layers, self.layer_norms):\n",
    "            # è‡ªæ³¨æ„åŠ› + æ®˜å·®é€£æ¥\n",
    "            attn_output = attention_layer(x, x, x, attention_mask)\n",
    "            x = layer_norm(x + attn_output)\n",
    "            \n",
    "            # FFN + æ®˜å·®é€£æ¥\n",
    "            ffn_output = self.ffn(x)\n",
    "            x = layer_norm(x + ffn_output)\n",
    "        \n",
    "        # æœ€çµ‚æ¨™æº–åŒ–\n",
    "        x = self.output_norm(x)\n",
    "        \n",
    "        # å…¨å±€å¹³å‡æ± åŒ–\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand_as(x)\n",
    "        sum_embeddings = torch.sum(x * mask_expanded, dim=1)\n",
    "        sum_mask = torch.sum(mask_expanded, dim=1)\n",
    "        pooled = sum_embeddings / (sum_mask + 1e-9)\n",
    "        \n",
    "        # åˆ†é¡\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "class TritonPythonModel:\n",
    "    \"\"\"Triton è‡ªå®šç¾©é‹ç®—å­æ¨¡å‹\"\"\"\n",
    "    \n",
    "    def initialize(self, args):\n",
    "        self.model_config = json.loads(args[\\'model_config\\'])\n",
    "        \n",
    "        # è¨­å‚™è¨­å®š\n",
    "        device_id = args.get(\\'model_instance_device_id\\', 0)\n",
    "        self.device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # åˆå§‹åŒ–è‡ªå®šç¾©æ¨¡å‹\n",
    "        self.model = CustomOpsModel()\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # æ•ˆèƒ½å„ªåŒ–\n",
    "        if torch.cuda.is_available():\n",
    "            # å•Ÿç”¨å„ªåŒ–\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            \n",
    "        print(f\"è‡ªå®šç¾©é‹ç®—å­æ¨¡å‹å·²è¼‰å…¥åˆ° {self.device}\")\n",
    "        \n",
    "    def execute(self, requests):\n",
    "        responses = []\n",
    "        \n",
    "        for request in requests:\n",
    "            try:\n",
    "                # ç²å–è¼¸å…¥\n",
    "                input_tensor = pb_utils.get_input_tensor_by_name(request, \"input_ids\")\n",
    "                input_data = torch.from_numpy(input_tensor.as_numpy()).to(self.device)\n",
    "                \n",
    "                # æ¨ç†\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_data)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # è¼¸å‡º\n",
    "                output_data = probs.cpu().numpy()\n",
    "                output_tensor = pb_utils.Tensor(\"output\", output_data)\n",
    "                response = pb_utils.InferenceResponse(output_tensors=[output_tensor])\n",
    "                \n",
    "            except Exception as e:\n",
    "                response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[],\n",
    "                    error=pb_utils.TritonError(f\"æ¨ç†éŒ¯èª¤: {str(e)}\")\n",
    "                )\n",
    "            \n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def finalize(self):\n",
    "        print(\"è‡ªå®šç¾©é‹ç®—å­æ¨¡å‹è³‡æºå·²æ¸…ç†\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "'''\n",
    "\n",
    "# å¯«å…¥è‡ªå®šç¾©é‹ç®—å­æ¨¡å‹\n",
    "with open(custom_model_dir / \"model.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_ops_code)\n",
    "\n",
    "# å‰µå»ºé…ç½®æ–‡ä»¶\n",
    "custom_config = '''\n",
    "name: \"custom_ops_model\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "\n",
    "input {\n",
    "  name: \"input_ids\"\n",
    "  data_type: TYPE_INT64\n",
    "  dims: [ -1 ]\n",
    "}\n",
    "\n",
    "output {\n",
    "  name: \"output\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: [ 5 ]\n",
    "}\n",
    "\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8, 16 ]\n",
    "  max_queue_delay_microseconds: 200\n",
    "}\n",
    "\n",
    "# è‡ªå®šç¾©é‹ç®—å­å„ªåŒ–åƒæ•¸\n",
    "parameters {\n",
    "  key: \"FORCE_CPU_ONLY_INPUT_TENSORS\"\n",
    "  value: { string_value: \"no\" }\n",
    "}\n",
    "'''\n",
    "\n",
    "config_path = MODEL_REPO / \"custom_ops_model\" / \"config.pbtxt\"\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_config.strip())\n",
    "\n",
    "print(\"âœ… è‡ªå®šç¾©é‹ç®—å­æ¨¡å‹å·²å‰µå»º\")\n",
    "print(f\"ğŸ“ æ¨¡å‹è·¯å¾‘: {custom_model_dir}\")\n",
    "print(f\"ğŸ”§ é…ç½®æ–‡ä»¶: {config_path}\")\n",
    "print(\"\\nğŸ¯ ç‰¹æ€§:\")\n",
    "print(\"- âœ… è‡ªå®šç¾©å¤šé ­æ³¨æ„åŠ›å±¤\")\n",
    "print(\"- âœ… å„ªåŒ–çš„æ³¨æ„åŠ›è¨ˆç®—\")\n",
    "print(\"- âœ… å¤šå±¤ Transformer æ¶æ§‹\")\n",
    "print(\"- âœ… CUDA å„ªåŒ–è¨­å®š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ç¸½çµèˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### ğŸ¯ æœ¬å¯¦é©—é‡é»å›é¡§\n",
    "\n",
    "é€šéæœ¬å¯¦é©—ï¼Œæˆ‘å€‘æ·±å…¥æ¢è¨äº† Triton PyTorch Backend çš„é«˜ç´šé…ç½®å’Œå„ªåŒ–æŠ€è¡“ï¼š\n",
    "\n",
    "#### æ ¸å¿ƒæŠ€è¡“æˆæœ\n",
    "1. **å‹•æ…‹å½¢ç‹€è™•ç†**: å¯¦ç¾äº†éˆæ´»çš„åºåˆ—é•·åº¦æ”¯æ´\n",
    "2. **è¨˜æ†¶é«”æ± ç®¡ç†**: å»ºç«‹äº†é«˜æ•ˆçš„è¨˜æ†¶é«”åˆ†é…ç­–ç•¥\n",
    "3. **è‡ªå®šç¾©é‹ç®—å­**: æ•´åˆäº†å„ªåŒ–çš„æ³¨æ„åŠ›æ©Ÿåˆ¶\n",
    "4. **æ€§èƒ½ç›£æ§**: æ§‹å»ºäº†å®Œæ•´çš„ç›£æ§å’Œåˆ†æå·¥å…·\n",
    "5. **é€²éšé…ç½®**: æŒæ¡äº†ä¼æ¥­ç´šé…ç½®æœ€ä½³å¯¦è¸\n",
    "\n",
    "#### ä¼æ¥­ç´šéƒ¨ç½²è¦é»\n",
    "- âœ… **CUDA Graph å„ªåŒ–**: é™ä½ GPU å…§æ ¸å•Ÿå‹•é–‹éŠ·\n",
    "- âœ… **å‹•æ…‹æ‰¹æ¬¡èª¿åº¦**: å¹³è¡¡å»¶é²èˆ‡ååé‡\n",
    "- âœ… **è¨˜æ†¶é«”é åˆ†é…**: é¿å…å‹•æ…‹åˆ†é…å°è‡´çš„æ€§èƒ½æŠ–å‹•\n",
    "- âœ… **æ¨¡å‹é ç†±**: ç¢ºä¿é¦–æ¬¡æ¨ç†çš„ç©©å®šæ€§èƒ½\n",
    "- âœ… **å¤šå¯¦ä¾‹éƒ¨ç½²**: æœ€å¤§åŒ– GPU è³‡æºåˆ©ç”¨ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæœ€ä½³å¯¦è¸ç¸½çµå ±å‘Š\n",
    "def generate_best_practices_report():\n",
    "    \"\"\"ç”Ÿæˆæœ€ä½³å¯¦è¸å ±å‘Š\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        \"title\": \"PyTorch Backend é«˜ç´šå„ªåŒ–æœ€ä½³å¯¦è¸\",\n",
    "        \"sections\": {\n",
    "            \"é…ç½®å„ªåŒ–\": {\n",
    "                \"å‹•æ…‹æ‰¹æ¬¡\": [\n",
    "                    \"æ ¹æ“šç¡¬é«”è³‡æºè¨­å®šåˆé©çš„ preferred_batch_size\",\n",
    "                    \"èª¿æ•´ max_queue_delay_microseconds å¹³è¡¡å»¶é²\",\n",
    "                    \"ä½¿ç”¨ preserve_ordering=false æé«˜ååé‡\"\n",
    "                ],\n",
    "                \"CUDA å„ªåŒ–\": [\n",
    "                    \"å•Ÿç”¨ CUDA Graph (graphs: true)\",\n",
    "                    \"é å®šç¾©å¸¸ç”¨è¼¸å…¥å½¢ç‹€çš„ graph_spec\",\n",
    "                    \"è¨­å®š busy_wait_events æ¸›å°‘å»¶é²\"\n",
    "                ],\n",
    "                \"å¯¦ä¾‹çµ„è¨­å®š\": [\n",
    "                    \"GPU æ¨¡å‹ä½¿ç”¨ KIND_GPU\",\n",
    "                    \"æ ¹æ“š GPU è¨˜æ†¶é«”è¨­å®šå¯¦ä¾‹æ•¸é‡\",\n",
    "                    \"å¤§æ¨¡å‹è€ƒæ…® model parallel\"\n",
    "                ]\n",
    "            },\n",
    "            \"è¨˜æ†¶é«”ç®¡ç†\": {\n",
    "                \"é åˆ†é…ç­–ç•¥\": [\n",
    "                    \"æ¨ç†å‰é åˆ†é…è¨˜æ†¶é«”æ± \",\n",
    "                    \"åˆ†ä¸åŒå¤§å°ç´šåˆ¥ç®¡ç†å¼µé‡\",\n",
    "                    \"å®šæœŸæ¸…ç†æœªä½¿ç”¨çš„å¼µé‡\"\n",
    "                ],\n",
    "                \"GPU è¨˜æ†¶é«”\": [\n",
    "                    \"ç›£æ§ GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡\",\n",
    "                    \"é¿å…è¨˜æ†¶é«”ç¢ç‰‡åŒ–\",\n",
    "                    \"ä½¿ç”¨ torch.cuda.empty_cache() é©æ™‚æ¸…ç†\"\n",
    "                ]\n",
    "            },\n",
    "            \"æ€§èƒ½èª¿å„ª\": {\n",
    "                \"PyTorch å„ªåŒ–\": [\n",
    "                    \"å•Ÿç”¨ torch.backends.cudnn.benchmark\",\n",
    "                    \"ä½¿ç”¨ JIT ç·¨è­¯ (torch.jit.trace)\",\n",
    "                    \"è€ƒæ…® TensorFloat-32 (TF32) å„ªåŒ–\"\n",
    "                ],\n",
    "                \"æ¨¡å‹å„ªåŒ–\": [\n",
    "                    \"å¯¦æ–½æ¨¡å‹é ç†±é™ä½å†·å•Ÿå‹•å»¶é²\",\n",
    "                    \"ä½¿ç”¨ torch.no_grad() é™ä½è¨˜æ†¶é«”ä½¿ç”¨\",\n",
    "                    \"è€ƒæ…®æ··åˆç²¾åº¦æ¨ç†\"\n",
    "                ]\n",
    "            },\n",
    "            \"ç›£æ§å‘Šè­¦\": {\n",
    "                \"é—œéµæŒ‡æ¨™\": [\n",
    "                    \"æ¨ç†å»¶é² (P50, P95, P99)\",\n",
    "                    \"ååé‡ (RPS)\",\n",
    "                    \"GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡\",\n",
    "                    \"éŒ¯èª¤ç‡\"\n",
    "                ],\n",
    "                \"å‘Šè­¦é–¾å€¼\": [\n",
    "                    \"P99 å»¶é² > 500ms\",\n",
    "                    \"éŒ¯èª¤ç‡ > 1%\",\n",
    "                    \"GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡ > 90%\",\n",
    "                    \"å¯¦ä¾‹å¥åº·ç‹€æ…‹ç•°å¸¸\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"common_pitfalls\": {\n",
    "            \"é…ç½®é™·é˜±\": [\n",
    "                \"å¿˜è¨˜è¨­å®š max_batch_size=0 ç”¨æ–¼å‹•æ…‹æ‰¹æ¬¡\",\n",
    "                \"å¯¦ä¾‹æ•¸é‡éå¤šå°è‡´è¨˜æ†¶é«”ä¸è¶³\",\n",
    "                \"æœªæ­£ç¢ºé…ç½®å‹•æ…‹å½¢ç‹€å°è‡´å½¢ç‹€ä¸åŒ¹é…\"\n",
    "            ],\n",
    "            \"æ€§èƒ½é™·é˜±\": [\n",
    "                \"é »ç¹çš„ CPU-GPU æ•¸æ“šå‚³è¼¸\",\n",
    "                \"æœªä½¿ç”¨æ‰¹æ¬¡æ¨ç†\",\n",
    "                \"è¨˜æ†¶é«”æ´©æ¼å°è‡´ OOM\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # è¼¸å‡ºå ±å‘Š\n",
    "    print(f\"ğŸ“‹ {report['title']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for section_name, content in report['sections'].items():\n",
    "        print(f\"\\nğŸ”¹ {section_name}\")\n",
    "        for subsection, items in content.items():\n",
    "            print(f\"  ğŸ“Œ {subsection}:\")\n",
    "            for item in items:\n",
    "                print(f\"     â€¢ {item}\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸  å¸¸è¦‹é™·é˜±\")\n",
    "    for category, pitfalls in report['common_pitfalls'].items():\n",
    "        print(f\"  ğŸš« {category}:\")\n",
    "        for pitfall in pitfalls:\n",
    "            print(f\"     â€¢ {pitfall}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# ç”Ÿæˆå ±å‘Š\n",
    "best_practices = generate_best_practices_report()\n",
    "\n",
    "print(\"\\nğŸ‰ PyTorch Backend é«˜ç´šé…ç½®å¯¦é©—å®Œæˆï¼\")\n",
    "print(\"\\nğŸ“š ä¸‹ä¸€æ­¥å­¸ç¿’å»ºè­°:\")\n",
    "print(\"1. ğŸ”§ å¯¦è¸ TensorRT Backend æ•´åˆ (Notebook 02)\")\n",
    "print(\"2. ğŸš€ æ¢ç´¢ vLLM Backend æ•´åˆ (Notebook 03)\")\n",
    "print(\"3. ğŸ› ï¸  é–‹ç™¼è‡ªå®šç¾© Python Backend (Notebook 04)\")\n",
    "print(\"4. ğŸ“Š æ·±å…¥æ€§èƒ½èª¿å„ªèˆ‡ç›£æ§\")\n",
    "print(\"5. ğŸ¢ ä¼æ¥­ç´šéƒ¨ç½²å¯¦è¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— ç›¸é—œè³‡æºèˆ‡å»¶ä¼¸é–±è®€\n",
    "\n",
    "### å®˜æ–¹æ–‡æª”\n",
    "- [Triton PyTorch Backend](https://github.com/triton-inference-server/pytorch_backend)\n",
    "- [CUDA Graph å„ªåŒ–æŒ‡å—](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)\n",
    "- [PyTorch JIT ç·¨è­¯](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)\n",
    "\n",
    "### æ€§èƒ½å„ªåŒ–\n",
    "- [PyTorch æ€§èƒ½èª¿å„ªæŒ‡å—](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "- [NVIDIA GPU å„ªåŒ–æœ€ä½³å¯¦è¸](https://docs.nvidia.com/deeplearning/performance/index.html)\n",
    "- [Triton æ€§èƒ½èª¿å„ª](https://github.com/triton-inference-server/server/blob/main/docs/optimization.md)\n",
    "\n",
    "### å¯¦è¸æ¡ˆä¾‹\n",
    "- [ä¼æ¥­ç´š AI æ¨ç†å¹³å°è¨­è¨ˆ](https://developer.nvidia.com/blog/deploying-ai-at-scale-with-triton-inference-server/)\n",
    "- [å¤§è¦æ¨¡ NLP æ¨¡å‹éƒ¨ç½²](https://developer.nvidia.com/blog/how-to-deploy-almost-any-hugging-face-model-on-nvidia-triton-inference-server/)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ å¯¦é©—å®Œæˆæ¨™èªŒ**: PyTorch Backend é«˜ç´šé…ç½®èˆ‡å„ªåŒ–æŠ€è¡“æŒæ¡ âœ…"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}