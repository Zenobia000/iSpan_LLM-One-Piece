{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: IAÂ³ - Model Merging and Deployment\n",
    "\n",
    "## ðŸŽ¯ Experiment Objectives\n",
    "\n",
    "This Notebook demonstrates one of the core advantages of IAÂ³ (Infused Adapter by Inhibiting and Amplifying Inner Activations): **scaling vectors can be fully merged into the base model, enabling zero-overhead inference deployment**.\n",
    "\n",
    "### Key Learning Points\n",
    "- Understand the mathematical merging principles of IAÂ³ scaling vectors\n",
    "- Implement complete model merging and saving workflow\n",
    "- Verify functional consistency before and after merging\n",
    "- Analyze performance improvements from merging\n",
    "- Master production deployment best practices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel, \n",
    "    IA3Config,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained IAÂ³ Model\n",
    "\n",
    "First, load the IAÂ³ adapter model trained in `02-Train.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and adapter paths\n",
    "base_model_name = \"gpt2\"\n",
    "adapter_path = \"./ia3-gpt2-imdb\"  # Assume this is the trained adapter path\n",
    "\n",
    "# Load base model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"Base model parameters: {base_model.num_parameters():,}\")\n",
    "print(f\"Base model memory usage: {base_model.get_memory_footprint() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IAÂ³ adapter (if exists)\n",
    "if os.path.exists(adapter_path):\n",
    "    peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    print(f\"Successfully loaded IAÂ³ adapter: {adapter_path}\")\n",
    "else:\n",
    "    # If no trained adapter exists, create a demo IAÂ³ configuration\n",
    "    print(\"No trained adapter found, creating demo IAÂ³ configuration...\")\n",
    "    \n",
    "    ia3_config = IA3Config(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"c_attn\", \"mlp.c_proj\"],\n",
    "        feedforward_modules=[\"mlp.c_proj\"],\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(base_model, ia3_config)\n",
    "    print(\"Created demo IAÂ³ model\")\n",
    "\n",
    "# Display trainable parameter statistics\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IAÂ³ Scaling Vector Merging Principles\n",
    "\n",
    "### 3.1 Theoretical Foundation\n",
    "\n",
    "IAÂ³ core consists of three scaling vectors:\n",
    "- `l_k`: Key vector scaling\n",
    "- `l_v`: Value vector scaling  \n",
    "- `l_ff`: Feed-forward network scaling\n",
    "\n",
    "**Mathematical Merging Principle**:\n",
    "```\n",
    "Original operation: output = scaling_vector âŠ™ (W @ input)\n",
    "After merging:      output = (scaling_vector âŠ™ W) @ input\n",
    "```\n",
    "\n",
    "Where `âŠ™` denotes element-wise multiplication. This merging is mathematically equivalent and doesn't change model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze IAÂ³ adapter structure\n",
    "def analyze_ia3_adapters(model):\n",
    "    \"\"\"Analyze IAÂ³ adapter structure and parameters\"\"\"\n",
    "    ia3_params = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'ia3' in name.lower():\n",
    "            ia3_params[name] = {\n",
    "                'shape': param.shape,\n",
    "                'dtype': param.dtype,\n",
    "                'requires_grad': param.requires_grad,\n",
    "                'device': param.device\n",
    "            }\n",
    "            print(f\"IAÂ³ parameter: {name}\")\n",
    "            print(f\"  Shape: {param.shape}\")\n",
    "            print(f\"  Data type: {param.dtype}\")\n",
    "            print(f\"  Parameter count: {param.numel():,}\")\n",
    "            print()\n",
    "    \n",
    "    return ia3_params\n",
    "\n",
    "ia3_structure = analyze_ia3_adapters(peft_model)\n",
    "print(f\"Total IAÂ³ adapters: {len(ia3_structure)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pre-merge Performance Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, test_prompts, num_runs=5):\n",
    "    \"\"\"Inference performance benchmark\"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                if run == 0:  # Only save results on first run\n",
    "                    results.append({\n",
    "                        'prompt': prompt,\n",
    "                        'generated': generated_text[len(prompt):].strip()\n",
    "                    })\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    return avg_time, results\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"This movie is absolutely\",\n",
    "    \"I think this film was\",\n",
    "    \"The acting in this movie\",\n",
    "    \"Overall, my impression is\"\n",
    "]\n",
    "\n",
    "print(\"=== Pre-merge Performance Test ===\")\n",
    "pre_merge_time, pre_merge_results = benchmark_inference(peft_model, tokenizer, test_prompts)\n",
    "print(f\"Average inference time: {pre_merge_time:.4f} seconds\")\n",
    "print(f\"Memory usage: {peft_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== Pre-merge Generation Examples ===\")\n",
    "for i, result in enumerate(pre_merge_results[:2]):  # Only show first two examples\n",
    "    print(f\"Prompt {i+1}: {result['prompt']}\")\n",
    "    print(f\"Generated: {result['generated']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Model Merging\n",
    "\n",
    "### 4.1 Using PEFT's merge_and_unload Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Starting Model Merging Process ===\")\n",
    "print(\"Merging IAÂ³ scaling vectors into base model weights...\")\n",
    "\n",
    "# Execute merge operation\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "print(\"âœ… Merge completed!\")\n",
    "print(f\"Merged model parameters: {merged_model.num_parameters():,}\")\n",
    "print(f\"Merged model memory usage: {merged_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Clean up original PEFT model to free memory\n",
    "del peft_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Post-merge Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Post-merge Performance Test ===\")\n",
    "post_merge_time, post_merge_results = benchmark_inference(merged_model, tokenizer, test_prompts)\n",
    "print(f\"Average inference time: {post_merge_time:.4f} seconds\")\n",
    "print(f\"Memory usage: {merged_model.get_memory_footprint() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== Post-merge Generation Examples ===\")\n",
    "for i, result in enumerate(post_merge_results[:2]):\n",
    "    print(f\"Prompt {i+1}: {result['prompt']}\")\n",
    "    print(f\"Generated: {result['generated']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance improvement\n",
    "time_improvement = (pre_merge_time - post_merge_time) / pre_merge_time * 100\n",
    "\n",
    "print(\"=== Performance Comparison Results ===\")\n",
    "print(f\"Pre-merge inference time: {pre_merge_time:.4f} seconds\")\n",
    "print(f\"Post-merge inference time: {post_merge_time:.4f} seconds\")\n",
    "print(f\"Inference speed improvement: {time_improvement:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Deployment Advantages Summary ===\")\n",
    "print(\"âœ… Inference latency: Zero overhead (scaling vectors merged)\")\n",
    "print(\"âœ… Memory efficiency: No need to load additional adapters\")\n",
    "print(\"âœ… Deployment simplification: Single model file only\")\n",
    "print(\"âœ… Hardware compatibility: Same requirements as original model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functional Consistency Verification\n",
    "\n",
    "Verify the consistency of model outputs before and after merging (under same random seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_consistency(pre_results, post_results, tolerance=1e-6):\n",
    "    \"\"\"Verify consistency of outputs before and after merging\"\"\"\n",
    "    print(\"=== Functional Consistency Verification ===\")\n",
    "    \n",
    "    consistent_count = 0\n",
    "    total_count = len(pre_results)\n",
    "    \n",
    "    for i, (pre, post) in enumerate(zip(pre_results, post_results)):\n",
    "        # Compare generated text (may not be identical due to randomness)\n",
    "        pre_text = pre['generated'].strip()\n",
    "        post_text = post['generated'].strip()\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  Prompt: {pre['prompt']}\")\n",
    "        print(f\"  Pre-merge: {pre_text[:100]}{'...' if len(pre_text) > 100 else ''}\")\n",
    "        print(f\"  Post-merge: {post_text[:100]}{'...' if len(post_text) > 100 else ''}\")\n",
    "        \n",
    "        # Check if identical (exact match)\n",
    "        if pre_text == post_text:\n",
    "            consistent_count += 1\n",
    "            print(f\"  âœ… Perfectly consistent\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Differences (normal due to generation randomness)\")\n",
    "    \n",
    "    print(f\"\\nPerfectly consistent samples: {consistent_count}/{total_count}\")\n",
    "    print(\"Note: Due to randomness in text generation, outputs may differ, which is normal.\")\n",
    "    print(\"The important thing is that model structure and parameters are correctly merged.\")\n",
    "\n",
    "verify_consistency(pre_merge_results, post_merge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Saving and Deployment\n",
    "\n",
    "### 6.1 Save Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set save path\n",
    "merged_model_path = \"./ia3-gpt2-merged\"\n",
    "\n",
    "print(f\"=== Saving merged model to: {merged_model_path} ===\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "print(\"âœ… Model saved successfully!\")\n",
    "\n",
    "# Check saved files\n",
    "saved_files = os.listdir(merged_model_path)\n",
    "print(f\"\\nSaved files: {saved_files}\")\n",
    "\n",
    "# Calculate model size\n",
    "total_size = 0\n",
    "for file in saved_files:\n",
    "    file_path = os.path.join(merged_model_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        total_size += size\n",
    "        print(f\"  {file}: {size / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nTotal model size: {total_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Verify Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from saved path\n",
    "print(\"=== Verifying Saved Model ===\")\n",
    "\n",
    "# Reload\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "reloaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(f\"âœ… Successfully loaded saved model\")\n",
    "print(f\"Reloaded model parameters: {reloaded_model.num_parameters():,}\")\n",
    "\n",
    "# Quick test\n",
    "test_prompt = \"This movie is absolutely\"\n",
    "inputs = reloaded_tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=reloaded_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = reloaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nTest generation:\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Generated: {generated_text[len(test_prompt):].strip()}\")\n",
    "print(\"\\nâœ… Saved model functions normally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Production Deployment Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment configuration file\n",
    "deployment_config = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": base_model_name,\n",
    "        \"peft_method\": \"IA3\",\n",
    "        \"merged\": True,\n",
    "        \"total_parameters\": merged_model.num_parameters(),\n",
    "        \"model_size_mb\": total_size / 1024**2\n",
    "    },\n",
    "    \"performance_metrics\": {\n",
    "        \"inference_time_seconds\": post_merge_time,\n",
    "        \"memory_usage_mb\": merged_model.get_memory_footprint() / 1024**2,\n",
    "        \"speed_improvement_percent\": time_improvement\n",
    "    },\n",
    "    \"deployment_requirements\": {\n",
    "        \"python_packages\": [\n",
    "            \"torch>=1.9.0\",\n",
    "            \"transformers>=4.20.0\",\n",
    "            \"tokenizers>=0.12.0\"\n",
    "        ],\n",
    "        \"minimum_gpu_memory_gb\": 4,\n",
    "        \"recommended_gpu_memory_gb\": 8\n",
    "    },\n",
    "    \"usage_example\": {\n",
    "        \"load_command\": f\"AutoModelForCausalLM.from_pretrained('{merged_model_path}')\",\n",
    "        \"inference_note\": \"No additional adapter loading required - model is fully merged\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration file\n",
    "config_path = os.path.join(merged_model_path, \"deployment_config.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"=== Production Deployment Guide ===\")\n",
    "print(json.dumps(deployment_config, indent=2, ensure_ascii=False))\n",
    "print(f\"\\nConfiguration file saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. IAÂ³ vs Other PEFT Methods Merging Comparison\n",
    "\n",
    "### 7.1 Merging Feasibility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT method merging capability comparison table\n",
    "peft_comparison = {\n",
    "    \"LoRA\": {\n",
    "        \"can_merge\": True,\n",
    "        \"merge_principle\": \"Matrix decomposition W + AÂ·B\",\n",
    "        \"inference_overhead\": \"Zero (after merge)\",\n",
    "        \"deployment_complexity\": \"Simple\"\n",
    "    },\n",
    "    \"IA3\": {\n",
    "        \"can_merge\": True,\n",
    "        \"merge_principle\": \"Scaling vector fusion\",\n",
    "        \"inference_overhead\": \"Zero (after merge)\",\n",
    "        \"deployment_complexity\": \"Simple\"\n",
    "    },\n",
    "    \"AdapterLayers\": {\n",
    "        \"can_merge\": False,\n",
    "        \"merge_principle\": \"Non-mergeable (new modules)\",\n",
    "        \"inference_overhead\": \"Present (bottleneck layers)\",\n",
    "        \"deployment_complexity\": \"Medium\"\n",
    "    },\n",
    "    \"Prompt_Tuning\": {\n",
    "        \"can_merge\": False,\n",
    "        \"merge_principle\": \"Non-mergeable (input modification)\",\n",
    "        \"inference_overhead\": \"Present (soft prompts)\",\n",
    "        \"deployment_complexity\": \"Medium\"\n",
    "    },\n",
    "    \"Prefix_Tuning\": {\n",
    "        \"can_merge\": False,\n",
    "        \"merge_principle\": \"Non-mergeable (attention modification)\",\n",
    "        \"inference_overhead\": \"Present (prefix injection)\",\n",
    "        \"deployment_complexity\": \"Complex\"\n",
    "    },\n",
    "    \"BitFit\": {\n",
    "        \"can_merge\": False,\n",
    "        \"merge_principle\": \"Already merged (bias parameters)\",\n",
    "        \"inference_overhead\": \"Zero (native by design)\",\n",
    "        \"deployment_complexity\": \"Simple\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== PEFT Method Merging Capability Comparison ===\")\n",
    "print(f\"{'Method':<15} {'Mergeable':<10} {'Merge Principle':<25} {'Inference OH':<15} {'Deploy Complexity':<15}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for method, info in peft_comparison.items():\n",
    "    can_merge = \"âœ…\" if info[\"can_merge\"] else \"âŒ\"\n",
    "    print(f\"{method:<15} {can_merge:<10} {info['merge_principle']:<25} {info['inference_overhead']:<15} {info['deployment_complexity']:<15}\")\n",
    "\n",
    "print(\"\\n=== IAÂ³ Unique Advantages ===\")\n",
    "print(\"âœ… Ultimate parameter efficiency: Only ~0.01% parameters\")\n",
    "print(\"âœ… Fully mergeable: Scaling vectors can be mathematically fused\")\n",
    "print(\"âœ… Zero inference overhead: Same as original model after merge\")\n",
    "print(\"âœ… Deployment friendly: Single model file, no extra dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Recommendations\n",
    "\n",
    "### 8.1 When to Choose IAÂ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== IAÂ³ Usage Scenario Recommendations ===\")\n",
    "print(\"\")\n",
    "print(\"ðŸŽ¯ Most Suitable Scenarios:\")\n",
    "print(\"  â€¢ Extremely resource-constrained environments (edge computing, mobile devices)\")\n",
    "print(\"  â€¢ Need for rapid prototyping and experimentation\")\n",
    "print(\"  â€¢ Inference speed-sensitive production environments\")\n",
    "print(\"  â€¢ Need to deploy multiple task models simultaneously\")\n",
    "print(\"  â€¢ Strict model size limitations\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸  Scenarios Requiring Careful Consideration:\")\n",
    "print(\"  â€¢ Complex tasks requiring extensive parameter adjustment\")\n",
    "print(\"  â€¢ Domains very different from pre-training tasks\")\n",
    "print(\"  â€¢ Need to learn entirely new feature representations\")\n",
    "print(\"\")\n",
    "print(\"ðŸ”„ Combination Strategies with Other Methods:\")\n",
    "print(\"  â€¢ IAÂ³ + LoRA: Balance efficiency and performance\")\n",
    "print(\"  â€¢ Start with IAÂ³ for quick validation, then use LoRA for fine-tuning\")\n",
    "print(\"  â€¢ Use different PEFT methods for different layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Deployment Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== IAÂ³ Deployment Checklist ===\")\n",
    "print(\"\")\n",
    "print(\"ðŸ“‹ Pre-merge Checks:\")\n",
    "print(\"  â–¡ Confirm IAÂ³ adapter training completed and saved\")\n",
    "print(\"  â–¡ Validate adapter performance on validation set\")\n",
    "print(\"  â–¡ Record pre-merge baseline performance\")\n",
    "print(\"\")\n",
    "print(\"ðŸ”„ Merge Process:\")\n",
    "print(\"  â–¡ Use merge_and_unload() method\")\n",
    "print(\"  â–¡ Check merged model parameter count unchanged\")\n",
    "print(\"  â–¡ Verify merged model functions normally\")\n",
    "print(\"\")\n",
    "print(\"âœ… Post-deployment Verification:\")\n",
    "print(\"  â–¡ Inference speed testing\")\n",
    "print(\"  â–¡ Memory usage monitoring\")\n",
    "print(\"  â–¡ Output quality sampling checks\")\n",
    "print(\"  â–¡ Long-term stability testing\")\n",
    "print(\"\")\n",
    "print(\"ðŸ“š Documentation:\")\n",
    "print(\"  â–¡ Save deployment configuration file\")\n",
    "print(\"  â–¡ Record performance benchmark data\")\n",
    "print(\"  â–¡ Prepare rollback plan\")\n",
    "print(\"  â–¡ Update API documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Through this experiment, we explored one of IAÂ³'s core advantages: **model merging and zero-overhead deployment**.\n",
    "\n",
    "### 9.1 Key Learning Outcomes\n",
    "\n",
    "1. **Theoretical Understanding**: Mastered the mathematical merging principles of IAÂ³ scaling vectors\n",
    "2. **Practical Experience**: Completed the full pipeline of merging, verification, and deployment\n",
    "3. **Performance Analysis**: Quantified inference efficiency improvements from merging\n",
    "4. **Best Practices**: Learned when to choose IAÂ³ and how to deploy correctly\n",
    "\n",
    "### 9.2 IAÂ³'s Unique Value\n",
    "\n",
    "- **Ultimate Efficiency**: Minimal parameter modifications for maximum adaptation capability\n",
    "- **Perfect Merging**: Scaling vectors can be losslessly fused into base model\n",
    "- **Deployment Friendly**: Single model file, zero inference overhead\n",
    "- **Theoretical Elegance**: Simple yet powerful feature re-weighting strategy\n",
    "\n",
    "IAÂ³ perfectly embodies the core philosophy of PEFT: **\"Moving mountains with minimal effort\"**, proving that in appropriate scenarios, the simplest methods are often the most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"=== Cleaning Up Experiment Resources ===\")\n",
    "del merged_model, reloaded_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"âœ… Resource cleanup completed\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Lab 5 - IAÂ³ Model Merging and Deployment Experiment Completed!\")\n",
    "print(f\"Merged model saved to: {merged_model_path}\")\n",
    "print(\"You can now deploy this merged model directly and enjoy zero inference overhead!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}