# Lab-1.7: DPO ç›´æ¥åå¥½å„ªåŒ–
## Direct Preference Optimization (DPO)

**å¯¦é©—å®¤é¡å‹**: æ¨¡å‹å°é½ŠæŠ€è¡“
**é›£åº¦ç­‰ç´š**: â­â­â­â­â­ (é«˜ç´š)
**é ä¼°æ™‚é–“**: 5-7å°æ™‚
**é©ç”¨GPU**: 16GB+ VRAM

---

## ğŸ“š å¯¦é©—å®¤æ¦‚è¿°

Direct Preference Optimization (DPO) æ˜¯ä¸€ç¨®å‰µæ–°çš„ LLM å°é½ŠæŠ€è¡“ï¼Œç„¡éœ€è¤‡é›œçš„å¼·åŒ–å­¸ç¿’ (RLHF)ï¼Œç›´æ¥å¾åå¥½æ•¸æ“šä¸­å­¸ç¿’ï¼Œä½¿æ¨¡å‹è¼¸å‡ºæ›´ç¬¦åˆäººé¡åå¥½ã€‚æœ¬å¯¦é©—å®¤å°‡æ·±å…¥æ¢ç´¢ DPO çš„åŸç†ã€å¯¦ç¾èˆ‡å¯¦éš›æ‡‰ç”¨ã€‚

### å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š
- âœ… ç†è§£ RLHF çš„å±€é™æ€§èˆ‡ DPO çš„å„ªå‹¢
- âœ… æŒæ¡ DPO çš„æ•¸å­¸åŸç†èˆ‡ç®—æ³•
- âœ… æº–å‚™åå¥½æ•¸æ“šé›† (preference pairs)
- âœ… å¯¦ç¾å®Œæ•´çš„ DPO è¨“ç·´æµç¨‹
- âœ… è©•ä¼°å°é½Šæ•ˆæœèˆ‡æ¨¡å‹è³ªé‡
- âœ… å°æ¯” SFT vs DPO çš„å·®ç•°

---

## ğŸ¯ æ ¸å¿ƒæŠ€è¡“æ¦‚è¦½

### ç‚ºä»€éº¼éœ€è¦ DPO?

**å‚³çµ± RLHF çš„å•é¡Œ**:
```
RLHF æµç¨‹ (Reinforcement Learning from Human Feedback):
  Phase 1: Supervised Fine-Tuning (SFT)
  Phase 2: Reward Model Training
  Phase 3: PPO Training (å¼·åŒ–å­¸ç¿’)

å•é¡Œ:
  âŒ æµç¨‹è¤‡é›œ (3å€‹éšæ®µ)
  âŒ PPO è¨“ç·´ä¸ç©©å®š
  âŒ éœ€è¦ç¶­è­·å¤šå€‹æ¨¡å‹ (policy, value, reward)
  âŒ è¶…åƒæ•¸èª¿å„ªå›°é›£
  âŒ è¨ˆç®—æˆæœ¬é«˜æ˜‚
```

**DPO çš„å‰µæ–°**:
```
DPO æµç¨‹:
  Phase 1: Supervised Fine-Tuning (SFT) - å¯é¸
  Phase 2: DPO Training (ç›´æ¥å„ªåŒ–)

å„ªå‹¢:
  âœ… æµç¨‹ç°¡å–® (1-2éšæ®µ)
  âœ… è¨“ç·´ç©©å®š
  âœ… åªéœ€è¦ policy model
  âœ… å®¹æ˜“èª¿å„ª
  âœ… æˆæœ¬é™ä½ 50-70%
```

### DPO æ ¸å¿ƒåŸç†

#### å‚³çµ± RLHF ç›®æ¨™å‡½æ•¸

```
RLHF ç›®æ¨™: æœ€å¤§åŒ–çå‹µ, åŒæ™‚ä¿æŒèˆ‡åƒè€ƒæ¨¡å‹æ¥è¿‘

max E[r(x, y)] - Î² KL(Ï€_Î¸ || Ï€_ref)

å…¶ä¸­:
  r(x, y): çå‹µæ¨¡å‹è©•åˆ†
  Ï€_Î¸: ç•¶å‰ç­–ç•¥ (policy model)
  Ï€_ref: åƒè€ƒæ¨¡å‹ (SFT model)
  Î²: KL æ•£åº¦æ¬Šé‡
```

#### DPO æ ¸å¿ƒæ´å¯Ÿ

**é—œéµç™¼ç¾**: å¯ä»¥ç›´æ¥å¾åå¥½æ•¸æ“šå„ªåŒ–ï¼Œç„¡éœ€é¡¯å¼çå‹µæ¨¡å‹ï¼

```
åå¥½æ•¸æ“šæ ¼å¼:
  (x, y_w, y_l)
  x: prompt (è¼¸å…¥)
  y_w: preferred response (è´çš„å›ç­”)
  y_l: rejected response (è¼¸çš„å›ç­”)

DPO æå¤±å‡½æ•¸:
  L_DPO = -log Ïƒ(Î² log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x))
                  - Î² log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x)))

å…¶ä¸­:
  Ïƒ: sigmoid å‡½æ•¸
  Î²: æº«åº¦åƒæ•¸ (é€šå¸¸ 0.1-0.5)
  Ï€_Î¸: è¨“ç·´ä¸­çš„æ¨¡å‹
  Ï€_ref: åƒè€ƒæ¨¡å‹ (é€šå¸¸æ˜¯ SFT æ¨¡å‹)
```

**ç›´è§€ç†è§£**:
- å¢åŠ  preferred response çš„æ©Ÿç‡
- é™ä½ rejected response çš„æ©Ÿç‡
- é€šé KL æ•£åº¦ä¿æŒèˆ‡åƒè€ƒæ¨¡å‹æ¥è¿‘

### DPO vs RLHF å°æ¯”

| ç‰¹æ€§ | RLHF (PPO) | DPO | å„ªå‹¢ |
|------|-----------|-----|------|
| **è¨“ç·´éšæ®µ** | 3éšæ®µ | 1-2éšæ®µ | DPO â¬† |
| **æ‰€éœ€æ¨¡å‹** | 4å€‹ (policy, value, reward, ref) | 2å€‹ (policy, ref) | DPO â¬† |
| **è¨“ç·´ç©©å®šæ€§** | ä¸ç©©å®š (RL) | ç©©å®š (ç›£ç£å­¸ç¿’) | DPO â¬† |
| **è¶…åƒæ•¸èª¿å„ª** | å›°é›£ (>10å€‹) | ç°¡å–® (~3å€‹) | DPO â¬† |
| **è¨ˆç®—æˆæœ¬** | é«˜ | ä¸­ç­‰ | DPO â¬† |
| **å¯¦ç¾è¤‡é›œåº¦** | è¤‡é›œ | ç°¡å–® | DPO â¬† |
| **æ•ˆæœ** | å¼· | ç›¸ç•¶ | ç›¸è¿‘ |

---

## ğŸ“‚ å¯¦é©—å®¤çµæ§‹

```
Lab-1.7-DPO_Alignment/
â”œâ”€â”€ README.md                         # æœ¬æ–‡æª”
â”œâ”€â”€ 01-Setup_and_Data.ipynb          # ç’°å¢ƒèˆ‡åå¥½æ•¸æ“šæº–å‚™
â”œâ”€â”€ 02-SFT_Baseline.ipynb            # SFT åŸºæº–æ¨¡å‹è¨“ç·´
â”œâ”€â”€ 03-DPO_Training.ipynb            # DPO å°é½Šè¨“ç·´
â””â”€â”€ 04-Evaluation_and_Compare.ipynb  # å°é½Šæ•ˆæœè©•ä¼°
```

---

## ğŸ“Š å¯¦é©—å…§å®¹è©³è§£

### Notebook 1: ç’°å¢ƒèˆ‡åå¥½æ•¸æ“šæº–å‚™ (01-Setup_and_Data.ipynb)
**æ™‚é–“**: 60-90åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- ç†è§£åå¥½æ•¸æ“šæ ¼å¼
- æº–å‚™ DPO è¨“ç·´æ•¸æ“šé›†
- å¯¦ç¾æ•¸æ“šè¼‰å…¥å™¨
- æ¢ç´¢æ€§æ•¸æ“šåˆ†æ

#### æ•¸æ“šæ ¼å¼

**åå¥½å° (Preference Pair)**:
```json
{
  "prompt": "Explain what is machine learning",
  "chosen": "Machine learning is a subset of AI that enables systems to learn from data...",
  "rejected": "ML is when computers learn stuff."
}
```

**æ•¸æ“šä¾†æº**:
- **Anthropic HH-RLHF**: äººé¡åå¥½å°è©±æ•¸æ“š
- **OpenAI Summarization**: æ‘˜è¦åå¥½æ•¸æ“š
- **Stanford SHP**: StackExchange åå¥½æ•¸æ“š
- **è‡ªå®šç¾©æ•¸æ“š**: ä½¿ç”¨ GPT-4 ç”Ÿæˆåå¥½å°

#### å¯¦é©—å…§å®¹

1. **æ•¸æ“šé›†è¼‰å…¥**
   ```python
   from datasets import load_dataset

   # è¼‰å…¥ Anthropic HH-RLHF æ•¸æ“šé›†
   dataset = load_dataset("Anthropic/hh-rlhf")

   # æŸ¥çœ‹æ•¸æ“šæ ¼å¼
   print(dataset['train'][0])
   # {
   #   'chosen': '...',
   #   'rejected': '...'
   # }
   ```

2. **æ•¸æ“šé è™•ç†**
   ```python
   def preprocess_preference_data(example, tokenizer):
       """è™•ç†åå¥½æ•¸æ“š"""
       # æå– prompt å’Œ responses
       prompt = extract_prompt(example['chosen'])
       chosen = extract_response(example['chosen'])
       rejected = extract_response(example['rejected'])

       # Tokenize
       prompt_tokens = tokenizer(prompt)
       chosen_tokens = tokenizer(chosen)
       rejected_tokens = tokenizer(rejected)

       return {
           'prompt': prompt_tokens,
           'chosen': chosen_tokens,
           'rejected': rejected_tokens
       }
   ```

3. **æ•¸æ“šçµ±è¨ˆåˆ†æ**
   - åå¥½å°æ•¸é‡
   - é•·åº¦åˆ†å¸ƒ
   - è³ªé‡è©•ä¼°

4. **DataLoader å¯¦ç¾**
   ```python
   class PreferenceDataCollator:
       """DPO æ•¸æ“šæ‰¹æ¬¡æ”¶é›†å™¨"""
       def __call__(self, features):
           # æ‰¹æ¬¡è™•ç† prompt, chosen, rejected
           batch = {
               'prompt_ids': pad_sequence([f['prompt'] for f in features]),
               'chosen_ids': pad_sequence([f['chosen'] for f in features]),
               'rejected_ids': pad_sequence([f['rejected'] for f in features])
           }
           return batch
   ```

---

### Notebook 2: SFT åŸºæº–æ¨¡å‹è¨“ç·´ (02-SFT_Baseline.ipynb)
**æ™‚é–“**: 60-90åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- è¨“ç·´ SFT (Supervised Fine-Tuning) åŸºæº–æ¨¡å‹
- å»ºç«‹åƒè€ƒæ¨¡å‹ (Ï€_ref)
- è©•ä¼° SFT æ¨¡å‹è³ªé‡
- ç‚º DPO æº–å‚™åˆå§‹æ¨¡å‹

#### SFT è¨“ç·´æµç¨‹

```python
# Phase 1: SFT - åœ¨ instruction æ•¸æ“šä¸Šå¾®èª¿
model = AutoModelForCausalLM.from_pretrained("gpt2")

# è¨“ç·´æ•¸æ“š: (instruction, response) å°
sft_dataset = [
    {"instruction": "...", "response": "..."},
    ...
]

# æ¨™æº–ç›£ç£å­¸ç¿’
for batch in sft_dataloader:
    loss = model(**batch).loss
    loss.backward()
    optimizer.step()

# å¾—åˆ° SFT æ¨¡å‹ (ä½œç‚º DPO çš„åƒè€ƒæ¨¡å‹å’Œåˆå§‹æ¨¡å‹)
```

#### å¯¦é©—å…§å®¹

1. **åŸºç¤æ¨¡å‹é¸æ“‡**
   - GPT-2 (124M) - å¿«é€Ÿå¯¦é©—
   - Llama-2-7B - ç”Ÿç”¢ç´šåˆ¥
   - Mistral-7B - SOTA æ€§èƒ½

2. **SFT æ•¸æ“šæº–å‚™**
   ```python
   # ä½¿ç”¨ Alpaca æˆ–é¡ä¼¼ instruction dataset
   from datasets import load_dataset

   sft_data = load_dataset("tatsu-lab/alpaca")
   # æˆ–ä½¿ç”¨ HH-RLHF çš„ chosen responses
   ```

3. **SFT è¨“ç·´**
   - è¨“ç·´ 2-3 epochs
   - ç›£æ§ loss å’Œ perplexity
   - ä¿å­˜æœ€ä½³ checkpoint

4. **è³ªé‡è©•ä¼°**
   - ç”Ÿæˆæ¨£æœ¬è©•ä¼°
   - Perplexity æ¸¬è©¦
   - äººå·¥è©•ä¼° (å¯é¸)

---

### Notebook 3: DPO å°é½Šè¨“ç·´ (03-DPO_Training.ipynb)
**æ™‚é–“**: 90-120åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- å¯¦ç¾ DPO æå¤±å‡½æ•¸
- è¨“ç·´ DPO å°é½Šæ¨¡å‹
- ç›£æ§è¨“ç·´éç¨‹æŒ‡æ¨™
- åˆ†æå°é½Šæ•ˆæœ

#### DPO æå¤±å‡½æ•¸å¯¦ç¾

```python
import torch.nn.functional as F

def dpo_loss(policy_model, ref_model, batch, beta=0.1):
    """
    DPO æå¤±å‡½æ•¸

    Args:
        policy_model: è¨“ç·´ä¸­çš„ç­–ç•¥æ¨¡å‹
        ref_model: åƒè€ƒæ¨¡å‹ (SFT, å‡çµ)
        batch: {'prompt', 'chosen', 'rejected'}
        beta: æº«åº¦åƒæ•¸

    Returns:
        loss, metrics
    """
    # è¨ˆç®— log probabilities
    with torch.no_grad():
        # åƒè€ƒæ¨¡å‹ log probs
        ref_chosen_logps = get_log_probs(ref_model, batch['prompt'], batch['chosen'])
        ref_rejected_logps = get_log_probs(ref_model, batch['prompt'], batch['rejected'])

    # ç­–ç•¥æ¨¡å‹ log probs
    policy_chosen_logps = get_log_probs(policy_model, batch['prompt'], batch['chosen'])
    policy_rejected_logps = get_log_probs(policy_model, batch['prompt'], batch['rejected'])

    # DPO loss
    chosen_rewards = beta * (policy_chosen_logps - ref_chosen_logps)
    rejected_rewards = beta * (policy_rejected_logps - ref_rejected_logps)

    # Bradley-Terry æ¨¡å‹
    loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()

    # éš±å¼çå‹µ
    implicit_rewards_chosen = (policy_chosen_logps - ref_chosen_logps).detach()
    implicit_rewards_rejected = (policy_rejected_logps - ref_rejected_logps).detach()

    metrics = {
        'loss': loss.item(),
        'rewards_chosen': implicit_rewards_chosen.mean().item(),
        'rewards_rejected': implicit_rewards_rejected.mean().item(),
        'reward_margin': (implicit_rewards_chosen - implicit_rewards_rejected).mean().item()
    }

    return loss, metrics


def get_log_probs(model, prompt_ids, response_ids):
    """è¨ˆç®—åºåˆ—çš„ log probability"""
    # çµ„åˆ prompt + response
    input_ids = torch.cat([prompt_ids, response_ids], dim=1)

    # å‰å‘å‚³æ’­
    outputs = model(input_ids)
    logits = outputs.logits

    # è¨ˆç®— log probs (åªè¨ˆç®— response éƒ¨åˆ†)
    prompt_len = prompt_ids.size(1)
    response_logits = logits[:, prompt_len-1:-1, :]  # å°æ‡‰ response tokens
    response_labels = response_ids

    # Log softmax
    log_probs = F.log_softmax(response_logits, dim=-1)

    # æ”¶é›†å°æ‡‰ token çš„ log prob
    gathered_log_probs = torch.gather(
        log_probs,
        dim=2,
        index=response_labels.unsqueeze(2)
    ).squeeze(2)

    # å¹³å‡ (æˆ–æ±‚å’Œ)
    return gathered_log_probs.sum(dim=1)
```

#### å¯¦é©—å…§å®¹

1. **DPO Trainer å¯¦ç¾**
   ```python
   class DPOTrainer:
       def __init__(self, policy_model, ref_model, beta=0.1):
           self.policy_model = policy_model
           self.ref_model = ref_model
           self.beta = beta

           # å‡çµåƒè€ƒæ¨¡å‹
           for param in self.ref_model.parameters():
               param.requires_grad = False

       def train_step(self, batch):
           loss, metrics = dpo_loss(
               self.policy_model,
               self.ref_model,
               batch,
               self.beta
           )
           return loss, metrics
   ```

2. **è¨“ç·´å¾ªç’°**
   - è¨“ç·´ 1-3 epochs
   - ç›£æ§ reward margin (chosen vs rejected)
   - æ—©åœæ©Ÿåˆ¶

3. **é—œéµæŒ‡æ¨™ç›£æ§**
   - **Reward Margin**: chosen å’Œ rejected çš„çå‹µå·®è·
   - **KL Divergence**: èˆ‡åƒè€ƒæ¨¡å‹çš„è·é›¢
   - **Accuracy**: æ¨¡å‹æ˜¯å¦åå¥½ chosen over rejected

4. **è¶…åƒæ•¸èª¿å„ª**
   - **Î² (beta)**: 0.1-0.5 (è¶Šå¤§è¶Šä¿å®ˆ)
   - **Learning Rate**: 1e-6 to 5e-6
   - **Epochs**: 1-3

---

### Notebook 4: å°é½Šæ•ˆæœè©•ä¼° (04-Evaluation_and_Compare.ipynb)
**æ™‚é–“**: 45-60åˆ†é˜

#### å¯¦é©—ç›®æ¨™
- è©•ä¼° DPO å°é½Šæ•ˆæœ
- å°æ¯” SFT vs DPO è¼¸å‡ºè³ªé‡
- æ¸¬è©¦ä¸åŒå ´æ™¯çš„è¡¨ç¾
- åˆ†æå°é½ŠæˆåŠŸèˆ‡å¤±æ•—æ¡ˆä¾‹

#### è©•ä¼°æ–¹æ³•

1. **è‡ªå‹•åŒ–è©•ä¼°**
   ```python
   # Win Rate æ¸¬è©¦
   def compute_win_rate(model_a, model_b, test_prompts):
       """è¨ˆç®—æ¨¡å‹ A ç›¸å°æ¨¡å‹ B çš„å‹ç‡"""
       wins = 0
       for prompt in test_prompts:
           response_a = model_a.generate(prompt)
           response_b = model_b.generate(prompt)

           # ä½¿ç”¨ GPT-4 æˆ–äººå·¥è©•åˆ¤
           if judge(response_a, response_b, prompt) == 'A':
               wins += 1

       return wins / len(test_prompts)
   ```

2. **äººé¡åå¥½æ¸¬è©¦**
   - A/B æ¸¬è©¦
   - Elo Rating
   - ä¸»è§€è³ªé‡è©•åˆ†

3. **å®‰å…¨æ€§è©•ä¼°**
   - æœ‰å®³å…§å®¹æª¢æ¸¬
   - åè¦‹åˆ†æ
   - æ‹’çµ•ä¸ç•¶è«‹æ±‚èƒ½åŠ›

4. **å°è©±è³ªé‡è©•ä¼°**
   - ç›¸é—œæ€§ (Relevance)
   - å¹«åŠ©æ€§ (Helpfulness)
   - ç„¡å®³æ€§ (Harmlessness)

#### è©•ä¼°æŒ‡æ¨™

| æŒ‡æ¨™ | SFT åŸºæº– | DPO ç›®æ¨™ | æ¸¬é‡æ–¹æ³• |
|------|---------|---------|----------|
| **Win Rate** | 50% (vs self) | >60% (vs SFT) | A/B æ¸¬è©¦ |
| **Reward Margin** | 0 | >0.5 | éš±å¼çå‹µ |
| **Helpfulness** | 3.5/5 | 4.2/5 | äººå·¥è©•åˆ† |
| **Harmlessness** | 3.8/5 | 4.5/5 | å®‰å…¨è©•ä¼° |

---

## ğŸ”§ æŠ€è¡“å¯¦ç¾ç´°ç¯€

### åå¥½æ•¸æ“šé›†ç¯„ä¾‹

#### Anthropic HH-RLHF æ ¼å¼
```json
{
  "chosen": "Human: How do I make pizza?\n\nAssistant: To make pizza, you'll need flour, yeast, water, tomato sauce, cheese, and toppings...",

  "rejected": "Human: How do I make pizza?\n\nAssistant: Just buy it from the store."
}
```

#### æ•¸æ“šæº–å‚™æµç¨‹
```python
def prepare_dpo_dataset(dataset, tokenizer):
    """æº–å‚™ DPO è¨“ç·´æ•¸æ“š"""
    processed = []

    for example in dataset:
        # åˆ†é›¢ prompt å’Œ response
        chosen_text = example['chosen']
        rejected_text = example['rejected']

        # æå–å…±åŒ prompt
        prompt = extract_common_prompt(chosen_text, rejected_text)
        chosen_response = chosen_text.replace(prompt, '').strip()
        rejected_response = rejected_text.replace(prompt, '').strip()

        # Tokenize
        prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)
        chosen_ids = tokenizer.encode(chosen_response, add_special_tokens=False)
        rejected_ids = tokenizer.encode(rejected_response, add_special_tokens=False)

        processed.append({
            'prompt_ids': prompt_ids,
            'chosen_ids': chosen_ids,
            'rejected_ids': rejected_ids
        })

    return processed
```

### DPO è¨“ç·´æœ€ä½³å¯¦è¸

#### 1. åƒè€ƒæ¨¡å‹é¸æ“‡
```python
# æ–¹æ¡ˆ 1: ä½¿ç”¨ SFT æ¨¡å‹ä½œç‚ºåƒè€ƒ
ref_model = copy.deepcopy(sft_model)
ref_model.eval()
for param in ref_model.parameters():
    param.requires_grad = False

# æ–¹æ¡ˆ 2: ä½¿ç”¨åŸºç¤æ¨¡å‹ (å¦‚æœæ²’æœ‰ SFT)
ref_model = AutoModelForCausalLM.from_pretrained("gpt2")
```

#### 2. Beta åƒæ•¸é¸æ“‡
- **Î² = 0.1**: æ¿€é€²å°é½Š, å¯èƒ½éåº¦å„ªåŒ–
- **Î² = 0.2-0.3**: å¹³è¡¡, æ¨è–¦èµ·é»
- **Î² = 0.5**: ä¿å®ˆ, ä¿æŒæ¥è¿‘åƒè€ƒæ¨¡å‹

#### 3. å­¸ç¿’ç‡èª¿åº¦
```python
# DPO å°å­¸ç¿’ç‡æ•æ„Ÿ
optimizer = torch.optim.AdamW(
    policy_model.parameters(),
    lr=5e-7,  # æ¯” SFT å° 10x
    betas=(0.9, 0.95)
)

# Cosine é€€ç«
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=total_steps
)
```

#### 4. è¨“ç·´ç›£æ§
```python
# é—œéµæŒ‡æ¨™
metrics_to_track = {
    'loss': [],
    'reward_margin': [],  # chosen - rejected çå‹µå·®
    'accuracy': [],       # P(chosen > rejected)
    'kl_div': []          # KL(policy || ref)
}

# ç†æƒ³è¨“ç·´æ›²ç·š:
# - loss ä¸‹é™
# - reward_margin ä¸Šå‡
# - accuracy > 60%
# - kl_div ä¿æŒè¼ƒå° (<10)
```

---

## ğŸš€ ç’°å¢ƒæº–å‚™

### å‰ç½®è¦æ±‚

#### ç¡¬é«”è¦æ±‚
- **GPU**: 16GB+ VRAM (æ¨è–¦ 24GB+)
  - 7B æ¨¡å‹éœ€è¦ 2 å€‹æ¨¡å‹åŒæ™‚è¼‰å…¥ (policy + ref)
- **å„²å­˜ç©ºé–“**: 50GB+ (æ¨¡å‹æª¢æŸ¥é»)

#### è»Ÿé«”ä¾è³´
```bash
# å®‰è£ trl (Transformer Reinforcement Learning)
pip install trl

# å®‰è£ datasets
pip install datasets

# é©—è­‰å®‰è£
python -c "from trl import DPOTrainer; print('âœ… TRL å¯ç”¨')"
```

### ä½¿ç”¨ TRL åº«çš„ DPO Trainer

```python
from trl import DPOTrainer, DPOConfig

# é…ç½®
config = DPOConfig(
    beta=0.1,
    learning_rate=5e-7,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=1,
    max_length=512,
    max_prompt_length=256
)

# è¨“ç·´å™¨
trainer = DPOTrainer(
    model=policy_model,
    ref_model=ref_model,
    args=config,
    train_dataset=dpo_dataset,
    tokenizer=tokenizer
)

# è¨“ç·´
trainer.train()
```

---

## ğŸ“ˆ æ€§èƒ½é æœŸ

### DPO vs SFT å°æ¯” (GPT-2 Small)

| æŒ‡æ¨™ | SFT åŸºæº– | DPO | æ”¹é€² |
|------|---------|-----|------|
| **Helpfulness** | 3.2/5 | 4.1/5 | +28% |
| **Harmlessness** | 3.5/5 | 4.3/5 | +23% |
| **Win Rate** | 50% | 68% | +36% |
| **Perplexity** | 25.3 | 26.1 | -3% (å¯æ¥å—) |

### è¨“ç·´æˆæœ¬å°æ¯”

| æ–¹æ³• | éšæ®µæ•¸ | GPU æ™‚ | ç›¸å°æˆæœ¬ |
|------|--------|-------|---------|
| **RLHF (PPO)** | 3 | 100 | 100% |
| **DPO** | 2 | 40 | 40% |
| **åƒ… SFT** | 1 | 20 | 20% |

---

## ğŸ’¡ æœ€ä½³å¯¦è¸

### æ•¸æ“šæº–å‚™å»ºè­°

1. **æ•¸æ“šè³ªé‡æœ€é‡è¦**
   - åå¥½å°å·®ç•°æ˜é¡¯
   - chosen çœŸæ­£æ›´å¥½
   - rejected å…·æœ‰ä»£è¡¨æ€§

2. **æ•¸æ“šé‡å»ºè­°**
   - æœ€å°‘: 10K åå¥½å°
   - æ¨è–¦: 50K-100K
   - å¤§è¦æ¨¡: 500K+ (SOTA æ¨¡å‹)

3. **æ•¸æ“šå¹³è¡¡**
   - ä¸åŒé ˜åŸŸå‡è¡¡
   - é›£åº¦åˆ†å¸ƒåˆç†
   - é¿å…åè¦‹

### è¨“ç·´æŠ€å·§

1. **å…ˆ SFT å¾Œ DPO**
   - SFT å»ºç«‹åŸºç¤èƒ½åŠ›
   - DPO å„ªåŒ–åå¥½å°é½Š
   - æ•ˆæœæœ€ä½³

2. **Beta èª¿å„ªç­–ç•¥**
   ```python
   # å¾å¤§åˆ°å°å˜—è©¦
   beta_values = [0.5, 0.3, 0.1, 0.05]

   # è§€å¯Ÿ reward margin å’Œ KL divergence
   # é¸æ“‡ margin æœ€å¤§ä¸” KL < 10 çš„ beta
   ```

3. **ç›£æ§éåº¦å„ªåŒ–**
   ```python
   # è­¦å‘Šä¿¡è™Ÿ:
   # - KL divergence > 20 (éåº¦åé›¢åƒè€ƒæ¨¡å‹)
   # - Loss æŒçºŒä¸‹é™ä½†ç”Ÿæˆè³ªé‡è®Šå·®
   # - Accuracy > 95% (éæ“¬åˆåå¥½æ•¸æ“š)

   # æ‡‰å°: æ—©åœ, å¢å¤§ beta, æ¸›å°å­¸ç¿’ç‡
   ```

---

## ğŸ“ å­¸ç¿’æª¢æŸ¥æ¸…å–®

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œæ‚¨æ‡‰è©²èƒ½å¤ :

### ç†è«–ç†è§£
- [ ] è§£é‡‹ DPO ç›¸æ¯” RLHF çš„å„ªå‹¢
- [ ] æ¨å° DPO æå¤±å‡½æ•¸
- [ ] ç†è§£ Bradley-Terry åå¥½æ¨¡å‹
- [ ] èªªæ˜éš±å¼çå‹µçš„æ¦‚å¿µ
- [ ] ç†è§£ beta åƒæ•¸çš„ä½œç”¨

### å¯¦ä½œæŠ€èƒ½
- [ ] æº–å‚™åå¥½æ•¸æ“šé›†
- [ ] å¯¦ç¾ DPO æå¤±å‡½æ•¸
- [ ] è¨“ç·´ DPO å°é½Šæ¨¡å‹
- [ ] ç›£æ§è¨“ç·´é—œéµæŒ‡æ¨™
- [ ] è©•ä¼°å°é½Šæ•ˆæœ

### æ‡‰ç”¨èƒ½åŠ›
- [ ] ç‚ºé …ç›®é¸æ“‡åˆé©çš„å°é½Šæ–¹æ³•
- [ ] æ”¶é›†æˆ–ç”Ÿæˆåå¥½æ•¸æ“š
- [ ] èª¿å„ª DPO è¶…åƒæ•¸
- [ ] è¨ºæ–·è¨“ç·´å•é¡Œ
- [ ] éƒ¨ç½²å°é½Šå¾Œçš„æ¨¡å‹

---

## ğŸš€ ä¸‹ä¸€æ­¥å­¸ç¿’

å®Œæˆæœ¬å¯¦é©—å®¤å¾Œï¼Œå»ºè­°ç¹¼çºŒ:

1. **Lab-1.8: ORPO Alignment**
   - Odds Ratio Preference Optimization
   - å–®éšæ®µå°é½Š (ç„¡éœ€ SFT)

2. **é«˜ç´šå°é½ŠæŠ€è¡“**
   - Constitutional AI
   - RLAIF (AI Feedback)
   - Iterative DPO

3. **ç”Ÿç”¢éƒ¨ç½²**
   - å°é½Šæ¨¡å‹æœå‹™åŒ–
   - A/B æ¸¬è©¦æ¡†æ¶
   - æŒçºŒæ”¹é€²æ©Ÿåˆ¶

---

**å¯¦é©—å®¤ç‹€æ…‹**: ğŸ”„ é–‹ç™¼ä¸­
**æœ€å¾Œæ›´æ–°**: 2025-10-08
**ç¶­è­·è€…**: LLM æ•™å­¸å°ˆæ¡ˆåœ˜éšŠ

**ç›¸é—œæ–‡ä»¶**:
- ç†è«–: `01-Theory/1.3-Optimization_and_Alignment.md` (DPO ç« ç¯€)
- å‰ç½®å¯¦é©—: `Lab-1.4-Training_Optimization_Basics`
- å¾ŒçºŒå¯¦é©—: `Lab-1.8-ORPO_Alignment`

**ç›¸é—œè«–æ–‡**:
- [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290) - NeurIPS 2023
- [Training language models to follow instructions with human feedback (RLHF)](https://arxiv.org/abs/2203.02155)
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)

**é–‹æºè³‡æº**:
- [HuggingFace TRL](https://github.com/huggingface/trl) - DPO Trainer å¯¦ç¾
- [Anthropic HH-RLHF Dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [Zephyr-7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) - ä½¿ç”¨ DPO å°é½Šçš„æ¨¡å‹
