{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab-1.7: DPO (Direct Preference Optimization) 訓練實作\n",
    "\n",
    "**實驗目標**: 實現並訓練 DPO 對齊模型\n",
    "\n",
    "這個 notebook 將實現 DPO 的核心訓練流程。DPO 通過直接優化偏好數據來改善模型行為，相比傳統 RLHF 更加穩定且高效。\n",
    "\n",
    "## DPO 核心原理\n",
    "\n",
    "DPO 的核心想法是直接從偏好數據中學習最優策略，而不需要顯式的獎勵模型：\n",
    "\n",
    "**DPO 損失函數**:\n",
    "```\n",
    "L_DPO = -E[(x,y_w,y_l)~D][log σ(β log π_θ(y_w|x)/π_ref(y_w|x) - β log π_θ(y_l|x)/π_ref(y_l|x))]\n",
    "```\n",
    "\n",
    "其中：\n",
    "- `π_θ`: 正在訓練的策略模型\n",
    "- `π_ref`: 參考模型 (SFT 模型)\n",
    "- `β`: 溫度參數，控制偏好強度\n",
    "- `y_w`: 偏好回應 (chosen)\n",
    "- `y_l`: 非偏好回應 (rejected)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 步驟 1: 環境準備與導入\n",
    "\n",
    "載入必要的庫並準備 DPO 訓練環境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from datasets import load_from_disk, Dataset\n",
    "from trl import DPOTrainer\n",
    "\n",
    "# 設置隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('🚀 開始 DPO 訓練實作')\n",
    "print(f'GPU 可用: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'當前 GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'GPU 記憶體: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 步驟 2: 載入 SFT 基線模型\n",
    "\n",
    "載入之前訓練的 SFT 模型作為 DPO 的起始點和參考模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型路徑配置\n",
    "BASE_MODEL_NAME = 'microsoft/DialoGPT-medium'\n",
    "SFT_MODEL_PATH = './sft_model_output'\n",
    "\n",
    "print(f'📦 載入 SFT 基線模型...')\n",
    "\n",
    "# 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 載入基礎模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f'✅ 基礎模型載入成功')\n",
    "\n",
    "# 檢查 SFT 模型是否存在\n",
    "if os.path.exists(SFT_MODEL_PATH):\n",
    "    try:\n",
    "        # 載入 SFT 模型作為策略模型\n",
    "        policy_model = PeftModel.from_pretrained(base_model, SFT_MODEL_PATH)\n",
    "        print(f'✅ SFT 策略模型載入成功: {SFT_MODEL_PATH}')\n",
    "        \n",
    "        # 參考模型使用原始基礎模型\n",
    "        reference_model = deepcopy(base_model)\n",
    "        print(f'✅ 參考模型準備完成')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'⚠️  SFT 模型載入失敗: {e}')\n",
    "        print('使用基礎模型進行 DPO 訓練')\n",
    "        policy_model = base_model\n",
    "        reference_model = deepcopy(base_model)\n",
    "else:\n",
    "    print('⚠️  未找到 SFT 模型，使用基礎模型')\n",
    "    policy_model = base_model\n",
    "    reference_model = deepcopy(base_model)\n",
    "\n",
    "print(f'模型參數量: {policy_model.num_parameters():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 步驟 3: 準備 DPO 訓練數據\n",
    "\n",
    "載入偏好數據集並確保格式正確。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 DPO 數據集\n",
    "try:\n",
    "    dpo_dataset = load_from_disk('./dpo_data')\n",
    "    print(f'✅ 載入 DPO 數據集，樣本數: {len(dpo_dataset)}')\n",
    "except:\n",
    "    print('⚠️  無法載入已保存的數據，創建模擬數據')\n",
    "    # 創建模擬 DPO 數據\n",
    "    mock_data = [\n",
    "        {\n",
    "            'prompt': '請解釋什麼是機器學習?',\n",
    "            'chosen': '機器學習是人工智能的一個分支，它讓計算機能夠從數據中學習並做出決策，而無需明確編程。通過算法分析大量數據，系統可以識別模式並提高性能。',\n",
    "            'rejected': '機器學習就是讓機器變聰明。'\n",
    "        },\n",
    "        {\n",
    "            'prompt': '如何學習程式設計?',\n",
    "            'chosen': '學習程式設計建議從基礎語法開始，選擇一門適合的語言如Python，多做練習項目，參與開源專案，並持續學習新技術。實作是最重要的學習方式。',\n",
    "            'rejected': '學程式就是寫code。'\n",
    "        },\n",
    "        {\n",
    "            'prompt': '什麼是深度學習?',\n",
    "            'chosen': '深度學習是機器學習的一個子領域，使用多層神經網路來學習數據的複雜模式。它在圖像識別、自然語言處理和語音識別等領域取得了突破性進展。',\n",
    "            'rejected': '深度學習就是很深的學習。'\n",
    "        }\n",
    "    ]\n",
    "    dpo_dataset = Dataset.from_list(mock_data)\n",
    "\n",
    "print(f'數據集大小: {len(dpo_dataset)}')\n",
    "print(f'數據欄位: {list(dpo_dataset.features.keys())}')\n",
    "\n",
    "# 檢查數據格式\n",
    "sample = dpo_dataset[0]\n",
    "print(f'\\n📝 數據樣本:')\n",
    "print(f'Prompt: {sample[\"prompt\"]}')\n",
    "print(f'Chosen: {sample[\"chosen\"][:100]}...')\n",
    "print(f'Rejected: {sample[\"rejected\"][:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 步驟 4: DPO 損失函數實現\n",
    "\n",
    "實現 DPO 的核心損失函數，理解其數學原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(model, input_ids, attention_mask, labels):\n",
    "    \"\"\"計算序列的對數概率\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # 計算每個 token 的對數概率\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # 收集目標 token 的對數概率\n",
    "        target_log_probs = torch.gather(log_probs[:, :-1], dim=-1, \n",
    "                                       index=labels[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # 僅計算非 padding token 的平均對數概率\n",
    "        mask = (labels[:, 1:] != tokenizer.pad_token_id).float()\n",
    "        sequence_log_prob = (target_log_probs * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        \n",
    "    return sequence_log_prob\n",
    "\n",
    "\n",
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps, \n",
    "             reference_chosen_logps, reference_rejected_logps, beta=0.1):\n",
    "    \"\"\"DPO 損失函數實現\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: 策略模型對 chosen 回應的對數概率\n",
    "        policy_rejected_logps: 策略模型對 rejected 回應的對數概率  \n",
    "        reference_chosen_logps: 參考模型對 chosen 回應的對數概率\n",
    "        reference_rejected_logps: 參考模型對 rejected 回應的對數概率\n",
    "        beta: 溫度參數\n",
    "    \"\"\"\n",
    "    # 計算相對於參考模型的獎勵\n",
    "    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps)\n",
    "    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps)\n",
    "    \n",
    "    # DPO 損失: 最大化 chosen 相對於 rejected 的偏好\n",
    "    loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "    \n",
    "    # 計算準確率 (chosen 獎勵是否高於 rejected)\n",
    "    accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "    \n",
    "    return loss, accuracy, chosen_rewards.mean(), rejected_rewards.mean()\n",
    "\n",
    "\n",
    "print('✅ DPO 損失函數實現完成')\n",
    "\n",
    "# 測試損失函數\n",
    "print('\\n🧪 測試 DPO 損失函數:')\n",
    "dummy_chosen = torch.tensor([-2.0, -1.5])\n",
    "dummy_rejected = torch.tensor([-3.0, -2.5])\n",
    "dummy_ref_chosen = torch.tensor([-2.2, -1.7])\n",
    "dummy_ref_rejected = torch.tensor([-2.8, -2.3])\n",
    "\n",
    "loss, acc, chosen_reward, rejected_reward = dpo_loss(\n",
    "    dummy_chosen, dummy_rejected, dummy_ref_chosen, dummy_ref_rejected, beta=0.1\n",
    ")\n",
    "\n",
    "print(f'Loss: {loss.item():.4f}')\n",
    "print(f'Accuracy: {acc.item():.4f}')\n",
    "print(f'Chosen Reward: {chosen_reward.item():.4f}')\n",
    "print(f'Rejected Reward: {rejected_reward.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 步驟 5: 使用 TRL DPOTrainer\n",
    "\n",
    "使用 TRL 庫的 DPOTrainer 來簡化 DPO 訓練流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備 DPO 訓練數據格式\n",
    "def format_dpo_dataset(dataset):\n",
    "    \"\"\"將數據集格式化為 DPOTrainer 需要的格式\"\"\"\n",
    "    formatted_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        formatted_data.append({\n",
    "            'prompt': sample['prompt'],\n",
    "            'chosen': sample['chosen'],\n",
    "            'rejected': sample['rejected']\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(formatted_data)\n",
    "\n",
    "# 格式化數據集\n",
    "formatted_dataset = format_dpo_dataset(dpo_dataset)\n",
    "print(f'✅ 數據集格式化完成，樣本數: {len(formatted_dataset)}')\n",
    "\n",
    "# 顯示格式化後的樣本\n",
    "sample = formatted_dataset[0]\n",
    "print(f'\\n📝 格式化樣本:')\n",
    "for key, value in sample.items():\n",
    "    print(f'{key}: {value[:50]}...' if len(str(value)) > 50 else f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 步驟 6: DPO 訓練配置\n",
    "\n",
    "設置 DPO 訓練的參數和配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO 訓練配置\n",
    "output_dir = './dpo_model_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # 基本設置\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-6,  # DPO 通常使用較小的學習率\n",
    "    \n",
    "    # 優化設置\n",
    "    warmup_steps=50,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    \n",
    "    # 記憶體優化\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # 其他設置\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print('⚙️  DPO 訓練配置:')\n",
    "print(f'輸出目錄: {output_dir}')\n",
    "print(f'訓練輪數: {training_args.num_train_epochs}')\n",
    "print(f'批次大小: {training_args.per_device_train_batch_size}')\n",
    "print(f'學習率: {training_args.learning_rate}')\n",
    "print(f'使用 FP16: {training_args.fp16}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 步驟 7: 開始 DPO 訓練\n",
    "\n",
    "使用 DPOTrainer 進行偏好優化訓練。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 創建 DPO Trainer\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=policy_model,\n",
    "        ref_model=reference_model,\n",
    "        args=training_args,\n",
    "        train_dataset=formatted_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        beta=0.1,  # DPO 溫度參數\n",
    "        max_length=512,\n",
    "        max_prompt_length=256,\n",
    "    )\n",
    "    \n",
    "    print('✅ DPOTrainer 創建成功')\n",
    "    \n",
    "    # 開始訓練\n",
    "    print('🚀 開始 DPO 訓練...')\n",
    "    \n",
    "    # 訓練前的記憶體狀態\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'訓練前 GPU 記憶體: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "    \n",
    "    # 執行訓練\n",
    "    dpo_trainer.train()\n",
    "    \n",
    "    print('✅ DPO 訓練完成！')\n",
    "    \n",
    "    # 訓練後的記憶體狀態\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'訓練後 GPU 記憶體: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'❌ DPO 訓練失敗: {e}')\n",
    "    print('\\n🔄 嘗試手動 DPO 訓練實現...')\n",
    "    \n",
    "    # 手動 DPO 訓練的簡化版本\n",
    "    from torch.optim import AdamW\n",
    "    \n",
    "    # 準備優化器\n",
    "    optimizer = AdamW(policy_model.parameters(), lr=1e-6)\n",
    "    policy_model.train()\n",
    "    reference_model.eval()\n",
    "    \n",
    "    print('🔧 使用手動實現進行 DPO 訓練...')\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, sample in enumerate(formatted_dataset):\n",
    "            # 準備輸入\n",
    "            prompt = sample['prompt']\n",
    "            chosen_text = f\"{prompt} {sample['chosen']}\"\n",
    "            rejected_text = f\"{prompt} {sample['rejected']}\"\n",
    "            \n",
    "            # Tokenization\n",
    "            chosen_tokens = tokenizer(chosen_text, return_tensors='pt', truncation=True, max_length=256)\n",
    "            rejected_tokens = tokenizer(rejected_text, return_tensors='pt', truncation=True, max_length=256)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                chosen_tokens = {k: v.cuda() for k, v in chosen_tokens.items()}\n",
    "                rejected_tokens = {k: v.cuda() for k, v in rejected_tokens.items()}\n",
    "            \n",
    "            # 計算對數概率\n",
    "            policy_chosen_logps = compute_log_probs(policy_model, **chosen_tokens, labels=chosen_tokens['input_ids'])\n",
    "            policy_rejected_logps = compute_log_probs(policy_model, **rejected_tokens, labels=rejected_tokens['input_ids'])\n",
    "            \n",
    "            ref_chosen_logps = compute_log_probs(reference_model, **chosen_tokens, labels=chosen_tokens['input_ids'])\n",
    "            ref_rejected_logps = compute_log_probs(reference_model, **rejected_tokens, labels=rejected_tokens['input_ids'])\n",
    "            \n",
    "            # 計算 DPO 損失\n",
    "            loss, acc, _, _ = dpo_loss(\n",
    "                policy_chosen_logps, policy_rejected_logps,\n",
    "                ref_chosen_logps, ref_rejected_logps\n",
    "            )\n",
    "            \n",
    "            # 反向傳播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                print(f'Step {i}: Loss={loss.item():.4f}, Acc={acc.item():.4f}')\n",
    "        \n",
    "        print(f'Epoch {epoch}: Avg Loss={total_loss/len(formatted_dataset):.4f}, Avg Acc={total_acc/len(formatted_dataset):.4f}')\n",
    "    \n",
    "    print('✅ 手動 DPO 訓練完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 步驟 8: 保存 DPO 模型\n",
    "\n",
    "保存訓練完成的 DPO 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 DPO 模型\n",
    "print('💾 保存 DPO 模型...')\n",
    "\n",
    "try:\n",
    "    # 如果使用 PEFT 模型\n",
    "    if hasattr(policy_model, 'save_pretrained'):\n",
    "        policy_model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f'✅ DPO 模型已保存至: {output_dir}')\n",
    "    else:\n",
    "        torch.save(policy_model.state_dict(), os.path.join(output_dir, 'dpo_model.pth'))\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f'✅ DPO 模型狀態已保存至: {output_dir}')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'⚠️  模型保存過程中出現問題: {e}')\n",
    "    print('嘗試基本保存方式...')\n",
    "    \n",
    "    # 基本保存方式\n",
    "    torch.save({\n",
    "        'model_state_dict': policy_model.state_dict(),\n",
    "        'config': policy_model.config,\n",
    "    }, os.path.join(output_dir, 'dpo_model_manual.pth'))\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f'✅ DPO 模型手動保存至: {output_dir}')\n",
    "\n",
    "# 列出保存的檔案\n",
    "saved_files = list(Path(output_dir).glob('*'))\n",
    "print(f'保存的檔案: {[f.name for f in saved_files]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 步驟 9: DPO 模型測試\n",
    "\n",
    "測試 DPO 訓練後的模型生成能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 DPO 模型\n",
    "def test_dpo_model(model, tokenizer, prompt, max_length=150):\n",
    "    \"\"\"測試 DPO 模型生成\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 準備輸入\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # 生成回應\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解碼輸出\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print('🧪 測試 DPO 模型生成能力...')\n",
    "\n",
    "test_prompts = [\n",
    "    \"Human: 請解釋什麼是機器學習?\\n\\nAssistant:\",\n",
    "    \"Human: 如何學習程式設計?\\n\\nAssistant:\",\n",
    "    \"Human: 什麼是深度學習?\\n\\nAssistant:\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f'\\n📝 測試 {i+1}:')\n",
    "    print(f'Prompt: {prompt.split(\"Assistant:\")[0]}Assistant:')\n",
    "    \n",
    "    try:\n",
    "        response = test_dpo_model(policy_model, tokenizer, prompt)\n",
    "        print(f'DPO Generated: {response}')\n",
    "    except Exception as e:\n",
    "        print(f'生成失敗: {e}')\n",
    "    \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 步驟 10: 訓練總結\n",
    "\n",
    "總結 DPO 訓練的結果和關鍵指標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO 訓練總結\n",
    "print('=== DPO 訓練總結 ===')\n",
    "print(f'✅ 基礎模型: {BASE_MODEL_NAME}')\n",
    "print(f'✅ SFT 模型路徑: {SFT_MODEL_PATH}')\n",
    "print(f'✅ DPO 訓練數據: {len(formatted_dataset)} 個偏好對')\n",
    "print(f'✅ 訓練輪數: {training_args.num_train_epochs}')\n",
    "print(f'✅ 學習率: {training_args.learning_rate}')\n",
    "print(f'✅ DPO 模型保存位置: {output_dir}')\n",
    "\n",
    "# 計算模型參數統計\n",
    "total_params = sum(p.numel() for p in policy_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
    "print(f'✅ 總參數: {total_params:,}')\n",
    "print(f'✅ 可訓練參數: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)')\n",
    "\n",
    "print('\\n🎯 DPO 對齊模型訓練完成！')\n",
    "print('下一步: 執行 04-Evaluation_and_Compare.ipynb 進行模型評估')\n",
    "\n",
    "# 清理 GPU 記憶體\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'\\nGPU 記憶體已清理，當前使用: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "\n",
    "print('\\n📚 DPO 核心概念總結:')\n",
    "print('• DPO 直接從偏好數據中學習，無需獎勵模型')\n",
    "print('• 使用 Bradley-Terry 模型來建模人類偏好')\n",
    "print('• 通過對比學習優化模型行為')\n",
    "print('• 相比 RLHF 更加穩定且易於實現')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}