#!/usr/bin/env python3
"""
Lab 0.6: æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦è¸ä¸»è…³æœ¬
å®Œæ•´çš„ç«¯åˆ°ç«¯æ¨¡å‹å£“ç¸®å¯¦é©—
"""

import torch
import numpy as np
import time
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from pathlib import Path
from datetime import datetime
import json

class ModelCompressionEngineeringLab:
    """æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦é©—å®¤"""

    def __init__(self, base_model="microsoft/DialoGPT-small"):
        self.base_model = base_model
        self.experiment_dir = Path(f"./results/compression_engineering_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        self.compression_results = {}

    def run_engineering_compression_pipeline(self):
        """é‹è¡Œå·¥ç¨‹åŒ–å£“ç¸®ç®¡ç·š"""

        print("=== æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–ç®¡ç·š ===")

        # éšæ®µ1: æ¨¡å‹åˆ†æèˆ‡åŸºç·š
        print("éšæ®µ1: æ¨¡å‹åˆ†æèˆ‡åŸºç·šå»ºç«‹...")
        baseline_result = self._establish_baseline()

        # éšæ®µ2: å£“ç¸®ç­–ç•¥é¸æ“‡
        print("\néšæ®µ2: å£“ç¸®ç­–ç•¥é¸æ“‡...")
        strategy = self._select_compression_strategy(baseline_result)

        # éšæ®µ3: å£“ç¸®å¯¦æ–½
        print("\néšæ®µ3: å£“ç¸®å¯¦æ–½...")
        compression_result = self._implement_compression(strategy)

        # éšæ®µ4: æ•ˆæœé©—è­‰èˆ‡å„ªåŒ–
        print("\néšæ®µ4: æ•ˆæœé©—è­‰...")
        validation_result = self._validate_compression_effects(compression_result)

        # æ•´åˆçµæœ
        pipeline_results = {
            'baseline': baseline_result,
            'strategy': strategy,
            'compression': compression_result,
            'validation': validation_result
        }

        # ä¿å­˜çµæœä¸¦ç”Ÿæˆå ±å‘Š
        self._save_pipeline_results(pipeline_results)
        report = self._generate_engineering_report(pipeline_results)

        print(f"\nâœ… å·¥ç¨‹åŒ–å£“ç¸®ç®¡ç·šå®Œæˆï¼")
        print(f"ğŸ“ çµæœä¿å­˜åœ¨: {self.experiment_dir}")

        return pipeline_results

    def _establish_baseline(self):
        """å»ºç«‹åŸºç·š"""

        print("  è¼‰å…¥åŸå§‹æ¨¡å‹ä¸¦å»ºç«‹åŸºç·š...")

        try:
            # è¼‰å…¥æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(self.base_model)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            model = AutoModelForCausalLM.from_pretrained(
                self.base_model,
                torch_dtype=torch.float16,
                device_map="auto"
            )

            # è¨ˆç®—åŸºç·šæŒ‡æ¨™
            model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)
            param_count = sum(p.numel() for p in model.parameters())

            # æ€§èƒ½åŸºç·šæ¸¬è©¦
            performance_baseline = self._benchmark_model_performance(model, tokenizer)

            baseline = {
                'model_name': self.base_model,
                'parameter_count': param_count,
                'model_size_mb': model_size_mb,
                'performance_baseline': performance_baseline,
                'baseline_established': True
            }

            print(f"    åƒæ•¸é‡: {param_count:,} ({param_count/1e9:.2f}B)")
            print(f"    æ¨¡å‹å¤§å°: {model_size_mb:.1f} MB")

            return baseline

        except Exception as e:
            print(f"    åŸºç·šå»ºç«‹å¤±æ•—: {e}")
            return {'baseline_established': False, 'error': str(e)}

    def _select_compression_strategy(self, baseline):
        """é¸æ“‡å£“ç¸®ç­–ç•¥"""

        if not baseline.get('baseline_established'):
            return {'strategy_selected': False}

        model_size_mb = baseline['model_size_mb']
        target_memory_gb = 8  # ç›®æ¨™ï¼šé©é…8GB GPU

        # åŸºæ–¼æ¨¡å‹å¤§å°é¸æ“‡ç­–ç•¥
        if model_size_mb > target_memory_gb * 1024 * 0.5:  # æ¨¡å‹ä½”ç”¨è¶…é50%ç›®æ¨™è¨˜æ†¶é«”
            strategy = {
                'method': 'aggressive_quantization',
                'target_precision': 'int4',
                'additional_techniques': ['model_sharding'],
                'expected_compression_ratio': 8.0
            }
        elif model_size_mb > target_memory_gb * 1024 * 0.2:
            strategy = {
                'method': 'moderate_quantization',
                'target_precision': 'int8',
                'additional_techniques': ['inference_optimization'],
                'expected_compression_ratio': 2.0
            }
        else:
            strategy = {
                'method': 'conservative_optimization',
                'target_precision': 'fp16',
                'additional_techniques': ['inference_engine_optimization'],
                'expected_compression_ratio': 1.2
            }

        print(f"    é¸æ“‡ç­–ç•¥: {strategy['method']}")
        print(f"    ç›®æ¨™ç²¾åº¦: {strategy['target_precision']}")
        print(f"    é æœŸå£“ç¸®æ¯”: {strategy['expected_compression_ratio']:.1f}x")

        return strategy

    def _implement_compression(self, strategy):
        """å¯¦æ–½å£“ç¸®"""

        if not strategy.get('method'):
            return {'compression_successful': False}

        print(f"  å¯¦æ–½{strategy['method']}...")

        try:
            # æ ¹æ“šç­–ç•¥åŸ·è¡Œå£“ç¸®
            if strategy['target_precision'] == 'int4':
                compressed_model = self._apply_int4_quantization()
            elif strategy['target_precision'] == 'int8':
                compressed_model = self._apply_int8_quantization()
            else:
                compressed_model = self._apply_fp16_optimization()

            # æ¸¬è©¦å£“ç¸®å¾Œæ•ˆæœ
            compression_test = self._test_compressed_model(compressed_model, strategy)

            return {
                'compression_successful': True,
                'strategy_applied': strategy,
                'compressed_model_info': compression_test,
                'implementation_time': datetime.now().isoformat()
            }

        except Exception as e:
            print(f"    å£“ç¸®å¯¦æ–½å¤±æ•—: {e}")
            return {'compression_successful': False, 'error': str(e)}

    def _apply_int4_quantization(self):
        """æ‡‰ç”¨INT4é‡åŒ–"""

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )

        model = AutoModelForCausalLM.from_pretrained(
            self.base_model,
            quantization_config=bnb_config,
            device_map="auto"
        )

        return model

    def _apply_int8_quantization(self):
        """æ‡‰ç”¨INT8é‡åŒ–"""

        model = AutoModelForCausalLM.from_pretrained(
            self.base_model,
            quantization_config=BitsAndBytesConfig(load_in_8bit=True),
            device_map="auto"
        )

        return model

    def _apply_fp16_optimization(self):
        """æ‡‰ç”¨FP16å„ªåŒ–"""

        model = AutoModelForCausalLM.from_pretrained(
            self.base_model,
            torch_dtype=torch.float16,
            device_map="auto"
        )

        return model

    def _test_compressed_model(self, model, strategy):
        """æ¸¬è©¦å£“ç¸®å¾Œæ¨¡å‹"""

        tokenizer = AutoTokenizer.from_pretrained(self.base_model)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # è¨ˆç®—å£“ç¸®å¾Œå¤§å°
        compressed_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)

        # åŠŸèƒ½æ¸¬è©¦
        test_prompt = "äººå·¥æ™ºèƒ½æŠ€è¡“"
        try:
            inputs = tokenizer(test_prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + 15,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )

            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            functional_test_passed = len(generated_text) > len(test_prompt)

        except Exception as e:
            functional_test_passed = False

        return {
            'compressed_size_mb': compressed_size_mb,
            'functional_test_passed': functional_test_passed,
            'compression_method': strategy['method'],
            'target_precision': strategy['target_precision']
        }

    def _benchmark_model_performance(self, model, tokenizer):
        """åŸºæº–æ¸¬è©¦æ¨¡å‹æ€§èƒ½"""

        test_prompts = ["AIæŠ€è¡“", "æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’"]
        inference_times = []

        for prompt in test_prompts:
            try:
                inputs = tokenizer(prompt, return_tensors="pt")
                if torch.cuda.is_available():
                    inputs = {k: v.cuda() for k, v in inputs.items()}

                start_time = time.time()

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 10,
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id
                    )

                inference_time = time.time() - start_time
                inference_times.append(inference_time)

            except Exception:
                inference_times.append(float('inf'))

        avg_inference_time = np.mean([t for t in inference_times if t != float('inf')])

        return {
            'avg_inference_time': avg_inference_time,
            'inference_success_rate': sum(1 for t in inference_times if t != float('inf')) / len(inference_times)
        }

    def _validate_compression_effects(self, compression_result):
        """é©—è­‰å£“ç¸®æ•ˆæœ"""

        if not compression_result.get('compression_successful'):
            return {'validation_passed': False}

        compressed_info = compression_result['compressed_model_info']

        # é©—è­‰æ¨™æº–
        validation_checks = {
            'functional_test': compressed_info['functional_test_passed'],
            'size_reduction': compressed_info['compressed_size_mb'] < 1000,  # ç°¡åŒ–æª¢æŸ¥
            'method_applied': compression_result['strategy_applied']['method'] != 'none'
        }

        all_passed = all(validation_checks.values())

        return {
            'validation_passed': all_passed,
            'validation_checks': validation_checks,
            'compression_summary': {
                'final_size_mb': compressed_info['compressed_size_mb'],
                'compression_method': compressed_info['compression_method'],
                'functional_integrity': compressed_info['functional_test_passed']
            }
        }

    def _save_pipeline_results(self, results: dict):
        """ä¿å­˜ç®¡ç·šçµæœ"""

        with open(self.experiment_dir / 'pipeline_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

    def _generate_engineering_report(self, results: dict) -> str:
        """ç”Ÿæˆå·¥ç¨‹åŒ–å ±å‘Š"""

        report = f"""# æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦é©—å ±å‘Š

å¯¦é©—æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åŸºç¤æ¨¡å‹: {self.base_model}

## å·¥ç¨‹åŒ–æµç¨‹åŸ·è¡Œçµæœ

### éšæ®µ1: åŸºç·šå»ºç«‹
{self._format_baseline_results(results.get('baseline', {}))}

### éšæ®µ2: ç­–ç•¥é¸æ“‡
{self._format_strategy_results(results.get('strategy', {}))}

### éšæ®µ3: å£“ç¸®å¯¦æ–½
{self._format_compression_results(results.get('compression', {}))}

### éšæ®µ4: æ•ˆæœé©—è­‰
{self._format_validation_results(results.get('validation', {}))}

## é—œéµå­¸ç¿’è¦é»

1. **ç³»çµ±æ€§æ€ç¶­**: å·¥ç¨‹åŒ–å£“ç¸®éœ€è¦ç³»çµ±æ€§çš„åˆ†æå’Œå¯¦æ–½æµç¨‹
2. **æ•ˆæœæ¬Šè¡¡**: å£“ç¸®æ¯”ã€æ€§èƒ½å’Œæº–ç¢ºæ€§ä¹‹é–“éœ€è¦æ¬Šè¡¡
3. **å……åˆ†æ¸¬è©¦**: æ¯å€‹éšæ®µéƒ½éœ€è¦å……åˆ†çš„æ¸¬è©¦å’Œé©—è­‰
4. **é¢¨éšªæ§åˆ¶**: å»ºç«‹å›æ»¾æ©Ÿåˆ¶å’Œç›£æ§é«”ç³»

## ç”Ÿç”¢éƒ¨ç½²å»ºè­°

- åœ¨ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²å‰é€²è¡Œæ›´å¤§è¦æ¨¡çš„æ¸¬è©¦
- å»ºç«‹A/Bæ¸¬è©¦æ©Ÿåˆ¶å°æ¯”å£“ç¸®å‰å¾Œæ•ˆæœ
- è¨­ç½®ç›£æ§å‘Šè­¦ï¼ŒåŠæ™‚ç™¼ç¾å•é¡Œ
- æº–å‚™å¿«é€Ÿå›æ»¾æ–¹æ¡ˆ

---
*æœ¬å ±å‘ŠåŸºæ–¼æ¼”ç¤ºå¯¦é©—ç”Ÿæˆï¼Œç”Ÿç”¢ç’°å¢ƒè«‹é€²è¡Œæ›´å®Œæ•´çš„æ¸¬è©¦ã€‚*
"""

        report_path = self.experiment_dir / 'engineering_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“‹ å·¥ç¨‹åŒ–å ±å‘Šå·²ä¿å­˜: {report_path}")

        return report

    def _format_baseline_results(self, baseline: dict) -> str:
        if baseline.get('baseline_established'):
            return f"""
âœ… åŸºç·šå»ºç«‹æˆåŠŸ
- åƒæ•¸é‡: {baseline['parameter_count']:,}
- æ¨¡å‹å¤§å°: {baseline['model_size_mb']:.1f} MB
- å¹³å‡æ¨ç†æ™‚é–“: {baseline['performance_baseline']['avg_inference_time']:.3f} ç§’
"""
        else:
            return f"âŒ åŸºç·šå»ºç«‹å¤±æ•—: {baseline.get('error', 'æœªçŸ¥éŒ¯èª¤')}"

    def _format_strategy_results(self, strategy: dict) -> str:
        if strategy.get('method'):
            return f"""
âœ… ç­–ç•¥é¸æ“‡å®Œæˆ
- å£“ç¸®æ–¹æ³•: {strategy['method']}
- ç›®æ¨™ç²¾åº¦: {strategy['target_precision']}
- é æœŸå£“ç¸®æ¯”: {strategy['expected_compression_ratio']:.1f}x
"""
        else:
            return "âŒ ç­–ç•¥é¸æ“‡å¤±æ•—"

    def _format_compression_results(self, compression: dict) -> str:
        if compression.get('compression_successful'):
            info = compression['compressed_model_info']
            return f"""
âœ… å£“ç¸®å¯¦æ–½æˆåŠŸ
- å£“ç¸®å¾Œå¤§å°: {info['compressed_size_mb']:.1f} MB
- åŠŸèƒ½æ¸¬è©¦: {'é€šé' if info['functional_test_passed'] else 'å¤±æ•—'}
- å£“ç¸®æ–¹æ³•: {info['compression_method']}
"""
        else:
            return f"âŒ å£“ç¸®å¯¦æ–½å¤±æ•—: {compression.get('error', 'æœªçŸ¥éŒ¯èª¤')}"

    def _format_validation_results(self, validation: dict) -> str:
        if validation.get('validation_passed'):
            summary = validation['compression_summary']
            return f"""
âœ… é©—è­‰é€šé
- æœ€çµ‚å¤§å°: {summary['final_size_mb']:.1f} MB
- åŠŸèƒ½å®Œæ•´æ€§: {'ä¿æŒ' if summary['functional_integrity'] else 'å—æ'}
- å£“ç¸®æ–¹æ³•: {summary['compression_method']}
"""
        else:
            return "âŒ é©—è­‰æœªé€šé"

def main():
    """ä¸»å¯¦é©—å‡½æ•¸"""

    print("Lab 0.6: æ¨¡å‹å£“ç¸®å·¥ç¨‹åŒ–å¯¦è¸")
    print("=" * 60)

    # å‰µå»ºå¯¦é©—å®¤å¯¦ä¾‹
    lab = ModelCompressionEngineeringLab()

    # é‹è¡Œå®Œæ•´å·¥ç¨‹åŒ–æµç¨‹
    results = lab.run_engineering_compression_pipeline()

    if results:
        print("\nğŸ“ å¯¦é©—å­¸ç¿’ç¸½çµ:")
        print("âœ… é«”é©—äº†å®Œæ•´çš„å·¥ç¨‹åŒ–å£“ç¸®æµç¨‹")
        print("âœ… ç†è§£äº†å„éšæ®µçš„æŠ€è¡“è¦é»å’Œæ³¨æ„äº‹é …")
        print("âœ… æŒæ¡äº†å£“ç¸®æ•ˆæœçš„è©•ä¼°å’Œé©—è­‰æ–¹æ³•")
        print("âœ… å»ºç«‹äº†å·¥ç¨‹åŒ–å¯¦æ–½çš„ç³»çµ±æ€§æ€ç¶­")

        print("\nğŸ’¡ å¯¦éš›æ‡‰ç”¨å»ºè­°:")
        print("- åœ¨çœŸå¯¦é …ç›®ä¸­ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œæ•¸æ“šé›†")
        print("- å»ºç«‹è‡ªå‹•åŒ–çš„å£“ç¸®å’Œæ¸¬è©¦æµç¨‹")
        print("- èˆ‡æ¥­å‹™åœ˜éšŠç·Šå¯†é…åˆï¼Œç¢ºä¿å£“ç¸®æ•ˆæœç¬¦åˆéœ€æ±‚")
        print("- æŒçºŒè·Ÿè¹¤æœ€æ–°çš„æ¨¡å‹å£“ç¸®æŠ€è¡“ç™¼å±•")

if __name__ == "__main__":
    main()