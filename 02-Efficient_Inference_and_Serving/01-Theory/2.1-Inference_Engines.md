# å¤§æ¨¡å‹æ¨ç†æŠ€è¡“ 2.1 æ¨ç†å¼•æ“ (Inference Engines)

æœ¬æ•™å­¸æ¨¡çµ„æ·±å…¥æ¢è¨å¤§å‹èªè¨€æ¨¡å‹ (LLM) æ¨ç†å¼•æ“çš„æ ¸å¿ƒæŠ€è¡“ï¼Œæ¶µè“‹å¾åŸºç¤åŸç†åˆ°ç”Ÿç”¢æ‡‰ç”¨çš„å®Œæ•´çŸ¥è­˜é«”ç³»ã€‚

**ğŸš¨ é‡è¦èªªæ˜**: æœ¬èª²ç¨‹æä¾›å®Œæ•´ç†è«–æ•™å­¸èˆ‡æ¶æ§‹åˆ†æï¼Œé©åˆå–®GPUç’°å¢ƒå­¸ç¿’èˆ‡å°è¦æ¨¡å¯¦é©—ã€‚

| å±¤æ¬¡ | å­¸ç¿’ç›®æ¨™ | æ ¸å¿ƒå…§å®¹ | ç”¢å‡º |
| :--- | :--- | :--- | :--- |
| **Fundamentals** | ç†è§£LLMæ¨ç†æŒ‘æˆ°èˆ‡å¼•æ“è¨­è¨ˆ | è‡ªå›æ­¸ç”Ÿæˆã€KV Cacheã€è¨˜æ†¶é«”ç“¶é ¸ | æŒæ¡æ¨ç†åŸºæœ¬æ¦‚å¿µ |
| **First Principles** | æŒæ¡PagedAttentionèˆ‡Continuous BatchingåŸç† | è™›æ“¬è¨˜æ†¶é«”ç®¡ç†ã€å‹•æ…‹æ‰¹æ¬¡èª¿åº¦ | ç†è§£æ ¸å¿ƒæ¼”ç®—æ³•è¨­è¨ˆ |
| **Body of Knowledge** | ç²¾é€šä¸»æµæ¨ç†å¼•æ“æ‡‰ç”¨ | vLLMã€TensorRT-LLMã€å¼•æ“é¸å‹ | å…·å‚™ç”Ÿç”¢éƒ¨ç½²èƒ½åŠ› |

---

## 1. Fundamentals (åŸºç¤)

### 1.1 LLM æ¨ç†çš„æ ¸å¿ƒæŒ‘æˆ°

#### 1.1.1 è‡ªå›æ­¸ç”Ÿæˆç‰¹æ€§

LLM æ¡ç”¨è‡ªå›æ­¸ (Autoregressive) æ–¹å¼é€å€‹ç”Ÿæˆ tokenï¼Œæ¯å€‹ token çš„ç”Ÿæˆä¾è³´æ–¼ä¹‹å‰æ‰€æœ‰ tokenã€‚

**ç”Ÿæˆæµç¨‹**:
```
ç”Ÿæˆ "Hello World"
Step 1: Input = ""      â†’ Output = "Hello"
Step 2: Input = "Hello" â†’ Output = " World"
```

**æ¨ç†ç“¶é ¸**:
- **åºåˆ—ä¾è³´**: ç„¡æ³•ä¸¦è¡Œç”Ÿæˆï¼Œå¿…é ˆé€å€‹ç”¢ç”Ÿ
- **KV Cache å¢é•·**: æ¯ç”Ÿæˆä¸€å€‹ tokenï¼ŒKV Cache ç·šæ€§å¢é•·
- **è¨˜æ†¶é«”é »å¯¬å—é™**: æ¨ç†éç¨‹ç‚º Memory-Bound è€Œé Compute-Bound

#### 1.1.2 è¨˜æ†¶é«”é »å¯¬ç“¶é ¸

**æ¡ˆä¾‹åˆ†æ**: Llama-2-7B æ¨ç†

```
æ¨¡å‹åƒæ•¸: 7B Ã— 2 bytes (FP16) = 14GB
å–®æ¬¡å‰å‘å‚³æ’­:
  - è¨ˆç®—é‡: ç´„ 45M FLOPs (çŸ©é™£ä¹˜æ³•)
  - è¨˜æ†¶é«”è¨ªå•: 14GB (è®€å–æ‰€æœ‰åƒæ•¸)

GPU æ€§èƒ½ (A100):
  - è¨ˆç®—èƒ½åŠ›: 312 TFLOPS (FP16 Tensor Core)
  - è¨˜æ†¶é«”é »å¯¬: 1.94 TB/s (80GB) / 1.6 TB/s (40GB)
  
æ™‚é–“åˆ†æ:
  - è¨ˆç®—æ™‚é–“: 45M / 312T â‰ˆ 0.14ms
  - è¨˜æ†¶é«”è¨ªå•æ™‚é–“: 14GB / 1.94TB/s â‰ˆ 7.2ms
  
çµè«–: è¨˜æ†¶é«”è¨ªå•æ™‚é–“ >> è¨ˆç®—æ™‚é–“ (Memory-Bound)
```

#### 1.1.3 KV Cache è¨˜æ†¶é«”å ç”¨

**KV Cache è¨ˆç®—** (Llama-2-7B ç¯„ä¾‹):

```python
num_layers = 32      # Transformer å±¤æ•¸
num_heads = 32       # æ³¨æ„åŠ›é ­æ•¸
head_dim = 128       # æ¯å€‹é ­çš„ç¶­åº¦
batch_size = 16      # æ‰¹æ¬¡å¤§å°
seq_len = 2048       # ç”Ÿæˆåºåˆ—é•·åº¦
precision = 2        # FP16 = 2 bytes

kv_cache_size = (
    2                 # K å’Œ V
    Ã— batch_size
    Ã— num_layers
    Ã— num_heads
    Ã— seq_len
    Ã— head_dim
    Ã— precision
) / (1024**3)  # è½‰ç‚º GB

# çµæœ: ç´„ 16GB (åƒ… KV Cache!)
```

**é—œéµæ´å¯Ÿ**:
- KV Cache éš¨åºåˆ—é•·åº¦ç·šæ€§å¢é•·
- æ‰¹æ¬¡æ¨ç†æ™‚è¨˜æ†¶é«”å ç”¨å·¨å¤§
- æˆç‚ºæ¨ç†æ“´å±•çš„ä¸»è¦ç“¶é ¸

---

## 2. First Principles (åŸç†)

### 2.1 PagedAttention: è™›æ“¬è¨˜æ†¶é«”ç®¡ç†

#### 2.1.1 å‚³çµ± KV Cache çš„è¨˜æ†¶é«”æµªè²»

**å•é¡Œ**: éœæ…‹è¨˜æ†¶é«”åˆ†é…å°è‡´ç¢ç‰‡åŒ–

```
å‚³çµ±æ–¹æ¡ˆ (é åˆ†é…æœ€å¤§é•·åº¦):
Request 1 (seq_len=1024): åˆ†é… 2048 ç©ºé–“ï¼Œæµªè²» 50%
Request 2 (seq_len=512):  åˆ†é… 2048 ç©ºé–“ï¼Œæµªè²» 75%

ç¸½è¨˜æ†¶é«”åˆ©ç”¨ç‡: ç´„ 30-50%
```

#### 2.1.2 PagedAttention æ ¸å¿ƒè¨­è¨ˆ

**éˆæ„Ÿ**: å€Ÿé‘‘ä½œæ¥­ç³»çµ±è™›æ“¬è¨˜æ†¶é«”ç®¡ç†

**æ ¸å¿ƒæ¦‚å¿µ**:
1. **ç‰©ç†å¡Š (Physical Block)**: å›ºå®šå¤§å°çš„ KV Cache å–®å…ƒ (å¦‚ 64 tokens)
2. **é‚è¼¯é  (Logical Page)**: è«‹æ±‚çš„é€£çºŒä½å€ç©ºé–“
3. **é è¡¨ (Page Table)**: é‚è¼¯é åˆ°ç‰©ç†å¡Šçš„æ˜ å°„

**è¨˜æ†¶é«”ç¯€çœ**:
```
PagedAttention:
Request 1 (1024 tokens): åˆ†é… 16 å€‹ block (1024/64)
Request 2 (512 tokens):  åˆ†é… 8 å€‹ block

è¨˜æ†¶é«”åˆ©ç”¨ç‡: >95%
ç¯€çœ: (50% - 5%) / 50% â‰ˆ 88% è¨˜æ†¶é«”ç¯€çœ
```

**Python å¯¦ç¾æ¦‚å¿µ**:
```python
class BlockManager:
    def __init__(self, num_blocks, block_size):
        self.num_blocks = num_blocks
        self.block_size = block_size  # æ¯å€‹ block çš„ tokens æ•¸
        self.free_blocks = list(range(num_blocks))
        
    def allocate(self, num_required):
        """åˆ†é…æŒ‡å®šæ•¸é‡çš„ blocks"""
        blocks = []
        for _ in range(num_required):
            if not self.free_blocks:
                raise OutOfMemoryError("KV Cache å·²æ»¿")
            blocks.append(self.free_blocks.pop(0))
        return blocks
    
    def free(self, blocks):
        """é‡‹æ”¾ blocks"""
        self.free_blocks.extend(blocks)
```

### 2.2 Continuous Batching: å‹•æ…‹æ‰¹æ¬¡èª¿åº¦

#### 2.2.1 Static Batching çš„å±€é™

**å•é¡Œ**: æ‰¹æ¬¡å…§æ‰€æœ‰è«‹æ±‚å¿…é ˆåŒæ™‚å®Œæˆ

```
Static Batching:
Batch = [Req1(é•·), Req2(çŸ­), Req3(çŸ­), Req4(çŸ­)]

æ™‚é–“è»¸:
Req1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (800ms, 100 tokens)
Req2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            (400ms, 50 tokens) - é–’ç½® 400ms
Req3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            (400ms, 50 tokens) - é–’ç½® 400ms
Req4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            (400ms, 50 tokens) - é–’ç½® 400ms

ç¸½æ™‚é–“: 800ms
GPU åˆ©ç”¨ç‡: 400ms / 800ms = 50%
```

#### 2.2.2 Continuous Batching å„ªåŒ–

**æ ¸å¿ƒæ€æƒ³**: è«‹æ±‚å®Œæˆå¾Œç«‹å³ç§»å‡ºæ‰¹æ¬¡ï¼Œè£œå……æ–°è«‹æ±‚

```
Continuous Batching:
æ™‚é–“: 0     200ms  400ms  600ms  800ms
Req1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Req2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (å®Œæˆï¼Œç§»å‡º)
Req5:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (è£œå……é€²å…¥)
Req3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (å®Œæˆï¼Œç§»å‡º)
Req6:                 â–ˆâ–ˆâ–ˆâ–ˆ (è£œå……é€²å…¥)

ååé‡æå‡: 2-3x
GPU åˆ©ç”¨ç‡: 85-95%
```

---

## 3. Body of Knowledge (å¯¦å‹™)

### 3.1 vLLM: é«˜æ•ˆæ¨ç†å¼•æ“

vLLM (UC Berkeley) é€é PagedAttention èˆ‡ Continuous Batching å¯¦ç¾æ¥­ç•Œé ˜å…ˆçš„æ¨ç†æ€§èƒ½ã€‚

#### 3.1.1 æ ¸å¿ƒæ¶æ§‹

```
vLLM æ¶æ§‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       API Server (OpenAIå…¼å®¹)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Scheduler               â”‚
â”‚  - Continuous Batching          â”‚
â”‚  - Request å„ªå…ˆç´šæ’ç¨‹            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Block Manager              â”‚
â”‚  - PagedAttention è¨˜æ†¶é«”ç®¡ç†     â”‚
â”‚  - ç‰©ç†å¡Šåˆ†é…èˆ‡å›æ”¶              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Model Executor            â”‚
â”‚  - PagedAttention è¨ˆç®—          â”‚
â”‚  - CUDA Kernel å„ªåŒ–             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.1.2 ä½¿ç”¨ç¯„ä¾‹

**åŸºç¤æ¨ç†**:
```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–å¼•æ“
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    tensor_parallel_size=1,      # å–®GPU
    gpu_memory_utilization=0.9,  # ä½¿ç”¨ 90% GPU è¨˜æ†¶é«”
    max_num_seqs=32,             # æœ€å¤§ä¸¦ç™¼è«‹æ±‚æ•¸
)

# é…ç½®ç”Ÿæˆåƒæ•¸
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100
)

# æ‰¹æ¬¡æ¨ç†
prompts = ["Hello, ", "Explain AI: ", "Code example:"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

**æ€§èƒ½å°æ¯”** (Llama-2-7B, A100):

| å¼•æ“ | ååé‡ (tokens/s) | TTFT (ms) | è¨˜æ†¶é«”åˆ©ç”¨ç‡ |
|------|------------------|-----------|-------------|
| HuggingFace | 250 | 450 | 60% |
| **vLLM** | **2,800** | **120** | **95%** |
| åŠ é€Ÿæ¯” | **11.2x** | **3.8x** | **1.6x** |

*ä¾†æº: Kwon et al., "Efficient Memory Management for Large Language Model Serving with PagedAttention", SOSP 2023*

---

### 3.2 TensorRT-LLM: NVIDIA å„ªåŒ–å¼•æ“

TensorRT-LLM æ¡ç”¨ AOT (Ahead-of-Time) ç·¨è­¯å„ªåŒ–ï¼Œæä¾›æ¥µè‡´æ€§èƒ½ã€‚

#### 3.2.1 æ¶æ§‹æ¦‚è¦½

TensorRT-LLM æ¡ç”¨åˆ†å±¤æ¶æ§‹è¨­è¨ˆï¼š

```
æ‡‰ç”¨å±¤ (Application Layer)
â”œâ”€â”€ Python API (tensorrt_llm.models)
â”œâ”€â”€ C++ Runtime (tensorrt_llm.runtime)
â””â”€â”€ Plugin System (tensorrt_llm.plugin)

ç·¨è­¯å±¤ (Compilation Layer)
â”œâ”€â”€ Model Definition (tensorrt_llm.models)
â”œâ”€â”€ Graph Optimization (tensorrt_llm.graph)
â””â”€â”€ Engine Building (tensorrt_llm.builder)

åŸ·è¡Œå±¤ (Execution Layer)
â”œâ”€â”€ TensorRT Core Engine
â”œâ”€â”€ CUDA Kernels
â””â”€â”€ Hardware Abstraction
```

#### 3.2.2 æ ¸å¿ƒçµ„ä»¶

**1. Plugin ç³»çµ±**
```python
# è‡ªå®šç¾© Plugin ç¯„ä¾‹
class FlashAttentionPlugin(trt.IPluginV2DynamicExt):
    def get_output_dimensions(self, index, inputs, exprBuilder):
        # å®šç¾©è¼¸å‡ºç¶­åº¦
        return inputs[0].d  # Q, K, V ç›¸åŒç¶­åº¦

    def enqueue(self, inputDesc, outputDesc, inputs, outputs, workspace, stream):
        # èª¿ç”¨ FlashAttention CUDA kernel
        flash_attention_kernel(inputs, outputs, stream)
```

**2. è‡ªå®šç¾© Kernel å„ªåŒ–**
- **Attention Kernel**: èåˆ QKV è¨ˆç®—ã€Softmaxã€è¼¸å‡ºæŠ•å½±
- **MLP Kernel**: èåˆ Gateã€Upã€Down æŠ•å½±
- **LayerNorm Kernel**: é«˜æ•ˆæ•¸å€¼ç©©å®šå¯¦ç¾

#### 3.2.3 ç·¨è­¯å„ªåŒ–æµç¨‹

```
TensorRT-LLM æµç¨‹:
HF Model â†’ Convert â†’ Build Engine â†’ Runtime
   â†“           â†“          â†“            â†“
 PyTorch   TRT-LLM    å„ªåŒ–ç·¨è­¯      æ¨ç†åŸ·è¡Œ
            æ ¼å¼      (kernelé¸æ“‡)   (CUDAåŸ·è¡Œ)
```

**è©³ç´°ç·¨è­¯æ­¥é©Ÿ**:
```python
import tensorrt_llm as tllm

# 1. æ¨¡å‹å®šç¾©
config = tllm.models.LlamaConfig(
    vocab_size=32000,
    hidden_size=4096,
    num_attention_heads=32,
    num_layers=32,
    dtype='float16'
)

# 2. æ§‹å»ºç¶²çµ¡
model = tllm.models.LlamaForCausalLM(config)

# 3. å¼•æ“ç·¨è­¯
engine = tllm.Builder().build_engine(
    model,
    max_batch_size=32,
    max_input_len=2048,
    max_output_len=512,
    optimize_level=3  # æœ€é«˜å„ªåŒ–ç­‰ç´š
)
```

#### 3.2.4 é‡åŒ–æ”¯æ´

| ç²¾åº¦ | è¨˜æ†¶é«”å ç”¨ | é€Ÿåº¦ | ç²¾åº¦æå¤± | é©ç”¨å ´æ™¯ |
|------|----------|------|---------|---------|
| FP16 | 100% | 1.0x | ç„¡ | æ¨™æº–æ¨ç† |
| INT8 | 50% | 1.5-2x | <1% | å¹³è¡¡æ€§èƒ½ |
| FP8 (H100) | 50% | 2-3x | <0.5% | æ¥µè‡´æ€§èƒ½ |
| INT4 | 25% | 3-4x | 2-3% | è³‡æºå—é™ |

**é‡åŒ–é…ç½®ç¯„ä¾‹**:
```python
# å•Ÿç”¨ FP8 é‡åŒ– (H100)
config.quant_mode = QuantMode.use_fp8_qdq()

# INT8 å¹³æ»‘é‡åŒ–
config.quant_mode = QuantMode.use_smooth_quant()
config.smooth_quant_alpha = 0.5

# INT4 é‡é‡é‡åŒ–
config.quant_mode = QuantMode.use_weight_only(use_int4_weights=True)
```

#### 3.2.5 æ€§èƒ½å„ªå‹¢

**æ¨ç†æ€§èƒ½å°æ¯”** (Llama-2-7B, A100):

| æ¨ç†å¼•æ“ | Throughput (tok/s) | Latency (ms) | Memory (GB) | GPU åˆ©ç”¨ç‡ |
|----------|-------------------|--------------|-------------|-----------|
| HuggingFace | 120 | 450 | 14.2 | 65% |
| vLLM | 2,100 | 28 | 16.8 | 90% |
| **TensorRT-LLM** | **2,800** | **22** | **12.5** | **95%** |

**æŠ€è¡“å„ªå‹¢**:
- **æ¥µè‡´å„ªåŒ–**: é‡å° NVIDIA GPU æ·±åº¦å®¢è£½åŒ–
- **è¨˜æ†¶é«”æ•ˆç‡**: æ¯” vLLM ç¯€çœ 25% è¨˜æ†¶é«”
- **å»¶é²å„ªå‹¢**: æœ€ä½çš„ TTFT (Time to First Token)
- **é‡åŒ–æ”¯æ´**: æ¥­ç•Œæœ€å…ˆé€²çš„ FP8ã€INT4 é‡åŒ–

---

### 3.3 SGLang: æ–°èˆˆçµæ§‹åŒ–ç”Ÿæˆå¼•æ“

SGLang (Structured Generation Language) æ˜¯å°ˆç‚ºçµæ§‹åŒ–ç”Ÿæˆè¨­è¨ˆçš„æ–°å‹æ¨ç†å¼•æ“ã€‚

#### 3.3.1 RadixAttention å‰µæ–°

**æ ¸å¿ƒå•é¡Œ**: å‚³çµ± KV Cache ç„¡æ³•åœ¨è«‹æ±‚é–“å…±äº«

**è§£æ±ºæ–¹æ¡ˆ**: RadixAttention - åŸºæ–¼ Radix Tree çš„ KV Cache å…±äº«

```
å‚³çµ±æ–¹å¼:
Request 1: "Translate to French: Hello"     â†’ ç¨ç«‹ KV Cache
Request 2: "Translate to French: Goodbye"   â†’ ç¨ç«‹ KV Cache
Request 3: "Translate to French: Thank you" â†’ ç¨ç«‹ KV Cache

RadixAttention:
              â”Œâ”€ "Hello"
"Translate to French: " â”¼â”€ "Goodbye"    (å…±äº«å‰ç¶´ KV Cache)
              â””â”€ "Thank you"
```

#### 3.3.2 Constrained Decoding

**åŠŸèƒ½**: ç¢ºä¿è¼¸å‡ºç¬¦åˆç‰¹å®šæ ¼å¼ (JSON, YAML, ç¨‹å¼ç¢¼)

```python
# SGLang çµæ§‹åŒ–ç”Ÿæˆç¯„ä¾‹
import sglang as sgl

@sgl.function
def generate_json(s, prompt):
    s += prompt
    s += "{\n"
    with s.constrain(format="json"):
        s += sgl.gen("content", max_tokens=200)
    s += "\n}"

# ä½¿ç”¨
result = generate_json("Generate a person profile:")
# ä¿è­‰è¼¸å‡ºæ˜¯æœ‰æ•ˆ JSON
```

#### 3.3.3 Multi-modal æ”¯æ´

SGLang åŸç”Ÿæ”¯æ´å¤šæ¨¡æ…‹æ¨¡å‹ (Vision-Language):

```python
# å¤šæ¨¡æ…‹ç”Ÿæˆ
@sgl.function
def describe_image(s, image_path, question):
    s += sgl.image(image_path)
    s += f"Question: {question}\nAnswer:"
    s += sgl.gen("answer", max_tokens=100)
```

---

### 3.4 å…¶ä»–æ¨ç†å¼•æ“ç°¡ä»‹

#### 3.4.1 LightLLM: å¤š GPU åˆ†æ•£å¼å„ªåŒ–

**ç‰¹è‰²**:
- é‡å°å¤š GPU æ¨ç†å„ªåŒ–
- æ”¯æ´ Tensor Parallel å’Œ Pipeline Parallel
- å‹•æ…‹æ‰¹æ¬¡èˆ‡è² è¼‰å‡è¡¡

**æ¶æ§‹**:
```
GPU 0: Layer 0-7   â”€â”
GPU 1: Layer 8-15  â”€â”¼â”€ Pipeline Parallel
GPU 2: Layer 16-23 â”€â”¤
GPU 3: Layer 24-31 â”€â”˜
```

#### 3.4.2 MNN-LLM: ç§»å‹•ç«¯éƒ¨ç½²

**ç‰¹è‰²**:
- å°ˆç‚ºç§»å‹•è¨­å‚™å„ªåŒ–
- æ”¯æ´ ARM CPUã€GPU åŠ é€Ÿ
- æ¥µä½åŠŸè€—æ¨ç†

**é©ç”¨å ´æ™¯**:
- æ‰‹æ©Ÿ APP æœ¬åœ°æ¨ç†
- é‚Šç·£è¨ˆç®—è¨­å‚™
- éš±ç§æ•æ„Ÿæ‡‰ç”¨

#### 3.4.3 Text Generation Inference (TGI)

**Hugging Face å®˜æ–¹æ¨ç†æœå‹™**:
- èˆ‡ HF ç”Ÿæ…‹æ·±åº¦æ•´åˆ
- æ”¯æ´ä¸»æµé–‹æºæ¨¡å‹
- Docker å®¹å™¨åŒ–éƒ¨ç½²

```bash
# TGI éƒ¨ç½²ç¯„ä¾‹
docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id meta-llama/Llama-2-7b-hf
```

---

## 4. å¼•æ“é¸å‹æŒ‡å—

### 4.1 æ€§èƒ½å°æ¯”çŸ©é™£

| å¼•æ“ | ååé‡ | å»¶é² | æ˜“ç”¨æ€§ | éƒ¨ç½² | æ¨è–¦å ´æ™¯ |
|------|--------|------|--------|------|----------|
| **vLLM** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | é€šç”¨é¦–é¸ |
| **TensorRT-LLM** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­ | NVIDIA é«˜æ€§èƒ½ |
| **SGLang** | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | çµæ§‹åŒ–ç”Ÿæˆ |
| **TGI** | â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | HF ç”Ÿæ…‹ |
| **LightLLM** | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ | å¤š GPU åˆ†æ•£å¼ |

### 4.2 è©³ç´°å°æ¯”åˆ†æ

#### 4.2.1 æ€§èƒ½ Benchmark (Llama-2-7B, A100)

| æŒ‡æ¨™ | vLLM | TensorRT-LLM | SGLang | TGI | HuggingFace |
|------|------|-------------|--------|-----|-------------|
| **ååé‡** (tok/s) | 2,100 | **2,800** | 1,900 | 1,200 | 120 |
| **TTFT** (ms) | 28 | **22** | 35 | 45 | 450 |
| **è¨˜æ†¶é«”ä½¿ç”¨** (GB) | 16.8 | **12.5** | 15.2 | 18.5 | 14.2 |
| **GPU åˆ©ç”¨ç‡** | 90% | **95%** | 85% | 75% | 65% |

#### 4.2.2 åŠŸèƒ½ç‰¹æ€§å°æ¯”

| åŠŸèƒ½ | vLLM | TensorRT-LLM | SGLang | TGI | èªªæ˜ |
|------|------|-------------|--------|-----|------|
| **PagedAttention** | âœ… | âœ… | âœ… | âŒ | KV Cache å„ªåŒ– |
| **Continuous Batching** | âœ… | âœ… | âœ… | âœ… | å‹•æ…‹æ‰¹æ¬¡ |
| **é‡åŒ–æ”¯æ´** | FP16/INT8 | FP16/INT8/FP8/INT4 | FP16/INT8 | FP16/INT8 | ç²¾åº¦æ”¯æ´ |
| **çµæ§‹åŒ–ç”Ÿæˆ** | åŸºç¤ | åŸºç¤ | **å¼·åŒ–** | åŸºç¤ | JSON/YAML |
| **å¤šæ¨¡æ…‹** | âŒ | éƒ¨åˆ† | âœ… | âŒ | Vision-Language |
| **åˆ†æ•£å¼æ¨ç†** | âœ… | âœ… | åŸºç¤ | âœ… | å¤š GPU |

### 4.3 ä½¿ç”¨å ´æ™¯å»ºè­°

#### 4.3.1 å¿«é€ŸåŸå‹é–‹ç™¼
**æ¨è–¦**: vLLM
```python
# æœ€ç°¡å–®çš„é–‹å§‹æ–¹å¼
from vllm import LLM

llm = LLM(model="meta-llama/Llama-2-7b-hf")
outputs = llm.generate(["Hello world"])
```

**å„ªå‹¢**:
- å®‰è£ç°¡å–®: `pip install vllm`
- API ç›´è§€æ˜“ç”¨
- èˆ‡ HuggingFace å®Œå…¨ç›¸å®¹
- è±å¯Œçš„æ–‡æª”èˆ‡ç¤¾ç¾¤æ”¯æ´

#### 4.3.2 ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²
**æ¨è–¦**: TensorRT-LLM (NVIDIA GPU) æˆ– vLLM

**TensorRT-LLM é©ç”¨**:
- éœ€è¦æ¥µè‡´æ€§èƒ½ (ä½å»¶é²ã€é«˜åå)
- NVIDIA GPU ç’°å¢ƒ (A100, H100)
- æœ‰å……è¶³çš„å·¥ç¨‹è³‡æºé€²è¡Œå„ªåŒ–

**vLLM é©ç”¨**:
- å¹³è¡¡æ€§èƒ½èˆ‡é–‹ç™¼æ•ˆç‡
- å¿«é€Ÿéƒ¨ç½²éœ€æ±‚
- å¤šç¨® GPU æ”¯æ´

#### 4.3.3 ç‰¹æ®Šéœ€æ±‚å ´æ™¯

**çµæ§‹åŒ–ç”Ÿæˆ**: SGLang
```python
# ç¢ºä¿ JSON æ ¼å¼è¼¸å‡º
@sgl.function
def generate_profile(s, name):
    s += f"Generate profile for {name}:"
    with s.constrain(format="json"):
        s += sgl.gen("profile", max_tokens=200)
```

**ä¼æ¥­ HF ç”Ÿæ…‹**: Text Generation Inference
```bash
# èˆ‡ HF Hub ç„¡ç¸«æ•´åˆ
docker run -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id your-private-model
```

### 4.4 æ±ºç­–æµç¨‹åœ–

```mermaid
graph TD
    A[é–‹å§‹é¸å‹] --> B{ä½¿ç”¨ NVIDIA GPU?}
    
    B -->|æ˜¯| C{éœ€è¦æ¥µè‡´æ€§èƒ½?}
    B -->|å¦| D[âŒ ä¸æ¨è–¦: é NVIDIA GPU<br/>è€ƒæ…® Llama.cpp æˆ–å‡ç´šç¡¬é«”]
    
    C -->|æ˜¯| E[TensorRT-LLM<br/>æ¥µè‡´æ€§èƒ½]
    C -->|å¦| F{åå¥½ HuggingFace ç”Ÿæ…‹?}
    
    F -->|æ˜¯| G[TGI<br/>HF æ•´åˆ]
    F -->|å¦| H[vLLM<br/>â­ é€šç”¨é¦–é¸<br/>80% å ´æ™¯æ¨è–¦]
    
    style E fill:#ff6b6b,color:#fff
    style G fill:#4ecdc4,color:#fff
    style H fill:#45b7d1,color:#fff
    style D fill:#f8d7da,color:#721c24
    style A fill:#667eea,color:#fff
```

**æ±ºç­–é‚è¼¯**:
1. **å…ˆåˆ¤æ–·ç¡¬é«”** - NVIDIA GPU æ˜¯é«˜æ€§èƒ½æ¨ç†çš„å¿…è¦æ¢ä»¶
2. **å¦‚ç„¡ NVIDIA GPU** - è€ƒæ…® Llama.cpp (CPU) æˆ–å‡ç´šç¡¬é«”
3. **å¦‚æœ‰ NVIDIA GPU** - æ ¹æ“šæ€§èƒ½éœ€æ±‚å’Œç”Ÿæ…‹åå¥½é¸æ“‡ï¼š
   - æ¥µè‡´æ€§èƒ½ â†’ TensorRT-LLM
   - HF ç”Ÿæ…‹ â†’ TGI  
   - å¹³è¡¡é¸æ“‡ â†’ vLLM â­ (å¤§å¤šæ•¸å ´æ™¯)

### 4.5 æœ€ä½³å¯¦è¸å»ºè­°

#### 4.5.1 é–‹ç™¼éšæ®µ
1. **å¾ vLLM é–‹å§‹**: å¿«é€Ÿé©—è­‰æƒ³æ³•
2. **æ€§èƒ½æ¸¬è©¦**: ä½¿ç”¨çœŸå¯¦å·¥ä½œè² è¼‰æ¸¬è©¦
3. **æ¼¸é€²å„ªåŒ–**: æ ¹æ“šç“¶é ¸é¸æ“‡å„ªåŒ–æ–¹å‘

#### 4.5.2 ç”Ÿç”¢éƒ¨ç½²
1. **è² è¼‰æ¸¬è©¦**: æ¨¡æ“¬çœŸå¯¦æµé‡
2. **ç›£æ§è¨­ç½®**: å»¶é²ã€ååé‡ã€éŒ¯èª¤ç‡
3. **ç‰ˆæœ¬æ§åˆ¶**: æ¨¡å‹èˆ‡å¼•æ“ç‰ˆæœ¬ç®¡ç†
4. **å›æ»¾æ©Ÿåˆ¶**: å¿«é€Ÿæ¢å¾©åˆ°ç©©å®šç‰ˆæœ¬

---

## 5. å»¶ä¼¸å­¸ç¿’èˆ‡å¯¦è¸æŒ‡å¼•

**è«–æ–‡**:
- vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention (SOSP 2023)
- FlashAttention: Fast and Memory-Efficient Exact Attention (NeurIPS 2022)

**è³‡æº**:
- vLLM å®˜æ–¹æ–‡æª”: https://docs.vllm.ai/
- TensorRT-LLM GitHub: https://github.com/NVIDIA/TensorRT-LLM
- SGLang å°ˆæ¡ˆ: https://github.com/sgl-project/sglang

**ä¸‹ä¸€æ­¥**: å­¸ç¿’ Lab-2.1 (vLLM Deployment) é€²è¡Œå¯¦ä½œç·´ç¿’

---

**ç‰ˆæœ¬**: v1.0  
**åˆ¶å®šæ—¥æœŸ**: 2025-10-09  
**ç¶­è­·è€…**: LLM æ•™å­¸å°ˆæ¡ˆåœ˜éšŠ
