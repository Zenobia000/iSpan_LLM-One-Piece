# 大模型推理技術 2.2 模型服務與優化 (Model Serving & Optimization)

本教學模組深入探討大型語言模型 (LLM) 服務架構與推理優化技術，涵蓋從服務設計到生產部署的完整知識體系。

| 層次 | 學習目標 | 核心內容 | 產出 |
| :--- | :--- | :--- | :--- |
| **Fundamentals** | 理解服務架構與優化目標 | API 設計、延遲與吞吐量 | 掌握服務設計原則 |
| **First Principles** | 掌握優化理論與數學模型 | 記憶體、吞吐量、延遲優化 | 理解優化演算法原理 |
| **Body of Knowledge** | 精通生產部署最佳實踐 | FastAPI、Triton、監控 | 具備生產部署能力 |

---

## 1. Fundamentals (基礎)

### 1.1 推理服務架構

**多層架構設計**:
```
┌──────────────────────────┐
│   API Gateway / LB       │  負載均衡、路由
├──────────────────────────┤
│   Application Layer      │  業務邏輯、認證
│   (FastAPI / Flask)      │
├──────────────────────────┤
│   Model Serving Layer    │  推理引擎
│   (vLLM / TensorRT)      │
├──────────────────────────┤
│   Monitoring & Logging   │  可觀測性
│   (Prometheus / Grafana) │
└──────────────────────────┘
```

### 1.2 推理優化三大目標

**1. 延遲優化 (Latency)**:
- TTFT (Time to First Token): 首個 token 延遲 (<500ms)
- ITL (Inter-Token Latency): Token 間延遲 (<50ms)

**2. 吞吐量優化 (Throughput)**:
- Tokens/second: 每秒生成 token 數
- Requests/second: 每秒處理請求數
- GPU 利用率: 目標 >85%

**3. 成本優化 (Cost)**:
- 記憶體效率: KV Cache 管理
- 批次大小: 最大化並發
- 資源利用: Auto-scaling

---

## 2. First Principles (原理)

### 2.1 吞吐量優化理論

**吞吐量模型**:
$$\text{Throughput} = \frac{\text{Batch Size} \times \text{Avg Output Len}}{\text{Latency Per Batch}}$$

**Continuous Batching 加速比**:
$$\text{Speedup} = \frac{B \times L_{avg}}{B \times L_{max}} = \frac{L_{avg}}{L_{max}}$$

其中 $L_{avg}$ 為平均輸出長度，$L_{max}$ 為最大輸出長度。

典型場景: $L_{avg} = 50$, $L_{max} = 200$，加速比 = 4x

### 2.2 Speculative Decoding 原理

**核心思想**: 使用小模型 draft，大模型 verify

**加速比計算**:
$$\text{Speedup} = \frac{\gamma}{1 + (1-\alpha)\gamma}$$

其中:
- $\gamma$: draft 每次生成的 tokens 數
- $\alpha$: 平均接受率

典型值: $\gamma=4$, $\alpha=0.7$，加速比 ≈ 2.3x

---

## 3. Body of Knowledge (實務)

### 3.1 FastAPI 服務構建

**基礎 API 實現**:
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from vllm import LLM, SamplingParams

app = FastAPI()
llm = LLM(model="meta-llama/Llama-2-7b-hf")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.8

@app.post("/generate")
async def generate(request: GenerateRequest):
    try:
        sampling_params = SamplingParams(
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        outputs = llm.generate([request.prompt], sampling_params)
        return {"text": outputs[0].outputs[0].text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 3.2 生產部署

**Docker 部署**:
```dockerfile
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

RUN pip install vllm fastapi uvicorn

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Kubernetes 部署**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-serving
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: vllm
        image: vllm:latest
        resources:
          limits:
            nvidia.com/gpu: 1
```

### 3.3 監控系統

**Prometheus Metrics**:
```python
from prometheus_client import Counter, Histogram

request_count = Counter('requests_total', 'Total requests')
request_duration = Histogram('request_duration_seconds', 'Request duration')

@app.post("/generate")
async def generate(request):
    request_count.inc()
    with request_duration.time():
        # 推理邏輯
        return result
```

---

## 延伸學習

**最佳實踐**:
- vLLM 生產部署指南
- FastAPI 異步優化
- Kubernetes GPU 調度

**下一步**: Lab-2.2, Lab-2.3 實作服務與優化

---

**版本**: v1.0  
**制定日期**: 2025-10-09  
**維護者**: LLM 教學專案團隊
