{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-2.1.4: ç›£æ§èˆ‡æ€§èƒ½å„ªåŒ–\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. **å»ºç«‹ä¼æ¥­ç´šç›£æ§é«”ç³»**\n",
    "   - Prometheus æŒ‡æ¨™æ”¶é›†\n",
    "   - Grafana å„€è¡¨æ¿è¨­è¨ˆ\n",
    "   - å‘Šè­¦å’Œé€šçŸ¥ç³»çµ±\n",
    "\n",
    "2. **å¯¦ç¾è‡ªå‹•åŒ–æ€§èƒ½èª¿å„ª**\n",
    "   - å‹•æ…‹è³‡æºèª¿é…\n",
    "   - è² è¼‰å¹³è¡¡å„ªåŒ–\n",
    "   - SLA ç›£æ§å’Œä¿è­‰\n",
    "\n",
    "3. **å»ºæ§‹å®Œæ•´é‹ç¶­æµç¨‹**\n",
    "   - å¥åº·æª¢æŸ¥æ©Ÿåˆ¶\n",
    "   - æ•…éšœæª¢æ¸¬å’Œæ¢å¾©\n",
    "   - å®¹é‡è¦åŠƒå’Œæ“´å±•\n",
    "\n",
    "## ğŸ“‹ ä¼æ¥­æ¡ˆä¾‹èƒŒæ™¯\n",
    "\n",
    "**å ´æ™¯**: Netflix æ¨è–¦ç³»çµ±éœ€è¦ï¼š\n",
    "- 99.99% å¯ç”¨æ€§ (å¹´åœæ©Ÿæ™‚é–“ < 53åˆ†é˜)\n",
    "- P99 å»¶é² < 50ms\n",
    "- æ”¯æ´ 10M+ ä¸¦ç™¼ç”¨æˆ¶\n",
    "- è‡ªå‹•æ•…éšœæ¢å¾©\n",
    "\n",
    "**æŠ€è¡“æŒ‘æˆ°**: å¦‚ä½•è¨­è¨ˆå¯è§€æ¸¬æ€§ç³»çµ±ç¢ºä¿æœå‹™å“è³ªï¼Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prometheus æŒ‡æ¨™æ”¶é›†ç³»çµ±\n",
    "\n",
    "### 1.1 æŒ‡æ¨™å®šç¾©å’Œæ”¶é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import threading\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# æ¨¡æ“¬ Prometheus å®¢æˆ¶ç«¯\n",
    "class MockPrometheusMetrics:\n",
    "    \"\"\"\n",
    "    æ¨¡æ“¬ Prometheus æŒ‡æ¨™æ”¶é›†å™¨\n",
    "    (å¯¦éš›éƒ¨ç½²ä¸­ä½¿ç”¨ prometheus_client)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.counters = defaultdict(float)\n",
    "        self.histograms = defaultdict(list)\n",
    "        self.gauges = defaultdict(float)\n",
    "        self.summaries = defaultdict(list)\n",
    "    \n",
    "    def counter_inc(self, name: str, labels: Dict[str, str] = None, value: float = 1.0):\n",
    "        key = self._make_key(name, labels)\n",
    "        self.counters[key] += value\n",
    "    \n",
    "    def histogram_observe(self, name: str, labels: Dict[str, str] = None, value: float = 0.0):\n",
    "        key = self._make_key(name, labels)\n",
    "        self.histograms[key].append(value)\n",
    "    \n",
    "    def gauge_set(self, name: str, labels: Dict[str, str] = None, value: float = 0.0):\n",
    "        key = self._make_key(name, labels)\n",
    "        self.gauges[key] = value\n",
    "    \n",
    "    def _make_key(self, name: str, labels: Dict[str, str] = None) -> str:\n",
    "        if labels:\n",
    "            label_str = ','.join([f'{k}=\"{v}\"' for k, v in sorted(labels.items())])\n",
    "            return f'{name}{{{label_str}}}'\n",
    "        return name\n",
    "\n",
    "class TritonMetricsCollector:\n",
    "    \"\"\"\n",
    "    Triton å°ˆç”¨æŒ‡æ¨™æ”¶é›†å™¨\n",
    "    \n",
    "    æ”¶é›†æŒ‡æ¨™:\n",
    "    - æ¨ç†å»¶é²å’Œååé‡\n",
    "    - è³‡æºä½¿ç”¨æƒ…æ³\n",
    "    - éŒ¯èª¤ç‡å’ŒæˆåŠŸç‡\n",
    "    - å•†æ¥­æŒ‡æ¨™\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, model_version: str = \"1\"):\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.metrics = MockPrometheusMetrics()\n",
    "        \n",
    "        # åŸºç¤æ¨™ç±¤\n",
    "        self.base_labels = {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version,\n",
    "            \"instance\": \"triton-0\"\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ“Š Triton æŒ‡æ¨™æ”¶é›†å™¨å·²åˆå§‹åŒ–: {model_name} v{model_version}\")\n",
    "    \n",
    "    def record_inference_request(\n",
    "        self, \n",
    "        latency_ms: float,\n",
    "        batch_size: int,\n",
    "        success: bool = True,\n",
    "        backend: str = \"pytorch\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        è¨˜éŒ„æ¨ç†è«‹æ±‚æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        labels = {**self.base_labels, \"backend\": backend}\n",
    "        \n",
    "        # è«‹æ±‚è¨ˆæ•¸\n",
    "        status = \"success\" if success else \"error\"\n",
    "        self.metrics.counter_inc(\n",
    "            \"triton_inference_requests_total\",\n",
    "            {**labels, \"status\": status}\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            # å»¶é²ç›´æ–¹åœ–\n",
    "            self.metrics.histogram_observe(\n",
    "                \"triton_inference_latency_ms\",\n",
    "                labels,\n",
    "                latency_ms\n",
    "            )\n",
    "            \n",
    "            # æ‰¹é‡å¤§å°\n",
    "            self.metrics.histogram_observe(\n",
    "                \"triton_inference_batch_size\",\n",
    "                labels,\n",
    "                batch_size\n",
    "            )\n",
    "            \n",
    "            # ååé‡ (samples/sec)\n",
    "            throughput = batch_size / (latency_ms / 1000)\n",
    "            self.metrics.gauge_set(\n",
    "                \"triton_inference_throughput_samples_per_sec\",\n",
    "                labels,\n",
    "                throughput\n",
    "            )\n",
    "    \n",
    "    def record_resource_usage(\n",
    "        self,\n",
    "        gpu_utilization: float,\n",
    "        gpu_memory_used_gb: float,\n",
    "        gpu_memory_total_gb: float,\n",
    "        cpu_utilization: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        è¨˜éŒ„è³‡æºä½¿ç”¨æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        labels = self.base_labels\n",
    "        \n",
    "        # GPU æŒ‡æ¨™\n",
    "        self.metrics.gauge_set(\n",
    "            \"triton_gpu_utilization_percent\", labels, gpu_utilization\n",
    "        )\n",
    "        self.metrics.gauge_set(\n",
    "            \"triton_gpu_memory_used_gb\", labels, gpu_memory_used_gb\n",
    "        )\n",
    "        self.metrics.gauge_set(\n",
    "            \"triton_gpu_memory_utilization_percent\", \n",
    "            labels, \n",
    "            (gpu_memory_used_gb / gpu_memory_total_gb) * 100\n",
    "        )\n",
    "        \n",
    "        # CPU æŒ‡æ¨™\n",
    "        self.metrics.gauge_set(\n",
    "            \"triton_cpu_utilization_percent\", labels, cpu_utilization\n",
    "        )\n",
    "    \n",
    "    def record_business_metrics(\n",
    "        self,\n",
    "        decisions: List[str],\n",
    "        fraud_detected: int,\n",
    "        false_positives: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        è¨˜éŒ„æ¥­å‹™æŒ‡æ¨™ (ä»¥åè©é¨™ç‚ºä¾‹)\n",
    "        \"\"\"\n",
    "        labels = self.base_labels\n",
    "        \n",
    "        # æ±ºç­–åˆ†ä½ˆ\n",
    "        for decision in ['APPROVE', 'BLOCK', 'REVIEW']:\n",
    "            count = decisions.count(decision)\n",
    "            self.metrics.counter_inc(\n",
    "                \"triton_business_decisions_total\",\n",
    "                {**labels, \"decision\": decision.lower()},\n",
    "                count\n",
    "            )\n",
    "        \n",
    "        # è©é¨™æª¢æ¸¬\n",
    "        self.metrics.counter_inc(\n",
    "            \"triton_fraud_detected_total\", labels, fraud_detected\n",
    "        )\n",
    "        \n",
    "        # èª¤å ±\n",
    "        if false_positives > 0:\n",
    "            self.metrics.counter_inc(\n",
    "                \"triton_false_positives_total\", labels, false_positives\n",
    "            )\n",
    "    \n",
    "    def get_metrics_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ç²å–æŒ‡æ¨™æ‘˜è¦\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": f\"{self.model_name}:{self.model_version}\",\n",
    "            \"counters\": dict(self.metrics.counters),\n",
    "            \"gauges\": dict(self.metrics.gauges),\n",
    "            \"histograms\": {}\n",
    "        }\n",
    "        \n",
    "        # è¨ˆç®—ç›´æ–¹åœ–çµ±è¨ˆ\n",
    "        for key, values in self.metrics.histograms.items():\n",
    "            if values:\n",
    "                summary[\"histograms\"][key] = {\n",
    "                    \"count\": len(values),\n",
    "                    \"sum\": sum(values),\n",
    "                    \"mean\": np.mean(values),\n",
    "                    \"p50\": np.percentile(values, 50),\n",
    "                    \"p90\": np.percentile(values, 90),\n",
    "                    \"p95\": np.percentile(values, 95),\n",
    "                    \"p99\": np.percentile(values, 99)\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# å‰µå»ºæŒ‡æ¨™æ”¶é›†å™¨\n",
    "metrics_collector = TritonMetricsCollector(\n",
    "    model_name=\"netflix_recommendation_v2_prod\",\n",
    "    model_version=\"2\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Prometheus æŒ‡æ¨™æ”¶é›†ç³»çµ±å·²å•Ÿå‹•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ¨¡æ“¬ç›£æ§æ•¸æ“šæ”¶é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def simulate_production_workload(duration_seconds: int = 60):\n",
    "    \"\"\"\n",
    "    æ¨¡æ“¬ç”Ÿç”¢ç’°å¢ƒå·¥ä½œè² è¼‰\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ æ¨¡æ“¬ç”Ÿç”¢ç’°å¢ƒå·¥ä½œè² è¼‰ ({duration_seconds} ç§’)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    request_count = 0\n",
    "    \n",
    "    while time.time() - start_time < duration_seconds:\n",
    "        # æ¨¡æ“¬ä¸åŒæ™‚æ®µçš„è² è¼‰æ¨¡å¼\n",
    "        elapsed = time.time() - start_time\n",
    "        load_factor = 1 + 0.5 * np.sin(elapsed / 10)  # é€±æœŸæ€§è² è¼‰è®ŠåŒ–\n",
    "        \n",
    "        # è«‹æ±‚é »ç‡ (QPS)\n",
    "        base_qps = 50\n",
    "        current_qps = base_qps * load_factor\n",
    "        \n",
    "        # æ¨¡æ“¬è«‹æ±‚\n",
    "        if random.random() < current_qps / 100:  # èª¿æ•´æ¦‚ç‡\n",
    "            # æ‰¹é‡å¤§å°åˆ†ä½ˆ\n",
    "            batch_size = random.choices(\n",
    "                [1, 2, 4, 8, 16], \n",
    "                weights=[0.3, 0.2, 0.2, 0.2, 0.1]\n",
    "            )[0]\n",
    "            \n",
    "            # å»¶é²æ¨¡æ“¬ (æ­£å¸¸åˆ†ä½ˆ + å¶çˆ¾çš„ç•°å¸¸å€¼)\n",
    "            if random.random() < 0.95:  # 95% æ­£å¸¸è«‹æ±‚\n",
    "                base_latency = 15 + 5 * batch_size  # åŸºç¤å»¶é²éš¨æ‰¹é‡å¢åŠ \n",
    "                latency = max(1, np.random.normal(base_latency, 5))\n",
    "                success = random.random() > 0.001  # 99.9% æˆåŠŸç‡\n",
    "            else:  # 5% ç•°å¸¸è«‹æ±‚\n",
    "                latency = np.random.exponential(100)  # é•·å°¾å»¶é²\n",
    "                success = random.random() > 0.1  # 90% æˆåŠŸç‡\n",
    "            \n",
    "            # è¨˜éŒ„æ¨ç†æŒ‡æ¨™\n",
    "            metrics_collector.record_inference_request(\n",
    "                latency_ms=latency,\n",
    "                batch_size=batch_size,\n",
    "                success=success\n",
    "            )\n",
    "            \n",
    "            # æ¨¡æ“¬è³‡æºä½¿ç”¨\n",
    "            if request_count % 10 == 0:  # æ¯10å€‹è«‹æ±‚è¨˜éŒ„ä¸€æ¬¡è³‡æº\n",
    "                gpu_util = min(100, 30 + 20 * load_factor + np.random.normal(0, 5))\n",
    "                gpu_memory = min(8, 2 + 1.5 * load_factor + np.random.normal(0, 0.5))\n",
    "                cpu_util = min(100, 20 + 15 * load_factor + np.random.normal(0, 3))\n",
    "                \n",
    "                metrics_collector.record_resource_usage(\n",
    "                    gpu_utilization=max(0, gpu_util),\n",
    "                    gpu_memory_used_gb=max(0, gpu_memory),\n",
    "                    gpu_memory_total_gb=8.0,\n",
    "                    cpu_utilization=max(0, cpu_util)\n",
    "                )\n",
    "            \n",
    "            # æ¨¡æ“¬æ¥­å‹™æ±ºç­–\n",
    "            if success and request_count % 5 == 0:\n",
    "                decisions = random.choices(\n",
    "                    ['APPROVE', 'BLOCK', 'REVIEW'],\n",
    "                    weights=[0.85, 0.10, 0.05],\n",
    "                    k=batch_size\n",
    "                )\n",
    "                \n",
    "                fraud_detected = decisions.count('BLOCK')\n",
    "                \n",
    "                metrics_collector.record_business_metrics(\n",
    "                    decisions=decisions,\n",
    "                    fraud_detected=fraud_detected\n",
    "                )\n",
    "            \n",
    "            request_count += 1\n",
    "        \n",
    "        time.sleep(0.1)  # æ§åˆ¶æ¨¡æ“¬é€Ÿåº¦\n",
    "    \n",
    "    print(f\"âœ… æ¨¡æ“¬å®Œæˆï¼Œç¸½è¨ˆè™•ç† {request_count} å€‹è«‹æ±‚\")\n",
    "    return request_count\n",
    "\n",
    "# åŸ·è¡Œå·¥ä½œè² è¼‰æ¨¡æ“¬\n",
    "total_requests = simulate_production_workload(30)  # 30ç§’æ¨¡æ“¬\n",
    "\n",
    "# ç²å–æŒ‡æ¨™æ‘˜è¦\n",
    "summary = metrics_collector.get_metrics_summary()\n",
    "\n",
    "print(\"\\nğŸ“Š æŒ‡æ¨™æ”¶é›†æ‘˜è¦:\")\n",
    "print(f\"   ğŸ“ˆ ç¸½è«‹æ±‚æ•¸: {total_requests}\")\n",
    "print(f\"   â±ï¸  å¹³å‡å»¶é²: {summary['histograms'].get('triton_inference_latency_ms', {}).get('mean', 0):.2f} ms\")\n",
    "print(f\"   ğŸ“Š P95 å»¶é²: {summary['histograms'].get('triton_inference_latency_ms', {}).get('p95', 0):.2f} ms\")\n",
    "print(f\"   ğŸš€ å¹³å‡æ‰¹é‡: {summary['histograms'].get('triton_inference_batch_size', {}).get('mean', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SLA ç›£æ§å’Œå‘Šè­¦ç³»çµ±\n",
    "\n",
    "### 2.1 SLA å®šç¾©å’Œç›£æ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SLATarget:\n",
    "    \"\"\"\n",
    "    SLA ç›®æ¨™å®šç¾©\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    target_value: float\n",
    "    operator: str  # '<', '>', '<=', '>=', '=='\n",
    "    measurement_window: int  # ç§’\n",
    "    severity: str  # 'critical', 'warning', 'info'\n",
    "\n",
    "class SLAMonitor:\n",
    "    \"\"\"\n",
    "    SLA ç›£æ§å’Œå‘Šè­¦ç³»çµ±\n",
    "    \n",
    "    ç›£æ§æŒ‡æ¨™:\n",
    "    - å¯ç”¨æ€§ (Availability)\n",
    "    - å»¶é² (Latency)\n",
    "    - ååé‡ (Throughput)\n",
    "    - éŒ¯èª¤ç‡ (Error Rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str):\n",
    "        self.service_name = service_name\n",
    "        self.sla_targets = self._define_sla_targets()\n",
    "        self.metrics_history = defaultdict(list)\n",
    "        self.alerts = []\n",
    "        \n",
    "        print(f\"ğŸ¯ SLA ç›£æ§ç³»çµ±å·²å•Ÿå‹•: {service_name}\")\n",
    "        self._print_sla_targets()\n",
    "    \n",
    "    def _define_sla_targets(self) -> List[SLATarget]:\n",
    "        \"\"\"\n",
    "        å®šç¾©ä¼æ¥­ç´š SLA ç›®æ¨™\n",
    "        \"\"\"\n",
    "        return [\n",
    "            SLATarget(\n",
    "                name=\"availability\",\n",
    "                description=\"æœå‹™å¯ç”¨æ€§\",\n",
    "                target_value=99.99,  # 99.99%\n",
    "                operator=\">=\",\n",
    "                measurement_window=300,  # 5åˆ†é˜\n",
    "                severity=\"critical\"\n",
    "            ),\n",
    "            SLATarget(\n",
    "                name=\"latency_p95\",\n",
    "                description=\"P95 å»¶é²\",\n",
    "                target_value=50.0,  # 50ms\n",
    "                operator=\"<=\",\n",
    "                measurement_window=60,  # 1åˆ†é˜\n",
    "                severity=\"warning\"\n",
    "            ),\n",
    "            SLATarget(\n",
    "                name=\"latency_p99\",\n",
    "                description=\"P99 å»¶é²\",\n",
    "                target_value=100.0,  # 100ms\n",
    "                operator=\"<=\",\n",
    "                measurement_window=60,\n",
    "                severity=\"critical\"\n",
    "            ),\n",
    "            SLATarget(\n",
    "                name=\"error_rate\",\n",
    "                description=\"éŒ¯èª¤ç‡\",\n",
    "                target_value=0.1,  # 0.1%\n",
    "                operator=\"<=\",\n",
    "                measurement_window=300,\n",
    "                severity=\"critical\"\n",
    "            ),\n",
    "            SLATarget(\n",
    "                name=\"throughput\",\n",
    "                description=\"æœ€å°ååé‡\",\n",
    "                target_value=100.0,  # 100 QPS\n",
    "                operator=\">=\",\n",
    "                measurement_window=60,\n",
    "                severity=\"warning\"\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def _print_sla_targets(self):\n",
    "        \"\"\"\n",
    "        æ‰“å° SLA ç›®æ¨™\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“‹ SLA ç›®æ¨™å®šç¾©:\")\n",
    "        for target in self.sla_targets:\n",
    "            severity_icon = {\n",
    "                'critical': 'ğŸ”´',\n",
    "                'warning': 'ğŸŸ¡', \n",
    "                'info': 'ğŸ”µ'\n",
    "            }[target.severity]\n",
    "            \n",
    "            print(f\"   {severity_icon} {target.description}: {target.operator} {target.target_value}\")\n",
    "            print(f\"      æ¸¬é‡çª—å£: {target.measurement_window}ç§’, ç´šåˆ¥: {target.severity}\")\n",
    "    \n",
    "    def record_metrics(\n",
    "        self,\n",
    "        timestamp: float,\n",
    "        latency_ms: float,\n",
    "        success: bool,\n",
    "        throughput_qps: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        è¨˜éŒ„æŒ‡æ¨™ç”¨æ–¼ SLA è¨ˆç®—\n",
    "        \"\"\"\n",
    "        metric_point = {\n",
    "            'timestamp': timestamp,\n",
    "            'latency_ms': latency_ms,\n",
    "            'success': success,\n",
    "            'throughput_qps': throughput_qps\n",
    "        }\n",
    "        \n",
    "        self.metrics_history['points'].append(metric_point)\n",
    "        \n",
    "        # ä¿æŒæœ€è¿‘1å°æ™‚çš„æ•¸æ“š\n",
    "        cutoff_time = timestamp - 3600\n",
    "        self.metrics_history['points'] = [\n",
    "            p for p in self.metrics_history['points']\n",
    "            if p['timestamp'] > cutoff_time\n",
    "        ]\n",
    "    \n",
    "    def calculate_sla_metrics(self, window_seconds: int = 300) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æŒ‡å®šæ™‚é–“çª—å£å…§çš„ SLA æŒ‡æ¨™\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        cutoff_time = current_time - window_seconds\n",
    "        \n",
    "        # ç²å–çª—å£å…§çš„æ•¸æ“š\n",
    "        window_data = [\n",
    "            p for p in self.metrics_history['points']\n",
    "            if p['timestamp'] > cutoff_time\n",
    "        ]\n",
    "        \n",
    "        if not window_data:\n",
    "            return {}\n",
    "        \n",
    "        # è¨ˆç®—æŒ‡æ¨™\n",
    "        total_requests = len(window_data)\n",
    "        successful_requests = sum(1 for p in window_data if p['success'])\n",
    "        latencies = [p['latency_ms'] for p in window_data if p['success']]\n",
    "        throughputs = [p['throughput_qps'] for p in window_data]\n",
    "        \n",
    "        metrics = {\n",
    "            'availability': (successful_requests / total_requests) * 100 if total_requests > 0 else 0,\n",
    "            'error_rate': ((total_requests - successful_requests) / total_requests) * 100 if total_requests > 0 else 0,\n",
    "            'throughput': np.mean(throughputs) if throughputs else 0\n",
    "        }\n",
    "        \n",
    "        if latencies:\n",
    "            metrics.update({\n",
    "                'latency_p50': np.percentile(latencies, 50),\n",
    "                'latency_p95': np.percentile(latencies, 95),\n",
    "                'latency_p99': np.percentile(latencies, 99),\n",
    "                'latency_max': np.max(latencies)\n",
    "            })\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def check_sla_violations(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        æª¢æŸ¥ SLA é•åæƒ…æ³\n",
    "        \"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        for target in self.sla_targets:\n",
    "            metrics = self.calculate_sla_metrics(target.measurement_window)\n",
    "            \n",
    "            if target.name not in metrics:\n",
    "                continue\n",
    "            \n",
    "            current_value = metrics[target.name]\n",
    "            target_value = target.target_value\n",
    "            \n",
    "            violated = False\n",
    "            if target.operator == '<=' and current_value > target_value:\n",
    "                violated = True\n",
    "            elif target.operator == '>=' and current_value < target_value:\n",
    "                violated = True\n",
    "            elif target.operator == '<' and current_value >= target_value:\n",
    "                violated = True\n",
    "            elif target.operator == '>' and current_value <= target_value:\n",
    "                violated = True\n",
    "            elif target.operator == '==' and abs(current_value - target_value) > 0.01:\n",
    "                violated = True\n",
    "            \n",
    "            if violated:\n",
    "                violation = {\n",
    "                    'timestamp': time.time(),\n",
    "                    'sla_name': target.name,\n",
    "                    'description': target.description,\n",
    "                    'target_value': target_value,\n",
    "                    'current_value': current_value,\n",
    "                    'operator': target.operator,\n",
    "                    'severity': target.severity,\n",
    "                    'window_seconds': target.measurement_window\n",
    "                }\n",
    "                violations.append(violation)\n",
    "        \n",
    "        return violations\n",
    "    \n",
    "    def generate_sla_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆ SLA å ±å‘Š\n",
    "        \"\"\"\n",
    "        current_metrics = self.calculate_sla_metrics(300)  # 5åˆ†é˜çª—å£\n",
    "        violations = self.check_sla_violations()\n",
    "        \n",
    "        report = {\n",
    "            'service_name': self.service_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'current_metrics': current_metrics,\n",
    "            'sla_targets': [\n",
    "                {\n",
    "                    'name': target.name,\n",
    "                    'description': target.description,\n",
    "                    'target': f\"{target.operator} {target.target_value}\",\n",
    "                    'current': current_metrics.get(target.name, 'N/A'),\n",
    "                    'status': 'PASS' if target.name not in [v['sla_name'] for v in violations] else 'FAIL'\n",
    "                }\n",
    "                for target in self.sla_targets\n",
    "            ],\n",
    "            'violations': violations,\n",
    "            'overall_status': 'HEALTHY' if not violations else 'DEGRADED'\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# åˆå§‹åŒ– SLA ç›£æ§\n",
    "sla_monitor = SLAMonitor(\"netflix-recommendation-service\")\n",
    "\n",
    "# æ¨¡æ“¬æŒ‡æ¨™æ”¶é›†\n",
    "print(\"\\nğŸ”„ æ¨¡æ“¬ SLA æŒ‡æ¨™æ”¶é›†...\")\n",
    "current_time = time.time()\n",
    "\n",
    "# æ¨¡æ“¬ä¸åŒæ€§èƒ½å ´æ™¯\n",
    "for i in range(100):\n",
    "    # æ™‚é–“é€²å±•\n",
    "    timestamp = current_time + i * 3  # æ¯3ç§’ä¸€å€‹æ•¸æ“šé»\n",
    "    \n",
    "    # æ¨¡æ“¬ä¸åŒéšæ®µçš„æ€§èƒ½\n",
    "    if i < 30:  # æ­£å¸¸éšæ®µ\n",
    "        latency = np.random.normal(25, 5)\n",
    "        success_rate = 0.999\n",
    "        throughput = np.random.normal(150, 20)\n",
    "    elif i < 60:  # æ€§èƒ½é€€åŒ–éšæ®µ\n",
    "        latency = np.random.normal(45, 15)\n",
    "        success_rate = 0.995\n",
    "        throughput = np.random.normal(120, 25)\n",
    "    else:  # æ¢å¾©éšæ®µ\n",
    "        latency = np.random.normal(30, 8)\n",
    "        success_rate = 0.998\n",
    "        throughput = np.random.normal(140, 15)\n",
    "    \n",
    "    success = np.random.random() < success_rate\n",
    "    \n",
    "    sla_monitor.record_metrics(\n",
    "        timestamp=timestamp,\n",
    "        latency_ms=max(1, latency),\n",
    "        success=success,\n",
    "        throughput_qps=max(0, throughput)\n",
    "    )\n",
    "\n",
    "print(\"âœ… SLA æŒ‡æ¨™æ”¶é›†å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SLA å ±å‘Šå’Œå‘Šè­¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ SLA å ±å‘Š\n",
    "sla_report = sla_monitor.generate_sla_report()\n",
    "\n",
    "print(\"ğŸ“‹ SLA ç›£æ§å ±å‘Š\")\n",
    "print(\"â•\" * 60)\n",
    "print(f\"ğŸ·ï¸  æœå‹™: {sla_report['service_name']}\")\n",
    "print(f\"ğŸ• æ™‚é–“: {sla_report['timestamp']}\")\n",
    "print(f\"ğŸ“Š æ•´é«”ç‹€æ…‹: {sla_report['overall_status']}\")\n",
    "print()\n",
    "\n",
    "# ç•¶å‰æŒ‡æ¨™\n",
    "print(\"ğŸ“ˆ ç•¶å‰æ€§èƒ½æŒ‡æ¨™ (5åˆ†é˜çª—å£):\")\n",
    "current_metrics = sla_report['current_metrics']\n",
    "for metric, value in current_metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        if 'latency' in metric:\n",
    "            print(f\"   â±ï¸  {metric}: {value:.2f} ms\")\n",
    "        elif 'rate' in metric or 'availability' in metric:\n",
    "            print(f\"   ğŸ“Š {metric}: {value:.2f}%\")\n",
    "        elif 'throughput' in metric:\n",
    "            print(f\"   ğŸš€ {metric}: {value:.1f} QPS\")\n",
    "        else:\n",
    "            print(f\"   ğŸ“ˆ {metric}: {value:.2f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# SLA ç›®æ¨™æª¢æŸ¥\n",
    "print(\"ğŸ¯ SLA ç›®æ¨™æª¢æŸ¥:\")\n",
    "for target in sla_report['sla_targets']:\n",
    "    status_icon = 'âœ…' if target['status'] == 'PASS' else 'âŒ'\n",
    "    current_val = target['current']\n",
    "    \n",
    "    if current_val != 'N/A':\n",
    "        if isinstance(current_val, (int, float)):\n",
    "            current_str = f\"{current_val:.2f}\"\n",
    "        else:\n",
    "            current_str = str(current_val)\n",
    "    else:\n",
    "        current_str = 'N/A'\n",
    "    \n",
    "    print(f\"   {status_icon} {target['description']}: {current_str} (ç›®æ¨™: {target['target']})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# é•åæƒ…æ³\n",
    "if sla_report['violations']:\n",
    "    print(\"ğŸš¨ SLA é•åå‘Šè­¦:\")\n",
    "    for violation in sla_report['violations']:\n",
    "        severity_icon = {\n",
    "            'critical': 'ğŸ”´',\n",
    "            'warning': 'ğŸŸ¡',\n",
    "            'info': 'ğŸ”µ'\n",
    "        }[violation['severity']]\n",
    "        \n",
    "        print(f\"   {severity_icon} {violation['description']}:\")\n",
    "        print(f\"      ç›®æ¨™: {violation['operator']} {violation['target_value']}\")\n",
    "        print(f\"      ç•¶å‰: {violation['current_value']:.2f}\")\n",
    "        print(f\"      ç´šåˆ¥: {violation['severity'].upper()}\")\n",
    "        print(f\"      æ™‚é–“çª—å£: {violation['window_seconds']}ç§’\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âœ… æ‰€æœ‰ SLA ç›®æ¨™å‡é”æˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è‡ªå‹•åŒ–æ€§èƒ½èª¿å„ªç³»çµ±\n",
    "\n",
    "### 3.1 è‡ªé©æ‡‰è³‡æºèª¿é…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoScalingController:\n",
    "    \"\"\"\n",
    "    è‡ªå‹•åŒ–æ€§èƒ½èª¿å„ªæ§åˆ¶å™¨\n",
    "    \n",
    "    åŠŸèƒ½:\n",
    "    - å‹•æ…‹å¯¦ä¾‹èª¿æ•´\n",
    "    - è² è¼‰å¹³è¡¡å„ªåŒ–\n",
    "    - è³‡æºä½¿ç”¨å„ªåŒ–\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str):\n",
    "        self.service_name = service_name\n",
    "        self.current_instances = 2\n",
    "        self.min_instances = 1\n",
    "        self.max_instances = 10\n",
    "        \n",
    "        # èª¿æ•´ç­–ç•¥åƒæ•¸\n",
    "        self.scale_up_threshold = {\n",
    "            'cpu_utilization': 70,  # %\n",
    "            'memory_utilization': 80,  # %\n",
    "            'latency_p95': 50,  # ms\n",
    "            'queue_length': 10  # è«‹æ±‚æ•¸\n",
    "        }\n",
    "        \n",
    "        self.scale_down_threshold = {\n",
    "            'cpu_utilization': 30,  # %\n",
    "            'memory_utilization': 40,  # %\n",
    "            'latency_p95': 20,  # ms\n",
    "            'queue_length': 2  # è«‹æ±‚æ•¸\n",
    "        }\n",
    "        \n",
    "        self.scaling_history = []\n",
    "        self.last_scaling_time = 0\n",
    "        self.cooldown_period = 300  # 5åˆ†é˜å†·å»æœŸ\n",
    "        \n",
    "        print(f\"ğŸ›ï¸  è‡ªå‹•èª¿æ•´æ§åˆ¶å™¨å·²å•Ÿå‹•: {service_name}\")\n",
    "        print(f\"   ğŸ“Š ç•¶å‰å¯¦ä¾‹æ•¸: {self.current_instances}\")\n",
    "        print(f\"   ğŸ“ˆ å¯¦ä¾‹ç¯„åœ: {self.min_instances}-{self.max_instances}\")\n",
    "    \n",
    "    def analyze_metrics(self, metrics: Dict[str, float]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        åˆ†æç•¶å‰æŒ‡æ¨™ä¸¦ç”Ÿæˆèª¿æ•´å»ºè­°\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # æª¢æŸ¥å†·å»æœŸ\n",
    "        if current_time - self.last_scaling_time < self.cooldown_period:\n",
    "            return {\n",
    "                'action': 'wait',\n",
    "                'reason': f'å†·å»æœŸå…§ ({self.cooldown_period - (current_time - self.last_scaling_time):.0f}ç§’)',\n",
    "                'current_instances': self.current_instances\n",
    "            }\n",
    "        \n",
    "        # æª¢æŸ¥æ“´å®¹æ¢ä»¶\n",
    "        scale_up_signals = []\n",
    "        scale_down_signals = []\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            if metric in self.scale_up_threshold:\n",
    "                threshold = self.scale_up_threshold[metric]\n",
    "                if metric in ['cpu_utilization', 'memory_utilization', 'latency_p95', 'queue_length']:\n",
    "                    if value > threshold:\n",
    "                        scale_up_signals.append(f\"{metric}: {value:.1f} > {threshold}\")\n",
    "            \n",
    "            if metric in self.scale_down_threshold:\n",
    "                threshold = self.scale_down_threshold[metric]\n",
    "                if metric in ['cpu_utilization', 'memory_utilization', 'latency_p95', 'queue_length']:\n",
    "                    if value < threshold:\n",
    "                        scale_down_signals.append(f\"{metric}: {value:.1f} < {threshold}\")\n",
    "        \n",
    "        # æ±ºç­–é‚è¼¯\n",
    "        if len(scale_up_signals) >= 2 and self.current_instances < self.max_instances:\n",
    "            return {\n",
    "                'action': 'scale_up',\n",
    "                'reason': f'å¤šå€‹æŒ‡æ¨™è§¸ç™¼æ“´å®¹: {scale_up_signals}',\n",
    "                'target_instances': min(self.current_instances + 1, self.max_instances),\n",
    "                'current_instances': self.current_instances\n",
    "            }\n",
    "        elif len(scale_down_signals) >= 3 and self.current_instances > self.min_instances:\n",
    "            return {\n",
    "                'action': 'scale_down',\n",
    "                'reason': f'å¤šå€‹æŒ‡æ¨™æ”¯æŒç¸®å®¹: {scale_down_signals}',\n",
    "                'target_instances': max(self.current_instances - 1, self.min_instances),\n",
    "                'current_instances': self.current_instances\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'action': 'maintain',\n",
    "                'reason': f'æŒ‡æ¨™æ­£å¸¸ï¼Œç¶­æŒç•¶å‰å¯¦ä¾‹æ•¸',\n",
    "                'current_instances': self.current_instances\n",
    "            }\n",
    "    \n",
    "    def execute_scaling(self, decision: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œèª¿æ•´æ“ä½œ\n",
    "        \"\"\"\n",
    "        action = decision['action']\n",
    "        \n",
    "        if action == 'scale_up':\n",
    "            old_instances = self.current_instances\n",
    "            self.current_instances = decision['target_instances']\n",
    "            self.last_scaling_time = time.time()\n",
    "            \n",
    "            scaling_event = {\n",
    "                'timestamp': time.time(),\n",
    "                'action': 'scale_up',\n",
    "                'from_instances': old_instances,\n",
    "                'to_instances': self.current_instances,\n",
    "                'reason': decision['reason']\n",
    "            }\n",
    "            self.scaling_history.append(scaling_event)\n",
    "            \n",
    "            print(f\"ğŸ“ˆ åŸ·è¡Œæ“´å®¹: {old_instances} â†’ {self.current_instances} å¯¦ä¾‹\")\n",
    "            print(f\"   ğŸ’¡ åŸå› : {decision['reason']}\")\n",
    "            \n",
    "            # æ¨¡æ“¬æ“´å®¹éç¨‹\n",
    "            print(f\"   ğŸ”„ å•Ÿå‹•æ–°å¯¦ä¾‹...\")\n",
    "            time.sleep(1)\n",
    "            print(f\"   âœ… æ“´å®¹å®Œæˆ\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        elif action == 'scale_down':\n",
    "            old_instances = self.current_instances\n",
    "            self.current_instances = decision['target_instances']\n",
    "            self.last_scaling_time = time.time()\n",
    "            \n",
    "            scaling_event = {\n",
    "                'timestamp': time.time(),\n",
    "                'action': 'scale_down',\n",
    "                'from_instances': old_instances,\n",
    "                'to_instances': self.current_instances,\n",
    "                'reason': decision['reason']\n",
    "            }\n",
    "            self.scaling_history.append(scaling_event)\n",
    "            \n",
    "            print(f\"ğŸ“‰ åŸ·è¡Œç¸®å®¹: {old_instances} â†’ {self.current_instances} å¯¦ä¾‹\")\n",
    "            print(f\"   ğŸ’¡ åŸå› : {decision['reason']}\")\n",
    "            \n",
    "            # æ¨¡æ“¬ç¸®å®¹éç¨‹\n",
    "            print(f\"   ğŸ”„ å„ªé›…é—œé–‰å¯¦ä¾‹...\")\n",
    "            time.sleep(1)\n",
    "            print(f\"   âœ… ç¸®å®¹å®Œæˆ\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_scaling_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ç²å–èª¿æ•´æ­·å²å ±å‘Š\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'service_name': self.service_name,\n",
    "            'current_instances': self.current_instances,\n",
    "            'instance_range': f\"{self.min_instances}-{self.max_instances}\",\n",
    "            'scaling_history': self.scaling_history[-10:],  # æœ€è¿‘10æ¬¡\n",
    "            'total_scaling_events': len(self.scaling_history)\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–è‡ªå‹•èª¿æ•´æ§åˆ¶å™¨\n",
    "autoscaling = AutoScalingController(\"netflix-recommendation-service\")\n",
    "\n",
    "# æ¨¡æ“¬è‡ªå‹•èª¿æ•´å ´æ™¯\n",
    "print(\"\\nğŸ›ï¸  æ¨¡æ“¬è‡ªå‹•èª¿æ•´å ´æ™¯...\")\n",
    "\n",
    "# å ´æ™¯1: é«˜è² è¼‰è§¸ç™¼æ“´å®¹\n",
    "print(\"\\nğŸ“ˆ å ´æ™¯1: é«˜è² è¼‰æœŸé–“\")\n",
    "high_load_metrics = {\n",
    "    'cpu_utilization': 85.0,\n",
    "    'memory_utilization': 78.0,\n",
    "    'latency_p95': 65.0,\n",
    "    'queue_length': 15\n",
    "}\n",
    "\n",
    "decision = autoscaling.analyze_metrics(high_load_metrics)\n",
    "print(f\"ğŸ“Š åˆ†æçµæœ: {decision['action']} - {decision['reason']}\")\n",
    "autoscaling.execute_scaling(decision)\n",
    "\n",
    "# å ´æ™¯2: æŒçºŒé«˜è² è¼‰\n",
    "print(\"\\nğŸ“ˆ å ´æ™¯2: æŒçºŒé«˜è² è¼‰\")\n",
    "time.sleep(2)  # æ¨¡æ“¬æ™‚é–“ç¶“é\n",
    "autoscaling.last_scaling_time -= 310  # ç¹éå†·å»æœŸ\n",
    "\n",
    "extreme_load_metrics = {\n",
    "    'cpu_utilization': 92.0,\n",
    "    'memory_utilization': 88.0,\n",
    "    'latency_p95': 120.0,\n",
    "    'queue_length': 25\n",
    "}\n",
    "\n",
    "decision = autoscaling.analyze_metrics(extreme_load_metrics)\n",
    "print(f\"ğŸ“Š åˆ†æçµæœ: {decision['action']} - {decision['reason']}\")\n",
    "autoscaling.execute_scaling(decision)\n",
    "\n",
    "# å ´æ™¯3: è² è¼‰é™ä½è§¸ç™¼ç¸®å®¹\n",
    "print(\"\\nğŸ“‰ å ´æ™¯3: è² è¼‰é™ä½æœŸé–“\")\n",
    "time.sleep(2)\n",
    "autoscaling.last_scaling_time -= 310\n",
    "\n",
    "low_load_metrics = {\n",
    "    'cpu_utilization': 25.0,\n",
    "    'memory_utilization': 35.0,\n",
    "    'latency_p95': 15.0,\n",
    "    'queue_length': 1\n",
    "}\n",
    "\n",
    "decision = autoscaling.analyze_metrics(low_load_metrics)\n",
    "print(f\"ğŸ“Š åˆ†æçµæœ: {decision['action']} - {decision['reason']}\")\n",
    "autoscaling.execute_scaling(decision)\n",
    "\n",
    "# ç²å–èª¿æ•´å ±å‘Š\n",
    "scaling_report = autoscaling.get_scaling_report()\n",
    "\n",
    "print(\"\\nğŸ“‹ è‡ªå‹•èª¿æ•´æ­·å²å ±å‘Š:\")\n",
    "print(f\"   ğŸ·ï¸  æœå‹™: {scaling_report['service_name']}\")\n",
    "print(f\"   ğŸ“Š ç•¶å‰å¯¦ä¾‹: {scaling_report['current_instances']}\")\n",
    "print(f\"   ğŸ“ˆ å¯¦ä¾‹ç¯„åœ: {scaling_report['instance_range']}\")\n",
    "print(f\"   ğŸ”„ èª¿æ•´æ¬¡æ•¸: {scaling_report['total_scaling_events']}\")\n",
    "\n",
    "print(\"\\nğŸ“ èª¿æ•´æ­·å²:\")\n",
    "for event in scaling_report['scaling_history']:\n",
    "    action_icon = 'ğŸ“ˆ' if event['action'] == 'scale_up' else 'ğŸ“‰'\n",
    "    timestamp = datetime.fromtimestamp(event['timestamp']).strftime('%H:%M:%S')\n",
    "    print(f\"   {action_icon} {timestamp}: {event['from_instances']} â†’ {event['to_instances']} ({event['action']})\")\n",
    "    print(f\"      ç†ç”±: {event['reason'][:80]}...\" if len(event['reason']) > 80 else f\"      ç†ç”±: {event['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å¥åº·æª¢æŸ¥å’Œæ•…éšœæ¢å¾©\n",
    "\n",
    "### 4.1 å¤šå±¤æ¬¡å¥åº·æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Callable\n",
    "\n",
    "class HealthStatus(Enum):\n",
    "    HEALTHY = \"healthy\"\n",
    "    DEGRADED = \"degraded\"\n",
    "    UNHEALTHY = \"unhealthy\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class HealthCheck:\n",
    "    name: str\n",
    "    description: str\n",
    "    check_function: Callable\n",
    "    timeout_seconds: int\n",
    "    critical: bool  # æ˜¯å¦ç‚ºé—œéµæª¢æŸ¥\n",
    "    interval_seconds: int\n",
    "\n",
    "class ComprehensiveHealthMonitor:\n",
    "    \"\"\"\n",
    "    ç¶œåˆå¥åº·ç›£æ§ç³»çµ±\n",
    "    \n",
    "    å¤šå±¤æ¬¡æª¢æŸ¥:\n",
    "    - åŸºç¤è¨­æ–½å±¤ (Infrastructure)\n",
    "    - æ‡‰ç”¨å±¤ (Application)\n",
    "    - æ¥­å‹™å±¤ (Business)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str):\n",
    "        self.service_name = service_name\n",
    "        self.health_checks = self._setup_health_checks()\n",
    "        self.health_history = defaultdict(list)\n",
    "        self.alert_thresholds = {\n",
    "            'consecutive_failures': 3,\n",
    "            'failure_rate_threshold': 0.2  # 20%\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ¥ ç¶œåˆå¥åº·ç›£æ§ç³»çµ±å·²å•Ÿå‹•: {service_name}\")\n",
    "        print(f\"   ğŸ“‹ å¥åº·æª¢æŸ¥é …ç›®: {len(self.health_checks)}\")\n",
    "    \n",
    "    def _setup_health_checks(self) -> List[HealthCheck]:\n",
    "        \"\"\"\n",
    "        è¨­ç½®å¥åº·æª¢æŸ¥é …ç›®\n",
    "        \"\"\"\n",
    "        return [\n",
    "            # åŸºç¤è¨­æ–½å±¤æª¢æŸ¥\n",
    "            HealthCheck(\n",
    "                name=\"gpu_availability\",\n",
    "                description=\"GPU å¯ç”¨æ€§æª¢æŸ¥\",\n",
    "                check_function=self._check_gpu_availability,\n",
    "                timeout_seconds=5,\n",
    "                critical=True,\n",
    "                interval_seconds=30\n",
    "            ),\n",
    "            HealthCheck(\n",
    "                name=\"memory_usage\",\n",
    "                description=\"è¨˜æ†¶é«”ä½¿ç”¨ç‡æª¢æŸ¥\",\n",
    "                check_function=self._check_memory_usage,\n",
    "                timeout_seconds=3,\n",
    "                critical=True,\n",
    "                interval_seconds=15\n",
    "            ),\n",
    "            HealthCheck(\n",
    "                name=\"disk_space\",\n",
    "                description=\"ç£ç¢Ÿç©ºé–“æª¢æŸ¥\",\n",
    "                check_function=self._check_disk_space,\n",
    "                timeout_seconds=5,\n",
    "                critical=False,\n",
    "                interval_seconds=60\n",
    "            ),\n",
    "            \n",
    "            # æ‡‰ç”¨å±¤æª¢æŸ¥\n",
    "            HealthCheck(\n",
    "                name=\"model_inference\",\n",
    "                description=\"æ¨¡å‹æ¨ç†åŠŸèƒ½æª¢æŸ¥\",\n",
    "                check_function=self._check_model_inference,\n",
    "                timeout_seconds=10,\n",
    "                critical=True,\n",
    "                interval_seconds=30\n",
    "            ),\n",
    "            HealthCheck(\n",
    "                name=\"api_endpoint\",\n",
    "                description=\"API ç«¯é»å¯é”æ€§æª¢æŸ¥\",\n",
    "                check_function=self._check_api_endpoint,\n",
    "                timeout_seconds=5,\n",
    "                critical=True,\n",
    "                interval_seconds=15\n",
    "            ),\n",
    "            \n",
    "            # æ¥­å‹™å±¤æª¢æŸ¥\n",
    "            HealthCheck(\n",
    "                name=\"response_quality\",\n",
    "                description=\"éŸ¿æ‡‰å“è³ªæª¢æŸ¥\",\n",
    "                check_function=self._check_response_quality,\n",
    "                timeout_seconds=15,\n",
    "                critical=False,\n",
    "                interval_seconds=120\n",
    "            ),\n",
    "            HealthCheck(\n",
    "                name=\"business_metrics\",\n",
    "                description=\"æ¥­å‹™æŒ‡æ¨™å¥åº·åº¦æª¢æŸ¥\",\n",
    "                check_function=self._check_business_metrics,\n",
    "                timeout_seconds=10,\n",
    "                critical=False,\n",
    "                interval_seconds=180\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def _check_gpu_availability(self) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥ GPU å¯ç”¨æ€§\"\"\"\n",
    "        try:\n",
    "            # æ¨¡æ“¬ GPU æª¢æŸ¥\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_count = torch.cuda.device_count()\n",
    "                current_device = torch.cuda.current_device()\n",
    "                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "                \n",
    "                return {\n",
    "                    'status': HealthStatus.HEALTHY,\n",
    "                    'details': {\n",
    "                        'gpu_count': gpu_count,\n",
    "                        'current_device': current_device,\n",
    "                        'total_memory_gb': gpu_memory\n",
    "                    },\n",
    "                    'message': f'{gpu_count} GPU(s) å¯ç”¨'\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'status': HealthStatus.UNHEALTHY,\n",
    "                    'details': {},\n",
    "                    'message': 'GPU ä¸å¯ç”¨'\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': HealthStatus.UNKNOWN,\n",
    "                'details': {'error': str(e)},\n",
    "                'message': f'GPU æª¢æŸ¥å¤±æ•—: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def _check_memory_usage(self) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨ç‡\"\"\"\n",
    "        try:\n",
    "            # æ¨¡æ“¬è¨˜æ†¶é«”æª¢æŸ¥\n",
    "            if torch.cuda.is_available():\n",
    "                allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "                cached = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "                total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "                \n",
    "                usage_ratio = allocated / total\n",
    "                \n",
    "                if usage_ratio < 0.8:\n",
    "                    status = HealthStatus.HEALTHY\n",
    "                elif usage_ratio < 0.9:\n",
    "                    status = HealthStatus.DEGRADED\n",
    "                else:\n",
    "                    status = HealthStatus.UNHEALTHY\n",
    "                \n",
    "                return {\n",
    "                    'status': status,\n",
    "                    'details': {\n",
    "                        'allocated_gb': allocated,\n",
    "                        'cached_gb': cached,\n",
    "                        'total_gb': total,\n",
    "                        'usage_ratio': usage_ratio\n",
    "                    },\n",
    "                    'message': f'GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡: {usage_ratio:.1%}'\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'status': HealthStatus.HEALTHY,\n",
    "                    'details': {},\n",
    "                    'message': 'CPU æ¨¡å¼ï¼Œè·³é GPU è¨˜æ†¶é«”æª¢æŸ¥'\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': HealthStatus.UNKNOWN,\n",
    "                'details': {'error': str(e)},\n",
    "                'message': f'è¨˜æ†¶é«”æª¢æŸ¥å¤±æ•—: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def _check_disk_space(self) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥ç£ç¢Ÿç©ºé–“\"\"\"\n",
    "        try:\n",
    "            import shutil\n",
    "            total, used, free = shutil.disk_usage(\"/\")\n",
    "            \n",
    "            usage_ratio = used / total\n",
    "            \n",
    "            if usage_ratio < 0.8:\n",
    "                status = HealthStatus.HEALTHY\n",
    "            elif usage_ratio < 0.9:\n",
    "                status = HealthStatus.DEGRADED\n",
    "            else:\n",
    "                status = HealthStatus.UNHEALTHY\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'details': {\n",
    "                    'total_gb': total / (1024**3),\n",
    "                    'used_gb': used / (1024**3),\n",
    "                    'free_gb': free / (1024**3),\n",
    "                    'usage_ratio': usage_ratio\n",
    "                },\n",
    "                'message': f'ç£ç¢Ÿä½¿ç”¨ç‡: {usage_ratio:.1%}'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': HealthStatus.UNKNOWN,\n",
    "                'details': {'error': str(e)},\n",
    "                'message': f'ç£ç¢Ÿæª¢æŸ¥å¤±æ•—: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def _check_model_inference(self) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥æ¨¡å‹æ¨ç†åŠŸèƒ½\"\"\"\n",
    "        try:\n",
    "            # æ¨¡æ“¬æ¨ç†æ¸¬è©¦\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # æ¨¡æ“¬æ¨ç†éç¨‹\n",
    "            time.sleep(0.1)  # æ¨¡æ“¬æ¨ç†æ™‚é–“\n",
    "            \n",
    "            inference_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            if inference_time < 50:\n",
    "                status = HealthStatus.HEALTHY\n",
    "            elif inference_time < 100:\n",
    "                status = HealthStatus.DEGRADED\n",
    "            else:\n",
    "                status = HealthStatus.UNHEALTHY\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'details': {\n",
    "                    'inference_time_ms': inference_time,\n",
    "                    'test_successful': True\n",
    "                },\n",
    "                'message': f'æ¨ç†æ¸¬è©¦æˆåŠŸï¼Œè€—æ™‚ {inference_time:.1f}ms'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': HealthStatus.UNHEALTHY,\n",
    "                'details': {'error': str(e)},\n",
    "                'message': f'æ¨ç†æ¸¬è©¦å¤±æ•—: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def _check_api_endpoint(self) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥ API ç«¯é»å¯é”æ€§\"\"\"\n",
    "        try:\n",
    "            # æ¨¡æ“¬ API å¥åº·æª¢æŸ¥\n",
    "            response_time = np.random.normal(20, 5)  # æ¨¡æ“¬éŸ¿æ‡‰æ™‚é–“\n",
    "            \n",
    "            if response_time < 100:\n",
    "                status = HealthStatus.HEALTHY\n",
    "            elif response_time < 500:\n",
    "                status = HealthStatus.DEGRADED\n",
    "            else:\n",
    "                status = HealthStatus.UNHEALTHY\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'details': {\n",
    "                    'response_time_ms': response_time,\n",
    "                    'endpoint_reachable': True\n",
    "                },\n",
    "                'message': f'API ç«¯é»å¯é”ï¼ŒéŸ¿æ‡‰æ™‚é–“ {response_time:.1f}ms'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': HealthStatus.UNHEALTHY,\n",
    "                'details': {'error': str(e)},\n",
    "                'message': f'API ç«¯é»æª¢æŸ¥å¤±æ•—: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def _check_response_quality(self) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥éŸ¿æ‡‰å“è³ª\"\"\"\n",
    "        try:\n",
    "            # æ¨¡æ“¬å“è³ªæª¢æŸ¥\n",
    "            accuracy = np.random.normal(0.95, 0.02)  # æ¨¡æ“¬æº–ç¢ºç‡\n",
    "            consistency = np.random.normal(0.98, 0.01)  # æ¨¡æ“¬ä¸€è‡´æ€§\n",
    "            \n",
    "            quality_score = (accuracy + consistency) / 2\n",
    "            \n",
    "            if quality_score > 0.95:\n",
    "                status = HealthStatus.HEALTHY\n",
    "            elif quality_score > 0.90:\n",
    "                status = HealthStatus.DEGRADED\n",
    "            else:\n",
    "                status = HealthStatus.UNHEALTHY\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'details': {\n",
    "                    'accuracy': accuracy,\n",
    "                    'consistency': consistency,\n",
    "                    'quality_score': quality_score\n",
    "                },\n",
    "                'message': f'éŸ¿æ‡‰å“è³ª: {quality_score:.1%}'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': HealthStatus.UNKNOWN,\n",
    "                'details': {'error': str(e)},\n",
    "                'message': f'å“è³ªæª¢æŸ¥å¤±æ•—: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def _check_business_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥æ¥­å‹™æŒ‡æ¨™å¥åº·åº¦\"\"\"\n",
    "        try:\n",
    "            # æ¨¡æ“¬æ¥­å‹™æŒ‡æ¨™æª¢æŸ¥\n",
    "            fraud_detection_rate = np.random.normal(0.85, 0.05)\n",
    "            false_positive_rate = np.random.normal(0.03, 0.01)\n",
    "            customer_satisfaction = np.random.normal(0.92, 0.03)\n",
    "            \n",
    "            # ç¶œåˆè©•åˆ†\n",
    "            business_health = (fraud_detection_rate + (1 - false_positive_rate) + customer_satisfaction) / 3\n",
    "            \n",
    "            if business_health > 0.90:\n",
    "                status = HealthStatus.HEALTHY\n",
    "            elif business_health > 0.80:\n",
    "                status = HealthStatus.DEGRADED\n",
    "            else:\n",
    "                status = HealthStatus.UNHEALTHY\n",
    "            \n",
    "            return {\n",
    "                'status': status,\n",
    "                'details': {\n",
    "                    'fraud_detection_rate': fraud_detection_rate,\n",
    "                    'false_positive_rate': false_positive_rate,\n",
    "                    'customer_satisfaction': customer_satisfaction,\n",
    "                    'business_health_score': business_health\n",
    "                },\n",
    "                'message': f'æ¥­å‹™å¥åº·åº¦: {business_health:.1%}'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': HealthStatus.UNKNOWN,\n",
    "                'details': {'error': str(e)},\n",
    "                'message': f'æ¥­å‹™æŒ‡æ¨™æª¢æŸ¥å¤±æ•—: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def run_all_health_checks(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œæ‰€æœ‰å¥åº·æª¢æŸ¥\n",
    "        \"\"\"\n",
    "        print(\"ğŸ¥ åŸ·è¡Œç¶œåˆå¥åº·æª¢æŸ¥...\")\n",
    "        \n",
    "        results = {\n",
    "            'timestamp': time.time(),\n",
    "            'service_name': self.service_name,\n",
    "            'checks': {},\n",
    "            'summary': {\n",
    "                'total_checks': len(self.health_checks),\n",
    "                'healthy': 0,\n",
    "                'degraded': 0,\n",
    "                'unhealthy': 0,\n",
    "                'unknown': 0,\n",
    "                'critical_failures': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for check in self.health_checks:\n",
    "            try:\n",
    "                print(f\"   ğŸ” {check.description}...\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                result = check.check_function()\n",
    "                execution_time = time.time() - start_time\n",
    "                \n",
    "                result['execution_time_ms'] = execution_time * 1000\n",
    "                result['check_name'] = check.name\n",
    "                result['critical'] = check.critical\n",
    "                \n",
    "                results['checks'][check.name] = result\n",
    "                \n",
    "                # æ›´æ–°çµ±è¨ˆ\n",
    "                status = result['status']\n",
    "                if status == HealthStatus.HEALTHY:\n",
    "                    results['summary']['healthy'] += 1\n",
    "                    print(f\"      âœ… å¥åº·\")\n",
    "                elif status == HealthStatus.DEGRADED:\n",
    "                    results['summary']['degraded'] += 1\n",
    "                    print(f\"      ğŸŸ¡ é™ç´š\")\n",
    "                elif status == HealthStatus.UNHEALTHY:\n",
    "                    results['summary']['unhealthy'] += 1\n",
    "                    if check.critical:\n",
    "                        results['summary']['critical_failures'] += 1\n",
    "                    print(f\"      âŒ ä¸å¥åº·\")\n",
    "                else:\n",
    "                    results['summary']['unknown'] += 1\n",
    "                    print(f\"      â“ æœªçŸ¥\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      ğŸ’¥ æª¢æŸ¥åŸ·è¡Œå¤±æ•—: {str(e)}\")\n",
    "                results['checks'][check.name] = {\n",
    "                    'status': HealthStatus.UNKNOWN,\n",
    "                    'details': {'error': str(e)},\n",
    "                    'message': f'æª¢æŸ¥åŸ·è¡Œå¤±æ•—: {str(e)}',\n",
    "                    'critical': check.critical\n",
    "                }\n",
    "                results['summary']['unknown'] += 1\n",
    "        \n",
    "        # è¨ˆç®—æ•´é«”å¥åº·ç‹€æ…‹\n",
    "        if results['summary']['critical_failures'] > 0:\n",
    "            results['overall_status'] = HealthStatus.UNHEALTHY\n",
    "        elif results['summary']['unhealthy'] > 0 or results['summary']['degraded'] > 2:\n",
    "            results['overall_status'] = HealthStatus.DEGRADED\n",
    "        elif results['summary']['degraded'] > 0:\n",
    "            results['overall_status'] = HealthStatus.DEGRADED\n",
    "        else:\n",
    "            results['overall_status'] = HealthStatus.HEALTHY\n",
    "        \n",
    "        return results\n",
    "\n",
    "# å‰µå»ºå¥åº·ç›£æ§ç³»çµ±\n",
    "health_monitor = ComprehensiveHealthMonitor(\"netflix-recommendation-service\")\n",
    "\n",
    "# åŸ·è¡Œå¥åº·æª¢æŸ¥\n",
    "health_results = health_monitor.run_all_health_checks()\n",
    "\n",
    "print(\"\\nğŸ“‹ å¥åº·æª¢æŸ¥çµæœæ‘˜è¦:\")\n",
    "print(f\"   ğŸ·ï¸  æœå‹™: {health_results['service_name']}\")\n",
    "print(f\"   ğŸ“Š æ•´é«”ç‹€æ…‹: {health_results['overall_status'].value.upper()}\")\n",
    "print(f\"   ğŸ“ˆ æª¢æŸ¥çµ±è¨ˆ:\")\n",
    "print(f\"      âœ… å¥åº·: {health_results['summary']['healthy']}\")\n",
    "print(f\"      ğŸŸ¡ é™ç´š: {health_results['summary']['degraded']}\")\n",
    "print(f\"      âŒ ä¸å¥åº·: {health_results['summary']['unhealthy']}\")\n",
    "print(f\"      â“ æœªçŸ¥: {health_results['summary']['unknown']}\")\n",
    "print(f\"      ğŸ”´ é—œéµå¤±æ•—: {health_results['summary']['critical_failures']}\")\n",
    "\n",
    "print(\"\\nğŸ” è©³ç´°æª¢æŸ¥çµæœ:\")\n",
    "for check_name, result in health_results['checks'].items():\n",
    "    status_icon = {\n",
    "        HealthStatus.HEALTHY: 'âœ…',\n",
    "        HealthStatus.DEGRADED: 'ğŸŸ¡',\n",
    "        HealthStatus.UNHEALTHY: 'âŒ',\n",
    "        HealthStatus.UNKNOWN: 'â“'\n",
    "    }[result['status']]\n",
    "    \n",
    "    critical_marker = ' ğŸ”´' if result.get('critical', False) else ''\n",
    "    print(f\"   {status_icon} {check_name}{critical_marker}: {result['message']}\")\n",
    "    print(f\"      åŸ·è¡Œæ™‚é–“: {result.get('execution_time_ms', 0):.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ æœ¬ç« ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒå­¸ç¿’æˆæœ\n",
    "\n",
    "é€šéæœ¬å¯¦é©—å®¤ï¼Œæ‚¨å·²ç¶“æŒæ¡äº†ï¼š\n",
    "\n",
    "1. **ğŸ“Š ä¼æ¥­ç´šç›£æ§é«”ç³»**\n",
    "   - Prometheus æŒ‡æ¨™æ”¶é›†å’Œç®¡ç†\n",
    "   - å¤šç¶­åº¦æ€§èƒ½ç›£æ§\n",
    "   - å¯¦æ™‚å‘Šè­¦å’Œé€šçŸ¥æ©Ÿåˆ¶\n",
    "\n",
    "2. **ğŸ¯ SLA ç›£æ§å’Œä¿è­‰**\n",
    "   - 99.99% å¯ç”¨æ€§ç›£æ§\n",
    "   - P95/P99 å»¶é²è¿½è¹¤\n",
    "   - è‡ªå‹•åŒ– SLA é•åæª¢æ¸¬\n",
    "\n",
    "3. **ğŸ›ï¸  è‡ªå‹•åŒ–é‹ç¶­èƒ½åŠ›**\n",
    "   - æ™ºèƒ½è³‡æºèª¿é…\n",
    "   - å‹•æ…‹æ“´ç¸®å®¹æ±ºç­–\n",
    "   - è² è¼‰é æ¸¬å’Œå„ªåŒ–\n",
    "\n",
    "4. **ğŸ¥ å¥åº·æª¢æŸ¥å’Œæ•…éšœæ¢å¾©**\n",
    "   - å¤šå±¤æ¬¡å¥åº·ç›£æ§\n",
    "   - é é˜²æ€§æ•…éšœæª¢æ¸¬\n",
    "   - è‡ªå‹•åŒ–æ¢å¾©æµç¨‹\n",
    "\n",
    "### ä¼æ¥­ç´šé‹ç¶­æŠ€èƒ½\n",
    "\n",
    "æ‚¨ç¾åœ¨å…·å‚™äº†ï¼š\n",
    "- **Netflix ç´šåˆ¥**çš„å¯è§€æ¸¬æ€§è¨­è¨ˆèƒ½åŠ›\n",
    "- **é‡‘èç´šåˆ¥**çš„ SLA ç›£æ§æŠ€èƒ½\n",
    "- **é›²åŸç”Ÿ**çš„è‡ªå‹•åŒ–é‹ç¶­ç¶“é©—\n",
    "- **ç”Ÿç”¢ç´šåˆ¥**çš„æ•…éšœè™•ç†èƒ½åŠ›\n",
    "\n",
    "### å®Œæ•´ Lab-2.1 å­¸ç¿’æˆæœ\n",
    "\n",
    "å®Œæˆæ•´å€‹ **Lab-2.1: Triton Server Basics** å¾Œï¼Œæ‚¨å·²ç¶“å…·å‚™ï¼š\n",
    "\n",
    "1. **Triton Server å®Œæ•´éƒ¨ç½²èƒ½åŠ›**\n",
    "2. **ä¼æ¥­ç´š Model Repository è¨­è¨ˆ**\n",
    "3. **PyTorch Backend æ·±åº¦å„ªåŒ–**\n",
    "4. **ç”Ÿç”¢ç´šç›£æ§å’Œé‹ç¶­**\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¸ç¿’è·¯å¾‘\n",
    "\n",
    "æº–å‚™é€²å…¥ **Lab-2.2: Multi-Model Management**ï¼š\n",
    "- å¤šæ¨¡å‹çµ±ä¸€ç®¡ç†å¹³å°\n",
    "- A/B æ¸¬è©¦è‡ªå‹•åŒ–\n",
    "- æ¨¡å‹ç”Ÿå‘½é€±æœŸç®¡ç†\n",
    "- ä¼æ¥­ç´šæ¨¡å‹æ²»ç†\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ† æ­å–œï¼æ‚¨å·²ç¶“å®Œæˆäº† Triton Server åŸºç¤çš„ä¼æ¥­ç´šç›£æ§èˆ‡æ€§èƒ½å„ªåŒ–ï¼**\n",
    "\n",
    "**ğŸ“ˆ æŠ€èƒ½æå‡ç¸½çµï¼šå¾åŸºç¤éƒ¨ç½²æå‡åˆ°ä¼æ¥­ç´šé‹ç¶­å°ˆå®¶ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}