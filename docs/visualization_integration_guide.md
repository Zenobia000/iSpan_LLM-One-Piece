# è¦–è¦ºåŒ–æ•´åˆæŒ‡å—
## Visualization Integration Guide for PEFT Labs

**ç‰ˆæœ¬**: v1.0
**æ—¥æœŸ**: 2025-10-09
**ç”¨é€”**: æŒ‡å°å¦‚ä½•åœ¨ PEFT å¯¦é©—å®¤ä¸­æ•´åˆè¦–è¦ºåŒ–å·¥å…·

---

## ğŸ“š æ¦‚è¿°

æœ¬æŒ‡å—èªªæ˜å¦‚ä½•åœ¨ç¾æœ‰çš„ PEFT å¯¦é©—å®¤ notebooks ä¸­æ•´åˆ `common_utils.visualization` å·¥å…·ã€‚

---

## ğŸ¯ æ•´åˆä½ç½®

### 02-Train.ipynb - è¨“ç·´éç¨‹è¦–è¦ºåŒ–

**ä½ç½®**: åœ¨ `trainer.train()` ä¹‹å¾Œï¼ŒFinal Evaluation ä¹‹å¾Œ

**æ·»åŠ çš„ Cell**:

#### Cell 1: Markdown èªªæ˜
```markdown
### Step 5: è¦–è¦ºåŒ–è¨“ç·´çµæœ

ä½¿ç”¨ common_utils æä¾›çš„è¦–è¦ºåŒ–å·¥å…·ä¾†å±•ç¤ºè¨“ç·´éç¨‹ã€‚
```

#### Cell 2: è¦–è¦ºåŒ–ä»£ç¢¼
```python
# å°å…¥è¦–è¦ºåŒ–å·¥å…·
import sys
sys.path.append('../../../..')  # æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
from common_utils.visualization import plot_training_curves, plot_parameter_distribution
from common_utils.training_helpers import print_trainable_parameters

print("=" * 60)
print("è¨“ç·´éç¨‹è¦–è¦ºåŒ–")
print("=" * 60)

# 1. æå–è¨“ç·´æ­·å²
log_history = trainer.state.log_history

# æå–æå¤±å€¼
train_losses = []
eval_losses = []
eval_perplexities = []

for entry in log_history:
    if 'loss' in entry and 'eval_loss' not in entry:
        train_losses.append(entry['loss'])
    if 'eval_loss' in entry:
        eval_losses.append(entry['eval_loss'])
    if 'eval_perplexity' in entry:
        eval_perplexities.append(entry['eval_perplexity'])

# 2. ç¹ªè£½è¨“ç·´æ›²ç·š
history = {
    'train_loss': train_losses,
    'eval_loss': eval_losses,
    'eval_perplexity': eval_perplexities
}

plot_training_curves(
    history,
    title="LoRA è¨“ç·´éç¨‹",
    save_path="./lora-llama2-7b-guanaco/training_curves.png"
)

print("âœ… è¨“ç·´æ›²ç·šå·²ç”Ÿæˆ")

# 3. åƒæ•¸åˆ†ä½ˆè¦–è¦ºåŒ–
model_stats = print_trainable_parameters(peft_model, verbose=False)

plot_parameter_distribution(
    model_stats,
    save_path="./lora-llama2-7b-guanaco/parameter_distribution.png"
)

print("âœ… åƒæ•¸åˆ†ä½ˆåœ–å·²ç”Ÿæˆ")
print("\næ‰€æœ‰è¦–è¦ºåŒ–åœ–è¡¨å·²ä¿å­˜è‡³: ./lora-llama2-7b-guanaco/")
```

---

### 03-Inference.ipynb - æ¨ç†æ€§èƒ½è¦–è¦ºåŒ–

**ä½ç½®**: åœ¨æ¨ç†æ¸¬è©¦ä¹‹å¾Œ

#### Cell: æ¨ç†æ€§èƒ½å°æ¯”
```python
from common_utils.visualization import plot_inference_benchmark

# æ¸¬è©¦æ¨ç†æ€§èƒ½
import time

def benchmark_inference(model, tokenizer, prompt, num_runs=5):
    """ç°¡å–®çš„æ¨ç†æ€§èƒ½æ¸¬è©¦"""
    latencies = []

    for _ in range(num_runs):
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        start = time.time()
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=50)
        latency = (time.time() - start) * 1000  # ms

        latencies.append(latency)

    return {
        'latency_ms': sum(latencies) / len(latencies),
        'throughput': 50 / (sum(latencies) / len(latencies) / 1000)  # tokens/s
    }

# æ¸¬è©¦
test_prompt = "What is machine learning?"

base_result = benchmark_inference(base_model, tokenizer, test_prompt)
lora_result = benchmark_inference(peft_model, tokenizer, test_prompt)

# è¦–è¦ºåŒ–å°æ¯”
benchmark_results = {
    'Base Model': base_result,
    'LoRA Fine-tuned': lora_result
}

plot_inference_benchmark(
    benchmark_results,
    save_path="./lora-llama2-7b-guanaco/inference_benchmark.png"
)

print("âœ… æ¨ç†æ€§èƒ½å°æ¯”åœ–å·²ç”Ÿæˆ")
```

---

### 04-Merge_and_Deploy.ipynb - éƒ¨ç½²å‰æª¢æŸ¥

**ä½ç½®**: åˆä½µå®Œæˆå¾Œ

#### Cell: æ¨¡å‹å¤§å°å°æ¯”
```python
import os
from common_utils.visualization import plt

# çµ±è¨ˆæ¨¡å‹å¤§å°
def get_model_size(path):
    """è¨ˆç®—ç›®éŒ„ä¸­æ‰€æœ‰æ–‡ä»¶çš„ç¸½å¤§å°"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            total_size += os.path.getsize(filepath)
    return total_size / (1024**3)  # GB

adapter_size = get_model_size("./lora-llama2-7b-guanaco")
merged_size = get_model_size("./lora-llama2-7b-guanaco-merged")

# è¦–è¦ºåŒ–
models = ['LoRA Adapter', 'Merged Model', 'Original Model (åƒè€ƒ)']
sizes = [adapter_size, merged_size, 13.5]  # Llama-2-7B FP16 ç´„ 13.5GB
colors = ['#2ecc71', '#3498db', '#95a5a6']

fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(models, sizes, color=colors, alpha=0.8)

ax.set_ylabel('æ¨¡å‹å¤§å° (GB)', fontsize=12, fontweight='bold')
ax.set_title('LoRA æ¨¡å‹å¤§å°å°æ¯”', fontsize=14, fontweight='bold')
ax.grid(axis='y', alpha=0.3)

# æ·»åŠ æ•¸å€¼æ¨™ç±¤
for bar, size in zip(bars, sizes):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
           f'{size:.2f}GB',
           ha='center', va='bottom', fontsize=11, fontweight='bold')

# æ·»åŠ æ•ˆç‡èªªæ˜
ax.text(0.5, -0.15,
       f'LoRA Adapter åƒ…ä½”åŸæ¨¡å‹å¤§å°çš„ {adapter_size/13.5*100:.2f}%',
       transform=ax.transAxes, ha='center', fontsize=10, style='italic')

plt.tight_layout()
plt.savefig("./lora-llama2-7b-guanaco-merged/model_size_comparison.png", dpi=300)
plt.show()

print(f"âœ… LoRA Adapter: {adapter_size:.2f}GB")
print(f"âœ… åˆä½µæ¨¡å‹: {merged_size:.2f}GB")
print(f"âœ… æ•ˆç‡: Adapter åƒ…ä½” {adapter_size/merged_size*100:.1f}%")
```

---

## ğŸ”§ æ¨™æº–åŒ–æ¨¡æ¿

### æ¨¡æ¿ 1: è¨“ç·´è¦–è¦ºåŒ– (æ‰€æœ‰ 02-Train.ipynb)

```python
# ========== åœ¨ trainer.train() ä¹‹å¾Œæ·»åŠ  ==========

# å°å…¥è¦–è¦ºåŒ–å·¥å…·
from common_utils.visualization import plot_training_curves
from common_utils.training_helpers import analyze_training_results

# è¦–è¦ºåŒ–è¨“ç·´éç¨‹
log_history = trainer.state.log_history

# æå–æŒ‡æ¨™
train_loss = [e['loss'] for e in log_history if 'loss' in e and 'eval' not in str(e)]
eval_loss = [e['eval_loss'] for e in log_history if 'eval_loss' in e]

history = {'train_loss': train_loss, 'eval_loss': eval_loss}

# æ·»åŠ å…¶ä»–è©•ä¼°æŒ‡æ¨™
for entry in log_history:
    for key in entry:
        if key.startswith('eval_') and key not in history:
            if key not in history:
                history[key] = []
            history[key].append(entry[key])

# ç¹ªè£½
plot_training_curves(
    history,
    title=f"{METHOD_NAME} è¨“ç·´éç¨‹",  # æ›¿æ› METHOD_NAME
    save_path=f"./{OUTPUT_DIR}/training_curves.png"
)

# åˆ†æçµæœ
results = analyze_training_results(trainer, f"./{OUTPUT_DIR}")
```

### æ¨¡æ¿ 2: åƒæ•¸åˆ†ä½ˆ (æ‰€æœ‰ 02-Train.ipynb æˆ– 01-Setup.ipynb)

```python
from common_utils.visualization import plot_parameter_distribution
from common_utils.training_helpers import print_trainable_parameters

# ç²å–åƒæ•¸çµ±è¨ˆ
model_stats = print_trainable_parameters(model, verbose=True)

# è¦–è¦ºåŒ–
plot_parameter_distribution(
    model_stats,
    save_path=f"./{OUTPUT_DIR}/parameter_distribution.png"
)
```

### æ¨¡æ¿ 3: æ–¹æ³•å°æ¯” (å¯é¸ï¼Œåœ¨å°æ¯”å¯¦é©—ä¸­)

```python
from common_utils.visualization import plot_peft_comparison

results = [
    {'method': 'LoRA r=8', 'trainable_params_%': 0.25, 'performance': 85.2, 'training_time': 120},
    {'method': 'LoRA r=16', 'trainable_params_%': 0.48, 'performance': 86.1, 'training_time': 135},
    {'method': 'LoRA r=32', 'trainable_params_%': 0.95, 'performance': 86.5, 'training_time': 155},
]

plot_peft_comparison(
    results,
    metrics=['trainable_params_%', 'performance', 'training_time']
)
```

---

## ğŸ“‹ æ•´åˆæª¢æŸ¥æ¸…å–®

### Lab-01 (LoRA) æ•´åˆ
- [ ] 02-Train.ipynb: æ·»åŠ è¨“ç·´æ›²ç·šè¦–è¦ºåŒ–
- [ ] 02-Train.ipynb: æ·»åŠ åƒæ•¸åˆ†ä½ˆåœ–
- [ ] 03-Inference.ipynb: æ·»åŠ æ¨ç†æ€§èƒ½å°æ¯”
- [ ] 04-Merge_and_Deploy.ipynb: æ·»åŠ æ¨¡å‹å¤§å°å°æ¯”

### Lab-02 (AdapterLayers) æ•´åˆ
- [ ] 02-Train.ipynb: è¨“ç·´æ›²ç·š
- [ ] 03-Inference.ipynb: å¤šä»»å‹™æ€§èƒ½å°æ¯”
- [ ] 04-Merge_and_Deploy.ipynb: Adapter çµæ§‹è¦–è¦ºåŒ–

### Lab-05 (IA3) æ•´åˆ
- [ ] 02-Train.ipynb: è¨“ç·´æ›²ç·š
- [ ] 02-Train.ipynb: IA3 vs LoRA åƒæ•¸å°æ¯”
- [ ] 03-Inference.ipynb: æ¨ç†æ€§èƒ½

### å…¶ä»– Labs (Lab-03, 04, 06, 07, 08)
- [ ] æŒ‰ç›¸åŒæ¨¡å¼æ•´åˆ

---

## ğŸš€ å¯¦æ–½è¨ˆåŠƒ

### Phase 1: è©¦é» (æœ¬é€±)
1. Lab-01 (LoRA) - å®Œæ•´æ•´åˆ
2. Lab-05 (IA3) - å®Œæ•´æ•´åˆ
3. é©—è­‰å·¥å…·å¯ç”¨æ€§

**é ä¼°å·¥æ™‚**: 2-3 å°æ™‚

### Phase 2: æ¨å»£ (ä¸‹é€±)
1. Lab-02, Lab-08 æ•´åˆ
2. å„ªåŒ–æ¨¡æ¿
3. å½¢æˆæ¨™æº–æµç¨‹

**é ä¼°å·¥æ™‚**: 2-3 å°æ™‚

### Phase 3: å®Œæˆ (ç¬¬ä¸‰é€±)
1. å‰©é¤˜å¯¦é©—å®¤æ•´åˆ
2. å“è³ªæª¢æŸ¥
3. æ›´æ–°æ–‡æª”

**é ä¼°å·¥æ™‚**: 2-3 å°æ™‚

**ç¸½å·¥æ™‚**: 6-9 å°æ™‚

---

## ğŸ’¡ æ³¨æ„äº‹é …

### è·¯å¾‘å•é¡Œ
notebooks åœ¨ä¸åŒå±¤ç´šï¼Œéœ€è¦æ­£ç¢ºæ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘:

```python
import sys
import os

# æ–¹æ³• 1: ç›¸å°è·¯å¾‘
sys.path.append(os.path.abspath('../../../..'))

# æ–¹æ³• 2: å‹•æ…‹è¨ˆç®—
current_dir = os.path.dirname(os.path.abspath('__file__'))
project_root = os.path.abspath(os.path.join(current_dir, '../../../..'))
sys.path.append(project_root)

# ç„¶å¾Œå°å…¥
from common_utils import plot_training_curves
```

### ä¾è³´æª¢æŸ¥
ç¢ºä¿æ‰€æœ‰ notebooks ç’°å¢ƒåŒ…å«:
```bash
pip install matplotlib seaborn pandas
```

### éŒ¯èª¤è™•ç†
å¦‚æœè¦–è¦ºåŒ–å¤±æ•—ï¼Œä¸æ‡‰ä¸­æ–·è¨“ç·´æµç¨‹:

```python
try:
    from common_utils.visualization import plot_training_curves
    plot_training_curves(history)
except Exception as e:
    print(f"âš ï¸  è¦–è¦ºåŒ–å¤±æ•—: {e}")
    print("è¨“ç·´çµæœå·²ä¿å­˜ï¼Œè«‹æ‰‹å‹•æŸ¥çœ‹ log_history")
```

---

## ğŸ“Š é æœŸæ•ˆæœ

### æ•´åˆå‰
- å­¸ç”Ÿåªèƒ½çœ‹åˆ°æ–‡å­—è¼¸å‡ºçš„ loss å€¼
- ç„¡æ³•ç›´è§€ç†è§£è¨“ç·´è¶¨å‹¢
- é›£ä»¥æ¯”è¼ƒä¸åŒé…ç½®

### æ•´åˆå¾Œ
- âœ… æ¸…æ™°çš„è¨“ç·´/é©—è­‰æå¤±æ›²ç·š
- âœ… å›°æƒ‘åº¦è®ŠåŒ–è¶¨å‹¢åœ–
- âœ… åƒæ•¸æ•ˆç‡è¦–è¦ºåŒ–
- âœ… æ¨ç†æ€§èƒ½å°æ¯”åœ–è¡¨
- âœ… ä¸€è‡´çš„è¦–è¦ºé¢¨æ ¼

---

## ğŸ“ æ•™å­¸åƒ¹å€¼

**å­¸ç¿’é«”é©—æå‡**:
- è¦–è¦ºåŒ–å­¸ç¿’æ•ˆæœæ›´ä½³
- å¿«é€Ÿè­˜åˆ¥è¨“ç·´å•é¡Œ
- ç›´è§€ç†è§£åƒæ•¸æ•ˆç‡æ¦‚å¿µ

**å¯¦ç”¨æ€§æå‡**:
- å¯ç”¨æ–¼å¯¦éš›é …ç›®
- æ¨™æº–åŒ–çš„æ€§èƒ½å ±å‘Š
- æ˜“æ–¼åˆ†äº«èˆ‡å±•ç¤º

---

**æ–‡æª”ç‰ˆæœ¬**: v1.0
**ç¶­è­·è€…**: LLM æ•™å­¸å°ˆæ¡ˆåœ˜éšŠ
**ä¸‹ä¸€æ­¥**: åœ¨ Lab-01 é€²è¡Œè©¦é»æ•´åˆ
