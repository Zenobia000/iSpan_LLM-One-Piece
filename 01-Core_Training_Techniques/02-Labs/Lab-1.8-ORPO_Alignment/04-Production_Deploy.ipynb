{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# Lab-1.8-04: ORPO 模型生產部署指南\n",
    "\n",
    "**部署目標**: 將訓練完成的 ORPO 對齊模型部署到生產環境\n",
    "- 模型量化與優化\n",
    "- 推理服務搭建\n",
    "- 性能監控與 A/B 測試\n",
    "- 持續優化策略\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. 環境準備和依賴安裝\n",
    "\n",
    "安裝生產部署所需的額外依賴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查和安裝部署相關依賴\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✅ {package} 已安裝\")\n",
    "    except ImportError:\n",
    "        print(f\"📦 安裝 {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# 部署相關依賴\n",
    "deployment_packages = [\n",
    "    'fastapi',\n",
    "    'uvicorn', \n",
    "    'gradio',\n",
    "    'prometheus_client',\n",
    "    'psutil'\n",
    "]\n",
    "\n",
    "print(\"🚀 檢查部署依賴...\")\n",
    "for package in deployment_packages:\n",
    "    install_if_missing(package)\n",
    "\n",
    "print(\"\\n📋 導入必要模組...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "# 部署相關\n",
    "import fastapi\n",
    "import uvicorn\n",
    "import gradio as gr\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "\n",
    "# 檢查設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🎯 部署設備: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-optimization-section",
   "metadata": {},
   "source": [
    "## 2. 模型載入與優化\n",
    "\n",
    "載入已訓練的 ORPO 模型並進行生產優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionORPOModel:\n",
    "    \"\"\"生產環境的 ORPO 模型包裝器\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"microsoft/DialoGPT-medium\", use_quantization=True):\n",
    "        self.model_name = model_name\n",
    "        self.use_quantization = use_quantization\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "        \n",
    "        # 性能統計\n",
    "        self.request_count = 0\n",
    "        self.total_inference_time = 0\n",
    "        self.avg_tokens_per_second = 0\n",
    "        \n",
    "        print(f\"🏭 初始化生產模型: {model_name}\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"載入和優化模型\"\"\"\n",
    "        print(\"📥 載入模型和 tokenizer...\")\n",
    "        \n",
    "        # 載入 tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # 量化配置\n",
    "        if self.use_quantization:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "            print(\"⚡ 使用 4-bit 量化\")\n",
    "        else:\n",
    "            bnb_config = None\n",
    "            print(\"🔥 使用全精度模型\")\n",
    "        \n",
    "        # 載入模型\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16 if self.use_quantization else torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 創建生成管道\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\",\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "        \n",
    "        print(\"✅ 模型載入完成\")\n",
    "        print(f\"📊 模型參數量: {self.model.num_parameters() / 1e6:.1f}M\")\n",
    "        \n",
    "    def generate_response(self, prompt: str, max_tokens: int = 256) -> Dict:\n",
    "        \"\"\"生成回應\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 生成回應\n",
    "            outputs = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated_text = outputs[0]['generated_text']\n",
    "            response = generated_text[len(prompt):].strip()\n",
    "            \n",
    "            # 計算性能指標\n",
    "            inference_time = time.time() - start_time\n",
    "            token_count = len(self.tokenizer.encode(response))\n",
    "            tokens_per_second = token_count / inference_time if inference_time > 0 else 0\n",
    "            \n",
    "            # 更新統計\n",
    "            self.request_count += 1\n",
    "            self.total_inference_time += inference_time\n",
    "            self.avg_tokens_per_second = (\n",
    "                (self.avg_tokens_per_second * (self.request_count - 1) + tokens_per_second) \n",
    "                / self.request_count\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'inference_time': inference_time,\n",
    "                'token_count': token_count,\n",
    "                'tokens_per_second': tokens_per_second,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error: {str(e)}\",\n",
    "                'inference_time': time.time() - start_time,\n",
    "                'token_count': 0,\n",
    "                'tokens_per_second': 0,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"獲取模型統計信息\"\"\"\n",
    "        return {\n",
    "            'request_count': self.request_count,\n",
    "            'total_inference_time': self.total_inference_time,\n",
    "            'avg_inference_time': self.total_inference_time / max(self.request_count, 1),\n",
    "            'avg_tokens_per_second': self.avg_tokens_per_second,\n",
    "            'model_name': self.model_name,\n",
    "            'quantized': self.use_quantization\n",
    "        }\n",
    "\n",
    "# 初始化生產模型\n",
    "production_model = ProductionORPOModel(use_quantization=True)\n",
    "production_model.load_model()\n",
    "\n",
    "print(\"🎉 生產模型初始化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-test-section",
   "metadata": {},
   "source": [
    "## 3. 性能基準測試\n",
    "\n",
    "測試模型在生產環境下的性能表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_performance_benchmark():\n",
    "    \"\"\"運行性能基準測試\"\"\"\n",
    "    \n",
    "    print(\"🚀 開始性能基準測試...\")\n",
    "    \n",
    "    # 測試用例\n",
    "    test_prompts = [\n",
    "        \"How can I improve my productivity at work?\",\n",
    "        \"What are the benefits of regular exercise?\",\n",
    "        \"Explain the concept of machine learning\",\n",
    "        \"How to cook a perfect pasta?\",\n",
    "        \"What is the meaning of life?\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\n📝 測試 {i+1}/{len(test_prompts)}: {prompt[:50]}...\")\n",
    "        \n",
    "        # 記錄系統資源\n",
    "        gpu_memory_before = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "        cpu_percent_before = psutil.cpu_percent()\n",
    "        \n",
    "        # 生成回應\n",
    "        result = production_model.generate_response(prompt, max_tokens=128)\n",
    "        \n",
    "        # 記錄資源使用\n",
    "        gpu_memory_after = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "        cpu_percent_after = psutil.cpu_percent()\n",
    "        \n",
    "        # 保存結果\n",
    "        test_result = {\n",
    "            'prompt': prompt,\n",
    "            'response': result['response'][:100] + '...' if len(result['response']) > 100 else result['response'],\n",
    "            'inference_time': result['inference_time'],\n",
    "            'token_count': result['token_count'],\n",
    "            'tokens_per_second': result['tokens_per_second'],\n",
    "            'gpu_memory_usage': gpu_memory_after - gpu_memory_before,\n",
    "            'cpu_usage': (cpu_percent_before + cpu_percent_after) / 2,\n",
    "            'success': result['success']\n",
    "        }\n",
    "        \n",
    "        results.append(test_result)\n",
    "        \n",
    "        print(f\"⏱️  推理時間: {result['inference_time']:.3f}s\")\n",
    "        print(f\"🔤 Token 數量: {result['token_count']}\")\n",
    "        print(f\"⚡ Token/秒: {result['tokens_per_second']:.1f}\")\n",
    "        \n",
    "        # 清理記憶體\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 運行基準測試\n",
    "benchmark_results = run_performance_benchmark()\n",
    "\n",
    "# 統計分析\n",
    "df = pd.DataFrame(benchmark_results)\n",
    "print(\"\\n📊 性能統計摘要:\")\n",
    "print(f\"平均推理時間: {df['inference_time'].mean():.3f}s\")\n",
    "print(f\"平均 Token/秒: {df['tokens_per_second'].mean():.1f}\")\n",
    "print(f\"成功率: {df['success'].mean()*100:.1f}%\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"平均 GPU 記憶體增長: {df['gpu_memory_usage'].mean():.3f} GB\")\n",
    "print(f\"平均 CPU 使用率: {df['cpu_usage'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring-section",
   "metadata": {},
   "source": [
    "## 4. 監控系統設置\n",
    "\n",
    "設置 Prometheus 監控指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitoring-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus 監控指標\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "class ModelMonitoring:\n",
    "    \"\"\"模型監控系統\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 請求計數器\n",
    "        self.request_count = Counter(\n",
    "            'orpo_model_requests_total',\n",
    "            'Total number of requests to ORPO model',\n",
    "            ['status']  # success, error\n",
    "        )\n",
    "        \n",
    "        # 推理時間分布\n",
    "        self.inference_duration = Histogram(\n",
    "            'orpo_model_inference_duration_seconds',\n",
    "            'Time spent on model inference',\n",
    "            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "        )\n",
    "        \n",
    "        # Token 生成速度\n",
    "        self.tokens_per_second = Gauge(\n",
    "            'orpo_model_tokens_per_second',\n",
    "            'Average tokens generated per second'\n",
    "        )\n",
    "        \n",
    "        # GPU 記憶體使用\n",
    "        self.gpu_memory_usage = Gauge(\n",
    "            'orpo_model_gpu_memory_gb',\n",
    "            'GPU memory usage in GB'\n",
    "        )\n",
    "        \n",
    "        # CPU 使用率\n",
    "        self.cpu_usage = Gauge(\n",
    "            'orpo_model_cpu_percent',\n",
    "            'CPU usage percentage'\n",
    "        )\n",
    "        \n",
    "        print(\"📊 監控系統初始化完成\")\n",
    "    \n",
    "    def record_request(self, result: Dict):\n",
    "        \"\"\"記錄請求指標\"\"\"\n",
    "        # 記錄請求狀態\n",
    "        status = 'success' if result['success'] else 'error'\n",
    "        self.request_count.labels(status=status).inc()\n",
    "        \n",
    "        # 記錄推理時間\n",
    "        self.inference_duration.observe(result['inference_time'])\n",
    "        \n",
    "        # 更新速度指標\n",
    "        self.tokens_per_second.set(result['tokens_per_second'])\n",
    "        \n",
    "        # 更新資源使用\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_memory_usage.set(torch.cuda.memory_allocated() / 1e9)\n",
    "        self.cpu_usage.set(psutil.cpu_percent())\n",
    "    \n",
    "    def get_metrics(self) -> str:\n",
    "        \"\"\"獲取 Prometheus 格式的指標\"\"\"\n",
    "        return generate_latest()\n",
    "\n",
    "# 初始化監控\n",
    "monitoring = ModelMonitoring()\n",
    "\n",
    "print(\"📈 監控系統就緒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-service-section",
   "metadata": {},
   "source": [
    "## 5. FastAPI 服務搭建\n",
    "\n",
    "創建 RESTful API 服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import PlainTextResponse\n",
    "import uvicorn\n",
    "\n",
    "# API 數據模型\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    response: str\n",
    "    inference_time: float\n",
    "    token_count: int\n",
    "    tokens_per_second: float\n",
    "    request_id: str\n",
    "\n",
    "# 創建 FastAPI 應用\n",
    "app = FastAPI(\n",
    "    title=\"ORPO Model API\",\n",
    "    description=\"Production API for ORPO-aligned language model\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"ORPO Model API is running\", \"status\": \"healthy\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"健康檢查端點\"\"\"\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1e9 if gpu_available else 0\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"gpu_available\": gpu_available,\n",
    "        \"gpu_memory_gb\": gpu_memory,\n",
    "        \"model_loaded\": production_model.model is not None,\n",
    "        \"total_requests\": production_model.request_count\n",
    "    }\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)\n",
    "async def generate_text(request: GenerationRequest):\n",
    "    \"\"\"文本生成端點\"\"\"\n",
    "    try:\n",
    "        # 生成唯一請求 ID\n",
    "        request_id = f\"req_{int(time.time()*1000)}_{production_model.request_count}\"\n",
    "        \n",
    "        # 生成回應\n",
    "        result = production_model.generate_response(\n",
    "            request.prompt, \n",
    "            max_tokens=request.max_tokens\n",
    "        )\n",
    "        \n",
    "        # 記錄監控指標\n",
    "        monitoring.record_request(result)\n",
    "        \n",
    "        if not result['success']:\n",
    "            raise HTTPException(status_code=500, detail=result.get('error', 'Generation failed'))\n",
    "        \n",
    "        return GenerationResponse(\n",
    "            response=result['response'],\n",
    "            inference_time=result['inference_time'],\n",
    "            token_count=result['token_count'],\n",
    "            tokens_per_second=result['tokens_per_second'],\n",
    "            request_id=request_id\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "async def get_stats():\n",
    "    \"\"\"獲取模型統計信息\"\"\"\n",
    "    return production_model.get_stats()\n",
    "\n",
    "@app.get(\"/metrics\", response_class=PlainTextResponse)\n",
    "async def get_metrics():\n",
    "    \"\"\"Prometheus 指標端點\"\"\"\n",
    "    return monitoring.get_metrics()\n",
    "\n",
    "print(\"🌐 FastAPI 服務配置完成\")\n",
    "print(\"可用端點:\")\n",
    "print(\"  GET  /          - 服務狀態\")\n",
    "print(\"  GET  /health    - 健康檢查\")\n",
    "print(\"  POST /generate  - 文本生成\")\n",
    "print(\"  GET  /stats     - 模型統計\")\n",
    "print(\"  GET  /metrics   - Prometheus 指標\")\n",
    "\n",
    "# 注意：在 Jupyter 中不能直接運行 uvicorn.run()\n",
    "# 實際部署時使用: uvicorn main:app --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-interface-section",
   "metadata": {},
   "source": [
    "## 6. Gradio 用戶界面\n",
    "\n",
    "創建友好的 Web 界面供用戶測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gradio_generate(prompt, max_tokens, temperature):\n",
    "    \"\"\"Gradio 介面的生成函數\"\"\"\n",
    "    try:\n",
    "        # 調整模型參數（這裡簡化處理）\n",
    "        result = production_model.generate_response(prompt, max_tokens=int(max_tokens))\n",
    "        \n",
    "        # 記錄監控指標\n",
    "        monitoring.record_request(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            stats_text = f\"\"\"📊 生成統計:\n",
    "⏱️ 推理時間: {result['inference_time']:.3f}s\n",
    "🔤 Token 數量: {result['token_count']}\n",
    "⚡ Token/秒: {result['tokens_per_second']:.1f}\n",
    "📈 總請求數: {production_model.request_count}\"\"\"\n",
    "            \n",
    "            return result['response'], stats_text\n",
    "        else:\n",
    "            return f\"❌ 生成失敗: {result.get('error', 'Unknown error')}\", \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"❌ 錯誤: {str(e)}\", \"\"\n",
    "\n",
    "# 創建 Gradio 界面\n",
    "def create_gradio_interface():\n",
    "    \"\"\"創建 Gradio 網頁界面\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"ORPO Model Demo\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # 🤖 ORPO 對齊模型演示\n",
    "            \n",
    "            這是一個基於 ORPO (Odds Ratio Preference Optimization) 訓練的對齊語言模型。\n",
    "            ORPO 是一種新的單階段對齊方法，相比傳統的 SFT+DPO 方法更加高效。\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                prompt_input = gr.Textbox(\n",
    "                    label=\"輸入提示 (Prompt)\",\n",
    "                    placeholder=\"請輸入您的問題或提示...\",\n",
    "                    lines=3\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    max_tokens_slider = gr.Slider(\n",
    "                        label=\"最大 Token 數\",\n",
    "                        minimum=50,\n",
    "                        maximum=512,\n",
    "                        value=256,\n",
    "                        step=10\n",
    "                    )\n",
    "                    \n",
    "                    temperature_slider = gr.Slider(\n",
    "                        label=\"溫度 (Temperature)\",\n",
    "                        minimum=0.1,\n",
    "                        maximum=2.0,\n",
    "                        value=0.7,\n",
    "                        step=0.1\n",
    "                    )\n",
    "                \n",
    "                generate_btn = gr.Button(\"🚀 生成回應\", variant=\"primary\")\n",
    "                \n",
    "            with gr.Column(scale=1):\n",
    "                stats_display = gr.Textbox(\n",
    "                    label=\"性能統計\",\n",
    "                    value=\"等待生成...\",\n",
    "                    lines=6,\n",
    "                    interactive=False\n",
    "                )\n",
    "        \n",
    "        response_output = gr.Textbox(\n",
    "            label=\"模型回應\",\n",
    "            lines=8,\n",
    "            interactive=False\n",
    "        )\n",
    "        \n",
    "        # 預設示例\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"如何提高工作效率？\", 200, 0.7],\n",
    "                [\"解釋一下機器學習的基本概念\", 300, 0.8],\n",
    "                [\"給我一些健康飲食的建議\", 250, 0.6],\n",
    "                [\"如何學習新的編程語言？\", 280, 0.7]\n",
    "            ],\n",
    "            inputs=[prompt_input, max_tokens_slider, temperature_slider]\n",
    "        )\n",
    "        \n",
    "        # 事件綁定\n",
    "        generate_btn.click(\n",
    "            fn=gradio_generate,\n",
    "            inputs=[prompt_input, max_tokens_slider, temperature_slider],\n",
    "            outputs=[response_output, stats_display]\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### 📋 使用說明\n",
    "            \n",
    "            1. **輸入提示**: 在上方文本框中輸入您的問題或提示\n",
    "            2. **調整參數**: 可以調整最大 Token 數和溫度參數\n",
    "            3. **生成回應**: 點擊「生成回應」按鈕獲得模型回應\n",
    "            4. **查看統計**: 右側會顯示生成的性能統計信息\n",
    "            \n",
    "            **參數說明**:\n",
    "            - **最大 Token 數**: 控制回應的最大長度\n",
    "            - **溫度**: 控制回應的創造性（越高越有創意，越低越保守）\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# 創建界面\n",
    "gradio_demo = create_gradio_interface()\n",
    "print(\"🎨 Gradio 界面已配置\")\n",
    "print(\"執行 gradio_demo.launch() 來啟動界面\")\n",
    "\n",
    "# 注意：在 Jupyter 中可以執行以下代碼來啟動界面\n",
    "# gradio_demo.launch(share=False, server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab-testing-section",
   "metadata": {},
   "source": [
    "## 7. A/B 測試框架\n",
    "\n",
    "實現模型版本對比測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dataclass\n",
    "class ABTestConfig:\n",
    "    \"\"\"A/B 測試配置\"\"\"\n",
    "    test_name: str\n",
    "    model_a_name: str\n",
    "    model_b_name: str\n",
    "    traffic_split: float = 0.5  # 流量分配比例 (0.5 = 50/50)\n",
    "    start_time: datetime = None\n",
    "    end_time: datetime = None\n",
    "    \n",
    "@dataclass\n",
    "class ABTestResult:\n",
    "    \"\"\"A/B 測試結果\"\"\"\n",
    "    user_id: str\n",
    "    model_version: str\n",
    "    prompt: str\n",
    "    response: str\n",
    "    inference_time: float\n",
    "    user_rating: Optional[int] = None  # 1-5 評分\n",
    "    timestamp: datetime = None\n",
    "\n",
    "class ABTestManager:\n",
    "    \"\"\"A/B 測試管理器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.active_tests: Dict[str, ABTestConfig] = {}\n",
    "        self.test_results: List[ABTestResult] = []\n",
    "        \n",
    "    def create_test(self, config: ABTestConfig):\n",
    "        \"\"\"創建新的 A/B 測試\"\"\"\n",
    "        if config.start_time is None:\n",
    "            config.start_time = datetime.now()\n",
    "        if config.end_time is None:\n",
    "            config.end_time = config.start_time + timedelta(days=7)\n",
    "            \n",
    "        self.active_tests[config.test_name] = config\n",
    "        print(f\"🧪 A/B 測試已創建: {config.test_name}\")\n",
    "        print(f\"   模型 A: {config.model_a_name}\")\n",
    "        print(f\"   模型 B: {config.model_b_name}\")\n",
    "        print(f\"   流量分配: {config.traffic_split*100:.0f}% / {(1-config.traffic_split)*100:.0f}%\")\n",
    "        \n",
    "    def assign_model(self, test_name: str, user_id: str) -> str:\n",
    "        \"\"\"為用戶分配模型版本\"\"\"\n",
    "        if test_name not in self.active_tests:\n",
    "            return \"default\"\n",
    "            \n",
    "        config = self.active_tests[test_name]\n",
    "        \n",
    "        # 檢查測試是否在有效期內\n",
    "        now = datetime.now()\n",
    "        if now < config.start_time or now > config.end_time:\n",
    "            return config.model_a_name  # 默認返回 A 版本\n",
    "        \n",
    "        # 基於用戶 ID 的一致性分配（確保同一用戶總是獲得相同版本）\n",
    "        random.seed(hash(user_id + test_name))\n",
    "        if random.random() < config.traffic_split:\n",
    "            return config.model_a_name\n",
    "        else:\n",
    "            return config.model_b_name\n",
    "    \n",
    "    def record_result(self, result: ABTestResult):\n",
    "        \"\"\"記錄測試結果\"\"\"\n",
    "        if result.timestamp is None:\n",
    "            result.timestamp = datetime.now()\n",
    "        self.test_results.append(result)\n",
    "        \n",
    "    def analyze_test(self, test_name: str) -> Dict:\n",
    "        \"\"\"分析 A/B 測試結果\"\"\"\n",
    "        if test_name not in self.active_tests:\n",
    "            return {\"error\": \"Test not found\"}\n",
    "        \n",
    "        config = self.active_tests[test_name]\n",
    "        \n",
    "        # 篩選相關結果\n",
    "        relevant_results = [\n",
    "            r for r in self.test_results \n",
    "            if r.model_version in [config.model_a_name, config.model_b_name]\n",
    "        ]\n",
    "        \n",
    "        if not relevant_results:\n",
    "            return {\"error\": \"No results found\"}\n",
    "        \n",
    "        # 分組統計\n",
    "        model_a_results = [r for r in relevant_results if r.model_version == config.model_a_name]\n",
    "        model_b_results = [r for r in relevant_results if r.model_version == config.model_b_name]\n",
    "        \n",
    "        def calculate_stats(results):\n",
    "            if not results:\n",
    "                return {\"count\": 0, \"avg_inference_time\": 0, \"avg_rating\": 0}\n",
    "            \n",
    "            ratings = [r.user_rating for r in results if r.user_rating is not None]\n",
    "            \n",
    "            return {\n",
    "                \"count\": len(results),\n",
    "                \"avg_inference_time\": sum(r.inference_time for r in results) / len(results),\n",
    "                \"avg_rating\": sum(ratings) / len(ratings) if ratings else 0,\n",
    "                \"rating_count\": len(ratings)\n",
    "            }\n",
    "        \n",
    "        model_a_stats = calculate_stats(model_a_results)\n",
    "        model_b_stats = calculate_stats(model_b_results)\n",
    "        \n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"model_a\": {\n",
    "                \"name\": config.model_a_name,\n",
    "                \"stats\": model_a_stats\n",
    "            },\n",
    "            \"model_b\": {\n",
    "                \"name\": config.model_b_name,\n",
    "                \"stats\": model_b_stats\n",
    "            },\n",
    "            \"total_results\": len(relevant_results)\n",
    "        }\n",
    "\n",
    "# 初始化 A/B 測試管理器\n",
    "ab_test_manager = ABTestManager()\n",
    "\n",
    "# 創建示例 A/B 測試\n",
    "test_config = ABTestConfig(\n",
    "    test_name=\"orpo_vs_baseline\",\n",
    "    model_a_name=\"ORPO_Model\",\n",
    "    model_b_name=\"Baseline_Model\",\n",
    "    traffic_split=0.5\n",
    ")\n",
    "\n",
    "ab_test_manager.create_test(test_config)\n",
    "\n",
    "# 模擬一些測試結果\n",
    "def simulate_ab_test_results():\n",
    "    \"\"\"模擬 A/B 測試結果\"\"\"\n",
    "    print(\"\\n🔬 模擬 A/B 測試結果...\")\n",
    "    \n",
    "    users = [f\"user_{i}\" for i in range(100)]\n",
    "    prompts = [\n",
    "        \"How to be more productive?\",\n",
    "        \"Explain quantum computing\",\n",
    "        \"Best practices for coding\",\n",
    "        \"How to stay healthy?\"\n",
    "    ]\n",
    "    \n",
    "    for user in users:\n",
    "        prompt = random.choice(prompts)\n",
    "        model_version = ab_test_manager.assign_model(\"orpo_vs_baseline\", user)\n",
    "        \n",
    "        # 模擬不同模型的性能差異\n",
    "        if model_version == \"ORPO_Model\":\n",
    "            inference_time = random.uniform(0.5, 1.2)\n",
    "            rating = random.choices([3, 4, 5], weights=[0.1, 0.4, 0.5])[0]\n",
    "        else:\n",
    "            inference_time = random.uniform(0.8, 1.8)\n",
    "            rating = random.choices([2, 3, 4], weights=[0.2, 0.5, 0.3])[0]\n",
    "        \n",
    "        result = ABTestResult(\n",
    "            user_id=user,\n",
    "            model_version=model_version,\n",
    "            prompt=prompt,\n",
    "            response=\"Generated response...\",\n",
    "            inference_time=inference_time,\n",
    "            user_rating=rating if random.random() > 0.3 else None  # 70% 用戶提供評分\n",
    "        )\n",
    "        \n",
    "        ab_test_manager.record_result(result)\n",
    "    \n",
    "    return ab_test_manager.analyze_test(\"orpo_vs_baseline\")\n",
    "\n",
    "# 運行模擬測試\n",
    "test_analysis = simulate_ab_test_results()\n",
    "print(\"\\n📊 A/B 測試分析結果:\")\n",
    "print(json.dumps(test_analysis, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment-checklist-section",
   "metadata": {},
   "source": [
    "## 8. 生產部署檢查清單\n",
    "\n",
    "確保模型準備好投入生產環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_readiness_check():\n",
    "    \"\"\"生產就緒檢查\"\"\"\n",
    "    \n",
    "    print(\"🔍 生產部署就緒檢查\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    checklist = {\n",
    "        \"模型載入\": production_model.model is not None,\n",
    "        \"GPU 可用性\": torch.cuda.is_available(),\n",
    "        \"記憶體充足\": torch.cuda.get_device_properties(0).total_memory > 8e9 if torch.cuda.is_available() else True,\n",
    "        \"監控系統\": monitoring is not None,\n",
    "        \"API 服務\": app is not None,\n",
    "        \"A/B 測試\": ab_test_manager is not None,\n",
    "        \"基準測試完成\": len(benchmark_results) > 0,\n",
    "    }\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    for check, passed in checklist.items():\n",
    "        status = \"✅\" if passed else \"❌\"\n",
    "        print(f\"{status} {check}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"🎉 所有檢查通過！模型已準備好投入生產環境。\")\n",
    "    else:\n",
    "        print(\"⚠️  部分檢查未通過，請先解決相關問題。\")\n",
    "    \n",
    "    # 部署建議\n",
    "    print(\"\\n📋 生產部署建議:\")\n",
    "    suggestions = [\n",
    "        \"🔧 使用 Docker 容器化部署\",\n",
    "        \"🔒 啟用 HTTPS 和身份驗證\",\n",
    "        \"📊 設置 Prometheus + Grafana 監控\",\n",
    "        \"🚦 配置負載均衡器\",\n",
    "        \"💾 設置定期模型備份\",\n",
    "        \"🔄 實施滾動更新策略\",\n",
    "        \"⚡ 考慮使用 NVIDIA Triton 推理服務器\",\n",
    "        \"📝 建立日誌聚合和分析\",\n",
    "        \"🧪 持續運行 A/B 測試\",\n",
    "        \"🔍 設置異常檢測和自動告警\"\n",
    "    ]\n",
    "    \n",
    "    for suggestion in suggestions:\n",
    "        print(f\"  {suggestion}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# 執行就緒檢查\n",
    "ready_for_production = production_readiness_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "docker-section",
   "metadata": {},
   "source": [
    "## 9. Docker 部署配置\n",
    "\n",
    "生成 Docker 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "docker-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docker_files():\n",
    "    \"\"\"生成 Docker 部署文件\"\"\"\n",
    "    \n",
    "    # Dockerfile\n",
    "    dockerfile_content = \"\"\"\n",
    "# ORPO Model Production Dockerfile\n",
    "FROM nvidia/cuda:12.1-runtime-ubuntu22.04\n",
    "\n",
    "# 設置工作目錄\n",
    "WORKDIR /app\n",
    "\n",
    "# 安裝系統依賴\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3 \\\n",
    "    python3-pip \\\n",
    "    git \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# 安裝 Python 依賴\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# 複製應用代碼\n",
    "COPY . .\n",
    "\n",
    "# 設置環境變量\n",
    "ENV PYTHONPATH=/app\n",
    "ENV CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# 暴露端口\n",
    "EXPOSE 8000 7860\n",
    "\n",
    "# 健康檢查\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# 啟動命令\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"1\"]\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # requirements.txt\n",
    "    requirements_content = \"\"\"\n",
    "torch>=2.1.0\n",
    "transformers>=4.35.0\n",
    "peft>=0.6.0\n",
    "bitsandbytes>=0.41.0\n",
    "accelerate>=0.24.0\n",
    "fastapi>=0.104.0\n",
    "uvicorn[standard]>=0.24.0\n",
    "gradio>=4.0.0\n",
    "prometheus-client>=0.19.0\n",
    "psutil>=5.9.0\n",
    "pydantic>=2.0.0\n",
    "numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # docker-compose.yml\n",
    "    docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  orpo-model:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"  # FastAPI\n",
    "      - \"7860:7860\"  # Gradio (如果需要)\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "      - MODEL_NAME=microsoft/DialoGPT-medium\n",
    "      - USE_QUANTIZATION=true\n",
    "    volumes:\n",
    "      - ./models:/app/models\n",
    "      - ./logs:/app/logs\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 60s\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "    restart: unless-stopped\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana-storage:/var/lib/grafana\n",
    "    restart: unless-stopped\n",
    "\n",
    "volumes:\n",
    "  grafana-storage:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # prometheus.yml\n",
    "    prometheus_config = \"\"\"\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'orpo-model'\n",
    "    static_configs:\n",
    "      - targets: ['orpo-model:8000']\n",
    "    metrics_path: '/metrics'\n",
    "    scrape_interval: 5s\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # main.py (簡化版的 FastAPI 應用)\n",
    "    main_py_content = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from prometheus_client import generate_latest, Counter, Histogram\n",
    "from fastapi.responses import PlainTextResponse\n",
    "\n",
    "# 初始化 FastAPI\n",
    "app = FastAPI(title=\"ORPO Model API\", version=\"1.0.0\")\n",
    "\n",
    "# Prometheus 指標\n",
    "REQUEST_COUNT = Counter('requests_total', 'Total requests', ['method', 'endpoint'])\n",
    "REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')\n",
    "\n",
    "# 全局變量\n",
    "model = None\n",
    "tokenizer = None\n",
    "text_generator = None\n",
    "\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 256\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    global model, tokenizer, text_generator\n",
    "    \n",
    "    model_name = os.getenv(\"MODEL_NAME\", \"microsoft/DialoGPT-medium\")\n",
    "    use_quantization = os.getenv(\"USE_QUANTIZATION\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    text_generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    REQUEST_COUNT.labels(method=\"GET\", endpoint=\"/\").inc()\n",
    "    return {\"message\": \"ORPO Model API\", \"status\": \"running\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    REQUEST_COUNT.labels(method=\"GET\", endpoint=\"/health\").inc()\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"gpu_available\": torch.cuda.is_available()\n",
    "    }\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: GenerationRequest):\n",
    "    REQUEST_COUNT.labels(method=\"POST\", endpoint=\"/generate\").inc()\n",
    "    \n",
    "    with REQUEST_DURATION.time():\n",
    "        if text_generator is None:\n",
    "            raise HTTPException(status_code=500, detail=\"Model not loaded\")\n",
    "        \n",
    "        try:\n",
    "            outputs = text_generator(\n",
    "                request.prompt,\n",
    "                max_new_tokens=request.max_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            response = outputs[0]['generated_text'][len(request.prompt):].strip()\n",
    "            \n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"prompt\": request.prompt\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/metrics\", response_class=PlainTextResponse)\n",
    "async def get_metrics():\n",
    "    return generate_latest()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # 保存文件\n",
    "    files = {\n",
    "        \"Dockerfile\": dockerfile_content,\n",
    "        \"requirements.txt\": requirements_content,\n",
    "        \"docker-compose.yml\": docker_compose_content,\n",
    "        \"prometheus.yml\": prometheus_config,\n",
    "        \"main.py\": main_py_content\n",
    "    }\n",
    "    \n",
    "    print(\"🐳 Docker 部署文件內容:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for filename, content in files.items():\n",
    "        print(f\"\\n📄 {filename}:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "    \n",
    "    print(\"\\n🚀 部署命令:\")\n",
    "    commands = [\n",
    "        \"# 構建和啟動服務\",\n",
    "        \"docker-compose up --build -d\",\n",
    "        \"\",\n",
    "        \"# 查看日誌\", \n",
    "        \"docker-compose logs -f orpo-model\",\n",
    "        \"\",\n",
    "        \"# 停止服務\",\n",
    "        \"docker-compose down\",\n",
    "        \"\",\n",
    "        \"# 重啟特定服務\",\n",
    "        \"docker-compose restart orpo-model\"\n",
    "    ]\n",
    "    \n",
    "    for cmd in commands:\n",
    "        print(cmd)\n",
    "    \n",
    "    return files\n",
    "\n",
    "# 生成 Docker 配置\n",
    "docker_files = generate_docker_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## 10. 部署總結與最佳實踐\n",
    "\n",
    "### 🎯 部署完成檢查清單\n",
    "\n",
    "✅ **模型優化**: 使用量化技術減少記憶體使用  \n",
    "✅ **API 服務**: FastAPI 提供高性能 RESTful 接口  \n",
    "✅ **用戶界面**: Gradio 提供友好的 Web 測試界面  \n",
    "✅ **監控系統**: Prometheus 指標收集和分析  \n",
    "✅ **A/B 測試**: 完整的模型版本對比框架  \n",
    "✅ **容器化**: Docker 配置確保一致的部署環境  \n",
    "✅ **健康檢查**: 自動化的服務狀態監控  \n",
    "✅ **性能基準**: 詳細的推理性能評估  \n",
    "\n",
    "### 🏗️ 生產環境架構建議\n",
    "\n",
    "```\n",
    "用戶請求 → 負載均衡器 → API Gateway → ORPO 模型服務\n",
    "                                      ↓\n",
    "                               監控系統 (Prometheus/Grafana)\n",
    "                                      ↓\n",
    "                               A/B 測試系統 → 數據分析\n",
    "```\n",
    "\n",
    "### 📊 關鍵性能指標 (KPIs)\n",
    "\n",
    "1. **延遲指標**:\n",
    "   - P50 推理時間 < 1.0s\n",
    "   - P95 推理時間 < 2.0s\n",
    "   - P99 推理時間 < 5.0s\n",
    "\n",
    "2. **吞吐量指標**:\n",
    "   - QPS (每秒查詢數) > 10\n",
    "   - Token/秒 > 50\n",
    "\n",
    "3. **可靠性指標**:\n",
    "   - 可用性 > 99.9%\n",
    "   - 錯誤率 < 0.1%\n",
    "\n",
    "4. **資源利用率**:\n",
    "   - GPU 利用率 60-80%\n",
    "   - 記憶體使用率 < 90%\n",
    "\n",
    "### 🔧 運維最佳實踐\n",
    "\n",
    "1. **監控告警**:\n",
    "   - 設置推理時間異常告警\n",
    "   - GPU 記憶體使用告警\n",
    "   - 服務健康狀態監控\n",
    "\n",
    "2. **自動化部署**:\n",
    "   - CI/CD 管道自動測試和部署\n",
    "   - 藍綠部署或滾動更新\n",
    "   - 自動回滾機制\n",
    "\n",
    "3. **數據管理**:\n",
    "   - 定期備份模型檢查點\n",
    "   - 日誌輪轉和歸檔\n",
    "   - 用戶反饋數據收集\n",
    "\n",
    "4. **安全防護**:\n",
    "   - API 身份驗證和授權\n",
    "   - 輸入內容過濾和驗證\n",
    "   - 速率限制防止濫用\n",
    "\n",
    "### 🚀 擴展策略\n",
    "\n",
    "1. **水平擴展**: 增加更多模型實例處理並發請求\n",
    "2. **模型分片**: 大型模型分佈式部署\n",
    "3. **緩存策略**: 常見查詢結果緩存\n",
    "4. **邊緣部署**: CDN 邊緣節點部署降低延遲\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 ORPO 模型生產部署指南完成！**\n",
    "\n",
    "現在您擁有了一個完整的 ORPO 對齊模型生產部署方案，包括：\n",
    "- 高性能推理服務\n",
    "- 全面的監控系統  \n",
    "- 科學的 A/B 測試框架\n",
    "- 容器化部署解決方案\n",
    "- 生產級別的最佳實踐建議\n",
    "\n",
    "這套完整的部署流程可以確保 ORPO 模型在生產環境中穩定、高效地運行，為用戶提供優質的 AI 服務體驗！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}