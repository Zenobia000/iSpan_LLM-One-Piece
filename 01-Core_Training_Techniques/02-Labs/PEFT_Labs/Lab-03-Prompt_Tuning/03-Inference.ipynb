{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: Prompt Tuning - Fine-Tuning a T5 Model for Summarization\n",
        "---\n",
        "## Notebook 3: Inference\n",
        "\n",
        "**Goal:** In this notebook, you will load the trained soft prompt and use the fine-tuned T5 model to generate summaries for new pieces of text.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Reload the base T5 model and tokenizer.\n",
        "-   Load the trained Prompt Tuning adapter from a checkpoint using `peft.PeftModel`.\n",
        "-   Write a function to perform inference and generate a summary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Reload Model and Adapter\n",
        "\n",
        "We will load the base `t5-small` model and then apply our trained soft prompt weights on top of it using `PeftModel.from_pretrained`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- Load Base Model and Tokenizer ---\n",
        "model_checkpoint = \"t5-small\"\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# --- Load PEFT Adapter ---\n",
        "output_dir = \"./t5-prompt-tuning-billsum\"\n",
        "latest_checkpoint = max(\n",
        "    [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")],\n",
        "    key=os.path.getmtime\n",
        ")\n",
        "print(f\"Loading adapter from: {latest_checkpoint}\")\n",
        "\n",
        "inference_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inference_model.to(device)\n",
        "inference_model.eval()\n",
        "\n",
        "print(\"âœ… Inference model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Perform Inference\n",
        "\n",
        "Now, let's test the model by giving it a piece of text to summarize. We'll use an example from the test set of the `billsum` dataset.\n",
        "\n",
        "The process is:\n",
        "1.  Tokenize the input text (including the \"summarize: \" prefix).\n",
        "2.  Use the `generate()` method to create the summary.\n",
        "3.  Decode the output tokens back into a string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a sample from the test set to summarize\n",
        "dataset = load_dataset(\"billsum\", split=\"test[:1]\")\n",
        "original_text = dataset[0][\"text\"]\n",
        "reference_summary = dataset[0][\"summary\"]\n",
        "\n",
        "# Prepare the input\n",
        "prompt = \"summarize: \" + original_text\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate the summary\n",
        "with torch.no_grad():\n",
        "    outputs = inference_model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        max_length=150,\n",
        "        num_beams=4, # Use beam search for higher quality summaries\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the results\n",
        "print(\"--- Original Text ---\")\n",
        "print(original_text)\n",
        "print(\"\\n--- Reference Summary ---\")\n",
        "print(reference_summary)\n",
        "print(\"\\n--- Generated Summary (via Prompt Tuning) ---\")\n",
        "print(generated_summary)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
