{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.5-03: 性能分析與診斷\n",
    "\n",
    "## 實驗目標\n",
    "\n",
    "本節將深入分析 vLLM 的性能數據，包括：\n",
    "- 歷史數據深度分析\n",
    "- 性能瓶頸識別與診斷\n",
    "- 負載測試與基準評估\n",
    "- 容量規劃與擴展建議\n",
    "- 性能優化策略制定\n",
    "\n",
    "## 分析框架\n",
    "\n",
    "### 1. 多維度性能分析\n",
    "```\n",
    "性能分析維度:\n",
    "├── 時間維度 (Time-based)\n",
    "│   ├── 趨勢分析 (Trend Analysis)\n",
    "│   ├── 週期性模式 (Periodic Patterns)\n",
    "│   └── 突發事件檢測 (Spike Detection)\n",
    "├── 資源維度 (Resource-based)\n",
    "│   ├── CPU/Memory/GPU 利用率\n",
    "│   ├── 瓶頸資源識別\n",
    "│   └── 資源配比優化\n",
    "└── 業務維度 (Business-based)\n",
    "    ├── 請求處理效率\n",
    "    ├── 用戶體驗指標\n",
    "    └── 成本效益分析\n",
    "```\n",
    "\n",
    "### 2. 性能指標體系\n",
    "- **響應性指標**: 延遲分佈、TTFT、TPOT\n",
    "- **吞吐量指標**: QPS、TPS、併發處理能力\n",
    "- **可靠性指標**: 錯誤率、可用性、穩定性\n",
    "- **效率指標**: 資源利用率、成本效益比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境初始化與數據載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# 科學計算與統計\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 時間序列分析\n",
    "try:\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    STATSMODELS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    STATSMODELS_AVAILABLE = False\n",
    "    print(\"⚠️  statsmodels 未安裝，部分時間序列分析功能不可用\")\n",
    "\n",
    "# 進階視覺化\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# 設定\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "print(\"✅ 性能分析環境初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceDataLoader:\n",
    "    \"\"\"性能數據載入器\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \".\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.monitoring_data = None\n",
    "        self.load_test_data = None\n",
    "    \n",
    "    def find_monitoring_files(self) -> List[Path]:\n",
    "        \"\"\"尋找監控數據文件\"\"\"\n",
    "        json_files = list(self.data_dir.glob(\"vllm_monitoring_data_*.json\"))\n",
    "        csv_files = list(self.data_dir.glob(\"vllm_monitoring_data_*.csv\"))\n",
    "        \n",
    "        return sorted(json_files + csv_files, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    def load_monitoring_data(self, file_path: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"載入監控數據\"\"\"\n",
    "        if file_path is None:\n",
    "            # 自動尋找最新的數據文件\n",
    "            files = self.find_monitoring_files()\n",
    "            if not files:\n",
    "                print(\"❌ 未找到監控數據文件\")\n",
    "                return self.generate_sample_data()\n",
    "            file_path = files[0]\n",
    "            print(f\"📁 使用數據文件: {file_path}\")\n",
    "        \n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if file_path.suffix == '.json':\n",
    "            return self._load_json_data(file_path)\n",
    "        elif file_path.suffix == '.csv':\n",
    "            return self._load_csv_data(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"不支援的文件格式: {file_path.suffix}\")\n",
    "    \n",
    "    def _load_json_data(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"載入 JSON 格式數據\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 建立 DataFrame\n",
    "        df_data = {}\n",
    "        \n",
    "        # 時間戳\n",
    "        timestamps = [datetime.fromisoformat(ts) for ts in data['timestamps']]\n",
    "        df_data['timestamp'] = timestamps\n",
    "        \n",
    "        # 系統指標\n",
    "        for metric_name, values in data['system_metrics'].items():\n",
    "            # 確保長度一致\n",
    "            padded_values = values + [None] * (len(timestamps) - len(values))\n",
    "            df_data[metric_name] = padded_values[:len(timestamps)]\n",
    "        \n",
    "        # vLLM 指標\n",
    "        for metric_name, values in data['vllm_metrics'].items():\n",
    "            padded_values = values + [None] * (len(timestamps) - len(values))\n",
    "            df_data[metric_name] = padded_values[:len(timestamps)]\n",
    "        \n",
    "        df = pd.DataFrame(df_data)\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        print(f\"✅ 載入 JSON 數據: {len(df)} 行, {len(df.columns)} 列\")\n",
    "        return df\n",
    "    \n",
    "    def _load_csv_data(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"載入 CSV 格式數據\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        print(f\"✅ 載入 CSV 數據: {len(df)} 行, {len(df.columns)} 列\")\n",
    "        return df\n",
    "    \n",
    "    def generate_sample_data(self, duration_hours: int = 2, interval_seconds: int = 30) -> pd.DataFrame:\n",
    "        \"\"\"生成示例數據用於分析\"\"\"\n",
    "        print(\"🔧 生成示例監控數據...\")\n",
    "        \n",
    "        # 時間範圍\n",
    "        end_time = datetime.now()\n",
    "        start_time = end_time - timedelta(hours=duration_hours)\n",
    "        timestamps = pd.date_range(start_time, end_time, freq=f'{interval_seconds}s')\n",
    "        \n",
    "        n_points = len(timestamps)\n",
    "        \n",
    "        # 生成基礎噪音\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # 時間因子 (模擬日常變化)\n",
    "        time_factor = np.sin(np.linspace(0, 4*np.pi, n_points)) * 0.2 + 1\n",
    "        \n",
    "        data = {\n",
    "            # 系統指標\n",
    "            'cpu_percent': np.clip(\n",
    "                30 + 20 * time_factor + np.random.normal(0, 5, n_points), 0, 100\n",
    "            ),\n",
    "            'memory_percent': np.clip(\n",
    "                60 + 10 * time_factor + np.random.normal(0, 3, n_points), 0, 100\n",
    "            ),\n",
    "            'gpu_memory_used': np.clip(\n",
    "                45 + 25 * time_factor + np.random.normal(0, 8, n_points), 0, 100\n",
    "            ),\n",
    "            'gpu_utilization': np.clip(\n",
    "                40 + 30 * time_factor + np.random.normal(0, 10, n_points), 0, 100\n",
    "            ),\n",
    "            \n",
    "            # vLLM 指標 (模擬)\n",
    "            'vllm_num_requests_running': np.random.poisson(3 * time_factor, n_points),\n",
    "            'vllm_num_requests_waiting': np.random.poisson(1 * time_factor, n_points),\n",
    "            'vllm_request_success_total': np.cumsum(np.random.poisson(2 * time_factor, n_points)),\n",
    "            'vllm_request_failure_total': np.cumsum(np.random.poisson(0.1 * time_factor, n_points)),\n",
    "            \n",
    "            # 延遲指標 (秒)\n",
    "            'vllm_time_to_first_token_seconds_sum': np.cumsum(\n",
    "                np.random.exponential(1.5 * time_factor, n_points)\n",
    "            ),\n",
    "            'vllm_time_to_first_token_seconds_count': np.cumsum(\n",
    "                np.random.poisson(1 * time_factor, n_points)\n",
    "            ),\n",
    "        }\n",
    "        \n",
    "        # 添加一些突發事件\n",
    "        spike_points = np.random.choice(n_points, size=5, replace=False)\n",
    "        for point in spike_points:\n",
    "            data['cpu_percent'][point] = min(95, data['cpu_percent'][point] + 30)\n",
    "            data['gpu_utilization'][point] = min(100, data['gpu_utilization'][point] + 40)\n",
    "        \n",
    "        df = pd.DataFrame(data, index=timestamps)\n",
    "        \n",
    "        print(f\"✅ 生成示例數據: {len(df)} 行, {len(df.columns)} 列\")\n",
    "        return df\n",
    "\n",
    "# 初始化數據載入器\n",
    "data_loader = PerformanceDataLoader()\n",
    "print(\"✅ 數據載入器已初始化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入監控數據\n",
    "df = data_loader.load_monitoring_data()\n",
    "\n",
    "# 數據基本資訊\n",
    "print(\"\\n📊 數據集基本資訊:\")\n",
    "print(f\"   時間範圍: {df.index.min()} 到 {df.index.max()}\")\n",
    "print(f\"   數據點數: {len(df)}\")\n",
    "print(f\"   時間跨度: {(df.index.max() - df.index.min()).total_seconds():.0f} 秒\")\n",
    "print(f\"   平均間隔: {(df.index.max() - df.index.min()).total_seconds() / (len(df) - 1):.1f} 秒\")\n",
    "\n",
    "print(\"\\n📈 可用指標:\")\n",
    "system_metrics = [col for col in df.columns if not col.startswith('vllm_')]\n",
    "vllm_metrics = [col for col in df.columns if col.startswith('vllm_')]\n",
    "\n",
    "print(f\"   系統指標 ({len(system_metrics)}): {', '.join(system_metrics[:5])}{'...' if len(system_metrics) > 5 else ''}\")\n",
    "print(f\"   vLLM 指標 ({len(vllm_metrics)}): {', '.join(vllm_metrics[:5])}{'...' if len(vllm_metrics) > 5 else ''}\")\n",
    "\n",
    "# 檢查數據完整性\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(\"\\n⚠️  數據缺失情況:\")\n",
    "    for col, missing in missing_data[missing_data > 0].items():\n",
    "        print(f\"   {col}: {missing} 個缺失值 ({missing/len(df)*100:.1f}%)\")\nelse:\n    print(\"\\n✅ 數據完整，無缺失值\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 綜合性能分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensivePerformanceAnalyzer:\n",
    "    \"\"\"綜合性能分析器\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data.copy()\n",
    "        self.data = self.data.fillna(method='ffill').fillna(0)  # 處理缺失值\n",
    "        \n",
    "        # 分類指標\n",
    "        self.system_metrics = [col for col in self.data.columns if not col.startswith('vllm_')]\n",
    "        self.vllm_metrics = [col for col in self.data.columns if col.startswith('vllm_')]\n",
    "        \n",
    "        # 性能基準線\n",
    "        self.performance_thresholds = {\n",
    "            'cpu_percent': {'good': 60, 'warning': 80, 'critical': 95},\n",
    "            'memory_percent': {'good': 70, 'warning': 85, 'critical': 95},\n",
    "            'gpu_memory_used': {'good': 70, 'warning': 85, 'critical': 95},\n",
    "            'gpu_utilization': {'good': 80, 'warning': 90, 'critical': 98}\n",
    "        }\n",
    "    \n",
    "    def calculate_derived_metrics(self):\n",
    "        \"\"\"計算衍生指標\"\"\"\n",
    "        print(\"🔧 計算衍生指標...\")\n",
    "        \n",
    "        # 請求成功率\n",
    "        if 'vllm_request_success_total' in self.data.columns and 'vllm_request_failure_total' in self.data.columns:\n",
    "            total_requests = self.data['vllm_request_success_total'] + self.data['vllm_request_failure_total']\n",
    "            self.data['success_rate'] = np.where(total_requests > 0, \n",
    "                                                self.data['vllm_request_success_total'] / total_requests * 100, 100)\n",
    "        \n",
    "        # 平均 TTFT\n",
    "        if ('vllm_time_to_first_token_seconds_sum' in self.data.columns and \n",
    "            'vllm_time_to_first_token_seconds_count' in self.data.columns):\n",
    "            count = self.data['vllm_time_to_first_token_seconds_count']\n",
    "            total_time = self.data['vllm_time_to_first_token_seconds_sum']\n",
    "            self.data['avg_ttft'] = np.where(count > 0, total_time / count, 0)\n",
    "        \n",
    "        # 請求處理速率 (QPS)\n",
    "        if 'vllm_request_success_total' in self.data.columns:\n",
    "            self.data['qps'] = self.data['vllm_request_success_total'].diff().fillna(0) / (\n",
    "                self.data.index.to_series().diff().dt.total_seconds().fillna(1)\n",
    "            )\n",
    "            self.data['qps'] = self.data['qps'].clip(lower=0)  # 移除負值\n",
    "        \n",
    "        # 系統負載指數 (綜合指標)\n",
    "        load_components = []\n",
    "        if 'cpu_percent' in self.data.columns:\n",
    "            load_components.append(self.data['cpu_percent'] / 100)\n",
    "        if 'memory_percent' in self.data.columns:\n",
    "            load_components.append(self.data['memory_percent'] / 100)\n",
    "        if 'gpu_utilization' in self.data.columns:\n",
    "            load_components.append(self.data['gpu_utilization'] / 100)\n",
    "        \n",
    "        if load_components:\n",
    "            self.data['system_load_index'] = np.mean(load_components, axis=0) * 100\n",
    "        \n",
    "        print(f\"✅ 衍生指標計算完成，新增 {len([col for col in self.data.columns if col in ['success_rate', 'avg_ttft', 'qps', 'system_load_index']])} 個指標\")\n",
    "    \n",
    "    def performance_summary_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"性能摘要統計\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        # 系統資源統計\n",
    "        for metric in self.system_metrics:\n",
    "            if metric in self.data.columns:\n",
    "                series = self.data[metric].dropna()\n",
    "                if len(series) > 0:\n",
    "                    stats[metric] = {\n",
    "                        'mean': series.mean(),\n",
    "                        'median': series.median(),\n",
    "                        'std': series.std(),\n",
    "                        'min': series.min(),\n",
    "                        'max': series.max(),\n",
    "                        'p95': series.quantile(0.95),\n",
    "                        'p99': series.quantile(0.99)\n",
    "                    }\n",
    "        \n",
    "        # vLLM 性能統計\n",
    "        derived_metrics = ['success_rate', 'avg_ttft', 'qps', 'system_load_index']\n",
    "        for metric in derived_metrics:\n",
    "            if metric in self.data.columns:\n",
    "                series = self.data[metric].dropna()\n",
    "                if len(series) > 0:\n",
    "                    stats[metric] = {\n",
    "                        'mean': series.mean(),\n",
    "                        'median': series.median(),\n",
    "                        'std': series.std(),\n",
    "                        'min': series.min(),\n",
    "                        'max': series.max(),\n",
    "                        'p95': series.quantile(0.95),\n",
    "                        'p99': series.quantile(0.99)\n",
    "                    }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def identify_performance_bottlenecks(self) -> Dict[str, Any]:\n",
    "        \"\"\"識別性能瓶頸\"\"\"\n",
    "        bottlenecks = {\n",
    "            'critical_periods': [],\n",
    "            'resource_constraints': [],\n",
    "            'performance_degradation': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # 檢查資源使用超過閾值的時間段\n",
    "        for metric, thresholds in self.performance_thresholds.items():\n",
    "            if metric in self.data.columns:\n",
    "                series = self.data[metric]\n",
    "                \n",
    "                # 檢查臨界期間\n",
    "                critical_mask = series > thresholds['critical']\n",
    "                warning_mask = series > thresholds['warning']\n",
    "                \n",
    "                if critical_mask.any():\n",
    "                    critical_periods = self._find_consecutive_periods(critical_mask)\n",
    "                    for start, end, duration in critical_periods:\n",
    "                        bottlenecks['critical_periods'].append({\n",
    "                            'metric': metric,\n",
    "                            'start_time': start,\n",
    "                            'end_time': end,\n",
    "                            'duration_seconds': duration,\n",
    "                            'max_value': series[start:end].max(),\n",
    "                            'severity': 'critical'\n",
    "                        })\n",
    "                \n",
    "                # 資源約束分析\n",
    "                high_usage_pct = (series > thresholds['warning']).mean() * 100\n",
    "                if high_usage_pct > 20:  # 超過 20% 的時間處於高使用狀態\n",
    "                    bottlenecks['resource_constraints'].append({\n",
    "                        'metric': metric,\n",
    "                        'high_usage_percentage': high_usage_pct,\n",
    "                        'avg_usage': series.mean(),\n",
    "                        'p95_usage': series.quantile(0.95)\n",
    "                    })\n",
    "        \n",
    "        # 性能衰退檢測\n",
    "        if 'qps' in self.data.columns and len(self.data) > 100:\n",
    "            qps_series = self.data['qps'].rolling(window=20).mean()\n",
    "            if len(qps_series.dropna()) > 50:\n",
    "                early_performance = qps_series.dropna().iloc[:25].mean()\n",
    "                late_performance = qps_series.dropna().iloc[-25:].mean()\n",
    "                \n",
    "                if early_performance > 0 and (late_performance / early_performance) < 0.8:\n",
    "                    bottlenecks['performance_degradation'].append({\n",
    "                        'metric': 'qps',\n",
    "                        'early_avg': early_performance,\n",
    "                        'late_avg': late_performance,\n",
    "                        'degradation_pct': (1 - late_performance / early_performance) * 100\n",
    "                    })\n",
    "        \n",
    "        # 生成建議\n",
    "        bottlenecks['recommendations'] = self._generate_recommendations(bottlenecks)\n",
    "        \n",
    "        return bottlenecks\n",
    "    \n",
    "    def _find_consecutive_periods(self, mask: pd.Series) -> List[Tuple]:\n",
    "        \"\"\"尋找連續的時間段\"\"\"\n",
    "        periods = []\n",
    "        start = None\n",
    "        \n",
    "        for i, (timestamp, value) in enumerate(mask.items()):\n",
    "            if value and start is None:\n",
    "                start = timestamp\n",
    "            elif not value and start is not None:\n",
    "                end = timestamp\n",
    "                duration = (end - start).total_seconds()\n",
    "                periods.append((start, end, duration))\n",
    "                start = None\n",
    "        \n",
    "        # 處理結尾的情況\n",
    "        if start is not None:\n",
    "            end = mask.index[-1]\n",
    "            duration = (end - start).total_seconds()\n",
    "            periods.append((start, end, duration))\n",
    "        \n",
    "        return periods\n",
    "    \n",
    "    def _generate_recommendations(self, bottlenecks: Dict) -> List[str]:\n",
    "        \"\"\"生成優化建議\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # 基於資源約束的建議\n",
    "        for constraint in bottlenecks['resource_constraints']:\n",
    "            metric = constraint['metric']\n",
    "            if metric == 'cpu_percent':\n",
    "                recommendations.append(\"CPU 使用率過高，建議優化算法或增加 CPU 核心數\")\n",
    "            elif metric == 'memory_percent':\n",
    "                recommendations.append(\"記憶體使用率過高，建議增加記憶體或優化記憶體使用\")\n",
    "            elif metric == 'gpu_memory_used':\n",
    "                recommendations.append(\"GPU 記憶體使用率過高，建議調整批次大小或模型分片\")\n",
    "            elif metric == 'gpu_utilization':\n",
    "                recommendations.append(\"GPU 使用率過高，建議增加 GPU 數量或優化計算\")\n",
    "        \n",
    "        # 基於性能衰退的建議\n",
    "        if bottlenecks['performance_degradation']:\n",
    "            recommendations.append(\"檢測到性能衰退，建議檢查模型快取和記憶體洩漏\")\n",
    "        \n",
    "        # 基於臨界期間的建議\n",
    "        if bottlenecks['critical_periods']:\n",
    "            recommendations.append(\"檢測到資源使用臨界期間，建議設置負載均衡和自動擴展\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# 初始化分析器\n",
    "analyzer = ComprehensivePerformanceAnalyzer(df)\n",
    "\n",
    "# 計算衍生指標\n",
    "analyzer.calculate_derived_metrics()\n",
    "\n",
    "print(\"✅ 綜合性能分析器已初始化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 詳細性能統計分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取性能統計\n",
    "performance_stats = analyzer.performance_summary_statistics()\n",
    "\n",
    "# 顯示詳細統計\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 詳細性能統計分析\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 系統資源統計\n",
    "print(\"\\n🖥️  系統資源使用統計:\")\n",
    "print(f\"{'指標':<20} {'平均值':<10} {'中位數':<10} {'P95':<10} {'P99':<10} {'最大值':<10} {'標準差':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in ['cpu_percent', 'memory_percent', 'gpu_memory_used', 'gpu_utilization']:\n",
    "    if metric in performance_stats:\n",
    "        stats = performance_stats[metric]\n",
    "        print(f\"{metric:<20} {stats['mean']:<10.1f} {stats['median']:<10.1f} \"\n",
    "              f\"{stats['p95']:<10.1f} {stats['p99']:<10.1f} {stats['max']:<10.1f} {stats['std']:<10.1f}\")\n",
    "\n",
    "# 性能指標統計\n",
    "print(\"\\n🚀 vLLM 性能指標統計:\")\n",
    "performance_metrics = ['success_rate', 'avg_ttft', 'qps', 'system_load_index']\n",
    "\n",
    "for metric in performance_metrics:\n",
    "    if metric in performance_stats:\n",
    "        stats = performance_stats[metric]\n",
    "        unit = '%' if metric in ['success_rate', 'system_load_index'] else ('s' if 'ttft' in metric else 'req/s')\n",
    "        print(f\"\\n   {metric}:\")\n",
    "        print(f\"     平均值: {stats['mean']:.3f} {unit}\")\n",
    "        print(f\"     中位數: {stats['median']:.3f} {unit}\")\n",
    "        print(f\"     P95: {stats['p95']:.3f} {unit}\")\n",
    "        print(f\"     P99: {stats['p99']:.3f} {unit}\")\n",
    "        print(f\"     範圍: {stats['min']:.3f} - {stats['max']:.3f} {unit}\")\n",
    "        print(f\"     變異係數: {(stats['std']/stats['mean']*100):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 性能瓶頸識別與診斷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行瓶頸分析\n",
    "bottlenecks = analyzer.identify_performance_bottlenecks()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔍 性能瓶頸診斷報告\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 臨界期間分析\n",
    "if bottlenecks['critical_periods']:\n",
    "    print(\"\\n⚠️  檢測到臨界性能期間:\")\n",
    "    for i, period in enumerate(bottlenecks['critical_periods'], 1):\n",
    "        print(f\"\\n   期間 {i}:\")\n",
    "        print(f\"     指標: {period['metric']}\")\n",
    "        print(f\"     時間: {period['start_time'].strftime('%H:%M:%S')} - {period['end_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"     持續時間: {period['duration_seconds']:.0f} 秒\")\n",
    "        print(f\"     最大值: {period['max_value']:.1f}%\")\n",
    "        print(f\"     嚴重程度: {period['severity']}\")\nelse:\n    print(\"\\n✅ 未檢測到臨界性能期間\")\n",
    "\n",
    "# 資源約束分析\n",
    "if bottlenecks['resource_constraints']:\n",
    "    print(\"\\n📈 資源約束分析:\")\n",
    "    for constraint in bottlenecks['resource_constraints']:\n",
    "        print(f\"\\n   {constraint['metric']}:\")\n",
    "        print(f\"     高使用率時間佔比: {constraint['high_usage_percentage']:.1f}%\")\n",
    "        print(f\"     平均使用率: {constraint['avg_usage']:.1f}%\")\n",
    "        print(f\"     P95 使用率: {constraint['p95_usage']:.1f}%\")\nelse:\n    print(\"\\n✅ 未發現顯著的資源約束問題\")\n",
    "\n",
    "# 性能衰退分析\n",
    "if bottlenecks['performance_degradation']:\n",
    "    print(\"\\n📉 性能衰退檢測:\")\n",
    "    for degradation in bottlenecks['performance_degradation']:\n",
    "        print(f\"\\n   {degradation['metric']}:\")\n",
    "        print(f\"     初期性能: {degradation['early_avg']:.2f}\")\n",
    "        print(f\"     後期性能: {degradation['late_avg']:.2f}\")\n",
    "        print(f\"     衰退幅度: {degradation['degradation_pct']:.1f}%\")\nelse:\n    print(\"\\n✅ 未檢測到顯著的性能衰退\")\n",
    "\n",
    "# 優化建議\n",
    "if bottlenecks['recommendations']:\n",
    "    print(\"\\n💡 優化建議:\")\n",
    "    for i, recommendation in enumerate(bottlenecks['recommendations'], 1):\n",
    "        print(f\"   {i}. {recommendation}\")\n",
    "else:\n",
    "    print(\"\\n✅ 系統運行良好，暫無特殊優化建議\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 進階時間序列分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesAnalyzer:\n",
    "    \"\"\"時間序列分析器\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "    \n",
    "    def trend_analysis(self, metric: str, window_size: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"趨勢分析\"\"\"\n",
    "        if metric not in self.data.columns:\n",
    "            return {}\n",
    "        \n",
    "        series = self.data[metric].dropna()\n",
    "        if len(series) < window_size:\n",
    "            return {}\n",
    "        \n",
    "        # 計算移動平均\n",
    "        rolling_mean = series.rolling(window=window_size).mean()\n",
    "        \n",
    "        # 線性趨勢擬合\n",
    "        x = np.arange(len(series))\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, series)\n",
    "        \n",
    "        # 趨勢強度判斷\n",
    "        trend_strength = abs(r_value)\n",
    "        if trend_strength > 0.7:\n",
    "            trend_type = \"強\" + (\"上升\" if slope > 0 else \"下降\")\n",
    "        elif trend_strength > 0.3:\n",
    "            trend_type = \"中等\" + (\"上升\" if slope > 0 else \"下降\")\n",
    "        else:\n",
    "            trend_type = \"無明顯趨勢\"\n",
    "        \n",
    "        return {\n",
    "            'slope': slope,\n",
    "            'r_squared': r_value**2,\n",
    "            'p_value': p_value,\n",
    "            'trend_type': trend_type,\n",
    "            'rolling_mean': rolling_mean,\n",
    "            'trend_line': slope * x + intercept\n",
    "        }\n",
    "    \n",
    "    def seasonality_detection(self, metric: str, period: int = None) -> Dict[str, Any]:\n",
    "        \"\"\"季節性檢測\"\"\"\n",
    "        if not STATSMODELS_AVAILABLE or metric not in self.data.columns:\n",
    "            return {}\n",
    "        \n",
    "        series = self.data[metric].dropna()\n",
    "        if len(series) < 100:  # 需要足夠的數據點\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # 自動檢測週期\n",
    "            if period is None:\n",
    "                # 假設數據間隔為分鐘級，嘗試小時週期\n",
    "                period = min(60, len(series) // 4)\n",
    "            \n",
    "            # 季節分解\n",
    "            decomposition = seasonal_decompose(\n",
    "                series, \n",
    "                model='additive', \n",
    "                period=period,\n",
    "                extrapolate_trend='freq'\n",
    "            )\n",
    "            \n",
    "            # 季節性強度\n",
    "            seasonal_strength = np.std(decomposition.seasonal) / np.std(series)\n",
    "            \n",
    "            return {\n",
    "                'has_seasonality': seasonal_strength > 0.1,\n",
    "                'seasonal_strength': seasonal_strength,\n",
    "                'period': period,\n",
    "                'trend': decomposition.trend,\n",
    "                'seasonal': decomposition.seasonal,\n",
    "                'residual': decomposition.resid\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"季節性分析失敗: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def anomaly_detection_advanced(self, metric: str, contamination: float = 0.1) -> Dict[str, Any]:\n",
    "        \"\"\"進階異常檢測\"\"\"\n",
    "        if metric not in self.data.columns:\n",
    "            return {}\n",
    "        \n",
    "        series = self.data[metric].dropna()\n",
    "        if len(series) < 20:\n",
    "            return {}\n",
    "        \n",
    "        # 準備數據\n",
    "        X = series.values.reshape(-1, 1)\n",
    "        \n",
    "        # Isolation Forest 異常檢測\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        \n",
    "        anomaly_labels = iso_forest.fit_predict(X)\n",
    "        anomaly_scores = iso_forest.decision_function(X)\n",
    "        \n",
    "        # 統計學異常檢測 (Z-score)\n",
    "        z_scores = np.abs(stats.zscore(series))\n",
    "        z_anomalies = z_scores > 3\n",
    "        \n",
    "        # 綜合異常檢測結果\n",
    "        anomalies = (anomaly_labels == -1) | z_anomalies\n",
    "        \n",
    "        anomaly_indices = series.index[anomalies]\n",
    "        anomaly_values = series[anomalies]\n",
    "        \n",
    "        return {\n",
    "            'anomaly_count': anomalies.sum(),\n",
    "            'anomaly_rate': anomalies.mean(),\n",
    "            'anomaly_timestamps': anomaly_indices,\n",
    "            'anomaly_values': anomaly_values,\n",
    "            'anomaly_scores': anomaly_scores,\n",
    "            'z_scores': z_scores\n",
    "        }\n",
    "    \n",
    "    def correlation_analysis(self, metrics: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"相關性分析\"\"\"\n",
    "        available_metrics = [m for m in metrics if m in self.data.columns]\n",
    "        \n",
    "        if len(available_metrics) < 2:\n",
    "            return {}\n",
    "        \n",
    "        # 計算相關性矩陣\n",
    "        correlation_matrix = self.data[available_metrics].corr()\n",
    "        \n",
    "        # 尋找強相關關係\n",
    "        strong_correlations = []\n",
    "        for i, metric1 in enumerate(available_metrics):\n",
    "            for j, metric2 in enumerate(available_metrics[i+1:], i+1):\n",
    "                corr_value = correlation_matrix.loc[metric1, metric2]\n",
    "                if abs(corr_value) > 0.7:  # 強相關閾值\n",
    "                    strong_correlations.append({\n",
    "                        'metric1': metric1,\n",
    "                        'metric2': metric2,\n",
    "                        'correlation': corr_value,\n",
    "                        'strength': 'strong'\n",
    "                    })\n",
    "                elif abs(corr_value) > 0.4:  # 中等相關閾值\n",
    "                    strong_correlations.append({\n",
    "                        'metric1': metric1,\n",
    "                        'metric2': metric2,\n",
    "                        'correlation': corr_value,\n",
    "                        'strength': 'moderate'\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'correlation_matrix': correlation_matrix,\n",
    "            'strong_correlations': strong_correlations\n",
    "        }\n",
    "\n",
    "# 初始化時間序列分析器\n",
    "ts_analyzer = TimeSeriesAnalyzer(analyzer.data)\n",
    "print(\"✅ 時間序列分析器已初始化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行時間序列分析\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📈 時間序列深度分析\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 選擇關鍵指標進行分析\n",
    "key_metrics = ['cpu_percent', 'memory_percent', 'gpu_utilization', 'system_load_index']\n",
    "available_metrics = [m for m in key_metrics if m in analyzer.data.columns]\n",
    "\n",
    "# 趨勢分析\n",
    "print(\"\\n📊 趨勢分析結果:\")\n",
    "for metric in available_metrics:\n",
    "    trend_result = ts_analyzer.trend_analysis(metric)\n",
    "    if trend_result:\n",
    "        print(f\"\\n   {metric}:\")\n",
    "        print(f\"     趨勢類型: {trend_result['trend_type']}\")\n",
    "        print(f\"     趨勢斜率: {trend_result['slope']:.4f} 單位/時間點\")\n",
    "        print(f\"     決定係數 (R²): {trend_result['r_squared']:.3f}\")\n",
    "        print(f\"     統計顯著性 (p值): {trend_result['p_value']:.4f}\")\n",
    "\n",
    "# 異常檢測\n",
    "print(\"\\n🔍 進階異常檢測:\")\n",
    "total_anomalies = 0\n",
    "for metric in available_metrics:\n",
    "    anomaly_result = ts_analyzer.anomaly_detection_advanced(metric)\n",
    "    if anomaly_result:\n",
    "        count = anomaly_result['anomaly_count']\n",
    "        rate = anomaly_result['anomaly_rate'] * 100\n",
    "        total_anomalies += count\n",
    "        \n",
    "        print(f\"\\n   {metric}:\")\n",
    "        print(f\"     異常點數量: {count}\")\n",
    "        print(f\"     異常率: {rate:.1f}%\")\n",
    "        \n",
    "        if count > 0:\n",
    "            worst_anomaly_idx = np.argmax(np.abs(anomaly_result['anomaly_scores']))\n",
    "            worst_timestamp = anomaly_result['anomaly_timestamps'][worst_anomaly_idx]\n",
    "            worst_value = anomaly_result['anomaly_values'].iloc[worst_anomaly_idx]\n",
    "            print(f\"     最嚴重異常: {worst_timestamp.strftime('%H:%M:%S')} (值: {worst_value:.1f})\")\n",
    "\n",
    "print(f\"\\n   總異常點數: {total_anomalies}\")\n",
    "\n",
    "# 相關性分析\n",
    "correlation_result = ts_analyzer.correlation_analysis(available_metrics)\n",
    "if correlation_result and correlation_result['strong_correlations']:\n",
    "    print(\"\\n🔗 強相關關係分析:\")\n",
    "    for corr in correlation_result['strong_correlations']:\n",
    "        print(f\"   {corr['metric1']} ↔ {corr['metric2']}: {corr['correlation']:.3f} ({corr['strength']})\")\nelse:\n    print(\"\\n   未發現顯著的強相關關係\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 高級性能視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_performance_dashboard():\n",
    "    \"\"\"創建綜合性能分析儀表板\"\"\"\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20, 16))\n",
    "    fig.suptitle('vLLM 綜合性能分析儀表板', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. 系統資源使用趨勢\n",
    "    ax1 = axes[0, 0]\n",
    "    for metric in ['cpu_percent', 'memory_percent', 'gpu_utilization']:\n",
    "        if metric in analyzer.data.columns:\n",
    "            ax1.plot(analyzer.data.index, analyzer.data[metric], \n",
    "                    label=metric.replace('_', ' ').title(), alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax1.set_title('系統資源使用趨勢', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('使用率 (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # 2. 性能指標分佈\n",
    "    ax2 = axes[0, 1]\n",
    "    performance_metrics = ['cpu_percent', 'memory_percent', 'gpu_utilization']\n",
    "    available_perf_metrics = [m for m in performance_metrics if m in analyzer.data.columns]\n",
    "    \n",
    "    if available_perf_metrics:\n",
    "        data_for_box = [analyzer.data[metric].dropna() for metric in available_perf_metrics]\n",
    "        labels = [m.replace('_', ' ').title() for m in available_perf_metrics]\n",
    "        \n",
    "        box_plot = ax2.boxplot(data_for_box, labels=labels, patch_artist=True)\n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "        for patch, color in zip(box_plot['boxes'], colors[:len(box_plot['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "    \n",
    "    ax2.set_title('性能指標分佈', fontweight='bold', fontsize=14)\n",
    "    ax2.set_ylabel('使用率 (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. vLLM 請求處理性能\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'qps' in analyzer.data.columns:\n",
    "        qps_data = analyzer.data['qps'].rolling(window=10).mean()\n",
    "        ax3.plot(analyzer.data.index, qps_data, color='purple', linewidth=2, label='QPS (10-point avg)')\n",
    "        ax3.fill_between(analyzer.data.index, qps_data, alpha=0.3, color='purple')\n",
    "    \n",
    "    if 'vllm_num_requests_running' in analyzer.data.columns:\n",
    "        ax3_twin = ax3.twinx()\n",
    "        ax3_twin.plot(analyzer.data.index, analyzer.data['vllm_num_requests_running'], \n",
    "                     color='orange', linewidth=2, alpha=0.7, label='Running Requests')\n",
    "        ax3_twin.set_ylabel('Active Requests', color='orange')\n",
    "        ax3_twin.tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    ax3.set_title('vLLM 請求處理性能', fontweight='bold', fontsize=14)\n",
    "    ax3.set_ylabel('QPS')\n",
    "    ax3.legend(loc='upper left')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 系統負載指數與異常點\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'system_load_index' in analyzer.data.columns:\n",
    "        load_data = analyzer.data['system_load_index']\n",
    "        ax4.plot(analyzer.data.index, load_data, color='red', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # 標記異常點\n",
    "        anomaly_result = ts_analyzer.anomaly_detection_advanced('system_load_index')\n",
    "        if anomaly_result and len(anomaly_result['anomaly_timestamps']) > 0:\n",
    "            ax4.scatter(anomaly_result['anomaly_timestamps'], \n",
    "                       anomaly_result['anomaly_values'],\n",
    "                       color='red', s=50, alpha=0.8, marker='x', label=f\"異常點 ({len(anomaly_result['anomaly_timestamps'])})\")\n",
    "            ax4.legend()\n",
    "        \n",
    "        # 添加警告線\n",
    "        ax4.axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='警告線 (80%)')\n",
    "        ax4.axhline(y=95, color='red', linestyle='--', alpha=0.7, label='臨界線 (95%)')\n",
    "    \n",
    "    ax4.set_title('系統負載指數與異常檢測', fontweight='bold', fontsize=14)\n",
    "    ax4.set_ylabel('負載指數 (%)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. 相關性熱力圖\n",
    "    ax5 = axes[2, 0]\n",
    "    correlation_metrics = ['cpu_percent', 'memory_percent', 'gpu_utilization', 'gpu_memory_used']\n",
    "    available_corr_metrics = [m for m in correlation_metrics if m in analyzer.data.columns]\n",
    "    \n",
    "    if len(available_corr_metrics) > 1:\n",
    "        corr_matrix = analyzer.data[available_corr_metrics].corr()\n",
    "        im = ax5.imshow(corr_matrix, cmap='RdYlBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "        \n",
    "        # 添加數值標籤\n",
    "        for i in range(len(available_corr_metrics)):\n",
    "            for j in range(len(available_corr_metrics)):\n",
    "                text = ax5.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "        \n",
    "        ax5.set_xticks(range(len(available_corr_metrics)))\n",
    "        ax5.set_yticks(range(len(available_corr_metrics)))\n",
    "        ax5.set_xticklabels([m.replace('_', '\\n') for m in available_corr_metrics], rotation=45)\n",
    "        ax5.set_yticklabels([m.replace('_', '\\n') for m in available_corr_metrics])\n",
    "        \n",
    "        # 添加顏色條\n",
    "        cbar = plt.colorbar(im, ax=ax5, shrink=0.8)\n",
    "        cbar.set_label('相關係數', rotation=270, labelpad=20)\n",
    "    \n",
    "    ax5.set_title('指標相關性分析', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # 6. 性能摘要雷達圖\n",
    "    ax6 = axes[2, 1]\n",
    "    \n",
    "    # 計算性能評分 (0-100)\n",
    "    scores = {}\n",
    "    labels = []\n",
    "    values = []\n",
    "    \n",
    "    if 'success_rate' in analyzer.data.columns:\n",
    "        scores['Success Rate'] = analyzer.data['success_rate'].mean()\n",
    "    \n",
    "    # CPU 效率 (100 - 平均使用率)\n",
    "    if 'cpu_percent' in analyzer.data.columns:\n",
    "        scores['CPU Efficiency'] = max(0, 100 - analyzer.data['cpu_percent'].mean())\n",
    "    \n",
    "    # Memory 效率\n",
    "    if 'memory_percent' in analyzer.data.columns:\n",
    "        scores['Memory Efficiency'] = max(0, 100 - analyzer.data['memory_percent'].mean())\n",
    "    \n",
    "    # GPU 效率\n",
    "    if 'gpu_utilization' in analyzer.data.columns:\n",
    "        scores['GPU Efficiency'] = max(0, 100 - analyzer.data['gpu_utilization'].mean())\n",
    "    \n",
    "    # 穩定性 (100 - 變異係數)\n",
    "    if 'system_load_index' in analyzer.data.columns:\n",
    "        cv = analyzer.data['system_load_index'].std() / analyzer.data['system_load_index'].mean()\n",
    "        scores['Stability'] = max(0, 100 - cv * 100)\n",
    "    \n",
    "    if scores:\n",
    "        labels = list(scores.keys())\n",
    "        values = list(scores.values())\n",
    "        \n",
    "        # 雷達圖\n",
    "        angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "        values += values[:1]  # 閉合圖形\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        ax6.plot(angles, values, 'o-', linewidth=2, label='Performance Score')\n",
    "        ax6.fill(angles, values, alpha=0.25)\n",
    "        ax6.set_xticks(angles[:-1])\n",
    "        ax6.set_xticklabels(labels)\n",
    "        ax6.set_ylim(0, 100)\n",
    "        ax6.grid(True)\n",
    "        \n",
    "        # 添加分數標籤\n",
    "        for angle, value, label in zip(angles[:-1], values[:-1], labels):\n",
    "            ax6.text(angle, value + 5, f'{value:.0f}', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax6.set_title('性能評分雷達圖', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 生成綜合性能儀表板\n",
    "print(\"📊 生成綜合性能分析儀表板...\")\n",
    "create_comprehensive_performance_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 負載測試與基準評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTestAnalyzer:\n",
    "    \"\"\"負載測試分析器\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        self.baseline_metrics = self._calculate_baseline()\n",
    "    \n",
    "    def _calculate_baseline(self) -> Dict[str, float]:\n",
    "        \"\"\"計算基準線性能指標\"\"\"\n",
    "        baseline = {}\n",
    "        \n",
    "        # 取前 25% 的數據作為基準線\n",
    "        baseline_data = self.data.iloc[:len(self.data)//4]\n",
    "        \n",
    "        for col in self.data.columns:\n",
    "            if baseline_data[col].dtype in ['int64', 'float64']:\n",
    "                baseline[col] = baseline_data[col].mean()\n",
    "        \n",
    "        return baseline\n",
    "    \n",
    "    def analyze_load_patterns(self) -> Dict[str, Any]:\n",
    "        \"\"\"分析負載模式\"\"\"\n",
    "        patterns = {\n",
    "            'peak_periods': [],\n",
    "            'low_periods': [],\n",
    "            'load_distribution': {},\n",
    "            'performance_under_load': {}\n",
    "        }\n",
    "        \n",
    "        if 'system_load_index' in self.data.columns:\n",
    "            load_series = self.data['system_load_index']\n",
    "            \n",
    "            # 識別高負載和低負載期間\n",
    "            high_load_threshold = load_series.quantile(0.8)\n",
    "            low_load_threshold = load_series.quantile(0.2)\n",
    "            \n",
    "            high_load_mask = load_series > high_load_threshold\n",
    "            low_load_mask = load_series < low_load_threshold\n",
    "            \n",
    "            # 尋找連續的高負載期間\n",
    "            patterns['peak_periods'] = self._find_consecutive_periods(high_load_mask)\n",
    "            patterns['low_periods'] = self._find_consecutive_periods(low_load_mask)\n",
    "            \n",
    "            # 負載分佈分析\n",
    "            patterns['load_distribution'] = {\n",
    "                'mean': load_series.mean(),\n",
    "                'median': load_series.median(),\n",
    "                'std': load_series.std(),\n",
    "                'min': load_series.min(),\n",
    "                'max': load_series.max(),\n",
    "                'q25': load_series.quantile(0.25),\n",
    "                'q75': load_series.quantile(0.75),\n",
    "                'high_load_percentage': (high_load_mask.sum() / len(load_series)) * 100,\n",
    "                'low_load_percentage': (low_load_mask.sum() / len(load_series)) * 100\n",
    "            }\n",
    "            \n",
    "            # 不同負載下的性能分析\n",
    "            high_load_data = self.data[high_load_mask]\n",
    "            low_load_data = self.data[low_load_mask]\n",
    "            medium_load_data = self.data[~(high_load_mask | low_load_mask)]\n",
    "            \n",
    "            for load_type, data_subset in [('high', high_load_data), ('medium', medium_load_data), ('low', low_load_data)]:\n",
    "                if len(data_subset) > 0:\n",
    "                    patterns['performance_under_load'][load_type] = {\n",
    "                        'avg_cpu': data_subset['cpu_percent'].mean() if 'cpu_percent' in data_subset.columns else 0,\n",
    "                        'avg_memory': data_subset['memory_percent'].mean() if 'memory_percent' in data_subset.columns else 0,\n",
    "                        'avg_gpu': data_subset['gpu_utilization'].mean() if 'gpu_utilization' in data_subset.columns else 0,\n",
    "                        'avg_qps': data_subset['qps'].mean() if 'qps' in data_subset.columns else 0,\n",
    "                        'data_points': len(data_subset)\n",
    "                    }\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _find_consecutive_periods(self, mask: pd.Series) -> List[Dict]:\n",
    "        \"\"\"尋找連續的時間段\"\"\"\n",
    "        periods = []\n",
    "        start = None\n",
    "        \n",
    "        for timestamp, value in mask.items():\n",
    "            if value and start is None:\n",
    "                start = timestamp\n",
    "            elif not value and start is not None:\n",
    "                end = timestamp\n",
    "                duration = (end - start).total_seconds()\n",
    "                periods.append({\n",
    "                    'start': start,\n",
    "                    'end': end,\n",
    "                    'duration_seconds': duration\n",
    "                })\n",
    "                start = None\n",
    "        \n",
    "        # 處理結尾的情況\n",
    "        if start is not None:\n",
    "            end = mask.index[-1]\n",
    "            duration = (end - start).total_seconds()\n",
    "            periods.append({\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'duration_seconds': duration\n",
    "            })\n",
    "        \n",
    "        return periods\n",
    "    \n",
    "    def capacity_planning_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"容量規劃分析\"\"\"\n",
    "        capacity_analysis = {\n",
    "            'current_utilization': {},\n",
    "            'bottleneck_analysis': {},\n",
    "            'scaling_recommendations': []\n",
    "        }\n",
    "        \n",
    "        # 當前資源使用情況\n",
    "        resource_metrics = ['cpu_percent', 'memory_percent', 'gpu_utilization', 'gpu_memory_used']\n",
    "        \n",
    "        for metric in resource_metrics:\n",
    "            if metric in self.data.columns:\n",
    "                series = self.data[metric]\n",
    "                capacity_analysis['current_utilization'][metric] = {\n",
    "                    'average': series.mean(),\n",
    "                    'peak': series.max(),\n",
    "                    'p95': series.quantile(0.95),\n",
    "                    'headroom': 100 - series.quantile(0.95),  # 剩餘容量\n",
    "                    'utilization_trend': 'increasing' if series.tail(20).mean() > series.head(20).mean() else 'stable'\n",
    "                }\n",
    "        \n",
    "        # 瓶頸分析\n",
    "        bottlenecks = []\n",
    "        for metric, utilization in capacity_analysis['current_utilization'].items():\n",
    "            if utilization['p95'] > 90:\n",
    "                bottlenecks.append({\n",
    "                    'resource': metric,\n",
    "                    'severity': 'critical',\n",
    "                    'p95_usage': utilization['p95'],\n",
    "                    'headroom': utilization['headroom']\n",
    "                })\n",
    "            elif utilization['p95'] > 80:\n",
    "                bottlenecks.append({\n",
    "                    'resource': metric,\n",
    "                    'severity': 'warning',\n",
    "                    'p95_usage': utilization['p95'],\n",
    "                    'headroom': utilization['headroom']\n",
    "                })\n",
    "        \n",
    "        capacity_analysis['bottleneck_analysis'] = bottlenecks\n",
    "        \n",
    "        # 擴展建議\n",
    "        recommendations = []\n",
    "        \n",
    "        for bottleneck in bottlenecks:\n",
    "            resource = bottleneck['resource']\n",
    "            if resource == 'cpu_percent':\n",
    "                if bottleneck['severity'] == 'critical':\n",
    "                    recommendations.append(\"立即增加 CPU 核心數或優化 CPU 密集型運算\")\n",
    "                else:\n",
    "                    recommendations.append(\"考慮在未來 1-2 個月內增加 CPU 資源\")\n",
    "            \n",
    "            elif resource == 'memory_percent':\n",
    "                if bottleneck['severity'] == 'critical':\n",
    "                    recommendations.append(\"立即增加記憶體容量或優化記憶體使用\")\n",
    "                else:\n",
    "                    recommendations.append(\"計劃增加記憶體容量以應對未來需求\")\n",
    "            \n",
    "            elif resource in ['gpu_utilization', 'gpu_memory_used']:\n",
    "                if bottleneck['severity'] == 'critical':\n",
    "                    recommendations.append(\"立即增加 GPU 資源或實施模型分片\")\n",
    "                else:\n",
    "                    recommendations.append(\"準備 GPU 擴展方案以應對負載增長\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"當前資源配置充足，持續監控即可\")\n",
    "        \n",
    "        capacity_analysis['scaling_recommendations'] = recommendations\n",
    "        \n",
    "        return capacity_analysis\n",
    "\n",
    "# 初始化負載測試分析器\n",
    "load_analyzer = LoadTestAnalyzer(analyzer.data)\n",
    "print(\"✅ 負載測試分析器已初始化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行負載模式分析\n",
    "load_patterns = load_analyzer.analyze_load_patterns()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📈 負載模式分析報告\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 負載分佈統計\n",
    "if 'load_distribution' in load_patterns and load_patterns['load_distribution']:\n",
    "    dist = load_patterns['load_distribution']\n",
    "    print(\"\\n🔍 負載分佈統計:\")\n",
    "    print(f\"   平均負載: {dist['mean']:.1f}%\")\n",
    "    print(f\"   中位數負載: {dist['median']:.1f}%\")\n",
    "    print(f\"   負載範圍: {dist['min']:.1f}% - {dist['max']:.1f}%\")\n",
    "    print(f\"   負載變異性: {dist['std']:.1f}% (標準差)\")\n",
    "    print(f\"   高負載時間佔比: {dist['high_load_percentage']:.1f}%\")\n",
    "    print(f\"   低負載時間佔比: {dist['low_load_percentage']:.1f}%\")\n",
    "\n",
    "# 峰值期間分析\n",
    "if load_patterns['peak_periods']:\n",
    "    print(f\"\\n⚡ 檢測到 {len(load_patterns['peak_periods'])} 個高負載期間:\")\n",
    "    for i, period in enumerate(load_patterns['peak_periods'][:5], 1):  # 顯示前5個\n",
    "        print(f\"   期間 {i}: {period['start'].strftime('%H:%M:%S')} - {period['end'].strftime('%H:%M:%S')} \"\n",
    "              f\"(持續 {period['duration_seconds']:.0f} 秒)\")\nelse:\n    print(\"\\n✅ 未檢測到顯著的高負載期間\")\n",
    "\n",
    "# 不同負載下的性能表現\n",
    "if 'performance_under_load' in load_patterns:\n",
    "    print(\"\\n🎯 不同負載下的性能表現:\")\n",
    "    for load_type, perf in load_patterns['performance_under_load'].items():\n",
    "        if perf['data_points'] > 0:\n",
    "            print(f\"\\n   {load_type.upper()} 負載 ({perf['data_points']} 個數據點):\")\n",
    "            print(f\"     平均 CPU: {perf['avg_cpu']:.1f}%\")\n",
    "            print(f\"     平均記憶體: {perf['avg_memory']:.1f}%\")\n",
    "            print(f\"     平均 GPU: {perf['avg_gpu']:.1f}%\")\n",
    "            print(f\"     平均 QPS: {perf['avg_qps']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行容量規劃分析\n",
    "capacity_analysis = load_analyzer.capacity_planning_analysis()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏗️  容量規劃分析報告\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 當前資源使用情況\n",
    "print(\"\\n📊 當前資源使用情況:\")\n",
    "print(f\"{'資源':<15} {'平均使用':<10} {'峰值使用':<10} {'P95使用':<10} {'剩餘容量':<10} {'使用趨勢':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for resource, utilization in capacity_analysis['current_utilization'].items():\n",
    "    print(f\"{resource:<15} {utilization['average']:<10.1f} {utilization['peak']:<10.1f} \"\n",
    "          f\"{utilization['p95']:<10.1f} {utilization['headroom']:<10.1f} {utilization['utilization_trend']:<10}\")\n",
    "\n",
    "# 瓶頸分析\n",
    "if capacity_analysis['bottleneck_analysis']:\n",
    "    print(\"\\n⚠️  資源瓶頸分析:\")\n",
    "    for bottleneck in capacity_analysis['bottleneck_analysis']:\n",
    "        severity_icon = \"🔴\" if bottleneck['severity'] == 'critical' else \"🟡\"\n",
    "        print(f\"   {severity_icon} {bottleneck['resource']}: {bottleneck['severity'].upper()} \"\n",
    "              f\"(P95: {bottleneck['p95_usage']:.1f}%, 剩餘: {bottleneck['headroom']:.1f}%)\")\nelse:\n    print(\"\\n✅ 未發現資源瓶頸\")\n",
    "\n",
    "# 擴展建議\n",
    "print(\"\\n💡 容量擴展建議:\")\n",
    "for i, recommendation in enumerate(capacity_analysis['scaling_recommendations'], 1):\n",
    "    print(f\"   {i}. {recommendation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能優化建議生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceOptimizationAdvisor:\n",
    "    \"\"\"性能優化建議生成器\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer, bottlenecks, load_patterns, capacity_analysis):\n",
    "        self.analyzer = analyzer\n",
    "        self.bottlenecks = bottlenecks\n",
    "        self.load_patterns = load_patterns\n",
    "        self.capacity_analysis = capacity_analysis\n",
    "        \n",
    "    def generate_comprehensive_recommendations(self) -> Dict[str, Any]:\n",
    "        \"\"\"生成綜合優化建議\"\"\"\n",
    "        recommendations = {\n",
    "            'immediate_actions': [],\n",
    "            'short_term_optimizations': [],\n",
    "            'long_term_planning': [],\n",
    "            'monitoring_improvements': [],\n",
    "            'performance_kpis': {},\n",
    "            'risk_assessment': {}\n",
    "        }\n",
    "        \n",
    "        # 立即行動建議\n",
    "        recommendations['immediate_actions'] = self._generate_immediate_actions()\n",
    "        \n",
    "        # 短期優化建議\n",
    "        recommendations['short_term_optimizations'] = self._generate_short_term_optimizations()\n",
    "        \n",
    "        # 長期規劃建議\n",
    "        recommendations['long_term_planning'] = self._generate_long_term_planning()\n",
    "        \n",
    "        # 監控改進建議\n",
    "        recommendations['monitoring_improvements'] = self._generate_monitoring_improvements()\n",
    "        \n",
    "        # 性能 KPI 建議\n",
    "        recommendations['performance_kpis'] = self._generate_performance_kpis()\n",
    "        \n",
    "        # 風險評估\n",
    "        recommendations['risk_assessment'] = self._assess_risks()\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _generate_immediate_actions(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"生成立即行動建議\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        # 檢查臨界期間\n",
    "        if self.bottlenecks['critical_periods']:\n",
    "            actions.append({\n",
    "                'action': '設置自動告警',\n",
    "                'description': '為臨界資源使用期間設置實時告警機制',\n",
    "                'priority': 'high',\n",
    "                'estimated_effort': '1-2 小時'\n",
    "            })\n",
    "        \n",
    "        # 檢查資源約束\n",
    "        critical_resources = [c for c in self.bottlenecks['resource_constraints'] \n",
    "                            if c['high_usage_percentage'] > 80]\n",
    "        \n",
    "        if critical_resources:\n",
    "            for resource in critical_resources:\n",
    "                if resource['metric'] == 'gpu_memory_used':\n",
    "                    actions.append({\n",
    "                        'action': '調整 GPU 記憶體配置',\n",
    "                        'description': '降低批次大小或啟用梯度累積來減少 GPU 記憶體使用',\n",
    "                        'priority': 'high',\n",
    "                        'estimated_effort': '30 分鐘'\n",
    "                    })\n",
    "                elif resource['metric'] == 'cpu_percent':\n",
    "                    actions.append({\n",
    "                        'action': '優化 CPU 使用',\n",
    "                        'description': '檢查並優化 CPU 密集型操作，考慮並行化處理',\n",
    "                        'priority': 'medium',\n",
    "                        'estimated_effort': '2-4 小時'\n",
    "                    })\n",
    "        \n",
    "        # 檢查異常檢測結果\n",
    "        if not actions:  # 如果沒有緊急問題\n",
    "            actions.append({\n",
    "                'action': '驗證當前配置',\n",
    "                'description': '系統運行正常，驗證當前配置是否為最佳實踐',\n",
    "                'priority': 'low',\n",
    "                'estimated_effort': '1 小時'\n",
    "            })\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def _generate_short_term_optimizations(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"生成短期優化建議\"\"\"\n",
    "        optimizations = []\n",
    "        \n",
    "        # 基於負載模式的優化\n",
    "        if self.load_patterns.get('peak_periods'):\n",
    "            optimizations.append({\n",
    "                'optimization': '實施負載均衡',\n",
    "                'description': '在高負載期間自動分散請求或調整服務容量',\n",
    "                'expected_benefit': '減少 20-30% 的響應延遲',\n",
    "                'timeline': '1-2 週'\n",
    "            })\n",
    "        \n",
    "        # 記憶體優化\n",
    "        if 'memory_percent' in self.analyzer.data.columns:\n",
    "            avg_memory = self.analyzer.data['memory_percent'].mean()\n",
    "            if avg_memory > 70:\n",
    "                optimizations.append({\n",
    "                    'optimization': '記憶體使用優化',\n",
    "                    'description': '實施記憶體池管理和垃圾回收優化',\n",
    "                    'expected_benefit': '降低 10-15% 的記憶體使用',\n",
    "                    'timeline': '1 週'\n",
    "                })\n",
    "        \n",
    "        # 快取優化\n",
    "        optimizations.append({\n",
    "            'optimization': 'KV 快取優化',\n",
    "            'description': '調整 KV 快取大小和策略以提升推理效率',\n",
    "            'expected_benefit': '提升 15-25% 的吞吐量',\n",
    "            'timeline': '3-5 天'\n",
    "        })\n",
    "        \n",
    "        # 並發優化\n",
    "        if 'qps' in self.analyzer.data.columns:\n",
    "            avg_qps = self.analyzer.data['qps'].mean()\n",
    "            if avg_qps < 10:  # 假設期望 QPS\n",
    "                optimizations.append({\n",
    "                    'optimization': '並發處理優化',\n",
    "                    'description': '調整並發數和批次處理策略',\n",
    "                    'expected_benefit': '提升 30-50% 的吞吐量',\n",
    "                    'timeline': '1 週'\n",
    "                })\n",
    "        \n",
    "        return optimizations\n",
    "    \n",
    "    def _generate_long_term_planning(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"生成長期規劃建議\"\"\"\n",
    "        planning = []\n",
    "        \n",
    "        # 基於容量分析的規劃\n",
    "        bottlenecks = self.capacity_analysis.get('bottleneck_analysis', [])\n",
    "        \n",
    "        if bottlenecks:\n",
    "            planning.append({\n",
    "                'plan': '基礎設施擴展',\n",
    "                'description': '根據資源瓶頸分析制定硬體擴展計劃',\n",
    "                'timeline': '3-6 個月',\n",
    "                'investment': '中等到高等'\n",
    "            })\n",
    "        \n",
    "        # 架構升級\n",
    "        planning.append({\n",
    "            'plan': '分散式架構升級',\n",
    "            'description': '實施多節點分散式推理架構以提升可擴展性',\n",
    "            'timeline': '6-12 個月',\n",
    "            'investment': '高等'\n",
    "        })\n",
    "        \n",
    "        # 自動化運維\n",
    "        planning.append({\n",
    "            'plan': 'DevOps 自動化',\n",
    "            'description': '建立自動化監控、部署和擴縮容機制',\n",
    "            'timeline': '2-4 個月',\n",
    "            'investment': '中等'\n",
    "        })\n",
    "        \n",
    "        # 模型優化\n",
    "        planning.append({\n",
    "            'plan': '模型壓縮與量化',\n",
    "            'description': '實施模型量化、蒸餾和剪枝以提升推理效率',\n",
    "            'timeline': '4-8 個月',\n",
    "            'investment': '中等'\n",
    "        })\n",
    "        \n",
    "        return planning\n",
    "    \n",
    "    def _generate_monitoring_improvements(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"生成監控改進建議\"\"\"\n",
    "        improvements = [\n",
    "            {\n",
    "                'improvement': '增加業務層監控',\n",
    "                'description': '監控模型准確性、用戶滿意度等業務指標',\n",
    "                'priority': 'high'\n",
    "            },\n",
    "            {\n",
    "                'improvement': '實施分散式追蹤',\n",
    "                'description': '使用 Jaeger 或 Zipkin 追蹤請求生命週期',\n",
    "                'priority': 'medium'\n",
    "            },\n",
    "            {\n",
    "                'improvement': '建立自定義指標',\n",
    "                'description': '添加特定於業務的 KPI 監控指標',\n",
    "                'priority': 'medium'\n",
    "            },\n",
    "            {\n",
    "                'improvement': '強化告警機制',\n",
    "                'description': '設置多層級告警和自動響應機制',\n",
    "                'priority': 'high'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return improvements\n",
    "    \n",
    "    def _generate_performance_kpis(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"生成性能 KPI 建議\"\"\"\n",
    "        kpis = {\n",
    "            'latency_targets': {\n",
    "                'p50_response_time': 1.0,  # 秒\n",
    "                'p95_response_time': 2.0,\n",
    "                'p99_response_time': 5.0,\n",
    "                'ttft_target': 0.5\n",
    "            },\n",
    "            'throughput_targets': {\n",
    "                'min_qps': 10,\n",
    "                'target_qps': 50,\n",
    "                'peak_qps': 100\n",
    "            },\n",
    "            'resource_targets': {\n",
    "                'max_cpu_usage': 80,  # %\n",
    "                'max_memory_usage': 85,\n",
    "                'max_gpu_usage': 90,\n",
    "                'target_gpu_utilization': 75\n",
    "            },\n",
    "            'reliability_targets': {\n",
    "                'min_success_rate': 99.0,  # %\n",
    "                'max_error_rate': 1.0,\n",
    "                'target_availability': 99.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return kpis\n",
    "    \n",
    "    def _assess_risks(self) -> Dict[str, Any]:\n",
    "        \"\"\"評估風險\"\"\"\n",
    "        risks = {\n",
    "            'high_priority': [],\n",
    "            'medium_priority': [],\n",
    "            'low_priority': []\n",
    "        }\n",
    "        \n",
    "        # 檢查高風險情況\n",
    "        if self.bottlenecks['critical_periods']:\n",
    "            risks['high_priority'].append({\n",
    "                'risk': '系統過載風險',\n",
    "                'description': '檢測到臨界資源使用期間，可能導致服務中斷',\n",
    "                'mitigation': '實施自動擴縮容和負載均衡'\n",
    "            })\n",
    "        \n",
    "        # 檢查性能衰退風險\n",
    "        if self.bottlenecks['performance_degradation']:\n",
    "            risks['medium_priority'].append({\n",
    "                'risk': '性能衰退風險',\n",
    "                'description': '檢測到性能下降趨勢，可能影響用戶體驗',\n",
    "                'mitigation': '定期性能基準測試和優化'\n",
    "            })\n",
    "        \n",
    "        # 檢查容量風險\n",
    "        critical_bottlenecks = [b for b in self.capacity_analysis.get('bottleneck_analysis', []) \n",
    "                              if b['severity'] == 'critical']\n",
    "        \n",
    "        if critical_bottlenecks:\n",
    "            risks['high_priority'].append({\n",
    "                'risk': '容量不足風險',\n",
    "                'description': '關鍵資源接近容量上限',\n",
    "                'mitigation': '制定緊急擴容計劃'\n",
    "            })\n",
    "        \n",
    "        # 如果沒有高風險，添加一些預防性建議\n",
    "        if not risks['high_priority'] and not risks['medium_priority']:\n",
    "            risks['low_priority'].append({\n",
    "                'risk': '監控覆蓋不足風險',\n",
    "                'description': '當前系統運行良好，但建議加強監控覆蓋',\n",
    "                'mitigation': '擴展監控指標和告警機制'\n",
    "            })\n",
    "        \n",
    "        return risks\n",
    "\n",
    "# 初始化優化建議生成器\n",
    "advisor = PerformanceOptimizationAdvisor(\n",
    "    analyzer, bottlenecks, load_patterns, capacity_analysis\n",
    ")\n",
    "\n",
    "# 生成綜合建議\n",
    "optimization_recommendations = advisor.generate_comprehensive_recommendations()\n",
    "\n",
    "print(\"✅ 性能優化建議生成完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示綜合優化建議\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 vLLM 性能優化建議報告\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 立即行動建議\n",
    "print(\"\\n🚨 立即行動建議:\")\n",
    "for i, action in enumerate(optimization_recommendations['immediate_actions'], 1):\n",
    "    priority_icon = \"🔴\" if action['priority'] == 'high' else \"🟡\" if action['priority'] == 'medium' else \"🟢\"\n",
    "    print(f\"\\n   {i}. {priority_icon} {action['action']}\")\n",
    "    print(f\"      描述: {action['description']}\")\n",
    "    print(f\"      優先級: {action['priority'].upper()}\")\n",
    "    print(f\"      預估工作量: {action['estimated_effort']}\")\n",
    "\n",
    "# 短期優化建議\n",
    "print(\"\\n📈 短期優化建議 (1-4 週):\")\n",
    "for i, optimization in enumerate(optimization_recommendations['short_term_optimizations'], 1):\n",
    "    print(f\"\\n   {i}. {optimization['optimization']}\")\n",
    "    print(f\"      描述: {optimization['description']}\")\n",
    "    print(f\"      預期效益: {optimization['expected_benefit']}\")\n",
    "    print(f\"      時間線: {optimization['timeline']}\")\n",
    "\n",
    "# 長期規劃建議\n",
    "print(\"\\n🏗️  長期規劃建議 (3-12 個月):\")\n",
    "for i, plan in enumerate(optimization_recommendations['long_term_planning'], 1):\n",
    "    investment_icon = \"💰\" if plan['investment'] == '高等' else \"💵\" if plan['investment'] == '中等' else \"💴\"\n",
    "    print(f\"\\n   {i}. {investment_icon} {plan['plan']}\")\n",
    "    print(f\"      描述: {plan['description']}\")\n",
    "    print(f\"      時間線: {plan['timeline']}\")\n",
    "    print(f\"      投資級別: {plan['investment']}\")\n",
    "\n",
    "# 監控改進建議\n",
    "print(\"\\n📊 監控改進建議:\")\n",
    "for i, improvement in enumerate(optimization_recommendations['monitoring_improvements'], 1):\n",
    "    priority_icon = \"🔴\" if improvement['priority'] == 'high' else \"🟡\" if improvement['priority'] == 'medium' else \"🟢\"\n",
    "    print(f\"   {i}. {priority_icon} {improvement['improvement']}\")\n",
    "    print(f\"      {improvement['description']}\")\n",
    "\n",
    "# 性能 KPI 建議\n",
    "print(\"\\n🎯 建議性能 KPI 目標:\")\n",
    "kpis = optimization_recommendations['performance_kpis']\n",
    "\n",
    "print(\"\\n   📊 延遲目標:\")\n",
    "for metric, target in kpis['latency_targets'].items():\n",
    "    unit = '秒' if 'time' in metric or 'ttft' in metric else ''\n",
    "    print(f\"     {metric}: ≤ {target} {unit}\")\n",
    "\n",
    "print(\"\\n   🚀 吞吐量目標:\")\n",
    "for metric, target in kpis['throughput_targets'].items():\n",
    "    print(f\"     {metric}: {target} QPS\")\n",
    "\n",
    "print(\"\\n   💻 資源使用目標:\")\n",
    "for metric, target in kpis['resource_targets'].items():\n",
    "    print(f\"     {metric}: {target}%\")\n",
    "\n",
    "# 風險評估\n",
    "print(\"\\n⚠️  風險評估:\")\n",
    "risks = optimization_recommendations['risk_assessment']\n",
    "\n",
    "for priority in ['high_priority', 'medium_priority', 'low_priority']:\n",
    "    if risks[priority]:\n",
    "        priority_name = priority.replace('_', ' ').title()\n",
    "        priority_icon = \"🔴\" if priority == 'high_priority' else \"🟡\" if priority == 'medium_priority' else \"🟢\"\n",
    "        print(f\"\\n   {priority_icon} {priority_name} 風險:\")\n",
    "        \n",
    "        for risk in risks[priority]:\n",
    "            print(f\"     • {risk['risk']}\")\n",
    "            print(f\"       {risk['description']}\")\n",
    "            print(f\"       緩解措施: {risk['mitigation']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 報告總結: 基於當前監控數據，系統整體運行\" + \n",
    "      (\"需要關注\" if risks['high_priority'] else \"良好\") + \n",
    "      \"。建議優先執行立即行動建議，並制定短期優化計劃。\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實驗總結\n",
    "\n",
    "本實驗完成了全面的 vLLM 性能分析與診斷，建立了完整的性能評估框架：\n",
    "\n",
    "### ✅ 核心成果\n",
    "\n",
    "1. **深度性能分析**\n",
    "   - 多維度性能統計分析\n",
    "   - 歷史趨勢與模式識別\n",
    "   - 異常檢測與根因分析\n",
    "\n",
    "2. **智能瓶頸診斷**\n",
    "   - 自動識別資源約束\n",
    "   - 臨界期間檢測\n",
    "   - 性能衰退分析\n",
    "\n",
    "3. **負載模式分析**\n",
    "   - 峰值與低谷期間識別\n",
    "   - 不同負載下的性能表現\n",
    "   - 負載分佈統計\n",
    "\n",
    "4. **容量規劃評估**\n",
    "   - 當前資源利用率分析\n",
    "   - 瓶頸資源識別\n",
    "   - 擴展建議生成\n",
    "\n",
    "5. **綜合優化建議**\n",
    "   - 分層級的行動建議\n",
    "   - 短期與長期規劃\n",
    "   - 風險評估與緩解策略\n",
    "\n",
    "### 🎯 技術亮點\n",
    "\n",
    "- **機器學習異常檢測**: 使用 Isolation Forest 進行智能異常識別\n",
    "- **時間序列分析**: 趨勢檢測、季節性分析、相關性評估\n",
    "- **統計學方法**: 多種統計指標和分佈分析\n",
    "- **視覺化儀表板**: 多面板綜合性能視覺化\n",
    "- **自動化建議**: 基於數據的智能優化建議生成\n",
    "\n",
    "### 📊 分析覆蓋範圍\n",
    "\n",
    "- **系統層面**: CPU、記憶體、GPU 資源分析\n",
    "- **應用層面**: vLLM 服務性能指標\n",
    "- **業務層面**: QPS、延遲、成功率分析\n",
    "- **運維層面**: 容量規劃、風險評估\n",
    "\n",
    "### 🔧 實用工具\n",
    "\n",
    "- **ComprehensivePerformanceAnalyzer**: 全面性能分析器\n",
    "- **TimeSeriesAnalyzer**: 時間序列分析工具\n",
    "- **LoadTestAnalyzer**: 負載測試分析器\n",
    "- **PerformanceOptimizationAdvisor**: 優化建議生成器\n",
    "\n",
    "### 📋 下一步\n",
    "\n",
    "繼續進行 **04-Alerting_and_Optimization.ipynb**，學習智能告警系統建設和自動化優化策略。\n",
    "\n",
    "---\n",
    "\n",
    "**應用價值**:\n",
    "- 提供數據驅動的性能優化決策依據\n",
    "- 建立標準化的性能評估流程\n",
    "- 支援預防性維護和容量規劃\n",
    "- 降低系統故障風險和運維成本"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}