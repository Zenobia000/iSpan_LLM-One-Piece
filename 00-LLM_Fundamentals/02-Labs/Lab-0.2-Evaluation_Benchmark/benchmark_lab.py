#!/usr/bin/env python3
"""
Lab 0.2: LLMè©•ä¼°åŸºæº–å¯¦è¸ä¸»è…³æœ¬
ä½¿ç”¨evaluation_toolkité€²è¡Œå®Œæ•´çš„æ¨¡å‹è©•ä¼°å¯¦é©—
"""

from evaluation_toolkit import LLMEvaluationToolkit
import pandas as pd
import json
from datetime import datetime
from pathlib import Path

class EvaluationBenchmarkLab:
    """è©•ä¼°åŸºæº–å¯¦é©—å®¤"""

    def __init__(self):
        self.toolkit = LLMEvaluationToolkit()
        self.experiment_dir = Path(f"./results/eval_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        self.experiment_dir.mkdir(parents=True, exist_ok=True)

    def run_multi_model_comparison(self):
        """é‹è¡Œå¤šæ¨¡å‹å°æ¯”è©•ä¼°"""

        print("=== å¤šæ¨¡å‹è©•ä¼°å°æ¯”å¯¦é©— ===")

        # æ¸¬è©¦æ¨¡å‹åˆ—è¡¨ï¼ˆä½¿ç”¨å°æ¨¡å‹é€²è¡Œæ¼”ç¤ºï¼‰
        test_models = [
            "microsoft/DialoGPT-small",
            "microsoft/DialoGPT-medium",
            # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹
        ]

        comparison_results = {}

        for model_name in test_models:
            print(f"\nğŸ“Š è©•ä¼°æ¨¡å‹: {model_name}")

            try:
                # é‹è¡Œå®Œæ•´è©•ä¼°
                result = self.toolkit.run_comprehensive_evaluation(model_name)
                comparison_results[model_name] = result

                # é¡¯ç¤ºé—œéµæŒ‡æ¨™
                if 'overall_evaluation' in result:
                    overall = result['overall_evaluation']
                    print(f"   ç¶œåˆè©•åˆ†: {overall['weighted_overall_score']:.3f}")
                    print(f"   æ€§èƒ½ç­‰ç´š: {overall['grade']}")

            except Exception as e:
                print(f"   âŒ è©•ä¼°å¤±æ•—: {e}")
                comparison_results[model_name] = {'error': str(e)}

        # ç”Ÿæˆå°æ¯”å ±å‘Š
        self._generate_comparison_report(comparison_results)

        return comparison_results

    def _generate_comparison_report(self, results: dict):
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""

        # å‰µå»ºå°æ¯”è¡¨æ ¼
        comparison_data = []

        for model_name, result in results.items():
            if 'error' in result:
                continue

            row = {'æ¨¡å‹': model_name.split('/')[-1]}

            # æå–é—œéµæŒ‡æ¨™
            if 'perplexity' in result:
                row['å›°æƒ‘åº¦'] = f"{result['perplexity']['perplexity']:.2f}"

            if 'language_understanding' in result:
                row['èªè¨€ç†è§£F1'] = f"{result['language_understanding']['f1_score']:.3f}"

            if 'generation_quality' in result:
                row['ç”Ÿæˆè³ªé‡'] = f"{result['generation_quality']['average_quality']['overall_score']:.3f}"

            if 'inference_performance' in result:
                perf = result['inference_performance']['overall_performance']
                row['æ¨ç†é€Ÿåº¦'] = f"{perf.get('avg_tokens_per_second', 0):.1f} TPS"

            if 'overall_evaluation' in result:
                row['ç¶œåˆè©•åˆ†'] = f"{result['overall_evaluation']['weighted_overall_score']:.3f}"

            comparison_data.append(row)

        df = pd.DataFrame(comparison_data)

        # ä¿å­˜çµæœ
        df.to_csv(self.experiment_dir / 'model_comparison.csv', index=False)

        print("\nğŸ“Š æ¨¡å‹å°æ¯”çµæœ:")
        print(df.to_string(index=False))

def main():
    """ä¸»å¯¦é©—å‡½æ•¸"""

    print("Lab 0.2: LLMè©•ä¼°åŸºæº–å¯¦è¸")
    print("=" * 50)

    lab = EvaluationBenchmarkLab()
    results = lab.run_multi_model_comparison()

    print(f"\nâœ… å¯¦é©—å®Œæˆï¼çµæœä¿å­˜åœ¨: {lab.experiment_dir}")

if __name__ == "__main__":
    main()