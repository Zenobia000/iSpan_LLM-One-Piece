{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 7: P-Tuning - Fine-Tuning a BERT Model for NLU Tasks\n",
        "---\n",
        "## Notebook 2: The Training Process\n",
        "\n",
        "**Goal:** In this notebook, you will fine-tune a BERT model using **P-Tuning** on a classification task. P-Tuning differs from Prompt Tuning by using an MLP-based prompt encoder instead of directly optimizing prompt embeddings.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Load a classification dataset and preprocess it for BERT.\n",
        "-   Configure P-Tuning with virtual tokens and prompt encoder.\n",
        "-   Apply P-Tuning to the BERT model.\n",
        "-   Fine-tune the model by training only the prompt encoder parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Dataset and Preprocess\n",
        "\n",
        "We will use the GLUE SST-2 dataset, which is a sentiment classification task. This is ideal for demonstrating P-Tuning on natural language understanding tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load the Base Model\n",
        "\n",
        "Next, we load the BERT model for sequence classification. This will be our base model that we'll apply P-Tuning to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, \n",
        "    num_labels=2  # Binary classification for SST-2\n",
        ")\n",
        "\n",
        "print(\"âœ… Base BERT model loaded.\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Configure P-Tuning\n",
        "\n",
        "Now we configure P-Tuning. Unlike Prompt Tuning which directly optimizes prompt embeddings, P-Tuning uses an MLP-based prompt encoder to generate better prompt representations.\n",
        "\n",
        "#### Key Hugging Face `peft` Components:\n",
        "\n",
        "-   `peft.PromptTuningConfig`: The configuration class for P-Tuning.\n",
        "    -   `task_type=\"SEQ_CLS\"`: We specify the task type for sequence classification.\n",
        "    -   `prompt_tuning_init=\"RANDOM\"`: Random initialization of prompt parameters.\n",
        "    -   `num_virtual_tokens`: The number of virtual tokens (prompt length).\n",
        "    -   `prompt_tuning_init_text=None`: P-Tuning doesn't use text initialization.\n",
        "-   `peft.get_peft_model`: Applies the configuration to our base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import get_peft_model, PromptTuningConfig, TaskType\n",
        "\n",
        "# --- P-Tuning Configuration ---\n",
        "# Note: P-Tuning uses PromptTuningConfig but with specific settings\n",
        "ptuning_config = PromptTuningConfig(\n",
        "    task_type=TaskType.SEQ_CLS,           # Sequence classification task\n",
        "    prompt_tuning_init=\"RANDOM\",          # Random initialization\n",
        "    num_virtual_tokens=10,                # Number of virtual tokens\n",
        "    prompt_tuning_init_text=None,         # No text initialization for P-Tuning\n",
        "    tokenizer_name_or_path=model_checkpoint\n",
        ")\n",
        "\n",
        "# --- Create PeftModel ---\n",
        "peft_model = get_peft_model(model, ptuning_config)\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Set Up Training\n",
        "\n",
        "The final step is to configure and run the training process using the `transformers.Trainer`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1\": f1_score(labels, predictions, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-ptuning-sst2\",\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,                    # Higher learning rate for PEFT\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "# --- Data Collator ---\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# --- Create Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"ðŸš€ Starting training with P-Tuning...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:1000]\")\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Load tokenizer\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"âœ… Dataset loaded and preprocessed.\")\n",
        "print(tokenized_datasets[\"train\"][0].keys())\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
