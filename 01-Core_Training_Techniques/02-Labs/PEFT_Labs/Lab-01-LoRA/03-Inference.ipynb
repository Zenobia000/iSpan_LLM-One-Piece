{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1: LoRA & QLoRA - Fine-Tuning a Llama-2 Model\n",
        "---\n",
        "## Notebook 3: Inference\n",
        "\n",
        "**Goal:** In this notebook, you will learn how to load your fine-tuned PEFT adapter and use it for inference.\n",
        "\n",
        "**You will learn to:**\n",
        "-   Reload the quantized base model.\n",
        "-   Use `peft.PeftModel` to load the LoRA adapter weights from your training checkpoint.\n",
        "-   Prepare a prompt using the tokenizer.\n",
        "-   Use the `generate()` method of the fine-tuned model to get a response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Reload Model and Adapter\n",
        "\n",
        "To perform inference, we first need to load the base model again in the exact same configuration as we did for training (i.e., with 4-bit quantization). Then, we'll load the LoRA adapter on top of it.\n",
        "\n",
        "#### Key Hugging Face `peft` Components:\n",
        "\n",
        "-   `peft.PeftModel`: This class is used to work with a model that has PEFT adapters.\n",
        "    -   `from_pretrained()`: This is the key method. It takes the **base model** as the first argument and the **path to the saved adapter** as the second argument. It then correctly loads the adapter weights and attaches them to the target modules of the base model.\n",
        "\n",
        "We need to find the latest checkpoint saved by the `Trainer` in our output directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNousResearch/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     11\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
            "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
            "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
            "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:5032\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5030\u001b[0m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[1;32m   5031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5032\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5034\u001b[0m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[1;32m   5035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
            "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1369\u001b[0m, in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   1368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1369\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1372\u001b[0m     tied_params \u001b[38;5;241m=\u001b[39m find_tied_parameters(model)\n",
            "File \u001b[0;32m~/python_workstation/side-project/iSpan_LLM-One-Piece/00-Course_Setup/.venv/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:127\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "# --- Base Model and Tokenizer Loading ---\n",
        "# Same configuration as in the training notebook\n",
        "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Load PEFT Adapter ---\n",
        "# Find the latest checkpoint from the training output directory\n",
        "output_dir = \"./lora-llama2-7b-guanaco\"\n",
        "# Find the latest checkpoint directory\n",
        "latest_checkpoint = max(\n",
        "    [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")],\n",
        "    key=os.path.getmtime\n",
        ")\n",
        "print(f\"Loading adapter from: {latest_checkpoint}\")\n",
        "\n",
        "# Load the PEFT model\n",
        "inference_model = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "\n",
        "print(\"✅ Inference model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Perform Inference\n",
        "\n",
        "Now we can use our fine-tuned `inference_model` to generate text. The process is standard for Hugging Face `transformers` models.\n",
        "\n",
        "#### Key Hugging Face `transformers` Components:\n",
        "\n",
        "-   `tokenizer()`: The tokenizer converts our prompt string into a format the model can understand (i.e., a sequence of token IDs). `return_tensors=\"pt\"` ensures the output is a PyTorch tensor. We also move it to the GPU with `.cuda()`.\n",
        "-   `model.generate()`: This is the core method for text generation.\n",
        "    -   It takes the `input_ids` from the tokenizer.\n",
        "    -   `max_new_tokens`: Sets the maximum length of the generated response.\n",
        "    -   `do_sample=True`: Enables sampling-based generation (like top-k or top-p), which usually produces more creative and less repetitive text than greedy decoding.\n",
        "    -   `top_k`: In top-k sampling, the model considers only the `k` most likely next tokens at each step.\n",
        "\n",
        "We will use a prompt that is similar in style to the `guanaco` dataset to see how well our fine-tuning worked.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Prompt ---\n",
            "Could you please tell me about the Yushan National Park in Taiwan?\n",
            "\n",
            "--- Generated Response ---\n",
            "Could you please tell me about the Yushan National Park in Taiwan?\n",
            "\n",
            "I'm planning a trip to Taiwan in a few months and I'd like to know more about the Yushan National Park. Could you please tell me about the park's location, size, and main attractions? What are the best ways to get there?\n",
            "\n",
            "I'm also interested in learning about the park's history and the native flora and fauna that can be found there.\n",
            "\n",
            "Thank you for your time and consideration.\n",
            "\n",
            "\n",
            "\n",
            "### About Yushan National Park\n",
            "\n",
            "Yushan National Park is a protected area located in Taiwan. It is one of the most popular national parks in Taiwan and is known for its beautiful scenery, hiking trails, and diverse wildlife.\n",
            "\n",
            "The park is located in the southern part of Taiwan, near the city of Tainan. It covers an area of about 200 square kilometers and is home to a variety of flora and fauna, including rare species of birds, mammals, and reptiles.\n",
            "\n",
            "The main attractions of the park include the Yushan Peak, which is the highest mountain in Taiwan, and the Jade Mountain, which is a popular hiking destination. The park also features several waterfalls, including the Seven-Step Waterfall and the Shuiyang Waterfall.\n",
            "\n",
            "There are several ways to get to Yushan National Park. The most popular option is to take a bus or a taxi from Tainan to the park's entrance. There are also several tour companies that offer guided tours to the park.\n",
            "\n",
            "The park is open all year round, but the best time to visit is during the spring and autumn seasons when the weather is mild and the flowers are in bloom.\n",
            "\n",
            "The park's history dates back to the 1930s when it was first established as a national park. Over the years, it has undergone several renovations and improvements to make it more accessible to visitors.\n",
            "\n",
            "The park's native flora and fauna include a variety of species of birds, mammals, and reptiles, many of which are endangered or threatened. The park is also home to several rare species of plants, including the Taiwanese endemic species of orchids and bamboo.\n",
            "\n",
            "In conclusion, Yushan National\n"
          ]
        }
      ],
      "source": [
        "# Prepare the prompt\n",
        "prompt = \"Could you please tell me about the Yushan National Park in Taiwan?\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "\n",
        "# Generate text\n",
        "# We use a context manager to disable gradient calculations for efficiency\n",
        "with torch.no_grad():\n",
        "    outputs = inference_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=500,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"--- Prompt ---\")\n",
        "print(prompt)\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Could you please tell me about the Yushan National Park in Taiwan?\\n\\nI'm planning a trip to Taiwan in a few months and I'd like to know more about the Yushan National Park. Could you please tell me about the park's location, size, and main attractions? What are the best ways to get there?\\n\\nI'm also interested in learning about the park's history and the native flora and fauna that can be found there.\\n\\nThank you for your time and consideration.\\n\\n\\n\\n### About Yushan National Park\\n\\nYushan National Park is a protected area located in Taiwan. It is one of the most popular national parks in Taiwan and is known for its beautiful scenery, hiking trails, and diverse wildlife.\\n\\nThe park is located in the southern part of Taiwan, near the city of Tainan. It covers an area of about 200 square kilometers and is home to a variety of flora and fauna, including rare species of birds, mammals, and reptiles.\\n\\nThe main attractions of the park include the Yushan Peak, which is the highest mountain in Taiwan, and the Jade Mountain, which is a popular hiking destination. The park also features several waterfalls, including the Seven-Step Waterfall and the Shuiyang Waterfall.\\n\\nThere are several ways to get to Yushan National Park. The most popular option is to take a bus or a taxi from Tainan to the park's entrance. There are also several tour companies that offer guided tours to the park.\\n\\nThe park is open all year round, but the best time to visit is during the spring and autumn seasons when the weather is mild and the flowers are in bloom.\\n\\nThe park's history dates back to the 1930s when it was first established as a national park. Over the years, it has undergone several renovations and improvements to make it more accessible to visitors.\\n\\nThe park's native flora and fauna include a variety of species of birds, mammals, and reptiles, many of which are endangered or threatened. The park is also home to several rare species of plants, including the Taiwanese endemic species of orchids and bamboo.\\n\\nIn conclusion, Yushan National\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) Compare with the Base Model\n",
        "\n",
        "To truly appreciate the effect of fine-tuning, you can run the same prompt through the `base_model` (without the LoRA adapter) and compare the responses. You will likely see that the fine-tuned model's response is more detailed, better formatted, or more aligned with the instruction-following style of the `guanaco` dataset.\n",
        "\n",
        "---\n",
        "Next, in the final notebook `04-Merge_and_Deploy.ipynb`, we will see how to merge the adapter weights back into the model for easy deployment.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM Engineering Course (Poetry)",
      "language": "python",
      "name": "llm-engineering-course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
